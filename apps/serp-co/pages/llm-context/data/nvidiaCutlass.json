[
  {
    "owner": "nvidia",
    "repo": "cutlass",
    "content": "TITLE: GEMM Triple Loop Nest in CUDA C++\nDESCRIPTION: This code snippet demonstrates the hierarchically blocked structure of GEMM in CUTLASS, mapped to NVIDIA GPUs. It illustrates a nested loop structure that targets concurrency among threadblocks, warps, CUDA cores, and Tensor Cores, while also exploiting memory locality within shared memory and registers. This example shows tiling at various levels to match hardware concurrency and memory locality.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/efficient_gemm.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nfor (int cta_n = 0; cta_n < GemmN; cta_n += CtaTileN) {                     // for each threadblock_y           } threadblock-level concurrency\n  for (int cta_m = 0; cta_m < GemmM; cta_m += CtaTileM) {                   //    for each threadblock_x        } \n\n    for (int cta_k = 0; cta_k < GemmK; cta_k += CtaTileK) {                 //       \"GEMM mainloop\" - no unrolling\n                                                                            //                       - one iteration of this loop is one \"stage\"\n                                                                            //\n      for (int warp_n = 0; warp_n < CtaTileN; warp_n += WarpTileN) {        // for each warp_y                  } warp-level parallelism\n        for (int warp_m = 0; warp_m < CtaTileM; warp_m += WarpTileM) {      //    for each warp_x               } \n                                                                            //\n          for (int warp_k = 0; warp_k < CtaTileK; warp_k += WarpTileK) {         //       fully unroll across CtaTileK\n                                                                            //         - one iteration of this loop is one \"k Group\"\n                                                                            //\n            for (int mma_k = 0; mma_k < WarpTileK; mma_k += MmaK) {         // for each mma instruction         } instruction-level parallelism\n              for (int mma_n = 0; mma_n < WarpTileN; mma_n += MmaN) {       //    for each mma instruction      } \n                for (int mma_m = 0; mma_m < WarpTileM; mma_m += MmaM) {     //        for each mma instruction  } \n                                                                            //\n                  mma_instruction(d, a, b, c);                              //            TensorCore matrix computation\n\n                }   // for mma_m\n              }   // for mma_n\n            }   // for mma_k\n\n          }   // for warp_k\n        }   // for warp_m\n      }   // for warp_n\n\n    }   // for cta_k\n  }   // for cta_m\n}   // for cta_n\n```\n\n----------------------------------------\n\nTITLE: Stream K Swizzling Function in CUTLASS Python\nDESCRIPTION: Modifies the kernel to use the stream K feature of CUTLASS by changing the swizzling function.  The implementation differs depending on the compute capability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Stream K is exposed through the threadblock swizzle method for pre-SM90 kernels,\n# and via the tile_scheduler attribute of the TileDescription for post-SM90 kernels\nif plan.cc < 90:\n    plan.swizzling_functor = cutlass.swizzle.ThreadblockSwizzleStreamK\n    plan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\nelse:\n    # Stream-K is currently only supported for warp-specialized cooperative kernels\n    td.kernel_schedule = cutlass.KernelScheduleType.TmaWarpSpecializedCooperative\n    td.epilogue_schedule = cutlass.EpilogueScheduleType.TmaWarpSpecializedCooperative\n    td.tile_scheduler = cutlass.TileSchedulerType.StreamK\n\n    plan.compile(td)\n    plan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Profile a Specific GEMM Problem Size\nDESCRIPTION: This command profiles a GEMM operation with specific dimensions: M=1024, N=1024, and K=128.  It executes the CUTLASS profiler to benchmark the performance of this specific GEMM configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --m=1024 --n=1024 --k=128\n```\n\n----------------------------------------\n\nTITLE: CollectiveBuilder Class Template Definition in C++\nDESCRIPTION: Defines the `CollectiveBuilder` class template within the `cutlass::gemm::collective` namespace. This class simplifies the construction of `CollectiveMma` instances by accepting higher-level parameters like architecture tag, operator class, data types, and memory layouts. It attempts to build the best-performing `CollectiveMma` based on the provided configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nnamespace cutlass::gemm::collective {\ntemplate <\n  class ArchTag,\n  class OpClass,\n  class ElementA,\n  class GmemLayoutA,\n  int AlignmentA,\n  class ElementB,\n  class GmemLayoutB,\n  int AlignmentB,\n  class ElementAccumulator,\n  class TileShape_MNK,\n  class ClusterShape_MNK,\n  class StageCountType,\n  class KernelScheduleType,\n  class Enable = void\n>\nstruct CollectiveBuilder {\n  static_assert(sizeof(ElementA) == 0, \"Could not build a collective for given parameters.\");\n};\n} // namespace cutlass::gemm::collective\n```\n\n----------------------------------------\n\nTITLE: GEMM with Leaky ReLU Activation (CUTLASS)\nDESCRIPTION: This snippet demonstrates how to run a GEMM with a leaky ReLU activation function, which requires a parameter (negative slope). It sets the `activation` field to a tuple containing the activation function name (\"leaky_relu\") and the parameter value (0.5) and runs the plan.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnegative_slope = 0.5\nplan.activation = (\"leaky_relu\", negative_slope)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: GEMM with GELU Activation\nDESCRIPTION: This example demonstrates a GEMM operation with GELU activation function using bfloat16 data type for inputs and float32 for accumulation and output.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 16 -ta bfloat16 -tb bfloat16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 64 128 64 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb ColumnMajor -ab 8 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle2 -p 512 256 128 -alpha 0.0 -beta 0.5 -gm GemmSplitKParallel -k 5 -bias -activ gelu\n```\n\n----------------------------------------\n\nTITLE: Running Distributed GEMM with Specified Parameters (bash)\nDESCRIPTION: This command executes a GEMM with the specified shape (m, n, k) and runs profiling over a specified number of iterations with a defined warmup period. It also performs a reference check against a single-device GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./65_distributed_gemm --m=16384 --n=106496 --k=16384 --warmup-iterations=10 --iterations=100\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS Profiler with Minimal Compilation\nDESCRIPTION: Configures the build system to compile only the CUTLASS Profiler, exclude unit tests, and enable unity build to minimize compilation time.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_ENABLE_TESTS=OFF -DCUTLASS_UNITY_BUILD_ENABLED=ON\n```\n\n----------------------------------------\n\nTITLE: Launching a GEMM kernel (CUTLASS 3.0+)\nDESCRIPTION: This example demonstrates launching a mixed-precision GEMM kernel targeting Hopper Tensor Cores using the CUTLASS 3.0+ API. It defines the element types, layouts, and alignment for the A, B, C, and D matrices. It then configures the core kernel components, including the accumulator type, architecture tag, operator class, tile shapes, cluster shapes, stage count, and kernel schedule. Finally, it allocates device memory using `cutlass::DeviceAllocation`, sets up strides, and launches the GEMM kernel using `cutlass::gemm::device::GemmUniversalAdapter`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_23\n\nLANGUAGE: c++\nCODE:\n```\n#include \"cutlass/cutlass.h\"\n#include \"cutlass/epilogue/collective/default_epilogue.hpp\"\n#include \"cutlass/epilogue/thread/linear_combination.h\"\n#include \"cutlass/gemm/collective/collective_builder.hpp\"\n#include \"cutlass/gemm/device/gemm_universal_adapter.h\"\n#include \"cutlass/gemm/kernel/gemm_universal.hpp\"\n\n#include \"cutlass/util/host_tensor.h\"\n#include \"cutlass/util/packed_stride.hpp\"\n\nusing namespace cute;\n\nint main(int argc, char const **args) {\n\n  // A matrix configuration\n  using         ElementA    = cutlass::half_t;                                // Element type for A matrix operand\n  using         LayoutA     = cutlass::layout::RowMajor;                      // Layout type for A matrix operand\n  constexpr int AlignmentA  = 128 / cutlass::sizeof_bits<ElementA>::value;    // Memory access granularity/alignment of A matrix in units of elements (up to 16 bytes)\n\n  // B matrix configuration\n  using         ElementB    = cutlass::half_t;                                // Element type for B matrix operand\n  using         LayoutB     = cutlass::layout::ColumnMajor;                   // Layout type for B matrix operand\n  constexpr int AlignmentB  = 128 / cutlass::sizeof_bits<ElementB>::value;    // Memory access granularity/alignment of B matrix in units of elements (up to 16 bytes)\n\n  // C/D matrix configuration\n  using         ElementC    = cutlass::half_t;                                // Element type for C and D matrix operands\n  using         LayoutC     = cutlass::layout::ColumnMajor;                   // Layout type for C and D matrix operands\n\n  // Core kernel configurations\n  using ElementAccumulator  = float;                                          // Element type for internal accumulation\n  using ArchTag             = cutlass::arch::Sm90;                            // Tag indicating the minimum SM that supports the intended feature\n  using OperatorClass       = cutlass::arch::OpClassTensorOp;                 // Operator class tag\n  using TilesShape          = Shape<_128,_128,_64>;                           // Threadblock-level tile size\n  using ClusterShape        = Shape<_1,_2,_1>;                                // Shape of the threadblocks in a cluster\n  using StageCountType = cutlass::gemm::collective::StageCountAuto;           // Stage count maximized based on the tile size\n  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;       // Kernel to launch based on the default setting in the Collective Builder\n\n  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<\n      ArchTag, OperatorClass,\n      ElementA, LayoutA, AlignmentA,\n      ElementB, LayoutB, AlignmentB,\n      ElementAccumulator,\n      TilesShape, ClusterShape,\n      cutlass::gemm::collective::StageCountAuto,\n      cutlass::gemm::collective::KernelScheduleAuto\n    >::CollectiveOp;\n\n  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<\n      cutlass::gemm::TagToStrideC_t<LayoutC>,\n      cutlass::gemm::TagToStrideC_t<LayoutC>,\n      cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>>;\n\n  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<\n      Shape<int,int,int>, // Indicates ProblemShape\n      CollectiveMainloop,\n      CollectiveEpilogue\n  >;\n\n  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;\n\n  Gemm gemm_op;\n  cutlass::Status status;\n\n  //\n  // Define the problem size\n  //\n\n  int M = 512;\n  int N = 256;\n  int K = 128;\n\n  float alpha = 1.25f;\n  float beta = -1.25f;\n\n  //\n  // Allocate device memory\n  //\n\n  cutlass::DeviceAllocation<typename Gemm::ElementA> block_A;\n  cutlass::DeviceAllocation<typename Gemm::ElementB> block_B;\n  cutlass::DeviceAllocation<typename Gemm::ElementC> block_C;\n  cutlass::DeviceAllocation<typename Gemm::EpilogueOutputOp::ElementOutput> block_D;\n\n  using StrideA = typename Gemm::GemmKernel::StrideA;\n  using StrideB = typename Gemm::GemmKernel::StrideB;\n  using StrideC = typename Gemm::GemmKernel::StrideC;\n  using StrideD = typename Gemm::GemmKernel::StrideD;\n\n  StrideA stride_A;\n  StrideB stride_B;\n  StrideC stride_C;\n  StrideD stride_D;\n\n  stride_A = cutlass::make_cute_packed_stride(StrideA{}, {M, K, 1});\n  stride_B = cutlass::make_cute_packed_stride(StrideB{}, {N, K, 1});\n  stride_C = cutlass::make_cute_packed_stride(StrideC{}, {M, N, 1});\n  stride_D = cutlass::make_cute_packed_stride(StrideD{}, {M, N, 1});\n\n  block_A.reset(M * K);\n  block_B.reset(K * N);\n  block_C.reset(M * N);\n  block_D.reset(M * N);\n\n  //\n  // Launch GEMM on the device\n  //\n\n  status = gemm_op({\n    cutlass::gemm::GemmUniversalMode::kGemm,\n    {M, N, K},\n    block_A.get(),\n    stride_A,\n    block_B.get(),\n    stride_B,\n    {block_C.get(), stride_C, block_D.get(), stride_D, {alpha, beta}}\n  });\n\n  if (status != cutlass::Status::kSuccess) {\n    return -1;\n  }\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Running the CUTLASS Profiler for Convolution\nDESCRIPTION: Executes the CUTLASS Profiler for a convolution operation with specified parameters, profiling single-precision convolution forward propagation (s1688fprop).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=s1688fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --pad_h=1 --pad_w=1\n```\n\n----------------------------------------\n\nTITLE: Conv2d: Running a Conv2d Fprop\nDESCRIPTION: This snippet demonstrates running a Conv2d forward propagation (Fprop) using the CUTLASS Python interface. It initializes a `cutlass.Conv2dFprop` plan with a given element type and accumulator type and then runs it with the input, weight, and output tensors, along with stride, padding, dilation, alpha, and beta parameters. The result is compared to a PyTorch conv2d operation for validation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Specifying `element_accumulator` is not required if it is the same as `element`\nplan = cutlass.Conv2dFprop(element=dtype, element_accumulator=torch.float32)\nplan.run(input, weight, tensor_C, output, stride, padding, dilation, alpha, beta, print_module=print_module)\n\noutput_torch = alpha * torch.ops.aten.conv2d(\n    input, weight, stride=stride, padding=padding, dilation=dilation\n) + beta * tensor_C\n\nassert torch.equal(output_torch, output)\n```\n\n----------------------------------------\n\nTITLE: Defining Kernel Schedules for GEMM with L2 Prefetch\nDESCRIPTION: These type definitions define the kernel schedules for the GEMM operation with L2 prefetch. The `KernelTmaWarpSpecializedFP8FastAccumWithPrefetch` schedule utilizes a single set of warps for both A and B operands, while `KernelTmaWarpSpecializedFP8FastAccumWithPrefetchAndSplitDMA` separates warps for A and B. The split DMA version is expected to be more performant due to weights being loaded into shared memory ahead of the `griddepcontrol`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/README.md#_snippet_1\n\nLANGUAGE: cxx\nCODE:\n```\n// Without separate warps for A and B\nusing KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccumWithPrefetch;\n\n// With separate warps for A and B\nusing KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccumWithPrefetchAndSplitDMA;\n```\n\n----------------------------------------\n\nTITLE: Fused Multiply-Add of Half-Precision Arrays C++\nDESCRIPTION: This example demonstrates the use of the `multiply_add` function object with arrays of half-precision elements. It performs elementwise multiply-add operations on each element of the arrays, leveraging potential native SIMD instructions for increased performance on compute capability SM60 and beyond.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nstatic int const kN = 8;\n\nArray<half_t, kN> a;\nArray<half_t, kN> b;\nArray<half_t, kN> c;\nArray<half_t, kN> d;\n\nmultiply_add<Array<half_t, kN>> mad_op;\n\nd = mad_op(a, b, c);   // efficient multiply-add for Array of half-precision elements\n```\n\n----------------------------------------\n\nTITLE: Creating a TiledCopy Instance (C++)\nDESCRIPTION: This code creates a `TiledCopy` instance named `copyA` using `make_tiled_copy`.  It specifies a `Copy_Atom` (using `UniversalCopy<uint128_t>` to force 128-bit copies of `TA` elements), a thread layout of 32x8, and a value layout of 4x1. This TiledCopy will be used to transfer data between global and shared memory. The function `print_latex` will print a visualization of the layout, but is not directly involved in the data transfer.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\n  TiledCopy copyA = make_tiled_copy(Copy_Atom<UniversalCopy<uint128_t>, TA>{},  // Atom: Copy TAs as if they were uint128_t\n                                    Layout<Shape<_32,_8>>{},                    // Thr layout 32x8 m-major\n                                    Layout<Shape< _4,_1>>{});                   // Val layout  4x1 m-major\n  print_latex(copyA);\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with CMake using 90a\nDESCRIPTION: This snippet demonstrates how to use CMake to configure the build process with the `-DCUTLASS_NVCC_ARCHS` option to specify the target CUDA architecture as 90a for maximizing performance on Hopper GH100.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ncmake .. -DCUTLASS_NVCC_ARCHS=\"90a\"\n```\n\n----------------------------------------\n\nTITLE: Running Ampere Gather/Scatter Convolution Example\nDESCRIPTION: This command executes the example kernel for Ampere gather/scatter convolution with a specified number of images (--n) and an image size (--i). The --no-check flag disables result verification. The output shows the performance of both dense and gather/scatter convolution kernels in terms of execution time and TFLOP/s.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$> ./examples/59_ampere_gather_scatter_conv/59_ampere_gather_scatter_conv --n=131072 --i=128 --no-check\nAmpere convolution forward propogation kernel supporting both affine and gather/scatter tensors.\n\nAllocating tensors ... done.\nInitializing data ... done.\nInitializing gather/scatter index buffers ... done.\n\nRunning dense fprop kernel\nConv TFLOP count = 0.927713\nConv dense perf: 31.027376ms | TFLOP/s = 29.899819\n\nRunning gather/scatter fprop kernel\nConv TFLOP count = 0.927713\nConv gather/scatter perf: 28.973721ms | TFLOP/s = 32.019117\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Threadblock-level GEMM Concept (C++)\nDESCRIPTION: This code defines a concept for threadblock-level matrix multiply accumulate operators in CUTLASS. It specifies the required types and methods for an `Mma` struct, including the shapes, data types, layouts, iterators, fragment objects, and the warp-level matrix multiply operator. It defines an `operator()` method that computes a matrix product accumulated in `D` using iterators and fragments.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nstruct Mma {\n  /// Shape of warp-level matrix operation (concept: GemmShape)\n  struct Shape;\n\n  /// Data type of multiplicand A (concept: numeric type)\n  struct ElementA;\n\n  /// Layout of multiplicand A (concept: Layout)\n  struct LayoutA;\n\n  /// Data type of multiplicand B (concept: numeric type)\n  struct ElementB;\n\n  /// Layout of multiplicand B (concept: Layout)\n  struct LayoutB;\n\n  /// Data type of accumulator matrix C (concept: numeric type)\n  struct ElementC;\n\n  /// Layout of accumulator matrix C (concept: LayoutC)\n  struct LayoutC;\n\n  /// Iterator of A operand in shared memory - satisfies: ReadableRandomAccessTileIteratorConcept\n  struct IteratorA;\n\n  /// Fragment object loaded from IteratorA (concept: Array<ElementA, ..>)\n  struct FragmentA;\n\n  /// Iterator of B operand in shared memory - satisfies: ReadableRandomAccessTileIteratorConcept\n  struct IteratorB;\n\n  /// Fragment object loaded from IteratorB (concept: Array<ElementB, ..>)\n  struct FragmentB;\n\n  /// Iterator of C operand in shared memory -\n  ///    satisfies: ReadableRandomAccessTileIteratorConcept | WriteableRandomAccessTileIteratorConcept\n  struct IteratorC;\n\n  /// Fragment object loaded from IteratorC (concept: Array<ElementC, ..>)\n  struct FragmentC;\n\n  /// Warp-level matrix multiply operator (concept: satisfies gemm::warp::Mma)\n  struct Operator;\n\n  //\n  // Method\n  //\n\n  /// Computes a matrix product accumulated in D\n  CUTLASS_DEVICE\n  void operator()(\n    FragmentC &D, \n    IteratorA iter_A, \n    IteratorB iter_B, \n    FragmentC const &C);\n};\n```\n\n----------------------------------------\n\nTITLE: Emit PyTorch CUDA Extension\nDESCRIPTION: This code emits a PyTorch CUDA extension from the CUTLASS kernel. It constructs the CUTLASS operation and then uses `cutlass.emit.pytorch` to generate the necessary files for the extension. The `jit=True` argument specifies that the extension should be compiled and loaded just-in-time.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nop = plan.construct()\ngrouped_gemm = cutlass.emit.pytorch(op, name='grouped_gemm', cc=plan.cc, sourcedir='out', jit=True)\n```\n\n----------------------------------------\n\nTITLE: Profiling SGEMM Kernels (CUDA Core) (CUTLASS)\nDESCRIPTION: This command line invokes the CUTLASS profiler to analyze the performance of SGEMM (single-precision GEMM) kernels with specific matrix dimensions. It sets the m, n, and k parameters to 3456, 4096, and 4096, respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=3456 --n=4096 --k=4096\n```\n\n----------------------------------------\n\nTITLE: Defining the GEMM Kernel\nDESCRIPTION: This snippet defines the overall GEMM kernel type by combining the previously defined CollectiveMainloop and CollectiveEpilogue. It uses the `GemmUniversal` template and requires the shape, mainloop, and epilogue types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_40\n\nLANGUAGE: c++\nCODE:\n```\nusing GemmKernel = cutlass::gemm::kernel::GemmUniversal<\n    Shape<int,int,int,int>,\n    CollectiveMainloop,\n    CollectiveEpilogue\n  >;\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Sparse BlockScaled Kernel Dispatch (1SM)\nDESCRIPTION: This policy uses 1 SM `tcgen05.mma.sp` instruction and automatically selects the optimal instruction kind (mxf8f6f4, mxf4, nvf4mxf4).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\n`KernelSparseTmaWarpSpecialized1SmBlockScaledSm100`\n```\n\n----------------------------------------\n\nTITLE: Profiling Subset Tensor Core Convolution Kernels\nDESCRIPTION: This command line profiles a subset of Tensor Core convolution kernels implementing forward propagation (fprop) using the CUTLASS profiler. It specifies the kernel pattern and convolution parameters (n, h, w, c, k, r, s). The output presents performance metrics such as runtime, memory bandwidth, and GFLOP/s.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*fprop_optimized_f16 --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Run GEMM with ReLU Activation - Python\nDESCRIPTION: This snippet demonstrates how to run a GEMM operation with a ReLU (Rectified Linear Unit) element-wise activation function using the CUTLASS Python interface. It initializes a new output tensor `tensor_D_relu`, sets the `activation` field of the GEMM plan to `cutlass.epilogue.relu`, and executes the GEMM. This applies the ReLU function element-wise after the GEMM computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/01_epilogue.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntensor_D_relu = np.zeros(tensor_C.shape).astype(type_D)\nplan.activation = cutlass.epilogue.relu\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D_relu, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: List Available Activation Functions - Python\nDESCRIPTION: This snippet retrieves and prints a list of available element-wise activation functions supported by CUTLASS. It uses the `activations()` method of the GEMM plan to obtain the list of activation functions and then iterates through the list, printing each activation function.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/01_epilogue.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nactivations = plan.activations()\nfor activation in activations:\n    print(activation)\n```\n\n----------------------------------------\n\nTITLE: Numeric Conversion Example in CUTLASS\nDESCRIPTION: This example demonstrates how to perform numeric conversions between data types using `NumericConverter` in CUTLASS. It converts a `float` to `half_t` and `tfloat32_t`. Requires the `cutlass/numeric_conversion.h` and `cutlass/numeric_types.h` headers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include \"cutlass/numeric_conversion.h\"\n#include \"cutlass/numeric_types.h\"\n\nNumericConverter<half_t, float>     convert_f32_to_f16;\nNumericConverter<tfloat32_t, float> convert_f32_to_tf32;\n\nhalf_t     x = convert_f32_to_f16(3.14159f);\ntfloat32_t y = convert_f32_to_tf32(3.14159f);\n```\n\n----------------------------------------\n\nTITLE: Switching to SIMT GEMM in CUTLASS Python\nDESCRIPTION: Changes the operation mode of the CUTLASS GEMM to use SIMT (Single Instruction, Multiple Threads) kernels instead of Tensor Core operations. This demonstrates how to explicitly choose a different GEMM implementation. It then runs and verifies the result.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntensor_D_simt = np.zeros(tensor_C.shape).astype(type_D)\nplan.opclass = cutlass.OpcodeClass.Simt\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D_simt, alpha, beta, print_module=print_module)\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.testing.assert_array_equal(tensor_D, tensor_D_simt)\n```\n\n----------------------------------------\n\nTITLE: Running Default GEMM with CUTLASS Python API\nDESCRIPTION: This snippet declares and runs a default GEMM operation using the CUTLASS Python interface. It initializes a `cutlass.Gemm` object with specified data types and layout, and then executes the GEMM operation using the previously initialized tensors. The `print_module` argument controls whether the emitted C++ code is printed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# We specify `element_accumulator` here so as to match the kernel run by NumPy below. However,\n# specifying `element_accumulator` is not required if it is the same as `element`\nplan = cutlass.Gemm(element=dtype, layout=cutlass.LayoutType.RowMajor, element_accumulator=np.float32)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Installing CUTLASS Python interface\nDESCRIPTION: Installs the CUTLASS Python interface using pip. This is necessary when running the notebook on Colab or other environments where CUTLASS is not pre-installed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!#pip install nvidia-cutlass\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Blackwell Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Blackwell GPU architecture using CMake.  The `CUTLASS_NVCC_ARCHS` flag specifies the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=100a              # compiles for NVIDIA Blackwell GPU architecture\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (NoSmem, 2SM)\nDESCRIPTION: This epilogue dispatch policy configures post-processing operations following matrix multiplication without using shared memory (NoSmem) across two SMs (Streaming Multiprocessors). It supports both dense and sparse matrix operations and is applicable for legacy and narrow precision computations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::NoSmemWarpSpecialized2Sm`\n```\n\n----------------------------------------\n\nTITLE: GEMM Array Example\nDESCRIPTION: This example showcases an array GEMM operation with float16 inputs and float32 output. The example specifies tensor layouts, block sizes, swizzle functions, and batch size.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 16 -ta float16 -tb float16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb RowMajor -ab 8 -lc ColumnMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle4 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Array -k 1 -batch 2\n```\n\n----------------------------------------\n\nTITLE: Creating a PyTorch CUDA Extension from CUTLASS Kernel\nDESCRIPTION: This snippet demonstrates how to create a just-in-time (JIT) compiled PyTorch CUDA extension from a CUTLASS kernel. It constructs the CUTLASS operation using `plan.construct()` and then calls `cutlass.emit.pytorch` to generate the CUDA extension.  The extension is then compiled and loaded for immediate use.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nop = plan.construct()\ngrouped_gemm = cutlass.emit.pytorch(op, name='grouped_gemm', cc=plan.cc, sourcedir='out', jit=True)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS 3.0 Hopper Kernel Example in C++\nDESCRIPTION: This code snippet demonstrates how to assemble a CUTLASS 3.0 Hopper kernel. It shows the steps involved in generating the collective layer mainloop specialization, specifying the collective layer epilogue type, composing the mainloop and epilogue together at the kernel layer, and wrapping up the kernel with the device adapter to obtain a host-side handle.  This is a typical example demonstrating the composition of CUTLASS GEMM kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// Step 1: Generate the required collective layer mainloop specialization\nusing CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<\n    ArchTag, OperatorClass,\n    ElementA, LayoutA, AlignmentA,\n    ElementB, LayoutB, AlignmentB,\n    ElementAccumulator,\n    TilesShape, ClusterShape,\n    cutlass::gemm::collective::StageCountAuto,\n    cutlass::gemm::collective::KernelScheduleAuto\n  >::CollectiveOp;\n\n// Step 2: Specify the collective layer epilogue type\nusing CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<\n    ElementC,\n    cutlass::gemm::TagToStrideC_t<LayoutC>,\n    cutlass::gemm::TagToStrideC_t<LayoutC>,\n    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>>;\n\n// Step 3: Compose the mainloop and epilogue together at the kernel layer\nusing GemmKernel = cutlass::gemm::kernel::GemmUniversal<\n    cute::Shape<int,int,int,int>, // ProblemShape [M,N,K,L]\n    CollectiveMainloop,\n    CollectiveEpilogue\n>;\n\n// Step 4: Wrap up the kernel::GemmUniversal kernel class\n// with the device adapter to obtain a host-side handle to the kernel\nusing GemmHandle = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;\n```\n\n----------------------------------------\n\nTITLE: Declaring and Running a GEMM with CUTLASS Python\nDESCRIPTION: Declares and runs a GEMM operation using the CUTLASS Python interface. It initializes a default GEMM operation for the given device (assuming SM80), leveraging FP16 Tensor Core operations, and then executes it on the previously created tensors.  It also verifies the result by comparing against a NumPy equivalent.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# We specify `element_accumulator` here so as to match the kernel run by NumPy below. However,\n# specifying `element_accumulator` is not required if it is the same as `element`\nplan = cutlass.Gemm(element=dtype, layout=cutlass.LayoutType.RowMajor, element_accumulator=np.float32)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\nLANGUAGE: python\nCODE:\n```\ntensor_D_numpy = (alpha * (tensor_A @ tensor_B)) + (beta * tensor_C)\nnp.testing.assert_array_equal(tensor_D, tensor_D_numpy)\n```\n\n----------------------------------------\n\nTITLE: TensorView Usage Example\nDESCRIPTION: This C++ code demonstrates the usage of `TensorView` to access elements within a tensor, with bounds checking. A `layout::ColumnMajor` object is created with a leading dimension `ldm`. A `TensorView` object, `view`, is constructed using the pointer `ptr`, the `layout` object, and the `extent`. The `contains()` method is used to verify if the specified `(row, column)` coordinate is within the bounds of the tensor before accessing it using `at()`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nint4_t *ptr = ...;\nint ldm = ...;\nMatrixCoord extent = ...;\n\nint row = ...;\nint column = ...;\n\nlayout::ColumnMajor layout(ldm);\nTensorView<int4_t, layout::ColumnMajor> view(ptr, layout, extent);\n\nMatrixCoord coord = {row, column};\n\nif (view.contains(coord)) {     // verify coordinate is in bounds before performing access\n  \n  int4_t x = ref.at(coord);  \n  ref.at({row, column}) = x * 2_s4;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Advancing to the Next Tile in C++\nDESCRIPTION: This C++ code snippet demonstrates the `advance()` function for the `Conv2dFpropActivationTileAccessIteratorAnalytic` class. It advances the iterator to the next tile by incrementing the filter position indices (`filter_s_`, `filter_r_`, `filter_c_`) and resetting them when they reach the problem size boundaries. This ensures that the iterator moves through the filter positions correctly.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n// cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h\n\n// Advance to the next access\nvoid advance() {\n  // moves to the next tile\n  ++filter_s_;\n  if (filter_s_ < problem_size_.S) {\n    return;\n  }\n  filter_s_ = 0;\n  \n  ++filter_r_;\n  if (filter_r_ < problem_size_.R) {\n    return;\n  }\n  filter_r_ = 0;\n  \n  filter_c_ += Shape::kRow * problem_size_.split_k_slices;\n}\n```\n\n----------------------------------------\n\nTITLE: Conv2d F32 Fprop Optimized Example\nDESCRIPTION: This example illustrates a Conv2d forward propagation with optimized implicit GEMM, using float32 tensors and TensorNHWC layout.  It includes configuration for padding, stride, and dilation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 16 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 2 -lb TensorNHWC -ab 2 -lc TensorNHWC -ac 2 -te float32 -ep LinearCombination -sw IdentitySwizzle2 -co fprop -st Strided -ia optimized -sm Serial -k 2 -nhwc 1 4 4 12 -krsc 8 3 3 12 -pad 0 0 0 0 -stride 3 3 -dilation 1 1 -alpha 1.0 -beta 1.0\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Device-wide GEMM Example (C++)\nDESCRIPTION: This example demonstrates how to launch a mixed-precision GEMM targeting Volta Tensor Cores using the `cutlass::gemm::device::Gemm` API. It defines the data types, layouts, accumulator type, architecture, and launches the GEMM operation.  The code initializes the GEMM operator and calls it with the matrix dimensions, pointers to the input and output matrices, and alpha/beta scaling factors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n  using Gemm = cutlass::gemm::device::Gemm<\n    cutlass::half_t,                           // ElementA\n    cutlass::layout::ColumnMajor,              // LayoutA\n    cutlass::half_t,                           // ElementB\n    cutlass::layout::ColumnMajor,              // LayoutB\n    cutlass::half_t,                           // ElementOutput\n    cutlass::layout::ColumnMajor,              // LayoutOutput\n    float,                                     // ElementAccumulator\n    cutlass::arch::OpClassTensorOp,            // tag indicating Tensor Cores\n    cutlass::arch::Sm70                        // tag indicating target GPU compute architecture\n  >;\n\n  Gemm gemm_op;\n  cutlass::Status status;\n \n  //\n  // Launch GEMM on the device\n  //\n \n  status = gemm_op({\n    {m, n, k},\n    {ptrA, lda},\n    {ptrB, ldb},\n    {ptrC, ldc},\n    {ptrD, ldd},\n    {alpha, beta}\n  });\n\n  if (status != cutlass::Status::kSuccess) {\n    return -1;\n  }\n```\n\n----------------------------------------\n\nTITLE: Profile CUTLASS GEMM Performance\nDESCRIPTION: This snippet uses the `CUDAEventProfiler` to measure the execution time of the CUTLASS GEMM kernel. It sets warmup and profile iterations, and then calls the profiler with the CUTLASS plan and input tensors, and reports the measured duration in milliseconds.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/04_epilogue_visitor.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwarmup_iterations = 10\nprofile_iterations = 50\n# Profile CUTLASS fused kernel\nduration = CUDAEventProfiler(\n    plan, warmup_iterations, profile_iterations,\n    tensor_A, tensor_B, tensor_C, tensor_D, \n    visitor_args=visitor_args)()\n\nprint(f\"CUTLASS duration: {duration:.2f} ms\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Tile Descriptions in CUTLASS Python\nDESCRIPTION: Accesses and prints tile descriptions for the GEMM kernel from the CUTLASS profiler. It retrieves available configurations (e.g., cluster, threadblock, and warp shape) for the kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntiles = plan.tile_descriptions()\nprint('{} tile descriptions returned'.format(len(tiles)))\nnum_print = 10\nprint('First {} tile descriptions are:'.format(num_print))\nfor td in tiles[:num_print]:\n    print(td)\n```\n\n----------------------------------------\n\nTITLE: TensorFill Example\nDESCRIPTION: This example shows how to use `cutlass::reference::host::TensorFill` and `cutlass::reference::device::TensorFill` to initialize a tensor with a uniform value in both host and device memory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/layout/matrix.h>\n#include <cutlass/util/reference/host/tensor_fill.h>\n#include <cutlass/util/reference/device/tensor_fill.h>\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n  int rows = 128;\n  int columns = 64;\n\n  float x = 3.14159f;\n\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> tensor({rows, columns});\n\n  // Initialize in host memory\n  cutlass::reference::host::TensorFill(tensor.host_view(), x);\n\n  // Initialize in device memory\n  cutlass::reference::device::TensorFill(tensor.device_view(), x);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Compute Mainloop (C++)\nDESCRIPTION: This code demonstrates a simple compute mainloop that iterates over tiles of global memory, copies them into shared memory, and performs the matrix-multiply operation. It copies data from global memory tensors `tAgA` and `tBgB` to shared memory tensors `tAsA` and `tBsB` using the `copy` function. Then, it computes the matrix multiplication using `gemm` on the thread-partitioned shared memory tensors `tCsA`, `tCsB`, and `tCrC`. Synchronization primitives ensure data consistency across threads.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_25\n\nLANGUAGE: c++\nCODE:\n```\n// TUTORIAL: Example of a very simple compute mainloop\n//   copy(.) operates on the global and shared memory via the tA|tB partitioning\n//   gemm(.) operates on the shared and register memory via the tC partitioning\n\nauto K_TILE_MAX = size<2>(tAgA);\n\nfor (int k_tile = 0; k_tile < K_TILE_MAX; ++k_tile)\n{\n  // Copy gmem to smem with tA|tB thread-partitioned tensors\n  copy(tAgA(_,_,k_tile), tAsA);      // A   (THR_M,THR_K) -> (THR_M,THR_K)\n  copy(tBgB(_,_,k_tile), tBsB);      // B   (THR_N,THR_K) -> (THR_N,THR_K)\n\n  cp_async_fence();        // Label the end of (potential) cp.async instructions\n  cp_async_wait<0>();      // Sync on all (potential) cp.async instructions\n  __syncthreads();         // Wait for all threads to write to smem\n\n  // Compute gemm on tC thread-partitioned smem\n  gemm(tCsA, tCsB, tCrC);            // (THR_M,THR_N) += (THR_M,BLK_K) * (THR_N,BLK_K)\n  __syncthreads();         // Wait for all threads to read from smem\n}\n```\n\n----------------------------------------\n\nTITLE: FP8 GEMM Specialization in CUTLASS Python\nDESCRIPTION: Demonstrates running CUTLASS's FP8 GEMMs using PyTorch.  It checks for PyTorch availability and for the float8_e4m3fn data type before proceeding.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    import torch\nexcept ImportError:\n    print(\"PyTorch is not available. Skipping FP8 example\")\n    import sys; sys.exit(0)\n\nif not hasattr(torch, \"float8_e4m3fn\"):\n    print(\"Version of PyTorch does not have the float8_e4m3fn data type. Skipping FP8 example\")\n    import sys; sys.exit(0)\n```\n\n----------------------------------------\n\nTITLE: Adding Blockwise GEMM Executable Example in CMake\nDESCRIPTION: This snippet uses a CMake macro `cutlass_example_add_executable` to create an executable for a blockwise GEMM (General Matrix Multiplication) example. It specifies the source file (`81_blackwell_gemm_blockwise.cu`) and test command options, random problem sizes, epilogue options, and small problem sizes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  81_blackwell_gemm_blockwise \n  81_blackwell_gemm_blockwise.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_EPILOGUE\n  TEST_SMALL\n)\n```\n\n----------------------------------------\n\nTITLE: Running the CUTLASS Profiler for All 2D Convolution Operations\nDESCRIPTION: Executes the CUTLASS Profiler for all CUTLASS 2-D convolution operators with the specified parameters.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --operation=conv2d --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Compiling CUTLASS Convolution Kernel with CMake\nDESCRIPTION: This command uses CMake to compile a specific CUTLASS CUDA convolution kernel for NVIDIA Ampere and Turing architectures. It specifies the target architectures and the kernel to be compiled, which is an optimized SIMT kernel for forward propagation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc\nmake cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Row Calculation in Rank2K Scheduler (C++)\nDESCRIPTION: This snippet demonstrates how to calculate the row index `i` given a threadblock ID `t` for a lower-triangular matrix in the Rank2K scheduler. This is part of mapping threadblock IDs to tile coordinates for efficient computation of triangular matrices. The formula accounts for zero-indexing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\ni = ceil(sqrt(2t + 2.25) - 0.5) - 1\n```\n\n----------------------------------------\n\nTITLE: Aggregate Initialization C++\nDESCRIPTION: This code snippet illustrates the preferred method of aggregate initialization when returning a struct. It avoids repeating the struct's name and assumes that the function's return type is known. This approach should be preferred when the function return type is known instead of using the name of the struct.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_13\n\nLANGUAGE: c++\nCODE:\n```\nstruct foo_result {\n  float value = 0.0f;\n  float error = 0.0f;\n  bool success = false;\n};\n\nfoo_result foo(std::span<const float> input) {\n  // ... code  ...\n\n  // Prefer this.  We know what type the function returns.\n  return {val, err, ok}; // prefer this\n\n  // Naming foo_result again here is unnecessary.\n  // return foo_result{val, err, ok};\n}\n```\n\n----------------------------------------\n\nTITLE: Parallel Split-K in CUTLASS Python\nDESCRIPTION: This snippet demonstrates the use of parallel split-K in the CUTLASS Python interface.  It initializes an empty tensor `grad_weight_parallel` and runs the wgrad operation with a parallel split-K strategy using 5 slices. After the operation, it asserts that the `grad_weight_parallel` tensor matches the expected `grad_weight_torch` tensor from a PyTorch implementation. Parallel split-K divides the reduction dimension across multiple threadblocks, storing partial results in global memory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Parallel Split-K with 5 slices\ngrad_weight_parallel = torch.zeros_like(grad_weight)\nplan_wgrad.run(\n    grad_output, input, tensor_C_wgrad, grad_weight_parallel,  \n    stride, padding, dilation, alpha, beta, print_module=print_module, split_k=(\"parallel\", 5))\nassert torch.equal(grad_weight_torch, grad_weight_parallel)\n```\n\n----------------------------------------\n\nTITLE: Conv2d Fprop F16 Few Channels Configuration\nDESCRIPTION: This example demonstrates a device convolution operation with fprop and few channels using float16 data type and TensorOp. It utilizes implicit GEMM and specifies tensor layouts in NHWC format.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 16 -ta float16 -tb float16 -tc float16 -tacc float32 -m multiply_add -op TensorOp -b 128 128 64 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 2 -lb TensorNHWC -ab 2 -lc TensorNHWC -ac 8 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -co fprop -st Strided -ia few_channels -sm Serial -k 1 -nhwc 1 16 16 2 -krsc 16 3 3 2 -pad 1 1 1 1 -stride 2 2 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: 3xTF32 GEMM Specialization in CUTLASS Python\nDESCRIPTION: Declares and runs a GEMM using the 3xTF32 feature of CUTLASS. This requires a device with SM80 or higher and utilizes the `multiply_add_fast_f32` math operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom cutlass.backend.utils.device import device_cc\n\n# 3xTF32 requires SM80 or higher\nif device_cc() >= 80:\n    plan = cutlass.op.Gemm(element=np.float32, layout=cutlass.LayoutType.RowMajor)\n    plan.math_operation = cutlass.MathOperation.multiply_add_fast_f32\n\n    # Create input/output tensors in FP32\n    A, B = [np.ones((128, 128)).astype(np.float32) for _ in range(2)]\n    C, D = [np.zeros((128, 128)).astype(np.float32) for _ in range(2)]\n\n    # Run the GEMM\n    plan.run(A, B, C, D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Loop Unrolling with CUTLASS_PRAGMA_UNROLL in C++\nDESCRIPTION: This code snippet demonstrates how to use `CUTLASS_PRAGMA_UNROLL` to instruct the CUDA compiler to unroll a loop. Loop unrolling is a crucial optimization technique in CUTLASS to achieve high performance by mapping array elements to registers and constructing an efficient instruction schedule. The loop's iteration count must be known at compile time for unrolling to be effective.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nint const kN = 8;\nArray<float, kN> x;                       // Array we would like to store in registers\n\nCUTLASS_PRAGMA_UNROLL                     // Directs the CUDA compiler to unroll this loop.\nfor (int idx = 0; idx < kN; ++idx) {      // Loop has constant number of iterations.\n\n  x[i] = float(idx);                      // Indirect access by induction variable results in\n                                          // direct register access.\n}\n```\n\n----------------------------------------\n\nTITLE: Profiling Subset Tensor Core Convolution Kernels\nDESCRIPTION: This command executes the CUTLASS profiler on a filtered subset of Tensor Core convolution kernels. The `--kernels` argument specifies the kernels to profile, while other arguments define the convolution problem dimensions (n, h, w, c, k, r, s).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*fprop_optimized_f16 --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Define type aliases for registers in SM70 operation\nDESCRIPTION: Defines type aliases for DRegisters, ARegisters, BRegisters, and CRegisters within the `SM70_8x8x4_F32F16F16F32_NT` Operation struct. These aliases specify the data types and number of values each thread passes into the PTX instruction for the A, B, C, and D matrices. It uses `float` for C and D registers and `uint32_t` for A and B registers, packing two 16-bit F16 values into each 32-bit integer.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nusing DRegisters = float[8];\nusing ARegisters = uint32_t[2];\nusing BRegisters = uint32_t[2];\nusing CRegisters = float[8];\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensor Elements in C++\nDESCRIPTION: This code demonstrates accessing elements of a `Tensor` using `operator()` and `operator[]`, which take `IntTuple`s of logical coordinates.  The `Tensor`'s `Layout` maps the logical coordinate to an offset that can be accessed by the iterator. It provides examples of reading and writing to `Tensor`s using natural coordinates, the variadic `operator()`, and the container-like `operator[]`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <class Coord>\ndecltype(auto) operator[](Coord const& coord) {\n  return data()[layout()(coord)];\n}\n```\n\nLANGUAGE: c++\nCODE:\n```\nTensor A = make_tensor<float>(Shape <Shape < _4,_5>,Int<13>>{},\n                              Stride<Stride<_12,_1>,    _64>{});\nfloat* b_ptr = ...;\nTensor B = make_tensor(b_ptr, make_shape(13, 20));\n\n// Fill A via natural coordinates op[]\nfor (int m0 = 0; m0 < size<0,0>(A); ++m0)\n  for (int m1 = 0; m1 < size<0,1>(A); ++m0)\n    for (int n = 0; n < size<1>(A); ++n)\n      A[make_coord(make_coord(m0,m1),n)] = n + 2 * m0;\n\n// Transpose A into B using variadic op()\nfor (int m = 0; m < size<0>(A); ++m)\n  for (int n = 0; n < size<1>(A); ++n)\n    B(n,m) = A(m,n);\n\n// Copy B to A as if they are arrays\nfor (int i = 0; i < A.size(); ++i)\n  A[i] = B[i];\n```\n\n----------------------------------------\n\nTITLE: GEMM Batched Example\nDESCRIPTION: This example demonstrates a batched GEMM operation with float32 inputs. The example shows how to run a batch of GEMM operations in a single kernel launch. It defines parameters such as tensor layouts, block size, swizzle functions, and the batch size.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add_fast_bf16 -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la RowMajor -aa 4 -lb ColumnMajor -ab 4 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw BatchedIdentitySwizzle -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Batched -k 1 -batch 3\n```\n\n----------------------------------------\n\nTITLE: Class Member Order C++\nDESCRIPTION: This code snippet shows the recommended order of members within a C++ class: type definitions, data members, and then methods. This convention promotes code readability and consistency, aligning with established practices in libraries like CUB and recommendations from experts like Howard Hinnant.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_15\n\nLANGUAGE: c++\nCODE:\n```\nclass A {\npublic:\n  // type definitions\nprotected:\n  // protected type definitions\nprivate:\n  // private type definitions\n\npublic:\n  // data members\nprotected:\n  // protected data members\n  // STRONGLY TO BE AVOIDED;\n  // please see C++ Core Guidelines\nprivate:\n  // private data members\n\npublic:\n  // methods\nprotected:\n  // protected methods\nprivate:\n  // private methods\n};\n```\n\n----------------------------------------\n\nTITLE: GEMM Launch with CUTLASS Library in C++\nDESCRIPTION: This C++ code demonstrates how to use the CUTLASS Library to launch a GEMM operation on a CUDA device. It includes defining the problem size, allocating device memory using `cutlass::HostTensor`, and calling `handle.gemm` to execute the computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\n#include \"cutlass/library/library.h\"\n#include \"cutlass/library/handle.h\"\n\nint main() {\n\n  //\n  // Define the problem size\n  //\n  int M = 512;\n  int N = 256;\n  int K = 128;\n\n  float alpha = 1.25f;\n  float beta = -1.25f;\n\n  //\n  // Allocate device memory\n  //\n\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> A({M, K});\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> B({K, N});\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> C({M, N});\n\n  float const *ptrA = A.device_data();\n  float const *ptrB = B.device_data();\n  float const *ptrC = C.device_data();\n  float       *ptrD = C.device_data();\n\n  int lda = A.device_ref().stride(0);\n  int ldb = B.device_ref().stride(0);\n  int ldc = C.device_ref().stride(0);\n  int ldd = D.device_ref().stride(0);\n\n  //\n  // CUTLASS Library call to execute device GEMM\n  //\n\n  cutlass::library::Handle handle;\n\n  //\n  // Launch GEMM on CUDA device.\n  //\n\n  cutlass::Status status = handle.gemm(\n    M,\n    N,\n    K,\n\n    cutlass::library::NumericTypeID::kF32,          // data type of internal accumulation\n    cutlass::library::NumericTypeID::kF32,          // data type of alpha/beta scalars\n\n    &alpha,                                         // pointer to alpha scalar\n\n    cutlass::library::NumericTypeID::kF32,          // data type of A matrix\n    cutlass::library::LayoutTypeID::kColumnMajor,   // layout of A matrix\n    ptrA,                                           // pointer to A matrix in device memory\n    lda,                                            // leading dimension of A matrix\n\n    cutlass::library::NumericTypeID::kF32,          // data type of B matrix\n    cutlass::library::LayoutTypeID::kColumnMajor,   // layout of B matrix\n    ptrB,                                           // pointer to B matrix in device memory\n    ldb,                                            // leading dimension of B matrix\n\n    &beta,                                          // pointer to beta scalar\n\n    cutlass::library::NumericTypeID::kF32,          // data type of C and D matrix\n\n    ptrC,                                           // pointer to C matrix in device memory\n    ldc,                                            // leading dimension fo C matrix\n\n    ptrD,                                           // pointer to D matrix in device memory\n    ldd                                             // leading dimension of D matrix\n  );\n\n  if (status != cutlass::Status::kSuccess) {\n    return -1;\n  }\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Returning Multiple Values via Struct in C++\nDESCRIPTION: This snippet illustrates how to return multiple values from a function using a struct, which allows for named fields and improved code self-documentation.  It also shows how to use structured binding with the returned struct.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nstruct my_computation_result {\n  float value = 0.0f;\n  float relative_error = 0.0f;\n  bool success = false;\n};\n\nmy_computation_result my_computation(float tolerance);\n\nvoid foo(float tolerance) {\n  // Approach 1: Use structured binding.  The names\n  // you choose on the left-hand side have nothing\n  // to do with the struct, so it's up to you\n  // to get the order right.  On the other hand,\n  // this code works whether my_computation returns\n  // a struct or a tuple.\n  auto [val, rel_err, ok] = my_computation(tolerance);\n\n  // Approach 2: Keep the struct and use its named fields.\n  // This approach prevents errors like mixing the order of return types.\n  // However, it only works for structs, not for tuples.\n\n  auto result = my_computation(tolerance);\n  if (not result.success) {\n    // computation did not succeed\n  }\n  else if (result.relative_error > tolerance) {\n    // successful but relative error too large\n  }\n  else {\n    // successful and relative error is in bounds\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying the Number of GPUs (cpp)\nDESCRIPTION: This code snippet demonstrates how to modify the number of GPUs used in the distributed GEMM computation. The `TP` alias defines the tensor parallelism degree. By changing the value assigned to `TP`, the user can control the number of GPUs involved in the parallel execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/README.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nusing TP = _8;\n```\n\n----------------------------------------\n\nTITLE: FP8 GEMM with CUTLASS Python API\nDESCRIPTION: This snippet demonstrates how to perform a GEMM operation with FP8 tensors using CUTLASS's Python interface. It requires the `cutlass` and `torch` libraries.  It initializes the GEMM plan with FP8 input types and float32 accumulator/output types, creates input/output tensors in FP8 format, and executes the GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif device_cc() >= 90:\n    plan = cutlass.op.Gemm(element=torch.float8_e4m3fn, element_C=torch.float32, element_accumulator=torch.float32,\n                        layout_A=cutlass.LayoutType.RowMajor, layout_B=cutlass.LayoutType.ColumnMajor,\n                        layout_C=cutlass.LayoutType.ColumnMajor)\n\n    # Create input/output tensors in FP8\n    A, B = [torch.ones((128, 128)).to(torch.float8_e4m3fn).to(\"cuda\") for _ in range(2)]\n    C, D = [torch.zeros((128, 128)).to(torch.float8_e4m3fn).to(\"cuda\") for _ in range(2)]\n\n    # Run the GEMM\n    plan.run(A, B, C, D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS Python Interface via pip\nDESCRIPTION: This command installs the CUTLASS Python interface from the PyPI package index. It ensures users are installing the NVIDIA-affiliated package.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-cutlass\n```\n\n----------------------------------------\n\nTITLE: Reference GEMM Implementation in CUTLASS (C++)\nDESCRIPTION: This C++ code snippet demonstrates a reference GEMM (General Matrix Multiply) implementation within the CUTLASS library. It showcases how to perform matrix multiplication using half-precision floating-point numbers (cutlass::half_t) and column-major layout. The code initializes host tensors, defines the GEMM operation, and performs the computation using a reference implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/numeric_types.h>\n#include <cutlass/layout/matrix.h>\n\n#include <cutlass/util/host_tensor.h>\n#include <cutlass/util/reference/host/gemm.h>\n\nint main() {\n\n  int M = 64;\n  int N = 32;\n  int K = 16;\n\n  float alpha = 1.5f;\n  float beta = -1.25f;\n\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A({M, K});\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> B({K, N});\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> C({M, N});\n\n  cutlass::reference::host::Gemm<\n    cutlass::half_t, cutlass::layout::ColumnMajor,   // ElementA and LayoutA\n    cutlass::half_t, cutlass::layout::ColumnMajor,   // ElementB and LayoutB\n    cutlass::half_t, cutlass::layout::ColumnMajor,   // ElementC and LayoutC\n    float,                                           // scalar type (alpha and beta)\n    float> gemm_op;                                  // internal accumulation type\n\n  gemm_op(\n    {M, N, K},             // problem size\n    alpha,                 // alpha scalar\n    A.host_view(),         // TensorView to host memory\n    B.host_view(),         // TensorView to host memory\n    beta,                  // beta scalar\n    C.host_view(),         // TensorView to host memory\n    D.host_view());        // TensorView to device memory\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Warp-level GEMM example using CUTLASS\nDESCRIPTION: This example demonstrates a warp-level GEMM operation using CUTLASS, accumulating matrix products in a register-backed array. The input data must be in shared memory loaded by iterators or on register-backed fragments. It uses `cutlass::gemm::warp::DefaultMmaTensorOp` to perform the GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n#include \"cutlass/gemm/warp/default_mma_tensor_op.h\"\n\nusing LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<\n    cutlass::sizeof_bits<Element>::value, 64>;\n\nusing LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous<\n    cutlass::sizeof_bits<Element>::value, 64>;\n\nusing WarpMma = typename cutlass::gemm::warp::DefaultMmaTensorOp<\n    cutlass::gemm::GemmShape<64, 64, 8>,                            // Overall warp-level GEMM operation\n    cutlass::gemm::GemmShape<16, 8, 8>,                             // Target instruction\n    cutlass::half_t, LayoutA,                                       // operand A type and layout\n    cutlass::half_t, LayoutB,                                       // operand B type and layout\n    float,                                                          // accumulator type\n    cutlass::layout::RowMajor>::Type;                               // accumulator layout\n\n//\n// Define a GEMM operation loading data from shared memory\n//\nint const kGemmK = 32;\n\n__shared__ ElementA smem_buffer_A[WarpMma::Shape::kM * kGemmK];\n__shared__ ElementB smem_buffer_B[WarpMma::Shape::kN * kGemmK];\n\n//\n// Construct iterators into SMEM tiles\n//\n\n// leading dimensions inferred from matrix problem size\nint lda = WarpMma::Shape::kM;\nint ldb = WarpMma::Shape::kN;\n\n// iterators into shared memory\nWarpMma::IteratorA warp_iterator_A({smem_buffer_A, lda});\nWarpMma::IteratorB warp_iterator_B({smem_buffer_B, ldb});\n\n// Fragments in registers storing the operands\nFragmentA frag_A;\nFragmentB frag_B;\nFragmentC accum;\n\nWarpMma mma;\n\naccum.clear();\n\n//\n// Accumulated outer product\n//\n\n#pragma unroll 1\nfor (int k = 0; k < kGemmK; k += WarpMma::Shape::kK) {\n\n  \n  iter_A.load(frag_A);  // Load fragments from A and B matrices\n  iter_B.load(frag_B);\n\n  ++iter_A; ++iter_B;   // Advance along GEMM K to next tile in A\n                        //   and B matrices\n\n                        // Compute matrix product\n  mma(accum, frag_A, frag_B, accum);\n}\n```\n\n----------------------------------------\n\nTITLE: Conv2d: Declaring and running Conv2d Dgrad and Wgrad\nDESCRIPTION: This snippet showcases the use of the CUTLASS Python interface for declaring and executing backward Conv2d operations, specifically data gradient (Dgrad) and weight gradient (Wgrad).  It initializes tensors for the gradients and then calls the `cutlass.Conv2dDgrad` and `cutlass.Conv2dWgrad` respectively. PyTorch's autograd functionality is used to compute the reference values for comparison and validation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngrad_output = torch.ceil(\n    torch.empty(size=(N, K, P, Q), dtype=type_A, device=\"cuda\").uniform_(-4.5, 3.5)\n).to(memory_format=torch.channels_last)\ngrad_input = torch.zeros_like(input)\ngrad_weight = torch.zeros_like(weight)\n\ntensor_C_dgrad = torch.ceil(\n    torch.empty(size=(N, C, H, W), dtype=type_A, device=\"cuda\").uniform_(-4.5, 3.5)\n).to(memory_format=torch.channels_last)\ntensor_C_wgrad = torch.ceil(\n    torch.empty(size=(K, C, R, S), dtype=type_B, device=\"cuda\").uniform_(-4.5, 3.5)\n).to(memory_format=torch.channels_last)\n\nplan_dgrad = cutlass.Conv2dDgrad(element=dtype, element_accumulator=torch.float32)\nplan_dgrad.run(grad_output, weight, tensor_C_dgrad, grad_input, stride, padding, dilation, alpha, beta, print_module=print_module)\n\ngrad_input_torch = alpha * torch.nn.grad.conv2d_input(\n    (N, C, H, W),\n    weight, grad_output,\n    stride=stride, padding=padding\n) + beta * tensor_C_dgrad\n\nassert torch.equal(grad_input_torch, grad_input)\n\nplan_wgrad = cutlass.Conv2dWgrad(element=dtype, element_accumulator=torch.float32)\nplan_wgrad.run(grad_output, input, tensor_C_wgrad, grad_weight, stride, padding, dilation, alpha, beta, print_module=print_module)\n\ngrad_weight_torch = alpha * torch.nn.grad.conv2d_weight(\n    input, (K, C, R, S), grad_output,\n    stride=stride, padding=padding\n) + beta * tensor_C_wgrad\n\nassert torch.equal(grad_weight_torch, grad_weight)\n```\n\n----------------------------------------\n\nTITLE: Launching a GEMM kernel (CUTLASS < 3.0)\nDESCRIPTION: This example demonstrates how to launch a mixed-precision GEMM kernel targeting Turing Tensor Cores using the older CUTLASS API. It defines the GEMM operation, allocates device memory using `cutlass::HostTensor`, and launches the kernel with the appropriate parameters, including matrix dimensions, pointers, leading dimensions, and alpha/beta scaling factors. It utilizes the `cutlass::gemm::device::Gemm` class.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_21\n\nLANGUAGE: c++\nCODE:\n```\n#include <cutlass/numeric_types.h>\n#include <cutlass/gemm/device/gemm.h>\n\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n\n  // Define the GEMM operation\n  using Gemm = cutlass::gemm::device::Gemm<\n    cutlass::half_t,                           // ElementA\n    cutlass::layout::ColumnMajor,              // LayoutA\n    cutlass::half_t,                           // ElementB\n    cutlass::layout::ColumnMajor,              // LayoutB\n    cutlass::half_t,                           // ElementOutput\n    cutlass::layout::ColumnMajor,              // LayoutOutput\n    float,                                     // ElementAccumulator\n    cutlass::arch::OpClassTensorOp,            // tag indicating Tensor Cores\n    cutlass::arch::Sm75                        // tag indicating target GPU compute architecture\n  >;\n\n  Gemm gemm_op;\n  cutlass::Status status;\n\n  //\n  // Define the problem size\n  //\n  int M = 512;\n  int N = 256;\n  int K = 128;\n\n  float alpha = 1.25f;\n  float beta = -1.25f;\n\n  //\n  // Allocate device memory\n  //\n\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A({M, K});\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> B({K, N});\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> C({M, N});\n\n  cutlass::half_t const *ptrA = A.device_data();\n  cutlass::half_t const *ptrB = B.device_data();\n  cutlass::half_t const *ptrC = C.device_data();\n  cutlass::half_t       *ptrD = C.device_data();\n\n  int lda = A.device_ref().stride(0);\n  int ldb = B.device_ref().stride(0);\n  int ldc = C.device_ref().stride(0);\n  int ldd = C.device_ref().stride(0);\n  //\n  // Launch GEMM on the device\n  //\n\n  status = gemm_op({\n    {M, N, K},\n    {ptrA, lda},            // TensorRef to A device tensor\n    {ptrB, ldb},            // TensorRef to B device tensor\n    {ptrC, ldc},            // TensorRef to C device tensor\n    {ptrD, ldd},            // TensorRef to D device tensor - may be the same as C\n    {alpha, beta}           // epilogue operation arguments\n  });\n\n  if (status != cutlass::Status::kSuccess) {\n    return -1;\n  }\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: GEMM with Leaky ReLU Activation\nDESCRIPTION: This example demonstrates a GEMM operation with leaky ReLU activation function using float64 data type and TensorOp. It includes specifying the activation argument.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 8 8 4 -ta float64 -tb float64 -tc float64 -tacc float64 -m multiply_add -op TensorOp -b 64 64 16 -s 4 -w 2 2 1 -cc 80 -la RowMajor -aa 1 -lb ColumnMajor -ab 1 -lc RowMajor -ac 1 -te float64 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 2 -bias -activ leaky_relu -activ_arg 0.2\n```\n\n----------------------------------------\n\nTITLE: Serial Split-K in CUTLASS Python\nDESCRIPTION: This snippet demonstrates the serial split-K strategy in the CUTLASS Python interface. It initializes an empty tensor `grad_weight_serial` and then runs the wgrad operation with a serial split-K strategy using 3 slices. A semaphore coordinates partial results.  Finally, it asserts that the `grad_weight_serial` tensor matches the PyTorch result `grad_weight_torch`. Serial split-K uses a semaphore for synchronization and avoids an extra reduction kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Serial Split-K with 3 slices\ngrad_weight_serial = torch.zeros_like(grad_weight)\nplan_wgrad.run(\n    grad_output, input, tensor_C_wgrad, grad_weight_serial,  \n    stride, padding, dilation, alpha, beta, print_module=print_module, split_k=(\"serial\", 3))\nassert torch.equal(grad_weight_torch, grad_weight_serial)\n```\n\n----------------------------------------\n\nTITLE: Building One Convolution CUDA Kernel (CMake and Make)\nDESCRIPTION: These commands build a single CUDA Core convolution kernel implementing forward propagation using CMake and Make. CMake is used to configure with the target architecture and exact kernel name. The `make` command compiles the selected kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Running Grouped GEMM with CUTLASS and PyTorch\nDESCRIPTION: This snippet demonstrates running a group of 50 GEMMs using both the CUTLASS Python interface and standard PyTorch operations. It generates the GEMM problems using the `generate_problems` function, runs the CUTLASS grouped GEMM operation using `plan.run`, computes the GEMM results using PyTorch matrix multiplication, and asserts that the results from both methods are close to each other.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nAs, Bs, Cs, Ds, = generate_problems(50)\n\nplan.run(As, Bs, Cs, Ds, print_module=True)\nDs_torch = [a @ b for a, b in zip(As, Bs)]\n\nfor d, d_torch in zip(Ds, Ds_torch):\n    assert torch.allclose(d, d_torch)\n```\n\n----------------------------------------\n\nTITLE: TensorView I/O Example\nDESCRIPTION: This example shows how to print a `cutlass::TensorView` to standard output in CSV format using `std::ostream::operator<<()`. This assumes the view refers to host memory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/util/tensor_view_io.h>\n\nint main() {\n  // Obtain a TensorView into host memory\n  cutlass::TensorView<float, cutlass::layout::ColumnMajor> view = tensor.host_view();\n\n  // Print to std::cout\n  std::cout << view << std::endl;\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Profile Grouped GEMM Extension\nDESCRIPTION: This code profiles the performance of the grouped GEMM extension and compares it to the performance of vanilla PyTorch GEMMs. It runs the GEMM operations multiple times and measures the execution time for both methods.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnum_warmup = 20\nnum_profile = 100\n\n# Warmup iterations\nfor _ in range(num_warmup):\n    Ds = grouped_gemm.run(As, Bs)\n    Ds_torch = [a @ b for a, b in zip(As, Bs)]\n    torch.cuda.synchronize()\n\n# Timing iterations\nimport time\ngrouped = 0\nnongrouped = 0\nfor _ in range(num_profile):\n    start = time.time()\n    Ds = grouped_gemm.run(As, Bs)\n    torch.cuda.synchronize()\n    grouped += time.time() - start\n\n    start = time.time()\n    Ds_torch = [a @ b for a, b in zip(As, Bs)]\n    torch.cuda.synchronize()\n    nongrouped += time.time() - start\n\nprint('Grouped:     {:.3f} us'.format(grouped * 1e6/num_profile))\nprint('Non-Grouped: {:.3f} us'.format(nongrouped * 1e6/num_profile))\nprint('Speedup: {:.3f}'.format(nongrouped / grouped))\n```\n\n----------------------------------------\n\nTITLE: Creating a TiledMMA Instance (C++)\nDESCRIPTION: This code creates a `TiledMMA` instance named `mmaC` using `make_tiled_mma`. It specifies a `UniversalFMA` instruction (which represents a fused multiply-add operation), and a layout of 16x16x1. This TiledMMA will be used for the matrix multiply-accumulate operation.  `print_latex` can be used to display the layout, but isn't part of the core compute.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_29\n\nLANGUAGE: cpp\nCODE:\n```\n  TiledMMA mmaC = make_tiled_mma(UniversalFMA<TC,TA,TB>{},\n                                 Layout<Shape<_16,_16,_1>>{});  // 16x16x1 UniversalFMA\n  print_latex(mmaC);\n```\n\n----------------------------------------\n\nTITLE: Conv2d Fprop F16 Configuration\nDESCRIPTION: This example demonstrates a device convolution operation with fprop (forward propagation) using float16 data type and TensorOp. It utilizes implicit GEMM and specifies tensor layouts in NHWC format.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 16 -ta float16 -tb float16 -tc float16 -tacc float32 -m multiply_add -op TensorOp -b 128 128 64 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 8 -lb TensorNHWC -ab 8 -lc TensorNHWC -ac 8 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -co fprop -st Strided -ia optimized -sm Serial -k 1 -nhwc 1 27 27 256 -krsc 512 3 3 256 -pad 1 1 1 1 -stride 2 1 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: Run GEMM with All Activations (CUTLASS)\nDESCRIPTION: This snippet iterates through all available activation functions, setting each one as the activation for the GEMM plan and running the plan with the input tensors. It prints the name of each activation before running the GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor activation in activations:\n    print('=============================================================================================')\n    print(f'Compiling and running activation {activation}')\n    print('=============================================================================================')\n    plan.activation = activation\n    plan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Verify CUTLASS GEMM Results with PyTorch\nDESCRIPTION: This code defines a PyTorch module `TorchReference` which implements the same epilogue logic as the CUTLASS epilogue. The forward function calls the pre-defined `example_epilogue` with the inputs.  It then compares the output of the CUTLASS GEMM with the result from the PyTorch implementation using `torch.equal` to verify the correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/04_epilogue_visitor.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TorchReference(torch.nn.Module):\n    def forward(self, A, B, alpha, C, beta, aux, bias):\n        accum = torch.matmul(A, B)\n        return example_epilogue(accum, alpha, C, beta, aux, bias)\n\ntorch_reference = TorchReference()\ntensor_D_ref, tensor_F_ref = torch_reference(tensor_A, tensor_B, alpha, tensor_C, beta, aux, bias)\n\nassert torch.equal(tensor_D, tensor_D_ref)\nassert torch.equal(tensor_F, tensor_F_ref)\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Network Topology\nDESCRIPTION: This command uses `nvidia-smi topo -m` to check the GPU network topology. It verifies that an NVLink network exists between every pair of GPUs, indicated by `NV*` links, which is necessary for inter-GPU communication in the distributed GEMM example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/REQUIREMENTS.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi topo -m\n```\n\n----------------------------------------\n\nTITLE: Using Input Value Distribution\nDESCRIPTION: These commands set the distribution of input values for the GEMM operands. The first example uses a uniform distribution between 0 and 3, and the second uses a Gaussian distribution with a mean of 0 and a standard deviation of 3.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --dist=uniform,min:0,max:3\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --dist=gaussian,mean:0,stddev:3\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --dist=sequential,start:0,delta:1\n```\n\n----------------------------------------\n\nTITLE: Inline PTX for Matrix Multiply Accumulate (MMA) using Turing Tensor Cores in C++\nDESCRIPTION: This C++ code snippet demonstrates how to use inline PTX assembly to perform a matrix multiply-accumulate operation using Turing Tensor Cores. It takes two packed 4-bit integer inputs (A and B) and accumulates the result into two 32-bit integer elements (C), storing the final result in D. The `mma.sync` instruction performs the warp-synchronous matrix multiply-accumulate operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\nunsigned A;   // eight packed 4-bit integer elements\nunsigned B;   // eight packed 4-bit integer elements\n\nint C[2];     // two 32-bit integer elements\nint D[2];     // two 32-bit integer elements\n\nasm volatile(\n  \"mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32 {%0,%1}, {%2}, {%3}, {%4,%5};\\n\"\n  : \"=r\"(D[0]), \"=r\"(D[1])\n  : \"r\"(A), \"r\"(B), \"r\"(C[0]), \"r\"(C[1]));\n```\n\n----------------------------------------\n\nTITLE: Tensor Engine Definition in C++\nDESCRIPTION: This code defines the structure of a Tensor Engine, which acts as a wrapper for an iterator or data array. It uses a simplified `std::array` interface to expose the iterator, value type, reference type, and the begin iterator. This allows the Tensor to access data stored in various memory locations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nusing iterator     =  // The iterator type\nusing value_type   =  // The iterator value-type\nusing reference    =  // The iterator reference-type\niterator begin()      // The iterator\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS Profiler\nDESCRIPTION: This command builds the CUTLASS profiler utility, which can be used to launch and benchmark different GEMM kernels. It uses the `make` command with the `cutlass_profiler` target and specifies 16 parallel jobs using the `-j16` flag.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Representing Full Tensors (CuTe)\nDESCRIPTION: This code creates tensor representations for A, B, and C matrices using `make_tensor` and `make_gmem_ptr`. It selects appropriate modes and uses strides to define the tensor layouts based on the problem shape.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n  // Preconditions\n  CUTE_STATIC_ASSERT_V(rank(shape_MNK) == Int<3>{});                      // (M, N, K)\n\n  CUTE_STATIC_ASSERT_V(congruent(select<0,2>(shape_MNK), dA));            // dA strides for shape MK\n  CUTE_STATIC_ASSERT_V(congruent(select<1,2>(shape_MNK), dB));            // dB strides for shape NK\n  CUTE_STATIC_ASSERT_V(congruent(select<0,1>(shape_MNK), dC));            // dC strides for shape MN\n\n  // Represent the full tensors\n  Tensor mA = make_tensor(make_gmem_ptr(A), select<0,2>(shape_MNK), dA);  // (M,K)\n  Tensor mB = make_tensor(make_gmem_ptr(B), select<1,2>(shape_MNK), dB);  // (N,K)\n  Tensor mC = make_tensor(make_gmem_ptr(C), select<0,1>(shape_MNK), dC);  // (M,N)\n```\n\n----------------------------------------\n\nTITLE: GEMM Grouped F16 Device Schedule Example\nDESCRIPTION: This example runs a GEMM Grouped operation on device using float16 inputs and float32 output. It reads problem sizes from a CSV file and executes the grouped GEMM with TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npython gemm_grouped.py -i 16 8 16 -ta float16 -tb float16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb ColumnMajor -ab 8 -lc ColumnMajor -ac 4 -te float32 -ep LinearCombination -p ./grouped_gemm_problem_size.csv -alpha 1.0 -beta 0.0 -pm Device\n```\n\n----------------------------------------\n\nTITLE: Convolution Indexing Functions\nDESCRIPTION: These are the functions f and g used to map output indices (p, q) and filter offsets (r, s) to input indices in the activation tensor. They incorporate stride and padding parameters to determine the correct input element for each convolution operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nf(p, r) = p * stride_h + R - r - 1 + pad_h\ng(q, s) = q * stride_w + S - s - 1 + pad_w\n```\n\n----------------------------------------\n\nTITLE: Creating and Printing CuTe Tensors with Custom Strides in C++\nDESCRIPTION: This code snippet demonstrates how to create CuTe Tensors with specific shapes and strides using the `make_tensor`, `make_shape`, and `make_stride` functions. It then uses `print_tensor` to display the resulting tensor layout. The example shows how to control the order of dimensions within the tensor by manipulating the strides, effectively transposing or reordering the axes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0z_tma_tensors.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nTensor a = make_tensor(make_inttuple_iter(0,0),\n                       make_shape (     4,      5),\n                       make_stride(E<0>{}, E<1>{}));\nprint_tensor(a);\n\nTensor b = make_tensor(make_inttuple_iter(0,0),\n                       make_shape (     4,      5),\n                       make_stride(E<1>{}, E<0>{}));\nprint_tensor(b);\n```\n\n----------------------------------------\n\nTITLE: List Available Activations (CUTLASS)\nDESCRIPTION: This snippet retrieves and prints a list of the supported element-wise activation functions in CUTLASS using the `activations()` method of the GEMM plan.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nactivations = plan.activations()\nfor activation in activations:\n    print(activation)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 2SM, Mxf8f6f4)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following a sparse matrix multiplication, using TMA (Tensor Memory Accelerator) across two SMs (Streaming Multiprocessors). It is designed specifically for narrow precision using the mxf8f6f4 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized2SmMxf8f6f4`\n```\n\n----------------------------------------\n\nTITLE: Running the CUTLASS Profiler for GEMM\nDESCRIPTION: Executes the CUTLASS Profiler for a GEMM operation with specified parameters.  This command profiles the performance of single-precision GEMM (sgemm) with matrices of size 4352x4096 and 4096x4096.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=4352 --n=4096 --k=4096\n```\n\n----------------------------------------\n\nTITLE: GEMM with Identity Activation (CUTLASS)\nDESCRIPTION: This snippet defines a CUTLASS GEMM plan with an identity activation function and runs it using the input tensors A, B, C, and D. The identity activation is the default, performing the operation `D = alpha * (A @ B) + beta * C`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nplan = cutlass.op.Gemm(element=np.float16, layout=cutlass.LayoutType.RowMajor)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Copy Subtile to Registers C++\nDESCRIPTION: This code snippet demonstrates how to copy a subtile from global memory to registers using CUTLASS tensors. It creates a tensor representing global memory, then iterates through the rows, copying each row to a register tensor and performing an operation on the register data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nTensor gmem = make_tensor(ptr, make_shape(Int<8>{}, 16));  // (_8,16)\nTensor rmem = make_tensor_like(gmem(_, 0));                // (_8)\nfor (int j = 0; j < size<1>(gmem); ++j) {\n  copy(gmem(_, j), rmem);\n  do_something(rmem);\n}\n```\n\n----------------------------------------\n\nTITLE: Configure GEMM kernels for Column/Row Major Layouts\nDESCRIPTION: This CMake command configures CUTLASS to compile GEMM kernels that expect A and B to be either column-major or row-major, targeting NVIDIA Ampere, Turing, and Volta architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_31\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"70;75;80\" -DCUTLASS_LIBRARY_KERNELS=gemm*nn,gemm*tt\n```\n\n----------------------------------------\n\nTITLE: GEMM with ReLU Activation (CUTLASS)\nDESCRIPTION: This snippet demonstrates running a GEMM with a ReLU element-wise activation function using CUTLASS. It sets the `activation` field of the GEMM plan to \"relu\" and executes the plan, storing the result in `tensor_D_relu`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntensor_D_relu = np.zeros(tensor_C.shape).astype(type_D)\nplan.activation = \"relu\"\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D_relu, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Configure Planar Complex GEMM kernels\nDESCRIPTION: This CMake command configures CUTLASS to compile all planar complex GEMM variants targeting NVIDIA Ampere, Turing, and Volta architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_32\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"70;75;80\" -DCUTLASS_LIBRARY_KERNELS=planar_complex\n```\n\n----------------------------------------\n\nTITLE: Running Blackwell Distributed GEMM Example (bash)\nDESCRIPTION: This command executes the `82_blackwell_distributed_gemm` binary with specified GEMM dimensions (m, n, k) and profiling parameters (warmup-iterations, iterations). It performs a distributed GEMM computation across multiple GPUs and profiles its performance.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/82_blackwell_distributed_gemm/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./82_blackwell_distributed_gemm --m=16384 --n=106496 --k=16384 --warmup-iterations=10 --iterations=100\n```\n\n----------------------------------------\n\nTITLE: GEMM F16 SplitK Parallel Example\nDESCRIPTION: This example demonstrates a GEMM operation using float16 input, float32 output and split-K parallel execution.  Parameters such as block sizes, tensor layouts, and swizzle functions are defined.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 16 -ta float16 -tb float16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 256 128 64 -s 3 -w 4 2 1 -cc 80 -la ColumnMajor -aa 8 -lb ColumnMajor -ab 8 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm GemmSplitKParallel -k 3\n```\n\n----------------------------------------\n\nTITLE: West Const Declaration Example in C++\nDESCRIPTION: This snippet shows an example of the \"West const\" style, which is not preferred in CUTLASS. It is presented for comparison with the preferred \"East const\" style.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nconst float const_float = /* whatever */;\nconst float* pointer_to_const_float = &const_float;\n```\n\n----------------------------------------\n\nTITLE: Defining Readable Tile Iterator Concept in C++\nDESCRIPTION: Defines a tile iterator concept for iterators that can load tiles from memory. It specifies a `Fragment` type, which holds each thread's part of the data, and a `load` method to read the tile from memory into the `Fragment` object. This concept is crucial for loading data into shared memory or registers for computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n/// Tile iterator capable of loading tiles from memory into fragments\nstruct ReadableTileIteratorConcept {\n  \n  using Fragment;              ///< fragment object derived from cutlass::Array<Element, N>\n\n  CUTLASS_DEVICE\n  void load(Fragment &frag);   ///< loads a fragment from memory\n};\n```\n\n----------------------------------------\n\nTITLE: GETT Implementation using CuTe and GEMM Kernel\nDESCRIPTION: This C++ code defines a `gett` function that implements a GETT (Generalized Tensor Times Tensor) operation using a GEMM device kernel. It sets up parameters for a GETT with two m-modes, defines shapes and strides for the input tensors, and configures the CTA tiler. It leverages an existing `gemm_device` kernel from `sgemm_1.cu` for computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_33\n\nLANGUAGE: cpp\nCODE:\n```\n// Setup params for a GETT with two m-modes.\n// The A and C tensors are assumed to be m0-major.\n//   Calls sgemm_1.cu's gemm_device<<<>>> without modification.\ntemplate <class TA, class TB, class TC,\n          class Alpha, class Beta>\nvoid\ngett(int m0, int m1, int n, int k,\n     Alpha alpha,\n     TA const* A, int ldAm1, int ldAk,  // m0-major\n     TB const* B, int ldBk,\n     Beta beta,\n     TC      * C, int ldCm1, int ldCn,  // m0-major\n     cudaStream_t stream = 0)\n{\n  using namespace cute;\n\n  // Define shapes (dynamic)\n  auto M = make_shape(m0, m1);                               // (m0,m1)-multimode M\n  auto N = int(n);\n  auto K = int(k);\n  auto prob_shape = make_shape(M, N, K);                     // (M, N, K)\n\n  // Define NT strides (mixed)\n  auto dA = make_stride(make_stride(Int<1>{}, ldAm1), ldAk); // (dM, dK)\n  auto dB = make_stride(Int<1>{}, ldB);                      // (dN, dK)\n  auto dC = make_stride(make_stride(Int<1>{}, ldCm1), ldCn); // (dM, dN)\n\n  // Define CTA tile sizes (static)\n  auto bM = Shape<_64, _2>{};    // Take _64 elements from m0 and _2 elements from m1\n  auto bN = Int<128>{};\n  auto bK = Int<  8>{};\n  auto cta_tiler = make_shape(bM, bN, bK);                   // (BLK_M, BLK_N, BLK_K)\n\n  // Define the smem layouts (static)\n  auto sA = make_layout(make_shape(bM, bK));                 // (m,k) -> smem_idx; m-major\n  auto sB = make_layout(make_shape(bN, bK));                 // (n,k) -> smem_idx; n-major\n  auto sC = make_layout(make_shape(bM, bN));                 // (m,n) -> smem_idx; m-major\n\n  // Define the thread layouts (static)\n  auto tA = make_layout(make_shape(Int<32>{}, Int< 8>{}));   // (m,k) -> thr_idx\n  auto tB = make_layout(make_shape(Int<32>{}, Int< 8>{}));   // (n,k) -> thr_idx\n  auto tC = make_layout(make_shape(Int<16>{}, Int<16>{}));   // (m,n) -> thr_idx\n\n  dim3 dimBlock(size(tC));\n  dim3 dimGrid(size(ceil_div(M, bM)),\n               size(ceil_div(N, bN)));\n  gemm_device<<<dimGrid, dimBlock, 0, stream>>>\n      (prob_shape, cta_tiler,\n       A, dA, sA, tA,\n       B, dB, sB, tB,\n       C, dC, sC, tC,\n       alpha, beta);\n}\n```\n\n----------------------------------------\n\nTITLE: By-Mode Composition with Tilers in CuTe (C++)\nDESCRIPTION: This C++ code demonstrates by-mode composition in CuTe using tilers. It defines a layout `a` and a tiler `tiler` composed of two layouts. It then calculates the composition of `a` and `tiler` to extract a sublayout. The same result is achieved by composing individual modes. Dependencies include the `make_layout`, `make_shape`, `make_stride`, `make_tile`, `composition`, `layout`, and `get` functions from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// (12,(4,8)):(59,(13,1))\nauto a = make_layout(make_shape (12,make_shape ( 4,8)),\n                     make_stride(59,make_stride(13,1)));\n// <3:4, 8:2>\nauto tiler = make_tile(Layout<_3,_4>{},  // Apply 3:4 to mode-0\n                       Layout<_8,_2>{}); // Apply 8:2 to mode-1\n\n// (_3,(2,4)):(236,(26,1))\nauto result = composition(a, tiler);\n// Identical to\nauto same_r = make_layout(composition(layout<0>(a), get<0>(tiler)),\n                          composition(layout<1>(a), get<1>(tiler)));\n```\n\n----------------------------------------\n\nTITLE: Running Grouped GEMM with PyTorch Extension and verification\nDESCRIPTION: This snippet demonstrates using the JIT-compiled PyTorch CUDA extension for grouped GEMM. It imports the generated extension (`grouped_gemm`), runs the GEMM operation using the extension's `run` method, computes the results using standard PyTorch GEMM operations, and asserts that the results from both methods are close.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport grouped_gemm\n\ngrouped_gemm.run(As, Bs)\n```\n\nLANGUAGE: python\nCODE:\n```\nDs = grouped_gemm.run(As, Bs)\nDs_torch = [a @ b for a, b in zip(As, Bs)]\nfor d, d_torch in zip(Ds, Ds_torch):\n    assert torch.allclose(d, d_torch)\n```\n\n----------------------------------------\n\nTITLE: Conv2d with SiLU Activation\nDESCRIPTION: This example demonstrates a Conv2d operation with SiLU activation function using float32 data type and TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 16 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 2 -lb TensorNHWC -ab 2 -lc TensorNHWC -ac 2 -te float32 -ep LinearCombination -sw IdentitySwizzle2 -co fprop -st Strided -ia optimized -sm Serial -k 2 -nhwc 1 4 4 12 -krsc 8 3 3 12 -pad 0 0 0 0 -stride 3 3 -dilation 1 1 -alpha 0.0 -beta 0.5 -bias -activ silu\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch CUDA Extension\nDESCRIPTION: This command builds and installs the generated PyTorch CUDA extension using the `setup.py` script.  The `TORCH_CUDA_ARCH_LIST` environment variable specifies the compute capability of the target device (e.g., 8.0 for SM80).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nTORCH_CUDA_ARCH_LIST=\"8.0\" python setup.py install\n```\n\n----------------------------------------\n\nTITLE: HostTensor Synchronization\nDESCRIPTION: This example demonstrates how to synchronize data between host and device memory using `tensor.sync_device()`. It also showcases writing data to the host memory using `tensor.host_ref().at({i, j})` and obtaining a device pointer.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nfloat idx = 0;\n\nfor (int i = 0; i < rows; ++i) {\n  for (int j = 0; j < columns; ++j) {\n\n    // Write the element at location {i, j} in host memory\n    tensor.host_ref().at({i, j}) = idx;\n\n    idx += 0.5f;\n  } \n}\n\n// Copy host memory to device memory\ntensor.sync_device();\n\n// Obtain a device pointer usable in CUDA kernels\nfloat *device_ptr = tensor.device_data();\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Sparse Kernel Dispatch Policy\nDESCRIPTION: This dispatch policy configures a sparse matrix multiplication kernel using TMA (Tensor Memory Accelerator) warp specialization with one SM (Streaming Multiprocessor) and is optimized for SM100 architecture. The policy targets data types {mx_float4_t, mx_float6_t, mx_float8_t} multiplied by either {mx_float4_t, mx_float6_t} or mx_float8_t.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n`KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100`\n```\n\n----------------------------------------\n\nTITLE: TensorFillRandomGaussian Example\nDESCRIPTION: This example demonstrates how to use `cutlass::reference::host::TensorFillRandomGaussian` and `cutlass::reference::device::TensorFillRandomGaussian` to initialize a tensor with a random Gaussian distribution in both host and device memory. It takes a seed, a mean, and a standard deviation as input.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/layout/matrix.h>\n#include <cutlass/util/reference/host/tensor_fill.h>\n#include <cutlass/util/reference/device/tensor_fill.h>\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n\n  int rows = 128;\n  int columns = 64;\n\n  double mean = 0.5;\n  double stddev = 2.0;\n  uint64_t seed = 0x2019;\n\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> tensor({rows, columns});\n\n  // Initialize in host memory\n  cutlass::reference::host::TensorFillRandomGaussian(\n    tensor.host_view(),\n    seed,\n    mean,\n    stddev);\n\n  // Initialize in device memory\n  cutlass::reference::device::TensorFillRandomGaussian(\n    tensor.device_view(),\n    seed,\n    mean,\n    stddev);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (NoSmem, 1SM)\nDESCRIPTION: This epilogue dispatch policy configures post-processing operations following matrix multiplication without using shared memory (NoSmem) on a single SM (Streaming Multiprocessor). It supports both dense and sparse matrix operations and is applicable for legacy and narrow precision computations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::NoSmemWarpSpecialized1Sm`\n```\n\n----------------------------------------\n\nTITLE: Half-Precision Tensor Initialization\nDESCRIPTION: This example shows how to initialize a half-precision tensor with a Gaussian distribution using `cutlass::reference::host::TensorFillRandomGaussian` and `cutlass::reference::device::TensorFillRandomGaussian`. It allocates a column-major tensor with `cutlass::half_t` elements.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/numeric_types.h>\n#include <cutlass/layout/matrix.h>\n#include <cutlass/util/reference/host/tensor_fill.h>\n#include <cutlass/util/reference/device/tensor_fill.h>\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n  int rows = 128;\n  int columns = 64;\n\n  double mean = 0.5;\n  double stddev = 2.0;\n  uint64_t seed = 0x2019;\n\n  // Allocate a column-major tensor with half-precision elements\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> tensor({rows, columns});\n\n  // Initialize in host memory\n  cutlass::reference::host::TensorFillRandomGaussian(\n    tensor.host_view(),\n    seed,\n    mean,\n    stddev);\n\n  // Initialize in device memory\n  cutlass::reference::device::TensorFillRandomGaussian(\n    tensor.device_view(),\n    seed,\n    mean,\n    stddev);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize and Launch Implicit GEMM in CUTLASS\nDESCRIPTION: This code snippet initializes and launches the Implicit GEMM operation on the device using CUTLASS. It first queries the workspace size, allocates the necessary memory, and then initializes the `ImplicitGemm` object with the arguments and workspace. Finally, it launches the kernel and checks for any errors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_11\n\nLANGUAGE: c++\nCODE:\n```\nImplicitGemm implicit_gemm_op;\n\n// Query workspace size\nsize_t workspace_size = implicit_gemm_op.get_workspace_size(arguments);\n\n// Allocate workspace memory\ncutlass::device_memory::allocation<uint8_t> workspace(workspace_size);\n\n// Initialize the Implicit GEMM object\ncutlass::Status status = implicit_gemm_op.initialize(arguments, workspace.get());\n\nif (status != cutlass::Status::kSuccess) {\n  /* error */\n}\n\n//\n// Launch initialized CUTLASS kernel\n//\n\nstatus = implicit_gemm_op();\n\nif (status != cutlass::Status::kSuccess) {\n  /* error */\n}\n```\n\n----------------------------------------\n\nTITLE: Running Cached CUTLASS Kernels\nDESCRIPTION: This snippet demonstrates how CUTLASS caches compiled kernels for faster execution. It changes the dimensions of the input tensors, sets the `opclass` to `cutlass.OpcodeClass.TensorOp`, and then runs the GEMM. The second execution should be much faster than the first due to the kernel being cached.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nm = 2400\nn = 3232\nk = 4096\n\ntensor_A = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, k)).astype(type_A))\ntensor_B = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(k, n)).astype(type_B))\ntensor_C = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, n)).astype(type_C))\ntensor_D = np.zeros(tensor_C.shape).astype(type_D)\n\nalpha = np.float16(1.)\nbeta = np.float16(2.)\n\nplan.opclass = cutlass.OpcodeClass.TensorOp\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies Based on CUDA Architecture (sm75+)\nDESCRIPTION: This snippet conditionally adds dependencies to the convolution test targets based on whether the maximum CUDA architecture is greater than or equal to 75 (sm75). If the condition is met, dependencies to the tensorop_f32_sm75, tensorop_s32, and tensorop_s32_interleaved implementations are added.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 75)\n\n  add_dependencies(\n    cutlass_test_unit_conv_device\n    cutlass_test_unit_conv_device_tensorop_f32_sm75\n    cutlass_test_unit_conv_device_tensorop_s32\n    cutlass_test_unit_conv_device_tensorop_s32_interleaved\n  )\n\n  add_dependencies(\n    test_unit_conv_device\n    test_unit_conv_device_tensorop_f32_sm75\n    test_unit_conv_device_tensorop_s32\n    test_unit_conv_device_tensorop_s32_interleaved\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Capacity Calculation Example\nDESCRIPTION: This C++ code demonstrates how to compute the capacity of a row-major matrix with padding. The `layout::RowMajor` object is initialized with a leading dimension that includes padding. The `capacity()` method then calculates the total number of elements needed to store the matrix, which is `rows * (columns + padding)`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nint lda = columns + padding;\nMatrixCoord extent{rows, columns};\n\nlayout::RowMajor layout(lda);\n\nauto capacity = layout.capacity(extent);    // returns rows * (columns + padding) \n```\n\n----------------------------------------\n\nTITLE: RowMajor Layout Example\nDESCRIPTION: This C++ code demonstrates the `layout::RowMajor` type in CUTLASS to compute the offset of an element in a row-major matrix. The `layout` object is initialized with the leading dimension `lda`. The `operator()` then calculates the linear offset from the logical coordinate `(row, column)` as `lda * row + column`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nlayout::RowMajor layout(lda); \n\nint offset = layout({row, column});     // returns lda * row + column\n```\n\n----------------------------------------\n\nTITLE: Including Headers for GEMM with L2 Prefetch\nDESCRIPTION: These header files provide the necessary definitions and interfaces to utilize the GEMM kernel with L2 weight prefetch in a CUTLASS project.  Including these headers grants access to the dispatch policy, builder, and the kernel implementation itself. These headers are required to use the custom GEMM kernel within a Cutlass project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/README.md#_snippet_0\n\nLANGUAGE: cxx\nCODE:\n```\n#include \"collective/dispatch_policy_extra.hpp\"\n#include \"collective/builder.hpp\"\n#include \"kernel/sm90_gemm_tma_warpspecialized_with_prefetch.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Numeric Conversion and Packing Example in CUTLASS\nDESCRIPTION: This example demonstrates how to convert and pack signed 32-bit integers to a vector of packed signed 8-bit integers using `NumericConverter` in CUTLASS.  Requires the `cutlass` library and assumes the types `int8_t` and `int` are defined.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n//\n// Example: convert and pack 32b signed integers to a vector of packed signed 8-bit integers.\n//\nint const kN = 16;\nArray<int8_t, kN> destination;\nArray<int,    kN> source;\n\nNumericConverter<descltype(destination), decltype(source)> convert;\n\ndestination = convert(source);\n```\n\n----------------------------------------\n\nTITLE: Conv2d: Running non-default Conv2ds, manual tile description\nDESCRIPTION: This code segment shows how to manually set the tile description based on the compute capability (cc). It sets the threadblock shape, warp count, stages and instruction shape based on the cc value and then runs the plan. This allows more control over the generated kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif plan.cc == 70:\n    plan.tile_description = {\n        \"threadblock_shape\": [64, 256, 32],\n        \"warp_count\": [1, 4, 1],\n        \"stages\": 2,\n        \"instruction_shape\": [8, 8, 4], # optional,\n        \"cluster_shape\": [1, 1, 1] # optional, only [1, 1, 1] is supported currently\n    }\nelif plan.cc == 75:\n    plan.tile_description = {\n        \"threadblock_shape\": [128, 64, 32],\n        \"warp_count\": [2, 1, 1],\n        \"stages\": 2,\n        \"instruction_shape\": [16, 8, 8], # optional,\n        \"cluster_shape\": [1, 1, 1] # optional, only [1, 1, 1] is supported currently\n    }\nelif plan.cc == 80:\n    plan.tile_description = {\n        \"threadblock_shape\": [128, 128, 64],\n        \"warp_count\": [2, 2, 1],\n        \"stages\": 4,\n        \"instruction_shape\": [16, 8, 16], # optional,\n        \"cluster_shape\": [1, 1, 1] # optional, only [1, 1, 1] is supported currently\n    }\nelif plan.cc == 86:\n    plan.tile_description = {\n        \"threadblock_shape\": [128, 64, 64],\n        \"warp_count\": [2, 2, 1],\n        \"stages\": 3,\n        \"instruction_shape\": [16, 8, 16],\n        \"cluster_shape\": [1, 1, 1]\n    }\n\nplan.run(input, weight, tensor_C, output, stride, padding, dilation, alpha, beta, print_module=print_module)\nassert torch.equal(output_torch, output)\n```\n\n----------------------------------------\n\nTITLE: Add S8-S32 Convolution Test Executable (SM75)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with S8 input, S32 output, and S32 accumulation on SM75 architecture, conditionally compiled based on the `CUTLASS_NVCC_MAX_ARCH` variable. It includes source files for forward propagation with both S8 and S4 input types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 75)\n  # Conv2d - S8 input, S32 output, S32 accumulation\n  cutlass_test_unit_add_executable(\n    cutlass_test_unit_conv_device_tensorop_s32\n    conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu\n    conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: GEMM with Tanh Activation\nDESCRIPTION: This example demonstrates a GEMM operation with tanh activation function using float32 data type and TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 4 -lb ColumnMajor -ab 4 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm GemmSplitKParallel -k 2 -bias -activ tanh\n```\n\n----------------------------------------\n\nTITLE: Describing A and B Tensors for GEMM\nDESCRIPTION: This C++ code snippet demonstrates how to define the A and B tensors for a block-scaled GEMM kernel using the CUTLASS library.  It specifies the element types, alignment requirements, and memory layouts for both tensors, using `mx_float4_t` for the element type, 128 for alignment, and `ColumnMajor` and `RowMajor` for the layouts of A and B respectively. These specific configurations are chosen to align with the `tcgen05.mma.kind:mxf8f6f4` instruction.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n  ///////////////////////////////////////////////////////////\n  //                Mainloop Builder Setup\n  ///////////////////////////////////////////////////////////\n  \n  ///////////////////////////////////////////\n  // 1. Describe A and B tensors\n  ///////////////////////////////////////////\n  using ElementA       = mx_float4_t;\n  constexpr int AlignA = 128;\n  using GmemLayoutA    = cutlass::layout::ColumnMajor;\n  using ElementB       = mx_float4_t;\n  constexpr int AlignB = 128;\n  using GmemLayoutB    = cutlass::layout::RowMajor;\n```\n\n----------------------------------------\n\nTITLE: Profiling Tensor Core GEMM Operations (CUTLASS)\nDESCRIPTION: This command executes kernels targeting Tensor Core operations using the `--op_class=tensorop` flag. It profiles GEMM operations with the specified matrix dimensions (m, n, k).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --op_class=tensorop --m=3456 --n=4096 --k=8192\n```\n\n----------------------------------------\n\nTITLE: HostTensor Example\nDESCRIPTION: This example demonstrates how to allocate a column-major matrix of single-precision floating-point elements using `cutlass::HostTensor`. It initializes the tensor with a specified number of rows and columns.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/layout/matrix.h>\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n  int rows = 32;\n  int columns = 16;\n\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> tensor({rows, columns});\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Layout Complement Interface in CuTe (C++)\nDESCRIPTION: This C++ code snippet presents the interface for the `complement` function in CuTe. The `complement` function calculates the layout representing the elements *not* covered by the input layout `layout_a` within a specified `cotarget` shape. It requires CuTe and its associated data types. The code provides post-conditions that the complement result satisfies.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// @post cosize(make_layout(@a layout_a, @a result))) >= size(@a cotarget)\n// @post cosize(@a result) >= round_up(size(@a cotarget), cosize(@a layout_a))\n// @post for all i, 1 <= i < size(@a result),\n//         @a result(i-1) < @a result(i)\n// @post for all i, 1 <= i < size(@a result),\n//         for all j, 0 <= j < size(@a layout_a),\n//           @a result(i) != @a layout_a(j)\nLayout complement(LayoutA const& layout_a, Shape const& cotarget)\n```\n\n----------------------------------------\n\nTITLE: Colab GPU Check\nDESCRIPTION: This snippet checks if an NVIDIA GPU is available in the Colab environment. If `nvidia-smi` fails, it instructs the user to select a GPU runtime.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!#nvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Use Raster Order and Swizzle Size\nDESCRIPTION: This command executes the CUTLASS profiler using a CUTLASS 3.x GEMM kernel, enabling runtime tile remapping and setting the raster order to 'M' (along M dimension) and the swizzle size to 2. These features are used to optimize memory access patterns.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --m=2048 --n=2048 --k=2048 --raster_order=M --swizzle_size=2\n```\n\n----------------------------------------\n\nTITLE: Defining Contiguous Memory Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept for tiles stored contiguously in memory. It includes an `Index` type for pointer offsets and the `add_pointer_offset` method to move the iterator within the contiguous memory block. This iterator is derived from the base `TileIteratorConcept`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n/// Tile iterator over partitions of a tensor in contiguous memory which may be referenced via a\n/// TensorRef object.\nstruct ContiguousMemoryTileIterator : public TileIteratorConcept {\n\n  using Index;            ///< index type used to add pointer offsets\n\n  /// Adds a linear offset in units of Element to internal pointer(s) into tensor\n  CUTLASS_DEVICE \n  void add_pointer_offset(Index pointer_offset);\n};\n```\n\n----------------------------------------\n\nTITLE: Array Usage Example with Range-Based For Loop in CUTLASS\nDESCRIPTION: This example shows how to use `Array` with a range-based for loop in CUTLASS. The `CUTLASS_PRAGMA_UNROLL` pragma is required to ensure that the array elements are stored in registers. Explicitly converting to `int64_t` or `double` might be required when printing the array values. Requires `cutlass` library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nint const kN;\nArray<T, kN> elements;\n\nCUTLASS_PRAGMA_UNROLL                        // required to ensure array remains in registers\nfor (auto x : elements) {\n  printf(\"%d, %f\", int64_t(x), double(x));   // explictly convert to int64_t or double\n}\n```\n\n----------------------------------------\n\nTITLE: Running GEMM kernel with PDL in CUTLASS\nDESCRIPTION: This code snippet demonstrates how to launch a GEMM kernel with PDL enabled. The `launch_with_pdl` parameter is set to `true` to signal the CUDA runtime to use PDL for this kernel launch. This requires a CUDA stream and potentially a CUDA adapter, depending on the specific CUTLASS GEMM implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/dependent_kernel_launch.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ngemm.run(\n  /* stream = */ stream,\n  /* cuda_adapter = */ nullptr,\n  /* launch_with_pdl = */ true\n);_\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS Profiler with Convolution and cuDNN (CUTLASS)\nDESCRIPTION: This command shows how to build the CUTLASS profiler with cuDNN enabled or disabled. `-DCUTLASS_LIBRARY_OPERATIONS=conv2d` enables convolution operations, and `-DCUTLASS_ENABLE_CUDNN=OFF` disables cuDNN dependency. The `make` command then compiles the profiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_LIBRARY_OPERATIONS=conv2d -DCUTLASS_ENABLE_CUDNN=OFF\n...\n$ make -j16 cutlass_profiler\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS Python Interface from source\nDESCRIPTION: This command installs the CUTLASS Python interface directly from the source code.  It allows for local development and testing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: GEMM F16 Example\nDESCRIPTION: This example demonstrates a GEMM operation using float16 inputs and float32 output. It configures tensor layouts (ColumnMajor, RowMajor), block sizes, swizzle functions, and specifies TensorOp for computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 16 -ta float16 -tb float16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb RowMajor -ab 8 -lc ColumnMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle4 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 1\n```\n\n----------------------------------------\n\nTITLE: Defining Writeable Random Access Contiguous Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept that supports writing tiles to contiguous memory with random access and both a tile offset and pointer offset. It inherits from `WriteableRandomAccessTileIteratorConcept` and `WriteableContiguousTileIteratorConcept`. It provides a `store` method to store a fragment given a `TensorCoord` tile offset and an `Index` pointer offset.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_12\n\nLANGUAGE: c++\nCODE:\n```\n/// Stores a fragment with a logical coordinate offset in units of whole tiles.\nstruct WriteableRandomAccessContiguousTileIteratorConcept : \n  public WriteableRandomAccessTileIteratorConcept, \n  public WriteableContiguousTileIteratorConcept {\n\n  /// Stores a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE \n  void store(\n    Fragment const &frag,                       ///< fragment to store to the location pointed to by the tensor\n    TensorCoord const &tile_offset,             ///< stores a tile with a logical offset in units of whole tiles\n    Index pointer_offset);                      ///< stores a tile with a logical offset AND a pointer offset\n}\n```\n\n----------------------------------------\n\nTITLE: GEMM F64 SplitK Example\nDESCRIPTION: This example showcases a GEMM operation with float64 and split-K parallelism.  Split-K divides the K dimension into smaller chunks for parallel processing. Key parameters are set, including data types, tensor layouts, and optimization options.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 8 8 4 -ta float64 -tb float64 -tc float64 -tacc float64 -m multiply_add -op TensorOp -b 64 64 16 -s 4 -w 2 2 1 -cc 80 -la RowMajor -aa 1 -lb ColumnMajor -ab 1 -lc RowMajor -ac 1 -te float64 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 2\n```\n\n----------------------------------------\n\nTITLE: Conv2d: General setup\nDESCRIPTION: This snippet initializes various Python packages, input/output tensors for Conv2d operation using torch. It also sets up the dimensions (N, H, W, C, K, R, S), stride, padding, and dilation parameters and declares types for tensors. Finally, the CUDA tensors are initialized with random data. This is a common prerequisite for all Conv2d operations in the example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport random\n\nimport cutlass\n\n# This controls whether the C++ GEMM declaration will be printed at each step. \n# Set to `false` to omit this information.\nprint_module = True\n\n# Input tensor: [N, H, W, C] under the channel-last layout\nN, H, W, C = [32, 28, 28, 64]\n\n# Weight tensor: [K, R, S, C] under the channel-last layout\nK, R, S = [128, 3, 3]\n\n# Stride, and padding\nstride = (2, 2)\npadding = (1, 1)\ndilation = (1, 1)\n\n# Compute the output size [N, P, Q, K]\nN, P, Q, K = cutlass.Conv2d.output_size((N, H, W, C), (K, R, S, C), padding, stride, dilation)\n\ndtype = torch.float16\ntype_A = torch.float16\ntype_B = torch.float16\ntype_C = torch.float16\ntype_D = torch.float16\n\ntorch.manual_seed(1234)\n\ninput = torch.ceil(\n    torch.empty(size=(N, C, H, W), dtype=type_A, device=\"cuda\").uniform_(-4.5, 3.5)\n).to(memory_format=torch.channels_last)\nweight = torch.ceil(\n    torch.empty(size=(K, C, R, S), dtype=type_B, device=\"cuda\").uniform_(-4.5, 3.5)\n).to(memory_format=torch.channels_last)\ntensor_C = torch.ceil(\n    torch.empty(size=(N, K, P, Q), dtype=type_B, device=\"cuda\").uniform_(-4.5, 3.5)\n).to(memory_format=torch.channels_last)\noutput = torch.zeros_like(tensor_C)\n\nalpha = 1.0\nbeta = 0.0\n```\n\n----------------------------------------\n\nTITLE: Initializing GEMM Matrices in Python\nDESCRIPTION: This snippet defines two utility functions for initializing GEMM matrices of random sizes. `initialize` creates A, B, C, and D matrices with random integer values based on given dimensions M, N, and K, and `generate_problems` generates a list of `problems` number of GEMMs with random sizes chosen from a set of valid sizes (128, 256, 512, 1024).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nrandom.seed(2023)\n\n# Utility function to initialize A, B, C, and D matrices corresponding to dimensions M, N, and K\ndef initialize(dtype, M, N, K):\n    sizes = [(M, K), (K, N), (M, N), (M, N)]\n    return [torch.randint(-3, 3, size, device='cuda').to(dtype) for size in sizes]\n\n# Utility function to generate `problems` GEMMs of random sizes\ndef generate_problems(problems):\n    valid_sizes = [128, 256, 512, 1024]\n    As, Bs, Cs, Ds = [], [], [], []\n    for _ in range(problems):\n        M, N, K = [random.choice(valid_sizes) for _ in range(3)]\n        A, B, C, D = initialize(dtype, M, N, K)\n        As.append(A)\n        Bs.append(B)\n        Cs.append(C)\n        Ds.append(D)\n    return As, Bs, Cs, Ds\n```\n\n----------------------------------------\n\nTITLE: Configure Optimized Convolution kernels\nDESCRIPTION: This CMake command configures CUTLASS to compile backward weight gradient convolution kernels with FP32 accumulation, FP16 input, and optimized global memory iterator targeting NVIDIA Ampere, Turing, and Volta Tensor Core operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_36\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='70;75;80' -DCUTLASS_LIBRARY_KERNELS=tensorop*s*wgrad_optimized_f16\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 2SM)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following matrix multiplication using the TMA (Tensor Memory Accelerator) across two SMs (Streaming Multiprocessors).  It's applicable to both dense and sparse matrices and supports legacy and narrow precision configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized2Sm`\n```\n\n----------------------------------------\n\nTITLE: Tiling and Partitioning Reference Tensors\nDESCRIPTION: This snippet illustrates how reference tensors are tiled and partitioned in the same way as the original data. It involves partitioning identity tensors to create tensors that map thread coordinates to original coordinates for the purpose of predication.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\nTensor tAcA = local_partition(cA, tA, thread_idx);\nTensor tBcB = local_partition(cB, tB, thread_idx);\n```\n\n----------------------------------------\n\nTITLE: Defining Writeable Tile Iterator Concept in C++\nDESCRIPTION: Defines a tile iterator concept for writing tiles to memory. It specifies a `Fragment` type, holding each thread's part of the data to be written, and a `store` method to write the tile from the `Fragment` object to memory. This concept is essential for writing results back to global memory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\n/// Tile iterator capable of storing tiles from memory\nstruct WriteableTileIteratorConcept {\n\n  using Fragment;                     ///< fragment object derived from cutlass::Array<Element, N>\n\n  /// Stores a fragment to memory\n  CUTLASS_DEVICE\n  void store(Fragment const &frag);             ///< stores a fragment to memory\n};\n```\n\n----------------------------------------\n\nTITLE: Define thread ID mapping for HMMA NT operation\nDESCRIPTION: Defines the thread ID mapping for the HMMA NT operation using the `Layout` template.  The `ThrID` alias maps logical thread IDs to quadpair thread indices within a warp. It maps eight logical thread ids [0,8) of the MMA operation onto the quadpair thread index [0,4)U[16,20) of a warp using a layout function with 4 elements with a stride of 1 and 2 of those with a stride of 16.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nusing ThrID = Layout<Shape <_4, _2>,\n                     Stride<_1,_16>>;\n```\n\n----------------------------------------\n\nTITLE: GemmUniversal Class Definition\nDESCRIPTION: Defines the `GemmUniversal` class, a stateless universal device GEMM kernel type that composes a collective mainloop and a collective epilogue. It supports both the 2.x and 3.x APIs based on the template arguments provided.  The template arguments define the problem shape, collective mainloop, epilogue, and tile scheduler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace cutlass::gemm::kernel {\n/*\n * Stateless universal device GEMM kernel type that treats GEMM as\n * a composition of a collective mainloop and a collective epilogue.\n *\n * Supports both the 2.x and 3.x APIs based on whether the first type is\n * a cute::tuple<> or not.\n * 2.x API implementation: cutlass/gemm/kernel/gemm_universal.h\n * 3.x API implementation: cutlass/gemm/kernel/gemm_*.hpp\n *\n * In the following declaration, the name preceding the 'Or' refers to\n * 3.x API type argument order, and the name succeeding the 'Or' refers to\n * 2.x API type argument order. Template arguments without two names\n * belong to the 3.x API only.\n**/\ntemplate <\n  class ProblemShapeOrThreadblockMma_, // (m, n, k) or (m, n, k, l)\n  class CollectiveMainloopOrEpilogue_,\n  class CollectiveEpilogueOrThreadblockSwizzle_,\n  class TileScheduler_ = void,\n  class Enable = void\n>\nclass GemmUniversal;\n} // namespace cutlass::gemm::kernel\n```\n\n----------------------------------------\n\nTITLE: Index Mapping with crd2idx in CuTe (C++)\nDESCRIPTION: This snippet illustrates how `cute::crd2idx` maps coordinates to indices using a specified shape and stride. The shape is defined as `Shape <_3,Shape< _2,_3>>` and the stride is defined as `Stride<_3,Stride<_12,_1>>`. It takes a coordinate, the shape, and the stride as input and returns the corresponding index.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nauto shape  = Shape <_3,Shape<  _2,_3>>{};\nauto stride = Stride<_3,Stride<_12,_1>>{};\nprint(crd2idx(   16, shape, stride));       // 17\nprint(crd2idx(_16{}, shape, stride));       // _17\nprint(crd2idx(make_coord(   1,   5), shape, stride));  // 17\nprint(crd2idx(make_coord(_1{},   5), shape, stride));  // 17\nprint(crd2idx(make_coord(_1{},_5{}), shape, stride));  // _17\nprint(crd2idx(make_coord(   1,make_coord(   1,   2)), shape, stride));  // 17\nprint(crd2idx(make_coord(_1{},make_coord(_1{},_2{})), shape, stride));  // _17\n```\n\n----------------------------------------\n\nTITLE: cuBLAS GEMM Example with Leading Dimension\nDESCRIPTION: This code snippet shows how cuBLAS uses the leading dimension (`lda`) to access elements in a matrix. Given a row and column index, the offset is calculated as `row + lda * column`.  `lda` defines the stride between columns, illustrating how memory is organized in column-major format. This example showcases the concept of leading dimension which is generalized by CUTLASS layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncublasGemmEx(\n  ...\n  ptr_A,      // pointer to first element of matrix A\n  lda,        // leading dimension\n  ...\n);\n```\n\n----------------------------------------\n\nTITLE: Define Arguments for CUTLASS Convolution\nDESCRIPTION: This code snippet collects the arguments for an implicit GEMM convolution operation into a structure. It includes the convolution mode (`kCrossCorrelation`), the number of K-dimension partitions (`split_k_slices`), and a `Conv2dProblemSize` object that encapsulates the dimensions of the input and output tensors, padding, stride, and dilation.  The arguments structure also contains references to the input, filter, and output tensors, as well as alpha and beta scaling factors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_10\n\nLANGUAGE: c++\nCODE:\n```\n//\n// Define arguments for CUTLASS Convolution\n//\n\n// mode (kCrossCorrelation or kConvolution)\ncutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation;\n\n// Split K dimension into 1 partitions\nint split_k_slices = 1;\n\ncutlass::conv::Conv2dProblemSize problem_size(      \n    options.input_size,\n    options.filter_size,\n    options.padding,\n    options.conv_stride,\n    options.dilation,\n    options.output_size(),\n    mode,\n    split_k_slices);\n\ntypename ImplicitGemm::Arguments arguments{\n  problem_size,\n  tensor_a.device_ref(),\n  tensor_b.device_ref(),\n  tensor_c.device_ref(),\n  tensor_c.device_ref(),\n  {options.alpha, options.beta},\n};\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Layout Concept Example\nDESCRIPTION: This code defines a C++ struct `LayoutConcept` that demonstrates the core requirements of a CUTLASS layout.  It specifies the rank of the tensor, stride rank, index types, constructor, packed layout helper, function call operator for offset calculation, inverse mapping function, stride accessor, and capacity computation. The layout concept maps logical index space to physical offsets in memory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n/// CUTLASS Layout concept example\nstruct LayoutConcept {\n\n  /// Logical rank of tensor\n  static int const kRank;\n\n  /// Rank of stride vector\n  static int const kStrideRank;\n\n  /// Index type used for coordinates\n  struct Index;\n\n  /// Long index type used for offsets\n  struct LongIndex;\n\n  /// Logical coordinate - satisfies Coord<kRank, ..>\n  struct TensorCoord;\n\n  /// Stride object - satisfies Coord<kStrideRank, ..>\n  struct Stride\n\n  //\n  // Methods\n  //\n\n  /// Constructor\n  CUTLASS_HOST_DEVICE\n  LayoutConcept();\n\n  /// Ctor\n  CUTLASS_HOST_DEVICE\n  LayoutConcept(Stride stride);\n\n  /// Helper returns a layout to a tightly packed tensor\n  CUTLASS_HOST_DEVICE\n  static LayoutConcept packed(TensorCoord const &extent);\n\n  /// Function call operator returns the offset of a coordinate in linear memory. \n  /// Assumes coordinate has convention (row, column)\n  CUTLASS_HOST_DEVICE\n  LongIndex operator()(TensorCoord const &coord) const;\n\n  /// Inverse of layout function, mapping linear offset to logical coordinate\n  CUTLASS_HOST_DEVICE\n  TensorCoord inverse(LongIndex offset) const;\n\n  /// Returns the stride of the layout\n  CUTLASS_HOST_DEVICE\n  Stride stride() const;\n\n  /// Returns the stride of the layout\n  CUTLASS_HOST_DEVICE\n  Stride & stride();\n\n  /// Compute the number of contiguous elements needed to store a tensor with the given size\n  CUTLASS_HOST_DEVICE\n  LongIndex capacity(TensorCoord const &extent) const;\n};\n```\n\n----------------------------------------\n\nTITLE: Add Executable for f16xf16 TensorOp GEMM Test (sm100)\nDESCRIPTION: This CMake snippet uses a custom function `cutlass_test_unit_gemm_device_add_executable` to add an executable for testing f16xf16 GEMM operations using Tensor Cores on the sm100 architecture. It enables batching and specifies the source files for the test, including fusion kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_sm100_f16xf16\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  f16_f16_void_f32.cu\n  f16_f16_f16_f16_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: GEMM Kernel Pseudocode in C++\nDESCRIPTION: This pseudocode illustrates the model for a GEMM kernel targeting a warp-synchronous matrix multiply instruction. It shows the nested loops structure and how the code relates to the inner and outer loops of the GEMM algorithm. The loops correspond to parallelism over thread block clusters.  It demonstrates a simplified view of the core computations in the GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n// cutlass::gemm::kernel::GemmUniversal: ClusterTileM and ClusterTileN loops\n//   are either rasterized by the hardware or scheduled by the kernel in persistent kernels.\n// Parallelism over thread block clusters\nfor (int cluster_m = 0; cluster_m < GemmM; cluster_m += ClusterTileM) {\n  for (int cluster_n = 0; cluster_n < GemmN; cluster_n += ClusterTileN) {\n\n    // cutlass::gemm::collective::CollectiveMma: mainloop that iterates over all k-tiles\n    // No loop unrolling is performed at this stage\n    for (int k_tile = 0; k_tile < size<2>(gmem_tensor_A); k_tile++) {\n\n      // loops inside cute::gemm(tiled_mma, a, b, c); Dispatch 5: (V,M,K) x (V,N,K) => (V,M,N)\n      // TiledMma uses the hardware instruction provided through its Mma_Atom\n      // TiledMma's atom layout, value layout, and permutations define the iteration order\n      for (int tiled_mma_k = 0; tiled_mma_k < size<2>(A); tiled_mma_k++) {\n        for (int tiled_mma_m = 0; tiled_mma_m < size<1>(A); tiled_mma_m++) {\n          for (int tiled_mma_n = 0; tiled_mma_n < size<1>(B); tiled_mma_n++) {\n\n            // TiledMma's vector mode dispatches to the underlying instruction.\n            mma.call(d, a, b, c);\n          } // tiled_mma_n\n        } // tiled_mma_m\n      } // tiled_mma_k\n    } // k_tile mainloop\n  } // cluster_m\n} // cluster_n\n```\n\n----------------------------------------\n\nTITLE: Kernel Schedule Structs in C++\nDESCRIPTION: Presents a list of different kernel schedule structs that can be composed with `CollectiveMma` implementations. These structs control the execution order and synchronization mechanisms within the kernel. The structs offer various levels of specialization, including warp-specialized, ping-pong buffering, and cooperative execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nstruct KernelCpAsyncWarpSpecialized { };\nstruct KernelCpAsyncWarpSpecializedPingpong { };\nstruct KernelCpAsyncWarpSpecializedCooperative { };\nstruct KernelTma { };\nstruct KernelTmaWarpSpecialized { };\nstruct KernelTmaWarpSpecializedPingpong { };\nstruct KernelTmaWarpSpecializedCooperative { };\n```\n\n----------------------------------------\n\nTITLE: Generating and Running CUTLASS B2B GEMMs\nDESCRIPTION: This shell script demonstrates how to generate code for back-to-back GEMMs using CUTLASS, build the generated code, and run the resulting executable. It requires a configuration file (config.json) specifying the GEMMs to be fused. It takes the configuration file path, output directory, and CUTLASS directory as input, and then executes the generated sample with specified matrix dimensions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/44_multi_gemm_ir_and_codegen/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd ir_gen\n\n# Set up basic variables\nout_dir=directory_to_emit_files\ncutlass_dir=$(pwd)/../../..\nconfig_file=$(pwd)/../config.json\n\n# Generate code for GEMMs described in `config_file`\n./generate.sh $config_file $out_dir $cutlass_dir\n\n# Build the generated code\ncd $out_dir\nmkdir build && cd build\ncmake .. -DGPU_ARCHS=\"75;80\"\nmake -j\n\n# Run the generated code with M=1024 K0=32 and Batch=1\n./sample 1024 32 1\n```\n\n----------------------------------------\n\nTITLE: Performance Optimization for Series of GEMM Shapes (CUTLASS)\nDESCRIPTION: This command optimizes kernel performance for a series of GEMM problem sizes. In this example, it searches over m, n, k = 1024 and 2048.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncutlass_profiler --kernels=*gemm* --enable-best-kernel-for-fixed-shape --m=1024,2048 --n=1024,2048 --k=1024,2048 --sort-results-flops-per-sec\n```\n\n----------------------------------------\n\nTITLE: TensorFillRandomGaussian with Bit Control\nDESCRIPTION: This example demonstrates how to use `cutlass::reference::host::TensorFillRandomGaussian` and `cutlass::reference::device::TensorFillRandomGaussian` to initialize a tensor with a random Gaussian distribution, limiting the number of bits of the mantissa less than 1 that are non-zero.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/layout/matrix.h>\n#include <cutlass/util/reference/host/tensor_fill.h>\n#include <cutlass/util/reference/device/tensor_fill.h>\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n\n  int rows = 128;\n  int columns = 64;\n\n  double mean = 0.5;\n  double stddev = 2.0;\n  uint64_t seed = 0x2019;\n\n  int bits_right_of_binary_decimal = 2;\n\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> tensor({rows, columns});\n\n  // Initialize in host memory\n  cutlass::reference::host::TensorFillRandomGaussian(\n    tensor.host_view(),\n    seed,\n    mean,\n    stddev,\n    bits_right_of_binary_decimal);\n\n  // Initialize in device memory\n  cutlass::reference::device::TensorFillRandomGaussian(\n    tensor.device_view(),\n    seed,\n    mean,\n    stddev,\n    bits_right_of_binary_decimal);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Define Thread Layouts (GEMM_NT) C++\nDESCRIPTION: This code defines thread layouts `tA` and `tB` for partitioning data across threads in the `gemm_nt` kernel. The layouts are created using `make_layout` and `make_shape`, and specify the dimensions of the thread grid (32x8). The layouts are M-major and N-major.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_15\n\nLANGUAGE: c++\nCODE:\n```\n  // Define thread layouts (static)\n  auto tA = make_layout(make_shape(Int<32>{},Int<8>{}));   // (m,k) -> thr_idx\n  auto tB = make_layout(make_shape(Int<32>{},Int<8>{}));   // (n,k) -> thr_idx\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Pipeline Example in CUTLASS C++\nDESCRIPTION: This C++ code demonstrates the usage of CUTLASS's asynchronous pipeline class `PipelineAsync` for synchronizing producer and consumer threads. It initializes a 4-stage pipeline with 2 producer threads and 1 consumer thread, using `producer_acquire`, `producer_commit`, `consumer_wait`, and `consumer_release` methods for synchronization. The code requires the cutlass/pipeline.h and cutlass/pipeline/pipeline.hpp headers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/pipeline.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// 4-stage Pipeline\nstatic constexpr int NumStages = 4;\nusing MainloopPipeline = typename cutlass::PipelineAsync<NumStages>;\nusing PipelineState = typename cutlass::PipelineState<NumStages>;\n\n// 2 producer threads and 1 consumer thread \ntypename MainloopPipeline::Params params;\nparams.producer_arv_count = 2;\nparams.consumer_arv_count = 1;\nMainloopPipeline pipeline(shared_storage.storage, params);\n  \n// Producer threads\nif (thread_idx == 0 or thread_idx == 1) {\n  PipelineState smem_pipe_write = cutlass::make_producer_start_state<MainloopPipeline>();\n  for ( ; iter > 0; --iter) {\n    pipeline.producer_acquire(smem_pipe_write);\n\n    // Producer ops\n    // If any memory operations are involved, then we also need\n    // to guarantee that writes are completed and visible to consumer(s).\n\n    pipeline.producer_commit(smem_pipe_write);\n    ++smem_pipe_write;\n  }\n}\nelse if (thread_idx == 2) {\n  PipelineState smem_pipe_read;\n  for (; iter > 0; --iter) {\n    pipeline.consumer_wait(smem_pipe_read);\n\n    // Consumer ops\n\n    pipeline.consumer_release(smem_pipe_read);\n    ++smem_pipe_read;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building all GEMM/Convolution Kernels with CMake and Make\nDESCRIPTION: These commands are used to build all available GEMM and Convolution kernels within CUTLASS. It involves first configuring the build environment with CMake, specifying the target architecture and enabling all library kernels. After CMake configuration, the `make` command compiles the kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_KERNELS=all\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Conv2d: Running non-default Conv2ds, tile descriptions\nDESCRIPTION: This snippet shows how to configure the tile description for the Conv2d kernel. First, it lists the available tile descriptions, then it selects one tile description randomly, set it to plan, and runs the plan. This demonstrates the customization of the Conv2d kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplan.opclass = \"tensor_op\"\ntiles = plan.tile_descriptions()\nprint(f'{len(tiles)} tile descriptions returned')\nnum_print = 10\nprint(f'First {num_print} tile descriptions are:')\nfor td in tiles[:num_print]:\n    print(td)\n\nrandom.seed(42)\nidx = random.randint(0, len(tiles)-1)\ntd = tiles[idx]\nprint(f'Tile description {idx} is: {td}')\nplan.tile_description = td\nplan.run(input, weight, tensor_C, output, stride, padding, dilation, alpha, beta, print_module=print_module)\nassert torch.equal(output_torch, output)\n```\n\n----------------------------------------\n\nTITLE: Outer Partitioning Example C++\nDESCRIPTION: This code demonstrates the outer-partitioning technique. It creates a tensor `A`, applies a tiler, and then slices the tiled tensor to extract a subtile using thread coordinates. The resulting tensor `thr_a` represents a portion of the tile assigned to a thread.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nTensor A = make_tensor(ptr, make_shape(8,24));  // (8,24)\nauto tiler = Shape<_4,_8>{};                    // (_4,_8)\n\nTensor tiled_a = zipped_divide(A, tiler);       // ((_4,_8),(2,3))\n\nTensor thr_a = tiled_a(threadIdx.x, make_coord(_,_)); // (2,3)\n```\n\n----------------------------------------\n\nTITLE: Partitioning Data with Thread Layouts (C++)\nDESCRIPTION: This code partitions the shared memory tensors `sA`, `sB`, and `gC` based on the thread layout `tC`. It uses the `local_partition` function to create thread-partitioned tensors `tCsA`, `tCsB`, and `tCgC`. It also allocates an accumulator tensor `tCrC` with the same shape and layout as `tCgC`. The code relies on compile-time assertions to validate the partitioning.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\n  // Partition sA (M,K) by the rows of tC\n  Tensor tCsA = local_partition(sA, tC, threadIdx.x, Step<_1, X>{});   // (THR_M,BLK_K)\n  // Partition sB (N,K) by the cols of tC\n  Tensor tCsB = local_partition(sB, tC, threadIdx.x, Step< X,_1>{});   // (THR_N,BLK_K)\n  // Partition gC (M,N) by the tile of tC\n  Tensor tCgC = local_partition(gC, tC, threadIdx.x, Step<_1,_1>{});   // (THR_M,THR_N)\n\n  // Allocate the accumulators -- same shape/layout as the partitioned data\n  Tensor tCrC = make_tensor_like(tCgC);                                // (THR_M,THR_N)\n\n  CUTE_STATIC_ASSERT_V(size<0>(tCrC) == size<0>(tCgC));                // THR_M\n  CUTE_STATIC_ASSERT_V(size<0>(tCrC) == size<0>(tCsA));                // THR_M\n  CUTE_STATIC_ASSERT_V(size<1>(tCrC) == size<1>(tCgC));                // THR_N\n  CUTE_STATIC_ASSERT_V(size<1>(tCrC) == size<0>(tCsB));                // THR_N\n  CUTE_STATIC_ASSERT_V(size<1>(tCsA) == size<1>(tCsB));                // BLK_K\n```\n\n----------------------------------------\n\nTITLE: TV-Layout Partitioning C++\nDESCRIPTION: This code constructs a TV-layout and composes it with a tensor to transform the tensor's shape and order. It then slices the resulting tensor based on the thread index to partition the data such that each thread has a specific set of values. This technique is commonly used for thread-value partitioning in CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// Construct a TV-layout that maps 8 thread indices and 4 value indices\n//   to 1D coordinates within a 4x8 tensor\n// (T8,V4) -> (M4,N8)\nauto tv_layout = Layout<Shape <Shape <_2,_4>,Shape <_2, _2>>,\n                        Stride<Stride<_8,_1>,Stride<_4,_16>>>{}; // (8,4)\n\n// Construct a 4x8 tensor with any layout\nTensor A = make_tensor<float>(Shape<_4,_8>{}, LayoutRight{});    // (4,8)\n// Compose A with the tv_layout to transform its shape and order\nTensor tv = composition(A, tv_layout);                           // (8,4)\n// Slice so each thread has 4 values in the shape and order that the tv_layout prescribes\nTensor  v = tv(threadIdx.x, _);                                  // (4)\n```\n\n----------------------------------------\n\nTITLE: Creating an Implicit Tensor (C++)\nDESCRIPTION: This code snippet demonstrates how to create an implicit CuTe Tensor using a counting iterator. The tensor maps logical coordinates to on-the-fly computed integers. This example initializes a tensor with a counting iterator starting at 42 and a shape of 4x5.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0z_tma_tensors.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTensor A = make_tensor(counting_iterator<int>(42), make_shape(4,5));\nprint_tensor(A);\n```\n\n----------------------------------------\n\nTITLE: CUTLASS GEMM Model Pseudocode (C++)\nDESCRIPTION: This pseudocode describes the CUTLASS GEMM model, illustrating the nested loop structure mirroring the execution model hierarchy.  It targets a warp-synchronous matrix multiply instruction and includes loops for CTA, warp, and MMA levels. The comments indicate the corresponding CUTLASS components involved in each loop.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nfor (int cta_n = 0; cta_n < GemmN; cta_n += CtaTileN) {                     // for each CTA       } CTA-level concurrency\n  for (int cta_m = 0; cta_m < GemmM; cta_m += CtaTileM) {                   //    for each CTA    }\n                                                                            //    \n                                                                            // cutlass::gemm::threadblock::Mma\n                                                                            //\n    for (int cta_k = 0; cta_k < GemmK; cta_k += CtaTileK) {                 //       \"GEMM mainloop\" - no unrolling - one iteration of this loop is one \"stage\"\n                                                                            //\n      for (int warp_n = 0; warp_n < CtaTileN; warp_n += WarpTileN) {        // for each warp      } warp-level concurrency\n        for (int warp_m = 0; warp_m < CtaTileM; warp_m += WarpTileM) {      //    for each warp   }\n                                                                            //\n          for (int warp_k = 0; warp_k < CtaTileK; warp_k += WarpTileK) {    //       fully unroll across CtaTileK - one iteration of this loop is one \"k Group\"\n                                                                            //\n            for (int mma_k = 0; mma_k < WarpTileK; mma_k += MmaK) {         // cutlass::gemm::warp::Mma\n              for (int mma_n = 0; mma_n < WarpTileN; mma_n += MmaN) {\n                for (int mma_m = 0; mma_m < WarpTileM; mma_m += MmaM) {\n                                                                            //\n                  mma_instruction(d, a, b, c);                              // cutlass::arch::mma - warp-wide matrix multiply instruction\n\n                }   // for mma_m\n              }   // for mma_n\n            }   // for mma_k\n\n          }   // for warp_k\n        }   // for warp_m\n      }   // for warp_n\n\n    }   // for cta_k\n  }   // for cta_m\n}   // for cta_n\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Collective Mainloop for FP8 GEMM\nDESCRIPTION: This snippet shows how to instantiate the collective mainloop for an FP8 GEMM kernel on SM100. It defines the mainloop schedule and then uses the `CollectiveBuilder` to create the `CollectiveOp` type, specifying architecture, operation class, data types, layouts, tile shapes, cluster shape, stage count, and the schedule.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_39\n\nLANGUAGE: c++\nCODE:\n```\nusing MainloopSchedule = cutlass::gemm::KernelTmaWarpSpecialized1SmSm100;\nusing CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<\n    cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp,\n    ElementA, LayoutA, 16 / sizeof(ElementA),\n    ElementB, LayoutB, 16 / sizeof(ElementB),\n    ElementAccumulator,\n    MmaTileShape, ClusterShape,\n    cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,\n    MainloopSchedule\n  >::CollectiveOp;\n```\n\n----------------------------------------\n\nTITLE: ColumnMajor Layout Example\nDESCRIPTION: This C++ code demonstrates how to use the `layout::ColumnMajor` type in CUTLASS to compute the offset of an element in a column-major matrix. The `layout` object is initialized with the leading dimension `lda`. The `operator()` then calculates the linear offset from the logical coordinate `(row, column)` as `row + lda * column`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nlayout::ColumnMajor layout(lda); \n\nint offset = layout({row, column});     // returns row  + lda * column\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target for 1D Conv Fprop Device Tests (CMake)\nDESCRIPTION: This CMake code adds an executable target named `cutlass_test_unit_conv1d_fprop_device_tensorop_sm90`. It disables source batching to control compiler memory usage and sets the batch size to 1. It then specifies the CUDA source files for different data types (s8, f16, tf32) and tensor operations used in the 1D convolution forward propagation tests on the SM90 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/fprop/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv1d_fprop_device_tensorop_sm90\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm90_conv1d_fprop_implicit_gemm_s8_s8_s32_tensorop_s32.cu\n  sm90_conv1d_fprop_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv1d_fprop_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv1d_fprop_implicit_gemm_tf32_tf32_f32_tensorop_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with Kernel Filters\nDESCRIPTION: Configures the build system to only compile kernels that match the supplied filter string `cutlass3x*`. Reduces build times by only including necessary kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_KERNELS=cutlass3x*\n```\n\n----------------------------------------\n\nTITLE: Get Threadblock Shape (C++)\nDESCRIPTION: This code snippet defines a static method `get_block_shape` within the `kernel::GemmUniversal` specialization. It returns a `dim3` representing the threadblock shape, which the device adapter uses to configure the kernel's execution. The returned `dim3` specifies the dimensions of the threadblock.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n// Returns a dim3 representing the threadblock shape.\nstatic dim3\nget_block_shape();\n```\n\n----------------------------------------\n\nTITLE: Numeric Conversion Example C++\nDESCRIPTION: This section describes numeric conversion operators defined in `numeric_conversion.h`. These operators are defined both for individual elements and arrays, allowing for optimized hardware support.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Simplified CLayout for F16 Accumulators\nDESCRIPTION: This code defines a simplified CLayout for F16 accumulators where each row of accumulators (m, :) is held by a single thread. The layout is simpler because of the direct mapping of threads to rows in the accumulator matrix. It uses `Shape` and `Stride` from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n  using CLayout = Layout<Shape <_8,_8>,\n                         Stride<_1,_8>>;\n```\n\n----------------------------------------\n\nTITLE: Choosing Performance Parameters for GEMM\nDESCRIPTION: This C++ code snippet shows the configuration of performance parameters for a block-scaled GEMM kernel using CUTLASS. It sets the `KernelMainloopPolicy` to `KernelTmaWarpSpecialized2SmMxf8f6f4Sm100`, the `MmaTileShape_MNK` to `cute::Shape<_256,_256,_128>`, and the `ClusterShape_MNK` to `cute::Shape<_4,_4,_1>`. These parameters are tailored to optimize performance for a GEMM problem, taking into account the 2SM MMA instruction and tile/cluster shapes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n  //////////////////////////////////////////\n  // 2. Choose Performance Parameters\n  //////////////////////////////////////////\n\n  // Tile and cluster shapes\n  // Collective MMA takes tile shape of the MMA operation as input\n  using KernelMainloopPolicy     = cutlass::gemm::KernelTmaWarpSpecialized2SmMxf8f6f4Sm100;\n  using MmaTileShape_MNK         = cute::Shape<_256,_256,_128>;\n  using ClusterShape_MNK         = cute::Shape<_4,_4,_1>;\n```\n\n----------------------------------------\n\nTITLE: Launching a GEMM kernel (CUTLASS < 3.0, simplified)\nDESCRIPTION: This is a simplified version of the previous example using helper methods defined in `HostTensor`. It uses `HostTensor::device_ref()` to get the `TensorRef` directly, reducing boilerplate code. It shows an alternative, more concise way to launch the GEMM kernel using the older CUTLASS API. Uses the `cutlass::gemm::device::Gemm` class.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_22\n\nLANGUAGE: c++\nCODE:\n```\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A({M, K});\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> B({K, N});\n  cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> C({M, N});\n\n  //\n  // Use the TensorRef returned by HostTensor::device_ref().\n  //\n\n  status = gemm_op({\n    {M, N, K},\n    A.device_ref(),            // TensorRef to A device tensor\n    B.device_ref(),            // TensorRef to B device tensor\n    C.device_ref(),            // TensorRef to C device tensor\n    C.device_ref(),            // TensorRef to D device tensor - may be the same as C\n    {alpha, beta}              // epilogue operation arguments\n  });\n```\n\n----------------------------------------\n\nTITLE: Run Grouped GEMM with CUTLASS and PyTorch\nDESCRIPTION: This code snippet generates a group of GEMM problems and executes them using both the CUTLASS Python interface and PyTorch. It then compares the results to verify correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nAs, Bs, Cs, Ds, = generate_problems(20)\n\nplan.run(As, Bs, Cs, Ds, print_module=True)\nDs_torch = [a @ b for a, b in zip(As, Bs)]\n\nfor d, d_torch in zip(Ds, Ds_torch):\n    assert torch.allclose(d, d_torch)\n```\n\n----------------------------------------\n\nTITLE: Define Shared Memory Layouts for gemm_tn in CuTe\nDESCRIPTION: This code snippet defines shared memory layouts for the `gemm_tn` operation using CuTe's layout and shape functionalities. It creates padded M-major and N-major layouts for shared memory access, designed to avoid bank conflicts and improve memory access patterns.  These layouts are essential for optimizing the data flow in GEMM kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_32\n\nLANGUAGE: cpp\nCODE:\n```\n// Define the smem layouts (static)\nauto sA = make_layout(make_shape (      bM,          bK),\n                    make_stride(Int<1>{}, bM+Int<1>{}));  // (m,k) -> smem_idx; padded m-major\nauto sB = make_layout(make_shape (      bN,          bK),\n                    make_stride(Int<1>{}, bN+Int<1>{}));  // (n,k) -> smem_idx; padded n-major\n```\n\n----------------------------------------\n\nTITLE: Define Convolution Fprop Kernel with Concrete Types\nDESCRIPTION: This code snippet defines an implicit GEMM convolution forward propagation (fprop) kernel using concrete data types, layouts, and tile shapes. It specifies 4-bit integer inputs and outputs (`cutlass::int4b_t`), NHWC tensor layouts (`cutlass::layout::TensorNHWC`), 32-bit integer accumulation (`int32_t`), and a linear combination epilogue with single-precision floating-point output (`float`). This configuration is tailored for a Turing architecture (Sm75) with specific tile sizes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\n/// Define an Implicit GEMM convolution forward propagation (fprop) kernel\nusing Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<\n  cutlass::int4b_t,                                    // data type of element a (mapped to activation for fprop)                         \n  cutlass::layout::TensorNHWC,                         // layout of element a (mapped to activation for fprop)\n  cutlass::int4b_t,                                    // data type of element b (mapped to filters for fprop)  \n  cutlass::layout::TensorNHWC,                         // layout of element b (mapped to filters for fprop)\n  int32_t,                                             // data type of element c (mapped to output for fprop)\n  cutlass::layout::TensorNHWC,                         // layout of element c (mapped to output for fprop)\n  int32_t,                                             // data type of internal accumulation\n  cutlass::arch::OpClassTensorOp,                      // opcode class tag\n  cutlass::arch::Sm75,                                 // target SM architecture\n  cutlass::gemm::GemmShape<128, 128, 128>,             // shape of threadblock tile\n  cutlass::gemm::GemmShape<64, 64, 128>,               // shape of warp-level GEMM tile\n  cutlass::gemm::GemmShape<8, 8, 32>,                  // shape of target math instruction\n  cutlass::epilogue::thread::LinearCombinationClamp<\n    int32_t,                                           // data type of output matrix\n    8,                                                 // The number of elements per vectorized\n                                                       // memory access. This becomes the vector width of\n                                                       // math instructions in the epilogue too.\n    int32_t,                                           // Data type of accumulator\n    float>;    ,                                       // epilogue operator \n  SwizzleThreadBlock,                                  // optional function to reorder threadblocks for locality\n  2,                                                   // number of pipeline stages in threadblock-scoped GEMM\n  cutlass::arch::OpMultiplyAddSaturate,                // math operation on data of element a and b\n  cutlass::conv::IteratorAlgorithm::kOptimized         // global memory iterator algorithm  \n>::Kernel\n```\n\n----------------------------------------\n\nTITLE: Auto Type Deduction C++\nDESCRIPTION: This code snippet demonstrates a scenario where the `auto` keyword is used for type deduction. It's important to avoid `auto` if the return type is known to prevent potential loss-of-precision errors, especially in mixed-precision computations. Note that the use of auto here requires the different branches to return different types, demonstrating the limitation of aggregate initialization.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_14\n\nLANGUAGE: c++\nCODE:\n```\nauto foo(std::span<const float> input) {\n  // ... code  ...\n\n  if constexpr (some_condition) {\n    return foo_result{val, err, ok};\n  }\n  else {\n    return bar_result{val, err, ok};\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Define CUTLASS Epilogue Visitor Functor\nDESCRIPTION: This snippet defines the epilogue visitor as a python function `example_epilogue` that takes several tensor inputs (accum, alpha, C, beta, aux, bias) and performs a series of operations, including relu activation, to produce two output tensors, D and F. It constructs example tensors for each input, using `FakeTensor` for the accumulator and `torch.Tensor` for the others, and then traces the function using `cutlass.epilogue.trace` to create the epilogue visitor object.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/04_epilogue_visitor.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define epilogue visitor\ndef example_epilogue(accum, alpha, C, beta, aux, bias):\n    F = alpha * accum + (beta * C + aux)\n    E = relu(F + 1) + bias\n    D = E + F\n    return D, F\n\n# Construct inputs and outputs\nalpha = 0.5\nbeta = 0.5\naux = torch.ceil(torch.empty(size=(m, n), dtype=type_C, device=\"cuda\").uniform_(scope_min, scope_max))\nbias = torch.ceil(torch.empty(size=(m, 1), dtype=type_C, device=\"cuda\").uniform_(scope_min, scope_max))\ntensor_F = torch.zeros_like(tensor_D)\nexamples_tensors = {\n    \"accum\": FakeTensor(element=torch.float32, shape=(m, n), layout_tag=cutlass.LayoutType.RowMajor),\n    \"alpha\": alpha,\n    \"C\": tensor_C,\n    \"beta\": beta,\n    \"aux\": aux,\n    \"bias\": bias,\n    \"D\": tensor_D,\n    \"F\": tensor_F\n}\n\n# Trace the epilogue visitor\nepilogue_visitor = cutlass.epilogue.trace(example_epilogue, examples_tensors)\n```\n\n----------------------------------------\n\nTITLE: GEMM with ReLU Activation\nDESCRIPTION: This example demonstrates a GEMM operation with ReLU activation function using float64 data type and TensorOp. It shows how to add bias and apply ReLU activation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 8 8 4 -ta float64 -tb float64 -tc float64 -tacc float64 -m multiply_add -op TensorOp -b 32 32 16 -s 4 -w 2 2 1 -cc 80 -la ColumnMajor -aa 1 -lb RowMajor -ab 1 -lc RowMajor -ac 1 -te float64 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 1 -bias -activ relu\n```\n\n----------------------------------------\n\nTITLE: Sweeping GEMM K Dimension Range (CUTLASS)\nDESCRIPTION: This command sweeps over a range of values for the K dimension of the GEMM operation from 8 to 4096, incrementing by 8. The m and n dimensions are fixed at 4352 and 4096, respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sgemm_128x128_nn --m=4352 --n=4096 --k=8:4096:8\n```\n\n----------------------------------------\n\nTITLE: Building Subset Tensor Core GEMM Kernels with CMake and Make\nDESCRIPTION: This set of commands builds a subset of Tensor Core GEMM kernels. It uses CMake to configure the build, specifying the target architectures and a wildcard-based filter for kernel names. The subsequent `make` command compiles the selected kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*gemm_f16_*_nt_align8\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Zipped Divide for Tensor Partitioning (CuTe)\nDESCRIPTION: This code segment illustrates the zipped_divide operation, a core part of the projective `local_tile` interface.  It divides the full matrix mA according to the tile sizes defined in `select<0,2>(cta_tiler)`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\n// ((BLK_M,BLK_K),(m,k))\nTensor gA_mk = zipped_divide(mA, select<0,2>(cta_tiler));\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Dense Kernel Dispatch Policy\nDESCRIPTION: This dispatch policy specifies the configuration for a dense matrix multiplication kernel using TMA (Tensor Memory Accelerator) warp specialization. It leverages a single SM (Streaming Multiprocessor) and is tailored for SM100 architecture. The policy is applicable to data types {mx_float4_t, mx_float6_t, mx_float8_t} multiplied by either {mx_float4_t, mx_float6_t} or mx_float8_t.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n`KernelTmaWarpSpecialized1SmMxf8f6f4Sm100`\n```\n\n----------------------------------------\n\nTITLE: Setting Iterator Algorithm in CUTLASS Python\nDESCRIPTION: This snippet demonstrates how to explicitly set the iterator algorithm to \"analytic\" in the CUTLASS Python interface. It then runs a convolution operation using the specified algorithm and asserts that the output matches the expected output from a PyTorch implementation. This ensures functional correctness while potentially exploring different performance characteristics.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nplan.iterator_algorithm = \"analytic\"\nplan.run(input, weight, tensor_C, output, stride, padding, dilation, alpha, beta, print_module=print_module)\nassert torch.equal(output_torch, output)\n```\n\n----------------------------------------\n\nTITLE: ALayout for NT Transpose (T8, V4) -> (m, k)\nDESCRIPTION: This code defines the ALayout for the NT transpose case, mapping 8 threads each owning 4 elements to (m, k) coordinates. It involves sub-strides along the M mode due to the arrangement of thread IDs. It uses `Shape` and `Stride` from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n  // (T8,V4) -> (m,k)\n  using ALayout = Layout<Shape <Shape <_4,_2>,_4>,\n                         Stride<Stride<_8,_4>,_1>>;\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 1SM, Mxf8f6f4)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following a sparse matrix multiplication, using TMA (Tensor Memory Accelerator) on a single SM (Streaming Multiprocessor). It is designed specifically for narrow precision using the mxf8f6f4 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized1SmMxf8f6f4`\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Profiler: Tensor Core Convolution Example\nDESCRIPTION: This bash snippet demonstrates profiling forward propagation convolution kernels on Tensor Cores. It specifies the use of `tensorop*fprop` kernels to analyze their performance with chosen input dimensions and filter characteristics.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=tensorop*fprop  --verification-providers=device --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Defining CLayout for GMMA (T128,V4) -> (M64,N8) in C++\nDESCRIPTION: This code snippet defines the CLayout for a GMMA operation where a thread block of size 128x4 maps to a matrix of size 64x8. It specifies the shape and stride of the layout, showing how threads and values are arranged in memory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n// (T128,V4) -> (M64,N8)\nusing CLayout = Layout<Shape <Shape <  _4, ...>, Shape < _2, ...>>,\n                       Stride<Stride<_128, ...>, Stride<_64, ...>>>;\n```\n\n----------------------------------------\n\nTITLE: Building the Example - CUDA\nDESCRIPTION: This snippet builds the CUTLASS example `09_turing_tensorop_conv2dfprop` for Turing architecture (compute capability 7.5) using CMake and Make. It creates a build directory, configures CMake with the appropriate architecture flag, and then compiles the example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir build && cd build\n\n$ cmake .. -DCUTLASS_NVCC_ARCHS=75\n```\n\n----------------------------------------\n\nTITLE: Warp-level Mma Concept in CUTLASS\nDESCRIPTION: This code defines the concept for warp-level matrix multiply-accumulate (Mma) operations in CUTLASS. It specifies the required types and methods, including the shape of the operation, data types and layouts of operands, iterators, fragments, and the matrix multiply-accumulate operator.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nstruct Mma {\n  /// Shape of warp-level matrix operation (concept: GemmShape)\n  struct Shape;\n\n  /// Data type of multiplicand A (concept: numeric type)\n  struct ElementA;\n\n  /// Layout of multiplicand A (concept: Layout)\n  struct LayoutA;\n\n  /// Data type of multiplicand B (concept: numeric type)\n  struct ElementB;\n\n  /// Layout of multiplicand B (concept: Layout)\n  struct LayoutB;\n\n  /// Data type of accumulator matrix C (concept: numeric type)\n  struct ElementC;\n\n  /// Layout of accumulator matrix C (concept: Layout)\n  struct LayoutC;\n\n  /// Iterator of A operand in shared memory - satisfies: ReadableRandomAccessTileIteratorConcept\n  struct IteratorA;\n\n  /// Fragment object loaded from IteratorA (concept: Array<ElementA, ..>)\n  struct FragmentA;\n\n  /// Iterator of B operand in shared memory - satisfies: ReadableRandomAccessTileIteratorConcept\n  struct IteratorB;\n\n  /// Fragment object loaded from IteratorB (concept: Array<ElementB, ..>)\n  struct FragmentB;\n\n  /// Iterator of C operand in shared memory - \n  ///     satisfies: ReadableRandomAccessTileIteratorConcept | WriteableRandomAccessTileIteratorConcept\n  struct IteratorC;\n\n  /// Fragment object loaded from IteratorC (concept: Array<ElementC, ..>)\n  struct FragmentC;\n\n  /// Indicates class of matrix operator (arch::OpClassSimt or arch::OpClassTensorOp)\n  struct OperatorClass;\n\n  //\n  // Methods\n  //\n\n  /// Computes a matrix multiply-accumulate\n  CUTLASS_DEVICE\n  void operator()(\n    FragmentC &D, \n    IteratorA A, \n    IteratorB B, \n    FragmentC const &C);\n};\n```\n\n----------------------------------------\n\nTITLE: Creating Owning Tensors in C++\nDESCRIPTION: This code demonstrates creating owning `Tensor`s, which behave like `std::array` and manage their own memory. The `make_tensor<T>` function is used to create a tensor of type `T` with a specified `Layout`.  Owning `Tensor`s must have static shapes and strides because CuTe does not dynamically allocate memory within CUDA kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n// Register memory (static layouts only)\nTensor rmem_4x8_col = make_tensor<float>(Shape<_4,_8>{});\nTensor rmem_4x8_row = make_tensor<float>(Shape<_4,_8>{},\n                                         LayoutRight{});\nTensor rmem_4x8_pad = make_tensor<float>(Shape <_4, _8>{},\n                                         Stride<_32,_2>{});\nTensor rmem_4x8_like = make_tensor_like(rmem_4x8_pad);\n```\n\n----------------------------------------\n\nTITLE: Iterating and Adding Example Subdirectories (CMake)\nDESCRIPTION: Iterates through a list of example directories and uses `add_subdirectory` to include them in the build process. Each example directory will be processed using its own CMakeLists.txt file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(EXAMPLE\n  00_basic_gemm\n  01_cutlass_utilities\n  02_dump_reg_shmem\n  03_visualize_layout\n  04_tile_iterator\n  05_batched_gemm\n  06_splitK_gemm\n  07_volta_tensorop_gemm\n  08_turing_tensorop_gemm\n  09_turing_tensorop_conv2dfprop\n  10_planar_complex\n  11_planar_complex_array\n  12_gemm_bias_relu\n  13_two_tensor_op_fusion\n  14_ampere_tf32_tensorop_gemm\n  15_ampere_sparse_tensorop_gemm\n  16_ampere_tensorop_conv2dfprop\n  17_fprop_per_channel_bias\n  18_ampere_fp64_tensorop_affine2_gemm\n  19_tensorop_canonical\n  20_simt_canonical\n  21_quaternion_gemm\n  22_quaternion_conv\n  23_ampere_gemm_operand_reduction_fusion\n  24_gemm_grouped\n  25_ampere_fprop_mainloop_fusion\n  26_ampere_wgrad_mainloop_fusion\n  27_ampere_3xtf32_fast_accurate_tensorop_gemm\n  28_ampere_3xtf32_fast_accurate_tensorop_fprop\n  29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\n  30_wgrad_split_k\n  31_basic_syrk\n  32_basic_trmm\n  33_ampere_3xtf32_tensorop_symm\n  34_transposed_conv2d\n  35_gemm_softmax\n  36_gather_scatter_fusion\n  37_gemm_layernorm_gemm_fusion\n  38_syr2k_grouped\n  cute\n  39_gemm_permute\n  41_fused_multi_head_attention\n  42_ampere_tensorop_group_conv\n  43_ell_block_sparse_gemm\n  45_dual_gemm\n  46_depthwise_simt_conv2dfprop\n  47_ampere_gemm_universal_streamk\n  48_hopper_warp_specialized_gemm\n  49_hopper_gemm_with_collective_builder\n  50_hopper_gemm_with_epilogue_swizzle\n  51_hopper_gett\n  52_hopper_gather_scatter_fusion\n  53_hopper_gemm_permute\n  54_hopper_fp8_warp_specialized_gemm\n  55_hopper_mixed_dtype_gemm\n  56_hopper_ptr_array_batched_gemm\n  57_hopper_grouped_gemm\n  58_ada_fp8_gemm\n  59_ampere_gather_scatter_conv\n  61_hopper_gemm_with_topk_and_softmax\n  62_hopper_sparse_gemm\n  63_hopper_gemm_with_weight_prefetch\n  64_ada_fp8_gemm_grouped\n  65_distributed_gemm\n  67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling\n  68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling\n  69_hopper_mixed_dtype_grouped_gemm\n  70_blackwell_gemm                             \n  71_blackwell_gemm_with_collective_builder     \n  72_blackwell_narrow_precision_gemm            \n  73_blackwell_gemm_preferred_cluster           \n  74_blackwell_gemm_streamk                     \n  75_blackwell_grouped_gemm                     \n  76_blackwell_conv                             \n  77_blackwell_fmha                             \n  78_blackwell_emulated_bf16x9_gemm             \n  79_blackwell_geforce_gemm             \n  80_blackwell_geforce_sparse_gemm\n  81_blackwell_gemm_blockwise \n  82_blackwell_distributed_gemm\n  83_blackwell_sparse_gemm\n  84_blackwell_narrow_precision_sparse_gemm\n  )\n\n  add_subdirectory(${EXAMPLE})\n\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Partitioning Global and Shared Memory Tensors C++\nDESCRIPTION: This code partitions global memory tensors (`gA`, `gB`) and shared memory tensors (`sA`, `sB`) using `local_partition` and the defined thread layouts (`tA`, `tB`).  The resulting tensors `tAgA`, `tAsA`, `tBgB`, and `tBsB` represent the sub-tensors owned by each thread. Assertions verify the dimension consistency between the partitioned global and shared memory tensors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n  Tensor tAgA = local_partition(gA, tA, threadIdx.x);    // (THR_M,THR_K,k)\n  Tensor tAsA = local_partition(sA, tA, threadIdx.x);    // (THR_M,THR_K)\n\n  Tensor tBgB = local_partition(gB, tB, threadIdx.x);    // (THR_N,THR_K,k)\n  Tensor tBsB = local_partition(sB, tB, threadIdx.x);    // (THR_N,THR_K)\n\n  CUTE_STATIC_ASSERT_V(size<0>(tAgA) == size<0>(tAsA));  // THR_M\n  CUTE_STATIC_ASSERT_V(size<1>(tAgA) == size<1>(tAsA));  // THR_K\n  CUTE_STATIC_ASSERT_V(size<0>(tBgB) == size<0>(tBsB));  // THR_N\n  CUTE_STATIC_ASSERT_V(size<1>(tBgB) == size<1>(tBsB));  // THR_K\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 2SM, Mxf4)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following a sparse matrix multiplication, using TMA (Tensor Memory Accelerator) across two SMs (Streaming Multiprocessors). It is designed specifically for narrow precision using the mxf4 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized2SmMxf4`\n```\n\n----------------------------------------\n\nTITLE: Changing Operation Mode to SIMT in CUTLASS\nDESCRIPTION: This snippet shows how to change the operation mode of a CUTLASS GEMM plan to use SIMT kernels instead of Tensor Core operations. It initializes a new output tensor, sets the `opclass` property of the plan to `cutlass.OpcodeClass.Simt`, and then runs the GEMM using the specified tensors and scalars.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntensor_D_simt = np.zeros(tensor_C.shape).astype(type_D)\nplan.opclass = cutlass.OpcodeClass.Simt\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D_simt, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Dense Kernel Dispatch Policy (2SM)\nDESCRIPTION: This dispatch policy specifies the configuration for a dense matrix multiplication kernel using TMA warp specialization across two SMs (Streaming Multiprocessors) and is tailored for the SM100 architecture.  The policy is applicable to data types {mx_float4_t, mx_float6_t, mx_float8_t} multiplied by either {mx_float4_t, mx_float6_t} or mx_float8_t.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n`KernelTmaWarpSpecialized2SmMxf8f6f4Sm100`\n```\n\n----------------------------------------\n\nTITLE: Composition Over Inheritance C++\nDESCRIPTION: This code snippet illustrates the preferred approach of using composition over inheritance for code reuse. Instead of `PipelineB` inheriting from `PipelineA`, both classes use a `PipelineImpl` object. This design avoids the potential issues of incorrect assumptions about the 'is-a' relationship and promotes compile-time polymorphism without runtime polymorphism.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_17\n\nLANGUAGE: c++\nCODE:\n```\nnamespace detail {\n\n// Implementation class; not for users\nclass PipelineImpl {\npublic:\n  PipelineImpl(Arg0 arg0, Arg1 arg1)\n    : arg0_(arg0), arg1_(arg1)\n  {}\n\n  void producer_acquire(uint32_t stage, uint32_t phase, uint32_t skip_wait) {\n    // ... implementation ...\n  }\n\n  void consumer_release(uint32_t stage, uint32_t skip) {\n    // ... implementation ...\n  }\n\nprivate:\n  Arg0 arg0_;\n  Arg1 arg1_;\n};\n\n} // namespace detail\n\nclass PipelineA {\npublic:\n  PipelineA(Arg0 arg0, Arg1 arg1) :\n    impl_(arg0, arg1)\n  {}\n\n  void producer_acquire(uint32_t stage, uint32_t phase, uint32_t skip_wait) {\n    impl_.producer_acquire(stage, phase, skip_wait);\n  }\n\n  void consumer_release(uint32_t stage, uint32_t skip) {\n    impl_.consumer_release(stage, skip);\n  }\n\nprivate:\n  detail::PipelineImpl impl_;\n};\n\n// A second kind of pipeline.\n// Note that this does NOT inherit from PipelineB!\n// The two pipeline classes have the same compile-time interface\n// (for compile-time polymorphism), but do not belong in an \n// inheritance hierarchy (as would imply run-time polymorphism).\nclass PipelineB {\npublic:\n  PipelineB(Arg0 arg0, Arg1 arg1, Arg2 arg2) :\n    impl_(arg0, arg1), otherTwo_(arg2)\n  {}\n\n  void producer_acquire(uint32_t stage, uint32_t phase, uint32_t skip_wait) {\n    impl_.producer_acquire(stage, phase, skip_wait);\n  }\n\n  void consumer_release(uint32_t stage, uint32_t skip) {\n    // this class doesn't actually use impl_ here\n    otherTwo_.other_action(stage, skip);\n    // ... some other code not using impl_ ...\n  }\n\nprivate:\n  detail::PipelineImpl impl_;\n  OtherTwo otherTwo_;\n  // ... other member data ...\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Data Types for MXFP8 GEMM\nDESCRIPTION: This snippet defines the data types for the A and B matrices when using the MXFP8 data type. It uses the `cutlass::mx_float8_t` template with `cutlass::float_e4m3_t` as the underlying element type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_41\n\nLANGUAGE: c++\nCODE:\n```\nusing ElementA = cutlass::mx_float8_t<cutlass::float_e4m3_t>;\nusing ElementB = cutlass::mx_float8_t<cutlass::float_e4m3_t>;\n```\n\n----------------------------------------\n\nTITLE: GEMM Int8 Example\nDESCRIPTION: This example demonstrates a GEMM operation using int8 inputs and int32 accumulation. Key parameters include input dimensions, tensor layouts, block sizes, swizzle functions, and alpha/beta values.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 32 -ta int8 -tb int8 -tc int8 -tacc int32 -m multiply_add -op TensorOp -b 128 128 128 -s 3 -w 2 2 1 -cc 80 -la RowMajor -aa 16 -lb ColumnMajor -ab 16 -lc RowMajor -ac 16 -te float32 -ep FastLinearCombinationClamp -sw IdentitySwizzle2 -p 512 512 512 -alpha 1.0 -beta 0.0 -gm Gemm -k 1\n```\n\n----------------------------------------\n\nTITLE: Performance Optimization for Fixed GEMM Shape (CUTLASS)\nDESCRIPTION: This command optimizes kernel performance for a specific GEMM problem size (m, n, k = 6144).  It searches for the best configuration within the constraints of the fixed shape while still exploring various kernel parameters.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncutlass_profiler --kernels=*gemm* --enable-best-kernel-for-fixed-shape --m=6144 --n=6144 --k=6144 --sort-results-flops-per-sec\n```\n\n----------------------------------------\n\nTITLE: HostTensor with Interleaved Layout\nDESCRIPTION: This example shows how to use `HostTensor` with an interleaved layout. It allocates a tensor with `ColumnMajorInterleaved` layout and initializes its elements.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nint rows = 4;\nint columns = 3;\n\ncutlass::HostTensor<float, cutlass::layout::ColumnMajorInterleaved<4>> tensor({rows, columns});\n\nfor (int i = 0; i < rows; ++i) {\n  for (int j = 0; j < columns; ++j) {\n\n    // Write the element at location {i, j} in host memory\n    tensor.host_ref().at({i, j}) = float(i) * 1.5f - float(j) * 2.25f;\n  } \n}\n\nstd::cout << tensor.host_view() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Exhaustive Kernel Search with GFLOPs Sorting (CUTLASS)\nDESCRIPTION: This command performs an exhaustive search to find the best-performing GEMM kernel and sorts the results based on GFLOPs per second. This is useful for identifying the most efficient kernels from a broad range of options.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncutlass_profiler --kernels=*gemm* --enable-kernel-performance-search --sort-results-flops-per-sec\n```\n\n----------------------------------------\n\nTITLE: Static Assertions on Tensor C++\nDESCRIPTION: This code snippet demonstrates how to use `CUTE_STATIC_ASSERT_V` to verify properties of a tensor at compile time, such as rank and whether a dimension has a static size. This allows for ensuring correct tensor configurations before runtime.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nCUTE_STATIC_ASSERT_V(rank(gmem) == Int<2>{});\nCUTE_STATIC_ASSERT_V(is_static<decltype(shape<0>(gmem))>{});\n```\n\n----------------------------------------\n\nTITLE: Building One CUDA Core GEMM Kernel with CMake and Make\nDESCRIPTION: These commands build a specific SGEMM CUDA core kernel using CMake and Make. CMake configures the build environment with target architectures and a specific kernel name. The `make` command then compiles the designated kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sgemm_128x128_8x2_nn_align1\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Tiling Tensor Operations C++\nDESCRIPTION: This snippet shows the different layout algebra operations that can be applied to a `Tensor` object, including `composition`, `logical_divide`, `zipped_divide`, `tiled_divide`, and `flat_divide`. These operations allow for arbitrary subtensors to be factored out of `Tensor`s, useful for tiling.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ncomposition(Tensor, Tiler)\nlogical_divide(Tensor, Tiler)\n zipped_divide(Tensor, Tiler)\n  tiled_divide(Tensor, Tiler)\n   flat_divide(Tensor, Tiler)\n```\n\n----------------------------------------\n\nTITLE: Defining Readable Random Access Contiguous Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept that supports reading tiles from contiguous memory with random access and both a tile offset and pointer offset. It inherits from `ReadableRandomAccessTileIteratorConcept` and `ReadableContiguousTileIteratorConcept`. It provides a `load` method to load a fragment given a `TensorCoord` tile offset and an `Index` pointer offset.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_10\n\nLANGUAGE: c++\nCODE:\n```\n/// Loads a fragment with a logical coordinate offset in units of whole tiles.\nstruct ReadableRandomAccessContiguousTileIteratorConcept : \n  public ReadableRandomAccessTileIteratorConcept, \n  ReadableContiguousTileIteratorConcept {\n\n  /// Loads a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE\n  void load(\n    Fragment &frag,                             ///< fragment to load from the tensor\n    TensorCoord const &tile_offset,             ///< loads a tile with a logical offset in units of whole tiles\n    Index pointer_offset);                      ///< loads a tile with a logical offset AND a pointer offset\n};\n```\n\n----------------------------------------\n\nTITLE: Installing CUTLASS Kernel List File\nDESCRIPTION: This snippet installs the generated kernel list file (`CUTLASS_LIBRARY_GENERATED_KERNEL_LIST_FILE`) to the specified destination directory (`${CMAKE_INSTALL_INFODIR}/cutlass/`). The `install` command copies the specified file to the designated destination during the installation process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(\n  FILES ${CUTLASS_LIBRARY_GENERATED_KERNEL_LIST_FILE}\n  DESTINATION ${CMAKE_INSTALL_INFODIR}/cutlass/\n  )\n```\n\n----------------------------------------\n\nTITLE: Packed Layout Example\nDESCRIPTION: This C++ code demonstrates the usage of the `::packed()` static method to construct a layout object for a densely packed tensor.  It initializes `extent` using `make_Coord()` and a coordinate. The layout object is created with `ArbitraryLayout::packed(extent)`.  The `offset` is calculated from the given coordinate using the constructed `layout` object.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ntypename ArbitraryLayout::TensorCoord extent = make_Coord(...);\ntypename ArbitraryLayout::TensorCoord coord;\n\nArbitraryLayout layout = ArbitraryLayout::packed(extent);\n\nint offset = layout({coord});\n```\n\n----------------------------------------\n\nTITLE: Schmoo Over Accumulator Types\nDESCRIPTION: This command profiles the GEMM operation while sweeping over different accumulator data types (f16 and f32). This allows evaluating the performance impact of using different precision accumulators.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --accumulator-type=f16,f32\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Installation\nDESCRIPTION: This snippet installs the CUTLASS Python interface using pip. It is intended for use in a Colab environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!#pip install nvidia-cutlass\n```\n\n----------------------------------------\n\nTITLE: GEMM F32 SplitK Parallel Example\nDESCRIPTION: This example demonstrates a GEMM operation using float32 data type with split-K parallelism. It specifies parameters like input dimensions, tensor layouts, block size, split-K parameter, and alpha/beta values. The GemmSplitKParallel kernel is used for parallel split-K execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 4 -lb ColumnMajor -ab 4 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm GemmSplitKParallel -k 2\n```\n\n----------------------------------------\n\nTITLE: Populating Predicate Tensors by Comparing Coordinates\nDESCRIPTION: This code snippet shows how to populate predicate tensors by comparing the coordinates of reference tensors with maximum coordinate bounds. It iterates through the dimensions of the partitioned identity tensors, comparing against the maximum coordinates and storing the boolean result in predicate tensors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nTensor cA   = make_identity_tensor(make_shape(size<0>(sA), size<1>(sA)));  // (BLK_M,BLK_K) -> (blk_m,blk_k)\nTensor tAcA = local_partition(cA, tA, thread_idx);\n\nTensor cB   = make_identity_tensor(make_shape(size<0>(sB), size<1>(sB)));  // (BLK_N,BLK_K) -> (blk_n,blk_k)\nTensor tBcB = local_partition(cB, tB, thread_idx);\n\n// Populate\nCUTE_UNROLL\nfor (int m = 0; m < size<0>(tApA); ++m) {\n  tApA(m,0) = get<0>(tAcA(m,0)) < m_max_coord;\n}\nCUTE_UNROLL\nfor (int n = 0; n < size<0>(tBpB); ++n) {\n  tBpB(n,0) = get<0>(tBcB(n,0)) < n_max_coord;\n}\n```\n\n----------------------------------------\n\nTITLE: Convolution Analytic Function\nDESCRIPTION: This code snippet shows the analytic function that describes the forward convolutional layer computation. It calculates the output tensor y as a convolution of the input tensor x and the filter tensor w, taking into account stride and padding.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ny[n, p, q, k] = sum_c(sum_r(sum_s( x[n, f(p, r), g(q, s), c] * w[k, r, s, c] )))\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies Based on CUDA Architecture (sm89+)\nDESCRIPTION: This snippet conditionally adds dependencies to the convolution test targets based on whether the maximum CUDA architecture is greater than or equal to 89 (sm89). If the condition is met, dependencies to the tensorop_f8_sm89 implementations are added.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 89)\n\n  add_dependencies(\n    cutlass_test_unit_conv_device\n    cutlass_test_unit_conv_device_tensorop_f8_sm89\n  )\n\n  add_dependencies(\n    test_unit_conv_device\n    test_unit_conv_device_tensorop_f8_sm89\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Partitioning Tensors Across CTAs (CuTe)\nDESCRIPTION: This code partitions the full tensors (mA, mB, mC) across the CTAs. It calculates the CTA coordinate using `blockIdx` and then extracts the appropriate blocks for the current threadblock using `local_tile` function.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\n  // Get the appropriate blocks for this threadblock\n  auto cta_coord = make_coord(blockIdx.x, blockIdx.y, _);              // (m,n,k)\n  Tensor gA = local_tile(mA, cta_tiler, cta_coord, Step<_1, X,_1>{});  // (BLK_M,BLK_K,k)\n  Tensor gB = local_tile(mB, cta_tiler, cta_coord, Step< X,_1,_1>{});  // (BLK_N,BLK_K,k)\n  Tensor gC = local_tile(mC, cta_tiler, cta_coord, Step<_1,_1, X>{});  // (BLK_M,BLK_N)\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Compute Capability\nDESCRIPTION: This command uses `nvidia-smi` to query the name and compute capability of the GPU(s) installed in the system. The output helps to verify that the GPUs are Hopper architecture (compute capability 9.0) which is required for the distributed GEMM example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/REQUIREMENTS.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi --query-gpu=name,compute_cap --format=csv\n```\n\n----------------------------------------\n\nTITLE: Define SMEM Layouts (GEMM_TN) C++\nDESCRIPTION: This code defines the shared memory layouts for A and B tensors in the `gemm_tn` kernel. It creates K-major layouts using `make_layout`, `make_shape`, and `LayoutRight{}`. These layouts determine shared memory access patterns.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n  // Define the smem layouts (static)\n  auto sA = make_layout(make_shape(bM,bK), LayoutRight{});   // (m,k) -> smem_idx; k-major\n  auto sB = make_layout(make_shape(bN,bK), LayoutRight{});   // (n,k) -> smem_idx; k-major\n```\n\n----------------------------------------\n\nTITLE: GEMM Setup: Input Tensors and Parameters\nDESCRIPTION: This snippet imports necessary packages like numpy and cutlass, and constructs the input (A, B, C) and output (D) tensors for the GEMM operation. It also defines parameters like alpha and beta and sets a random seed for reproducibility.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport cutlass\n\n# This controls whether ther C++ GEMM declaration will be printed at each step. Set to `false` to\n# omit this information.\nprint_module = True\n\nm = 256\nn = m\nk = m\n\ntype_A = np.float16\ntype_B = np.float16\ntype_C = np.float16\ntype_D = np.float16\n\nnp.random.seed(1234)\nscope_min = -4\nscope_max = 4\ntensor_A = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, k)).astype(type_A))\ntensor_B = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(k, n)).astype(type_B))\ntensor_C = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, n)).astype(type_C))\n\nalpha = np.float16(1.)\nbeta = np.float16(0.)\n\ntensor_D = np.zeros(tensor_C.shape).astype(type_D)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for Blackwell Architecture (CMake)\nDESCRIPTION: This CMake snippet conditionally adds example executables based on the `CUTLASS_NVCC_ARCHS` variable. It checks if the target architecture matches '120a' (Blackwell) and then adds specific example executables using `cutlass_example_add_executable`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/80_blackwell_geforce_sparse_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 120a)\ncutlass_example_add_executable(\n  80a_blackwell_geforce_mxfp8_bf16_sparse_gemm\n  80a_blackwell_geforce_mxfp8_bf16_sparse_gemm.cu\n)  \n\ncutlass_example_add_executable(\n  80b_blackwell_geforce_nvfp4_nvfp4_sparse_gemm\n  80b_blackwell_geforce_nvfp4_nvfp4_sparse_gemm.cu\n)  \n\nendif()\n```\n\n----------------------------------------\n\nTITLE: GEMM F64 Example\nDESCRIPTION: This example demonstrates a GEMM (General Matrix Multiplication) operation using float64 data type in CUTLASS. It specifies various parameters like input dimensions, tensor layouts, block size, split-K parameter, and alpha/beta values.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 8 8 4 -ta float64 -tb float64 -tc float64 -tacc float64 -m multiply_add -op TensorOp -b 32 32 16 -s 4 -w 2 2 1 -cc 80 -la ColumnMajor -aa 1 -lb RowMajor -ab 1 -lc RowMajor -ac 1 -te float64 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 1\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 1SM)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following matrix multiplication using the TMA (Tensor Memory Accelerator) on a single SM (Streaming Multiprocessor). It's applicable to both dense and sparse matrices, and supports legacy and narrow precision configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized1Sm`\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target for 3D Conv Fprop Device Tests (CMake)\nDESCRIPTION: This CMake code adds an executable target named `cutlass_test_unit_conv3d_fprop_device_tensorop_sm90`. It disables source batching to control compiler memory usage and sets the batch size to 1. It then specifies the CUDA source files for different data types (s8, f16, tf32) and tensor operations used in the 3D convolution forward propagation tests on the SM90 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/fprop/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv3d_fprop_device_tensorop_sm90\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm90_conv3d_fprop_implicit_gemm_s8_s8_s32_tensorop_s32.cu\n  sm90_conv3d_fprop_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv3d_fprop_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv3d_fprop_implicit_gemm_tf32_tf32_f32_tensorop_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: ArithmeticTupleIterator Example (C++)\nDESCRIPTION: This code snippet shows how to create and use an ArithmeticTupleIterator. It demonstrates how to create an iterator with initial values, offset it by another tuple, and print the dereferenced value.  The purpose is to allow working with TMA coordinate offsets.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0z_tma_tensors.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nArithmeticTupleIterator citer_1 = make_inttuple_iter(42, Int<2>{}, Int<7>{});\nArithmeticTupleIterator citer_2 = citer_1 + make_tuple(Int<0>{}, 5, Int<2>{});\nprint(*citer_2);\n```\n\n----------------------------------------\n\nTITLE: Conv2d F32 Wgrad Optimized Example\nDESCRIPTION: This example showcases a Conv2d weight gradient (Wgrad) operation with optimized implicit GEMM. It specifies parameters such as tensor layouts, block sizes, swizzle functions, padding, stride, and dilation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 1 -lb TensorNHWC -ab 1 -lc TensorNHWC -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -co wgrad -st Strided -ia optimized -sm Serial -k 1 -nhwc 1 8 8 1 -krsc 1 3 3 1 -pad 1 1 1 1 -stride 1 1 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: Configure Convolution kernels with FP32 accumulation and FP16 input\nDESCRIPTION: This CMake command configures CUTLASS to compile all forward propagation convolution kernels with FP32 accumulation and FP16 input targeting NVIDIA Ampere's 16816 Tensor Core operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_35\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='80' -DCUTLASS_LIBRARY_KERNELS=s16816fprop_*_f16\n```\n\n----------------------------------------\n\nTITLE: AlignedBuffer Shared Memory Allocation Example in CUTLASS\nDESCRIPTION: This example demonstrates how to define a shared memory allocation with guaranteed alignment using `AlignedBuffer` in CUTLASS. The `data()` method provides a pointer with the specified alignment. Requires the `cutlass` library and CUDA.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nint const kN = 32;\nint const kAlignment = 16;                  // alignment in bytes\n\n// Define a shared memory allocation in device code\n__shared__ AlignedBuffer<complex<half_t>, kN, kAlignment> matrix_tile;\n\ncomplex<half_t> *ptr = matrix_tile.data();  // ptr is guaranteed to have 128b (16 Byte) alignment\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable Example in CMake\nDESCRIPTION: This CMake code snippet adds an executable example named `35_gemm_softmax` using the CUTLASS helper macro `cutlass_example_add_executable`. The source file for the executable is `gemm_softmax.cu`. This macro handles the necessary compilation and linking steps to create the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/35_gemm_softmax/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  35_gemm_softmax\n  gemm_softmax.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Turing Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Turing GPU architecture using CMake. The `CUTLASS_NVCC_ARCHS` flag specifies the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=75               # compiles for NVIDIA Turing GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Display GEMM Profiler Arguments\nDESCRIPTION: This command displays the complete set of arguments available for the GEMM operation within the CUTLASS profiler. It is useful for understanding the available configuration options for GEMM kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --operation=gemm --help\n```\n\n----------------------------------------\n\nTITLE: Configure Convolution kernels for multiple NVIDIA Architectures\nDESCRIPTION: This CMake command configures CUTLASS to compile all forward propagation (fprop) convolution kernels targeting CUDA Cores for multiple NVIDIA architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_34\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='50;60;61;70;75;80' -DCUTLASS_LIBRARY_KERNELS=sfprop\n```\n\n----------------------------------------\n\nTITLE: Defining Random Access Tile Iterator Concept in C++\nDESCRIPTION: Defines a tile iterator concept that supports random access to tiles in contiguous memory based on the logical coordinate system of the underlying tensor. It inherits from `BidirectionalTileIteratorConcept` and `ContiguousMemoryTileIterator`. It includes `Layout`, `TensorRef`, and `TensorCoord` types and provides methods (`add_tile_offset`, `+=`, `-=`) to advance the iterator by tile offsets.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\n/// Tile iterator offering random access to tiles in contiguous memory.\nstruct RandomAccessTileIteratorConcept : \n  public BidirectionalTileIteratorConcept, \n  public ContiguousMemoryTileIterator {\n\n  using Layout;           ///< Layout object mapping \n  using TensorRef;        ///< Tensor Reference object\n  using TensorCoord;      ///< Logical coordinate in referenced tensor\n\n  ///< advances in units of whole tiles along the logical coordinate space of the tensor\n  CUTLASS_DEVICE\n  RandomAccessTileIteratorConcept & add_tile_offset(TensorCoord const &tile_offset);\n\n  ///< advances in units of whole tiles along the logical coordinate space of the tensor\n  CUTLASS_DEVICE\n  RandomAccessTileIteratorConcept & operator+=(TensorCoord const &tile_offset);\n\n  ///< advances in units of whole tiles along the logical coordinate space of the tensor\n  CUTLASS_DEVICE\n  RandomAccessTileIteratorConcept & operator-=(TensorCoord const &tile_offset);\n};\n```\n\n----------------------------------------\n\nTITLE: ALayout for TN Transpose (T8, V4) -> (m, k)\nDESCRIPTION: This code snippet defines the ALayout for the TN transpose case, mapping 8 threads each owning 4 elements to (m, k) coordinates.  It assumes the TN transpose and calculates the strides accordingly. It uses `Shape` and `Stride` from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n  // (T8,V4) -> (m,k)\n  using ALayout = Layout<Shape <_8,_4>,\n                         Stride<_1,_8>>;\n```\n\n----------------------------------------\n\nTITLE: Implementation Inheritance C++\nDESCRIPTION: This code snippet showcases a scenario where inheritance is used for code reuse but violates the 'is-a' relationship. `PipelineB` inherits from `PipelineA` only for implementation convenience, which can lead to incorrect assumptions and potential slicing issues if polymorphism is not handled correctly. Slicing can also corrupt class invariants.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_16\n\nLANGUAGE: c++\nCODE:\n```\nclass PipelineA {\npublic:\n  PipelineA(Arg0 arg0, Arg1 arg1)\n    : arg0_(arg0), arg1_(arg1)\n  {}\n\n  void producer_acquire(uint32_t stage, uint32_t phase, uint32_t skip_wait) {\n    // ... implementation ... \n  }\n\n  void consumer_release(uint32_t stage, uint32_t skip) {\n    // ... implementation ...\n  }\n\nprivate:\n  Arg0 arg0_;\n  Arg1 arg1_;\n};\n\nclass PipelineB : public PipelineA {\npublic:\n  PipelineB(Arg0 arg0, Arg1 arg1, Arg2 arg2) :\n    PipelineA(arg0, arg1), arg2_(arg2)\n  {}\n\n  // Reuse PipelineA::producer_acquire via inheritance\n\n  // Override PipelineA::consumer_release\n  void consumer_release(uint32_t stage, uint32_t skip) {\n    // ... some other implementation, not invoking parent ...\n  }\n\nprivate:\n  Arg2 arg2_;\n};\n```\n\n----------------------------------------\n\nTITLE: Defining NT Strides (CuTe)\nDESCRIPTION: This code defines the strides for the A, B, and C matrices in the NT (non-transposed) configuration using `make_stride`.  `ldA`, `ldB`, and `ldC` represent the leading dimensions of the matrices.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n  // Define NT strides (mixed)\n  auto dA = make_stride(Int<1>{}, ldA);    // (dM, dK)\n  auto dB = make_stride(Int<1>{}, ldB);    // (dN, dK)\n  auto dC = make_stride(Int<1>{}, ldC);    // (dM, dN)\n```\n\n----------------------------------------\n\nTITLE: Compute Matrix Multiplication on Partitioned Data (C++)\nDESCRIPTION: This code performs the matrix multiplication on the thread-partitioned tensors `tCsA`, `tCsB`, and `tCrC`. The `gemm` function computes the matrix product and accumulates the result into `tCrC`. Each thread operates on its own subtensor of the data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\ngemm(tCsA, tCsB, tCrC);\n```\n\n----------------------------------------\n\nTITLE: Add F16-F32 Convolution Test Executable (SM80)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with F16 input, F32 output, and F32 accumulation on SM80 architecture, conditionally compiled based on the `CUTLASS_NVCC_MAX_ARCH` variable. It includes source files for 2D and 3D convolution, strided data gradients, and group convolutions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n  # Conv - F16 input, F32 output, F32 accumulation\n  cutlass_test_unit_add_executable(\n    cutlass_test_unit_conv_device_tensorop_f32_sm80\n\n    # Conv2d\n    conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu\n    conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu\n    conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu\n\n    # Conv2d (small channel count specializations)\n    conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu\n    conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu\n\n    # Conv2d (Strided Dgrad)\n    conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu\n    conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu\n    conv2d_strided_dgrad_implicit_gemm_swizzling4_sm80.cu\n\n    # Conv3d\n    conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu\n    conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu\n\n    # Group Conv2d\n    group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Ampere Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Ampere GPU architecture using CMake.  The `CUTLASS_NVCC_ARCHS` flag specifies the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=80               # compiles for NVIDIA Ampere GPU architecture\n```\n\n----------------------------------------\n\nTITLE: AlignedBuffer Vectorized Memory Access Example in CUTLASS\nDESCRIPTION: This example shows how to use `AlignedBuffer` for vectorized memory access to shared memory allocations in CUTLASS. It uses `reinterpret_cast` to treat the aligned buffer as an array of `AlignedArray` objects.  Requires the `cutlass` library and CUDA.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nint const kN = 1024;\n\n__shared__ AlignedBuffer<half_t, kN> smem_buffer;\n\nAlignedArray<half_t, 8> *ptr = reinterpret_cast<AlignedArray<half_t, 8> *>(smem_buffer.data());\n\nAlignedArray<half_t, 8> x = ptr[threadIdx.x];     // 128b shared memory load\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Profiler: Example Convolution Command\nDESCRIPTION: This bash snippet shows an example command for profiling a specific convolution operation using the CUTLASS profiler. It specifies various parameters like input/output dimensions, data types, and convolution characteristics to test and analyze the performance of the CUTLASS implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Conv2d --Activation=f16:nhwc --Filter=f16:nhwc --Output=f16 --accumulator-type=f32 --n=32 --h=14 --w=14 --c=8 --k=64 --r=3 --s=3 --pad_h=1 --pad_w=1 --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1\n```\n\n----------------------------------------\n\nTITLE: Defining CLayout for GMMA (T128,V4) -> (M64,N8) with M-mode repetition in C++\nDESCRIPTION: This code snippet extends the previous CLayout definition by adding repetition along the M-mode. It defines the layout for a GMMA operation, specifying the shape and stride with an added dimension to account for the repetition of threads down the M-mode.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n// (T128,V4) -> (M64,N8)\nusing CLayout = Layout<Shape <Shape <  _4, _8, ...>, Shape < _2, ...>>,\n                       Stride<Stride<_128, _1, ...>, Stride<_64, ...>>>;\n```\n\n----------------------------------------\n\nTITLE: Profiling Subset Tensor Core GEMM Kernels\nDESCRIPTION: This command line executes the CUTLASS profiler on a subset of Tensor Core GEMM kernels. The `--kernels` argument specifies the kernel names to profile using a wildcard pattern, and other arguments define the problem size (m, n, k).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*gemm_f16_*_nt_align8 --m=3456 --n=4096 --k=4096\n```\n\n----------------------------------------\n\nTITLE: Defining Test Configurations in CMake\nDESCRIPTION: This snippet defines various test configurations for FMHA and MLA examples.  Each `set` command creates a variable containing a list of command-line options to be passed to the executable during testing.  These tests cover different scenarios such as basic operation, causal masking, variable length sequences, high-dimensional inputs, and grouped query attention.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/77_blackwell_fmha/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_BASIC --b=1 --h=4 --q=512 --k=512 --d=128 --verify --mask=no)\nset(TEST_CAUSAL --b=1 --h=4 --q=512 --k=512 --d=128 --verify --mask=causal)\nset(TEST_VARLEN --b=1 --h=4 --q=512 --k=512 --d=128 --verify --mask=residual --varlen)\nset(TEST_HDIM64 --b=2 --h=4 --q=512 --k=512 --d=64 --verify)\nset(TEST_GQA --b=2 --h=4 --h_k=2 --q=512 --k=512 --d=64 --verify)\n\nset(TEST_GEN_BASIC --b=1 --h=4 --k=512 --d=128 --verify)\nset(TEST_GEN_VARLEN --b=1 --h=4 --k=512 --d=128 --verify  --varlen)\nset(TEST_GEN_HDIM64 --b=2 --h=4 --k=512 --d=64 --verify)\nset(TEST_GEN_GQA --b=2 --h=4 --h_k=2 --k=512 --d=64 --verify)\nset(TEST_GEN_REMAP --b=2 --h=4 --h_k=2 --k=512 --d=128 --verify --remap)\nset(TEST_GEN_CACHEONLY --b=2 --h=4 --h_k=2 --k=512 --d=128 --verify --cache-only)\n\nset(TEST_MLA_BASIC --b=1 --k=512 --verify)\n```\n\n----------------------------------------\n\nTITLE: Run Grouped GEMM with PyTorch Extension\nDESCRIPTION: This code snippet imports the compiled PyTorch CUDA extension and uses it to run a group of GEMM operations. The results are then compared to the results obtained using vanilla PyTorch GEMMs.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport grouped_gemm\n\ngrouped_gemm.run(As, Bs)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nDs = grouped_gemm.run(As, Bs)\nDs_torch = [a @ b for a, b in zip(As, Bs)]\nfor d, d_torch in zip(Ds, Ds_torch):\n    assert torch.allclose(d, d_torch)\n```\n\n----------------------------------------\n\nTITLE: GEMM Device Kernel Definition (CuTe)\nDESCRIPTION: This is the kernel entry point for the GEMM operation. It defines the template parameters for problem shape, CTA tiler, data types, memory layouts, strides, thread layouts, and scalar constants. It launches the kernel with specified thread bounds.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <class ProblemShape, class CtaTiler,\n          class TA, class AStride, class ASmemLayout, class AThreadLayout,\n          class TB, class BStride, class BSmemLayout, class BThreadLayout,\n          class TC, class CStride, class CSmemLayout, class CThreadLayout,\n          class Alpha, class Beta>\n__global__ static\n__launch_bounds__(decltype(size(CThreadLayout{}))::value)\nvoid\ngemm_device(ProblemShape shape_MNK, CtaTiler cta_tiler,\n            TA const* A, AStride dA, ASmemLayout sA_layout, AThreadLayout tA,\n            TB const* B, BStride dB, BSmemLayout sB_layout, BThreadLayout tB,\n            TC      * C, CStride dC, CSmemLayout          , CThreadLayout tC,\n            Alpha alpha, Beta beta)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Convolution Weight Gradient Test\nDESCRIPTION: This CMake code uses a custom function `cutlass_test_unit_add_executable` to create an executable named `cutlass_test_unit_conv_wgrad_device_tensorop_sm90`.  The executable is built from a list of CUDA source files implementing different convolution weight gradient tests that use tensor cores on the SM90 architecture. These files cover 1D, 2D, and 3D convolutions with different data types (f16, f32).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/wgrad/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv_wgrad_device_tensorop_sm90\n\n  sm90_conv1d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv2d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv3d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n\n  sm90_conv1d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv2d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv3d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Shared Memory Allocation and Tensor Creation C++\nDESCRIPTION: This snippet allocates shared memory buffers `smemA` and `smemB` based on `ABlockLayout` and `BBlockLayout`.  It then creates `Tensor` objects `sA` and `sB` that point to these shared memory regions using the specified shared memory layouts `sA_layout` and `sB_layout`. The `cosize_v` is used to determine the required size of the shared memory buffers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n  // Shared memory buffers\n  __shared__ TA smemA[cosize_v<ABlockLayout>];\n  __shared__ TB smemB[cosize_v<BBlockLayout>];\n  Tensor sA = make_tensor(make_smem_ptr(smemA), sA_layout);  // (BLK_M,BLK_K)\n  Tensor sB = make_tensor(make_smem_ptr(smemB), sB_layout);  // (BLK_N,BLK_K)\n```\n\n----------------------------------------\n\nTITLE: Preferred Function Design with Return Values in C++\nDESCRIPTION: This snippet demonstrates the preferred function design in CUTLASS, which favors returning values over using in-out mutable references. It aims to keep functions pure and improve code readability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n// Instead of passing in-out mutable references ...\nvoid not_preferred(float& input_and_output); // not preferred\n\n// keep functions pure and return value types instead\nfloat preferred(float input); // preferred\n```\n\n----------------------------------------\n\nTITLE: Executing Ampere Gather/Scatter Convolution\nDESCRIPTION: These commands execute the compiled Ampere gather/scatter convolution example with different input sizes and configurations, including options to disable correctness checks.  The parameters --n sets the problem size and --i controls the number of iterations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./59_ampere_gather_scatter_conv\n./59_ampere_gather_scatter_conv --n=108\n./59_ampere_gather_scatter_conv --n=4096 --i=1\n./59_ampere_gather_scatter_conv --n=1080 --i=1000\n./59_ampere_gather_scatter_conv --n=131072 --i=1000 --no-check\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for FP8 Groupwise GEMM in CUTLASS (CUDA)\nDESCRIPTION: This code snippet defines an executable named '67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling' using the 'cutlass_example_add_executable' macro. It specifies the corresponding CUDA source file '67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu'. This executable showcases the use of Hopper architecture with FP8 precision and warp-specialized GEMM using groupwise scaling in CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling\n  67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Conv2d Strided Dgrad Optimized F16 Configuration\nDESCRIPTION: This example demonstrates a device convolution operation with strided dgrad and optimized implementation using float16 data type and TensorOp. It utilizes implicit GEMM and specifies tensor layouts in NHWC format.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 16 -ta float16 -tb float16 -tc float16 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 4 -lb TensorNHWC -ab 4 -lc TensorNHWC -ac 4 -te float32 -ep LinearCombination -sw StridedDgradIdentitySwizzle1 -co dgrad -st Strided -ia optimized -sm Serial -k 1 -nhwc 1 56 56 12 -krsc 8 1 1 12 -pad 0 0 0 0 -stride 2 2 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: Import CUTLASS and Declare GroupedGemm\nDESCRIPTION: This code snippet imports the necessary libraries (cutlass and torch) and declares a grouped GEMM operation using the CUTLASS Python interface. It specifies the data type as float16 and the layout as RowMajor.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cutlass\nimport torch\n\ndtype = torch.float16\nplan = cutlass.op.GroupedGemm(element=dtype, layout=cutlass.LayoutType.RowMajor)\n```\n\n----------------------------------------\n\nTITLE: TensorFillRandomUniform Example\nDESCRIPTION: This example demonstrates how to use `cutlass::reference::host::TensorFillRandomUniform` and `cutlass::reference::device::TensorFillRandomUniform` to initialize a tensor with a random uniform distribution in both host and device memory. It takes a seed, a maximum value, and a minimum value as input.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/layout/matrix.h>\n#include <cutlass/util/reference/host/tensor_fill.h>\n#include <cutlass/util/reference/device/tensor_fill.h>\n#include <cutlass/util/host_tensor.h>\n\nint main() {\n  int rows = 128;\n  int columns = 64;\n\n  double maximum = 4;\n  double minimum = -4;\n  uint64_t seed = 0x2019;\n\n  cutlass::HostTensor<float, cutlass::layout::ColumnMajor> tensor({rows, columns});\n\n  // Initialize in host memory\n  cutlass::reference::host::TensorFillRandomUniform(\n    tensor.host_view(),\n    seed,\n    maximum,\n    minimum);\n\n  // Initialize in device memory\n  cutlass::reference::device::TensorFillRandomUniform(\n    tensor.device_view(),\n    seed,\n    maximum,\n    minimum);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Hopper Subdirectory (CMake)\nDESCRIPTION: This command adds the 'hopper' subdirectory to the current CMake project. This allows CMake to find and process the CMakeLists.txt file within the 'hopper' directory, integrating its build targets into the overall project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(hopper)\n```\n\n----------------------------------------\n\nTITLE: Non-persistent Kernel Definition C++\nDESCRIPTION: This is a pseudo-code representation of a non-persistent kernel.  It sets up common data structures and then performs coordinate-specific compute based on the block index. The kernel executes once for each work coordinate.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_cluster_launch_control.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n// Non-persistent kernel\n__device__ non_persistent_kernel(...) {\n  setup_common_data_structures();\n  dim3 workCoordinates = blockIdx;\n  coordinate_specific_compute(workCoordinates);\n}\n```\n\n----------------------------------------\n\nTITLE: Specify Tensor Layouts\nDESCRIPTION: This command runs the GEMM profiler with specific tensor layouts for the A and B matrices. A is set to f16 with column-major layout, and B is set to any datatype (*) with row-major layout. This demonstrates how to specify the layout of input tensors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --A=f16:column --B=*:row\n```\n\n----------------------------------------\n\nTITLE: Compiling with Specific Tile Description in CUTLASS Python\nDESCRIPTION: Selects a tile description at random and compiles and runs the GEMM operation with that tile description. It filters for threadblock shapes greater than or equal to 128.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntiles = [td for td in tiles if td.threadblock_shape[0] >= 128]\nidx = random.randint(0, len(tiles)-1)\ntd = tiles[idx]\nprint('Tile description {} is: {}'.format(idx, td))\nplan.compile(td)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Conditional Target Definition for sm100 Architecture - CMake\nDESCRIPTION: This code block conditionally defines a custom target for sm100 sparse GEMM tests only if the `CUTLASS_NVCC_ARCHS` variable matches '100a'. It creates the `cutlass_test_unit_gemm_device_sm100_sparse` target, which depends on the `cutlass_test_unit_gemm_device_sm100_sparse_general` and `cutlass_test_unit_gemm_device_sm100_sparse_streamk` targets.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\n\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm100_sparse\n  DEPENDS\n  cutlass_test_unit_gemm_device_sm100_sparse_general\n  cutlass_test_unit_gemm_device_sm100_sparse_streamk\n)\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS Example with synclog (Shell)\nDESCRIPTION: This shell command demonstrates how to build a CUTLASS example after enabling `synclog`.  The commands navigate to the example directory and then uses `make` to build the example executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\n$ cd examples/54_hopper_fp8_warp_specialized_gemm\n$ make\n```\n\n----------------------------------------\n\nTITLE: Applying Coord to Rest Mode (CuTe)\nDESCRIPTION: This code segment shows how to apply the coord to the rest mode of the tensor after applying the tiler. It extracts the correct tiles for this CTA by slicing into the rest-mode using `gA_mk(make_coord(_,_), select<0,2>(cta_coord))`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\n// (BLK_M,BLK_K,k)\nTensor gA = gA_mk(make_coord(_,_), select<0,2>(cta_coord));\n```\n\n----------------------------------------\n\nTITLE: Adding Groupwise GEMM Executable Example in CMake\nDESCRIPTION: This snippet uses a CMake macro `cutlass_example_add_executable` to create an executable for a groupwise GEMM (General Matrix Multiplication) example. It specifies the source file (`81_blackwell_gemm_groupwise.cu`) and test command options, random problem sizes, epilogue options, and small problem sizes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  81_blackwell_gemm_groupwise \n  81_blackwell_gemm_groupwise.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_EPILOGUE\n  TEST_SMALL\n)\n```\n\n----------------------------------------\n\nTITLE: Reference Check Code Snippet\nDESCRIPTION: This C++ code snippet demonstrates the im2col transformation by implementing convolutions as GEMMs with layout transformations. It iterates through the logical dimensions of the output and calculates the accumulator value based on stencil and activation tensors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nfor (size_t logical_m = 0; logical_m < size<0>(mOutputRef); ++logical_m) {\n  for (size_t logical_n = 0; logical_n < size<1>(mOutputRef); ++logical_n) {\n    auto accumulator = float(0);\n    for (size_t logical_k = 0; logical_k < size<1>(mStencil); ++logical_k) {\n      accumulator += mStencil(logical_m, logical_k) * mActivation(logical_n, logical_k);\n    }\n    mOutputRef(logical_m, logical_n) = accumulator;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Python Test for FMHA Backward Pass in CMake\nDESCRIPTION: This code snippet defines a test named `ctest_examples_41_fmha_backward_python` that executes a Python script `fmha_backward_test.py`. The script is executed using the Python3 interpreter and the executable target for the FMHA backward pass example is passed as an argument.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/41_fused_multi_head_attention/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_test(\n  NAME ctest_examples_41_fmha_backward_python\n  COMMAND ${Python3_EXECUTABLE} ${CMAKE_CURRENT_LIST_DIR}/fmha_backward_test.py $<TARGET_FILE:41_fused_multi_head_attention_backward>\n)\n```\n\n----------------------------------------\n\nTITLE: Creating gemm_grouped Executable via CMake\nDESCRIPTION: This CMake snippet uses the `cutlass_example_add_executable` macro to create an executable named `24_gemm_grouped` from the source file `gemm_grouped.cu`. The macro encapsulates the necessary CMake commands to build the executable within the CUTLASS build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/24_gemm_grouped/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  24_gemm_grouped\n  gemm_grouped.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: GEMM Grouped F64 Host Schedule Example\nDESCRIPTION: This example performs a GEMM Grouped operation on host using float64 inputs. It uses a host-side scheduling mechanism and reads problem sizes from a CSV file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npython gemm_grouped.py -i 8 8 4 -ta float64 -tb float64 -tc float64 -tacc float64 -m multiply_add -op TensorOp -b 64 64 16 -s 4 -w 2 2 1 -cc 80 -la RowMajor -aa 1 -lb RowMajor -ab 1 -lc ColumnMajor -ac 1 -te float64 -ep LinearCombination -p ./grouped_gemm_problem_size.csv -alpha 1.0 -beta 1.0 -pm Host\n```\n\n----------------------------------------\n\nTITLE: Defining ALayout for GMMA (T128,V64x8) -> (M64,K16) in C++\nDESCRIPTION: This code defines the ALayout for GMMA when consuming A sources directly from shared memory. All threads are mapped to the (0,0) element, and the values remain unchanged, allowing the GMMA Descriptor Constructor to inspect the (M,K) layout.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n// (T128,V64x8) -> (M64,K16)\nusing ALayout = Layout<Shape <_128, Shape <_64,_16>>,\n                       Stride<  _0, Stride< _1,_64>>>;\n```\n\n----------------------------------------\n\nTITLE: Partitioning Tensors with TiledMMA (C++)\nDESCRIPTION: This code demonstrates how to use a `TiledMMA` instance to partition shared memory tensors for the matrix multiply-accumulate operation. It retrieves a thread MMA slice using `mma.get_slice(threadIdx.x)`, then partitions the `sA`, `sB`, and `gC` tensors using `partition_A`, `partition_B`, and `partition_C`, respectively. A fragment tensor `tCrC` is allocated using `make_fragment_C` to hold the accumulated result.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_30\n\nLANGUAGE: cpp\nCODE:\n```\n  ThrMMA thr_mma = mma.get_slice(threadIdx.x);\n  Tensor tCsA = thr_mma.partition_A(sA);        // (MMA,MMA_M,MMA_K)\n  Tensor tCsB = thr_mma.partition_B(sB);        // (MMA,MMA_N,MMA_K)\n  Tensor tCgC = thr_mma.partition_C(gC);        // (MMA,MMA_M,MMA_N)\n  // Allocate the accumulators -- same size as the projected data\n  Tensor tCrC = thr_mma.make_fragment_C(tCgC);  // (MMA,MMA_M,MMA_N)\n```\n\n----------------------------------------\n\nTITLE: Build and run CUTLASS Docker container with CUDA 12.1\nDESCRIPTION: These commands build and run a Docker container for CUTLASS using CUDA 12.1 and PyTorch. The `docker build` command creates an image tagged as `cutlass-cuda12.1:latest` using the Dockerfile located at `docker/Dockerfile-cuda12.1-pytorch`.  The `docker run` command then launches the container with GPU support enabled (`--gpus all`), interactive terminal (`-it`), and removes the container upon exit (`--rm`).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/_sources/install.md.txt#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t cutlass-cuda12.1:latest -f docker/Dockerfile-cuda12.1-pytorch .\ndocker run --gpus all -it --rm cutlass-cuda12.1:latest\n```\n\n----------------------------------------\n\nTITLE: Comparing CUTLASS GEMM Output with NumPy\nDESCRIPTION: This snippet calculates the GEMM result using NumPy for comparison with the CUTLASS output. It performs the matrix multiplication of tensor_A and tensor_B, scales it by alpha, adds the scaled tensor_C (scaled by beta), and then asserts that the CUTLASS output (tensor_D) is equal to the NumPy result using `np.testing.assert_array_equal`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntensor_D_numpy = (alpha * (tensor_A @ tensor_B)) + (beta * tensor_C)\nnp.testing.assert_array_equal(tensor_D, tensor_D_numpy)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds subdirectories to the current CMake project. Each subdirectory likely contains its own CMakeLists.txt file defining how to build the components within that directory.  The subdirectories added are 'thread', 'warp', and 'threadblock', representing different levels of parallelism in CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/epilogue/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(thread)\nadd_subdirectory(warp)\nadd_subdirectory(threadblock)\n```\n\n----------------------------------------\n\nTITLE: GEMM Matrix Inverse Indexing Conversion\nDESCRIPTION: These equations define the inverse mapping between the matrix indices i and j and the tensor index (n, p, q, k). They serve to retrieve the original tensor indices from the GEMM matrix indices.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nk = j\n\nn = i / (PQ)\nresidual = i % (PQ)\n\np = residual / Q\nq = residual % Q\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper Bulk Store Unit Test Executable with CMake\nDESCRIPTION: This CMake snippet is used to create an executable called `cutlass_test_unit_cute_hopper_bulk_store` from the source file `bulk_store.cu`.  The `cutlass_test_unit_add_executable` function encapsulates the details of compiling and linking the source file to create the executable, and this test focuses on bulk store operations on the Hopper architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_bulk_store\n  bulk_store.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Linking the Executable Target\nDESCRIPTION: This snippet defines the executable target, specifies the source files, includes the CUDA toolkit include directories, and links against the necessary CUTLASS libraries. The `target_link_libraries` command ensures the executable is linked with the CUTLASS and library components.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/60_cutlass_import/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(example)\n\ntarget_sources(example PRIVATE main.cpp)\n\ntarget_include_directories(\n  example\n  PRIVATE\n  ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}\n  )\n\ntarget_link_libraries(\n  example \n  PRIVATE\n  nvidia::cutlass::cutlass\n  nvidia::cutlass::library\n  )\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Initialize Tensors - Python\nDESCRIPTION: This snippet imports necessary libraries such as NumPy and CUTLASS, and initializes input tensors A, B, C, and D with random values. The dimensions (m, n, k) and data types (float16) of the tensors are also defined.  A random seed is set for reproducibility.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/01_epilogue.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport cutlass\n\n# This controls whether ther C++ GEMM declaration will be printed at each step. Set to `false` to\n# omit this information.\nprint_module = True\n\nm = 256\nn = m\nk = m\n\ntype_A = np.float16\ntype_B = np.float16\ntype_C = np.float16\ntype_D = np.float16\n\nnp.random.seed(1234)\nscope_min = -4\nscope_max = 4\ntensor_A = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, k)).astype(type_A))\ntensor_B = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(k, n)).astype(type_B))\ntensor_C = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, n)).astype(type_C))\n\nalpha = np.float16(1.)\nbeta = np.float16(0.)\n\ntensor_D = np.zeros(tensor_C.shape).astype(type_D)\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Owning Tensors in C++\nDESCRIPTION: This code snippet demonstrates creating non-owning `Tensor`s, which are views of existing memory. The `make_tensor` function is used with a random-access iterator and a `Layout` to construct the `Tensor`. It includes examples of creating tensors with untagged pointers, global memory, and shared memory using static or dynamic layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nfloat* A = ...;\n\n// Untagged pointers\nTensor tensor_8   = make_tensor(A, make_layout(Int<8>{}));  // Construct with Layout\nTensor tensor_8s  = make_tensor(A, Int<8>{});               // Construct with Shape\nTensor tensor_8d2 = make_tensor(A, 8, 2);                   // Construct with Shape and Stride\n\n// Global memory (static or dynamic layouts)\nTensor gmem_8s     = make_tensor(make_gmem_ptr(A), Int<8>{});\nTensor gmem_8d     = make_tensor(make_gmem_ptr(A), 8);\nTensor gmem_8sx16d = make_tensor(make_gmem_ptr(A), make_shape(Int<8>{},16));\nTensor gmem_8dx16s = make_tensor(make_gmem_ptr(A), make_shape (      8  ,Int<16>{}),\n                                                   make_stride(Int<16>{},Int< 1>{}));\n\n// Shared memory (static or dynamic layouts)\nLayout smem_layout = make_layout(make_shape(Int<4>{},Int<8>{}));\n__shared__ float smem[decltype(cosize(smem_layout))::value];   // (static-only allocation)\nTensor smem_4x8_col = make_tensor(make_smem_ptr(smem), smem_layout);\nTensor smem_4x8_row = make_tensor(make_smem_ptr(smem), shape(smem_layout), LayoutRight{});\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Executable\nDESCRIPTION: This function, `cutlass_test_unit_add_executable`, simplifies the process of adding a CUTLASS unit test executable. It takes the executable name as input, parses arguments, and sets necessary compile definitions, include directories, and link libraries. It also handles OpenMP linking and sets up result caching. The test command options are also configured.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(cutlass_test_unit_add_executable NAME)\n\n  set(options WITHOUT_CUDA DO_NOT_LOWERCASE_TEST_NAME)\n  set(oneValueArgs)\n  set(multiValueArgs TEST_SETS_SUPPORTED EXTRA_INCLUDE_DIRS)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  cutlass_add_executable(${NAME} ${__UNPARSED_ARGUMENTS} BATCH_SOURCES OFF)\n\n  target_compile_definitions(${NAME} PUBLIC CUTLASS_TARGET_NAME=\"${NAME}\")\n\n  target_include_directories(\n    ${NAME}\n    PRIVATE\n    ${CUTLASS_UNIT_TEST_COMMON_DIR}\n    ${__EXTRA_INCLUDE_DIRS}\n  )\n  if (__WITHOUT_CUDA)\n    # Avoid CUDA dependencies for host-only unit tests that provide the\n    # WITHOUT_CUDA argument.\n    target_link_libraries(\n      ${NAME}\n      PUBLIC\n      GTest::gtest \n    )\n  else()\n    target_link_libraries(\n      ${NAME}\n      PRIVATE\n      cutlass_test_unit_infra\n      cutlass_test_unit_infra_lib\n    )\n  endif()\n\n  if (CUTLASS_ENABLE_OPENMP_TESTS AND OpenMP_CXX_FOUND)\n    target_link_libraries(${NAME} PRIVATE OpenMP::OpenMP_CXX)\n  endif()\n\n  string(REGEX REPLACE cutlass_ \"\" NAME_STEM ${NAME})\n\n  set(RESULT_CACHE_FILE \"${CUTLASS_TEST_UNIT_RESULTS_CACHE_DIR}/cached_results_${NAME}.txt\")\n\n  if (EXISTS ${RESULT_CACHE_FILE})\n    set(RESULT_CACHE_FILE_ARGS RESULT_CACHE_FILE ${RESULT_CACHE_FILE})\n  endif()\n\n  set(CUTLASS_TEST_UNIT_TEST_COMMAND_OPTIONS --gtest_output=xml:${NAME_STEM}.gtest.xml)\n\n  if (__DO_NOT_LOWERCASE_TEST_NAME)\n    set(DO_NOT_LOWERCASE_TEST_NAME DO_NOT_LOWERCASE_TEST_NAME)\n  else()\n    set(DO_NOT_LOWERCASE_TEST_NAME)\n  endif()\n  \n  cutlass_add_executable_tests(\n    ${NAME_STEM} ${NAME}\n    TEST_SETS_SUPPORTED ${__TEST_SETS_SUPPORTED}\n    TEST_COMMAND_OPTIONS CUTLASS_TEST_UNIT_TEST_COMMAND_OPTIONS\n    ${RESULT_CACHE_FILE_ARGS}\n    ${DO_NOT_LOWERCASE_TEST_NAME}\n    )\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Generate Group of GEMM Problems\nDESCRIPTION: This utility function generates a list of GEMM problems with random sizes. It selects dimensions (M, N, K) from a predefined set of valid sizes and initializes the A, B, C, and D matrices for each problem.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Utility function to generate `problems` GEMMs of random sizes\ndef generate_problems(problems):\n    valid_sizes = [128, 256, 512, 1024]\n    As, Bs, Cs, Ds = [], [], [], []\n    for _ in range(problems):\n        M, N, K = [random.choice(valid_sizes) for _ in range(3)]\n        A, B, C, D = initialize(dtype, M, N, K)\n        As.append(A)\n        Bs.append(B)\n        Cs.append(C)\n        Ds.append(D)\n    return As, Bs, Cs, Ds\n```\n\n----------------------------------------\n\nTITLE: Grouping and Flattening Layout Modes in CuTe (C++)\nDESCRIPTION: This snippet demonstrates how to group and flatten layout modes using `group` and `flatten`. Grouping combines modes into a single mode, while flattening converts a hierarchical layout into a flat layout. Layout `a` is defined as `Layout<Shape<_2,_3,_5,_7>>`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a = Layout<Shape<_2,_3,_5,_7>>{};  // (_2,_3,_5,_7):(_1,_2,_6,_30)\nLayout b = group<0,2>(a);                 // ((_2,_3),_5,_7):((_1,_2),_6,_30)\nLayout c = group<1,3>(b);                 // ((_2,_3),(_5,_7)):((_1,_2),(_6,_30))\nLayout f = flatten(b);                    // (_2,_3,_5,_7):(_1,_2,_6,_30)\nLayout e = flatten(c);                    // (_2,_3,_5,_7):(_1,_2,_6,_30)\n```\n\n----------------------------------------\n\nTITLE: GEMM Convolution with Tensor Indexing\nDESCRIPTION: This code demonstrates how to implement convolution using a GEMM approach with explicit tensor indexing. The code iterates over the GEMM dimensions and maps them to the corresponding tensor indices to perform the convolution operation. It involves calculation of multiple residuals and division which could be computational intensive.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nint GEMM_M = N * P * Q;\nint GEMM_N = K;\nint GEMM_K = C * R * S;\n\nfor (int gemm_i = 0; gemm_i < GEMM_M; ++gemm_i) {\n  for (int gemm_j = 0; gemm_j < GEMM_N; ++gemm_j) {\n\n    int n = gemm_i / (PQ);\n    int npq_residual = gemm_i % (PQ);\n\n    int p = npq_residual / Q;\n    int q = npq_residual % Q;\n\n    Accumulator accum = 0;\n\n    for (int gemm_k = 0; gemm_k < GEMM_K; ++gemm_k) {\n\n      int k = gemm_j;\n\n      int c = gemm_k / (RS);\n      int crs_residual = gemm_k % (RS);\n\n      int r = crs_residual / S;\n      int s = crs_residual % S;\n\n      int h = f(p, r);\n      int w = g(q, s);\n\n      ElementA a = tensor_A.at({n, h, w, c});\n      ElementB b = tensor_B.at({k, r, s, c});\n\n      accum += a * b;\n    }\n\n    C[gemm_i * K + gemm_j] = accum;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tensor Core Warp-level Mma Template in CUTLASS\nDESCRIPTION: This code defines the template for warp-level matrix multiply operators targeting Tensor Cores in CUTLASS.  The `Policy` type specifies implementation-level details to affect performance or internal implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\nnamespace cutlass {\nnamespace gemm {\nnamespace warp {\n\n/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.\ntemplate <\n  /// Size of the Gemm problem - concept: gemm::GemmShape<>\n  typename Shape_,\n  /// Data type of A elements\n  typename ElementA_,\n  /// Layout of A matrix (concept: MatrixLayout)\n  typename LayoutA_,\n  /// Data type of B elements\n  typename ElementB_,\n  /// Layout of B matrix (concept: MatrixLayout)\n  typename LayoutB_,\n  /// Element type of C matrix\n  typename ElementC_,\n  /// Layout of C matrix (concept: MatrixLayout)\n  typename LayoutC_,\n  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)\n  typename Policy_,\n  /// Used for partial specialization\n  typename Enable = bool\n>\nclass MmaTensorOp {}\n\n} // namespace warp\n} // namespace gemm\n} // namespace cutlass\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS Python Interface from source in editable mode\nDESCRIPTION: This command installs the CUTLASS Python interface from source in editable mode. Changes to the source code are immediately reflected without needing to reinstall the package.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Defining a CUTLASS Executable in CMake\nDESCRIPTION: This CMake code snippet defines a CUTLASS executable named '53_hopper_gemm_permute' using the source file '53_hopper_gemm_permute.cu'.  The `cutlass_example_add_executable` function is presumed to be a custom function defined within the CUTLASS build system to simplify executable creation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/53_hopper_gemm_permute/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  53_hopper_gemm_permute\n  53_hopper_gemm_permute.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: GEMM Operation with CUTLASS Python Interface\nDESCRIPTION: This code snippet demonstrates how to use the CUTLASS Python interface to run a GEMM (General Matrix Multiplication) operation. It initializes the GEMM plan with float16 data type and row-major layout, creates four NumPy arrays, and executes the GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cutlass\nimport numpy as np\n\nplan = cutlass.op.Gemm(element=np.float16, layout=cutlass.LayoutType.RowMajor)\nA, B, C, D = [np.ones((1024, 1024), dtype=np.float16) for i in range(4)]\nplan.run(A, B, C, D)\n```\n\n----------------------------------------\n\nTITLE: Compiling Hopper Gather Scatter Fusion Example in CUTLASS\nDESCRIPTION: This snippet utilizes the `cutlass_example_add_executable` macro to compile the `52_hopper_gather_scatter_fusion` example. It takes the name of the executable and the CUDA source file as input, automatically configuring the build process for the example. The output is an executable file that demonstrates gather-scatter fusion on the Hopper architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/52_hopper_gather_scatter_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: Makefile\nCODE:\n```\ncutlass_example_add_executable(\n  52_hopper_gather_scatter_fusion\n  52_hopper_gather_scatter_fusion.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Configure GEMM kernels for Turing Tensor Cores\nDESCRIPTION: This CMake command configures CUTLASS to compile only GEMM kernels that target NVIDIA Turing Tensor Cores (architecture 75).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_29\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=75 -DCUTLASS_LIBRARY_KERNELS=tensorop*gemm\n```\n\n----------------------------------------\n\nTITLE: Profiling Grouped GEMM Extension\nDESCRIPTION: This snippet profiles the performance of the grouped GEMM extension and compares it to standard PyTorch GEMM. It performs warmup iterations to stabilize the GPU, then measures the execution time of both the grouped GEMM extension and the PyTorch GEMM operations over multiple iterations. It calculates and prints the average execution time and speedup.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnum_warmup = 20\nnum_profile = 100\n\n# Warmup iterations\nfor _ in range(num_warmup):\n    Ds = grouped_gemm.run(As, Bs)\n    Ds_torch = [a @ b for a, b in zip(As, Bs)]\n    torch.cuda.synchronize()\n\n# Timing iterations\nimport time\ngrouped = 0\nnongrouped = 0\nfor _ in range(num_profile):\n    start = time.time()\n    Ds = grouped_gemm.run(As, Bs)\n    torch.cuda.synchronize()\n    grouped += time.time() - start\n\n    start = time.time()\n    Ds_torch = [a @ b for a, b in zip(As, Bs)]\n    torch.cuda.synchronize()\n    nongrouped += time.time() - start\n\nprint('Grouped:     {:.3f} us'.format(grouped * 1e6/num_profile))\nprint('Non-Grouped: {:.3f} us'.format(nongrouped * 1e6/num_profile))\nprint('Speedup: {:.3f}'.format(nongrouped / grouped))\n```\n\n----------------------------------------\n\nTITLE: By-Mode Composition with Shapes as Tilers in CuTe (C++)\nDESCRIPTION: This C++ code shows how to use a `Shape` as a tiler in CuTe's by-mode composition.  It defines a layout `a` and a shape `tiler`, and computes their composition. The shape `tiler` is interpreted as a tuple of layouts with stride 1. This example requires CuTe library functions such as `make_layout`, `make_shape`, `make_stride`, and `composition`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// (12,(4,8)):(59,(13,1))\nauto a = make_layout(make_shape (12,make_shape ( 4,8)),\n                     make_stride(59,make_stride(13,1)));\n// (8, 3)\nauto tiler = make_shape(Int<3>{}, Int<8>{});\n// Equivalent to <3:1, 8:1>\n// auto tiler = make_tile(Layout<_3,_1>{},  // Apply 3:1 to mode-0\n//                        Layout<_8,_1>{}); // Apply 8:1 to mode-1\n\n// (_3,(4,2)):(59,(13,1))\nauto result = composition(a, tiler);\n```\n\n----------------------------------------\n\nTITLE: Defining WriteableReadableForwardContiguousTileIteratorConcept in CUTLASS\nDESCRIPTION: Defines the `WriteableReadableForwardContiguousTileIteratorConcept` in CUTLASS. This concept combines forward iteration, readability, and writability for contiguous tiles. It provides methods for advancing the iterator, loading data from memory into a fragment, and storing a fragment back to memory, including offset variations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_14\n\nLANGUAGE: c++\nCODE:\n```\n/// This tile iterator embodies several of the above:\n///\n///   - ForwardTileIteratorConcept\n///   - ReadableContiguousTileIteratorConcept\n///   - WriteableContiguousTileIteratorConcept\n/// \n/// It is restated explicitly for convenience of the reader.\n/// \nstruct WriteableReadableForwardContiguousTileIteratorConcept {\n\n  //\n  // Data types\n  //\n  \n  using Element;           ///< Element type composing tile.\n  using Shape;             ///< Shape type describing extent of tile. The shape concept depends \n                           ///  on iterator implementation\n  using Index;             ///< index type used as base for TensorCoord\n  using Fragment;          ///< fragment object derived from cutlass::Array<Element, N>\n\n  //\n  // Methods\n  //\n\n  /// Adds a linear offset in units of Element to internal pointer(s) into tensor\n  CUTLASS_DEVICE \n  void add_pointer_offset(Index offset);\n\n  /// true if iterators point to same tile, false if otherwise\n  CUTLASS_DEVICE bool operator==(WriteableReadableForwardContiguousTileIteratorConcept const &it);\n\n  ///< false if iterators point to same tile, true if otherwise\n  CUTLASS_DEVICE bool operator!=(WriteableReadableForwardContiguousTileIteratorConcept const &it);\n\n  /// pre-increment - traverse to next tile in sequence\n  CUTLASS_DEVICE\n  WriteableReadableForwardContiguousTileIteratorConcept & \n  operator++();\n\n  ///< post-increment - traverse to next tile in sequence\n  CUTLASS_DEVICE\n  WriteableReadableForwardContiguousTileIteratorConcept \n  operator++(int);\n\n  /// Loads a fragment from memory\n  CUTLASS_DEVICE\n  void load(Fragment &frag);                    ///< fragment to be loaded from memory\n\n  /// Loads a fragment from memory with additional logical offset\n  CUTLASS_DEVICE\n  void load_with_pointer_offset(\n    Fragment &frag,                             ///< fragment to be loaded from memory\n    Index pointer_offset);                      ///< linear offset (in units of Element) when loading\n\n  /// Stores a fragment to memory\n  CUTLASS_DEVICE\n  void store(Fragment const &frag);             ///< fragment to store to memory\n\n  /// Stores a fragment from memory with additional logical offset\n  CUTLASS_DEVICE\n  void store_with_pointer_offset(\n    Fragment const &frag,                       ///< fragment to store to memory\n    Index pointer_offset);                      ///< linear offset (in units of Element) when storing\n};\n```\n\n----------------------------------------\n\nTITLE: DeviceAllocation Example\nDESCRIPTION: This example demonstrates allocating memory on the device using `cutlass::DeviceAllocation<>`. The allocated memory is automatically freed when `device_alloc` goes out of scope.  It also shows how to pass the device memory pointer to a CUDA kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/layout/matrix.h>\n#include <cutlass/layout/tensor_view.h>\n#include <cutlass/util/device_memory.h>\n\n__global__ void kernel(float *device_ptr) {\n\n}\n\nint main() {\n\n  size_t N = 1024;\n\n  cutlass::DeviceAllocation<float> device_alloc(N);\n\n  // Call a CUDA kernel passing device memory as a pointer argument\n  kernel<<< grid, block >>>(alloc.get());\n\n  if (cudaGetLastError() != cudaSuccess) {\n    return -1;\n  }\n\n  // Device memory is automatically freed when device_alloc goes out of scope\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion Based on CUTLASS_NVCC_ARCHS\nDESCRIPTION: This CMake code conditionally includes the block of code related to SM100 architecture.  The if statement `if (CUTLASS_NVCC_ARCHS MATCHES 100a)` checks if the variable `CUTLASS_NVCC_ARCHS` matches the string '100a', indicating that the SM100 architecture is targeted.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\n\n...\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: C++ Tensor Layout Comments\nDESCRIPTION: This snippet illustrates the recommended style for tensor layout comments in CUTLASS code. Layout comments should be right-aligned at column 120. For related tensors with long comments, try to align the layout comments of related tensors for better readability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\nTensor mC = make_tensor(make_gmem_ptr(params.ptr_C), make_shape(M,N), params.dC);                              // (M,N)\nTensor mD = make_tensor(make_gmem_ptr(params.ptr_D), make_shape(M,N), params.dD);                              // (M,N)\nTensor mAux = make_tensor(make_gmem_ptr(params.ptr_Aux), make_shape(M,N), params.dAux);                        // (M,N)\n\nauto thr_mma = tiled_mma.get_thread_slice(thread_idx);\nTensor tCgD = thr_mma.partition_C(gD);                                                             // (VEC,THR_M,THR_N)\nTensor tCgC = thr_mma.partition_C(gC);                                                             // (VEC,THR_M,THR_N)\nTensor tCgAux = thr_mma.partition_C(gAux);                                                         // (VEC,THR_M,THR_N)\n```\n\nLANGUAGE: C++\nCODE:\n```\nTensor my_tensor = make_tensor<Type>(Layout<Shape<_2,_2>{}, Stride<_1,_2>>{});                           // (2,2):(1,2)\n    \n// Related tensors\nTensor my_tensor1 = make_tensor<Type>(ThisIsAVeryComplicatedLayoutWithAVeryLongName);         // ((Mode0_0,Mode0_1,Mode0_2),Mode1,Mode2,Mode3)\nTensor my_tensor2_related = make_tensor<Type>(ThisIsAVeryComplicatedLayoutWithAVeryLongName); // ((Mode0_0,Mode0_1,Mode0_2),Mode1,Mode2,Mode3)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf8_mxf8_f32_f32_f32_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f32_f32_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f32_f32_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f32_f32_q_tnt.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_void_f32_q_tnt.cu\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f32_f32_q_tnn.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_void_f32_q_tnn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Test Case for Prefetching in CUTLASS\nDESCRIPTION: This CMake command sets the test case parameters for the weight prefetch example. It defines the matrix dimensions M, N, and K along with the number of iterations to be performed during the test. This allows the user to quickly set parameters for running and testing the CUTLASS example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_PREFETCH_CASE --m=8192 --n=64 --k=8192 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Disabling Overlap and Prefetch in GEMM Example\nDESCRIPTION: These commands demonstrate how to disable overlap and modify overlap and prefetch ratios within the GEMM with weight prefetch example.  The `--o` flag controls the overlap ratio, and the `--p` flag controls the prefetch ratio. Setting `--o` and `--p` to `-1.0` disables both features, while setting them to other values adjusts their respective behaviors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\necho \"Without overlap and prefetch\"\n./63_hopper_gemm_with_weight_prefetch --o=-1.0 --p=-1.0\n\necho \"Overlap ratio of 0.5, best effort prefetch\"\n./63_hopper_gemm_with_weight_prefetch --o=0.5 --p=-1.0\n\necho \"Overlap ratio of 0.8, prefetch ratio of 0.7\"\n./63_hopper_gemm_with_weight_prefetch --o=0.8 --p=0.7\n```\n\n----------------------------------------\n\nTITLE: Calculating Increments for Optimized Iterator in C++\nDESCRIPTION: This C++ code snippet shows how the pointer increments are calculated within the `Conv2dFpropActivationIteratorOptimizedParams` struct. These increments are precomputed on the host and used by the optimized iterator to efficiently move to the next filter position.  It calculates offsets for S, R and C dimensions based on the problem size, dilation, and strides.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\n// cutlass/conv/threadblock/conv2d_params.h\nstruct Conv2dFpropActivationIteratorOptimizedParams<layout::TensorNHWC> {\n ...\n// next S\ninc_next[0] = conv_sign * (int64_t(layout.stride()[0]) * problem_size.dilation_w) * element_size_bits / 8;\n\n// next R\ninc_next[1] = conv_sign * (\n    int64_t(layout.stride()[1]) * problem_size.dilation_h\n    - (problem_size.S - 1) * layout.stride()[0] * problem_size.dilation_w\n  ) * element_size_bits / 8;\n\n// next C\ninc_next[2] = (\n    threadblock_shape.column() * problem_size.split_k_slices\n    - conv_sign * int64_t(problem_size.R - 1) * layout.stride()[1] * problem_size.dilation_h\n    - conv_sign * int64_t(problem_size.S - 1) * layout.stride()[0] * problem_size.dilation_w\n  ) * element_size_bits / 8;\n\n ...\n}\n```\n\n----------------------------------------\n\nTITLE: GEMM Initialization and Tensor Creation Python\nDESCRIPTION: Initializes the CUTLASS environment by importing necessary packages and constructing input and output tensors (A, B, C, D) using NumPy. The tensors are initialized with random values and the data type is set to float16.  It also sets the alpha and beta scaling factors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport random\n\nimport cutlass\n\n# This controls whether the C++ GEMM declaration will be printed at each step. \n# Set to `False` to omit this information.\nprint_module = True\n\nm = 128\nn = m\nk = m\n\ndtype = np.float16\ntype_A = np.float16\ntype_B = np.float16\ntype_C = np.float16\ntype_D = np.float16\n\nnp.random.seed(1234)\nrandom.seed(1234)\nscope_min = -4\nscope_max = 4\ntensor_A = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, k)).astype(type_A))\ntensor_B = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(k, n)).astype(type_B))\ntensor_C = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, n)).astype(type_C))\n\nalpha = np.float16(1.)\nbeta = np.float16(0.)\n\ntensor_D = np.zeros(tensor_C.shape).astype(type_D)\n```\n\n----------------------------------------\n\nTITLE: Check Kernel Implementation Compatibility (C++)\nDESCRIPTION: This snippet defines a static method `can_implement` within the `kernel::GemmUniversal` specialization. It returns true if the kernel can execute the provided GEMM arguments, allowing the device adapter to determine if the kernel is suitable for the given input. It validates the provided `Arguments` structure.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n// Returns true if the kernel can execute the provided GEMM arguments.\nstatic bool\ncan_implement(Arguments const& args);\n```\n\n----------------------------------------\n\nTITLE: Build PyTorch CUDA Extension (bash)\nDESCRIPTION: This command builds the generated PyTorch CUDA extension using the `setup.py` script. It specifies the compute capability of the target device using the `TORCH_CUDA_ARCH_LIST` environment variable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTORCH_CUDA_ARCH_LIST=\"8.0\" python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Static Persistent Kernel Definition C++\nDESCRIPTION: This pseudo-code shows a static persistent kernel. It initializes common data structures and iteratively computes based on coordinates fetched by a static tile scheduler.  The loop continues as long as the fetched coordinate is valid.  Requires `staticTileScheduler` object.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_cluster_launch_control.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// Static Persistent Kernel\n__device__ static_persistent_kernel(...) {\n  setup_common_data_structures(...);\n  dim3 workCoordinates = blockIdx;\n  bool isValidId;\n  do {\n    coordinate_specific_compute(workCoordinates);\n    std::tie(isValidId, workCoordinates) = staticTileScheduler.fetch_next_work();\n  } while (isValidId);\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Tile Sizes (CuTe)\nDESCRIPTION: This code defines the static tile sizes for the CTAs (Cooperative Thread Arrays) in the GEMM operation. It sets the tile dimensions for M, N, and K using `Int<>` and combines them into a shape using `make_shape` to create the CTA tiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n  // Define CTA tile sizes (static)\n  auto bM = Int<128>{};\n  auto bN = Int<128>{};\n  auto bK = Int<  8>{};\n  auto cta_tiler = make_shape(bM, bN, bK);  // (BLK_M, BLK_N, BLK_K)\n```\n\n----------------------------------------\n\nTITLE: Add F16-F16 Convolution Test Executable (SM80)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with F16 input, F16 output, and F16 accumulation on SM80 architecture, conditionally compiled based on the `CUTLASS_NVCC_MAX_ARCH` variable. It includes source files for forward propagation, data gradient, and weight gradient.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n  \n  # Conv - F16 input, F16 output, F16 accumulation \n  cutlass_test_unit_add_executable(\n    cutlass_test_unit_conv_device_tensorop_f16_sm80\n  \n    conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu\n    conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu \n    conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu \n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Writeable Random Access Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept for writing tiles to memory with random access based on a tile offset. It inherits from `RandomAccessTileIteratorConcept` and `WriteableContiguousTileIteratorConcept`. It provides a `store` method that accepts a `Fragment` and a `TensorCoord` tile offset.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_11\n\nLANGUAGE: c++\nCODE:\n```\n/// Stores a fragment with a logical coordinate offset in units of whole tiles.\nstruct WriteableRandomAccessTileIteratorConcept : \n  public RandomAccessTileIteratorConcept, \n  public WriteableContiguousTileIteratorConcept {\n  \n  /// Stores a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE \n  void store(\n    Fragment const &frag,                       ///< fragment to store to the location pointed to by the tensor\n    TensorCoord const &tile_offset);            ///< stores a tile with a given offset from the current iterator\n};\n```\n\n----------------------------------------\n\nTITLE: GEMM Index Relation\nDESCRIPTION: This describes how output tensor indices y[n, p, q, k] relate to output matrix indices Cij in the GEMM formulation. The formulas map tensor indices to matrix indices and vice versa, enabling the computation to be expressed as a matrix multiplication.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ny[n, p, q, k] = Cij\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for INT4/BF16 GEMM (CMake)\nDESCRIPTION: Uses the `cutlass_example_add_executable` macro to create an executable for INT4/BF16 grouped GEMM tests. It links the source file and includes test configurations. The executable is named `69_hopper_int4_bf16_grouped_gemm`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  69_hopper_int4_bf16_grouped_gemm\n  69_hopper_int4_bf16_grouped_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n  TEST_DIRECT_BATCHED\n  TEST_SCALE_PERCOL\n  TEST_SCALE_GROUP\n)\n```\n\n----------------------------------------\n\nTITLE: Define Convolution Fprop Kernel in CUTLASS\nDESCRIPTION: This code snippet demonstrates how to define an implicit GEMM convolution forward propagation (fprop) kernel using the `cutlass::conv::kernel::DefaultConv2dFprop` template. It takes several template arguments that specify the data types, layouts, opcode class, SM architecture, tile shapes, and other parameters for the kernel. The purpose is to create a generic convolution kernel that can be specialized for different configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\n/// Define an Implicit GEMM convolution forward propagation (fprop) kernel\nusing Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<\n  ElementInputA,                                          // data type of element a (mapped to activation for fprop)                         \n  LayoutInputA,                                           // layout of element a (mapped to activation for fprop)\n  ElementInputB,                                          // data type of element b (mapped to filters for fprop)  \n  LayoutInputB,                                           // layout of element b (mapped to filters for fprop)\n  ElementC,                                               // data type of element c (mapped to output for fprop)\n  LayoutC,                                                // layout of element c (mapped to output for fprop)\n  ElementAccumulator,                                     // data type of internal accumulation\n  MMAOp,                                                  // opcode class tag\n  SmArch,                                                 // target SM architecture\n  ThreadblockShape,                                       // shape of threadblock tile\n  WarpShape,                                              // shape of warp-level GEMM tile\n  InstructionShape,                                       // shape of target math instruction\n  EpilogueOp,                                             // epilogue operator \n  SwizzleThreadBlock,                                     // optional function to reorder threadblocks for locality\n  NumStages,                                              // number of pipeline stages in threadblock-scoped GEMM\n  cutlass::arch::OpMultiplyAddSaturate,                   // math operation on data of element a and b\n  cutlass::conv::IteratorAlgorithm::kOptimized            // global memory iterator algorithm  \n>::Kernel\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf4xmxf8)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf4_mxf8_bf16_bf16_tn_layout.cu`, `mxf4_mxf8_bf16_bf16_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf4xmxf8\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf4_mxf8_bf16_bf16_tn_layout.cu\n  mxf4_mxf8_bf16_bf16_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Auto Kernel Dispatch Policy\nDESCRIPTION: This auto-dispatch policy selects the most efficient kernel for GEMM operations based on Mma Tile Size, data type, and layout combinations. It automatically chooses the instruction kind (mxf8f6f4, mxf4, nvf4mxf4) and utilizes either 1 or 2 SMs (Streaming Multiprocessors) using `tcgen05.mma(.sp)` instructions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::gemm::collective::KernelScheduleAuto`\n```\n\n----------------------------------------\n\nTITLE: Adding Blackwell Subdirectory (CMake)\nDESCRIPTION: This command adds the 'blackwell' subdirectory to the current CMake project. This allows CMake to find and process the CMakeLists.txt file within the 'blackwell' directory, integrating its build targets into the overall project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(blackwell)\n```\n\n----------------------------------------\n\nTITLE: Define Thread Layouts (GEMM_TN) C++\nDESCRIPTION: This code defines thread layouts `tA` and `tB` for partitioning data across threads in the `gemm_tn` kernel. These are K-major thread layouts and specify the thread grid dimensions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_16\n\nLANGUAGE: c++\nCODE:\n```\n  // Define thread layouts (static)\n  auto tA = make_layout(make_shape(Int<32>{},Int<8>{}), LayoutRight{});  // (m,k) -> thr_idx; k-major\n  auto tB = make_layout(make_shape(Int<32>{},Int<8>{}), LayoutRight{});  // (n,k) -> thr_idx; k-major\n```\n\n----------------------------------------\n\nTITLE: SIMT Math Instructions Warp-level Mma Template\nDESCRIPTION: This defines the template for warp-level matrix multiply operators targeting CUDA Cores and SIMT math instructions. The `Policy` type specifies implementation-level details that may affect performance or internal implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\n/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.\ntemplate <\n  /// Size of the Gemm problem - concept: gemm::GemmShape<>\n  typename Shape_,\n  /// Data type of A elements\n  typename ElementA_,\n  /// Layout of A matrix (concept: MatrixLayout)\n  typename LayoutA_,\n  /// Data type of B elements\n  typename ElementB_,\n  /// Layout of B matrix (concept: MatrixLayout)\n  typename LayoutB_,\n  /// Element type of C matrix\n  typename ElementC_,\n  /// Layout of C matrix (concept: MatrixLayout)\n  typename LayoutC_,\n  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)\n  typename Policy_,\n  /// Used for partial specialization\n  typename Enable = bool\n>\nclass MmaSimt;\n```\n\n----------------------------------------\n\nTITLE: Defining Readable Random Access Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept that supports reading tiles from memory with random access based on a tile offset. It inherits from `RandomAccessTileIteratorConcept` and `ReadableTileIteratorConcept`.  It provides a `load` method that accepts a `Fragment` and a `TensorCoord` tile offset.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\n/// Loads a fragment with a logical coordinate offset in units of whole tiles.\nstruct ReadableRandomAccessTileIteratorConcept : \n  public RandomAccessTileIteratorConcept, \n  public ReadableTileIteratorConcept {\n\n  /// Loads a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE\n  void load(\n    Fragment &frag,                             ///< fragment to load from the tensor\n    TensorCoord const &tile_offset);            ///< loads a tile with a logical offset in units of whole tiles\n};\n```\n\n----------------------------------------\n\nTITLE: MatrixCoord Structure Definition C++\nDESCRIPTION: This code snippet defines a `MatrixCoord` structure that inherits from `Coord<2>`. It provides named accessors for the row and column indices, improving code readability when working with matrix coordinates.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nstruct MatrixCoord : public Coord<2> {\n  Index & row();\n  Index & column();\n};\n```\n\n----------------------------------------\n\nTITLE: Test CUTLASS Installation\nDESCRIPTION: This code snippet verifies the successful installation of the CUTLASS Python interface. It performs a simple GEMM operation using the installed cutlass package.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport cutlass\nimport numpy as np\n\nplan = cutlass.op.Gemm(element=np.float16, layout=cutlass.LayoutType.RowMajor)\nA, B, C, D = [np.ones((128, 128), dtype=np.float16) for i in range(4)]\nplan.run(A, B, C, D)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS INT4 BF16 GEMM Example\nDESCRIPTION: This CMake code adds an executable target for a CUTLASS example that tests int4-bf16 GEMM on the Hopper architecture. It links the source file `55_hopper_int4_bf16_gemm.cu` and includes various test configurations defined earlier, such as direct batched conversion and different scaling modes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n    55_hopper_int4_bf16_gemm\n    55_hopper_int4_bf16_gemm.cu\n    TEST_COMMAND_OPTIONS\n    TEST_DIRECT_BATCHED\n    TEST_SCALE_PERCOL\n    TEST_SCALE_GROUP\n    TEST_SCALE_RESIDUE\n    # TEST_ALPHA_BETA\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating build directory and running CMake - Bash\nDESCRIPTION: This snippet shows how to create a build directory within the CUTLASS project and then run CMake to generate the build files. The `CUTLASS_NVCC_ARCHS` CMake variable can be used to specify the target CUDA architecture versions. The example provided compiles for NVIDIA's Ampere Architecture (CUDA architecture 8.0).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir build && cd build\n\n$ cmake .. -DCUTLASS_NVCC_ARCHS=80               # compiles for NVIDIA's Ampere Architecture\n```\n\n----------------------------------------\n\nTITLE: CMake Project Setup and CUTLASS Dependency\nDESCRIPTION: This snippet sets up the CMake project, specifies the minimum required CMake version, defines the project name, and attempts to find the CUTLASS package using `find_package`. It also handles specifying the CUTLASS directory if it's not in a standard location.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/60_cutlass_import/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n\nproject(cutlass_import_example VERSION 0.2 LANGUAGES CXX CUDA)\n\nif (CUTLASS_DIR)\n  message(STATUS \"Using CUTLASS specified at ${CUTLASS_DIR}.\")\n  list(APPEND CMAKE_PREFIX_PATH ${CUTLASS_DIR})\nendif()\n\nfind_package(NvidiaCutlass 2.0 REQUIRED)\n\nmessage(STATUS \"CUTLASS: ${NvidiaCutlass_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper TMA Load Unit Test Executable with CMake\nDESCRIPTION: This CMake snippet builds the TMA (Tensor Memory Accelerator) load unit test executable, `cutlass_test_unit_cute_hopper_tma_load`, using the `tma_load.cu` source file.  It utilizes the `cutlass_test_unit_add_executable` custom function to encapsulate the build process, creating a part of the CUTLASS Hopper test suite.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_tma_load\n  tma_load.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Epilogue Options (CMake)\nDESCRIPTION: Defines CMake variables for testing different epilogue options, specifying alpha and beta values, and setting iterations to 0 for correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: GEMM F16 SplitK Serial Example\nDESCRIPTION: This example demonstrates a GEMM operation using float16 with split-K serial execution. The example configures block size, tensor layouts, swizzle functions, and split-K value. The accumulation type is float32.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 16 -ta float16 -tb float16 -tc float16 -tacc float32 -m multiply_add -op TensorOp -b 128 128 64 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb ColumnMajor -ab 8 -lc RowMajor -ac 8 -te float32 -ep LinearCombination -sw IdentitySwizzle2 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 2\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f32 Sparse GEMM Compressor Test\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_add_executable` function (presumably defined elsewhere) to create an executable named `cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f32`. The executable is built from the source file `sm90_sparse_gemm_compressor_f32.cu`, which likely contains the test implementation for the sparse GEMM compressor with f32 precision.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/device/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f32\n\n    sm90_sparse_gemm_compressor_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Compiling CUTLASS Profiler\nDESCRIPTION: This command compiles the CUTLASS Profiler, a command-line tool for testing and profiling CUTLASS kernels. It requires the `make` utility to be installed and configured with the appropriate build environment for CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ make cutlass_profiler -j\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executable in CMake\nDESCRIPTION: This snippet adds an executable to the CMake project using the `cutlass_example_add_executable` function. The function takes the name of the executable and the source file as input. This CMake macro simplifies the process of creating an executable from a source file within the CUTLASS project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/37_gemm_layernorm_gemm_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  37_gemm_layernorm_gemm_fusion\n  gemm_layernorm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable with Broadcast (CMake)\nDESCRIPTION: This CMake function call creates an executable named '47_ampere_gemm_universal_streamk_broadcast' using the source file 'ampere_gemm_universal_streamk_broadcast.cu'.  It includes 'TEST_COMMAND_OPTIONS' and 'TEST_COMMAND_00' which defines the command line arguments for testing the execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/47_ampere_gemm_universal_streamk/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  47_ampere_gemm_universal_streamk_broadcast\n  ampere_gemm_universal_streamk_broadcast.cu\n  TEST_COMMAND_OPTIONS\n  TEST_COMMAND_00\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Mixed Data Type GEMM (CMake)\nDESCRIPTION: Uses a custom CMake macro `cutlass_example_add_executable` to create an executable for mixed data type grouped GEMM tests. It links the specified source file and includes various test configurations defined earlier. The executable name is `69_hopper_mixed_dtype_grouped_gemm`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  69_hopper_mixed_dtype_grouped_gemm\n  69_hopper_mixed_dtype_grouped_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n  TEST_DIRECT_BATCHED\n  TEST_SCALE_PERCOL\n  TEST_SCALE_GROUP\n)\n```\n\n----------------------------------------\n\nTITLE: Possibly Non-Exhaustive Function Example C++\nDESCRIPTION: This example illustrates a function where the conditional branches might not be exhaustive.  The absence of an unadorned `else` clause indicates that some cases may not be handled, prompting a review of the function logic. If branches are meant to be exhaustive, add an `else` with `static_assert`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<class T>\nconstexpr auto possibly_nonexhaustive(T t) {\n  if constexpr (some_condition_v<T>) {\n    return some_function(t);\n  }\n  else if constexpr (another_condition_v<T>) {\n    return another_function(t);\n  }\n \n  // NOTE lack of unadorned \"else\" here\n}\n```\n\n----------------------------------------\n\nTITLE: Installing CUTLASS Profiler Executable in CMake\nDESCRIPTION: Installs the `cutlass_profiler` executable to the `${CMAKE_INSTALL_BINDIR}` directory.  It also exports the target as part of the `NvidiaCutlass` export set.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(\n  TARGETS cutlass_profiler\n  EXPORT NvidiaCutlass\n  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n  )\n```\n\n----------------------------------------\n\nTITLE: Conv2d Fprop F16 Fixed Channels Configuration\nDESCRIPTION: This example demonstrates a device convolution operation with fprop and fixed channels using float16 data type and TensorOp. It utilizes implicit GEMM and specifies tensor layouts in NHWC format.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 16 -ta float16 -tb float16 -tc float16 -tacc float32 -m multiply_add -op TensorOp -b 128 128 64 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 8 -lb TensorNHWC -ab 8 -lc TensorNHWC -ac 8 -te float32 -ep LinearCombination -sw IdentitySwizzle2 -co fprop -st Strided -ia fixed_channels -sm Serial -k 1 -nhwc 1 8 8 8 -krsc 16 3 3 8 -pad 1 1 1 1 -stride 2 2 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: Executing Copy with TiledCopy (C++)\nDESCRIPTION: This code executes the data copy operation using the `TiledCopy` instance `copy_a`. The `cute::copy` function takes the `TiledCopy` object, the partitioned source tensor `tAgA`, and the destination fragment tensor `tArA` as arguments.  This performs the actual data transfer according to the defined tiling and copy atom.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_28\n\nLANGUAGE: cpp\nCODE:\n```\ncute::copy(copy_a, tAgA, tArA);\n```\n\n----------------------------------------\n\nTITLE: Creating Predicate Tensors for Thread-Parallelized Tiles\nDESCRIPTION: This snippet shows how to create predicate tensors corresponding to tiles of global matrices A and B that have been partitioned over threads. It uses the sizes of the partitioned tensors to create boolean tensors, utilizing strides to optimize for predication only over the leftmost dimension.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nTensor tApA = make_tensor<bool>(make_shape (size<0>(tAgA), size<1>(tAgA)),\n                                make_stride(     Int<1>{},      Int<0>{}));\nTensor tBpB = make_tensor<bool>(make_shape (size<0>(tBgB), size<1>(tBgB)),\n                                make_stride(     Int<1>{},      Int<0>{}));\n```\n\n----------------------------------------\n\nTITLE: Defining Direct Batched Test Parameters (CMake)\nDESCRIPTION: Defines CMake variables for direct batched tests, specifying matrix dimensions (m, n, k), mode, and setting iterations to 0.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_DIRECT_BATCHED --m=2048 --n=5120 --k=8192 --mode=0 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Defining Executable: Ampere TF32 TensorOp GEMM (CMake)\nDESCRIPTION: This snippet defines a CUTLASS example executable using CMake's `cutlass_example_add_executable` macro. It creates an executable named `14_ampere_tf32_tensorop_gemm` built from the source file `ampere_tf32_tensorop_gemm.cu`. This example showcases a tensor operation GEMM kernel for the Ampere architecture utilizing TF32 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/14_ampere_tf32_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  14_ampere_tf32_tensorop_gemm\n  ampere_tf32_tensorop_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Profiler: Convolution Arguments\nDESCRIPTION: This bash snippet shows the available command-line arguments for the CUTLASS profiler when used for convolution operations. These arguments control aspects of the convolution such as input dimensions, filter sizes, padding, strides, dilation, data types, and hardware-specific configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --help --operation=Conv2d\n\nConv2d\n\n  [enum]      --conv_kind                                       Convolutional operator (fprop, dgrad, wgrad)\n  [int]       --n,--input_n                                     Input N dimension of the Conv2d problem space\n  [int]       --h,--input_h                                     Input H dimension of the Conv2d problem space\n  [int]       --w,--input_w                                     Input W dimension of the Conv2d problem space\n  [int]       --c,--input_c                                     Input C dimension of the Conv2d problem space\n  [int]       --k,--filter_k                                    Filter K dimension of the Conv2d problem space\n  [int]       --r,--filter_r                                    Filter R dimension of the Conv2d problem space\n  [int]       --s,--filter_s                                    Filter S dimension of the Conv2d problem space\n  [int]       --p,--output_p                                    Output P dimension of the Conv2d problem space\n  [int]       --q,--output_q                                    Output Q dimension of the Conv2d problem space\n  [int]       --g,--groups                                      Number of convolution groups\n  [int]       --pad_h                                           Padding in H direction\n  [int]       --pad_w                                           Padding in W direction\n  [int]       --stride_h                                        Stride in H direction\n  [int]       --stride_w                                        Stride in W direction\n  [int]       --dilation_h                                      Dilation in H direction\n  [int]       --dilation_w                                      Dilation in W direction\n  [tensor]    --Activation                                      Tensor storing the Activation operand\n  [tensor]    --Filter                                          Tensor storing the Filter operand\n  [tensor]    --Output                                          Tensor storing the Output operand\n  [enum]      --conv_mode                                       Convolution filter mode (conv, cross)\n  [enum]      --iterator_algorithm,--iterator_algo              Convolution iterator algorithm (analytic, optimized)\n  [scalar]    --alpha,--epilogue::alpha                         Epilogue scalar alpha\n  [scalar]    --beta,--epilogue::beta                           Epilogue scalar beta\n  [enum]      --split_k_mode,--split-k-mode                     SplitK mode for serial or parallel reduction (serial, parallel)\n  [int]       --split_k_slices,--split-k-slices                 Number of partitions of K dimension\n  [enum]      --eq_gemm_provider,--eq-gemm-provider             Enable profiling equivalent gemm by the following providers (cutlass)\n  [enum]      --op_class,--opcode-class                         Class of math instruction (simt, tensorop, wmmatensorop, wmma)\n  [enum]      --accum,--accumulator-type                        Math instruction accumulator data type\n  [int]       --cta_m,--threadblock-shape::m                    Threadblock shape in the M dimension\n  [int]       --cta_n,--threadblock-shape::n                    Threadblock shape in the N dimension\n  [int]       --cta_k,--threadblock-shape::k                    Threadblock shape in the K dimension\n  [int]       --cluster_m,--cluster-shape::m                    Cluster shape in the M dimension\n  [int]       --cluster_n,--cluster-shape::n                    Cluster shape in the N dimension\n  [int]       --cluster_k,--cluster-shape::k                    Cluster shape in the K dimension\n  [int]       --cluster_m_fallback,--cluster-shape-fallback::m  Fallback cluster shape in the M dimension\n  [int]       --cluster_n_fallback,--cluster-shape-fallback::n  Fallback cluster shape in the N dimension\n  [int]       --cluster_k_fallback,--cluster-shape-fallback::k  Fallback cluster shape in the K dimension\n  [int]       --stages,--threadblock-stages                     Number of stages of threadblock-scoped matrix multiply\n  [int]       --warps_m,--warp-count::m                         Number of warps within threadblock along the M dimension\n  [int]       --warps_n,--warp-count::n                         Number of warps within threadblock along the N dimension\n  [int]       --warps_k,--warp-count::k                         Number of warps within threadblock along the K dimension\n  [int]       --inst_m,--instruction-shape::m                   Math instruction shape in the M dimension\n  [int]       --inst_n,--instruction-shape::n                   Math instruction shape in the N dimension\n  [int]       --inst_k,--instruction-shape::k                   Math instruction shape in the K dimension\n  [int]       --min_cc,--minimum-compute-capability             Minimum device compute capability\n  [int]       --max_cc,--maximum-compute-capability             Maximum device compute capability\n```\n\n----------------------------------------\n\nTITLE: Defining Readable Contiguous Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept for reading from contiguous memory with an optional pointer offset. It combines the `ReadableTileIteratorConcept` and `ContiguousMemoryTileIterator` concepts. It provides the `load_with_pointer_offset` method to load a fragment with an additional linear offset.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n/// Union of the following tile iterator concepts:\n///\n///   - ReadableTileIteratorConcept\n///   - ContiguousMemoryTileIterator\n///\nstruct ReadableContiguousTileIteratorConcept : \n  public ReadableTileIteratorConcept, \n  public ContiguousMemoryTileIterator {\n\n  /// Loads a fragment from memory with additional logical offset\n  CUTLASS_DEVICE\n  void load_with_pointer_offset(\n    Fragment &frag,                             ///< fragment to load from the tensor\n    Index pointer_offset);                      ///< loads a tile with a linear offset\n};\n```\n\n----------------------------------------\n\nTITLE: Using Scoped Enums in C++\nDESCRIPTION: This code snippet demonstrates the usage of scoped enums (enum classes) in C++. It shows how to define an enumerated type with a specific scope, using capital letters for the type name and prefixing enumerators with 'k'.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nenum class MatrixOperation {\n  kNone,\n  kTranspose,\n  kConjugate,\n  kHermitian\n};\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensors for CUTLASS GEMM in Python\nDESCRIPTION: This snippet initializes input and output tensors (A, B, C, D) using NumPy for a GEMM operation with specific dimensions (m, n, k), data types (float16), and random values. It also sets the alpha and beta scalars used in the GEMM computation. The `print_module` variable controls whether the generated C++ code is printed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport random\n\nimport cutlass\n\n# This controls whether ther C++ GEMM declaration will be printed at each step. Set to `false` to\n# omit this information.\nprint_module = True\n\nm = 128\nn = m\nk = m\n\ndtype = np.float16\ntype_A = np.float16\ntype_B = np.float16\ntype_C = np.float16\ntype_D = np.float16\n\nnp.random.seed(1234)\nrandom.seed(1234)\nscope_min = -4\nscope_max = 4\ntensor_A = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, k)).astype(type_A))\ntensor_B = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(k, n)).astype(type_B))\ntensor_C = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, n)).astype(type_C))\n\nalpha = np.float16(1.)\nbeta = np.float16(0.)\n\ntensor_D = np.zeros(tensor_C.shape).astype(type_D)\n```\n\n----------------------------------------\n\nTITLE: Writing NVRTC Environment File (CMake)\nDESCRIPTION: Writes the generated NVRTC environment code to a file named `environment.cpp`. This file includes the generated header strings and defines the `cutlass::nvrtc` namespace containing the header data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nfile(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/cutlass/nvrtc/environment.cpp\"\n  \"#include <cutlass/nvrtc/environment.h>\\n\"\n  \"${NVRTC_INCLUDES_HEADERS}\"\n  \"\\n\"\n  \"namespace cutlass {\\n\"\n  \"namespace nvrtc {\\n\"\n  \"\\n\"\n  \"${NVRTC_INCLUDES_STRINGS}\"\n  \"\\n\"\n  \"${NVRTC_INCLUDES_NAMES}\"\n  \"\\n\"\n  \"} // namespace nvrtc\\n\"\n  \"} // namespace cutlass\\n\"\n)\n```\n\n----------------------------------------\n\nTITLE: Get Grid Shape in Threadblocks (C++)\nDESCRIPTION: This code snippet defines a static method `get_grid_shape` within the `kernel::GemmUniversal` specialization. It returns a `dim3` representing the grid shape in terms of threadblocks. This allows the kernel to define its own grid planning logic, especially useful for schedules like persistent kernels. The `Params` object is used to determine the appropriate grid shape.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\n// Returns a dim3 representing the grid shape in terms of threadblocks.\nstatic dim3\nget_grid_shape(Params const& params);\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Targets for Convolution Tests\nDESCRIPTION: This snippet defines custom targets for CUTLASS convolution unit tests, specifically for device implementations using SIMT cores. These targets are defined with a dependency on the corresponding SIMT implementation. This is the base target extended by architecture specific implementations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_conv_device\n  DEPENDS\n  cutlass_test_unit_conv_device_simt\n)\n\n add_custom_target(\n  test_unit_conv_device\n  DEPENDS\n  test_unit_conv_device_simt\n)\n```\n\n----------------------------------------\n\nTITLE: Define 3D CUTLASS executable\nDESCRIPTION: This snippet defines another CUTLASS executable, similarly using `cutlass_example_add_executable`. It creates an executable named `25_ampere_3d_fprop_mainloop_fusion` from the `ampere_3d_fprop_mainloop_fusion.cu` CUDA source file. The executable likely implements a 3D variant of the forward propagation mainloop fusion.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/25_ampere_fprop_mainloop_fusion/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CUTLASS\nCODE:\n```\ncutlass_example_add_executable(\n  25_ampere_3d_fprop_mainloop_fusion\n  ampere_3d_fprop_mainloop_fusion.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating Executable for Transposed Conv2D Example with CUTLASS\nDESCRIPTION: This CMake function creates an executable named `34_transposed_conv2d` from the CUDA source file `34_transposed_conv2d.cu`. It leverages the `cutlass_example_add_executable` macro provided by the CUTLASS build system to simplify the process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/34_transposed_conv2d/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  34_transposed_conv2d\n  34_transposed_conv2d.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Mixed-Dtype GEMM Example\nDESCRIPTION: This CMake code adds an executable target for a CUTLASS example that tests mixed-dtype GEMM on the Hopper architecture. It links the source file `55_hopper_mixed_dtype_gemm.cu` and includes various test configurations defined earlier, such as direct batched conversion and different scaling modes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  55_hopper_mixed_dtype_gemm\n  55_hopper_mixed_dtype_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_DIRECT_BATCHED\n  TEST_SCALE_PERCOL\n  TEST_SCALE_ZERO_PERCOL\n  TEST_SCALE_GROUP\n  TEST_SCALE_ZERO_GROUPED\n  TEST_SCALE_RESIDUE\n  TEST_SCALE_ZERO_RESIDUE\n  # TEST_ALPHA_BETA\n  )\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: Links the 'cutlass_tools_util_includes' library against 'cublas' if the 'CUTLASS_ENABLE_CUBLAS' option is enabled.  This utilizes a generator expression to conditionally link the library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/util/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(\n  cutlass_tools_util_includes\n  INTERFACE\n \t$<$<BOOL:${CUTLASS_ENABLE_CUBLAS}>:cublas>\n  )\n```\n\n----------------------------------------\n\nTITLE: GEMM Matrix Indexing Conversion\nDESCRIPTION: These equations define the mapping between the tensor index (n, p, q, k) and the matrix indices i and j in the GEMM representation. They provide a way to translate tensor operations into matrix operations for efficient computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ni = q + Q * (p + P * n)\nj = k\n```\n\n----------------------------------------\n\nTITLE: Creating a TiledMMA from MMA_Atom in C++\nDESCRIPTION: This code snippet creates a TiledMMA object from an MMA_Atom, defining the layout of atoms and the tile size. It demonstrates a simple TiledMMA configuration with a single atom and a natural tile size.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nMMA_Atom mma = MMA_Atom<SM70_8x8x4_F32F16F16F32_NT>{};\nprint_latex(mma);\n```\n\nLANGUAGE: cpp\nCODE:\n```\nTiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                                  Layout<Shape<_1,_1,_1>>{},   // Layout of Atoms\n                                  Tile<_8,_8,_4>{});           // Tiler\n    print_latex(mma);\n```\n\n----------------------------------------\n\nTITLE: Defining Complete CLayout for HMMA Accumulators\nDESCRIPTION: This code defines the complete CLayout, including both thread and value strides, to map logical thread and value IDs to (m, n) coordinates in the C matrix.  It considers the positions of values V0 to V7 within each thread. The strides are calculated based on the column-major encoding of (m, n) coordinates.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n  // (T8,V8) -> (m,n)\n  using CLayout = Layout<Shape <Shape <_2, _2,_2>, Shape <_2,_2, _2>>,\n                         Stride<Stride<_1,_16,_4>, Stride<_8,_2,_32>>>;\n```\n\n----------------------------------------\n\nTITLE: Construct Fusion Operation for Epilogue in CUTLASS C++\nDESCRIPTION: This code snippet demonstrates how to construct a fusion operation for the epilogue in CUTLASS using `LinCombBlockScaleFactor`. It defines a fusion operation that combines linear combination with block scale factors. The snippet specifies the SFDVectorSize, element types (ElementD, ElementCompute, ElementSFD, ElementC), and GmemLayoutSFD to configure the fusion operation. This fusion operation is then passed into the epilogue builder.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\n//\n// Construct FusionOperation\n//\nconstexpr int SFDVectorSize = 16;\n// Define the fusion operation applied during epilogue\nusing FusionOperation = cutlass::epilogue::fusion::LinCombBlockScaleFactor<\n    SFDVectorSize,\n    ElementD, ElementCompute, \n    ElementSFD, GmemLayoutSFD,\n    ElementC\n  >;\n\nusing CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<\n    cutlass::arch::Sm100, cutlass::arch::OpClassBlockScaledTensorOp,      // Arch and Tensorop spec\n    MmaTileShape_MNK, ClusterShape_MNK,                                   // MMA tile shape, and cluster shape\n    cutlass::epilogue::collective::EpilogueTileAuto,                      // Epilogue subtile shape. Auto will find a suitable tile shape\n    ElementAccumulator, ElementCompute,                                   // Mma instr's accumulator type and compute precision for epilogue\n    ElementC, GmemLayoutC, AlignC,                                        // C tensor description\n    ElementD, GmemLayoutD, AlignD,                                        // D tensor description\n    cutlass::epilogue::TmaWarpSpecialized2Sm                              // Epilogue schedule policy\n    FusionOperation                                                       // <================================== Pass the fusion config into epilogue builder.\n  >::CollectiveOp;\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS NVRTC Library (CMake)\nDESCRIPTION: Creates a static library named `cutlass_nvrtc` that includes the generated environment files and header files. It uses a custom CMake function `cutlass_add_library`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_add_library(cutlass_nvrtc STATIC\n  cutlass/nvrtc/environment.h\n  ${GENERATED_SOURCE_FILES}\n  ${GENERATED_HEADER_FILES}\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable (CMake)\nDESCRIPTION: This CMake code snippet uses the `cutlass_example_add_executable` function to create an executable named `17_fprop_per_channel_bias` from the CUDA source file `fprop_per_channel_bias.cu`. The function likely handles necessary configurations and dependencies for building the CUTLASS example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/17_fprop_per_channel_bias/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  17_fprop_per_channel_bias \n  fprop_per_channel_bias.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Dense BlockScaled Kernel Dispatch (1SM)\nDESCRIPTION: This policy uses 1 SM `tcgen05.mma` instruction and automatically selects the optimal instruction kind (mxf8f6f4, mxf4, nvf4mxf4).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n`KernelTmaWarpSpecialized1SmBlockScaledSm100`\n```\n\n----------------------------------------\n\nTITLE: Defining Executable for CUTLASS Convolution Dgrad Device Tensorop SM90 Tests (CMake)\nDESCRIPTION: This snippet defines an executable target `cutlass_test_unit_conv_dgrad_device_tensorop_sm90` using `cutlass_test_unit_add_executable` in CMake. It specifies that batch sources are enabled (`BATCH_SOURCES ON`) and sets the batch size to 1 (`BATCH_SIZE 1`).  The subsequent lines list the CUDA source files to be compiled into the executable, covering 1D, 2D, and 3D convolutions with different tensor operation configurations on the SM90 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/dgrad/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv_dgrad_device_tensorop_sm90\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm90_conv1d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv2d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv3d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n\n  sm90_conv1d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv2d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv3d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Executables for Fused Convolution Examples with CMake\nDESCRIPTION: This loop iterates through a list of fused convolution examples and creates an executable for each. The cutlass_example_add_executable macro is used to create the executable, and add_dependencies ensures that the 13_fused_two_convs target depends on each of the individual example executables.  This uses CMake's foreach loop to generate multiple targets based on a variable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/13_two_tensor_op_fusion/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(FUSION_CONV_EXAMPLE\n  fused_two_convs_f16_sm75_rf\n  fused_two_convs_f16_sm75_shmem\n  fused_two_convs_f16_sm80_rf\n  fused_two_convs_f16_sm80_shmem\n  fused_two_convs_s8_sm75_rf\n  fused_two_convs_s8_sm75_shmem\n  fused_two_convs_s8_sm80_rf\n  fused_two_convs_s8_sm80_shmem\n)\n\n  cutlass_example_add_executable(\n    13_${FUSION_CONV_EXAMPLE}\n    ${FUSION_CONV_EXAMPLE}.cu\n  )\n\n  add_dependencies(13_fused_two_convs 13_${FUSION_CONV_EXAMPLE})\n\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable CMake\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` function to create an executable for a CUTLASS example. It links the specified source file (`57_hopper_grouped_gemm.cu`) and passes the defined test parameter variables as options. These options configure the behavior and parameters of the test execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/57_hopper_grouped_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  57_hopper_grouped_gemm\n  57_hopper_grouped_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for SM120 BSSP Stream K GEMM Tests\nDESCRIPTION: This snippet creates an executable for SM120 BSSP GEMM tests with stream-k optimization. It utilizes `cutlass_test_unit_gemm_device_add_executable` and enables source batching (`BATCH_SOURCES ON`, `BATCH_SIZE 1`) to control compiler memory usage. The source file contains the CUDA implementation of stream-k GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_sm120_bssp_stream_k\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n  sm120_bssp_gemm_f4_f4_f32_tensor_op_f32_stream_k.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Writeable Contiguous Tile Iterator in C++\nDESCRIPTION: Defines a tile iterator concept for writing to contiguous memory with an optional pointer offset. It combines the `WriteableTileIteratorConcept` and `ContiguousMemoryTileIterator` concepts. It provides the `store_with_pointer_offset` method to store a fragment with an additional linear offset.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n/// Union of the following tile iterator concepts:\n///\n///   - WriteableTileIteratorConcept\n///   - ContiguousMemoryTileIterator\n///\nstruct WriteableContiguousTileIteratorConcept : \n  public WriteableTileIteratorConcept, \n  public ContiguousMemoryTileIterator {\n\n  /// Loads a fragment from memory with additional logical offset\n  CUTLASS_DEVICE\n  void store_with_pointer_offset(\n    Fragment const &frag,                       ///< fragment to store to the tensor\n    Index pointer_offset);                      ///< stores a tile with a linear offset\n};\n```\n\n----------------------------------------\n\nTITLE: Thread Layout Preconditions C++\nDESCRIPTION: This code contains static assertions to validate thread layouts (AThreadLayout, BThreadLayout) and their relationship to the `cta_tiler`.  The assertions ensure that the thread layouts are static, have the same size (number of threads), and that the block dimensions of the `cta_tiler` are divisible by the thread dimensions defined in `tA` and `tB`.  This ensures proper partitioning of data across threads.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n  static_assert(is_static<AThreadLayout>::value);\n  static_assert(is_static<BThreadLayout>::value);\n\n  CUTE_STATIC_ASSERT_V(size(tA) == size(tB));                          // NumThreads\n\n  CUTE_STATIC_ASSERT_V(size<0>(cta_tiler) % size<0>(tA) == Int<0>{});  // BLK_M / THR_M\n  CUTE_STATIC_ASSERT_V(size<2>(cta_tiler) % size<1>(tA) == Int<0>{});  // BLK_K / THR_K\n  CUTE_STATIC_ASSERT_V(size<1>(cta_tiler) % size<0>(tB) == Int<0>{});  // BLK_N / THR_N\n  CUTE_STATIC_ASSERT_V(size<2>(cta_tiler) % size<1>(tB) == Int<0>{});  // BLK_K / THR_K\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Blackwell SM100 GPU\nDESCRIPTION: Configures the build system to compile CUTLASS for the NVIDIA Blackwell SM100 GPU architecture. Uses CMake to generate the build files.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=100a           # compiles for NVIDIA Blackwell SM100 GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Setting CUDACXX environment variable - Bash\nDESCRIPTION: This snippet demonstrates how to set the `CUDACXX` environment variable to point to the NVCC compiler in the CUDA Toolkit. This is a necessary step before building CUTLASS with CMake. The `CUDA_INSTALL_PATH` should be replaced with the actual path to the CUDA installation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc\n```\n\n----------------------------------------\n\nTITLE: Defining cutlass_add_cutlass_library Function\nDESCRIPTION: This CMake function, `cutlass_add_cutlass_library`, generates static and shared libraries from given source files. It supports building object libraries, shared libraries, and static libraries. It also handles cases when a monolithic library is being built.  The function takes a `SUFFIX` argument to create library variations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(cutlass_add_cutlass_library)\n#\n# Generates static and shared libraries with the given SOURCES. The public CMake\n# targets produces will be cutlass_library(_${SUFFIX})? and \n# cutlass_library(_${SUFFIX})?_static.\n# \n# SUFFIX: An additional string to be joined to the default names. If suffix is given,\n#   the generated libraries will be linked as a dependency of the main cutlass library.\n\n  set(options)\n  set(oneValueArgs SUFFIX)\n  set(multiValueArgs)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  set(DEFAULT_NAME cutlass_library)\n\n  set(__NAME ${DEFAULT_NAME})\n  set(__OUTPUT_NAME cutlass)\n  set(__EXPORT_NAME library)\n\n  if (__SUFFIX)\n    string(APPEND __NAME _${__SUFFIX})\n    string(APPEND __OUTPUT_NAME _${__SUFFIX})\n    string(APPEND __EXPORT_NAME _${__SUFFIX})\n  endif()\n\n  cutlass_add_library(\n    ${__NAME}_objs\n    OBJECT\n    ${__UNPARSED_ARGUMENTS}\n    )  \n\n  target_link_libraries(${__NAME}_objs\n    PUBLIC cutlass_library_includes\n    PRIVATE cutlass_library_internal_interface\n    )\n\n  if (CUTLASS_BUILD_MONO_LIBRARY AND __SUFFIX)\n\n    # If we're only building a single monolithic library then we\n    # simply link the generated object files to the default library. \n\n    target_link_libraries(${DEFAULT_NAME} PRIVATE $<BUILD_INTERFACE:${__NAME}_objs>)\n    target_link_libraries(${DEFAULT_NAME}_static PRIVATE $<BUILD_INTERFACE:${__NAME}_objs>)\n\n  else()\n\n    cutlass_add_library(\n      ${__NAME} \n      SHARED\n      EXPORT_NAME ${__EXPORT_NAME}\n      \"\"\n      )\n\n    target_compile_features(${__NAME} INTERFACE cxx_std_17)\n    \n    set_target_properties(\n      ${__NAME}\n      PROPERTIES\n      OUTPUT_NAME ${__OUTPUT_NAME}\n      WINDOWS_EXPORT_ALL_SYMBOLS 1\n      )\n    \n    target_link_libraries(\n      ${__NAME}\n      PUBLIC cutlass_library_includes\n      PRIVATE $<BUILD_INTERFACE:${__NAME}_objs>\n      cuda_driver\n      )\n    \n    set_target_properties(${__NAME} PROPERTIES DEBUG_POSTFIX \"${CUTLASS_LIBRARY_DEBUG_POSTFIX}\")\n    \n    cutlass_add_library(\n      ${__NAME}_static\n      STATIC\n      EXPORT_NAME ${__EXPORT_NAME}_static\n      \"\"\n      )\n\n    target_compile_features(${__NAME}_static INTERFACE cxx_std_17)\n    \n    if (WIN32)\n      set(STATIC_OUTPUT_NAME ${__OUTPUT_NAME}.static)\n    else()\n      set(STATIC_OUTPUT_NAME ${__OUTPUT_NAME})\n    endif()\n    \n    set_target_properties(\n      ${__NAME}_static\n      PROPERTIES\n      OUTPUT_NAME ${STATIC_OUTPUT_NAME}\n      WINDOWS_EXPORT_ALL_SYMBOLS 1\n      )\n    \n    target_link_libraries(\n      ${__NAME}_static\n      PUBLIC cutlass_library_includes\n      PRIVATE $<BUILD_INTERFACE:${__NAME}_objs>\n      cuda_driver\n      )\n    \n    set_target_properties(${__NAME}_static PROPERTIES DEBUG_POSTFIX \"${CUTLASS_LIBRARY_DEBUG_POSTFIX}\")\n    \n    install(\n      TARGETS ${__NAME} ${__NAME}_static\n      EXPORT NvidiaCutlass\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      )\n    \n    if (__SUFFIX)\n    \n      # The partial libraries generated will be registered as linked libraries\n      # to the main cutlass library so users automatically get the necessary link\n      # commands to pull in all kernels by default.\n    \n      target_link_libraries(${DEFAULT_NAME} PUBLIC ${__NAME})\n      target_link_libraries(${DEFAULT_NAME}_static PUBLIC ${__NAME}_static)\n    \n    endif()\n\n  endif()\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Define S8 Interleaved Convolution Sources (SM80)\nDESCRIPTION: This CMake code defines the source files for the S8 interleaved convolution test executable on SM80 architecture using `cutlass_target_sources`. It specifies the source files for forward propagation with both S8 and S4 interleaved input types as private sources.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n    # Conv2d - S8 interleaved input, S8 interleaved output, S32 accumulation\n    cutlass_target_sources(\n      cutlass_test_unit_conv_device_tensorop_s32_interleaved\n      PRIVATE\n      conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu\n      conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Profiler Help Output\nDESCRIPTION: This snippet shows the output of `cutlass_profiler --help`, providing a list of available command-line options for the CUTLASS profiler.  It describes the purpose of each option, including execution modes, device selection, data initialization, profiling parameters, verification settings, and reporting options.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nCUTLASS Performance Tool\nusage:\n\n    cutlass_profiler [options]\n\n  --help\n\n  --mode=<string>                                  Cutlass profiler execution mode.\n                                                    --mode=profile    regular verification and profiling (default)\n                                                    --mode=dry_run    no kernels are launched or workspaces allocated\n                                                    --mode=enumerate  lists all operation kind and operations\n                                                    --mode=trace      executes a single device-side computation with\n                                                                       no other kernel launches\n\n  --device-info                                    Prints information on all GPUs present in the system\n\n  --operation=<operation_kind>                     CUTLASS operation to profile.\n\n  --kernels=<string_list>                          Filter operations by kernel names. For example, call all kernels with\n                                                   (\"s1688\" and \"nt\") or (\"s844\" and \"tn\" and \"align8\") in their\n                                                   operation name using --kernels=\"s1688*nt, s884*tn*align8\"\n\n  --kernels-file=<path>                            Same behavior as `kernels`, but kernel names are specified in a file with\n                                                   one kernel name on each line. Set of profiled kernels is the union of kernels\n                                                   specified here and those specified in `kernels`.\n\n  --ignore-kernels=<string_list>                   Excludes kernels whose names match anything in this list.\n\nDevice:\n  --device=<int>                                   CUDA Device ID\n\n  --compute-capability=<int>                       Override the compute capability.\n\n  --llc-capacity=<capacity in KiB>                 Capacity of last-level cache in kilobytes. If this is non-zero,\n                                                   profiling phases cycle through different input tensors to induce\n                                                   capacity misses in the L2.\n\n  --allocations=<name>:<device>,<name>:<device>    Pairs of allocation names to devices. If <device> is negative,\n                                                   the execution device is used\n\n\nInitialization:\n  --initialization=<bool>                          Enables initialization (default: true). If false, device memory is\n                                                   not initialized after allocation.\n\n  --initialization-provider=<provider>             Selects initialization provider {host, device*}. (default: '*')\n\n  --dist=<distribution>                            Data distribution of input tensors {uniform*, gaussian, identity, sequential}\n                                                    --dist=uniform,min:<double>,max:<double>,scale:<integer>\n                                                    --dist=gaussian,mean:<double>,stddev:<double>,scale:<integer>\n                                                    --dist=sequential,start:<double>,delta:<double>,scale:<integer>\n                                                    --dist=identity\n\n  --seed=<int>                                     Random number generator seed. Used to enforce deterministic\n                                                   initialization.\n\n\nLibrary:\n  --library-algo-mode=<mode>                       Indicates algorithm mode used to call libraries such as cuBLAS and cuDNN.\n                                                   mode={default*,matching,best}\n\n  --library-algos=<range-list>                     If --algorithm-mode=best, permits specifying a selection of algorithms.\n\n\nProfiling:\n  --workspace-count=<workspace count>              Number of discrete workspaces maintained to avoid cache-resident\n                                                 If zero (default), the amount is chosen for each workload based on\n                                                 capacity of the last-level cache.\n\n  --profiling-iterations=<iterations>              Number of iterations to profile each kernel. If zero, kernels\n                                                   are launched up to the profiling duration. If non-zero, this\n                                                   overrides `profiling-duration` and `min-iterations`.\n\n  --profiling-duration=<duration>                  Time to spend profiling each kernel (ms). Overriden by\n                                                   `profiling-iterations` when `profiling-iterations` != 0.\n                                                   Note that `min-iterations` must also be satisfied.\n\n  --min-iterations=<iterations>                    Minimum number of iterations to spend profiling each kernel, even if\n                                                   `profiling-duration` has been met.\n\n  --warmup-iterations=<iterations>                 Number of iterations to execute each kernel prior to profiling (default: 10).\n\n  --use-cuda-graphs=<bool>                         If true, kernels are launched in a CUDA graph. Useful when the kernel launch time is a bottleneck.\n\n  --sleep-duration=<duration>                      Number of ms to sleep between profiling periods (ms).\n\n  --profiling-enabled=<bool>                       If true, profiling is actually conducted.\n\nVerification:\n  --verification-enabled=<bool>                    Whether to perform verification checks.\n\n  --epsilon=<error>                                Error threshold. Setting to zero (default) requires\n                                                   bit-level equivalence.\n\n  --nonzero-floor=<floor>                          Results whose absolute value is less than this quantity\n                                                   are treated as zero for comparisons.\n\n  --save-workspace=<string>                        Specifies when to save the GEMM inputs and results to the filesystem.\n                                                    --save-workspace=never      never save workspace (default)\n                                                    --save-workspace=incorrect  save workspace for incorrect results\n                                                    --save-workspace=always     always save workspace\n\n  --verification-providers=<providers>             List of providers used to verify result. (default: '*')\n                                                   Gemm verification-providers {cublas*}\n                                                   Conv2d verification-providers {cudnn*, device*, host}\n\n\nReport:\n  --append=<bool>                                  If true, result is appended to possibly existing file. Otherwise,\n                                                   any existing file is overwritten.\n\n  --output=<path>                                  Path to output file for machine readable results. Operation kind and '.csv' is appended.\n\n  --junit-output=<path>                            Path to junit output file for result reporting. Operation kind and '.junit.xml' is appended.\n\n  --report-not-run=<bool>                          If true, reports the status of all kernels including those that\n                                                   do not satisfy the given arguments.\n\n  --tags=<column:tag,...>                          Inserts leading columns in output table and uniform values for each\n                                                   column. Useful for generating pivot tables.\n\n  --verbose=<bool>                                 Prints human-readable text to stdout. If false, nothing is written to stdout.\n\n\nAbout:\n  --version                                        CUTLASS 2.4.0 built on Nov 19 2020 at 11:59:00\n\n\nOperations:\n\n     gemm                                          General matrix-matrix product. D = alpha * A*B + beta * C\n     spgemm                                        Structured sparse GEMM. D = alpha * A*B + beta * C\n     conv2d                                        Conv2d operation. Output(Tensor4D) = alpha * Input(Tensor4D) * Filter(Tensor4D) + beta * Input(Tensor4D)\n     conv3d                                        Conv3d operation. Output(Tensor5D) = alpha * Input(Tensor5D) * Filter(Tensor5D) + beta * Input(Tensor5D)\n\n\nFor details about a particular function, specify the function name with --help.\n\nExample:\n\n  $ cutlass_profiler --operation=Gemm --help\n\n  $ cutlass_profiler --operation=Conv3d --help\n\n  $ cutlass_profiler --operation=Conv2d --help\n\n```\n\n----------------------------------------\n\nTITLE: Concatenating Layouts with make_layout in CuTe (C++)\nDESCRIPTION: This snippet showcases how to concatenate layouts using `make_layout`.  It demonstrates how to combine multiple layouts into a single layout, creating nested or hierarchical layouts.  Layouts `a` and `b` are defined as `Layout<_3,_1>` and `Layout<_4,_3>` respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a = Layout<_3,_1>{};                     // 3:1\nLayout b = Layout<_4,_3>{};                     // 4:3\nLayout row = make_layout(a, b);                 // (3,4):(1,3)\nLayout col = make_layout(b, a);                 // (4,3):(3,1)\nLayout q   = make_layout(row, col);             // ((3,4),(4,3)):((1,3),(3,1))\nLayout aa  = make_layout(a);                    // (3):(1)\nLayout aaa = make_layout(aa);                   // ((3)):((1))\nLayout d   = make_layout(a, make_layout(a), a); // (3,(3),3):(1,(1),1)\n```\n\n----------------------------------------\n\nTITLE: Write Tensors to CSV using CUTLASS\nDESCRIPTION: This code snippet demonstrates how to write input and output tensors to a file as CSV using `cutlass::HostTensor<>`. It opens an output file stream, writes the input and filter tensors to the file, synchronizes the device memory to the host, and then writes the computed output tensor to the file. This is useful for debugging and verifying the results of the convolution operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_12\n\nLANGUAGE: c++\nCODE:\n```\nstd::ofstream output_workspace(ss.str());\n\noutput_workspace \n  << \"Input = \\n\" << tensor_a.host_view() << \"\\n\\n\"\n  << \"Filters = \\n\" << tensor_b.host_view() << \"\\n\\n\";\n\n// Copy device memory to host backing store\ntensor_c.sync_host();\n\noutput_workspace << \"Computed = \\n\" << tensor_c.host_view() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Hopper Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Hopper GPU architecture using CMake. The `CUTLASS_NVCC_ARCHS` flag specifies the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a              # compiles for NVIDIA Hopper GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Run GEMM with All Activation Functions - Python\nDESCRIPTION: This snippet iterates through all available activation functions obtained from `plan.activations()` and runs the GEMM operation with each of them.  It sets the `activation` field of the GEMM plan to the current activation function in the loop and then executes the GEMM, printing information about the activation before running.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/01_epilogue.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor activation in activations:\n    print('=============================================================================================')\n    print(f'Compiling and running activation {activation}')\n    print('=============================================================================================')\n    plan.activation = activation\n    plan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 1SM, Nvf4)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following a sparse matrix multiplication, using TMA (Tensor Memory Accelerator) on a single SM (Streaming Multiprocessor). It is designed specifically for narrow precision using the nvf4 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized1SmNvf4`\n```\n\n----------------------------------------\n\nTITLE: Grouped GEMM problem sizes example\nDESCRIPTION: Example of grouped GEMM problem sizes with varying K dimensions. This is intended to illustrate potential load imbalance issues if the work is not distributed evenly based on the K dimension.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n0 1152x768x128\n1 1152x768x1024\n2 768x1152x128\n3 768x1152x1024\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor Core Convolution Test Executable\nDESCRIPTION: This snippet defines a test executable for convolution operations using Tensor Cores. The implementation targets F16 input, F32 output, and F32 accumulation on sm70 architecture. It specifies source files for convolution forward propagation with broadcast and implicit GEMM for forward, data gradient, and weight gradient computations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv_device_tensorop_f32_sm70\n  conv2d_fprop_with_broadcast_sm70.cu \n  conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu\n  conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu\n  conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Unit Test Executable with CMake\nDESCRIPTION: This CMake command defines a CUTLASS unit test executable named 'cutlass_test_unit_cute_layout'. It compiles the 'layout_operator.cu' source file and links it into an executable. This command is part of the CUTLASS testing framework and relies on predefined CMake macros.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/layout/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_layout\n  layout_operator.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Compiling the CUTLASS Profiler\nDESCRIPTION: Compiles the CUTLASS Profiler using the `make` command.  The `-j12` option enables parallel compilation with 12 jobs.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make cutlass_profiler -j12\n```\n\n----------------------------------------\n\nTITLE: Adding Executable in CUTLASS (CMake)\nDESCRIPTION: This snippet demonstrates how to add a CUDA executable to the CUTLASS build system using the `cutlass_example_add_executable` macro. The first argument specifies the name of the executable, and the subsequent arguments are the source files to compile. The source file is '30_wgrad_split_k.cu', which is assumed to be a CUDA file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/30_wgrad_split_k/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  30_wgrad_split_k\n  30_wgrad_split_k.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Profiling One CUDA Core Convolution Kernel\nDESCRIPTION: This command executes the CUTLASS profiler on one CUDA Core convolution kernel.  The `--kernels` argument specifies the specific kernel, and other arguments define convolution problem dimensions (n, h, w, c, k, r, s).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Defining Scale Group Test Parameters (CMake)\nDESCRIPTION: Defines CMake variables for group-wise scaling tests, specifying matrix dimensions (m, n, k, c), scaling mode, and setting iterations to 0.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_SCALE_GROUP --m=2048 --n=5120 --k=8192 --c=512 --mode=1 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper Cooperative GEMM Unit Test Executable with CMake\nDESCRIPTION: This snippet uses a custom CMake function `cutlass_test_unit_add_executable` to create an executable named `cutlass_test_unit_cute_hopper_cooperative_gemm`. The executable is built from the source file `cooperative_gemm.cu`, and is part of the unit tests for CUTLASS on the Hopper architecture. This simplifies the build configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_cooperative_gemm\n  cooperative_gemm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable and Linking Libraries in CMake\nDESCRIPTION: This CMake snippet conditionally adds an executable target named `10_planar_complex` based on `planar_complex.cu`. It then links this target against the `cutlass_lib`, `cutlass_tools_util_includes`, and `cuda` libraries. The check `CUTLASS_ENABLE_LIBRARY` ensures the example is built only if CUTLASS library is enabled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/10_planar_complex/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_ENABLE_LIBRARY)\n\n# Planar Complex GEMM example\ncutlass_example_add_executable(\n  10_planar_complex\n  planar_complex.cu\n)\n\ntarget_link_libraries(\n  10_planar_complex\n  PRIVATE\n  cutlass_lib\n  cutlass_tools_util_includes\n  cuda\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f16 SM120 TensorOp GEMM Tests\nDESCRIPTION: This CMake function adds an executable named `cutlass_test_unit_gemm_device_tensorop_f16_sm120`. Similar to the f32 case, it compiles a set of CUDA files implementing different GEMM configurations but with f16 data type and leveraging Tensor Cores for the SM120 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_f16_sm120\n\n  sm120_gemm_f4_f6_f16_tensor_op_narrow_output.cu\n  sm120_gemm_f4_f6_f16_tensor_op.cu\n  sm120_gemm_f4_f8_f16_tensor_op.cu\n  sm120_gemm_f6_f8_f16_tensor_op.cu\n  sm120_gemm_f4_f4_f16_tensor_op.cu\n  sm120_gemm_f6_f6_f16_tensor_op.cu\n  sm120_gemm_f8_f8_f16_tensor_op.cu\n)\n```\n\n----------------------------------------\n\nTITLE: HostTensor Data Access\nDESCRIPTION: These code snippets demonstrate how to access internal host-side and device memory of a `HostTensor` using methods like `host_data()`, `host_ref()`, `host_view()`, `device_data()`, `device_ref()`, and `device_view()`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nfloat *host_ptr = tensor.host_data();\ncutlass::TensorRef<float, cutlass::layout::ColumnMajor> host_ref = tensor.host_ref();\ncutlass::TensorView<float, cutlass::layout::ColumnMajor> host_view = tensor.host_view();\n```\n\nLANGUAGE: C++\nCODE:\n```\nfloat *device_ptr = tensor.device_data();\ncutlass::TensorRef<float, cutlass::layout::ColumnMajor> device_ref = tensor.device_ref();\ncutlass::TensorView<float, cutlass::layout::ColumnMajor> device_view = tensor.device_view();\n```\n\n----------------------------------------\n\nTITLE: Mainloop Builder Setup Skeleton\nDESCRIPTION: This C++ code block represents the skeleton for setting up the main loop builder for a block-scaled GEMM kernel using CUTLASS. It outlines the steps to describe the A and B tensors, choose performance parameters, and define the collective mainloop.  The missing type definitions (TBD) need to be filled in based on specific requirements.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\n  ///////////////////////////////////////////////////////////\n  //                Mainloop Builder Setup\n  ///////////////////////////////////////////////////////////\n  \n  ///////////////////////////////////////////\n  // 1. Describe A and B tensors\n  ///////////////////////////////////////////\n  using ElementA       = // TBD\n  constexpr int AlignA = // TBD\n  using GmemLayoutA    = // TBD\n  using ElementB       = // TBD\n  constexpr int AlignB = // TBD\n  using GmemLayoutB    = // TBD\n\n  // Mma's accumulator type\n  using ElementAccumulator = float;           // Always float for block scaled tcgen05.mma instructions\n\n  //////////////////////////////////////////\n  // 2. Choose Performance Parameters\n  //////////////////////////////////////////\n\n  // Tile and cluster shapes\n  // Collective MMA takes tile shape of the MMA operation as input\n  using KernelMainloopPolicy     = // TBD\n  using MmaTileShape_MNK         = // TBD\n  using ClusterShape_MNK         = // TBD\n\n  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<\n      cutlass::arch::Sm100, cutlass::arch::OpClassBlockScaledTensorOp,      // Arch and Tensorop spec\n      ElementA, GmemLayoutA, AlignA,                                        // A tensor elem type, layout and alignment requirement\n      ElementB, GmemLayoutB, AlignB,                                        // B tensor elem type, layout and alignment requirement\n      ElementAccumulator,                                                   // Mma instruction accumulator type\n      MmaTileShape_MNK, ClusterShape_MNK,                                   // Mma instruction tile shape, cluster shape\n      // Epilogue's SMEM usage that needs to be subtracted from overall SMEM capacity \n      cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>, \n      KernelMainloopPolicy                                                  // Kernel schedule policy.\n                                                                            // Auto or using targeted scheduling policy\n    >::CollectiveOp;\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License Text\nDESCRIPTION: This snippet represents the BSD-3-Clause license text applicable to the CUTLASS project. It defines the permissions and restrictions for using, modifying, and distributing the software. The text includes clauses for copyright retention, redistribution conditions, and a disclaimer of warranty.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_9\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Running GEMM with Weight Prefetch Example\nDESCRIPTION: This command executes the GEMM with weight prefetch example application. It sets the dimensions of the GEMM operation using the `--m`, `--n`, and `--k` parameters, specifying the matrix sizes. This execution runs the example to demonstrate the performance of the L2 weight prefetch in a GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./63_hopper_gemm_with_weight_prefetch --m=8192 --n=1 --k=8192\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Targets for CUTLASS Unit Tests\nDESCRIPTION: This snippet adds two custom targets: `cutlass_test_unit` and `test_unit`. These targets can be used to trigger the build of all unit tests within the CUTLASS project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(cutlass_test_unit)\nadd_custom_target(test_unit)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Unit Tests\nDESCRIPTION: This snippet iterates through a list of subdirectories (`SUBDIRS`) and adds each as a subdirectory to the build. It also creates dependencies between the `cutlass_test_unit` and `test_unit` targets and the subdirectory-specific targets (e.g., `cutlass_test_unit_core`).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(SUBDIR ${SUBDIRS})\n\n  add_subdirectory(${SUBDIR})\n  add_dependencies(cutlass_test_unit cutlass_test_unit_${SUBDIR})\n  add_dependencies(test_unit test_unit_${SUBDIR})\n\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Creating NVRTC Test Executable with CUTLASS\nDESCRIPTION: This CMake function creates an executable named 'cutlass_test_unit_nvrtc_thread' from the specified CUDA source files ('nvrtc_gemm.cu', 'nvrtc_contraction.cu') and a header file ('testbed.h').  It sets up a test unit to verify NVRTC functionality within CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/thread/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_nvrtc_thread\n  nvrtc_gemm.cu\n  nvrtc_contraction.cu\n  testbed.h\n)\n```\n\n----------------------------------------\n\nTITLE: Allocate Tensors on Different Devices\nDESCRIPTION: This command profiles the GEMM operation with the execution happening on device 0, the C tensor allocated on device 1, and the D tensor allocated on device 2. It demonstrates how to specify device allocations for the tensors.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --device=0 --allocations=C:1,D:2 --operation=Gemm --m=1024 --n=1024 --k=128\n```\n\n----------------------------------------\n\nTITLE: Adding Source Groups (CMake)\nDESCRIPTION: Adds source groups to the project for the generated header and source files.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"Generated\\\\Header Files\" FILES ${GENERATED_HEADER_FILES})\nsource_group(\"Generated\\\\Source Files\" FILES ${GENERATED_SOURCE_FILES})\n```\n\n----------------------------------------\n\nTITLE: Array Template Definition in CUTLASS\nDESCRIPTION: This snippet shows the template definition of the `Array` container in CUTLASS. It is a statically sized array, similar to `std::array`, but with specializations for packing elements smaller than one byte. This container is intended for storing arrays of numeric elements with optimal memory usage. Requires the `cutlass` library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <\n  typename T,       // element type\n  int N             // number of elements\n>\nstruct Array;\n```\n\n----------------------------------------\n\nTITLE: Shared Memory Column Index Calculation in C++\nDESCRIPTION: This C++ snippet calculates the store column index for a permuted shared memory layout, used to avoid bank conflicts when storing data to shared memory. It takes the lane ID within a warp and applies an XOR operation to permute the column index. This transformation ensures conflict-free stores when using Turing Tensor Cores.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nint store_column = (lane_id % 8) ^ (lane_id / 8);\n```\n\n----------------------------------------\n\nTITLE: Hopper Kernel Instantiation with Specific Architectures\nDESCRIPTION: These commands demonstrate how to instantiate kernels for the Hopper architecture (SM90) with specific configurations. It sets the NVCC architecture, specifies the kernels to be instantiated, sets the instantiation level to 'max', and enables unity builds. The `CUTLASS_LIBRARY_INSTANTIATION_LEVEL` is used to instantiate all possible combinations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. \\\n  -DCUTLASS_NVCC_ARCHS=\"90a\" \\\n  -DCUTLASS_LIBRARY_KERNELS=\"cutlass3x_sm90_tensorop_s64x64x16gemm_f16_f16_f32_void_f32_*\" \\\n  -DCUTLASS_LIBRARY_INSTANTIATION_LEVEL=\"max\" \\\n  -DCUTLASS_UNITY_BUILD_ENABLED=ON\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f6f4xf6f4 GEMM Test\nDESCRIPTION: This CMake command adds an executable named `cutlass_test_unit_gemm_device_tensorop_sm100_f6f4xf6f4` for testing GEMM operations with f6f4xf6f4 precision on sm100 architecture using tensor cores. It configures the build to use batch sources and sets the batch size to 1. The `.cu` files provide the implementation for different data layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_sm100_f6f4xf6f4\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  f6f4_f6f4_void_f32_tn_layout.cu\n  f6f4_f6f4_void_f32_nn_layout.cu\n  f6f4_f6f4_void_f32_nt_layout.cu\n  f6f4_f6f4_void_f32_tt_layout.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch CUDA Extension within directory\nDESCRIPTION: This command builds the PyTorch CUDA extension using the `setup.py` script from within the output directory.  The `TORCH_CUDA_ARCH_LIST` environment variable specifies the compute capability of the target device (e.g., 8.0 for SM80).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd out\nTORCH_CUDA_ARCH_LIST=\"8.0\" python setup.py\n```\n\n----------------------------------------\n\nTITLE: Executing GEMM with TiledMMA (C++)\nDESCRIPTION: This code executes the matrix multiply-accumulate operation using the `TiledMMA` instance `mma`. The `cute::gemm` function takes the `TiledMMA` object, the partitioned input tensors `tCsA` and `tCsB`, and the output fragment tensor `tCrC` as arguments.  This performs the actual GEMM operation according to the defined tiling and FMA instruction.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_31\n\nLANGUAGE: cpp\nCODE:\n```\ncute::gemm(mma, tCsA, tCsB, tCrC);\n```\n\n----------------------------------------\n\nTITLE: Configure Convolution kernels for Ampere Tensor Core\nDESCRIPTION: This CMake command configures CUTLASS to compile all convolution kernels targeting NVIDIA Ampere's 16816 Tensor Core operation (architecture 80).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_33\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='80' -DCUTLASS_LIBRARY_KERNELS=s16816fprop,s16816dgrad,s16816wgrad\n```\n\n----------------------------------------\n\nTITLE: Conditional Source Append CMake\nDESCRIPTION: This snippet conditionally appends a source file specific to the Blackwell architecture (`pipeline_cluster_launch_control_async_warp_specialized_blackwell.cu`) to the `PIPELINE_SOURCES` list. The condition checks if `CUTLASS_NVCC_ARCHS` matches '100a', which indicates the Blackwell architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/pipeline/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\n  list(APPEND PIPELINE_SOURCES \n    pipeline_cluster_launch_control_async_warp_specialized_blackwell.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enable All Kernels during CMake configuration\nDESCRIPTION: This CMake command enables the compilation of all CUTLASS Library kernels.  It is executed when configuring the project with CMake.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_26\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='70;75;80' -DCUTLASS_LIBRARY_KERNELS=all\n```\n\n----------------------------------------\n\nTITLE: Defining Turing TensorOp GEMM Executable in CUTLASS (CMake)\nDESCRIPTION: This CMake code defines an executable named '08_turing_tensorop_gemm' using the 'cutlass_example_add_executable' macro. The executable is built from the source file 'turing_tensorop_gemm.cu'.  The macro simplifies the process of creating a build target for a CUTLASS example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/08_turing_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  08_turing_tensorop_gemm\n  turing_tensorop_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: GEMM F32 SplitK Serial Example\nDESCRIPTION: This example demonstrates a GEMM operation using float32 with split-K serial execution and multiply_add_fast_f32. The example configures block size, tensor layouts, swizzle functions, and split-K value.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add_fast_f32 -op TensorOp -b 64 64 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 4 -lb ColumnMajor -ab 4 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 4\n```\n\n----------------------------------------\n\nTITLE: Grouped Kernel Scheduler Pseudocode C++\nDESCRIPTION: This pseudocode illustrates the main loop executed by threadblocks in a CUTLASS grouped kernel. The `ProblemVisitor` object is used to retrieve the next tile to compute, followed by the MMA and epilogue operations.  The `advance` method updates the scheduler after completing the tile.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nProblemVisitor problem_visitor;\n \nwhile (problem_visitor.next_tile()) {\n    //\n    // Get next tile index from scheduler\n    //\n \n    //\n    // Compute MMA and epilogue\n    //\n \n    // Inform the scheduler that we are done with the current tile\n    problem_visitor.advance(gridDim.x);\n}\n```\n\n----------------------------------------\n\nTITLE: Conv2d F32 Fprop Example\nDESCRIPTION: This example demonstrates a Conv2d forward propagation operation using float32. It utilizes TensorOp, Strided layout, and ImplicitGemm for optimized convolution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 16 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 4 -lb TensorNHWC -ab 4 -lc TensorNHWC -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -co fprop -st Strided -ia optimized -sm Serial -k 1 -nhwc 1 13 17 8 -krsc 24 3 3 8 -pad 0 0 0 0 -stride 2 2 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: Modifying Swizzling Functor for CUTLASS GEMM\nDESCRIPTION: This snippet demonstrates how to modify the swizzling function used by the CUTLASS GEMM kernel. It checks the compute capability (cc) and, if it's not SM90, sets the `swizzling_functor` to `cutlass.swizzle.ThreadblockSwizzleStreamK` to use the stream K feature. Then, it runs the GEMM operation with the modified swizzling function.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Stream K is only supported pre-SM90 (at least when this example was written)\nif plan.cc != 90:\n    plan.swizzling_functor = cutlass.swizzle.ThreadblockSwizzleStreamK\n    plan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Defining Test Command Options in CMake\nDESCRIPTION: This snippet sets CMake variables that define the test command options. These options specify parameters for tests, including the number of iterations, alpha and beta values for epilogue operations, and problem dimensions (m, n, k). These settings configure the test execution environment for CUTLASS examples.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/79_blackwell_geforce_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_RANDOM --iterations=0)\nset(TEST_RANDOM_LARGE_GROUP --groups=50 --iterations=0)\n\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=0)\nset(TEST_EPILOGUE_LARGE_GROUP --alpha=1.5 --beta=2.0 --groups=50 --iterations=0)\n\nset(TEST_EPILOGUE_OP --beta=0.5 --iterations=1)\nset(TEST_EPILOGUE_OP_LARGE_GROUP --alpha=1.5 --iterations=1)\n\nset(TEST_FIXED --m=2048 --n=5120 --k=8192 --iterations=0)\nset(TEST_FIXED_LARGE_GROUP --m=2048 --n=512 --k=512 --groups=51 --iterations=0)\n\nset(TEST_SMALL --m=256 --n=128 --iterations=0)\nset(TEST_SMALL_LARGE_GROUP --m=128 --n=128 --groups=50 --iterations=0)\n\nset(TEST_RANDOM_PERF --iterations=10)\nset(TEST_RANDOM_PERF_LARGE_GROUP --groups=50 --iterations=10)\n```\n\n----------------------------------------\n\nTITLE: Instantiating All Kernel Sizes\nDESCRIPTION: This set of commands configures the CUTLASS build to instantiate all tile sizes and threadblock cluster sizes for each data type, math instruction, and layout. It disables unity builds and sets specific NVCC architectures, then compiles the CUTLASS Profiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"70;75;80\" -DCUTLASS_LIBRARY_KERNELS=all  -DCUTLASS_UNITY_BUILD_ENABLED=ON\n...\n$ make cutlass_profiler -j\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS Unit Tests\nDESCRIPTION: Builds the `test_unit` target to compile and run all unit tests. This command executes all available unit tests to ensure the library's correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ make test_unit -j\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Sparse GEMM Compressor Tests\nDESCRIPTION: This CMake code creates a custom target named `cutlass_test_unit_sm90_structured_sparse_gemm_compressor` that depends on three other targets: `cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f32`, `cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f16`, and `cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f8`. This ensures that the specified executables are built when this target is built.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/device/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor\n    DEPENDS\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f32\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f16\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f8\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executables (CUDA)\nDESCRIPTION: This snippet uses a conditional statement to check if the CUTLASS_NVCC_ARCHS variable matches '120a', indicating support for the Blackwell architecture. If true, it adds several CUTLASS example executables using the `cutlass_example_add_executable` function, linking them to their respective CUDA source files and associated test configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/79_blackwell_geforce_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 120a)\ncutlass_example_add_executable(\n  79a_blackwell_geforce_nvfp4_bf16_gemm\n  79a_blackwell_geforce_nvfp4_bf16_gemm.cu\n)\n\ncutlass_example_add_executable(\n  79b_blackwell_geforce_nvfp4_nvfp4_gemm\n  79b_blackwell_geforce_nvfp4_nvfp4_gemm.cu\n)\n\ncutlass_example_add_executable(\n  79c_blackwell_geforce_mixed_mxfp8_mxfp6_bf16_gemm\n  79c_blackwell_geforce_mixed_mxfp8_mxfp6_bf16_gemm.cu\n)\n\ncutlass_example_add_executable(\n  79d_blackwell_geforce_nvfp4_grouped_gemm\n  79d_blackwell_geforce_nvfp4_grouped_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executable with Sparse Groups and CMake\nDESCRIPTION: This CMake code conditionally adds an executable for a CUTLASS example, specifically the Hopper FP8 warp specialized grouped GEMM with blockwise scaling and sparse groups, only if the compiler is not MSVC. This is due to a known issue with MSVC that prevents the example from compiling correctly. The `cutlass_example_add_executable` macro is used with the source file and test parameters.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT MSVC)\n\ncutlass_example_add_executable(\n  68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling_with_sparse_groups\n  68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling_with_sparse_groups.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Blackwell SM120 Blockscaled Sparse MMA TMA CUDA\nDESCRIPTION: This header file defines collective mainloops targeting blockscaled datatypes with support for sparse GEMM on Blackwell SM120 architecture within the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_3\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/gemm/collective/sm120_blockscaled_sparse_mma_tma.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Building Blackwell Distributed GEMM Executable with CUTLASS\nDESCRIPTION: This snippet demonstrates how to build an executable for a distributed GEMM example on Blackwell architecture using the `cutlass_example_add_executable` CMake macro.  It takes the name of the executable and the CUDA source file as input, linking necessary CUTLASS libraries.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/82_blackwell_distributed_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  82_blackwell_distributed_gemm\n  82_blackwell_distributed_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CLayout Shape (T8, V8) for HMMA\nDESCRIPTION: This code snippet defines the initial shape of the CLayout for HMMA, representing the mapping from 8 threads each owning 8 values to (m, n) coordinates.  The stride is left undefined initially and to be filled later based on the HMMA pattern. It uses `Shape` and `Stride` from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n  // (T8,V8) -> (m,n)\n  using CLayout = Layout<Shape <_8, _8>,\n                         Stride<_?, _?>;  // Stride to be filled in below\n```\n\n----------------------------------------\n\nTITLE: Setting GEMM Test Parameters in CMake\nDESCRIPTION: This CMake code snippet sets parameters for various GEMM tests, including direct batched conversion and different scaling modes. It defines matrix dimensions (m, n, k), group size (g), mode, and the number of iterations, which is set to 0 to disable performance benchmarking and only run correctness checks.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_DIRECT_BATCHED --m=2048 --n=2048 --k=2048 --l=2 --mode=0 --iterations=0)\n\nset(TEST_SCALE_PERCOL --m=4096 --n=5120 --k=8192 --g=8192 --mode=1 --iterations=0)\nset(TEST_SCALE_ZERO_PERCOL --m=4096 --n=5120 --k=8192 --g=8192 --mode=2 --iterations=0)\n\nset(TEST_SCALE_GROUP --m=2048 --n=5120 --k=8192 --g=512 --mode=1 --iterations=0)\nset(TEST_SCALE_ZERO_GROUPED --m=2048 --n=5120 --k=8192 --g=256 --mode=2 --iterations=0)\n\nset(TEST_SCALE_RESIDUE --m=128 --n=128 --k=320 --g=128 --mode=1 --iterations=0)\nset(TEST_SCALE_ZERO_RESIDUE --m=128 --n=128 --k=192 --g=128 --mode=2 --iterations=0)\n\nset(TEST_ALPHA_BETA --alpha=0.5 --beta=0.7 --mode=2 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Tensor Manipulation in CUTLASS 3.0 GEMM Kernel (C++)\nDESCRIPTION: This code snippet demonstrates tensor creation, slicing, and tiling within a CUTLASS 3.0 GEMM kernel, showcasing the use of CuTe for managing tensor layouts. It initializes tensors `mA_mkl` and `mB_nkl` from global memory, extracts batch slices `mA_mk` and `mB_nk`, and then slices these to obtain local tiles `gA` and `gB` for processing by a thread block.  The parameters `M`, `K`, `L`, `N`, `blk_shape`, `blk_coord_mnkl`, `params.mainloop.ptr_A`, `params.mainloop.ptr_B`, `params.mainloop.dA`, `params.mainloop.dB`, and the `Step` template are essential for defining the GEMM operation and tile structure.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// Represent the full tensors\nTensor mA_mkl = make_tensor(make_gmem_ptr(params.mainloop.ptr_A), make_shape(M,K,L), params.mainloop.dA); // (m,k,l)\nTensor mB_nkl = make_tensor(make_gmem_ptr(params.mainloop.ptr_B), make_shape(N,K,L), params.mainloop.dB); // (n,k,l)\n\n// Get batch slice\nTensor mA_mk = mA_mkl(_,_,get<3>(blk_coord_mnkl)); // (m,k)\nTensor mB_nk = mB_nkl(_,_,get<3>(blk_coord_mnkl)); // (n,k)\n\n// Slice to get the tiles for which this thread block is responsible\nTensor gA = local_tile(mA_mk, blk_shape, take<0,3>(blk_coord_mnkl), Step<_1, X,_1>{}); // (BLK_M,BLK_K,k)\nTensor gB = local_tile(mB_nk, blk_shape, take<0,3>(blk_coord_mnkl), Step< X,_1,_1>{}); // (BLK_N,BLK_K,k)\n```\n\n----------------------------------------\n\nTITLE: Adding Host Subdirectory in CUTLASS CMake\nDESCRIPTION: This snippet adds a subdirectory named `host` to the CMake project.  This indicates that there is C++ host code present which likely interacts with the CUTLASS CUDA kernels.  It is necessary to include the host code in the overall build process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/thread/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(host)\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with PDL support\nDESCRIPTION: This command configures the CMake build system for CUTLASS with support for Programmatic Dependent Launch (PDL) targeting the Blackwell architecture (SM100). It sets the CUDA architecture and enables the GDC (Graph Definition Compilation) flag.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/82_blackwell_distributed_gemm/REQUIREMENTS.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake $PATH -DCUTLASS_NVCC_ARCHS=\"100a\" -DCUTLASS_ENABLE_GDC_FOR_SM100=1\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Profiler Build Command\nDESCRIPTION: This command shows how to build the CUTLASS Profiler, a command-line application designed to load the CUTLASS Instance Library and execute all operations contained within. It demonstrates a simple `make` command.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ make cutlass_profiler -j\n```\n\n----------------------------------------\n\nTITLE: Globbing NVRTC Sources (CMake)\nDESCRIPTION: Uses `file(GLOB_RECURSE)` to find all `.hpp` files in the `kernel/thread` directory and its subdirectories, storing the results in the `NVRTC_SOURCES` variable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE NVRTC_SOURCES RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} kernel/thread/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Conditional Executable Addition (CMake)\nDESCRIPTION: This snippet conditionally adds a CUDA executable example to the build process if the `CUTLASS_NVCC_ARCHS` variable matches the string `100a`. It uses the `cutlass_example_add_executable` function to specify the executable's name and source file. The `endif()` statement closes the conditional block.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/78_blackwell_emulated_bf16x9_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  78_blackwell_emulated_bf16x9_gemm\n  78_blackwell_emulated_bf16x9_gemm.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Example Executable (CMake)\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` CMake macro to define and build an example executable. It specifies the source file, the executable name, and a list of configuration options defined in the previous snippet. These options are passed as command-line arguments to the executable during testing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/56_hopper_ptr_array_batched_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  56_hopper_ptr_array_batched_gemm\n  56_hopper_ptr_array_batched_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_SQUARE\n  TEST_SQUARE_LARGE_BATCH\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_BATCH\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_BATCH\n  TEST_SMALLK\n  TEST_SMALLK_LARGE_BATCH\n  )\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sublayouts with layout<I...> in CuTe (C++)\nDESCRIPTION: This snippet demonstrates retrieving sublayouts from a parent layout using the `layout<I...>` syntax.  It showcases how to access specific dimensions of the layout, and the layout `a` is initialized as `Layout<Shape<_4,Shape<_3,_6>>>`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a   = Layout<Shape<_4,Shape<_3,_6>>>{}; // (4,(3,6)):(1,(4,12))\nLayout a0  = layout<0>(a);                     // 4:1\nLayout a1  = layout<1>(a);                     // (3,6):(4,12)\nLayout a10 = layout<1,0>(a);                   // 3:4\nLayout a11 = layout<1,1>(a);                   // 6:12\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with Specific Operations\nDESCRIPTION: Configures the build system to compile only 2-D convolution kernels.  Reduces build times by only including necessary operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_OPERATIONS=conv2d\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executables for SM100\nDESCRIPTION: This CMake code block adds several executable targets for different CUTLASS MMA tutorials, specifically designed for the SM100 architecture. Each tutorial showcases a different aspect or optimization technique within CUTLASS, such as TMA (Tensor Memory Accelerator) and multicast.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/blackwell/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  cute_tutorial_01_mma_sm100\n  01_mma_sm100.cu\n)\n\ncutlass_example_add_executable(\n  cute_tutorial_02_mma_tma_sm100\n  02_mma_tma_sm100.cu\n)\n\ncutlass_example_add_executable(\n  cute_tutorial_03_mma_tma_multicast_sm100\n  03_mma_tma_multicast_sm100.cu\n)\n\ncutlass_example_add_executable(\n  cute_tutorial_04_mma_tma_2sm_sm100\n  04_mma_tma_2sm_sm100.cu\n)\n\ncutlass_example_add_executable(\n  cute_tutorial_05_mma_tma_epi_sm100\n  05_mma_tma_epi_sm100.cu\n)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Sparse Kernel Dispatch Policy (2SM)\nDESCRIPTION: This dispatch policy configures a sparse matrix multiplication kernel using TMA (Tensor Memory Accelerator) warp specialization with two SMs (Streaming Multiprocessors) and is optimized for SM100 architecture. The policy targets data types {mx_float4_t, mx_float6_t, mx_float8_t} multiplied by either {mx_float4_t, mx_float6_t} or mx_float8_t.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n`KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100`\n```\n\n----------------------------------------\n\nTITLE: GEMM BF16 SplitK Parallel Example\nDESCRIPTION: This example demonstrates a GEMM operation using bfloat16 inputs and float32 output with split-K parallel execution.  It specifies block sizes, tensor layouts, swizzle functions, and the GemmSplitKParallel kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 16 -ta bfloat16 -tb bfloat16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 64 128 64 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb ColumnMajor -ab 8 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle2 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm GemmSplitKParallel -k 5\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License\nDESCRIPTION: This snippet contains the text of the BSD-3-Clause license applicable to the NVIDIA CUTLASS project. It defines the rights and obligations of users who redistribute or use the software, including requirements to retain copyright notices and disclaimers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/efficient_gemm.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Creating a custom target depending on several test executables (SM100)\nDESCRIPTION: This CMake code creates a custom target named `cutlass_test_unit_gemm_device_sm100_blockscaled`. This target depends on a list of other targets representing compiled test executables which allows triggering them together.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm100_blockscaled\n  DEPENDS\n  cutlass_test_unit_gemm_device_bstensorop_sm100_nvf4xnvf4\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf4xmxf4\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf6xmxf6\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf8xmxf8\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf6xmxf8\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf8xmxf6\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf4xmxf8\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf8xmxf4\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf6xmxf4\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf4xmxf6\n)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Unconstrained Template Function Example in C++\nDESCRIPTION: This snippet illustrates the potential issues with unconstrained template functions having common names, which can lead to ambiguous overloads due to argument-dependent lookup (ADL). This is an example of what NOT to do.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include <cassert>\n#include <memory>\n#include <utility>\n\n// Uncomment the line below to observe unwarranted build errors.\n//#define BAD_CUTLASS_SWAP 1\n\nnamespace cutlass {\nstruct Bar {\n  float f;\n};\n} // namespace cutlass\n\n#ifdef BAD_CUTLASS_SWAP\nnamespace cutlass {\n\n// don't do this\ntemplate<class T>\nvoid swap(T& a, T& b) {\n  T tmp = a;\n  a = b;\n  b = tmp;\n}\n\n} // namespace cutlass\n#endif // BAD_CUTLASS_SWAP\n\nnamespace other {\n\n#ifdef BAD_CUTLASS_SWAP\nusing cutlass::swap;\n#endif // BAD_CUTLASS_SWAP\n\n// Imagine for the sake of this example\n// that \"foo\" is a less common name,\n// and that T is constrained via\n// std::enable_if or a requires clause.\ntemplate<class T>\nvoid foo(T& a, T& b) {\n  // The usual idiom for using std::swap is the \"swap two-step\":\n  //\n  // 1. import std::swap into the current scope, then\n  // 2. call swap without namespace qualification.\n  //\n  // That won't build if we have another swap\n  // overload available in the scope already.\n\n  using std::swap;\n  swap(a, b); // OBSERVE UNWARRANTED BUILD ERROR HERE\n}\n\n} // namespace other\n\nint main() {\n  int x = 42;\n  int y = 43;\n  other::foo(x, y);\n  assert(x == 43);\n  assert(y == 42);\n\n  cutlass::Bar a{42.0};\n  cutlass::Bar b{43.0};\n  other::foo(a, b);\n  assert(a.f == 43.0);\n  assert(b.f == 42.0);\n\n  // GCC 7.5 std::unique_ptr::reset calls swap,\n  // leading to the same issue as above.\n  // GCC 12.2's implementation of std::unique_ptr\n  // does not have this issue.  Nevertheless,\n  // breaking the swap two-step will break users' code,\n  // just by them happening to include your headers.\n  auto ptr = std::make_unique<cutlass::Bar>(cutlass::Bar{666.0f});\n  ptr.reset(new cutlass::Bar{777.0f}); // OBSERVE UNWARRANTED BUILD ERROR HERE\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target with Dependencies (cutlass_test)\nDESCRIPTION: This snippet defines a custom target named 'cutlass_test_unit_reduction'. It depends on the 'cutlass_test_unit_reduction_thread', 'cutlass_test_unit_reduction_kernel', and 'cutlass_test_unit_reduction_device' targets. This ensures that these dependency targets are built before the 'cutlass_test_unit_reduction' target.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/reduction/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_reduction\n  DEPENDS\n  cutlass_test_unit_reduction_thread\n  cutlass_test_unit_reduction_kernel\n  cutlass_test_unit_reduction_device\n  )\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This command adds the current directory to the list of include directories for the project. This allows the compiler to find header files located in the current directory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/13_two_tensor_op_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(\n  .\n)\n```\n\n----------------------------------------\n\nTITLE: Installing a stable release of CUTLASS Python Interface\nDESCRIPTION: This command installs the stable release of the `nvidia-cutlass` Python package using pip. Any other packages with the name `cutlass` are not affiliated with NVIDIA CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs_src/source/install.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install nvidia-cutlass\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition for Includes Check (CMake)\nDESCRIPTION: This snippet conditionally adds the 'self_contained_includes' subdirectory to the build process if the CUTLASS_ENABLE_SELF_CONTAINED_INCLUDES_CHECK CMake option is enabled. This subdirectory likely contains scripts or tools to verify that the CUTLASS headers are self-contained and do not rely on undocumented dependencies.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_ENABLE_SELF_CONTAINED_INCLUDES_CHECK)\n  add_subdirectory(self_contained_includes)\nendif()\n```\n\n----------------------------------------\n\nTITLE: GemmUniversalAdapter Class Definition\nDESCRIPTION: Defines the `GemmUniversalAdapter` class, a stateful, reusable GEMM handle built around a kernel of type `cutlass::gemm::kernel::*`. It manages the lifetime of the underlying `kernel::Params` struct and exposes APIs to create it from the host-facing arguments.  It supports both 2.x and 3.0 APIs by specializing the implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\n/*! \n  GemmUniversalAdapter is a stateful, reusable GEMM handle built around a kernel\n  of type cutlass::gemm::kernel::*\n\n  It manages the lifetime of the underlying `kernel::Params` struct, and exposes APIs\n  to create it from the host facing arguments. For power users, new static methods\n  are exposed in 3.x APIs that bypass the stateful methods or args->params lowering.\n\n  It supports kernel types that implement both the 2.x and 3.0 APIs,\n  however, this is done by specializing the implementation of GemmUniversalAdapter\n  on the two kernel API types, and thus, GemmUniversalAdapter's behavior might\n  differ between the two specializations.\n*/\ntemplate <class GemmKernel_, class Enable = void>\nclass GemmUniversalAdapter;\n```\n\n----------------------------------------\n\nTITLE: GEMM Grouped with Sigmoid Activation\nDESCRIPTION: This example demonstrates a grouped GEMM operation with sigmoid activation function using float64 data type and TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\npython gemm_grouped.py -i 8 8 4 -ta float64 -tb float64 -tc float64 -tacc float64 -m multiply_add -op TensorOp -b 64 64 16 -s 4 -w 2 2 1 -cc 80 -la RowMajor -aa 1 -lb RowMajor -ab 1 -lc ColumnMajor -ac 1 -te float64 -ep LinearCombination -p ./grouped_gemm_problem_size.csv -alpha 0.0 -beta 0.5 -pm Host -bias -activ sigmoid -bias -activ sigmoid\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS Python Interface (Colab)\nDESCRIPTION: This snippet shows how to install the CUTLASS Python interface using pip, which is necessary when running the example in a Colab environment. It is commented out by default and must be uncommented to execute.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!#pip install nvidia-cutlass\n```\n\n----------------------------------------\n\nTITLE: Configuring CUTLASS Distributed GEMM Executable in CMake\nDESCRIPTION: This CMake snippet configures a CUTLASS example executable named `65_distributed_gemm`. It uses the `cutlass_example_add_executable` macro and specifies the CUDA source file, `65_distributed_gemm.cu`, as input. This CMake function defines the executable build settings for the CUTLASS project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  65_distributed_gemm\n  65_distributed_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable in CMake\nDESCRIPTION: This CMake snippet defines an executable named `61_hopper_gemm_with_topk_and_softmax` using the `cutlass_example_add_executable` function. It specifies the source file `61_hopper_gemm_with_topk_and_softmax.cu`, which contains the implementation for the GEMM, TopK, and Softmax operations for the Hopper architecture.  No specific dependencies are explicitly listed in the snippet, but it relies on the CUTLASS library and CUDA.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/61_hopper_gemm_with_topk_and_softmax/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  61_hopper_gemm_with_topk_and_softmax\n  61_hopper_gemm_with_topk_and_softmax.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Running GEMM with User-Managed Parameters in CUTLASS (C++)\nDESCRIPTION: This code snippet shows the primary `run()` entry point API for CUTLASS GEMM, which allows users to create and manage their own parameters (`Params`). It demonstrates a static function that takes a `Params` object and a CUDA stream as input, and returns a `Status` code indicating the execution status. This provides manual control over the construction of `GemmKernel::Params` for custom kernels.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n// Primary run() entry point API that is static allowing users to create and manage their own params.\nstatic Status\nrun(Params& params, cudaStream_t stream = nullptr);\n```\n\n----------------------------------------\n\nTITLE: PredicateVector Usage C++\nDESCRIPTION: This example demonstrates how to use `PredicateVector` to manage an array of boolean predicates. It shows how to set predicates using an iterator and then use them to conditionally guard memory instructions within a loop.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nunsigned mask;\nPredicateVector<kBits> predicates;\n\n// Nested scope to update predicates via an iterator\n{\n  auto pred_it = predicates.begin();\n\n  CUTLASS_PRAGMA_UNROLL\n  for (int bit = 0; bit < kBits; ++bit, ++pred_it) {\n    bool guard = (mask & (1u << bit));\n    pred_it.set(guard);\n  }\n}\n\n// Efficient use of predicates to guard memory instructions\nT *ptr;\nArray<T, kAccesses> fragment;\n\nauto pred_it = predicates.const_begin();\nfor (int access = 0; access < kAccesses; ++access, ++pred_it) {\n  if (*pred_it) {\n    fragment[access] = ptr[access];\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for SM120 BSSP Epilogue Fusion GEMM Tests\nDESCRIPTION: This snippet adds an executable for SM120 BSSP GEMM tests with epilogue fusion.  It uses `cutlass_test_unit_gemm_device_add_executable` and sets `BATCH_SOURCES` to `ON` with a `BATCH_SIZE` of 1. This is likely done to manage compiler memory usage by processing source files in smaller batches.  It specifies the source file containing the CUDA code for epilogue fusion.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_sm120_bssp_epilogue_fusion\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n  sm120_bssp_gemm_f4_f4_f32_tensor_op_epilogue_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: By-mode Coalesce Example\nDESCRIPTION: This example demonstrates how to use the `coalesce` function with a target profile to maintain a 2-D layout. It coalesces the sub-layouts individually based on the structure specified by `Step<_1,_1>{}`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nauto a = Layout<Shape <_2,Shape <_1,_6>>,\n                Stride<_1,Stride<_6,_2>>>{};\nauto result = coalesce(a, Step<_1,_1>{});   // (_2,_6):(_1,_2)\n// Identical to\nauto same_r = make_layout(coalesce(layout<0>(a)),\n                          coalesce(layout<1>(a)));\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Test Executable in CMake\nDESCRIPTION: This snippet defines a CUTLASS test unit executable named `cutlass_test_unit_core` using the `cutlass_test_unit_add_executable` CMake function. It lists the CUDA source files that will be compiled and linked to create the executable. The listed files cover a range of core CUTLASS features like array manipulation, numerical conversions, and tensor operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/core/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_core\n  array.cu\n  half.cu\n  bfloat16.cu\n  float8.cu\n  tfloat32.cu\n  complex.cu\n  uint128.cu\n  quaternion.cu\n  matrix.cu\n  predicate_vector.cu\n  tensor_ref.cu\n  tensor_view.cu\n  matrix_coord.cu\n  numeric_conversion.cu\n  numeric_conversion_subbyte.cu\n  fast_numeric_conversion.cu\n  functional.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Bidirectional Tile Iterator Concept in C++\nDESCRIPTION: Defines a tile iterator concept that supports traversal in both forward and backward directions along a defined sequence. It inherits from `ForwardTileIteratorConcept` and adds decrement operators (`--`) to traverse to the previous tile.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\n/// Tile iterator which may be traverse in both directions along a defined sequence.\nstruct BidirectionalTileIteratorConcept : public ForwardTileIteratorConcept {\n\n  CUTLASS_DEVICE\n  BidirectionalTileIteratorConcept & operator--();      ///< pre-decrement - traverse to previous tile in sequence\n\n  CUTLASS_DEVICE\n  BidirectionalTileIteratorConcept operator--(int);     ///< post-decrement - traverse to previous tile in sequence\n};\n```\n\n----------------------------------------\n\nTITLE: Suppressing Missing Return Warnings with CUTE_GCC_UNREACHABLE C++\nDESCRIPTION: This code snippet shows how to use the `CUTE_GCC_UNREACHABLE` macro to suppress spurious \"missing return statement\" warnings in GCC. This macro invokes `__builtin_unreachable()` to indicate that a particular code path should never be taken. Use only when sure about exhaustive condition checks.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_24\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<class T>\nconstexpr auto second_form(T t) {\n  if constexpr (some_condition_v<T>) {\n    return some_function(t);\n  }\n  else if constexpr (another_condition_v<T>) {\n    return another_function(t);\n  }\n  else {\n    static_assert(sizeof(T) < 0, \"This branch always fails\");\n  }\n  CUTE_GCC_UNREACHABLE;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Blackwell Sparse GEMM Executable in CUTLASS (CMake)\nDESCRIPTION: This CMake snippet conditionally adds the '83_blackwell_sparse_gemm' executable if the `CUTLASS_NVCC_ARCHS` variable matches '100a'. It uses the `cutlass_example_add_executable` function, assuming it is defined elsewhere in the CUTLASS build system. This function takes the name of the executable and the corresponding source file as arguments. This part determines whether this specific CUDA sample is built based on the detected NVCC architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/83_blackwell_sparse_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\n\ncutlass_example_add_executable(\n  83_blackwell_sparse_gemm\n  83_blackwell_sparse_gemm.cu\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Utilities Directory Structure\nDESCRIPTION: This snippet outlines the directory structure for CUTLASS Utilities, a companion library that supports CUTLASS test programs, examples, and other client applications.  It contains reference implementations (host and device side) of CUTLASS operators for functional testing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ntools/\n  util/\n    include/\n      cutlass/\n        util/                   # CUTLASS Utility companion library\n\n          reference/            #  functional reference implementation of CUTLASS operators\n                                #    (minimal consideration for performance)\n            \n            detail/\n              *\n\n            device/             #  device-side reference implementations of CUTLASS operators\n              thread/\n              kernel/\n                *\n            host/               #  host-side reference implementations of CUTLASS operators\n              *\n```\n\n----------------------------------------\n\nTITLE: Copy Tensor Elements (with Copy_Atom)\nDESCRIPTION: Copies elements from a source Tensor to a destination Tensor, using a specified Copy_Atom to override the default copy implementation. This overload allows callers to specify a non-default copy implementation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/04_algorithms.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <class... CopyArgs,\n          class SrcEngine, class SrcLayout,\n          class DstEngine, class DstLayout>\nCUTE_HOST_DEVICE\nvoid\ncopy(Copy_Atom<CopyArgs...>       const& copy_atom,\n     Tensor<SrcEngine, SrcLayout> const& src,\n     Tensor<DstEngine, DstLayout>      & dst);\n```\n\n----------------------------------------\n\nTITLE: Configure GEMM kernels with FP32 accumulation\nDESCRIPTION: This CMake command configures CUTLASS to compile all GEMM kernels with FP32 accumulation targeting NVIDIA Ampere, Turing, and Volta architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_30\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"70;75;80\" -DCUTLASS_LIBRARY_KERNELS=s*gemm\n```\n\n----------------------------------------\n\nTITLE: Copy Partitioned Tensors C++\nDESCRIPTION: This snippet demonstrates copying data from a partitioned global memory tensor (`tAgA`) to a partitioned shared memory tensor (`tAsA`) using the `copy` function. Because the tensors are partitioned using the same thread layout, `copy` efficiently transfers the data owned by each thread.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\ncopy(tAgA(_,_,0), tAsA);\n```\n\n----------------------------------------\n\nTITLE: Checking Peer to Peer Access\nDESCRIPTION: This command uses `nvidia-smi` to check if the driver enables peer-to-peer (P2P) access between GPUs. It verifies that P2P access is enabled (indicated by `OK`) between all pairs of GPUs.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/82_blackwell_distributed_gemm/REQUIREMENTS.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi topo -p2p r\n```\n\n----------------------------------------\n\nTITLE: Multiply-Add Operation with Complex Numbers C++\nDESCRIPTION: This example demonstrates the use of the `multiply_add` function object with complex numbers. It computes `d = a * b + c` using four real-valued multiply-add operations, which can be optimized using native hardware instructions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\ncomplex<float> a;\ncomplex<float> b;\ncomplex<float> c;\ncomplex<float> d;\n\nmultiply_add<complex<float>> mad_op;\n\nd = mad_op(a, b, c);    // four single-precision multiply-add instructions\n```\n\n----------------------------------------\n\nTITLE: Conv1D Dgrad Descriptor\nDESCRIPTION: This string defines a 1D convolution backward data gradient (dgrad) operation. It includes information about input and output dimensions, padding, stride, dilation, correlation type, and alpha/beta scaling factors. The parameters specify the shape of the input and filter, padding amounts (padl, padu), stride, dilation (dil), and correlation settings (corr_alpha1_beta0). The 'h_h_h_h' indicates the data layout, and the trailing numbers might be hash values.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_dgrad_device_tensorop_sm90.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(1,8,64)_(64,1,64)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2903348270 3426051534 1688878159 1937506630\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(1,8,64)_(64,1,16)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2903348270 4139873761 674035563 2969043798\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(2,8,64)_(64,1,96)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 1021860383 1204062629 320546932 1467678853\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(7,8,64)_(64,1,256)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 1658584634 3542030567 2895019148 3285129185\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(2,6,64)_(64,3,256)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2768091490 3498596809 1298679529 150156903\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(2,8,32)_(32,3,256)_padl(1)_padu(1)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2903348270 1317709417 1298679529 2499651710\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(2,6,64)_(64,4,256)_padl(0)_padu(1)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2768091490 2047019225 1298679529 2646233918\n```\n\nLANGUAGE: text\nCODE:\n```\nconv1d dgrad_(2,13,256)_(256,3,64)_padl(0)_padu(1)_str(1)_dil(2)_corr_alpha1_beta0 h_h_h_h 2818970884 3498596809 626177484 3228881313\n```\n\n----------------------------------------\n\nTITLE: SM120 Sparse GEMM Executable with Stream-K Optimization (CMake)\nDESCRIPTION: This CMake code configures a build target for testing sparse GEMM with stream-k optimization on SM120 architecture.  It utilizes the `cutlass_test_unit_gemm_device_add_executable` function to create an executable. The `BATCH_SOURCES` and `BATCH_SIZE` directives manage the compiler memory footprint. Stream-k is an optimization method focusing on improving memory access patterns related to the K dimension in GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_sparse_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_sparse_gemm_device_tensorop_sm120_stream_k\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n  sm120_sparse_gemm_f4_f4_f32_tensor_op_f32_stream_k.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Scale Factor Tensor Layouts with CUTLASS C++\nDESCRIPTION: This code snippet shows how to create scale factor tensor layouts using `Sm1xxBlockScaledConfig` in CUTLASS. It defines SFA and SFB tensor layouts based on a problem shape (M, N, K, L) and a scale factor vector size (SFVecSize). The functions `tile_atom_to_shape_SFA` and `tile_atom_to_shape_SFB` are used to generate the layouts. The snippet also demonstrates how to create tensors using these layouts and access specific elements.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\nauto problem_shape = make_shape(M, N, K, L);\nusing SfConfig = Sm1xxBlockScaledConfig<SFVecSize>;\n\n// SFA shape: ((32,4), ceil(M/128)), ((SFVecSize,4), ceil(K/4), L)\nauto layout_sfa = SfConfig::tile_atom_to_shape_SFA(problem_shape);\n// SFB shape: ((32,4), ceil(N/128)), ((SFVecSize,4), ceil(K/4), L)\nauto layout_sfb = SfConfig::tile_atom_to_shape_SFB(problem_shape);\n\nauto tensor_sfa = make_tensor(aptr, layout_sfa);\nauto tensor_sfb = make_tensor(bptr, layout_sfb);\n// Access SF for for element m,k of A tensor\nauto val_a_mk = tensor_sfa(make_coord(m,k,0));\n```\n\n----------------------------------------\n\nTITLE: Setting NVRTC FP16 Flag (CMake)\nDESCRIPTION: Sets the CUTLASS_NVRTC_HAS_CUDA_FP16 variable to FALSE. This variable controls whether CUDA FP16 support is enabled in NVRTC compilation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(CUTLASS_NVRTC_HAS_CUDA_FP16 FALSE)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Grouped GEMM using CMake\nDESCRIPTION: This CMake code uses the 'cutlass_test_unit_gemm_device_add_executable' macro to create an executable for grouped GEMM operations. It includes a CUDA source file (.cu) that implements the group GEMM fusion for the SM120 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_grouped_gemm_device_tensorop_sm120\n  sm120_bs_gemm_nvf4_nvf4_f32_nvf4_group_gemm_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License\nDESCRIPTION: This snippet displays the full text of the BSD-3-Clause license applied to the NVIDIA CUTLASS project. It outlines the conditions for redistribution, modification, and use of the software, including disclaimers of warranty and liability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/04_algorithms.md#_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Conditional Example Addition for Blackwell Architectures (CMake)\nDESCRIPTION: This CMake snippet conditionally adds CUTLASS examples for Blackwell architectures (84a/84b) if the `CUTLASS_NVCC_ARCHS` variable matches `100a`. It uses the `cutlass_example_add_executable` function to define executables related to sparse GEMM operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/84_blackwell_narrow_precision_sparse_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  84a_blackwell_nvfp4_bf16_sparse_gemm\n  84a_blackwell_nvfp4_bf16_sparse_gemm.cu\n  )\n\ncutlass_example_add_executable(\n  84b_blackwell_mixed_mxfp8_bf16_sparse_gemm\n  84b_blackwell_mixed_mxfp8_bf16_sparse_gemm.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf8mxf4_mxf4mxf8_f32_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8mxf4_mxf4mxf8_f32_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8mxf4_mxf4mxf8_f32_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf8_mxf4_f32_f16_mxf8_q_tnt.cu\n  sm100_bssp_gemm_mxf8_mxf4_f32_f16_f16_q_tnt.cu\n  sm100_bssp_gemm_mxf8_mxf4_f32_f32_f32_q_tnt.cu\n\n  sm100_bssp_gemm_mxf4_mxf8_f32_f16_f16_q_tnt.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_4x4x5x128_3x3_256x3x6_pad_h0w0_stride_h1w1_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1527509174 2779255425 1217669626 2378687243\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_4x2x3x256_1x1_328x3x5_pad_h1w1_stride_h1w1_dil_h1w1_corr_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 3153833039 1258133396 2989357662 2523233417\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x32x32x16_6x16_32x3x3_pad_h1w1_stride_h6w2_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1441608968 3983207745 3357056235 3334646036\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x233x35x48_233x35_24x7x5_pad_h3w2_stride_h1w1_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 3916058745 443607763 3443985888 160172027\n```\n\n----------------------------------------\n\nTITLE: Conv3D Dgrad Descriptor\nDESCRIPTION: This string defines a 3D convolution backward data gradient (dgrad) operation. It specifies parameters like input and filter dimensions, padding, stride, dilation, correlation type, and scaling factors. Input and filter shapes, padding amounts (padl, padu) for each dimension, stride, dilation, and correlation settings are included. 'h_h_h_h' is data layout, and the numbers are likely hash values or identifiers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_dgrad_device_tensorop_sm90.txt#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nconv3d dgrad_(1,1,8,8,64)_(64,1,1,1,16)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 1260538290 4139873761 2377544542 2164020490\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d dgrad_(2,1,8,8,64)_(64,1,1,1,96)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 913918101 1204062629 2917884534 3849387550\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d dgrad_(2,3,4,6,64)_(64,3,4,5,96)_padl(1,1,1)_padu(1,1,1)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 3168426837 2541149106 3798069020 2802887403\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d dgrad_(2,2,4,5,64)_(64,3,4,5,96)_padl(1,0,1)_padu(0,2,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 1829721897 2541149106 3798069020 2514979496\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d dgrad_(2,13,6,5,64)_(64,3,4,5,96)_padl(1,0,1)_padu(0,2,0)_str(1,1,1)_dil(2,2,3)_corr_alpha1_beta0 h_h_h_h 3720598515 2541149106 2799255748 1382638415\n```\n\n----------------------------------------\n\nTITLE: Adding Grouped Blockwise GEMM Executable Example in CMake\nDESCRIPTION: This snippet adds an executable for a grouped blockwise GEMM example using the `cutlass_example_add_executable` CMake macro. It uses the source file `81_blackwell_grouped_gemm_blockwise.cu` and applies pre-defined test command options and small problem size configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  81_blackwell_grouped_gemm_blockwise \n  81_blackwell_grouped_gemm_blockwise.cu\n  TEST_COMMAND_OPTIONS\n  TEST_SMALL\n)\n```\n\n----------------------------------------\n\nTITLE: Affine Im2col Transformed Activation Layout\nDESCRIPTION: This code defines the affine im2col transformed activation tensor layout using CuTe.  It specifies the shape and stride for a dense 3D convolution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/README.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// im2col transformed activation layout: ((nzpq), (ctrs)) => idx\nauto xformed_act_layout = make_layout(\n  make_shape (make_shape (      N,     Z,   P, Q), make_shape (  C,      T,   R, S)),\n  make_stride(make_stride(D*H*W*C, H*W*C, W*C, C), make_stride(_1{}, H*W*C, W*C, C)));\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution based on CUTLASS NVCC Architecture in CMake\nDESCRIPTION: This CMake snippet conditionally executes the code block if the CUTLASS_NVCC_ARCHS variable matches '100a'. This likely corresponds to a specific NVIDIA architecture, like Blackwell, determining whether these GEMM examples are built.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Import CUTLASS and PyTorch Packages\nDESCRIPTION: This snippet imports necessary packages, including `torch` for tensor operations, `cutlass` for CUTLASS functionality, `cutlass.epilogue` for epilogue-related functions, `cutlass.Tensor` (aliased as `FakeTensor`), and `cutlass.utils.profiler.CUDAEventProfiler` for performance profiling. It also sets a flag to control the printing of C++ GEMM declarations and checks if the device compute capability is SM80 or SM90, exiting if it is not.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/04_epilogue_visitor.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport cutlass\nfrom cutlass.epilogue import relu\nfrom cutlass import Tensor as FakeTensor\nfrom cutlass.utils.profiler import CUDAEventProfiler\n\n# This controls whether ther C++ GEMM declaration will be printed at each step. Set to `false` to\n# omit this information.\nprint_module = True\n\n# The Epilogue Visitor feature currently only works for SM80 and 90\nfrom cutlass.backend.utils.device import device_cc\nif device_cc() not in [80, 90]:\n    import sys\n    sys.exit()\n\nm = 16384\nn = m\nk = 512\n\ntype_A = torch.float16\ntype_B = torch.float16\ntype_C = torch.float16\ntype_D = torch.float16\n\ntorch.manual_seed(2023)\nscope_min = -4\nscope_max = 4\ntensor_A = torch.ceil(torch.empty(size=(m, k), dtype=type_A, device=\"cuda\").uniform_(scope_min, scope_max))\ntensor_B = torch.ceil(torch.empty(size=(k, n), dtype=type_B, device=\"cuda\").uniform_(scope_min, scope_max))\ntensor_C = torch.ceil(torch.empty(size=(m, n), dtype=type_C, device=\"cuda\").uniform_(scope_min, scope_max))\ntensor_D = torch.zeros_like(tensor_C)\n\nplan = cutlass.op.Gemm(element=torch.float16, layout=cutlass.LayoutType.RowMajor, element_accumulator=torch.float32)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (nvf4_nvf4_f32_f16_nvf4_o)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f16_nvf4_o` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f16_nvf4_o\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f16_nvf4_o_tnt_sfd.cu\n  sm100_bssp_gemm_nvf4_nvf4_f32_void_nvf4_o_tnt_sfd.cu\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f16_nvf4_o_tnn_sfd.cu\n  sm100_bssp_gemm_nvf4_nvf4_f32_void_nvf4_o_tnn_sfd.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Define shape for HMMA NT operation\nDESCRIPTION: Defines the logical shape of the HMMA NT operation as 8x8x4 using the `Shape` template. This shape represents the MxNxK dimensions of the MMA operation, where M = 8, N = 8, and K = 4.  The `Shape_MNK` type alias is used within the `MMA_Traits` struct to represent this shape.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nusing Shape_MNK = Shape <_8,_8,_4>;\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Executable in CMake\nDESCRIPTION: This CMake function adds an executable target named 'cutlass_test_unit_util' to the build system. It links together source files such as 'tensor_reduce.cu', 'cutlass_test_levels.cu', and 'rms_norm.cu' to create the executable. This executable will be used to run tests and benchmarks related to tensor operations within the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_util\n  tensor_reduce.cu\n  cutlass_test_levels.cu\n  rms_norm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Run PyTorch Docker Container\nDESCRIPTION: This command launches an NGC PyTorch Docker container with GPU support and port mapping for JupyterLab.  It allows users to access CUTLASS examples in a pre-configured environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all -it --rm nvcr.io/nvidia/pytorch:23.08-py3 -p 8888:8888\n```\n\n----------------------------------------\n\nTITLE: Adding Narrow Precision Subdirectory - CMake\nDESCRIPTION: This line adds the 'narrow_precision' subdirectory to the build process. This allows CMake to find and process the CMakeLists.txt file within that subdirectory, including any targets or definitions it contains.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(narrow_precision)\n```\n\n----------------------------------------\n\nTITLE: Creating Executable for Collective Builder using CMake\nDESCRIPTION: This CMake function creates an executable named '49_collective_builder' using the source file '49_collective_builder.cu'. The 'cutlass_example_add_executable' function is specific to the CUTLASS build system and helps setup the executable with necessary dependencies. It aims to demonstrate how to build and run a CUTLASS-based collective communication operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/49_hopper_gemm_with_collective_builder/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  49_collective_builder\n  49_collective_builder.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Example Executable with CMake\nDESCRIPTION: This snippet defines a CUTLASS example executable named '45_dual_gemm' using the `cutlass_example_add_executable` CMake macro. It specifies 'dual_gemm.cu' as the source file for the executable. The macro likely handles compiler flags, linking, and other build configurations required by CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/45_dual_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  45_dual_gemm\n  dual_gemm.cu \n  )\n```\n\n----------------------------------------\n\nTITLE: Run Functional Test and Save Results\nDESCRIPTION: This command runs a functional test of the GEMM kernels with multiple problem sizes and beta values, profiling each configuration for one iteration. The results are saved in a CSV file named 'functional-test.csv'.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler  --operation=Gemm \\\n   --m=8,56,120,136,256,264,512,520,1024,1032,4096,8192,16384 \\\n   --n=8,56,120,136,256,264,512,520,1024,1032,4096,8192,16384 \\\n   --k=8,16,32,64,128,256,288,384,504,512,520 \\\n   --beta=0,1,2 --profiling-iterations=1 \\\n   --providers=cutlass --output=functional-test.csv\n```\n\n----------------------------------------\n\nTITLE: Map Macro Tile to True Tile (Python)\nDESCRIPTION: This Python snippet maps macro tile indices `(i_macro, j_macro)` to true tile indices `(i, j)` based on the threadblock shape. It considers cases where the threadblock shape M is greater than N, M is less than N, or M equals N, adjusting the indices accordingly using the ratio `r` and the threadblock ID `t`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif (ThreadblockShape::M > ThreadblockShape::N):\n    r = ThreadblockShape::M / ThreadblockShape::N\n    i = i_macro\n    j = (j_macro * r) + (t % r)\nelif (ThreadblockShape::M < ThreadblockShape::N):\n    r = ThreadblockShape::N / ThreadblockShape::M\n    i = (i_macro * r) + (t % r)\n    j = j_macro\nelse:\n    i = i_macro\n    j = j_macro\n```\n\n----------------------------------------\n\nTITLE: GEMM Grouped F16 Device Schedule Example 2\nDESCRIPTION: This example runs a GEMM Grouped operation on device using float16 inputs and float32 output. It reads problem sizes from a CSV file and executes the grouped GEMM with TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npython gemm_grouped.py -i 16 8 16 -ta float16 -tb float16 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la ColumnMajor -aa 8 -lb ColumnMajor -ab 8 -lc ColumnMajor -ac 4 -te float32 -ep LinearCombination -p ./grouped_gemm_problem_size.csv -alpha 2.0 -beta 1.0 -pm Device\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executables in CMake\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` CMake function to add executables for FP16 and FP8 GEMM examples. The examples are added conditionally based on the `CUTLASS_NVCC_ARCHS` variable. The defined swizzle options are passed as arguments.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/70_blackwell_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CUTLASS_NVCC_ARCHS STREQUAL \"100\")\ncutlass_example_add_executable(\n  70_blackwell_fp16_gemm\n  70_blackwell_fp16_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_SWIZZLE_1\n  TEST_SWIZZLE_2\n  TEST_SWIZZLE_5\n  TEST_SWIZZLE_5_UNEVEN\n)\n\ncutlass_example_add_executable(\n  70_blackwell_fp8_gemm\n  70_blackwell_fp8_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_SWIZZLE_1\n  TEST_SWIZZLE_2\n  TEST_SWIZZLE_5\n  TEST_SWIZZLE_5_UNEVEN\n)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ReLU Verification\nDESCRIPTION: This snippet verifies the result of the GEMM with ReLU activation by comparing it to a reference implementation of ReLU applied to the output tensor D.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/01_epilogue.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrelu_ref = (tensor_D >= 0).astype(type_D) * tensor_D\nnp.testing.assert_array_equal(relu_ref, tensor_D_relu)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for SM120 BSSP General GEMM Tests\nDESCRIPTION: This snippet uses the `cutlass_test_unit_gemm_device_add_executable` function to create an executable for general SM120 BSSP GEMM tests. It specifies the target name and lists the source files required for the test. These source files contain the CUDA code that implements the GEMM operations for different data types and layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_sm120_bssp_general\n\n  sm120_bssp_gemm_f4_f4_f32_tensor_op.cu\n  sm120_bssp_gemm_f6_f4_f32_tensor_op.cu\n  sm120_bssp_gemm_f8_f6_f32_tensor_op.cu\n  sm120_bssp_gemm_f4t_f4n_f4t_tensor_op.cu\n  sm120_bssp_gemm_f8t_f8n_f8t_tensor_op.cu\n)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_1x17x11x288_17x11_160x3x3_pad_h1w1_stride_h1w1_dil_h1w1_corr_alpha2_beta2 s4nhwc_s4nhwc_inhwc_i_f 2451612476 2037530725 2255344759 3879663798\n```\n\n----------------------------------------\n\nTITLE: Defining SM100 Sparse GEMM Unit Test Target (CMake)\nDESCRIPTION: This snippet defines a custom target named `cutlass_test_unit_gemm_device_sm100_sparse_narrow_precision`. This target depends on other targets which build the individual test executables for different narrow precision data types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm100_sparse_narrow_precision\n  DEPENDS\n  cutlass_test_unit_gemm_device_sm100_sparse_f4xf4\n  cutlass_test_unit_gemm_device_sm100_sparse_f6xf6\n  cutlass_test_unit_gemm_device_sm100_sparse_f4f6xf4f6\n  cutlass_test_unit_gemm_device_sm100_sparse_f4f8xf4f8\n  cutlass_test_unit_gemm_device_sm100_sparse_f6f8xf6f8\n)\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x124x224x96_229x129_24x7x7_pad_h3w3_stride_h1w1_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 805200720 858259717 3333097025 285381561\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS Profiler (make)\nDESCRIPTION: This snippet shows how to build the CUTLASS profiler using the `make` command. It's a straightforward compilation step, assuming the necessary dependencies and environment are already configured.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Adding Grouped Groupwise GEMM Executable Example in CMake\nDESCRIPTION: This snippet adds an executable for a grouped groupwise GEMM example using the `cutlass_example_add_executable` CMake macro. It specifies the source file `81_blackwell_grouped_gemm_groupwise.cu` along with test command options and configurations for small problem sizes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  81_blackwell_grouped_gemm_groupwise \n  81_blackwell_grouped_gemm_groupwise.cu\n  TEST_COMMAND_OPTIONS\n  TEST_SMALL\n)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable with Visitor\nDESCRIPTION: This code defines an executable named `15_ampere_sparse_tensorop_gemm_with_visitor` using the `cutlass_example_add_executable` macro. It links the source file `ampere_sparse_tensorop_gemm_with_visitor.cu` to the executable. This likely demonstrates a GEMM operation that utilizes a visitor pattern for additional flexibility or customization.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/15_ampere_sparse_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  15_ampere_sparse_tensorop_gemm_with_visitor\n  ampere_sparse_tensorop_gemm_with_visitor.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: GEMM K Dimension Mapping\nDESCRIPTION: These equations map the linearized CRS indices to the inner GEMM K dimension and vice versa. This allows the triple loop nest over CRS to be represented as a single loop, improving computational efficiency. The inverse relations calculate the individual indices from the linearized index.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ngemm_k = s + S * (r + R * c)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Path\nDESCRIPTION: Sets the CUDACXX environment variable to the path of the nvcc compiler. This ensures that CMake can find the CUDA compiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x17x11x288_17x11_160x3x3_pad_h1w1_stride_h1w1_dil_h1w1_corr_alpha2_beta2 fnhwc_fnhwc_fnhwc_f_f 3160120111 1536824438 422976060 2522704893\n```\n\n----------------------------------------\n\nTITLE: Creating Executable for Hopper GEMM with Weight Prefetch in CUTLASS\nDESCRIPTION: This CMake command adds an executable named '63_hopper_gemm_with_weight_prefetch' which compiles the provided CUDA source file. The executable links to other command options and the predefined test prefetch case to customize the runtime execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  63_hopper_gemm_with_weight_prefetch\n  63_hopper_gemm_with_weight_prefetch.cu\n  TEST_COMMAND_OPTIONS\n  TEST_PREFETCH_CASE\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executable with CMake\nDESCRIPTION: This CMake command adds an executable for a CUTLASS example, specifically the Hopper FP8 warp specialized grouped GEMM with blockwise scaling. It uses the `cutlass_example_add_executable` macro and includes the specified source file and test parameters defined earlier. It uses a list of pre-defined test configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling\n  68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Utility Subdirectory\nDESCRIPTION: This command includes the 'util' subdirectory in the CMake build process. This subdirectory likely contains utility functions or modules used by other parts of the CUTLASS project. It is always included, regardless of other build flags.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(util)\n```\n\n----------------------------------------\n\nTITLE: Conditional Target Definition for SM120 BSSP GEMM Tests\nDESCRIPTION: This snippet conditionally defines a custom target for SM120 BSSP GEMM unit tests.  It checks if the `CUTLASS_NVCC_ARCHS` variable matches \"120a\" and then adds a custom target that depends on other specific test executables. This ensures that the tests are only built and run when the target architecture is SM120.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 120a)\n\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm120_bssp\n  DEPENDS\n  cutlass_test_unit_gemm_device_sm120_bssp_general\n  cutlass_test_unit_gemm_device_sm120_bssp_stream_k\n  cutlass_test_unit_gemm_device_sm120_bssp_epilogue_fusion\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS Python interface in developer mode\nDESCRIPTION: This command installs the CUTLASS Python interface in developer mode. This mode allows changes to the Python interface source to be immediately reflected when using it. It requires Python and setup.py to be present in the current directory. The `--user` flag installs the package for the current user.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/_sources/install.md.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py develop --user\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Utility Executable in CMake\nDESCRIPTION: This snippet defines a CUTLASS utility executable named '01_cutlass_utilities' using the custom CMake function `cutlass_example_add_executable`. The source file for this executable is specified as 'cutlass_utilities.cu'. This function is assumed to be defined elsewhere in the CMake project and handles the details of compiling and linking the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/01_cutlass_utilities/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  01_cutlass_utilities\n  cutlass_utilities.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x32x32x16_6x16_32x3x3_pad_h1w1_stride_h6w2_dil_h1w1_corr_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1441608968 3983207745 3357056235 4220059751\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 1SM, Mxf4)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following a sparse matrix multiplication, using TMA (Tensor Memory Accelerator) on a single SM (Streaming Multiprocessor). It is designed specifically for narrow precision using the mxf4 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized1SmMxf4`\n```\n\n----------------------------------------\n\nTITLE: Finding Python Interpreter with CMake\nDESCRIPTION: Finds the Python 3 interpreter (version 3.5 or higher) using CMake's `find_package` command. It is a prerequisite for some of the CUTLASS build tools or scripts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Python3 3.5 COMPONENTS Interpreter REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: HostTensor Declaration\nDESCRIPTION: Declaration of the HostTensor class template, used for allocating tensors in both host and device memory. It's compatible with all CUTLASS numeric data types and layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename Element, typename Layout>\nclass HostTensor;\n```\n\n----------------------------------------\n\nTITLE: Profiling Single SGEMM CUDA Kernel\nDESCRIPTION: This command invokes the CUTLASS profiler to run a single SGEMM CUDA kernel. The `--kernels` argument specifies the 'sgemm' kernel, and additional arguments define the matrix dimensions (m, n, k).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=3456 --n=4096 --k=4096\n```\n\n----------------------------------------\n\nTITLE: Short function declaration and definition in C++\nDESCRIPTION: Demonstrates the formatting style for short function headers in CUTLASS. Short function headers can be written on one line without a newline between the parameter parenthesis and the opening curly bracket of the function body.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint short_name(int x, int y) {\n  return x + y;\n}\n```\n\n----------------------------------------\n\nTITLE: Running CUTLASS Example with synclog (Shell)\nDESCRIPTION: This shell command demonstrates how to run a CUTLASS example with synclog enabled and redirect its output to a file. The `--iterations=0` flag sets the profiling iteration count to 0, ensuring synclog information is printed only for the reference run.  The output is redirected to `synclog.txt`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\n$ ./54_hopper_fp8_warp_specialized_gemm --iterations=0 &> synclog.txt\n```\n\n----------------------------------------\n\nTITLE: Adding Another Subdirectory (CMake)\nDESCRIPTION: This line adds a subdirectory named `device_3x` to the build process. This likely includes additional unit tests or components related to that specific device configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(device_3x)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable for Sparse GEMM with CMake\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` macro to define a new executable target named `43_ell_block_sparse_gemm`. It takes the name of the executable and the source file `ell_block_sparse_gemm.cu` as arguments.  This macro likely handles the compilation and linking process for the given CUDA source file to create an executable within the CUTLASS build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/43_ell_block_sparse_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  43_ell_block_sparse_gemm\n  ell_block_sparse_gemm.cu \n  )\n```\n\n----------------------------------------\n\nTITLE: Adding SM100 Sparse F4xF4 GEMM Test Executable (CMake)\nDESCRIPTION: This snippet utilizes a CMake function `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for the `f4xf4` sparse GEMM test on the SM100 architecture. It specifies the source files to be compiled and linked into the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_f4xf4\n  sm100_sp_gemm_f4_f4_f32_f16_f8_tn.cu\n  sm100_sp_gemm_f4_f4_f32_f16_f16_tn.cu\n  sm100_sp_gemm_f4_f4_f32_f32_f32_tn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_1x124x224x96_229x129_24x7x7_pad_h3w3_stride_h1w1_dil_h1w1_conv_alpha1_beta0 s4nhwc_s4nhwc_inhwc_i_f 260007636 1848202930 520645775 4223629118\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS SIMT Example Executable in CMake\nDESCRIPTION: This snippet defines an executable named '20_simt_canonical' using the 'cutlass_example_add_executable' CMake macro. The macro takes the executable name and the CUDA source file ('simt_canonical.cu') as arguments. This sets up the build system to compile and link the specified source file into an executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/20_simt_canonical/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  20_simt_canonical\n  simt_canonical.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Registry Key for Long Path Support (PowerShell)\nDESCRIPTION: This PowerShell command shows the location of the registry key that enables long path support in Windows. Enabling this is important as CUTLASS has many files and directory paths that challenge the default maximum file path length limit, often resulting in build failures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/build/building_in_windows_with_visual_studio.md#_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\nComputer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\\LongPathsEnabled\n```\n\n----------------------------------------\n\nTITLE: Convolution 2D Weight Gradient Configuration\nDESCRIPTION: This configuration defines parameters for a 2D convolution weight gradient calculation. It specifies input and output dimensions, padding, stride, dilation, alpha/beta values, and hash values.  The parameters are formatted as conv2d wgrad_(input_dims)_(output_dims)_padl(pad_left)_padu(pad_up)_str(stride)_dil(dilation)_corr_alpha1_beta0 h_h_h_h hash1 hash2 hash3 hash4\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_wgrad_device_tensorop_sm90.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_(1,8,8,64)_(1,8,8,128)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1260538290 2039847788 1911791555 1010376807\nconv2d wgrad_(1,8,8,16)_(1,8,8,128)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1021860383 2039847788 626177484 4023661355\nconv2d wgrad_(2,8,8,96)_(2,8,8,128)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1661538138 3542030567 2917884534 798509586\nconv2d wgrad_(7,8,8,256)_(7,8,8,128)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 854475261 2837606078 184863710 2833795023\nconv2d wgrad_(2,6,6,256)_(2,8,8,128)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 269005094 3542030567 587374223 722686340\nconv2d wgrad_(2,8,8,256)_(2,8,8,128)_padl(1,1)_padu(1,1)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1788526765 3542030567 587374223 3596796540\nconv2d wgrad_(2,8,5,256)_(2,8,8,128)_padl(1,1)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1986808706 3542030567 2799255748 3793023440\nconv2d wgrad_(2,8,5,256)_(2,16,16,128)_padl(1,1)_padu(0,0)_str(2,3)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1986808706 2047019225 2799255748 1869700538\nconv2d wgrad_(2,15,5,256)_(2,16,16,128)_padl(1,1)_padu(0,0)_str(1,1)_dil(2,3)_corr_alpha1_beta0 h_h_h_h 2554054572 2047019225 2799255748 1061991327\nconv2d wgrad_(2,8,2,256)_(2,16,16,128)_padl(1,1)_padu(0,0)_str(2,3)_dil(2,3)_corr_alpha1_beta0 h_h_h_h 913918101 2047019225 2799255748 1723667339\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Predicate Tensor\nDESCRIPTION: This snippet demonstrates how to create a predicate tensor and use it to mask off out-of-bounds elements in a tensor A. It involves creating an identity layout, dividing it into tiles, creating a boolean tensor to represent the predicate, and using it to control memory access.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n// Create the predicate tensor\nLayout idA  = make_layout(shape(A));   // e.g. 1000:1\nLayout idAB = logical_divide(idA, B);  // e.g. (128,8):(1,128)\n\nTensor pred = make_tensor<bool>(shape(idAB));\nfor (int i = 0; i < size(pred); ++i) {\n  pred(i) = idAB(i) < size(A);\n}\n\n// ... intervening code ...\n\n// Use the predicate tensor.  c is some coordinate.\n// This code would likely live inside some algorithm.\nif (pred(c)) { copy(idAB(c), smem(c)); }\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Test Unit Executable in CMake\nDESCRIPTION: This CMake code defines a test unit executable named `cutlass_test_unit_gemm_threadblock`. It utilizes the `cutlass_test_unit_add_executable` function, which likely handles compiler flags, dependencies, and linking specific to CUTLASS test units.  The source files listed are CUDA files (.cu) that implement different GEMM test cases.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/threadblock/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_gemm_threadblock\n  mma_multistage.cu\n  mma_multistage_sparse.cu\n  mma_pipelined_sm80.cu\n  mma_multistage_slicedk.cu\n  mma_pipelined_slicedk.cu\n  mma_pipelined_wmma_sm70.cu\n  mma_pipelined_wmma_sm75.cu\n  mma_singlestage_wmma_sm70.cu\n  mma_singlestage_wmma_sm75.cu\n  mma_pipelined_sm70.cu\n  mma_pipelined_sm75.cu\n  mma_pipelined_simt.cu\n  mma_planar_complex_sm80.cu\n\n)\n```\n\n----------------------------------------\n\nTITLE: Define SMEM Layouts (GEMM_NT) C++\nDESCRIPTION: This snippet defines the shared memory layouts for A and B tensors in the `gemm_nt` kernel. It uses `make_layout` with `make_shape` to create M-major and N-major layouts based on block dimensions `bM`, `bK`, `bN`. These layouts determine how data is stored in shared memory and affects the efficiency of memory accesses.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_10\n\nLANGUAGE: c++\nCODE:\n```\n  // Define the smem layouts (static)\n  auto sA = make_layout(make_shape(bM, bK));   // (m,k) -> smem_idx; m-major\n  auto sB = make_layout(make_shape(bN, bK));   // (n,k) -> smem_idx; n-major\n```\n\n----------------------------------------\n\nTITLE: Setting Random Problem Size Test Options in CMake\nDESCRIPTION: This snippet defines CMake variables to set up test options for GEMM (General Matrix Multiplication) examples using random problem sizes. The `--iterations=0` flag likely configures the test to run a single iteration with randomly generated data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_RANDOM --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Add F8-F8 Convolution Test Executable (SM89)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with F8 input and output on SM89 architecture, conditionally compiled based on the `CUTLASS_NVCC_MAX_ARCH` variable. It includes source files for forward propagation with F32 and F16 tensor op.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 89)\n  # Conv - F8 input, F8 output\n  cutlass_test_unit_add_executable(\n    cutlass_test_unit_conv_device_tensorop_f8_sm89\n\n    conv2d_fprop_implicit_gemm_f8nhwc_f8nhwc_f8nhwc_tensor_op_f32_sm89.cu\n    conv2d_fprop_implicit_gemm_f8nhwc_f8nhwc_f8nhwc_tensor_op_f16_sm89.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Thread-level Mma Concept in CUTLASS\nDESCRIPTION: This code defines the concept for thread-level matrix multiply-accumulate (Mma) operations in CUTLASS.  It specifies required types and methods, including data types, layouts and fragments.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nstruct Mma {\n\n  /// Shape of warp-level matrix operation (concept: GemmShape)\n  struct Shape;\n\n  /// Data type of multiplicand A (concept: numeric type)\n  struct ElementA;\n\n  /// Layout of multiplicand A (concept: Layout)\n  struct LayoutA;\n\n  /// Fragment object loaded from IteratorA (concept: Array<ElementA, ..>)\n  struct FragmentA;\n\n  /// Data type of multiplicand B (concept: numeric type)\n  struct ElementB;\n\n  /// Layout of multiplicand B (concept: Layout)\n  struct LayoutB;\n\n  /// Fragment object loaded from IteratorA (concept: Array<ElementB, ..>)\n  struct FragmentB;\n\n  /// Data type of accumulator matrix C (concept: numeric type)\n  struct ElementC;\n\n  /// Layout of accumulator matrix C (concept: Layout)\n  struct LayoutC;\n\n  /// Fragment object loaded from IteratorA (concept: Array<ElementC, ..>)\n  struct FragmentC;\n\n  //\n  // Methods\n  //\n\n  /// Computes a matrix multiply-accumulate\n  CUTLASS_DEVICE\n  void operator()(\n    FragmentC &D, \n    FragmentA const &A, \n    FragmentB const &B, \n    FragmentC const &C);\n};\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Reduction Test Unit - CMake\nDESCRIPTION: This CMake command adds an executable target named 'cutlass_test_unit_reduction_device'. It compiles the specified CUDA source files ('tensor_reduce_strided.cu' and 'tensor_reduce_contiguous.cu') and links them to create the executable. This executable will be used to test the device-side reduction functionality of the Cutlass library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/reduction/device/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_reduction_device\n  tensor_reduce_strided.cu\n  tensor_reduce_contiguous.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Executable Target for Planar Complex Array GEMM in CMake\nDESCRIPTION: This snippet creates an executable named '11_planar_complex_array' from the source file 'planar_complex_array.cu'. It then links the executable with the CUTLASS library ('cutlass_lib'), CUDA libraries ('cuda'), and CUTLASS utility includes ('cutlass_tools_util_includes'). This setup uses the 'cutlass_example_add_executable' macro and 'target_link_libraries' command which are CMake functions to create the executable and link necessary libraries.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/11_planar_complex_array/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_ENABLE_LIBRARY)\n\n# Planar Complex Array GEMM example\ncutlass_example_add_executable(\n  11_planar_complex_array\n  planar_complex_array.cu\n)\n\ntarget_link_libraries(\n  11_planar_complex_array\n  PRIVATE\n  cutlass_lib\n  cutlass_tools_util_includes\n  cuda\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: GEMM F32 Example\nDESCRIPTION: This example demonstrates a GEMM operation using float32 data type in CUTLASS. It specifies various parameters like input dimensions, tensor layouts, block size, split-K parameter, and alpha/beta values. It also uses multiply_add_fast_bf16 for optimized performance.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython gemm.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add_fast_bf16 -op TensorOp -b 128 128 32 -s 3 -w 2 2 1 -cc 80 -la RowMajor -aa 4 -lb ColumnMajor -ab 4 -lc RowMajor -ac 4 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -p 512 256 128 -alpha 1.0 -beta 0.5 -gm Gemm -k 1\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_1x124x224x96_229x129_24x7x7_pad_h3w3_stride_h1w1_dil_h1w1_corr_alpha1_beta0 s4nhwc_s4nhwc_inhwc_i_f 260007636 1848202930 520645775 3465036547\n```\n\n----------------------------------------\n\nTITLE: CUTLASS BSD-3-Clause License\nDESCRIPTION: This snippet contains the BSD-3-Clause license under which CUTLASS is distributed.  It outlines the terms and conditions for redistribution and use of the software, including the requirement to retain copyright notices and disclaimers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Installing Clang Dependencies (Ubuntu)\nDESCRIPTION: This command installs the necessary packages on Ubuntu to resolve missing dependencies when building with Clang.  It includes essential development tools, libraries, and build systems.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/build/building_with_clang_as_host_compiler.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo apt-get install clang cmake ninja-build pkg-config libgtk-3-dev liblzma-dev libstdc++-12-dev\n```\n\n----------------------------------------\n\nTITLE: Printing Half-Precision Data\nDESCRIPTION: This C++ code snippet demonstrates how to print the contents of a variable storing half-precision data using CUTLASS. It includes necessary headers for CUTLASS, numeric types, and core I/O.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_20\n\nLANGUAGE: c++\nCODE:\n```\n#include <iostream>\n#include <cutlass/cutlass.h>\n#include <cutlass/numeric_types.h>\n#include <cutlass/core_io.h>\n\nint main() {\n\n  cutlass::half_t x = 2.25_hf;\n\n  std::cout << x << std::endl;\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executable in CMake\nDESCRIPTION: Uses the `cutlass_example_add_executable` CMake function to create an executable named '23_ampere_gemm_operand_reduction_fusion'. It links this executable with the source file 'ampere_gemm_operand_reduction_fusion.cu' and passes in test command options from `TEST_COMMAND_OPTIONS`, standard test parameters from `TEST_STANDARD`, and large performance check parameters from `TEST_LARGE_PERFCHECK`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  23_ampere_gemm_operand_reduction_fusion\n  ampere_gemm_operand_reduction_fusion.cu\n  TEST_COMMAND_OPTIONS\n  TEST_STANDARD\n  TEST_LARGE_PERFCHECK\n  )\n```\n\n----------------------------------------\n\nTITLE: Selecting a Distributed GEMM Schedule (cpp)\nDESCRIPTION: This snippet shows how to select a specific distributed GEMM schedule using the `cutlass::distributed::schedules` namespace. It defines the type `DistSchedule` as an alias for the chosen schedule, which is used throughout the distributed GEMM implementation. Users can change this line to experiment with different schedules supported by CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nusing DistSchedule = cutlass::distributed::schedules::AllGather1D_TilingCD_RotatingA<TP>;\n```\n\n----------------------------------------\n\nTITLE: Logical Product Compatibility Post-Conditions in CuTe\nDESCRIPTION: This code snippet demonstrates the compatibility post-conditions of the `logical_product` function in the CuTe library.  It specifies that the rank of the result is 2, layout A is compatible with mode-0 of the result, and layout B is compatible with mode-1 of the result.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_13\n\nLANGUAGE: c++\nCODE:\n```\n// @post rank(result) == 2\n// @post compatible(layout_a, layout<0>(result))\n// @post compatible(layout_b, layout<1>(result))\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable using CMake\nDESCRIPTION: This CMake code snippet defines an executable named `00_basic_gemm` using the `cutlass_example_add_executable` macro. The macro likely handles the compilation and linking of the specified source file `basic_gemm.cu` and any necessary CUTLASS libraries. It assumes the macro is defined within the CUTLASS project's CMake environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/00_basic_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  00_basic_gemm\n  basic_gemm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Thread-level Mma Template in CUTLASS\nDESCRIPTION: This code defines the template for thread-level matrix multiply operators in CUTLASS. It accepts template arguments for shape, data types, layouts, and the operator type. It utilizes partial specialization.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api.md#_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\nnamespace cutlass {\nnamespace gemm {\nnamespace thread {\n\n/// Structure to compute the matrix product\ntemplate <\n  /// Size of the Gemm problem - concept: gemm::GemmShape<>\n  typename Shape,\n  /// Data type of A elements\n  typename ElementA,\n  /// Layout of A matrix (concept: MatrixLayout)\n  typename LayoutA,\n  /// Data type of B elements\n  typename ElementB,\n  /// Layout of B matrix (concept: MatrixLayout)\n  typename LayoutB,\n  /// Element type of C matrix\n  typename ElementC,\n  /// Layout of C matrix (concept: MatrixLayout)\n  typename LayoutC,\n  /// Concept: arch::OpMultiplyAdd or arch::Mma<>\n  typename Operator = arch::OpMultiplyAdd,\n  /// Used for partial specialization\n  typename Enable = bool\n>\nstruct Mma;\n\n} // namespace thread\n} // namespace gemm\n} // namespace cutlass\n```\n\n----------------------------------------\n\nTITLE: Permuting M-Mode in TiledMMA with Custom Tile Layout in C++\nDESCRIPTION: This code demonstrates how to permute the M-mode in a TiledMMA to create a valid TiledMMA with contiguous access in m-coordinates in the A-matrix. It uses a scatter permutation (4,4,2):(1,8,4) on the M-mode.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nTiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                                  Layout<Shape <_2,_2>,\n                                         Stride<_2,_1>>{},       // 2x2 n-major layout of Atoms\n                                  Tile<Layout<Shape <_4,_4,_2>,\n                                              Stride<_1,_8,_4>>, // Permutation on M, size 32\n                                       _32,                      // Permutation on N, size 32 identity\n                                       _4>{});                   // Permutation on K, size 4 identity\n    print_latex(mma);\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_3x28x28x256_14x14_256x2x2_pad_h0w0_stride_h2w2_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1467525936 2166215692 2099843274 2254410747\n```\n\n----------------------------------------\n\nTITLE: Defining Another Custom Target in CMake\nDESCRIPTION: This snippet defines another custom target named 'test_unit_epilogue'.  Similar to the previous target, it depends on other custom targets relating to thread, warp and threadblock epilogue testing.  The difference in naming suggests a different scope or level of testing compared to 'cutlass_test_unit_epilogue'.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/epilogue/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  test_unit_epilogue\n  DEPENDS\n  test_unit_epilogue_thread\n  test_unit_epilogue_warp\n  test_unit_epilogue_threadblock\n )\n```\n\n----------------------------------------\n\nTITLE: Zipped Divide Example in CuTe\nDESCRIPTION: This code demonstrates the usage of `logical_divide` and `zipped_divide` functions to partition a layout. It showcases how the modes are rearranged into more convenient forms. This example requires the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n// A: shape is (9,32)\nauto layout_a = make_layout(make_shape (Int< 9>{}, make_shape (Int< 4>{}, Int<8>{})),\n                            make_stride(Int<59>{}, make_stride(Int<13>{}, Int<1>{})));\n// B: shape is (3,8)\nauto tiler = make_tile(Layout<_3,_3>{},           // Apply     3:3     to mode-0\n                       Layout<Shape <_2,_4>,\n                              Stride<_1,_8>>{});\n\n// ((TileM,RestM), (TileN,RestN)) with shape ((3,3), (8,4))\nauto ld = logical_divide(layout_a, tiler);\n// ((TileM,TileN), (RestM,RestN)) with shape ((3,8), (3,4))\nauto zd = zipped_divide(layout_a, tiler);\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with CMake and specifying architecture\nDESCRIPTION: This snippet demonstrates how to create a build directory, navigate into it, and then use CMake to configure the build process.  It includes the `-DCUTLASS_NVCC_ARCHS` option to specify the target CUDA architecture(s) for which the code should be compiled.  This allows reducing compile time and tailoring the build to specific hardware.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir build && cd build\n\n$ cmake .. -DCUTLASS_NVCC_ARCHS=80               # compiles for NVIDIA's Ampere Architecture\n```\n\n----------------------------------------\n\nTITLE: Enabling synclog in CUTLASS (Shell)\nDESCRIPTION: This shell command demonstrates how to enable the `synclog` tool in CUTLASS during compilation. The `-DCUTLASS_ENABLE_SYNCLOG=1` flag is added to the CMake configuration to enable the feature. `-DCUTLASS_NVCC_ARCHS=90a` specifies the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\n$ mkdir build && cd build && \n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_ENABLE_SYNCLOG=1\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Executable CMake\nDESCRIPTION: This snippet uses the `cutlass_test_unit_add_executable` CMake function to create an executable named `cutlass_test_unit_pipeline`.  This executable is built from the source files listed in the `PIPELINE_SOURCES` variable, thus creating a test unit for the pipeline implementations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/pipeline/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_pipeline\n  ${PIPELINE_SOURCES}\n)\n```\n\n----------------------------------------\n\nTITLE: Profiling CUTLASS Convolution Kernel with Profiler\nDESCRIPTION: This command line uses the CUTLASS profiler to evaluate the performance of a specific convolution kernel.  It specifies the kernel to be profiled and sets various parameters, including input dimensions, filter sizes, and data types, to configure the convolution operation.  The profiler outputs performance metrics such as runtime, memory bandwidth, and GFLOP/s.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Defining Cutlass Example Executable in CUDA\nDESCRIPTION: This code snippet uses the `cutlass_example_add_executable` macro to define an executable named `39_gemm_permute`. The macro takes the executable name and the source file `gemm_permute.cu` as arguments. This likely compiles the CUDA source file into an executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/39_gemm_permute/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  39_gemm_permute\n  gemm_permute.cu \n  )\n```\n\n----------------------------------------\n\nTITLE: clangd Global Configuration Example (config.yaml)\nDESCRIPTION: This is an example of a global clangd configuration file (config.yaml) for CUDA projects on SM90 architecture. It configures the compiler, include paths, CUDA architecture, C++ standard, preprocessor definitions, and diagnostic settings. It also includes instructions for removing problematic CUDA-specific flags.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/ide_setup.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nCompileFlags:\n  Compiler: /usr/local/cuda/bin/nvcc\n  Add:\n    - --cuda-path=/usr/local/cuda\n    - --cuda-gpu-arch=sm_90a\n    - -I/usr/local/cuda/include\n    - \"-xcuda\"\n    # report all errors\n    - \"-ferror-limit=0\"\n    - --cuda-gpu-arch=sm_90a\n    - --std=c++17\n    - \"-D__INTELLISENSE__\"\n    - \"-D__CLANGD__\"\n    - \"-DCUDA_12_0_SM90_FEATURES_SUPPORTED\"\n    - \"-DCUTLASS_ARCH_MMA_SM90_SUPPORTED=1\"\n    - \"-D_LIBCUDACXX_STD_VER=12\"\n    - \"-D__CUDACC_VER_MAJOR__=12\"\n    - \"-D__CUDACC_VER_MINOR__=3\"\n    - \"-D__CUDA_ARCH__=900\"\n    - \"-D__CUDA_ARCH_FEAT_SM90_ALL\"\n    - \"-Wno-invalid-constexpr\"\n  Remove:\n    # strip CUDA fatbin args\n    - \"-Xfatbin*\"\n    # strip CUDA arch flags\n    - \"-gencode*\"\n    - \"--generate-code*\"\n    # strip CUDA flags unknown to clang\n    - \"-ccbin*\"\n    - \"--compiler-options*\"\n    - \"--expt-extended-lambda\"\n    - \"--expt-relaxed-constexpr\"\n    - \"-forward-unknown-to-host-compiler\"\n    - \"-Werror=cross-execution-space-call\"\nHover:\n  ShowAKA: No\nInlayHints:\n  Enabled: No\nDiagnostics:\n  Suppress:\n    - \"variadic_device_fn\"\n    - \"attributes_not_allowed\"\n```\n\n----------------------------------------\n\nTITLE: Defining WriteableReadableRandomAccessContiguousTileIteratorConcept in CUTLASS\nDESCRIPTION: Defines the `WriteableReadableRandomAccessContiguousTileIteratorConcept` in CUTLASS. This concept provides random access, readability, and writability for contiguous tiles, making it suitable for GEMM operands. It includes methods for incrementing, decrementing, and offsetting the iterator, as well as loading and storing fragments with various offset options.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_15\n\nLANGUAGE: c++\nCODE:\n```\n/// This tile iterator embodies several of the above:\n///\n///   - ReadableRandomAccessContiguousTileIteratorConcept\n///   - WriteableRandomAccessContiguousTileIteratorConcept\n/// \n/// It is restated explicitly for convenience of the reader.\n/// \nstruct WriteableReadableRandomAccessContiguousTileIteratorConcept {\n\n  //\n  // Data types\n  //\n  \n  using Element;           ///< Element type composing tile.\n  using Shape;             ///< Shape type describing extent of tile. The shape concept depends \n                           ///  on iterator implementation\n  using Layout;            ///< Layout object mapping \n  using TensorRef;         ///< Tensor Reference object\n  using TensorCoord;       ///< Logical coordinate in referenced tensor\n  using Index;             ///< index type used as base for TensorCoord\n  using Fragment;          ///< fragment object derived from cutlass::Array<Element, N>\n\n  //\n  // Methods\n  //\n\n  /// Adds a linear offset in units of Element to internal pointer(s) into tensor\n  CUTLASS_DEVICE \n  void add_pointer_offset(Index pointer_offset);\n\n  /// true if iterators point to same tile, false if otherwise\n  CUTLASS_DEVICE bool operator==(WriteableReadableRandomAccessContiguousTileIteratorConcept const &it);\n\n  ///< false if iterators point to same tile, true if otherwise\n  CUTLASS_DEVICE bool operator!=(WriteableReadableRandomAccessContiguousTileIteratorConcept const &it);\n\n  /// pre-increment - traverse to next tile in sequence\n  CUTLASS_DEVICE\n  WriteableReadableRandomAccessContiguousTileIteratorConcept & \n  operator++();\n\n  ///< post-increment - traverse to next tile in sequence\n  CUTLASS_DEVICE\n  WriteableReadableRandomAccessContiguousTileIteratorConcept \n  operator++(int);\n\n  /// pre-decrement - traverse to previous tile in sequence\n  CUTLASS_DEVICE\n  WriteableReadableRandomAccessContiguousTileIteratorConcept & \n  operator--();\n\n  ///< post-decrement - traverse to previous tile in sequence\n  CUTLASS_DEVICE\n  WriteableReadableRandomAccessContiguousTileIteratorConcept \n  operator--(int);\n\n  ///< advances in units of whole tiles along the logical coordinate space of the tensor\n  CUTLASS_DEVICE\n  WriteableReadableRandomAccessContiguousTileIteratorConcept & operator+=(TensorCoord const &tile_offset);\n\n  ///< advances in units of whole tiles along the logical coordinate space of the tensor\n  CUTLASS_DEVICE\n  WriteableReadableRandomAccessContiguousTileIteratorConcept & operator-=(TensorCoord const &tile_offset);\n\n  /// Loads a fragment from memory\n  CUTLASS_DEVICE\n  void load(Fragment &frag);                    ///< fragment to be loaded from memory\n\n  /// Loads a fragment from memory with additional logical offset\n  CUTLASS_DEVICE\n  void load_with_pointer_offset(\n    Fragment &frag,                             ///< fragment to be loaded from memory\n    Index pointer_offset);                      ///< linear offset (in units of Element) when loading\n\n  /// Loads a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE\n  void load(\n    Fragment &frag,                             ///< fragment to be loaded from memory\n    TensorCoord const &tile_offset);            ///< loads a tile with a logical offset in units of whole tiles\n\n  /// Loads a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE\n  void load(\n    Fragment &frag,                             ///< fragment to be loaded from memory\n    TensorCoord const &tile_offset,             ///< loads a tile with a logical offset in units of whole tiles\n    Index pointer_offset);                      ///< loads a tile with a logical offset AND a pointer offset\n\n  /// Stores a fragment to memory\n  CUTLASS_DEVICE\n  void store(Fragment const &frag);             ///< fragment to store to memory\n\n  /// Loads a fragment from memory with additional logical offset\n  CUTLASS_DEVICE\n  void store_with_pointer_offset(\n    Fragment const &frag,                       ///< fragment to store to memory\n    Index pointer_offset);                      ///< linear offset (in units of Element) when loading\n\n  /// Stores a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE \n  void store(\n    Fragment const &frag,                       ///< fragment to store to memory\n    TensorCoord const &tile_offset);            ///< stores with logical offset in units of whole tiles\n\n  /// Stores a fragment from memory with logical offset in units of whole tiles.\n  CUTLASS_DEVICE \n  void store(\n    Fragment const &frag,                       ///< fragment to store to memory\n    TensorCoord const &tile_offset,             ///< stores with logical offset in units of whole tiles\n    Index pointer_offset);\n};\n```\n\n----------------------------------------\n\nTITLE: Save Workspace on Incorrect Results\nDESCRIPTION: This command sets the threadblock (CTA) tile size to 256x128x32 and enables saving the workspace if the results are incorrect. This helps in debugging and analyzing potential issues with the GEMM kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --cta_m=256 --cta_n=128  --cta_k=32 --save-workspace=incorrect\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable using CMake\nDESCRIPTION: This snippet uses `cutlass_example_add_executable` to define an executable named `36_gather_scatter_fusion`. The source code for this executable is `gather_scatter_fusion.cu`. This requires CMake and the CUTLASS library to be properly configured.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/36_gather_scatter_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  36_gather_scatter_fusion\n  gather_scatter_fusion.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Printing CuTe Objects on Host or Device\nDESCRIPTION: This code snippet demonstrates how to conditionally print CuTe objects within a CUDA kernel using the `thread0()` function, ensuring that printing occurs only on global thread 0. It highlights the importance of removing print statements from device code after debugging to avoid performance penalties.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/00_quickstart.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nif (thread0()) {\n  print(some_cute_object);\n}\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf8xmxf6)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf8_mxf6_f16_f8_tn_layout.cu`, `mxf8_mxf6_f16_f8_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf8xmxf6\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf8_mxf6_f16_f8_tn_layout.cu\n  mxf8_mxf6_f16_f8_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Linking Target Libraries (CMake)\nDESCRIPTION: Links the `cutlass_nvrtc` library with the `nvidia::nvrtc` and `nvidia::cuda_driver` libraries.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(cutlass_nvrtc PUBLIC nvidia::nvrtc nvidia::cuda_driver)\n```\n\n----------------------------------------\n\nTITLE: Add CUTLASS Executable for Blackwell Architecture (CMake)\nDESCRIPTION: This CMake snippet conditionally adds a CUTLASS example executable if the `CUTLASS_NVCC_ARCHS` variable contains '100a', indicating support for the Blackwell architecture. It uses the `cutlass_example_add_executable` macro to define the executable '71_blackwell_gemm_with_collective_builder' and its corresponding CUDA source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/71_blackwell_gemm_with_collective_builder/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  71_blackwell_gemm_with_collective_builder\n  71_blackwell_gemm_with_collective_builder.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Printing a 1D Layout in C++\nDESCRIPTION: This template function, `print1D`, iterates through a layout and prints the index value for each coordinate i.  It treats all modes of the layout as a single multi-mode. It requires a `Layout` object as input and uses the `size` function to determine the bounds of the layout. The function prints the elements separated by spaces.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <class Shape, class Stride>\nvoid print1D(Layout<Shape,Stride> const& layout)\n{\n  for (int i = 0; i < size(layout); ++i) {\n    printf(\"%3d  \", layout(i));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Define MMA Traits struct for SM70 operation\nDESCRIPTION: Defines the `MMA_Traits` struct specialization for the `SM70_8x8x4_F32F16F16F32_NT` operation. This struct specifies the logical compute types (ValTypeD, ValTypeA, ValTypeB, ValTypeC) for the D, A, B, and C matrices, as well as the logical shape (Shape_MNK) of the MMA operation, thread ID mapping (ThrID), and layouts (ALayout, BLayout, CLayout) for the A, B, and C matrices.  It defines the data types, shapes, and layouts used in the MMA operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <>\nstruct MMA_Traits<SM70_8x8x4_F32F16F16F32_NT>\n{\n  using ValTypeD = float;\n  using ValTypeA = half_t;\n  using ValTypeB = half_t;\n  using ValTypeC = float;\n\n  using Shape_MNK = Shape<_8,_8,_4>;\n  using ThrID   = SM70_QuadPair;\n  using ALayout = SM70_8x4_Col;\n  using BLayout = SM70_8x4_Col;\n  using CLayout = SM70_8x8_32b;\n};\n```\n\n----------------------------------------\n\nTITLE: Coalesce with Target Profile\nDESCRIPTION: This snippet shows the overload of `coalesce` that takes a target profile as a parameter. It allows applying coalesce at the terminals of the target profile, enabling selective coalescing of sub-layouts while maintaining a desired shape.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// Apply coalesce at the terminals of trg_profile\nLayout coalesce(Layout const& layout, IntTuple const& trg_profile)\n```\n\n----------------------------------------\n\nTITLE: Running Cached Kernels in CUTLASS Python\nDESCRIPTION: Demonstrates the use of cached compiled kernels in CUTLASS.  A new GEMM is run with different tensor sizes, and the code shows that the second execution is faster due to the kernel being cached from the previous run. The opclass is switched back to TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nm = 2400\nn = 3232\nk = 4096\n\ntensor_A = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, k)).astype(type_A))\ntensor_B = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(k, n)).astype(type_B))\ntensor_C = np.ceil(np.random.uniform(low=scope_min, high=scope_max, size=(m, n)).astype(type_C))\ntensor_D = np.zeros(tensor_C.shape).astype(type_D)\n\nalpha = np.float16(1.)\nbeta = np.float16(2.)\n\nplan.opclass = cutlass.OpcodeClass.TensorOp\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Defining CLayout for GMMA (T128,V4) -> (M64,N8) with N-mode offset in C++\nDESCRIPTION: This code snippet further extends the CLayout definition by incorporating an offset in the N-mode. It refines the layout for a GMMA operation by adding dimensions to the shape and adjusting strides to account for repetition and offsets in both the M and N modes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n// (T128,V4) -> (M64,N8)\nusing CLayout = Layout<Shape <Shape <  _4, _8, ...>, Shape < _2, _2>>,\n                       Stride<Stride<_128, _1, ...>, Stride<_64, _8>>>;\n```\n\n----------------------------------------\n\nTITLE: Constructor with a long parameter list formatting in C++\nDESCRIPTION: Shows the formatting style for constructors with long parameter lists in CUTLASS. The line is broken after the parentheses, and the colon that starts the constructor's initializer list is aligned flush with the comma on the next line. Parameters are double-indented.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nclass YesTheCommunityAgreesThatTheNameOfThisClassIsIndeedExtremelyLong {\npublic:\n  CUTLASS_HOST_DEVICE\n  YesTheCommunityAgreesThatTheNameOfThisClassIsIndeedExtremelyLong(\n      int this_is_the_first_parameter_and_its_name_is_long,\n      int this_is_the_second_parameter_and_its_name_is_also_long,\n      int this_is_the_third_parameter_and_its_name_is_long_too)\n  : x_(this_is_the_first_parameter_and_its_name_is_long)\n  , y_(this_is_the_second_parameter_and_its_name_is_also_long)\n  , z_(this_is_the_third_parameter_and_its_name_is_long_too) {\n    // constructor body\n    // more of the constructor body\n  }\n\nprivate:\n  int x_ = 0;\n  int y_ = 0;\n  int z_ = 0;\n};\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policy CMP0112\nDESCRIPTION: This CMake command sets the policy CMP0112 to NEW.  This policy likely relates to some aspect of CMake's behavior regarding variable scoping or target definition. Setting it explicitly ensures consistent behavior across different CMake versions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_policy(SET CMP0112 NEW)\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf8xmxf4)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf8_mxf4_f16_bf16_tn_layout.cu`, `mxf8_mxf4_f16_bf16_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf8xmxf4\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf8_mxf4_f16_bf16_tn_layout.cu\n  mxf8_mxf4_f16_bf16_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Target for Conv Fprop Device Tests (CMake)\nDESCRIPTION: This CMake code adds a custom target named `cutlass_test_unit_conv_fprop_device`. This target depends on other targets related to 1D, 2D, and 3D convolution forward propagation tests on a device using tensor operations for the SM90 architecture.  The custom target serves as an aggregator, ensuring these individual tests are built when `cutlass_test_unit_conv_fprop_device` is invoked.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/fprop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_conv_fprop_device\n  DEPENDS\n  cutlass_test_unit_conv1d_fprop_device_tensorop_sm90\n  cutlass_test_unit_conv2d_fprop_device_tensorop_sm90\n  cutlass_test_unit_conv3d_fprop_device_tensorop_sm90\n)\n```\n\n----------------------------------------\n\nTITLE: Adding NVRTC Headers (CMake)\nDESCRIPTION: Calls the `add_nvrtc_headers` macro with different include paths and header file lists. This includes CUTLASS headers, utility headers, device headers, CUTE headers, and headers from the test directory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_nvrtc_headers(${PROJECT_SOURCE_DIR}/include \"${CUTLASS_CUTLASS};${CUTLASS_UTIL};${CUTLASS_DEVICE}\")\nadd_nvrtc_headers(${PROJECT_SOURCE_DIR}/include \"${CUTLASS_CUTE}\")\nadd_nvrtc_headers(${PROJECT_SOURCE_DIR}/test \"${CUTLASS_NVRTC};${CUTLASS_UTIL};${CUTLASS_DEVICE}\")\nadd_nvrtc_headers(${CMAKE_CURRENT_SOURCE_DIR} \"${NVRTC_SOURCES}\")\n\nadd_nvrtc_headers(\"${CMAKE_CURRENT_SOURCE_DIR}/stdlib\" \"assert.h;stdint.h\")\nif(CUTLASS_NVRTC_HAS_CUDA_FP16)\n  add_nvrtc_headers(\"${CMAKE_CURRENT_SOURCE_DIR}/stdlib\" \"cuda_fp16.h;cuda_fp16.hpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Collective Mainloop for MXFP8 GEMM\nDESCRIPTION: This snippet shows how to instantiate the collective mainloop for an MXFP8 GEMM kernel on SM100. It defines the mainloop using the `CollectiveBuilder`, specifying architecture, operation class, data types, layouts, tile shapes, cluster shape, and kernel schedule. Notably, the element size is set to 16 for both A and B.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_42\n\nLANGUAGE: c++\nCODE:\n```\nusing CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<\n    cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp,\n    ElementA, LayoutA, 16,\n    ElementB, LayoutB, 16,\n    ElementAccumulator,\n    MmaTileShape, ClusterShape,\n    cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,\n    cutlass::gemm::KernelScheduleAuto\n  >::CollectiveOp;\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Executable with CMake\nDESCRIPTION: This snippet shows how to add a CUTLASS test unit executable named `cutlass_test_unit_substrate` using the source file `dependent_false.cpp`. The `cutlass_test_unit_add_executable` function configures the build process for the test executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/substrate/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_substrate\n\n  dependent_false.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Include Directories for Hopper GEMM CUTLASS example\nDESCRIPTION: This CMake command specifies the include directories for the '63_hopper_gemm_with_weight_prefetch' target. By setting the current directory (.) as a public include directory, the compiler knows where to find the headers needed to compile the source files.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/63_hopper_gemm_with_weight_prefetch/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(63_hopper_gemm_with_weight_prefetch PUBLIC .)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f8 Sparse GEMM Compressor Test\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_add_executable` function to create an executable named `cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f8`. The executable is built from the source file `sm90_sparse_gemm_compressor_f8.cu`, which likely contains the test implementation for the sparse GEMM compressor with f8 precision.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/device/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f8\n\n    sm90_sparse_gemm_compressor_f8.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Layout Composition with Compile-Time Shapes and Strides in CuTe (C++)\nDESCRIPTION: This C++ code snippet demonstrates layout composition in CuTe using compile-time shapes and strides.  It defines two layouts, `a` and `b`, and computes their composition `c`. The resulting layout `c` is then printed, which represents the reshaping operation. The expected output is `(_5,(_2,_2)):(_16,(_80,_4))`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a = make_layout(make_shape (Int<10>{}, Int<2>{}),\n                       make_stride(Int<16>{}, Int<4>{}));\nLayout b = make_layout(make_shape (Int< 5>{}, Int<4>{}),\n                       make_stride(Int< 1>{}, Int<5>{}));\nLayout c = composition(a, b);\nprint(c);\n```\n\n----------------------------------------\n\nTITLE: Incorrect Alignment of Reference and Pointer Types in C++\nDESCRIPTION: This snippet shows the incorrect alignment of reference and pointer symbols. This is for comparison with the correct alignment above.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nint const &var;\nint const *var;\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Test Executable using CMake\nDESCRIPTION: This CMake function creates an executable named `cutlass_test_unit_epilogue_thread` using the specified CUDA source files.  It relies on the CUTLASS CMake infrastructure to compile and link the CUDA code.  The source files include activation functions, linear combinations and complex number operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/epilogue/thread/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_epilogue_thread\n  activation.cu\n  linear_combination.cu\n  linear_combination_planar_complex.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Volta TensorOp GEMM Executable with CMake\nDESCRIPTION: This snippet utilizes the `cutlass_example_add_executable` CMake function to create an executable named `07_volta_tensorop_gemm` from the CUDA source file `volta_tensorop_gemm.cu`.  It demonstrates a fundamental pattern for integrating CUDA-based examples into the CUTLASS build system. The executable will then be available for compilation and execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/07_volta_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  07_volta_tensorop_gemm\n  volta_tensorop_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds several subdirectories to the project using the `add_subdirectory` command. Each subdirectory represents a different aspect of the CUTLASS library, such as core functionality, architecture-specific code (Volta, Turing, Ampere, Hopper), memory layout, and MSVC compilation support. The `add_subdirectory` command includes the CMakeLists.txt file located in each listed subdirectory into the current build process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(core)\nadd_subdirectory(volta)\nadd_subdirectory(turing)\nadd_subdirectory(ampere)\nadd_subdirectory(hopper)\nadd_subdirectory(layout)\nadd_subdirectory(msvc_compilation)\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf4xmxf6)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf4_mxf6_f32_f16_tn_layout.cu`, `mxf4_mxf6_f32_f16_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf4xmxf6\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf4_mxf6_f32_f16_tn_layout.cu\n  mxf4_mxf6_f32_f16_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Batched GEMM Executable in CMake\nDESCRIPTION: This CMake macro adds an executable named '05_batched_gemm' which compiles 'batched_gemm.cu'. The 'cutlass_example_add_executable' macro is expected to be defined in the CUTLASS project's CMake setup. This will result in a CUDA executable that can perform batched GEMM operations using the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/05_batched_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  05_batched_gemm\n  batched_gemm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Function cutlass_example_add_executable (CMake)\nDESCRIPTION: Defines a function `cutlass_example_add_executable` to streamline the creation of executables for the CUTLASS examples. The function handles setting dependencies, linking libraries (CUTLASS, CUDA, cuBLAS), defining include directories, installing the executables, and setting up the testing framework for each example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(cutlass_example_add_executable NAME)\n\n  set(options)\n  set(oneValueArgs DISABLE_TESTS)\n  set(multiValueArgs DEPENDS DEPENDEES TEST_COMMAND_OPTIONS)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  if (NOT DEFINED __DISABLE_TESTS)\n    set(__DISABLE_TESTS OFF)\n  endif()\n\n  cutlass_add_executable(${NAME} ${__UNPARSED_ARGUMENTS} BATCH_SOURCES OFF)\n\n  add_dependencies(cutlass_examples ${NAME})\n\n  target_link_libraries(\n    ${NAME}\n    PRIVATE\n    CUTLASS\n    cutlass_tools_util_includes\n    $<$<BOOL:${CUTLASS_ENABLE_CUBLAS}>:nvidia::cublas>\n    cuda\n    )\n\n  target_include_directories(\n    ${NAME}\n    PRIVATE\n    ${CUTLASS_EXAMPLES_COMMON_SOURCE_DIR}\n    ${CUTLASS_EXAMPLES_UTILS_DIR}\n    )\n\n  install(\n    TARGETS ${NAME}\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n\n  cutlass_add_executable_tests(\n    test_examples_${NAME} ${NAME}\n    DEPENDS ${__DEPENDS}\n    DEPENDEES test_examples ${__DEPENDEES}\n    TEST_COMMAND_OPTIONS ${__TEST_COMMAND_OPTIONS}\n    DISABLE_EXECUTABLE_INSTALL_RULE\n    DISABLE_TESTS ${__DISABLE_TESTS}\n    )\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting Test Standard Parameters in CMake\nDESCRIPTION: Defines CMake variables `TEST_STANDARD` which sets the GEMM dimensions to 1024x1024x1024. This parameter is later used to specify the gemm size for the test executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_STANDARD --m=1024 --n=1024 --k=1024)\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Auto Epilogue Dispatch Policy\nDESCRIPTION: This auto-dispatch policy selects the most efficient epilogue for post-processing operations following GEMM, optimizing for performance based on kernel parameters and hardware capabilities.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::collective::EpilogueScheduleAuto`\n```\n\n----------------------------------------\n\nTITLE: Conditional Target Addition in CMake\nDESCRIPTION: This snippet conditionally adds a custom target named `cutlass_test_unit_gemm_device_sm100_tensorop_narrow_precision` that depends on other unit test executables. This conditional inclusion is based on whether the `CUTLASS_NVCC_ARCHS` variable matches `100a`, signifying the presence of the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm100_tensorop_narrow_precision\n  DEPENDS\n  cutlass_test_unit_gemm_device_tensorop_sm100_f6f4xf6f4\n  cutlass_test_unit_gemm_device_tensorop_sm100_f6f4xf8\n  cutlass_test_unit_gemm_device_tensorop_sm100_f8xf6f4\n)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building a CUTLASS Executable (CUDA)\nDESCRIPTION: This snippet demonstrates how to create an executable named `32_basic_trmm` from the CUDA source file `basic_trmm.cu` using the `cutlass_example_add_executable` macro.  This macro likely handles the compilation and linking process needed to create the executable. It depends on the CUTLASS build environment being properly configured.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/32_basic_trmm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  32_basic_trmm\n  basic_trmm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf6xmxf8)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf6_mxf8_void_f32_tn_layout.cu`, `mxf6_mxf8_void_f32_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf6xmxf8\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf6_mxf8_void_f32_tn_layout.cu\n  mxf6_mxf8_void_f32_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Epilogue Test Options in CMake\nDESCRIPTION: This snippet defines CMake variables to set up test options including an epilogue operation on GEMM (General Matrix Multiplication) examples with random problem sizes. `--alpha` and `--beta` likely control scaling factors for the epilogue operation, and `--iterations=0` configures a single iteration with random data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Return Statements - Second Form C++\nDESCRIPTION: This code snippet demonstrates another function with an `auto` return type inside `if constexpr` ... `else` conditions. The `else` branch uses `static_assert` to fail at compile time. GCC 10 may spuriously warn about a missing return. `dependent_false<T>` can be used instead of `sizeof(T) < 0` to avoid unconditional failures in C++17.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_23\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<class T>\nconstexpr auto second_form(T t) {\n  if constexpr (some_condition_v<T>) {\n    return some_function(t);\n  }\n  else if constexpr (another_condition_v<T>) {\n    return another_function(t);\n  }\n  else {\n    static_assert(sizeof(T) < 0, \"This branch always fails\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: clangd Local Configuration Example\nDESCRIPTION: This is an example of a local clangd configuration file to specify per-project settings, particularly include paths for the CUTLASS project. It configures include paths, ensuring that clangd can locate the necessary header files for code completion and other language features. Absolute paths are required.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/ide_setup.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nCompileFlags:\n  Add:\n    - -I</absolute/path/to/cutlass>/include/\n    - -I</absolute/path/to/cutlass>/tools/util/include/\n    - -I</absolute/path/to/cutlass>/cutlass/examples/common/\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for StreamK sm100 Sparse GEMM Tests - CMake\nDESCRIPTION: This code uses the `cutlass_test_unit_gemm_device_add_executable_split_file` CMake function to create an executable for StreamK sm100 sparse GEMM tests. It specifies source files and sets `BATCH_SOURCES` to `ON` with `BATCH_SIZE 1` to disable source batching to control compiler memory usage. The `sm100_sp_gemm_f16_f16_f32_f32_f32_streamk.cu` file contains the StreamK kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_streamk\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_sp_gemm_f16_f16_f32_f32_f32_streamk.cu\n)\n```\n\n----------------------------------------\n\nTITLE: BLayout for TN Transpose (T8, V4) -> (n, k)\nDESCRIPTION: This code defines the BLayout for the TN transpose case, mapping 8 threads each owning 4 elements to (n, k) coordinates. It is constructed similarly to ALayout for TN, but represents the layout for the B matrix. It uses `Shape` and `Stride` from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n  // (T8,V4) -> (n,k)\n  using BLayout = Layout<Shape <_8,_4>,\n                         Stride<_1,_8>>;\n```\n\n----------------------------------------\n\nTITLE: Adding SM100 Sparse F6xF6 GEMM Test Executable (CMake)\nDESCRIPTION: This snippet utilizes a CMake function `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for the `f6xf6` sparse GEMM test on the SM100 architecture. It specifies the source files to be compiled and linked into the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_f6xf6\n\n  sm100_sp_gemm_f6_f6_f32_f16_f8_tn.cu\n  sm100_sp_gemm_f6_f6_f32_f16_f16_tn.cu\n  sm100_sp_gemm_f6_f6_f32_f32_f32_tn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Sources for SIMT Convolution Test (sm80+)\nDESCRIPTION: This snippet conditionally adds source files for the SIMT convolution test executable when the maximum CUDA architecture is greater than or equal to 80 (sm80). These additional sources include implementations optimized for sm80, covering various convolution operations with different data types and broadcast mechanisms, including 2D and 3D convolutions and deconvlutions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n\n  cutlass_target_sources(\n    cutlass_test_unit_conv_device_simt\n    PRIVATE\n    conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu\n    conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu\n    conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu\n    conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu\n    conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu\n    conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu\n    deconv2d_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu\n\n    conv2d_fprop_with_broadcast_simt_sm80.cu\n    deconv2d_with_broadcast_simt_sm80.cu\n\n    conv3d_fprop_implicit_gemm_f32ndhwc_f32ndhwc_f32ndhwc_simt_f32_sm80.cu\n    conv3d_dgrad_implicit_gemm_f32ndhwc_f32ndhwc_f32ndhwc_simt_f32_sm80.cu\n    conv3d_wgrad_implicit_gemm_f32ndhwc_f32ndhwc_f32ndhwc_simt_f32_sm80.cu\n    deconv3d_implicit_gemm_f32ndhwc_f32ndhwc_f32ndhwc_simt_f32_sm80.cu\n\n    conv3d_fprop_with_broadcast_simt_sm80.cu\n    deconv3d_with_broadcast_simt_sm80.cu\n\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf6xmxf4)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf6_mxf4_f16_f16_tn_layout.cu`, `mxf6_mxf4_f16_f16_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf6xmxf4\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf6_mxf4_f16_f16_tn_layout.cu\n  mxf6_mxf4_f16_f16_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper Bulk Load Unit Test Executable with CMake\nDESCRIPTION: This CMake snippet configures the build for `cutlass_test_unit_cute_hopper_bulk_load` using the `cutlass_test_unit_add_executable` helper function. This executable targets testing bulk load operations on the Hopper architecture within the CUTLASS library.  The source code is located in `bulk_load.cu`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_bulk_load\n  bulk_load.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Incrementing Iteration Counters in C++\nDESCRIPTION: This C++ code snippet shows how the `operator++()` function is implemented for the `Conv2dFpropActivationTileAccessIteratorAnalytic` class. It increments the contiguous and strided iteration counters and resets them when they reach their maximum values, effectively moving to the next memory access within the tile.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n// cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h\n\n// Update iterator to thread's next contiguous, strided memory access\nConv2dFpropActivationTileAccessIteratorAnalytic &operator++() {\n  ++iteration_contiguous_;\n  if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous) {\n    return *this;\n  }\n  iteration_contiguous_ = 0;\n  \n  ++iteration_strided_;\n  if (iteration_strided_ < ThreadMap::Iterations::kStrided) {\n    return *this;\n  }\n  iteration_strided_ = 0;\n \n  return *this;\n}\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Volta Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Volta GPU architecture using CMake. The `CUTLASS_NVCC_ARCHS` flag specifies the target architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=70               # compiles for NVIDIA Volta GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Setting Test Command Options for various Operations in CMake\nDESCRIPTION: Sets the command line options for running tests for different operations (Conv2d, Conv3d, SparseGemm, RankK, Rank2K, Trmm, Symm, GroupedGemm) using the CUTLASS profiler. These options specify the operation, providers, verification providers, JUnit output file, and whether to print the kernel before running.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_CONV2D --operation=Conv2d     --providers=cutlass --verification-providers=cudnn,device       --junit-output=test_cutlass_profiler_conv2d  --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_CONV3D --operation=Conv3d     --providers=cutlass --verification-providers=cudnn,device,host  --junit-output=test_cutlass_profiler_conv3d  --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_SPGEMM --operation=SparseGemm --providers=cutlass --verification-providers=cublas,device,host --junit-output=test_cutlass_profiler_spgemm  --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_RANK_K   --operation=RankK       --providers=cutlass --verification-providers=cublas        --junit-output=test_cutlass_profiler_rank_k    --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_RANK_2K   --operation=Rank2K       --providers=cutlass --verification-providers=cublas        --junit-output=test_cutlass_profiler_rank_2k --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_TRMM   --operation=Trmm       --providers=cutlass --verification-providers=device,host        --junit-output=test_cutlass_profiler_trmm    --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_SYMM   --operation=Symm       --providers=cutlass --verification-providers=cublas,host        --junit-output=test_cutlass_profiler_symm    --print-kernel-before-running=true)\nset(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_GROUPED_GEMM --operation=GroupedGemm --providers=cutlass --verification-providers=device --junit-output=test_cutlass_profiler_grouped_gemm --print-kernel-before-running=true)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Tests for CUTLASS Profiler in CMake\nDESCRIPTION: Adds executable tests for the CUTLASS profiler, specifying dependencies and test command options for various operations. It disables the executable install rule.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_add_executable_tests(\n  test_profiler cutlass_profiler\n  DEPENDEES test_all\n  TEST_COMMAND_OPTIONS\n    GEMM\n    CONV2D\n    CONV3D\n    SPGEMM\n    RANK_K\n    RANK_2K\n    TRMM\n    SYMM\n    GROUPED_GEMM\n  TEST_COMMAND_OPTIONS_PREFIX\n    CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_\n  DISABLE_EXECUTABLE_INSTALL_RULE\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining sgemm_sm80 Executable (CMake/CUDA)\nDESCRIPTION: This command uses a custom CMake macro `cutlass_example_add_executable` to define an executable named 'cute_tutorial_sgemm_sm80'. The source file for this executable is 'sgemm_sm80.cu', indicating a CUDA implementation targeting SM80 architecture. It creates an executable target within the Cutlass build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_sgemm_sm80\n  sgemm_sm80.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Building Subset Tensor Core Convolution Kernels\nDESCRIPTION: These commands demonstrate how to build a subset of Tensor Core convolution kernels for forward propagation (fprop). The `cmake` command configures the build, specifying target architectures and a wildcard-based kernel name pattern that matches fprop kernels. The subsequent `make` command builds the cutlass profiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*fprop_optimized_f16\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS and PyTorch (Colab)\nDESCRIPTION: This command installs the CUTLASS Python interface and PyTorch. It is intended to be used within a Google Colab environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!#pip install nvidia-cutlass torch --extra-index-url https://download.pytorch.org/whl/cu121\n```\n\n----------------------------------------\n\nTITLE: Including GNUInstallDirs Module\nDESCRIPTION: This snippet includes the GNUInstallDirs CMake module. This module defines standard installation directories such as `${CMAKE_INSTALL_PREFIX}/bin` and `${CMAKE_INSTALL_PREFIX}/lib`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(GNUInstallDirs)\n```\n\n----------------------------------------\n\nTITLE: Schmoo Over Problem Size and Beta\nDESCRIPTION: This command performs a parameter sweep (schmoo) over the problem size (M, N, K) and the beta epilogue scalar. It tests different combinations of these parameters to analyze their impact on performance. The ranges are defined using the start:end:step format.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cutlass_profiler --operation=Gemm --m=1024:4096:256 --n=1024:4096:256 --k=128:8192:128 --beta=0,1,2.5\n```\n\n----------------------------------------\n\nTITLE: Compiling CUTLASS Hopper GEMM Example (CMake)\nDESCRIPTION: This CMake snippet compiles the `50_hopper_gemm_with_epilogue_swizzle.cu` CUDA file into an executable named `50_hopper_gemm_with_epilogue_swizzle`. It uses the `cutlass_example_add_executable` function, which is assumed to be defined elsewhere in the CMake project, and handles the necessary dependencies and compiler flags for building a CUTLASS example targeting the Hopper architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/50_hopper_gemm_with_epilogue_swizzle/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  50_hopper_gemm_with_epilogue_swizzle\n  50_hopper_gemm_with_epilogue_swizzle.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Dense BlockScaled Kernel Dispatch (2SM)\nDESCRIPTION: This policy uses 2 SM `tcgen05.mma` instruction and automatically selects the optimal instruction kind (mxf8f6f4, mxf4, nvf4mxf4).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\n`KernelTmaWarpSpecialized2SmBlockScaledSm100`\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for NVRTC Test\nDESCRIPTION: This CMake command adds the current binary directory as a private include directory for the 'cutlass_test_unit_nvrtc_thread' executable. This ensures that header files generated during the build process are available during compilation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/thread/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(cutlass_test_unit_nvrtc_thread PRIVATE ${CMAKE_CURRENT_BINARY_DIR})\n```\n\n----------------------------------------\n\nTITLE: GEMM Grouped F32 Device Schedule Example\nDESCRIPTION: This example showcases a GEMM Grouped operation on device with float32 inputs and Simt (SIMT-based) operation. Problem sizes are loaded from a CSV file, and a device schedule is utilized.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npython gemm_grouped.py -i 1 1 1 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op Simt -b 128 64 8 -s 4 -w 2 2 1 -cc 80 -la RowMajor -aa 1 -lb RowMajor -ab 1 -lc RowMajor -ac 1 -te float32 -ep LinearCombination -p ./grouped_gemm_problem_size.csv -alpha 2.0 -beta 1.0 -pm Device\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Executable in CMake\nDESCRIPTION: This CMake code snippet adds an executable named 'cutlass_test_unit_cute_turing' which is built from the source file 'cooperative_gemm.cu'. It leverages the 'cutlass_test_unit_add_executable' function, which is specific to the CUTLASS build system. This function handles linking CUTLASS libraries and setting up the build environment correctly for a test executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/turing/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_turing\n  cooperative_gemm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: If-Else Brackets and Spacing in C++\nDESCRIPTION: This snippet demonstrates the preferred style for if-else statements in CUTLASS, including the use of braces even for single-line bodies, spacing after control flow keywords, and a new line between the closing brace of an if branch and the else keyword.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nif (condition) { // space after if, and between ) and {\n  // ... code ...\n} // newline after }\nelse {\n  // ... other code ...\n}\n\n// space after keyword for\nfor (int k = 0; k < num_iters; ++k) {\n  // ... still more code ...\n}\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_4x4x5x128_3x3_256x3x6_pad_h0w0_stride_h1w1_dil_h1w1_corr_alpha1_beta0 s4nhwc_s4nhwc_inhwc_i_f 1998572675 3327908208 4290039622 1215151709\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Example Executable (CMake)\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` macro to create an executable named '03_visualize_layout'.  It includes the source files 'visualize_layout.cpp' and 'register_layout.cu'.  It also passes test command options and the previously defined 'TEST_COMMAND_00' to the executable's build process for testing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/03_visualize_layout/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  03_visualize_layout\n  visualize_layout.cpp\n  register_layout.cu\n  TEST_COMMAND_OPTIONS\n  TEST_COMMAND_00\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding SM100 Sparse F4F6xF4F6 GEMM Test Executable (CMake)\nDESCRIPTION: This snippet utilizes a CMake function `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for the `f4f6xf4f6` sparse GEMM test on the SM100 architecture. It specifies the source files to be compiled and linked into the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_f4f6xf4f6\n\n  sm100_sp_gemm_f4_f6_f32_f16_f16_tn.cu\n  sm100_sp_gemm_f6_f4_f32_f16_f16_tn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper TMA Store Unit Test Executable with CMake\nDESCRIPTION: This CMake code defines the build process for the `cutlass_test_unit_cute_hopper_tma_store` executable using the `cutlass_test_unit_add_executable` function. The executable tests TMA store operations on the Hopper architecture. It compiles the `tma_store.cu` source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_tma_store\n  tma_store.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Verify ReLU Activation Result - Python\nDESCRIPTION: This snippet verifies that the result of the GEMM operation with ReLU activation is correct.  It computes a reference ReLU result (`relu_ref`) by applying the ReLU function directly to the output tensor `tensor_D` and then asserts that `relu_ref` is equal to the result obtained from CUTLASS (`tensor_D_relu`).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/01_epilogue.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrelu_ref = (tensor_D >= 0).astype(type_D) * tensor_D\nnp.testing.assert_array_equal(relu_ref, tensor_D_relu)\n```\n\n----------------------------------------\n\nTITLE: Defining Base Tile Iterator Concept in C++\nDESCRIPTION: Defines the base concept for all tile iterators in CUTLASS. It specifies the `Element` type, representing the data type of elements in the tile, and the `Shape` type, describing the extent of the tile. This serves as a foundation for more specialized tile iterator concepts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n/// Base concept for all tile iterators\nstruct TileIteratorConcept {\n  using Element;           ///< Element type composing tile (concept: numeric type or Array<>)\n  using Shape;             ///< Shape type describing extent of tile. The shape concept depends \n                           ///  on iterator implementation.\n};\n```\n\n----------------------------------------\n\nTITLE: GemmUniversal Specialization Example\nDESCRIPTION: Shows an example of how `GemmUniversal` can be specialized based on template parameters, specifically with `KernelMultistage`. This allows composition with various mainloops based on their `DispatchPolicy::Schedule` type.  It uses `std::enable_if_t` and `std::is_base_of_v` to conditionally enable the specialization.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\n// An example of the SM90 KernelMultistage kernel's\n// specialization logic that allows it to be composed\n// with many mainloops such as `MainloopSm80CpAsync`\n// and `MainloopSm70TwoStage`.\ntemplate <\n  class ProblemShape_,\n  class CollectiveMainloop_,\n  class CollectiveEpilogue_,\n  class TileScheduler_\n>\nclass GemmUniversal<\n  ProblemShape_,\n  CollectiveMainloop_,\n  CollectiveEpilogue_,\n  TileScheduler_,\n  std::enable_if_t<std::is_base_of_v<KernelMultistage, typename CollectiveMainloop_::DispatchPolicy::Schedule>>>\n```\n\n----------------------------------------\n\nTITLE: Calling cutlass_add_cutlass_library with Source Files\nDESCRIPTION: This snippet calls the `cutlass_add_cutlass_library` function with a list of source files to be included in the CUTLASS library. The listed files cover a range of functionalities including handle management, operation tables, reference GEMM implementations, reduction operations, and convolution implementations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_add_cutlass_library(\n\n  src/handle.cu\n  src/manifest.cpp\n  src/operation_table.cu\n  src/singleton.cu\n  src/util.cu\n\n  # files split for parallel compilation\n  src/reference/gemm_int4.cu\n  \n  src/reference/block_scaled_gemm_fp4a_vs16.cu   \n  src/reference/block_scaled_gemm_fp4a_vs32.cu   \n  src/reference/block_scaled_gemm_mixed8bitsa.cu\n  src/reference/gemm_f4_f4_f32.cu\n  src/reference/gemm_f4_f6_f32.cu\n  src/reference/gemm_f4_f8_f32.cu\n  src/reference/gemm_f6_f4_f32.cu\n  src/reference/gemm_f6_f6_f32.cu\n  src/reference/gemm_f6_f8_f32.cu\n  src/reference/gemm_f8_f4_f32.cu\n  src/reference/gemm_f8_f6_f32.cu\n\n  src/reference/blockwise_gemm_fp8_fp16out.cu   \n  src/reference/blockwise_gemm_fp8_fp32out.cu   \n  src/reference/blockwise_gemm_fp8_bf16out.cu   \n  \n  src/reference/gemm_s8_s8_s32.cu\n  src/reference/gemm_u8_u8_s32.cu\n  src/reference/gemm_int8_interleaved_32.cu\n  src/reference/gemm_int8_interleaved_64.cu\n  src/reference/gemm_e4m3a_e4m3out.cu\n  src/reference/gemm_e5m2a_e4m3out.cu\n  src/reference/gemm_e4m3a_e5m2out.cu\n  src/reference/gemm_e5m2a_e5m2out.cu\n  src/reference/gemm_fp8in_fp16out.cu\n  src/reference/gemm_fp8in_bf16out.cu\n  src/reference/gemm_fp8in_fp32out.cu\n  src/reference/gemm_fp32out.cu\n  src/reference/gemm_fp_other.cu\n  src/reference/gemm_fp_mixed_input.cu\n  src/reference/gemm_int_mixed_input.cu\n  src/reference/initialize_reference_operations.cu\n\n  # cutlass reduction instances in cutlass library\n\n  src/reduction/reduction_device.cu\n  src/reduction/init_reduction_operations.cu\n  \n  # cutlass conv reference instances in cutlass library\n\n  src/reference/conv2d.cu\n  src/reference/conv3d.cu\n\n  )\n```\n\n----------------------------------------\n\nTITLE: SM120 Sparse GEMM Unit Test Target Definition (CMake)\nDESCRIPTION: This CMake code defines a custom target `cutlass_test_unit_gemm_device_sm120_sptensorop` which depends on other executable targets.  It orchestrates the execution of several unit tests related to sparse GEMM on the SM120 architecture. These dependencies are other custom targets generated by `cutlass_test_unit_gemm_device_add_executable` function. The overall effect is to group these tests together under a single target for easier execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_sparse_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 120a)\n\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm120_sptensorop\n  DEPENDS\n  cutlass_test_unit_sparse_gemm_device_tensorop_sm120\n  cutlass_test_unit_sparse_gemm_device_tensorop_sm120_stream_k\n  cutlass_test_unit_sparse_gemm_device_tensorop_sm120_epilogue_fusion\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable - wgmma_sm90 CUDA\nDESCRIPTION: This CMake function adds an executable target named `cute_tutorial_wgmma_sm90` that compiles the `wgmma_sm90.cu` CUDA source file. It utilizes the `cutlass_example_add_executable` CMake macro, assumed to be defined elsewhere in the CUTLASS build system, to handle CUDA compilation and linking.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/hopper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_wgmma_sm90\n  wgmma_sm90.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Mapping 2.x Layout Tags to 3.0 Stride Types in C++\nDESCRIPTION: The following code snippet demonstrates how CUTLASS 3.0 maps CUTLASS 2.x layout tags to CuTe stride types using metafunctions within the `GemmUniversalAdapter`.  This involves creating aliases for LayoutA, LayoutB, LayoutC and LayoutD to corresponding 2.x layout tags. This is done to provide a convenient way for users to transition from CUTLASS 2.x to 3.0.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n  // Map back to 2.x type as best as possible\n  using LayoutA = gemm::detail::StrideToLayoutTagA_t<typename GemmKernel::StrideA>;\n  using LayoutB = gemm::detail::StrideToLayoutTagB_t<typename GemmKernel::StrideB>;\n  using LayoutC = gemm::detail::StrideToLayoutTagC_t<typename GemmKernel::StrideC>;\n  using LayoutD = gemm::detail::StrideToLayoutTagC_t<typename GemmKernel::StrideD>;\n\n  // Legacy: Assume MultiplyAdd only since we do not use this tag type in 3.0\n  using MathOperator = cutlass::arch::OpMultiplyAdd;\n\n  // If our TiledMMA's instruction thread layout size is larger than 1,\n  // we know it's a tensorop\n  using OperatorClass = std::conditional_t<\n      (cute::size(typename GemmKernel::TiledMma::AtomThrID{}) > 1),\n      cutlass::arch::OpClassTensorOp, cutlass::arch::OpClassSimt>;\n\n  // Assume TiledMma's ShapeMNK is the same as 2.x's ThreadblockShape\n  using ThreadblockShape = cutlass::gemm::GemmShape<\n      cute::size<0>(TileShape{}),\n      cute::size<1>(TileShape{}),\n      cute::size<2>(TileShape{})>;\n\n  using ClusterShape = cutlass::gemm::GemmShape<\n      cute::size<0>(typename GemmKernel::DispatchPolicy::ClusterShape{}),\n      cute::size<1>(typename GemmKernel::DispatchPolicy::ClusterShape{}),\n      cute::size<2>(typename GemmKernel::DispatchPolicy::ClusterShape{})>;\n\n  // We get the instruction shape directly from our TiledMma's atom shape\n  using InstructionShape = cutlass::gemm::GemmShape<\n      cute::size<0>(typename CollectiveMainloop::TiledMma::AtomShape_MNK{}),\n      cute::size<1>(typename CollectiveMainloop::TiledMma::AtomShape_MNK{}),\n      cute::size<2>(typename CollectiveMainloop::TiledMma::AtomShape_MNK{})>;\n\n  static int constexpr kStages = CollectiveMainloop::DispatchPolicy::Stages;\n  static int const kThreadCount = GemmKernel::MaxThreadsPerBlock;\n\n  // Warp shape is not a primary API type in 3.x,\n  // but we can best approximate it by inspecting the TiledMma\n  // For this, we make the assumption that we always have 4 warps along M,\n  // and the rest along N, with none along K.  We also always round up\n  // the warp count to 4 if the tiled mma is smaller than 128 threads.\n  static constexpr int WarpsInMma = std::max(4, CUTE_STATIC_V(cute::size(typename GemmKernel::TiledMma{})) / 32);\n  static constexpr int WarpsInMmaM = 4;\n  static constexpr int WarpsInMmaN = cute::ceil_div(WarpsInMma, WarpsInMmaM);\n  using WarpCount = cutlass::gemm::GemmShape<WarpsInMmaM, WarpsInMmaN, 1>;\n  using WarpShape = cutlass::gemm::GemmShape<\n      CUTE_STATIC_V(cute::tile_size<0>(typename CollectiveMainloop::TiledMma{})) / WarpsInMmaM,\n      CUTE_STATIC_V(cute::tile_size<1>(typename CollectiveMainloop::TiledMma{})) / WarpsInMmaN,\n      CUTE_STATIC_V(cute::tile_size<2>(typename CollectiveMainloop::TiledMma{}))>;\n\n  // Inspect TiledCopy for A and B to compute the alignment size\n  static int constexpr kAlignmentA = gemm::detail::get_alignment_count_from_gmem_tiled_copy<\n      typename CollectiveMainloop::GmemTiledCopyA, ElementA>();\n  static int constexpr kAlignmentB = gemm::detail::get_alignment_count_from_gmem_tiled_copy<\n      typename CollectiveMainloop::GmemTiledCopyB, ElementB>();\n```\n\n----------------------------------------\n\nTITLE: Array Copy and Clear Example in CUTLASS\nDESCRIPTION: This example demonstrates how to copy an `Array` object and how to set all its elements to zero using the `clear()` method.  Copying the entire object is more efficient than accessing elements individually. Requires the `cutlass/array.h` header.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include <cutlass/array.h>\n\nint const kN;\nArray<T, kN> source;\nArray<T, kN> destination;\n\nsource.clear();         // set all elements to value of zero\n\ndestination = source;   // copy to `destination`\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Executable in CMake\nDESCRIPTION: This snippet uses the CMake function `cutlass_test_unit_add_executable` to create an executable named `cutlass_test_unit_cluster_launch` from the source file `cluster_launch.cu`. This is part of the CUTLASS library's testing infrastructure and relies on the CMake functions provided by CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cluster_launch/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cluster_launch\n  cluster_launch.cu\n)\n```\n\n----------------------------------------\n\nTITLE: TensorView Construction from TensorRef Example\nDESCRIPTION: This C++ code demonstrates the succinct construction of a `TensorView` object from a `TensorRef` object and an `extent`. It initializes a `layout::ColumnMajor` and a `TensorRef`. Then, it uses the `TensorRef` object and an `extent` to construct a `TensorView` named `view`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nlayout::ColumnMajor layout(ldm);\nTensorRef<int4_t, layout::ColumnMajor> ref(ptr, layout);\n\nTensorView<int4_t, layout::ColumnMajor> view(ref, extent);    // construct TensorView from TensorRef and extent\n```\n\n----------------------------------------\n\nTITLE: CUTLASS 3.x API Detection Meta-function (C++)\nDESCRIPTION: This code snippet defines a meta-function `IsCutlass3GemmKernel` within the `cutlass::gemm::detail` namespace. This meta-function detects whether a `kernel::Gemm` or `kernel::GemmUniversal` implements the CUTLASS 3.x API. It achieves this by checking for the presence of a problem shape type alias within the kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n// include/cutlass/gemm/gemm.h\n\nnamespace cutlass:gemm::detail {\n\n// The following metafunction is used to detect whether a\n// `kernel::Gemm` or `kernel::GemmUniversal` implements the CUTLASS 3.x API,\n// by checking whether the problem shape type is aliased within.\ntemplate <class GemmKernel, class = void>\nstruct IsCutlass3GemmKernel;\n\n} // namespace cutlass:gemm::detail\n```\n\n----------------------------------------\n\nTITLE: TensorRef Usage Example\nDESCRIPTION: This C++ code demonstrates the usage of `TensorRef` to access elements within a tensor.  A `layout::ColumnMajor` object is created with a leading dimension `ldm`. A `TensorRef` object, `ref`, is constructed using the pointer `ptr` and the `layout` object. The `at()` method is used to access and modify an element at the specified `(row, column)` coordinate.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/layout.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nint4_t *ptr = ...;\nint ldm = ...;\n\nint row = ...;\nint column = ...;\n\nlayout::ColumnMajor layout(ldm);\nTensorRef<int4_t, layout::ColumnMajor> ref(ptr, layout);\n\nint4_t x = ref.at({row, column});     // loads a 4-bit signed integer from the tensor\n\nref.at({row, column}) = x * 2_s4;     // transforms this quantity and stores it back\n```\n\n----------------------------------------\n\nTITLE: Add Executable for f8xf8 TensorOp GEMM Test (sm100)\nDESCRIPTION: This CMake snippet utilizes the `cutlass_test_unit_gemm_device_add_executable` function to create an executable for testing f8xf8 GEMM using Tensor Cores on sm100. Batching is enabled, and the specific source files for the f8xf8 tests, including a fusion kernel, are defined.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_sm100_f8xf8\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  f8_f8_void_f32.cu\n  f8_f8_f16_f8_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Logical Divide Implementation in CuTe\nDESCRIPTION: This code snippet shows the implementation of the `logical_divide` function. It divides a given layout by a tiler layout using composition and complement. This function requires the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <class LShape, class LStride,\n          class TShape, class TStride>\nauto logical_divide(Layout<LShape,LStride> const& layout,\n                    Layout<TShape,TStride> const& tiler)\n{\n  return composition(layout, make_layout(tiler, complement(tiler, size(layout))));\n}\n```\n\n----------------------------------------\n\nTITLE: Conv2d F32 Fprop SIMT Example\nDESCRIPTION: This example demonstrates a Conv2d Fprop with SIMT-based operation. It shows the configuration for tensor layouts, swizzle functions, padding, stride, and dilation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 1 1 1 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op Simt -b 128 128 8 -s 4 -w 4 2 1 -cc 80 -la TensorNHWC -aa 4 -lb TensorNHWC -ab 4 -lc TensorNHWC -ac 1 -te float32 -ep LinearCombination -sw IdentitySwizzle4 -co fprop -st Strided -ia analytic -sm Parallel -k 3 -nhwc 1 71 80 32 -krsc 64 5 5 32 -pad 2 2 2 2 -stride 2 2 -dilation 1 1 -alpha 1.0 -beta 1.0\n```\n\n----------------------------------------\n\nTITLE: Defining Forward Tile Iterator Concept in C++\nDESCRIPTION: Defines a tile iterator concept that supports forward traversal by one tile in a predefined sequence. It includes equality operators (`==` and `!=`) to compare iterators and increment operators (`++`) to advance to the next tile.  The sequence is often relevant to the context in which the iterator is used.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\n/// Tile iterator that may be incremented along a traversal sequence.\nstruct ForwardTileIteratorConcept {\n\n  CUTLASS_DEVICE bool operator==(TileIterator const &it);        ///< true if iterators point to same tile, false if otherwise\n  CUTLASS_DEVICE bool operator!=(TileIterator const &it);        ///< false if iterators point to same tile, true if otherwise\n\n  CUTLASS_DEVICE ForwardTileIteratorConcept & operator++();      ///< pre-increment - advance to next tile in sequence\n  CUTLASS_DEVICE ForwardTileIteratorConcept operator++(int);     ///< post-increment - advance to next tile in sequence\n};\n```\n\n----------------------------------------\n\nTITLE: Selecting Modes of the Tiler and Coord (CuTe)\nDESCRIPTION: This code shows an alternative way to use only the M- and K-modes of the tiler and coord when partitioning tensors across CTAs.  It uses `select<0,2>` to select the desired modes and then applies `local_tile`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\n  // Use select<0,2> to use only the M- and K-modes of the tiler and coord\n  Tensor gA = local_tile(mA, select<0,2>(cta_tiler), select<0,2>(cta_coord));\n```\n\n----------------------------------------\n\nTITLE: SM90_TMA_STORE_3D Interface (C++)\nDESCRIPTION: This code snippet shows the interface for TMA-store with 3-D coordinates. It demonstrates how the copy function takes a pointer to the TMA descriptor, a pointer to SMEM, and coordinates into the GMEM tensor.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0z_tma_tensors.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstruct SM90_TMA_STORE_3D {\n  CUTE_DEVICE static void\n  copy(void const* const desc_ptr,\n       void const* const smem_ptr,\n       int32_t const& crd0, int32_t const& crd1, int32_t const& crd2) {\n    // ... invoke CUDA PTX instruction ...\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Building Subset Tensor Core Convolution Kernels (CMake, Make)\nDESCRIPTION: These commands compile a filtered subset of Tensor Core convolution kernels using CMake and Make. The CMake configuration specifies target architectures and employs a wildcard pattern to select specific kernels. `make` is then used to compile.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*fprop_optimized_f16\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition for Unit Tests (CMake)\nDESCRIPTION: This snippet conditionally adds the 'unit' subdirectory, which contains the unit tests, to the build process if the CUTLASS_ENABLE_GTEST_UNIT_TESTS CMake option is enabled. If the option is disabled, it creates a phony 'test_unit' target.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_ENABLE_GTEST_UNIT_TESTS)\n  add_subdirectory(unit)\nelse()\n  # Always provide at least the phony test_unit target.\n  add_custom_target(test_unit)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conv2d with HardSwish Activation\nDESCRIPTION: This example demonstrates a Conv2d operation with HardSwish activation function using float16 data type and TensorOp.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 16 -ta float16 -tb float16 -tc float16 -tacc float32 -m multiply_add -op TensorOp -b 128 128 64 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 2 -lb TensorNHWC -ab 2 -lc TensorNHWC -ac 8 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -co fprop -st Strided -ia few_channels -sm Serial -k 1 -nhwc 1 16 16 2 -krsc 16 3 3 2 -pad 1 1 1 1 -stride 2 2 -dilation 1 1 -alpha 0.0 -beta 0.5 -bias -activ hardswish\n```\n\n----------------------------------------\n\nTITLE: Compiling and running CUTLASS unit tests - Bash\nDESCRIPTION: This snippet demonstrates how to compile and run the CUTLASS unit tests from the `build/` directory using the `make` command. The `-j` option allows for parallel execution of the tests. Successful execution results in a summary of tests run and tests passed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ make test_unit -j\n...\n...\n...\n[----------] Global test environment tear-down\n[==========] 946 tests from 57 test cases ran. (10812 ms total)\n[  PASSED  ] 946 tests.\n```\n\n----------------------------------------\n\nTITLE: Linking OpenMP Library (CMake)\nDESCRIPTION: This CMake snippet conditionally links the OpenMP library to the '59_ampere_gather_scatter_conv' target. It checks if 'CUTLASS_ENABLE_OPENMP_TESTS' is enabled and if the OpenMP library is found ('OpenMP_CXX_FOUND'). If both conditions are true, it links the OpenMP library using 'target_link_libraries'.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_ENABLE_OPENMP_TESTS AND OpenMP_CXX_FOUND)\n  target_link_libraries(59_ampere_gather_scatter_conv PRIVATE OpenMP::OpenMP_CXX)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Test Command Options for GEMM (CMake)\nDESCRIPTION: This CMake command sets a variable 'TEST_COMMAND_00' containing command-line arguments for testing the GEMM kernel. Specifically, it defines non-square matrix dimensions (M=512, N=768, K=1152) to ensure internal transpose mechanisms are correctly handled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/47_ampere_gemm_universal_streamk/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_COMMAND_00 --m=512 --n=768 --k=1152)\n```\n\n----------------------------------------\n\nTITLE: Linking CUTLASS Test Unit Library (Infra Lib) Dependencies\nDESCRIPTION: This snippet links the 'cutlass_test_unit_infra_lib' library with its dependencies. It includes 'cutlass_test_unit_infra' as a public dependency, indicating that 'cutlass_test_unit_infra_lib' depends on 'cutlass_test_unit_infra'.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(\n  cutlass_test_unit_infra_lib\n  PUBLIC\n  cutlass_test_unit_infra\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Target in CMake\nDESCRIPTION: This snippet defines a custom target named 'cutlass_test_unit_epilogue'. This target depends on other custom targets related to thread, warp, and threadblock epilogue testing.  Executing this target will trigger the build and execution of all its dependencies.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/epilogue/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_epilogue\n  DEPENDS\n  cutlass_test_unit_epilogue_thread\n  cutlass_test_unit_epilogue_warp\n  cutlass_test_unit_epilogue_threadblock\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding GEMM Thread Executable in CUTLASS with CMake\nDESCRIPTION: This snippet defines an executable named `cutlass_test_unit_gemm_thread` composed of several CUDA source files (`gemm_sm50.cu`, `gemm_sm60.cu`, `gemm_sm61.cu`) and a header file (`testbed.h`).  It links these files together to create a test unit executable specifically for GEMM operations on different SM architectures.  Dependencies: CUDA compiler and CUTLASS library setup.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/thread/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_gemm_thread\n  gemm_sm50.cu\n  gemm_sm60.cu\n  gemm_sm61.cu\n  testbed.h\n  )\n```\n\n----------------------------------------\n\nTITLE: AlignedArray Usage Example in CUTLASS\nDESCRIPTION: This example demonstrates how to use `AlignedArray` in CUTLASS. It shows a 128b aligned memory access by copying data into an `AlignedArray`. Requires the `cutlass` library and assumes `half_t` is a defined type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nint const kN = 8;\nArrayAligned<half_t, kN> source;\nArrayAligned<half_t, kN> const *ptr = ...;\n\nsource = *ptr;          // 128b aligned memory access\n```\n\n----------------------------------------\n\nTITLE: Setting CUDACXX environment variable\nDESCRIPTION: This command sets the `CUDACXX` environment variable to point to the NVCC compiler within the installed CUDA Toolkit.  This is necessary for CMake to find the CUDA compiler during the build process.  The `CUDA_INSTALL_PATH` variable should be set appropriately for your CUDA installation location.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc\n```\n\n----------------------------------------\n\nTITLE: Adding Blackwell GEMM Executable in CMake\nDESCRIPTION: This CMake code snippet conditionally adds a CUTLASS example executable, `74_blackwell_gemm_streamk`, if the `CUTLASS_NVCC_ARCHS` variable matches `100a`, indicating the Blackwell architecture. The `cutlass_example_add_executable` function is used to simplify the process of adding the executable, which depends on the CUDA source file `blackwell_gemm_streamk.cu`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/74_blackwell_gemm_streamk/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  74_blackwell_gemm_streamk\n  blackwell_gemm_streamk.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating SM120 Sparse GEMM Executable (CMake)\nDESCRIPTION: This snippet uses a CMake function, `cutlass_test_unit_gemm_device_add_executable`, to create an executable for testing sparse GEMM on SM120 architecture. It specifies the source files (`.cu` files) required for the executable. The function encapsulates the logic to compile and link these CUDA sources into an executable named `cutlass_test_unit_sparse_gemm_device_tensorop_sm120`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_sparse_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_sparse_gemm_device_tensorop_sm120\n\n  sm120_sparse_gemm_f4_f4_f32_tensor_op.cu\n  sm120_sparse_gemm_f6_f4_f32_tensor_op.cu\n  sm120_sparse_gemm_f8_f6_f32_tensor_op.cu\n  sm120_sparse_gemm_f4_f4_f16_tensor_op.cu\n  sm120_sparse_gemm_f6_f4_f16_tensor_op.cu\n  sm120_sparse_gemm_f8_f6_f16_tensor_op.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Including CUTLASS Library Manifest File\nDESCRIPTION: This snippet includes a CMake file containing the manifest for the auto-instantiated kernels in the CUTLASS library. It first defines the path to the manifest file `CUTLASS_LIBRARY_MANIFEST_CMAKE_FILE`. Then, it checks if the file exists. If the file exists, it is included using the `include` command. If the file does not exist, a status message is printed to the console.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\n# include auto-instantiated kernels in he CUTLASS Deliverables Library\nset(CUTLASS_LIBRARY_MANIFEST_CMAKE_FILE ${CMAKE_CURRENT_BINARY_DIR}/generated/manifest.cmake)\nif(EXISTS \"${CUTLASS_LIBRARY_MANIFEST_CMAKE_FILE}\")\n  include(${CUTLASS_LIBRARY_MANIFEST_CMAKE_FILE})\nelse()\n  message(STATUS \"auto-generated library manifest cmake file (${CUTLASS_LIBRARY_MANIFEST_CMAKE_FILE}) not found.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Target for CUTLASS Hopper Unit Tests in CMake\nDESCRIPTION: This CMake snippet defines a custom target named `cutlass_test_unit_cute_hopper` which aggregates multiple test executables as dependencies.  The target's purpose is to provide a single point to trigger the execution of all Hopper related unit tests, managed via the DEPENDS keyword.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_cute_hopper\n  DEPENDS\n  cutlass_test_unit_cute_hopper_cooperative_gemm\n  cutlass_test_unit_cute_hopper_stsm\n  cutlass_test_unit_cute_hopper_tma_load\n  cutlass_test_unit_cute_hopper_tma_store\n  cutlass_test_unit_cute_hopper_bulk_load\n  cutlass_test_unit_cute_hopper_bulk_store\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Masked Tile Iterator Concept in CUTLASS\nDESCRIPTION: Defines the `MaskedTileIteratorConcept` in CUTLASS. This concept specifies an iterator capable of handling tiles that are not 'whole' in memory by using a mask object to guard against out-of-bounds memory accesses. The mask's specific behavior and operations are implementation-defined details for each iterator.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_13\n\nLANGUAGE: c++\nCODE:\n```\n/// Supports iterating over tiles that are not 'whole' in memory. Iterator maintains a mask object\n/// which guards against out-of-bounds access.\n///\n/// Note, this concept definition does not formally define operations on the mask or methods it\n/// supports. These remain implementation-dependent details of iterators implementing this concept.\nstruct MaskedTileIteratorConcept {\n\n  using Mask;                                        ///< mask object used to guard against acceses.\n\n  CUTLASS_DEVICE void clear_mask();                  ///< efficiently disables all accesses guarded by mask\n  CUTLASS_DEVICE void enable_mask();                 ///< efficiently enables all accesses guarded by mask\n\n  CUTLASS_DEVICE void get_mask(Mask &mask);          ///< gets the mask\n  CUTLASS_DEVICE void set_mask(Mask const &mask);    ///< sets the mask\n};\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Epilogue Dispatch Policy (TMA, 2SM, Nvf4)\nDESCRIPTION: This epilogue dispatch policy specifies the configuration for post-processing operations following a sparse matrix multiplication, using TMA (Tensor Memory Accelerator) across two SMs (Streaming Multiprocessors). It is designed specifically for narrow precision using the nvf4 data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n`cutlass::epilogue::TmaWarpSpecialized2SmNvf4`\n```\n\n----------------------------------------\n\nTITLE: Inner Partitioning Example C++\nDESCRIPTION: This code demonstrates the inner-partitioning technique. It creates a tensor `A`, applies a tiler, and then slices the tiled tensor to extract a subtile using threadgroup coordinates. The resulting tensor `cta_a` represents a tile assigned to a threadgroup.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nTensor A = make_tensor(ptr, make_shape(8,24));  // (8,24)\nauto tiler = Shape<_4,_8>{};                    // (_4,_8)\n\nTensor tiled_a = zipped_divide(A, tiler);       // ((_4,_8),(2,3))\n\nTensor cta_a = tiled_a(make_coord(_,_), make_coord(blockIdx.x, blockIdx.y));  // (_4,_8)\n```\n\n----------------------------------------\n\nTITLE: Running CUTLASS Python Interface in a Docker Container\nDESCRIPTION: This command runs the CUTLASS Python interface within an NGC PyTorch Docker container. This container provides a pre-configured environment for using CUTLASS with PyTorch.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs_src/source/install.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all -it --rm nvcr.io/nvidia/pytorch:23.08-py3\n```\n\n----------------------------------------\n\nTITLE: Setting Test Parameters CMake\nDESCRIPTION: This snippet sets CMake variables that define parameters for running CUTLASS tests. Each variable configures aspects such as random problem sizes, epilogue parameters, fixed problem sizes, small problem sizes, and performance test iterations. Setting `--iterations=0` disables performance benchmarking and only runs the correctness check.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/57_hopper_grouped_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_RANDOM --iterations=0)\nset(TEST_RANDOM_LARGE_GROUP --groups=500 --iterations=0)\n\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=0)\nset(TEST_EPILOGUE_LARGE_GROUP --alpha=1.5 --beta=2.0 --groups=500 --iterations=0)\n\nset(TEST_EPILOGUE_OP --beta=0.5 --iterations=1)\nset(TEST_EPILOGUE_OP_LARGE_GROUP --alpha=1.5 --iterations=1)\n\nset(TEST_FIXED --m=2048 --n=5120 --k=8192 --groups=50 --iterations=0)\nset(TEST_FIXED_LARGE_GROUP --m=2048 --n=512 --k=512 --groups=512 --iterations=0)\n\nset(TEST_SMALL --m=256 --n=128 --iterations=0)\nset(TEST_SMALL_LARGE_GROUP --m=128 --n=128 --groups=500 --iterations=0)\n\nset(TEST_RANDOM_PERF --iterations=10)\nset(TEST_RANDOM_PERF_LARGE_GROUP --groups=500 --iterations=10)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable - wgmma_tma_sm90 CUDA\nDESCRIPTION: This CMake function adds an executable target named `cute_tutorial_wgmma_tma_sm90` that compiles the `wgmma_tma_sm90.cu` CUDA source file. It utilizes the `cutlass_example_add_executable` CMake macro, assumed to be defined elsewhere in the CUTLASS build system, to handle CUDA compilation and linking. This example specifically targets wgmma using TMA (Tensor Memory Accelerator).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/hopper/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_wgmma_tma_sm90\n  wgmma_tma_sm90.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Define CUTLASS Example Executable using CMake\nDESCRIPTION: This CMake snippet defines an executable using the `cutlass_example_add_executable` macro. It creates an executable named `26_ampere_wgrad_mainloop_fusion` from the source file `ampere_wgrad_mainloop_fusion.cu`. This assumes the `cutlass_example_add_executable` macro is defined elsewhere, likely in a CUTLASS CMake utility file. The purpose is to compile the provided source file and create an executable application.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/26_ampere_wgrad_mainloop_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  26_ampere_wgrad_mainloop_fusion\n  ampere_wgrad_mainloop_fusion.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Conv2d Dgrad F32 Configuration\nDESCRIPTION: This example demonstrates a device convolution operation with dgrad (gradient of the output with respect to the input) using float32 data type and TensorOp. It utilizes implicit GEMM and specifies tensor layouts in NHWC format.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 16 8 8 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op TensorOp -b 128 128 16 -s 3 -w 2 2 1 -cc 80 -la TensorNHWC -aa 4 -lb TensorNHWC -ab 4 -lc TensorNHWC -ac 4 -te float32 -ep LinearCombination -sw StridedDgradIdentitySwizzle1 -co dgrad -st Strided -ia optimized -sm Serial -k 2 -nhwc 1 27 27 256 -krsc 512 3 3 256 -pad 1 1 1 1 -stride 2 1 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Profiler: CUDA Core Convolution Example\nDESCRIPTION: This bash snippet demonstrates profiling a forward propagation convolution kernel on CUDA cores using the CUTLASS profiler. It uses `simt_sfprop` kernels and device verification to assess performance with specific input dimensions and filter sizes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=simt_sfprop  --verification-providers=device --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n```\n\n----------------------------------------\n\nTITLE: Initializing CUTLASS GroupedGemm in Python\nDESCRIPTION: This snippet initializes a CUTLASS grouped GEMM operation with a specified data type and layout. It imports the necessary cutlass and torch libraries, defines the data type as float16, and creates a GroupedGemm object with the RowMajor layout. This object will be used to compile and run the grouped GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/02_pytorch_extension_grouped_gemm.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cutlass\nimport torch\n\ndtype = torch.float16\nplan = cutlass.op.GroupedGemm(element=dtype, layout=cutlass.LayoutType.RowMajor)\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Shapes for GEMM (CuTe)\nDESCRIPTION: This code defines the dynamic shapes (M, N, K) for the GEMM operation using the `make_shape` function. The values are taken from host function arguments m, n, and k and converted to integers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n  // Define shapes (dynamic)\n  auto M = int(m);\n  auto N = int(n);\n  auto K = int(k);\n  auto prob_shape = make_shape(M, N, K);    // (M, N, K)\n```\n\n----------------------------------------\n\nTITLE: Static Assertion for Shape and Stride Congruence in C++\nDESCRIPTION: This snippet demonstrates how to use a `static_assert` to ensure that a given `Shape` and `Stride` are congruent, meaning they have the same tuple profiles. This checks for compatibility at compile time.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nstatic_assert(congruent(my_shape, my_stride));\n```\n\n----------------------------------------\n\nTITLE: Creating Aliases for cutlass_library\nDESCRIPTION: This snippet creates aliases `cutlass_lib` and `cutlass_lib_static` for `cutlass_library` and `cutlass_library_static` respectively. This maintains backward compatibility with older code.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cutlass_lib ALIAS cutlass_library)\nadd_library(cutlass_lib_static ALIAS cutlass_library_static)\n```\n\n----------------------------------------\n\nTITLE: Thread ID Layout for Hopper GMMA\nDESCRIPTION: This code defines the thread ID layout for Hopper's GMMA (Group MMA) operation.  It assumes a simple 1D contiguous layout for 128 threads (4 warps) within a warpgroup. It represents the thread IDs as a linear sequence.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nusing ThrID = Layout<_128, _1>;\n```\n\n----------------------------------------\n\nTITLE: Setting target architecture with CMake - Bash\nDESCRIPTION: These snippets show how to specify the target architecture for CUTLASS during the CMake configuration step using the `CUTLASS_NVCC_ARCHS` flag. The architecture `90a` is used to maximize performance on Hopper GH100, and `100a` is used for Blackwell.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. -DCUTLASS_NVCC_ARCHS=\"90a\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. -DCUTLASS_NVCC_ARCHS=\"100a\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation of CUDA Example using CMake\nDESCRIPTION: This CMake snippet conditionally adds a CUDA executable if the `CUTLASS_NVCC_ARCHS` variable matches `100a`. It uses the `cutlass_example_add_executable` macro, assuming this macro is defined elsewhere in the CMake project to properly handle the creation of the executable from the CUDA source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/73_blackwell_gemm_preferred_cluster/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  73_blackwell_gemm_preferred_cluster\n  blackwell_gemm_preferred_cluster.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Stateless Universal Device GEMM Kernel Type (C++)\nDESCRIPTION: This code snippet declares the `GemmUniversal` class, a stateless universal device GEMM kernel type. It's designed to treat GEMM as a composition of a collective mainloop and a collective epilogue. It leverages SFIANE (Substitution Failure Is Not An Error) to shim both 2.x and 3.0 API kernels based on `ProblemShapeOrThreadblockMma_`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_backwards_compatibility.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n/*\n * Stateless universal device GEMM kernel type that treats GEMM as\n * a composition of a collective mainloop and a collective epilogue.\n * SFIANE shims both 2.x and 3.0 API kernels based on ProblemShapeOrThreadblockMma_.\n**/\ntemplate <\n  class ProblemShapeOrThreadblockMma_,\n  class CollectiveMainloopOrEpilogue_,\n  class CollectiveEpilogueOrThreadblockSwizzle_,\n  class TileScheduler_ = void,\n  class Enable = void\n>\nclass GemmUniversal;\n```\n\n----------------------------------------\n\nTITLE: Adding Executable using CMake in CUTLASS\nDESCRIPTION: This CMake command adds an executable named 'cutlass_test_unit_epilogue_threadblock' by compiling and linking together the specified CUDA source files. These files seem to be related to different implementations and optimizations for epilogue operations within the threadblock, including SIMT, tensor operations, Volta tensor operations, WMMA tensor operations, and planar complex epilogues.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/epilogue/threadblock/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_epilogue_threadblock\n  \n  predicated_tile_iterator.cu\n  output_tile_threadmap.cu\n  epilogue_simt.cu\n  epilogue_simt_sm60.cu\n  epilogue_simt_sm61.cu\n  epilogue_tensor_op.cu\n  epilogue_volta_tensor_op.cu\n  epilogue_wmma_tensor_op_sm70.cu\n  epilogue_planar_complex.cu\n  epilogue_with_reduction_tensor_op.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Appending NVRTC Includes Strings (CMake)\nDESCRIPTION: Appends strings to `NVRTC_INCLUDES_STRINGS` and `NVRTC_INCLUDES_NAMES` to define C++ arrays for the CUTLASS header content and names, respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nstring(APPEND NVRTC_INCLUDES_STRINGS \"char const *kCutlassHeaders[] = {\\n\")\nstring(APPEND NVRTC_INCLUDES_NAMES \"char const *kCutlassHeaderNames[] = {\\n\")\n```\n\n----------------------------------------\n\nTITLE: Epilogue Dispatch Policy CUDA\nDESCRIPTION: This header file defines the epilogue dispatch policies for collectives, kernel layers, and builders within CUTLASS. This is part of the CUTLASS 3.9 update and supports Blackwell SM120, enabling full EVT fusions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_1\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/epilogue/dispatch_policy.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to CUTLASS Profiler Target in CMake\nDESCRIPTION: Specifies the libraries that the `cutlass_profiler` target should be linked against.  It includes `cutlass_lib`, `cutlass_tools_util_includes`, and conditionally includes `nvidia::cublas` and `nvidia::cudnn` based on the `CUTLASS_ENABLE_CUBLAS` and `CUTLASS_ENABLE_CUDNN` flags. It also includes `cudart` and `cuda_driver`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(\n  cutlass_profiler\n  PRIVATE \n  cutlass_lib\n  cutlass_tools_util_includes\n  $<$<BOOL:${CUTLASS_ENABLE_CUBLAS}>:nvidia::cublas>\n  $<$<BOOL:${CUTLASS_ENABLE_CUDNN}>:nvidia::cudnn>\n  cudart\n  cuda_driver\n  )\n```\n\n----------------------------------------\n\nTITLE: Setting a Cache Variable for Device Reference (CMake)\nDESCRIPTION: This snippet defines a CMake cache variable, `CUTLASS_CONV_TEST_UNIT_REFERENCE_DEVICE_ENABLED`, which controls whether device reference verification is enabled during convolution unit tests.  The variable is initialized with `ON` and can be toggled by the user through the CMake interface.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUTLASS_CONV_TEST_UNIT_REFERENCE_DEVICE_ENABLED ON CACHE BOOL\n  \"Enable/Disable convolution device reference for conv unit tests.\")\n```\n\n----------------------------------------\n\nTITLE: Error Handling for CUTLASS Library Generation\nDESCRIPTION: This snippet checks the result of the library instance generation process. If the `cutlass_lib_INSTANCE_GENERATION_RESULT` variable is not equal to 0, it indicates an error during the generation process. The snippet then uses the `message` command to report a fatal error, directing the user to the log file for more details.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT cutlass_lib_INSTANCE_GENERATION_RESULT EQUAL 0)\n  message(FATAL_ERROR \"Error generating library instances. See ${CMAKE_CURRENT_BINARY_DIR}/library_instance_generation.log\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable with Stream K Source File using CMake\nDESCRIPTION: This CMake code uses the 'cutlass_test_unit_gemm_device_add_executable' macro to create an executable that focuses on stream K optimization within the GEMM computation. It includes the CUDA source file (.cu) specifically designed for stream K for SM120.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120_stream_k\n  sm120_bs_gemm_nvf4_nvf4_f32_f32_stream_k.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Add Custom Target for TensorOp GEMM Tests (sm100)\nDESCRIPTION: This CMake snippet defines a custom target `cutlass_test_unit_gemm_device_sm100_tensorop` that depends on several executables. These executables test different data types (f16, f8, s8) using Tensor Cores on the sm100 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm100_tensorop\n  DEPENDS\n  cutlass_test_unit_gemm_device_tensorop_sm100_f16xf16\n  cutlass_test_unit_gemm_device_tensorop_sm100_f8xf8\n  cutlass_test_unit_gemm_device_tensorop_sm100_s8xs8\n)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Layout Composition\nDESCRIPTION: This snippet shows the post-conditions for the `composition` function. It ensures that the resulting layout is compatible with the second layout being composed and that the result of applying the composition is equivalent to applying the layouts sequentially.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// @post compatible(@a layout_b, @a result)\n// @post for all i, 0 <= i < size(@a layout_b), @a result(i) == @a layout_a(@a layout_b(i)))\nLayout composition(LayoutA const& layout_a, LayoutB const& layout_b)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable (CMake)\nDESCRIPTION: This CMake code snippet defines an executable named '46_depthwise_simt_conv2dfprop' using the `cutlass_example_add_executable` function. The executable is built from the CUDA source file 'depthwise_simt_conv2dfprop.cu'. This function is specific to the CUTLASS build environment. The function assumes that the CUDA compiler is available and configured within the CMake environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/46_depthwise_simt_conv2dfprop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  46_depthwise_simt_conv2dfprop\n  depthwise_simt_conv2dfprop.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policy CMP0112\nDESCRIPTION: This snippet sets the CMake policy CMP0112 to NEW.  This policy controls whether `target_link_libraries` prefers imported targets over global libraries with the same name.  Setting it to `NEW` enables the modern CMake behavior.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_policy(SET CMP0112 NEW)\n```\n\n----------------------------------------\n\nTITLE: Run GEMM with CUTLASS Epilogue Visitor\nDESCRIPTION: This snippet demonstrates how to run a GEMM operation using the CUTLASS plan with the defined epilogue visitor. It sets the `epilogue_visitor` field of the plan to the generated `epilogue_visitor` object. It passes the required input tensors and arguments to the `plan.run` function, using a dictionary for the visitor arguments and enabling module printing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/04_epilogue_visitor.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nvisitor_args = {\n    \"alpha\": alpha, \"C\": tensor_C, \"beta\": beta, \n    \"aux\": aux, \"bias\": bias, \"D\": tensor_D, \"F\": tensor_F\n}\n\nplan.epilogue_visitor = epilogue_visitor\nplan.run(\n    tensor_A, tensor_B, tensor_C, tensor_D, \n    visitor_args=visitor_args, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Defining TN Strides (CuTe)\nDESCRIPTION: This code defines the strides for A, B, and C matrices in the TN (transposed) configuration. The leading dimensions are used to set the strides for each matrix.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\n  // Define TN strides (mixed)\n  auto dA = make_stride(ldA, Int<1>{});    // (dM, dK)\n  auto dB = make_stride(ldB, Int<1>{});    // (dN, dK)\n  auto dC = make_stride(Int<1>{}, ldC);    // (dM, dN)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds the 'thread', 'kernel', and 'device' directories as subdirectories in the CMake project. This allows the build system to process the CMakeLists.txt files within these subdirectories and include their targets in the overall build.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/reduction/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thread)\nadd_subdirectory(kernel)\nadd_subdirectory(device)\n```\n\n----------------------------------------\n\nTITLE: Map Threadblock ID to Macro Tile ID\nDESCRIPTION: This snippet describes how to map a zero-indexed threadblock ID `t` to its 'macro tile ID' `t_macro`. The mapping is done by integer division of the threadblock ID by the ratio of the maximum dimension of the grid to the minimum dimension of the grid.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nt_macro = t // r\n```\n\n----------------------------------------\n\nTITLE: Comparing Tensor Core and SIMT GEMM Outputs\nDESCRIPTION: This snippet compares the output of the Tensor Core GEMM (tensor_D) with the output of the SIMT GEMM (tensor_D_simt) using `np.testing.assert_array_equal`. This verifies that both operations produce the same result.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnp.testing.assert_array_equal(tensor_D, tensor_D_simt)\n```\n\n----------------------------------------\n\nTITLE: Checking NVLink Network Topology\nDESCRIPTION: This command uses `nvidia-smi` to check the GPU network topology. It verifies that there is an NVLink network (`NV*` links) between every pair of GPUs.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/82_blackwell_distributed_gemm/REQUIREMENTS.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi topo -m\n```\n\n----------------------------------------\n\nTITLE: Installing CUTLASS Python Interface from Source\nDESCRIPTION: This command installs the CUTLASS Python interface from source. It requires navigating to the root of the CUTLASS directory. This installs the package in a standard way.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs_src/source/install.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: East Const Declaration in C++\nDESCRIPTION: This snippet illustrates the \"East const\" convention used in CUTLASS, where the const or constexpr keyword goes after the type it modifies.  It provides examples of const declarations for floats, references, and pointers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nfloat constexpr compile_time_constant = 42.3f;\n\nfloat const const_float = /* whatever */;\nfloat const& reference_to_const_float = const_float;\nfloat const* pointer_to_const_float = &const_float;\nfloat const* const const_pointer_to_const_float = &const_float;\n\nfloat nonconst_float;\nfloat& reference_to_nonconst_float = nonconst_float;\nfloat* pointer_to_nonconst_float = &nonconst_float;\nfloat* const pointer_to_nonconst_float = &nonconst_float;\n```\n\n----------------------------------------\n\nTITLE: Defining Cutlass Test Unit Executable with CMake\nDESCRIPTION: This CMake snippet defines a test unit executable for the Cutlass library. It utilizes the `cutlass_test_unit_add_executable` function to specify the target name `cutlass_test_unit_reduction_thread` and associates it with the source files `reduction_thread.cu` and `testbed.h`. These files will be compiled and linked to create the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/reduction/thread/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_reduction_thread\n  reduction_thread.cu\n  testbed.h\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding SM100 Sparse F6F8xF6F8 GEMM Test Executable (CMake)\nDESCRIPTION: This snippet utilizes a CMake function `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for the `f6f8xf6f8` sparse GEMM test on the SM100 architecture. It specifies the source files to be compiled and linked into the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_f6f8xf6f8\n\n  sm100_sp_gemm_f6_f8_f32_f16_f16_tn.cu\n  sm100_sp_gemm_f8_f6_f32_f16_f16_tn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Printing a 2D Layout in C++\nDESCRIPTION: This template function, `print2D`, iterates through a rank-2 layout and prints the index value for each coordinate (m, n).  It requires a `Layout` object as input and uses the `size` function to determine the bounds of the layout dimensions. The function prints the elements in a 2D table format, separated by spaces.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <class Shape, class Stride>\nvoid print2D(Layout<Shape,Stride> const& layout)\n{\n  for (int m = 0; m < size<0>(layout); ++m) {\n    for (int n = 0; n < size<1>(layout); ++n) {\n      printf(\"%3d  \", layout(m,n));\n    }\n    printf(\"\\n\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Instance Library Directory Structure\nDESCRIPTION: This snippet shows the directory structure of the CUTLASS Instance Library, which contains pre-defined instantiations of CUTLASS templates. It includes header files defining the API for launching kernels and structs describing the operator instances, as well as Python scripts used to procedurally generate these instances.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntools/\n  library/                   # static/dynamic library containing all kernel instantiations of interest\n                             # (with some build-level filter switches to compile specific subsets)\n\n    include/\n      cutlass/\n        library/             # header files for CUTLASS Deliverables Library (in cutlass::library:: namespace)\n\n          handle.h           # implements a host-side API for launching kernels, similar to cuBLAS\n          library.h          # defines enums and structs to describe the tiled structure of operator instances          \n          manifest.h         # collection of all instances\n\n    src/\n\npython/\n    cutlass_library/       # scripts to procedurally generate CUTLASS template instances\n\n      gemm_operations.py\n      library.py\n      generator.py            # entry point of procedural generation scripts - invoked by cmake\n      manifest.py\n```\n\n----------------------------------------\n\nTITLE: Checking Peer-to-Peer Access\nDESCRIPTION: This command utilizes `nvidia-smi topo -p2p r` to verify that peer-to-peer (P2P) access is enabled between GPUs. The expected output is 'OK' for each GPU pair, confirming that direct memory access between GPUs is available, which is crucial for distributed computations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/REQUIREMENTS.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi topo -p2p r\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable in CMake\nDESCRIPTION: This snippet defines an executable named '04_tile_iterator' within the CUTLASS project. The `cutlass_example_add_executable` function is responsible for configuring the build process for this executable. The source code for the executable is located in 'tile_iterator.cu'. This snippet is likely found in a CMakeLists.txt file used to build the CUTLASS examples.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/04_tile_iterator/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  04_tile_iterator\n  tile_iterator.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable\nDESCRIPTION: This code defines an executable named `15_ampere_sparse_tensorop_gemm_universal` using the `cutlass_example_add_executable` macro. It links the source file `ampere_sparse_tensorop_gemm_universal.cu` to the executable. This executable likely demonstrates a more general or universal GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/15_ampere_sparse_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  15_ampere_sparse_tensorop_gemm_universal\n  ampere_sparse_tensorop_gemm_universal.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining SIMT Convolution Test Executable\nDESCRIPTION: This snippet defines a test executable for convolution operations using SIMT (CUDA cores). It specifies the source files to be compiled into the executable, including different convolution forward, backward data gradient, and backward weight gradient implementations for various data types (F32, CF32, F16) and CUDA architectures (sm50, sm60).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv_device_simt\n  \n  # F32  \n  conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu\n\n  # CF32\n  conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu\n  conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu\n  conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu \n\n  # F16\n  conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu\n  depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu\n  depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu\n  depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up the Collective Epilogue\nDESCRIPTION: This C++ code snippet defines the epilogue for a block-scaled GEMM kernel using CUTLASS, specifying the output layout, datatype, and alignment for the C and D tensors.  It utilizes `cutlass::epilogue::collective::CollectiveBuilder` to construct the epilogue, setting parameters such as tile shapes, data types, and the epilogue schedule policy (`cutlass::epilogue::TmaWarpSpecialized2Sm`) to configure the post-processing stage of the GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\n  // Describe C and D tensors\n  using ElementC = cutlass::half_t;\n  constexpr int AlignC = 8;\n  using GmemLayoutC = cutlass::layout::RowMajor;\n  using ElementD = cutlass::float_e2m1_t;\n  constexpr int AlignD = 32;\n  using GmemLayoutD = cutlass::layout::RowMajor;\n  // Mma's accumulator type\n  using ElementAccumulator = float;\n  // Epilogue computation's precision type\n  using ElementCompute = float;\n  \n  //\n  // Construct CollectiveEpilogue\n  //\n\n  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<\n      cutlass::arch::Sm100, cutlass::arch::OpClassBlockScaledTensorOp,      // Arch and Tensorop spec\n      MmaTileShape_MNK, ClusterShape_MNK,                                   // MMA tile shape, and cluster shape\n      cutlass::epilogue::collective::EpilogueTileAuto,                      // Epilogue subtile shape. Auto will find a suitable tile shape\n      ElementAccumulator, ElementCompute,                                   // Mma instr's accumulator type and compute precision for epilogue\n      ElementC, GmemLayoutC, AlignC,                                        // C tensor description\n      ElementD, GmemLayoutD, AlignD,                                        // D tensor description\n      cutlass::epilogue::TmaWarpSpecialized2Sm                              // Epilogue schedule policy\n    >::CollectiveOp;\n```\n\n----------------------------------------\n\nTITLE: Installing CMake Targets\nDESCRIPTION: Installs the 'cutlass_tools_util_includes' target and exports it under the name 'NvidiaCutlass'.  This allows other CMake projects to easily find and use this target.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/util/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(\n  TARGETS cutlass_tools_util_includes\n  EXPORT NvidiaCutlass\n  )\n```\n\n----------------------------------------\n\nTITLE: Epilogue Predication with Identity Tensors\nDESCRIPTION: This snippet demonstrates epilogue predication, where a tensor of coordinates is used for predication to manage out-of-bounds access during the final accumulation.  It shows the creation of an identity tensor for the output matrix, partitioning it, and then conditionally updating elements based on coordinate comparison.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\n// Repeat with a tensor of coordinates for predication\nTensor cC   = make_identity_tensor(make_shape(size<0>(gC), size<1>(gC)));\nTensor tCcC = thr_mma.partition_C(cC);\n\nconst bool isBetaZero = (beta == 0);\n\nCUTE_UNROLL\nfor (int i = 0; i < size(tCrC); ++i) {\n  if (elem_less(tCcC(i), make_coord(m_max_coord,n_max_coord))) {\n    tCgC(i) = isBetaZero ? alpha * tCrC(i) : alpha * tCrC(i) + beta * tCgC(i);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with CMake using 100a\nDESCRIPTION: This snippet demonstrates how to use CMake to configure the build process with the `-DCUTLASS_NVCC_ARCHS` option to specify the target CUDA architecture as 100a for maximizing performance on Blackwell architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ncmake .. -DCUTLASS_NVCC_ARCHS=\"100a\" \n```\n\n----------------------------------------\n\nTITLE: Half-Precision Example with CUDA Kernel in CUTLASS\nDESCRIPTION: This example demonstrates the usage of `cutlass::half_t` in both host and device (CUDA kernel) code. It includes a simple CUDA kernel that multiplies a half-precision value by 2.0 and prints the result.  The host code reads a half-precision value, prints its double, and then launches the kernel. Requires CUDA and CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\n#include <cutlass/numeric_types.h>\n\n__global__ void kernel(cutlass::half_t x) {\n  printf(\"Device: %f\\n\", float(x * 2.0_hf));\n}\n\nint main() {\n\n  cutlass::half_t x = 0.5_hf;\n\n  std::cin >> x;\n\n  std::cout << \"Host: \" << 2.0_hf * x << std::endl;\n\n  kernel<<< dim3(1,1), dim3(1,1,1) >>>(x);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Library Subdirectory (Conditional)\nDESCRIPTION: This conditional block adds the 'library' subdirectory if the `CUTLASS_ENABLE_LIBRARY` variable is set to true. The 'library' subdirectory likely contains the core CUTLASS library implementation.  Enabling the library is a prerequisite for enabling the profiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_ENABLE_LIBRARY)\n  add_subdirectory(library)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Test Groups (CMake)\nDESCRIPTION: Defines a CMake variable for test groups. The `--groups` option allows the creation of multiple independent GEMM problems within a single test invocation. Iterations are set to 0 for correctness checks.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_RANDOM_LARGE_GROUP --groups=100 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Printing the Operation Class of CUTLASS GEMM\nDESCRIPTION: This snippet demonstrates how to print the operation class (`opclass`) of the CUTLASS GEMM plan. The `opclass` property indicates the type of kernel being used (e.g., Tensor Core or SIMT).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(plan.opclass)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM StreamK Test (SM100)\nDESCRIPTION: This CMake code adds an executable for a blockscaled sparse GEMM StreamK test. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_streamk` with batch compilation enabled and specifies the source files to be compiled. This test case likely focuses on the StreamK optimization for sparse GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_streamk\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f16_nvf4_o_tnt_streamk.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_tnt_streamk.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining cutlass_library_internal_interface Library\nDESCRIPTION: This snippet creates an interface library named `cutlass_library_internal_interface`. It sets the include directories to the source and binary directories for internal headers and links it against `cutlass_library_includes`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cutlass_library_internal_interface INTERFACE)\nadd_library(nvidia::cutlass::library::obj_interface ALIAS cutlass_library_internal_interface)\n\ntarget_include_directories(\n  cutlass_library_internal_interface\n  INTERFACE\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src>\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}/include>\n  )\n\ntarget_link_libraries(\n  cutlass_library_internal_interface\n  INTERFACE\n  cutlass_library_includes\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Test Executable in CMake\nDESCRIPTION: This CMake function `cutlass_test_unit_add_executable` defines an executable named `cutlass_test_unit_gemm_warp`. It links the executable with a list of CUDA source files, each designed for a specific SM architecture (e.g., sm50, sm60, sm70, sm80, sm90) and GEMM variations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/warp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_gemm_warp\n  gemm_sm50.cu\n  gemm_sm60.cu\n  gemm_sm61.cu\n  gemm_sm70.cu\n  gemm_sm75.cu\n  gemm_sm80.cu\n  gemm_complex_sm80.cu\n  gemm_sparse_sm80.cu\n  gemm_gaussian_complex_sm80.cu\n  gemm_mixed_input_sm80.cu\n  gemm_sm90.cu\n  gemm_complex_sm90.cu\n  wmma_sm70.cu\n  wmma_sm72.cu\n  wmma_sm75.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Compute Capability\nDESCRIPTION: This command uses `nvidia-smi` to query the name and compute capability of the GPUs. It is used to verify that the GPUs are Blackwell GPUs with compute capability 10.0.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/82_blackwell_distributed_gemm/REQUIREMENTS.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi --query-gpu=name,compute_cap --format=csv\n```\n\n----------------------------------------\n\nTITLE: Gather/Scatter Activation Layout\nDESCRIPTION: This code defines the gather/scatter activation tensor layout using CuTe.  It uses IndexedGather to access memory indirectly via an index buffer, enabling gather/scatter convolutions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/README.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// Inner layout of the composition:\n// ((nzpq), (csrt)) => (idx_buffer_idx, dense_offset)\nauto EG = E<0>{};  // Gather basis     (1,0) (idx_buffer_idx) \nauto EC = E<1>{};  // Contiguous basis (0,1) (dense_offset)    \nauto xformed_act_logical_inner = make_layout(\n  make_shape (make_shape (       N,      Z,    P,  Q), make_shape ( C,      T,    R,  S)),\n  make_stride(make_stride(D*H*W*EG, H*W*EG, W*EG, EG), make_stride(EC, H*W*EG, W*EG, EG)));\n\n// Outer layout of the composition:\n// (idx_buffer_idx, dense_offset) => idx\n// IndexedGather obtains idx by applying (gmem_base_ptr + gather_idx_buf[idx_buffer_idx] + dense_offset)\nauto xformed_act_gather_outer = make_layout(\n  make_shape(_1{},_1{}),\n  make_stride(CustomStride{IndexedGather{gather_idx_buf}, C}, _1{}));\n\n// Compose the inner and outer layouts\n// ((nzpq), (ctrs)) => idx\nauto xformed_act_composed_layout = composition(\n  xformed_act_gather_outer,\n  make_arithmetic_tuple(_0{}, _0{}),\n  xformed_act_logical_inner);\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to Profiler Output (CUTLASS)\nDESCRIPTION: This command demonstrates how to add custom tags to the profiler output for facilitating the generation of pivot tables and charts. The `--tags=<column>:<value>` option allows prepending additional columns to the CSV output.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sgemm_128x128_nn            \\\n                                    --m=3456 --n=4096 --k=8:4096:8 --output=report.csv \\\n                                    --tags=cutlass:2.2,date:2020-06-08\n```\n\n----------------------------------------\n\nTITLE: Add TF32-F32 Convolution Test Executable (SM80)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with TF32 input, F32 output, and F32 accumulation on SM80 architecture, conditionally compiled based on the `CUTLASS_NVCC_MAX_ARCH` variable. It includes source files for 2D and 3D convolution's forward, data gradient and weight gradient computations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n  # Conv - TF32 input, F32 output, F32 accumulation\n  cutlass_test_unit_add_executable(\n    cutlass_test_unit_conv_device_tensorop_f32_tf32_sm80\n  \n    conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu\n    conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu\n    conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu\n  \n    conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu\n    conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32nhdwc_tensor_op_f32_sm80.cu\n    conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Test Unit Executable in CMake\nDESCRIPTION: This CMake command uses the `cutlass_test_unit_add_executable` macro to define a test executable.  It creates an executable named `cutlass_test_unit_transform_kernel` using `filter_format_transformer.cu` as the source file.  This is a standard practice in CUTLASS projects for creating and managing unit tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/kernel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_transform_kernel\n  filter_format_transformer.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Executable (tiled_cp_async)\nDESCRIPTION: This snippet utilizes `cutlass_test_unit_add_executable` to generate the `cutlass_test_unit_cute_ampere_tiled_cp_async` executable.  It compiles `tiled_cp_async.cu` to create this executable. This test unit likely evaluates tiled and asynchronous memory copy operations on the Ampere architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/ampere/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_ampere_tiled_cp_async\n  tiled_cp_async.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling PDL for SM90/SM100 using CMake\nDESCRIPTION: This CMake command enables PDL-related instructions during the CUTLASS build process for SM90 and SM100 architectures. It adds the necessary compilation flags to include PDL functionality in the compiled kernels. This must be executed before building CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/dependent_kernel_launch.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake . -DCUTLASS_ENABLE_GDC_FOR_SM90=1\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x124x224x96_229x129_24x7x7_pad_h3w3_stride_h1w1_dil_h1w1_corr_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 805200720 858259717 3333097025 2967627788\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Executable (cute_ampere)\nDESCRIPTION: This snippet uses the `cutlass_test_unit_add_executable` CMake function to create an executable named `cutlass_test_unit_cute_ampere`.  It links several CUDA source files including `cp_sync.cu`, `ldsm.cu`, `cooperative_gemm.cu`, and `cooperative_copy.cu` into the executable. This test unit likely evaluates cooperative GEMM and memory copy functionality on Ampere architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/ampere/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_ampere\n  cp_sync.cu\n  ldsm.cu\n  cooperative_gemm.cu\n  cooperative_copy.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Installing CUTLASS Python Interface from Source in Developer Mode\nDESCRIPTION: This command installs the CUTLASS Python interface from source in developer mode. This allows changes to the CUTLASS Python interface to be reflected immediately when using the interface. It requires navigating to the root of the CUTLASS directory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs_src/source/install.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Launch JupyterLab for CUTLASS Examples\nDESCRIPTION: This command launches JupyterLab from the current directory, pointing to the CUTLASS examples located in the `../examples/python` directory. It allows users to access and run the CUTLASS Python interface examples.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njupyter-lab ../examples/python\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution based on Architecture (CMake)\nDESCRIPTION: This snippet demonstrates conditional execution based on the value of `CUTLASS_NVCC_ARCHS`. If the variable matches \"100a\", the contained commands are executed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\n\n...\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_32x24x32x32_24x31_32x1x2_pad_h0w0_stride_h1w1_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1089703540 1191155361 852881505 3112064590\n```\n\n----------------------------------------\n\nTITLE: Coordinate Mapping with idx2crd in CuTe (C++)\nDESCRIPTION: This snippet demonstrates the use of `cute::idx2crd` to map indices and coordinates to their equivalent natural coordinates within a given shape. It showcases how different input types (integers and coordinates) are handled by the function, and the shape is defined as `Shape<_3,Shape<_2,_3>>`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nauto shape = Shape<_3,Shape<_2,_3>>{};\nprint(idx2crd(   16, shape));                                // (1,(1,2))\nprint(idx2crd(_16{}, shape));                                // (_1,(_1,_2))\nprint(idx2crd(make_coord(   1,5), shape));                   // (1,(1,2))\nprint(idx2crd(make_coord(_1{},5), shape));                   // (_1,(1,2))\nprint(idx2crd(make_coord(   1,make_coord(1,   2)), shape));  // (1,(1,2))\nprint(idx2crd(make_coord(_1{},make_coord(1,_2{})), shape));  // (_1,(1,_2))\n```\n\n----------------------------------------\n\nTITLE: Building CUTLASS with PDL Support\nDESCRIPTION: This command configures the CMake build system for CUTLASS with support for Programmatic Dependent Launch (PDL) for SM90 architecture. It sets the CUDA architecture to 90a and enables GDC for SM90.  This configuration ensures CUTLASS is built to leverage specific hardware features for improved performance.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/65_distributed_gemm/REQUIREMENTS.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake $PATH -DCUTLASS_NVCC_ARCHS=\"90a\" -DCUTLASS_ENABLE_GDC_FOR_SM90=1\n```\n\n----------------------------------------\n\nTITLE: SM120 Sparse GEMM Executable with Epilogue Fusion (CMake)\nDESCRIPTION: This CMake snippet configures the build process for a specific sparse GEMM test executable that incorporates epilogue fusion on the SM120 architecture. The `BATCH_SOURCES` and `BATCH_SIZE` options are used to control the compiler's memory usage during the compilation of the source files. Epilogue fusion is a technique used to optimize the final stages of GEMM calculations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_sparse_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_sparse_gemm_device_tensorop_sm120_epilogue_fusion\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n  sm120_sparse_gemm_f4_f4_f32_tensor_op_epilogue_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining cutlass_library_includes Interface Library\nDESCRIPTION: This snippet creates an interface library named `cutlass_library_includes`. It defines include directories for both build and install interfaces, and links the `CUTLASS` and `cutlass_tools_util_includes` libraries to it.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cutlass_library_includes INTERFACE)\nadd_library(nvidia::cutlass::library::includes ALIAS cutlass_library_includes)\nset_target_properties(cutlass_library_includes PROPERTIES EXPORT_NAME library::includes)\n\ntarget_include_directories(\n  cutlass_library_includes\n  INTERFACE\n  $<INSTALL_INTERFACE:include>\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n  )\n\ntarget_link_libraries(\n  cutlass_library_includes \n  INTERFACE \n  CUTLASS\n  cutlass_tools_util_includes\n  )\n\ninstall(\n  TARGETS cutlass_library_includes\n  EXPORT NvidiaCutlass\n  )\n\ninstall(\n  DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Executable with CMake\nDESCRIPTION: This snippet utilizes the `cutlass_example_add_executable` CMake function to define a CUDA executable. It specifies the name of the executable and the source file. The executable is named `48_hopper_warp_specialized_gemm` and is built from `48_hopper_warp_specialized_gemm.cu`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/48_hopper_warp_specialized_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  48_hopper_warp_specialized_gemm\n  48_hopper_warp_specialized_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library in CMake\nDESCRIPTION: Defines an INTERFACE library 'cutlass_tools_util_includes' and an alias 'nvidia::cutlass::tools::util' for it.  This allows other targets to link against this utility library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/util/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(cutlass_tools_util_includes INTERFACE)\nadd_library(nvidia::cutlass::tools::util ALIAS cutlass_tools_util_includes)\nset_target_properties(cutlass_tools_util_includes PROPERTIES EXPORT_NAME tools::util)\n```\n\n----------------------------------------\n\nTITLE: Profiling Subset Tensor Core GEMM Kernels\nDESCRIPTION: This command line executes the CUTLASS profiler with a specified kernel pattern and problem dimensions (m, n, k).  It focuses on Tensor Core GEMM kernels with FP32 accumulation and FP16 input, configured in the previous build step.  The output includes performance metrics such as runtime, memory bandwidth, and achieved GFLOP/s.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*gemm_f16_*_nt_align8 --m=3456 --n=4096 --k=4096\n\n...\n```\n\n----------------------------------------\n\nTITLE: Globbing Python Generator Sources\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to find all Python source files in the `scripts/` directory and its subdirectories. The `CONFIGURE_DEPENDS` flag ensures that CMake re-runs if any of these files change. The files found are stored in the `GENERATOR_PYTHON_SOURCES` variable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE GENERATOR_PYTHON_SOURCES CONFIGURE_DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/scripts/*.py)\n```\n\n----------------------------------------\n\nTITLE: Installing Include Directory in CMake\nDESCRIPTION: Installs the contents of the 'include' directory from the current source directory to the specified installation include directory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/util/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(\n  DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper TMA Multi-Cast Load Unit Test Executable with CMake\nDESCRIPTION: This CMake configuration creates the `cutlass_test_unit_cute_hopper_tma_mcast_load` executable, which is responsible for testing TMA multi-cast load operations on the Hopper architecture. The executable is built from the `tma_mcast_load.cu` source file using the helper function `cutlass_test_unit_add_executable`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_tma_mcast_load\n  tma_mcast_load.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Configure GEMM kernels for Ampere Tensor Cores\nDESCRIPTION: This CMake command configures CUTLASS to compile only GEMM kernels that target NVIDIA Ampere Tensor Cores (architecture 80).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_28\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=80 -DCUTLASS_LIBRARY_KERNELS=tensorop*gemm\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target for FMHA Variable SeqLen in CMake\nDESCRIPTION: This code snippet uses the `cutlass_example_add_executable` macro to create an executable target for the fused multi-head attention example with variable sequence length. It specifies the target name and the source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/41_fused_multi_head_attention/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  41_fused_multi_head_attention_variable_seqlen\n  fused_multihead_attention_variable_seqlen.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_4x2x3x256_1x1_328x3x5_pad_h1w1_stride_h1w1_dil_h1w1_conv_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 3153833039 1258133396 2989357662 845687444\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS test executable in CMake\nDESCRIPTION: This CMake code snippet adds an executable for CUTLASS tests. It utilizes the `cutlass_test_unit_add_executable` function to create an executable named `cutlass_test_unit_cute_volta` using the provided CUDA source files: `vectorization_auto.cu` and `cooperative_gemm.cu`. The executable likely contains test cases designed to be run on Volta architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/volta/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_volta\n  vectorization_auto.cu\n  cooperative_gemm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Operation Class in CUTLASS Python\nDESCRIPTION: Prints the current operation mode (opclass) of the CUTLASS GEMM plan. This shows whether the GEMM is using Tensor Core operations or SIMT operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/00_basic_gemm.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(plan.opclass)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable\nDESCRIPTION: This code defines an executable named `15_ampere_sparse_tensorop_gemm` using the `cutlass_example_add_executable` macro. It links the source file `ampere_sparse_tensorop_gemm.cu` to the executable. The macro likely handles the compilation and linking process for the CUDA source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/15_ampere_sparse_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  15_ampere_sparse_tensorop_gemm\n  ampere_sparse_tensorop_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Pascal Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Pascal GPU architecture using CMake. The `CUTLASS_NVCC_ARCHS` flag specifies the target architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"60;61\"          # compiles for NVIDIA Pascal GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Defining Fixed Problem Sizes with Large Groups (CMake)\nDESCRIPTION: Defines CMake variables for fixed problem sizes with large groups, specifying matrix dimensions (m, n, k), group size, and iterations set to 0 for correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_FIXED_LARGE_GROUP --m=2048 --n=512 --k=512 --groups=100 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Finalizing NVRTC Includes Strings (CMake)\nDESCRIPTION: Appends closing braces and computes the size of the header array.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nstring(APPEND NVRTC_INCLUDES_STRINGS \"};\\n\")\nstring(APPEND NVRTC_INCLUDES_NAMES \"};\\n\")\n\nstring(APPEND NVRTC_INCLUDES_STRINGS \"const size_t kCutlassHeaderCount = sizeof(kCutlassHeaders) / sizeof(*kCutlassHeaders);\\n\")\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Library (Infra)\nDESCRIPTION: This snippet defines a CUTLASS object library named 'cutlass_test_unit_infra'. It includes the 'filter_architecture.cpp' source file. This library serves as a foundation for other unit tests by providing common functionalities.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_add_library(\n  cutlass_test_unit_infra \n  OBJECT\n  common/filter_architecture.cpp\n  )\n```\n\n----------------------------------------\n\nTITLE: Setting CUTLASS Generator Compiler Version\nDESCRIPTION: This snippet sets the CUTLASS generator compiler version using the current CUDA compiler version, and specifies the path to the generated kernel list file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUTLASS_GENERATOR_CUDA_COMPILER_VERSION ${CMAKE_CUDA_COMPILER_VERSION})\nset(CUTLASS_LIBRARY_GENERATED_KERNEL_LIST_FILE ${CMAKE_CURRENT_BINARY_DIR}/generated_kernels.txt CACHE STRING \"Generated kernel listing file\")\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Template Library Directory Structure\nDESCRIPTION: This code snippet represents the directory structure for the CUTLASS Template Library, which contains CUDA templates for linear algebra subroutines and solvers. It shows the organization of header files related to architecture features, GEMM, layout, reduction, transformation, and utilities.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ninclude/                     # Top-level include directory. Client applications should target this path.\n  cutlass/                   # CUDA Templates for Linear Algebra Subroutines and Solvers - headers only\n\n    arch/                    # direct exposure of architecture features (including instruction-level GEMMs)\n      *\n    gemm/                    # code specialized for general matrix product computations\n      thread/                #   thread-level operators\n      warp/                  #   warp-level operators\n      collective/            #   3.x API operators for all threads a tiled mma/copy are built over\n      threadblock/           #   CTA-level operators\n      kernel/                #   CUDA kernel entry points\n      device/                #   launches kernel(s) over a full device\n      *                      # scope-agnostic components and basic vocabulary type definitions for GEMM\n\n    layout/                  # layout definitions for matrices, tensors, and other mathematical objects in memory\n      *\n\n    reduction/               # bandwidth-limited reduction kernels that do not fit the \"gemm\" models\n      thread/                #   thread-level operators\n      warp/                  #   warp-level operators\n      threadblock/           #   CTA-level operators\n      kernel/                #   CUDA kernel entry points\n      device/                #   launches kernel(s) over a full device\n      *                      # scope-agnostic components and basic vocabulary type definitions\n\n    transform/               # code specialized for layout, type, and domain transformations\n      thread/                #   thread-level operators\n      warp/                  #   warp-level operators\n      threadblock/           #   CTA-level operators\n      kernel/                #   CUDA kernel entry points\n      device/                #   launches kernel(s) over a full device\n      *                      # scope-agnostic components and basic vocabulary type definitions\n\n    util/                    # miscellaneous CUTLASS components\n      *\n    *                        # core vocabulary types and fundamental arithmetic operators\n\n  cute /                     # CuTe Layout, layout algebra, MMA/Copy atoms, tiled MMA/Copy\n    algorithm/               # Definitions of core operations such as copy, gemm, and operations on cute::tuples\n    arch/                    # Bare bones PTX wrapper structs for copy and math instructions\n    atom/                    # Meta-information either link to or built from arch/ operators\n      mma_atom.hpp           # cute::Mma_Atom and cute::TiledMma\n      copy_atom.hpp          # cute::Copy_Atom and cute::TiledCopy\n      *sm*.hpp               # Arch specific meta-information for copy and math operations\n    container/               # Core container types used across CuTe, namely, cute::tuple\n    numeric/                 # CuTe's internal numerics implementation\n    *                        # Core library types such as Shape, Stride, Layout, Tensor, and associated operations\n```\n\n----------------------------------------\n\nTITLE: Accessing Tile Descriptions for CUTLASS Kernels\nDESCRIPTION: This snippet retrieves the tile descriptions for available GEMM configurations from the CUTLASS profiler. It then prints the number of tile descriptions and the first few tile descriptions to the console.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntiles = plan.tile_descriptions()\nprint('{} tile descriptions returned'.format(len(tiles)))\nnum_print = 10\nprint('First {} tile descriptions are:'.format(num_print))\nfor td in tiles[:num_print]:\n    print(td)\n```\n\n----------------------------------------\n\nTITLE: Defining Test Parameter Sets with CMake\nDESCRIPTION: This snippet defines CMake variables to store test parameters for different problem sizes and configurations, including random, fixed, and small sizes, as well as epilogue operations. These parameters are later used when adding executables for CUTLASS examples. The `--iterations=0` flag disables performance benchmarking, focusing on correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_RANDOM --iterations=0)\nset(TEST_RANDOM_LARGE_GROUP --groups=500 --iterations=0)\n\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=0)\nset(TEST_EPILOGUE_LARGE_GROUP --alpha=1.5 --beta=2.0 --groups=500 --iterations=0)\n\nset(TEST_EPILOGUE_OP --beta=0.5 --iterations=0)\nset(TEST_EPILOGUE_OP_LARGE_GROUP --alpha=1.5 --iterations=0)\n\nset(TEST_FIXED --m=2048 --n=5120 --k=512 --groups=50 --iterations=0)\nset(TEST_FIXED_LARGE_GROUP --m=2048 --n=512 --k=512 --groups=512 --iterations=0)\n\nset(TEST_SMALL --m=256 --n=128 --iterations=0)\nset(TEST_SMALL_LARGE_GROUP --m=128 --n=128 --groups=500 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Creating Executable using CMake\nDESCRIPTION: This snippet creates an executable named 'cutlass_test_unit_gemm_thread_host' using the CMake function 'cutlass_test_unit_add_executable'. It takes the target name and a list of source files as input. The source files listed are 'gemm_sm60_host.cu' (CUDA) and 'testbed_host.h' (C++).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/thread/host/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_gemm_thread_host\n  gemm_sm60_host.cu\n  testbed_host.h\n  )\n```\n\n----------------------------------------\n\nTITLE: Conditional Block for SM120 Architecture in CMake\nDESCRIPTION: This CMake block conditionally includes the test unit configurations if the CUTLASS_NVCC_ARCHS variable matches '120a'. This ensures that the specific tests are only built when targeting the SM120 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 120a)\n\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm120_bs\n  DEPENDS\n  cutlass_test_unit_bs_gemm_device_tensorop_epilogue_fusion_sm120\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120_stream_k\n  cutlass_test_unit_bs_grouped_gemm_device_tensorop_sm120\n)\n\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_gemm_device_tensorop_epilogue_fusion_sm120\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm120_bs_gemm_nvf4_nvf4_f32_f32_epilogue_fusion.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_nvf4_epilogue_fusion.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_bf16_epilogue_fusion.cu\n)\n\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120\n\n  sm120_bs_gemm_nvf4_nvf4_f32_bf16.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_f16.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_f32.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_f32_narrow_output.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_epilogue.cu\n  sm120_bs_gemm_mxf4_mxf4_f32_f32.cu\n  sm120_bs_gemm_mxf6_mxf8_f32_f32.cu\n)\n\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120_stream_k\n  sm120_bs_gemm_nvf4_nvf4_f32_f32_stream_k.cu\n)\n\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_grouped_gemm_device_tensorop_sm120\n  sm120_bs_gemm_nvf4_nvf4_f32_nvf4_group_gemm_fusion.cu\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable for streamk GEMM (CMake)\nDESCRIPTION: This CMake function call creates an executable named '47_ampere_gemm_universal_streamk' using the source file 'ampere_gemm_universal_streamk.cu'. It links the necessary CUTLASS libraries and sets up build dependencies.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/47_ampere_gemm_universal_streamk/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  47_ampere_gemm_universal_streamk\n  ampere_gemm_universal_streamk.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Define CUTLASS Executable in CMake\nDESCRIPTION: This CMake function creates an executable called '02_dump_reg_shmem' from the source file 'dump_reg_shmem.cu'. The 'DISABLE_TESTS ON' option indicates that associated tests should be excluded during the build process. This is a common way to define how executables are built in a CMake-based project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/02_dump_reg_shmem/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  02_dump_reg_shmem \n  dump_reg_shmem.cu\n  DISABLE_TESTS ON\n)\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Variables for Test Parameters\nDESCRIPTION: This snippet defines CMake variables that represent different sets of test parameters. These parameters include the problem size (m, n, k), epilogue configurations (alpha, beta), group sizes, and iteration counts. The `iterations` parameter is set to 0 to disable performance benchmarking and focus on correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/75_blackwell_grouped_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_RANDOM --iterations=0)\nset(TEST_RANDOM_LARGE_GROUP --groups=50 --iterations=0)\n\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=0)\nset(TEST_EPILOGUE_LARGE_GROUP --alpha=1.5 --beta=2.0 --groups=50 --iterations=0)\n\nset(TEST_EPILOGUE_OP --beta=0.5 --iterations=1)\nset(TEST_EPILOGUE_OP_LARGE_GROUP --alpha=1.5 --iterations=1)\n\nset(TEST_FIXED --m=2048 --n=5120 --k=8192 --iterations=0)\nset(TEST_FIXED_LARGE_GROUP --m=2048 --n=512 --k=512 --groups=51 --iterations=0)\n\nset(TEST_SMALL --m=256 --n=128 --iterations=0)\nset(TEST_SMALL_LARGE_GROUP --m=128 --n=128 --groups=50 --iterations=0)\n\nset(TEST_RANDOM_PERF --iterations=10)\nset(TEST_RANDOM_PERF_LARGE_GROUP --groups=50 --iterations=10)\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License Text\nDESCRIPTION: This snippet provides the full text of the BSD-3-Clause license, detailing the rights and restrictions associated with the use, modification, and redistribution of the software.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_12\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: SM100 Sparse MMA WarpSpecialized CUDA\nDESCRIPTION: This header file defines collective mainloop that target for SM100 Sparse GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_6\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/gemm/collective/sm100_sparse_mma_warpspecialized.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Building One CUDA Core GEMM Kernel\nDESCRIPTION: These commands demonstrate how to build a specific SGEMM CUDA kernel. The `cmake` command configures the build to target NVIDIA Ampere and Turing architectures, specifying the exact kernel name using `CUTLASS_LIBRARY_KERNELS`.  The `make` command compiles the profiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sgemm_128x128_8x2_nn_align1\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Setting Performance Test Options (CMake)\nDESCRIPTION: Defines a CMake variable to set test command options for performance testing.  The `--iterations` option is set to 10, indicating that performance benchmarking should be run.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_RANDOM_PERF --iterations=10)\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for GEMM Thread Test Unit in CUTLASS with CMake\nDESCRIPTION: This snippet defines a dependency between the `test_unit_gemm_thread` executable and the `test_unit_gemm_thread_host` target.  This ensures that the host code is built before the main test executable, preventing linking errors due to missing symbols or outdated code. This is crucial for coordinating CUDA kernels with their host-side drivers and setup.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/thread/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(test_unit_gemm_thread test_unit_gemm_thread_host)\n```\n\n----------------------------------------\n\nTITLE: Configuring NVRTC Header File with CMake\nDESCRIPTION: This CMake command configures an input file 'nvrtc_config.in' to generate the output header file 'nvrtc_config.hpp'. This is likely used to define NVRTC-specific configurations for the project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/thread/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(nvrtc_config.in nvrtc_config.hpp)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target for Hopper Unit Tests in CMake\nDESCRIPTION: This CMake snippet defines a custom target named `test_unit_cute_hopper` and specifies its dependencies.  These dependencies represent individual test executables which will be built and linked together when the `test_unit_cute_hopper` target is built. This facilitates the execution of all specified Hopper-related unit tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  test_unit_cute_hopper\n  DEPENDS\n  test_unit_cute_hopper_stsm\n  test_unit_cute_hopper_tma_load\n  test_unit_cute_hopper_tma_store\n  test_unit_cute_hopper_bulk_load\n  test_unit_cute_hopper_bulk_store\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for INT4/FP8 GEMM (CMake)\nDESCRIPTION: Uses the `cutlass_example_add_executable` macro to create an executable for INT4/FP8 grouped GEMM tests. It links the source file and includes test configurations. The executable is named `69_hopper_int4_fp8_grouped_gemm`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  69_hopper_int4_fp8_grouped_gemm\n  69_hopper_int4_fp8_grouped_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n  TEST_DIRECT_BATCHED\n  TEST_SCALE_PERCOL\n  TEST_SCALE_GROUP\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Ampere Gather-Scatter Convolution (CMake)\nDESCRIPTION: This CMake snippet adds an executable target named '59_ampere_gather_scatter_conv' using the 'cutlass_example_add_executable' macro. The source file for the executable is specified as 'ampere_gather_scatter_conv.cu'. This creates an executable for a CUTLASS example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  59_ampere_gather_scatter_conv\n  ampere_gather_scatter_conv.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Epilogue Options with Large Groups (CMake)\nDESCRIPTION: Defines CMake variable for epilogue options, specifying alpha, beta and groups parameters. Iterations set to 0 for correctness checks.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_EPILOGUE_LARGE_GROUP --alpha=2.0 --beta=2.0 --groups=100 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable with Macro in CMake\nDESCRIPTION: This CMake snippet defines an executable named '64_ada_fp8_gemm_grouped' using the 'cutlass_example_add_executable' macro. The source file associated with this executable is 'ada_fp8_gemm_grouped.cu'. The macro likely handles the compilation and linking process for the CUTLASS example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/64_ada_fp8_gemm_grouped/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  64_ada_fp8_gemm_grouped\n  ada_fp8_gemm_grouped.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Conv2D Dgrad Kernel Configuration\nDESCRIPTION: This snippet defines a Conv2D data gradient (dgrad) kernel configuration. Similar to fprop, it specifies tensor layouts (hnhwc and fnhwc), kernel parameters (padding, stride, dilation), alpha/beta values, and configuration-specific numerical identifiers.  It describes how the gradient with respect to the input data is computed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_f32_sm80.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_1x11x7x8_6x4_8x1x1_pad_h0w0_stride_h2w2_dil_h1w1_corr_alpha1_beta0 hnhwc_hnhwc_fnhwc_f_f 2125023038 1152388039 2564029204 3847371704\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Sparse BlockScaled Kernel Dispatch (2SM)\nDESCRIPTION: This policy uses 2 SM `tcgen05.mma.sp` instruction and automatically selects the optimal instruction kind (mxf8f6f4, mxf4, nvf4mxf4).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\n`KernelSparseTmaWarpSpecialized2SmBlockScaledSm100`\n```\n\n----------------------------------------\n\nTITLE: Conv2d F32 Wgrad SIMT Example\nDESCRIPTION: This example demonstrates a Conv2d Wgrad with SIMT-based operation. It outlines tensor layouts, swizzle functions, padding, stride, and dilation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/customizable/README.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npython conv2d.py -i 1 1 1 -ta float32 -tb float32 -tc float32 -tacc float32 -m multiply_add -op Simt -b 128 128 8 -s 4 -w 2 4 1 -cc 80 -la TensorNHWC -aa 4 -lb TensorNHWC -ab 4 -lc TensorNHWC -ac 1 -te float32 -ep LinearCombination -sw IdentitySwizzle1 -co wgrad -st Strided -ia optimized -sm Serial -k 2 -nhwc 1 27 27 256 -krsc 512 3 3 256 -pad 1 1 1 1 -stride 2 1 -dilation 1 1 -alpha 1.0 -beta 0.0\n```\n\n----------------------------------------\n\nTITLE: Linking CUTLASS Library in CMake\nDESCRIPTION: This CMake command demonstrates how to link a target against the CUTLASS Library. It specifies that the `10_planar_complex` target should be linked privately against `cutlass_lib` and `cutlass_tools_util_includes`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(\n  10_planar_complex\n  PRIVATE\n  cutlass_lib\n  cutlass_tools_util_includes\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for General sm100 Sparse GEMM Tests - CMake\nDESCRIPTION: This code uses the `cutlass_test_unit_gemm_device_add_executable_split_file` CMake function to create an executable for general sm100 sparse GEMM tests.  It specifies source files and sets `BATCH_SOURCES` to `ON` with `BATCH_SIZE 1` to disable source batching to control compiler memory usage.  The specified .cu files contain the kernels for different data types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_general\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_sp_gemm_s8_s8_s32_s8_s8_imma.cu\n  sm100_sp_gemm_f8_f8_f32_f16_f8_qmma.cu\n  sm100_sp_gemm_f8_f8_f32_f16_f16_qmma.cu\n  sm100_sp_gemm_f8_f8_f32_f32_f32_qmma.cu\n  sm100_sp_gemm_f32_f32_f32_f32_f32_tfmma.cu\n  sm100_sp_gemm_f16_f16_f32_f16_f16_hmma.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Install cutlass_library package\nDESCRIPTION: This command installs the cutlass_library package in development mode. It enables users to make changes to the library and test them without reinstalling.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython setup_library.py develop --user\n```\n\n----------------------------------------\n\nTITLE: Combining Layouts with append, prepend, replace in CuTe (C++)\nDESCRIPTION: This snippet demonstrates combining layouts using `append`, `prepend`, and `replace`. It shows how to add layouts to the end, beginning, or replace existing parts of a layout. Layouts `a` and `b` are defined as `Layout<_3,_1>` and `Layout<_4,_3>` respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a = Layout<_3,_1>{};                     // 3:1\nLayout b = Layout<_4,_3>{};                     // 4:3\nLayout ab = append(a, b);                       // (3,4):(1,3)\nLayout ba = prepend(a, b);                      // (4,3):(3,1)\nLayout c  = append(ab, ab);                     // (3,4,(3,4)):(1,3,(1,3))\nLayout d  = replace<2>(c, b);                   // (3,4,4):(1,3,3)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Unit Test Executable in CMake\nDESCRIPTION: This CMake function defines the executable 'cutlass_test_unit_cute_core' and specifies the C++ source files that should be compiled and linked into it.  The WITHOUT_CUDA flag indicates that the executable is not using CUDA.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/core/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_core\n  WITHOUT_CUDA\n  array_subbyte.cpp\n  bitfield.cpp\n  coalesce.cpp\n  compact_xmajor.cpp\n  compare.cpp\n  complement.cpp\n  composition.cpp\n  constants.cpp\n  core_unit.cpp\n  domain_distribute.cpp\n  int_tuple.cpp\n  inverse_left.cpp\n  inverse_right.cpp\n  logical_divide.cpp\n  logical_product.cpp\n  math.cpp\n  mixedbits.cpp\n  nullspace.cpp\n  pointer.cpp\n  reverse.cpp\n  swizzle_layout.cpp\n  transform.cpp\n  tuple.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: BLayout for NT Transpose (T8, V4) -> (n, k)\nDESCRIPTION: This code defines the BLayout for the NT transpose case, mapping 8 threads each owning 4 elements to (n, k) coordinates. With the (N, K) ordering, the layout is the same as the ALayout for NT. It uses `Shape` and `Stride` from the CuTe library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n  // (T8,V4) -> (n,k)\n  using BLayout = Layout<Shape <Shape <_4,_2>,_4>,\n                         Stride<Stride<_8,_4>,_1>>;\n```\n\n----------------------------------------\n\nTITLE: CMake Build Command (Command Line)\nDESCRIPTION: This command builds the CUTLASS project using CMake on the command line. The `--build` option specifies the build directory, `--config` sets the build configuration (Release or Debug), and `-j 4` defines the maximum build parallelism.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/build/building_in_windows_with_visual_studio.md#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake --build . --config Release -j 4\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (nvf4_nvf4_f32_f16_f16_o)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f16_f16_o` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f16_f16_o\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f16_f16_o_tnt.cu\n  sm100_bssp_gemm_nvf4_nvf4_f32_void_f16_o_tnt.cu\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f16_f16_o_tnn.cu\n  sm100_bssp_gemm_nvf4_nvf4_f32_void_f16_o_tnn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Static Assertions for Thread Layouts (C++)\nDESCRIPTION: These static assertions check the validity of the thread layouts. They ensure that the layout is static, the number of threads is consistent across layouts, and the block dimensions are divisible by the thread dimensions. These checks are performed at compile time to catch errors early.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n  static_assert(is_static<CThreadLayout>::value);\n\n  CUTE_STATIC_ASSERT_V(size(tC) == size(tA));                          // NumThreads\n\n  CUTE_STATIC_ASSERT_V(size<0>(cta_tiler) % size<0>(tC) == Int<0>{});  // BLK_M / THR_M\n  CUTE_STATIC_ASSERT_V(size<1>(cta_tiler) % size<1>(tC) == Int<0>{});  // BLK_N / THR_N\n```\n\n----------------------------------------\n\nTITLE: Logical Product Implementation in CuTe\nDESCRIPTION: This code snippet presents the implementation of the `logical_product` function. This function creates a two-mode layout from two input layouts. It requires the CuTe library for layouts, shapes, strides, and composition.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <class LShape, class LStride,\n          class TShape, class TStride>\nauto logical_product(Layout<LShape,LStride> const& layout,\n                     Layout<TShape,TStride> const& tiler)\n{\n  return make_layout(layout, composition(complement(layout, size(layout)*cosize(tiler)), tiler));\n}\n```\n\n----------------------------------------\n\nTITLE: Linking CUTLASS Test Unit Library (Infra) Dependencies\nDESCRIPTION: This snippet links the 'cutlass_test_unit_infra' library with its dependencies. It includes public dependencies like `CUTLASS`, `cutlass_tools_util_includes`, `GTest::gtest`, `cudart`, and `cuda_driver`. It conditionally links `nvidia::cublas` if `CUTLASS_ENABLE_CUBLAS` is enabled. These dependencies are required for the library to function correctly.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(\n  cutlass_test_unit_infra\n  PUBLIC\n  CUTLASS\n  cutlass_tools_util_includes\n  $<$<BOOL:${CUTLASS_ENABLE_CUBLAS}>:nvidia::cublas>\n  GTest::gtest\n  cudart\n  cuda_driver\n  )\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Hopper GPU\nDESCRIPTION: Configures the build system to compile CUTLASS for the NVIDIA Hopper GPU architecture.  Uses CMake to generate the build files.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a            # compiles for NVIDIA Hopper GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running a Specific Tile Configuration\nDESCRIPTION: This snippet picks a random tile configuration from the available tile descriptions, compiles the CUTLASS GEMM plan using that configuration, and then runs the GEMM operation with the selected tile configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/00_basic_gemm.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nidx = random.randint(0, len(tiles)-1)\ntd = tiles[idx]\nprint('Tile description {} is: {}'.format(idx, td))\nplan.compile(td)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, alpha, beta, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS INT4 FP8 GEMM Example\nDESCRIPTION: This CMake code adds an executable target for a CUTLASS example that tests int4-fp8 GEMM on the Hopper architecture. It links the source file `55_hopper_int4_fp8_gemm.cu` and includes various test configurations defined earlier, such as direct batched conversion and different scaling modes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/55_hopper_mixed_dtype_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  55_hopper_int4_fp8_gemm\n  55_hopper_int4_fp8_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_DIRECT_BATCHED\n  TEST_SCALE_PERCOL\n  TEST_SCALE_GROUP\n  TEST_SCALE_RESIDUE\n  # TEST_ALPHA_BETA\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Full CLayout for GMMA (T128,V4) -> (M64,N8) in C++\nDESCRIPTION: This code snippet defines the full CLayout for a 64x8 accumulator in GMMA. It includes the shape and stride for both thread and value IDs, representing the mapping (T128, V4) -> (M64, N8).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n// (T128,V4) -> (M64,N8)\nusing CLayout = Layout<Shape <Shape <  _4, _8,  _4>, Shape < _2, _2>>,\n                       Stride<Stride<_128, _1, _16>, Stride<_64, _8>>>;\n```\n\n----------------------------------------\n\nTITLE: Defining Coord Template Class C++\nDESCRIPTION: The `Coord` template class is a container for defining logical coordinates in tensors. It is parameterized by rank and index type and supports vector operations like addition, subtraction, and scalar multiplication.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <\n  int Rank,\n  typename Index = int\n>\nclass Coord;\n```\n\n----------------------------------------\n\nTITLE: Vector Operations on Coord C++\nDESCRIPTION: This example demonstrates vector operations on `Coord` objects, specifically calculating an offset using addition and scalar multiplication. It initializes a `Coord` object representing a stride and then computes a new coordinate based on the base coordinate and thread indices.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nCoord<2> compute_offset(Coord<2> const & base) {\n  \n  Coord<2> stride = make_Coord(1, kM);\n\n  return base + stride * make_Coord(threadIdx.x, threadIdx.y); \n}\n```\n\n----------------------------------------\n\nTITLE: Outputting Profiler Results to CSV (CUTLASS)\nDESCRIPTION: This command demonstrates how to output the profiler results to a CSV file. The `--output=<filename.csv>` option specifies the file where the comma-separated values will be written.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/profiler.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sgemm_128x128_nn            \\\n                                    --m=3456 --n=4096 --k=8:4096:8 --output=report.csv\n```\n\n----------------------------------------\n\nTITLE: Adding Executables Conditionally Based on Architecture\nDESCRIPTION: This snippet conditionally adds two executables, `75_blackwell_grouped_gemm` and `75_blackwell_grouped_gemm_block_scaled`, if the `CUTLASS_NVCC_ARCHS` variable matches `100a`. The `cutlass_example_add_executable` function is used to create the executables, linking them with the specified source files and test parameter sets defined earlier.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/75_blackwell_grouped_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  75_blackwell_grouped_gemm\n  75_blackwell_grouped_gemm.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n  )\n\ncutlass_example_add_executable(\n  75_blackwell_grouped_gemm_block_scaled\n  75_blackwell_grouped_gemm_block_scaled.cu\n  TEST_COMMAND_OPTIONS\n  TEST_RANDOM\n  TEST_RANDOM_LARGE_GROUP\n  TEST_EPILOGUE\n  TEST_EPILOGUE_LARGE_GROUP\n  TEST_EPILOGUE_OP\n  TEST_EPILOGUE_OP_LARGE_GROUP\n  TEST_FIXED\n  TEST_FIXED_LARGE_GROUP\n  TEST_SMALL\n  TEST_SMALL_LARGE_GROUP\n  TEST_RANDOM_PERF\n  TEST_RANDOM_PERF_LARGE_GROUP\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Layout Composition with Dynamic Shapes and Strides in CuTe (C++)\nDESCRIPTION: This C++ code snippet shows layout composition in CuTe using dynamic shapes and strides. It defines two layouts, `a` and `b`, and calculates their composition `c`. The resulting layout `c` is printed, showing the reshaping effect. The expected output is `((5,1),(2,2)):((16,4),(80,4))`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a = make_layout(make_shape (10, 2),\n                       make_stride(16, 4));\nLayout b = make_layout(make_shape ( 5, 4),\n                       make_stride( 1, 5));\nLayout c = composition(a, b);\nprint(c);\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Target in CMake\nDESCRIPTION: This CMake code defines a custom target named 'cutlass_test_unit_gemm_device_sm120_bs'.  It depends on other defined executables, ensuring that those executables are built before this target. This pattern is used to create aggregated targets.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm120_bs\n  DEPENDS\n  cutlass_test_unit_bs_gemm_device_tensorop_epilogue_fusion_sm120\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120_stream_k\n  cutlass_test_unit_bs_grouped_gemm_device_tensorop_sm120\n)\n```\n\n----------------------------------------\n\nTITLE: Install Sphinx Documentation Dependencies\nDESCRIPTION: These commands install the necessary packages for building the CUTLASS Python interface documentation using Sphinx.  Pandoc is also required.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install pandoc\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade Sphinx furo pandoc myst-parser sphinx-copybutton nbsphinx nbsphinx-link sphinx-inline-tabs\n```\n\n----------------------------------------\n\nTITLE: Adding Executable using cutlass_example_add_executable\nDESCRIPTION: This snippet uses the 'cutlass_example_add_executable' macro to define an executable named '31_basic_syrk'. The source file 'basic_syrk.cu' is specified as the input for building this executable. This macro likely simplifies the process of adding executables within the CUTLASS build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/31_basic_syrk/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  31_basic_syrk\n  basic_syrk.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Target Definition (SM120)\nDESCRIPTION: This CMake code block conditionally defines a custom target `cutlass_test_unit_gemm_device_sm120_tensorop` if the `CUTLASS_NVCC_ARCHS` variable matches `120a`. This target depends on two other targets that create executables for testing f32 and f16 GEMM tensor operations on the SM120 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 120a)\n\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm120_tensorop\n  DEPENDS\n  cutlass_test_unit_gemm_device_tensorop_f32_sm120\n  cutlass_test_unit_gemm_device_tensorop_f16_sm120\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies Based on CUDA Architecture (sm80+)\nDESCRIPTION: This snippet conditionally adds dependencies to the convolution test targets based on whether the maximum CUDA architecture is greater than or equal to 80 (sm80). If the condition is met, dependencies to the tensorop_f16_sm80, tensorop_f32_sm80, and tensorop_f32_tf32_sm80 implementations are added.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n\n  add_dependencies(\n    cutlass_test_unit_conv_device\n    cutlass_test_unit_conv_device_tensorop_f16_sm80\n    cutlass_test_unit_conv_device_tensorop_f32_sm80\n    cutlass_test_unit_conv_device_tensorop_f32_tf32_sm80\n  )\n\n  add_dependencies(\n    test_unit_conv_device\n    test_unit_conv_device_tensorop_f16_sm80\n    test_unit_conv_device_tensorop_f32_sm80\n    test_unit_conv_device_tensorop_f32_tf32_sm80\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Data Types and Cluster Shapes for FP8 GEMM\nDESCRIPTION: This code snippet defines the data types for the A, B, and C matrices, the accumulator, and the compute element for an FP8 GEMM kernel in CUTLASS. It also specifies the layout for each of the matrices and defines the MMA (Matrix Multiply Accumulate) tile shape and cluster shape.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_37\n\nLANGUAGE: c++\nCODE:\n```\nusing LayoutA = cutlass::layout::RowMajor;\nusing LayoutB = cutlass::layout::ColumnMajor;\nusing LayoutC = cutlass::layout::ColumnMajor;\nusing ElementA = cutlass::float_e4m3_t;\nusing ElementB = cutlass::float_e4m3_t;\nusing ElementC = cutlass::float_e4m3_t;\nusing ElementD = cutlass::float_e4m3_t;\nusing ElementAccumulator = float;\nusing ElementCompute = float;\nusing ElementBias = cutlass::half_t;\nusing MmaTileShape = cute::Shape<_128,_64,Int<128 / sizeof(ElementA)>>;\nusing ClusterShape = cute::Shape<_1,_1,_1>;\n```\n\n----------------------------------------\n\nTITLE: Setting Source File Properties in CMake\nDESCRIPTION: This snippet uses the `set_property` command in CMake to set compilation flags for specific CUDA source files. It applies \"--use_fast_math -ftemplate-backtrace-limit=0\" to CUDA source files related to Blackwell FMHA and MLA implementations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/77_blackwell_fmha/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(\n  SOURCE\n      77_blackwell_fmha.cu\n      77_blackwell_fmha_gen.cu\n      77_blackwell_mla.cu\n      77_blackwell_fmha_bwd.cu\n  PROPERTY\n      COMPILE_FLAGS \"--use_fast_math -ftemplate-backtrace-limit=0\"\n)\n```\n\n----------------------------------------\n\nTITLE: Expanding TiledMMA to 32x32x4 with 2x2 Atom Layout in C++\nDESCRIPTION: This code expands the previous TiledMMA's tile size to 32x32x4 while maintaining the 2x2 atom layout. This replicates the previous TiledMMA across values instead of threads, resulting in a larger data partition.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\nTiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                                  Layout<Shape <_2,_2>,\n                                         Stride<_2,_1>>{},  // 2x2 n-major layout of Atoms\n                                  Tile<_32,_32,_4>{});      // 32x32x4 tiler\n    print_latex(mma);\n```\n\n----------------------------------------\n\nTITLE: Excluding build for MSVC (CMake)\nDESCRIPTION: This CMake snippet uses an if statement to prevent the execution of the subsequent commands if the MSVC compiler is being used.  This likely means that the enclosed source code is not compatible with MSVC.  The 'endif()' statement closes the block.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT MSVC)\n\ncutlass_example_add_executable(\n  59_ampere_gather_scatter_conv\n  ampere_gather_scatter_conv.cu\n)\n\nif (CUTLASS_ENABLE_OPENMP_TESTS AND OpenMP_CXX_FOUND)\n  target_link_libraries(59_ampere_gather_scatter_conv PRIVATE OpenMP::OpenMP_CXX)\nendif()\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Test Swizzle Options in CMake\nDESCRIPTION: This snippet sets CMake variables to define test swizzle options for CUTLASS examples. These variables are later used when adding executables to specify different swizzle configurations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/70_blackwell_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_SWIZZLE_1 --swizzle=1)\nset(TEST_SWIZZLE_2 --swizzle=2)\nset(TEST_SWIZZLE_5 --swizzle=5)\nset(TEST_SWIZZLE_5_UNEVEN --swizzle=5 --m=4096 --n=16384)\n```\n\n----------------------------------------\n\nTITLE: Inverse GEMM K Dimension Mapping\nDESCRIPTION: These equations map the GEMM K dimension back to the linearized CRS indices, allowing translation between matrix operations and the original convolutional operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nc = gemm_k / (RS)\nresidual = gemm_k % (RS)\n\nr = residual / S\ns = residual % S\n```\n\n----------------------------------------\n\nTITLE: Checking if a Type is an Integer\nDESCRIPTION: This snippet demonstrates how to use CuTe traits to check whether a given type `T` is a static or dynamic integer. It highlights the use of `cute::is_integral<T>`, `cute::is_std_integral<T>`, `cute::is_static<T>`, and `cute::is_constant<N,T>` traits. These traits are essential for working with integers in CuTe, allowing for generic handling of both compile-time and run-time integer values.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncute::is_integral<T>\ncute::is_std_integral<T>\ncute::is_static<T>\ncute::is_constant<N,T>\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Targets in CMake\nDESCRIPTION: This snippet defines custom build targets 'cutlass_test_unit_transform' and 'test_unit_transform' using the 'add_custom_target' command in CMake. These targets depend on other targets related to threadblock and kernel transformations, ensuring that those dependencies are built before the custom targets.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_transform\n  DEPENDS\n  cutlass_test_unit_transform_threadblock\n  cutlass_test_unit_transform_kernel\n)\n\nadd_custom_target(\n  test_unit_transform\n  DEPENDS\n  test_unit_transform_threadblock\n  test_unit_transform_kernel\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: Specifies the include directories for the 'cutlass_tools_util_includes' library. It defines both install and build interface paths, using generator expressions to determine the correct path based on the build environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/util/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(\n  cutlass_tools_util_includes\n  INTERFACE\n  $<INSTALL_INTERFACE:include>\n  $<BUILD_INTERFACE:${CUTLASS_TOOLS_UTIL_INCLUDE_DIR}>\n  )\n```\n\n----------------------------------------\n\nTITLE: Mainloop Dispatch Policy Example in C++\nDESCRIPTION: Illustrates an example of a dispatch policy for the `CollectiveMma` class, specifically tailored for Hopper TMA warp-specialized mainloop implementations. The template parameters define the number of pipeline stages, cluster shape, architecture tag, and kernel schedule.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// n-buffer in smem (Hopper TMA),\n// pipelined with Hopper GMMA and TMA,\n// warp-specialized dynamic schedule\ntemplate<\n  int Stages_,\n  class ClusterShape_ = Shape<_1,_1,_1>,\n  class KernelSchedule = KernelTmaWarpSpecializedCooperative\n>\nstruct MainloopSm90TmaGmmaWarpSpecialized {\n  constexpr static int Stages = Stages_;\n  using ClusterShape = ClusterShape_;\n  using ArchTag = arch::Sm90;\n  using Schedule = KernelSchedule;\n};\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable using CMake\nDESCRIPTION: This CMake code defines an executable named '29_3xtf32_complex_gemm' using the 'cutlass_example_add_executable' function. The executable is built from the source file '29_3xtf32_complex_gemm.cu'. This is part of the CUTLASS library's example build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  29_3xtf32_complex_gemm\n  29_3xtf32_complex_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Targets for CUTLASS Examples (CMake)\nDESCRIPTION: Defines two custom targets: `cutlass_examples` and `test_examples`. These targets can be used to trigger the build of all examples and their associated tests, respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(cutlass_examples)\nadd_custom_target(test_examples)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target for FMHA Backward Pass in CMake\nDESCRIPTION: This code snippet uses the `cutlass_example_add_executable` macro to create an executable target for the fused multi-head attention backward pass example. It also disables tests for this target.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/41_fused_multi_head_attention/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  41_fused_multi_head_attention_backward\n  fused_multi_head_attention_backward.cu\n  DISABLE_TESTS ON\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Epilogue Operation Options (CMake)\nDESCRIPTION: Defines CMake variable for epilogue operations by specifying beta value, and setting iterations to 1.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_EPILOGUE_OP --beta=0.5 --iterations=1)\n```\n\n----------------------------------------\n\nTITLE: Defining sgemm_sm70 Executable (CMake/CUDA)\nDESCRIPTION: This command uses a custom CMake macro `cutlass_example_add_executable` to define an executable named 'cute_tutorial_sgemm_sm70'. The source file for this executable is 'sgemm_sm70.cu', indicating a CUDA implementation targeting SM70 architecture. It creates an executable target within the Cutlass build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_sgemm_sm70\n  sgemm_sm70.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Executable with Split File Support\nDESCRIPTION: This function, `cutlass_test_unit_add_executable_split_file`, extends `cutlass_test_unit_add_executable` by splitting the input source files if `CUTLASS_UNIT_TEST_SPLIT_FILES` is enabled. This helps to reduce memory consumption during builds. If splitting is not enabled, it simply calls `cutlass_test_unit_add_executable` with the provided arguments.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(cutlass_test_unit_add_executable_split_file NAME)\n  # Given the input arguments to cutlass_test_unit_add_executable, creates\n  # a new set of arguments in which each file has at most one TEST definition,\n  # and calls cutlass_test_unit_add_executable with the newly-formed arguments.\n  # The goal of this is to reduce the memory consumed while building CUTLASS\n  # tests with a high degree of parallelism while not requiring developers\n  # to split unit tests across multiple files artificially.\n\n  # Get all arguments other than the NAME of the target\n  list(SUBLIST ARGV 1 ${ARGC} SUBARGV)\n\n  if (CUTLASS_UNIT_TEST_SPLIT_FILES)\n    execute_process(\n      WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n      COMMAND ${Python3_EXECUTABLE} ${CUTLASS_SOURCE_DIR}/tools/util/scripts/split_test_cmake.py\n        ${NAME}\n        ${CMAKE_CURRENT_SOURCE_DIR}\n        --src_files ${SUBARGV}\n        --dst_dir ${CMAKE_CURRENT_BINARY_DIR}\n      RESULT_VARIABLE cutlass_test_SPLIT_RESULT\n      OUTPUT_VARIABLE cutlass_test_SPLIT_OUTPUT\n      OUTPUT_FILE ${CMAKE_CURRENT_BINARY_DIR}/test_split_files.txt\n      ERROR_FILE ${CMAKE_CURRENT_BINARY_DIR}/test_split_error.log\n    )\n\n    if(NOT cutlass_test_SPLIT_RESULT EQUAL 0)\n      message(FATAL_ERROR \"Error splitting unit test. See ${CMAKE_CURRENT_BINARY_DIR}/test_split_error.log\")\n    endif()\n\n    # Forward the values printed by split_test_cmake.py as arguments to cutlass_test_unit_add_executable.\n    # We additionally specify to add -I${CMAKE_CURRENT_SOURCE_DIR} to the target. This is necessary because\n    # the splitting process writes new files to ${CMAKE_CURRENT_BINARY_DIR}, but many CUTLASS unit tests\n    # use relative imports for including testbeds (e.g., '#include \\\"../testbed.hpp\\\"'). These headers are\n    # not written to ${CMAKE_CURRENT_BINARY_DIR} during the splitting process, so we must indicate that\n    # headers can also be searched for from ${CMAKE_CURRENT_SOURCE_DIR}.\n    file(STRINGS ${CMAKE_CURRENT_BINARY_DIR}/test_split_files.txt NEW_OPTIONS)\n    cutlass_test_unit_add_executable(${NAME} ${NEW_OPTIONS} EXTRA_INCLUDE_DIRS ${CMAKE_CURRENT_SOURCE_DIR})\n  else()\n    # Simply pass arguments through\n    cutlass_test_unit_add_executable(${ARGV})\n  endif()\nendfunction() \n```\n\n----------------------------------------\n\nTITLE: Setting CUTLASS Examples Source Directory (CMake)\nDESCRIPTION: Sets the variable `CUTLASS_EXAMPLES_COMMON_SOURCE_DIR` to the path of the common source directory for CUTLASS examples. This variable is later used to include common headers and source files.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(CUTLASS_EXAMPLES_COMMON_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/common)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation of CUTLASS Example (CMake)\nDESCRIPTION: This snippet checks if the C++ compiler is GNU GCC and if its version is less than 8.0. If the conditions are not met, it adds an executable named '62_hopper_sparse_gemm' built from '62_hopper_sparse_gemm.cu'. This is a workaround for a known issue in older GCC versions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/62_hopper_sparse_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 8.0))\ncutlass_example_add_executable(\n  62_hopper_sparse_gemm\n  62_hopper_sparse_gemm.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initialize GEMM Matrices\nDESCRIPTION: This utility function initializes A, B, C, and D matrices with random integer values within a specific range. The matrices are allocated on the CUDA device and converted to the specified data type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/02_pytorch_extension_grouped_gemm.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport random\nrandom.seed(2023)\n\n# Utility function to initialize A, B, C, and D matrices corresponding to dimensions M, N, and K\ndef initialize(dtype, M, N, K):\n    sizes = [(M, K), (K, N), (M, N), (M, N)]\n    return [torch.randint(-3, 3, size, device='cuda').to(dtype) for size in sizes]\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Targets (CMake)\nDESCRIPTION: Adds custom targets `cutlass_test_unit_nvrtc` and `test_unit_nvrtc` that depend on `cutlass_test_unit_nvrtc_thread` and `test_unit_nvrtc_thread`, respectively.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(cutlass_test_unit_nvrtc DEPENDS cutlass_test_unit_nvrtc_thread)\nadd_custom_target(test_unit_nvrtc DEPENDS test_unit_nvrtc_thread)\n```\n\n----------------------------------------\n\nTITLE: Adding Profiler Subdirectory (Conditional with Dependency Check)\nDESCRIPTION: This conditional block adds the 'profiler' subdirectory if `CUTLASS_ENABLE_PROFILER` is true, but only if `CUTLASS_ENABLE_LIBRARY` is also true. It checks for this dependency and issues an error message if the profiler is enabled without the library. The profiler depends on the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_ENABLE_PROFILER)\n  if (NOT CUTLASS_ENABLE_LIBRARY)\n    message(SEND_ERROR \"Build conflict: The CUTLASS profiler requires the CUTLASS library.\")\n    message(SEND_ERROR \"  CUTLASS_ENABLE_PROFILER = ${CUTLASS_ENABLE_PROFILER}\")\n    message(SEND_ERROR \"  CUTLASS_ENABLE_LIBRARY = ${CUTLASS_ENABLE_LIBRARY}\")\n  else()\n    add_subdirectory(profiler)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for CUTLASS Profiler in CMake\nDESCRIPTION: Specifies the include directories required for compiling the CUTLASS profiler.  It adds the current list directory and the `include` subdirectory as private include paths.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(\n  cutlass_profiler\n  PRIVATE\n  ${CMAKE_CURRENT_LIST_DIR}/include\n  )\n```\n\n----------------------------------------\n\nTITLE: Generating CUTLASS Library Instances with Python\nDESCRIPTION: This snippet uses CMake's `execute_process` to run a Python script that generates CUTLASS library instances.  It sets environment variables, passes command-line arguments to the Python script to configure the generation process. The script uses the `generator.py` script to generate the library instances based on the specified parameters. The generated outputs are logged to a file, and the script checks for errors during execution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(\n  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/../../python/cutlass_library\n  COMMAND ${CMAKE_COMMAND} -E env PYTHONPATH=${CUTLASS_LIBRARY_PACKAGE_DIR}\n    ${Python3_EXECUTABLE} ${CUTLASS_SOURCE_DIR}/python/cutlass_library/generator.py\n    --operations \"${CUTLASS_LIBRARY_OPERATIONS}\" \n    --build-dir ${PROJECT_BINARY_DIR}\n    --curr-build-dir ${CMAKE_CURRENT_BINARY_DIR}\n    --generator-target library\n    --architectures \"${CUTLASS_NVCC_ARCHS_ENABLED}\"\n    --kernels \"${CUTLASS_LIBRARY_KERNELS}\"\n    --instantiation-level \"${CUTLASS_LIBRARY_INSTANTIATION_LEVEL}\"\n    --ignore-kernels \"${CUTLASS_LIBRARY_IGNORE_KERNELS}\"\n    --exclude-kernels \"${CUTLASS_LIBRARY_EXCLUDE_KERNELS}\"\n    --kernel-filter-file \"${KERNEL_FILTER_FILE}\"\n    --selected-kernel-list \"${CUTLASS_LIBRARY_GENERATED_KERNEL_LIST_FILE}\"\n    --cuda-version \"${CUTLASS_GENERATOR_CUDA_COMPILER_VERSION}\"\n    --log-level INFO\n    --disable-cutlass-package-imports\n  RESULT_VARIABLE cutlass_lib_INSTANCE_GENERATION_RESULT\n  OUTPUT_VARIABLE cutlass_lib_INSTANCE_GENERATION_OUTPUT\n  OUTPUT_FILE ${CMAKE_CURRENT_BINARY_DIR}/library_instance_generation.log\n  ERROR_FILE ${CMAKE_CURRENT_BINARY_DIR}/library_instance_generation.log\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf6_mxf6_f32_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf6_mxf6_f32_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf6_mxf6_f32_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf6_mxf6_f32_f16_f16_q_tnt.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Executables for Fused GEMM Examples with CMake\nDESCRIPTION: This loop iterates through a list of fused GEMM examples and creates an executable for each. The cutlass_example_add_executable macro is used to create the executable, and add_dependencies ensures that the 13_fused_two_gemms target depends on each of the individual example executables. This allows for building all fused GEMM examples using the main target.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/13_two_tensor_op_fusion/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(FUSION_GEMM_EXAMPLE\n  fused_two_gemms_f16_sm75_rf\n  fused_two_gemms_f16_sm75_shmem\n  fused_two_gemms_grouped_f16_sm80_rf\n  fused_two_gemms_f16_sm80_rf\n  fused_two_gemms_f16_sm80_shmem\n  fused_two_gemms_s8_sm75_rf\n  fused_two_gemms_s8_sm75_shmem\n  fused_two_gemms_s8_sm80_rf\n  fused_two_gemms_s8_sm80_shmem\n)\n  cutlass_example_add_executable(\n    13_${FUSION_GEMM_EXAMPLE}\n    ${FUSION_GEMM_EXAMPLE}.cu\n  )\n\n  add_dependencies(13_fused_two_gemms 13_${FUSION_GEMM_EXAMPLE})\n\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies Based on CUDA Architecture (sm70+)\nDESCRIPTION: This snippet conditionally adds dependencies to the convolution test targets based on whether the maximum CUDA architecture is greater than or equal to 70 (sm70). If the condition is met, dependencies to the tensorop_f32_sm70 implementations are added.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 70)\n\n  add_dependencies(\n    cutlass_test_unit_conv_device\n    cutlass_test_unit_conv_device_tensorop_f32_sm70\n  )\n\n  add_dependencies(\n    test_unit_conv_device\n    test_unit_conv_device_tensorop_f32_sm70\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Function call formatting with a long function name in C++\nDESCRIPTION: Illustrates formatting a function or function object call with a long name in CUTLASS. The line is broken right after the invoking open parenthesis, and parameters are indented to improve readability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ndetail::very_long_function_object_name<TemplateArgument>{}(\n  params.long_parameter_name, some_operator.another_long_function_name());\n\ndetail::an_even_longer_function_object_name<TemplateArgument1, TemplateArgument2>{}(\n  params.long_parameter_name, some_operator.long_member_function_name(),\n  another_operator.another_long_member_function_name(x, y, z));\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Profiler Executable Target in CMake\nDESCRIPTION: Creates an executable target named `cutlass_profiler` using the source files defined in `CUTLASS_TOOLS_PROFILER_SOURCES`. It also creates an alias and sets the export name.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_add_executable(\n  cutlass_profiler\n  ${CUTLASS_TOOLS_PROFILER_SOURCES}\n)\nadd_executable(nvidia::cutlass::profiler ALIAS cutlass_profiler)\nset_target_properties(cutlass_profiler PROPERTIES EXPORT_NAME profiler)\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory\nDESCRIPTION: Creates a build directory and navigates into it. This is standard practice for out-of-source builds with CMake.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: Defining Another Custom Target with Dependencies in CMake\nDESCRIPTION: This snippet demonstrates how to define another custom target in CMake with specified dependencies. The `add_custom_target` command is used to create a target named `test_unit_gemm`, which depends on other targets (`test_unit_gemm_thread`, `test_unit_gemm_warp`, `test_unit_gemm_threadblock`, `test_unit_gemm_device`).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  test_unit_gemm\n  DEPENDS\n  test_unit_gemm_thread\n  test_unit_gemm_warp\n  test_unit_gemm_threadblock\n  test_unit_gemm_device\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f16 Sparse GEMM Compressor Test\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_add_executable` function to create an executable named `cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f16`. The executable is built from the source file `sm90_sparse_gemm_compressor_f16.cu`, which likely contains the test implementation for the sparse GEMM compressor with f16 precision.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/device/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n    cutlass_test_unit_sm90_structured_sparse_gemm_compressor_f16\n\n    sm90_sparse_gemm_compressor_f16.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Column Calculation in Rank2K Scheduler (C++)\nDESCRIPTION: This snippet shows how to calculate the column index `j` given a threadblock ID `t` and row index `i` for a lower-triangular matrix in the Rank2K scheduler.  This is necessary for mapping threadblock IDs to tile coordinates. The formula adjusts for zero-based indexing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nj = t - (i(i+1)/2)\n```\n\n----------------------------------------\n\nTITLE: Define data types for HMMA NT operation\nDESCRIPTION: Specifies the data types used in the HMMA NT operation for the D, A, B, and C matrices. `ValTypeD` and `ValTypeC` are defined as `float`, while `ValTypeA` and `ValTypeB` are defined as `half_t`. These type definitions are used in the `MMA_Traits` struct to describe the logical compute types of the matrices.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nusing ValTypeD = float;\nusing ValTypeA = half_t;\nusing ValTypeB = half_t;\nusing ValTypeC = float;\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_4x2x3x256_1x1_328x3x5_pad_h1w1_stride_h1w1_dil_h1w1_corr_alpha1_beta0 s4nhwc_s4nhwc_inhwc_i_f 1104335117 3889837239 961406695 372399325\n```\n\n----------------------------------------\n\nTITLE: Defining Small Problem Sizes with Large Groups (CMake)\nDESCRIPTION: Defines CMake variables for small problem sizes and large groups, specifying matrix dimensions (m, n), groups and setting iterations to 0 for correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_SMALL_LARGE_GROUP --m=128 --n=128 --groups=100 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Collective Epilogue for FP8 GEMM\nDESCRIPTION: This snippet demonstrates how to instantiate the collective epilogue for a Blackwell SM100 FP8 GEMM kernel. It defines the epilogue schedule, fusion operation, and then uses the `CollectiveBuilder` to create the `CollectiveOp` type, specifying architecture, operation class, tile shapes, data types, layouts, and schedule.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_38\n\nLANGUAGE: c++\nCODE:\n```\nusing EpilogueSchedule = cutlass::epilogue::TmaWarpSpecialized1Sm;\n\nusing FusionOperation = cutlass::epilogue::fusion::LinearCombination<\n  ElementD,\n  ElementCompute,\n  ElementC\n>;\n\nusing CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<\n    cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp,\n    MmaTileShape, ClusterShape,\n    cutlass::epilogue::collective::EpilogueTileAuto,\n    ElementAccumulator, ElementCompute,\n    ElementC, LayoutC, 16 / sizeof(ElementC),\n    ElementD, LayoutC, 16 / sizeof(ElementD),\n    EpilogueSchedule,\n    FusionOperation\n  >::CollectiveOp;\n```\n\n----------------------------------------\n\nTITLE: Building All GEMM/Convolution Kernels\nDESCRIPTION: These commands demonstrate how to build all supported CUTLASS templates, resulting in a large number of kernels and potentially long build times. The first command uses `cmake` to configure the build, specifying the CUDA architecture and setting `CUTLASS_LIBRARY_KERNELS` to `all`. The second command then builds the `cutlass_profiler` using `make`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_KERNELS=all\n...\n$ make cutlass_profiler -j16\n```\n\n----------------------------------------\n\nTITLE: Generating CUDA Source Files in CMake\nDESCRIPTION: Iterates through the `header_files_to_check` list, creating a corresponding CUDA (.cu) file for each header. The path of each header is escaped by replacing '/' with '%', and a CUDA file is created in the build directory that includes the respective header file. These generated CUDA source files are then added to a list named `_gen_source_files`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/self_contained_includes/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(_gen_source_files \"\")\nforeach(header_file ${header_files_to_check})\n  string(REPLACE \"/\" \"%\" header_file_esc ${header_file})\n\n  file(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/${header_file_esc}.cu\"\n       \"#include <${header_file}>\")\n\n  list(APPEND _gen_source_files\n       \"${CMAKE_CURRENT_BINARY_DIR}/${header_file_esc}.cu\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet demonstrates how to include subdirectories within a CMake project. The `add_subdirectory` command is used to add the specified directories (thread, warp, threadblock, device) to the build process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thread)\nadd_subdirectory(warp)\nadd_subdirectory(threadblock)\nadd_subdirectory(device)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target for FMHA Fixed SeqLen in CMake\nDESCRIPTION: This code snippet uses the `cutlass_example_add_executable` macro to create an executable target for the fused multi-head attention example with fixed sequence length. It specifies the target name and the source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/41_fused_multi_head_attention/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  41_fused_multi_head_attention_fixed_seqlen\n  fused_multihead_attention_fixed_seqlen.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License\nDESCRIPTION: This snippet presents the BSD 3-Clause License under which the CUTLASS software is distributed. It outlines the conditions for redistribution and use, including the requirement to retain copyright notices and disclaimers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/doxygen_mainpage.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: SMEM Layout Preconditions C++\nDESCRIPTION: This code asserts static properties and dimension compatibility between shared memory layouts (ASmemLayout, BSmemLayout, CSmemLayout) and a `CtaTiler`.  The assertions ensure that the sizes of shared memory layouts match the corresponding tile dimensions defined by the `CtaTiler`, specifically ensuring BLK_M, BLK_N, and BLK_K are consistent.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n  // Preconditions\n  static_assert(is_static<ASmemLayout>::value);\n  static_assert(is_static<BSmemLayout>::value);\n  static_assert(is_static<CSmemLayout>::value);\n\n  CUTE_STATIC_ASSERT_V(size<0>(ASmemLayout{}) == size<0>(cta_tiler));  // BLK_M\n  CUTE_STATIC_ASSERT_V(size<0>(CSmemLayout{}) == size<0>(cta_tiler));  // BLK_M\n  CUTE_STATIC_ASSERT_V(size<0>(BSmemLayout{}) == size<1>(cta_tiler));  // BLK_N\n  CUTE_STATIC_ASSERT_V(size<1>(CSmemLayout{}) == size<1>(cta_tiler));  // BLK_N\n  CUTE_STATIC_ASSERT_V(size<1>(ASmemLayout{}) == size<2>(cta_tiler));  // BLK_K\n  CUTE_STATIC_ASSERT_V(size<1>(BSmemLayout{}) == size<2>(cta_tiler));  // BLK_K\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_4x4x5x128_3x3_256x3x6_pad_h0w0_stride_h1w1_dil_h1w1_conv_alpha1_beta0 s4nhwc_s4nhwc_inhwc_i_f 1998572675 3327908208 4290039622 3031516069\n```\n\n----------------------------------------\n\nTITLE: Defining Fixed Problem Sizes (CMake)\nDESCRIPTION: Defines CMake variables for fixed problem sizes, specifying matrix dimensions (m, n, k), group size, and setting iterations to 0 for correctness. This defines a specific GEMM problem to be tested.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_FIXED --m=2048 --n=5120 --k=8192 --groups=16 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f6f4xf8 GEMM Test\nDESCRIPTION: This CMake command adds an executable named `cutlass_test_unit_gemm_device_tensorop_sm100_f6f4xf8` for testing GEMM operations with f6f4xf8 precision on sm100 architecture. It uses batch sources, sets batch size to 1, and includes CUDA source files for different layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_sm100_f6f4xf8\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  f6f4_f8_void_f32_tn_layout.cu\n  f6f4_f8_void_f32_nt_layout.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Epilogue Operation Options with Large Groups (CMake)\nDESCRIPTION: Defines CMake variable for epilogue operations and large groups by specifying alpha value and setting iterations to 1.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_EPILOGUE_OP_LARGE_GROUP --alpha=0.25 --iterations=1)\n```\n\n----------------------------------------\n\nTITLE: CollectiveMma Class Template Definition in C++\nDESCRIPTION: Defines the `CollectiveMma` class template within the `cutlass::gemm::collective` namespace. This class serves as the main interface for collective matrix multiply-accumulate (MMA) operations. The template parameters define the dispatch policy, data types, memory layouts, and sub-operations involved in the MMA.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/gemm_api_3x.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nnamespace cutlass::gemm::collective {\n\ntemplate <\n  class DispatchPolicy,\n  class TileShape,\n  class ElementA,\n  class StrideA,\n  class ElementB,\n  class StrideB,\n  class TiledMma,\n  class GmemTiledCopyA,\n  class SmemLayoutAtomA,\n  class SmemCopyAtomA,\n  class TransformA,\n  class GmemTiledCopyB,\n  class SmemLayoutAtomB,\n  class SmemCopyAtomB,\n  class TransformB\n>\nstruct CollectiveMma {\n  static_assert(sizeof(ElementA) == 0, \"Could not find a mainloop specialization.\");\n};\n\n} // namespace cutlass::gemm::collective\n```\n\n----------------------------------------\n\nTITLE: Compiling and running CUTLASS unit tests\nDESCRIPTION: These commands show how to compile and run the CUTLASS unit tests after the CMake configuration step.  `make test_unit -j` compiles the unit tests, and the `-j` flag allows parallel compilation for faster build times. The output shows a summary of the test results.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ make test_unit -j\n...\n...\n...\n[----------] Global test environment tear-down\n[==========] 946 tests from 57 test cases ran. (10812 ms total)\n[  PASSED  ] 946 tests.\n```\n\n----------------------------------------\n\nTITLE: Setting Target Include Directories (CMake)\nDESCRIPTION: Sets the include directories for the `cutlass_nvrtc` target. It makes the current source directory public and the current binary directory private.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(\n  cutlass_nvrtc \n  PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}\n  PRIVATE ${CMAKE_CURRENT_BINARY_DIR}\n  )\n```\n\n----------------------------------------\n\nTITLE: CUTLASS conv2d fprop configuration\nDESCRIPTION: This configuration string defines the parameters for a conv2d forward propagation operation within CUTLASS. It specifies attributes such as kernel dimensions, padding, stride, dilation, and tensor layouts, as well as numerical values for alpha and beta.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_s32.txt#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_4x2x3x256_1x1_328x3x5_pad_h1w1_stride_h1w1_dil_h1w1_conv_alpha1_beta0 s4nhwc_s4nhwc_inhwc_i_f 1104335117 3889837239 961406695 2003082608\n```\n\n----------------------------------------\n\nTITLE: Using Predicate Tensors with copy_if\nDESCRIPTION: This snippet demonstrates how to use predicate tensors with the `copy_if` function to conditionally copy elements based on the values in the predicate tensors. It involves an additional condition check on the column index before using `copy_if`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n// Prefetch k_tile=0, gate these on k_residue as well\nCUTE_UNROLL\nfor (int k = 0; k < size<1>(tAsA); ++k) {\n  if (get<1>(tAcA(0,k)) >= -k_residue) { // some other condition on the column index\n    copy_if(tApA, tAgA(_,k,0), tAsA(_,k,0));\n  }\n}\n\nCUTE_UNROLL\nfor (int k = 0; k < size<1>(tBsB); ++k) {\n  if (get<1>(tBcB(0,k)) >= -k_residue) { // some other condition on the column index\n    copy_if(tBpB, tBgB(_,k,0), tBsB(_,k,0));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling and Listing Example Directory - CUDA\nDESCRIPTION: This snippet demonstrates how to compile the CUTLASS example `09_turing_tensorop_conv2dfprop` and list the contents of its directory. The `make` command compiles the specific example, and `ls` command displays the directory content.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ make 09_turing_tensorop_conv2dfprop\n\n$ ls examples/09_turing_tensorop_conv2dfprop \nexamples/09_turing_tensorop_conv2dfprop\n```\n\n----------------------------------------\n\nTITLE: Adding Blackwell Convolution Examples via CMake\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` CMake function to add three convolution examples as executable targets.  The examples are named `76_blackwell_conv_fprop`, `76_blackwell_conv_dgrad`, and `76_blackwell_conv_wgrad`, and their corresponding source files are specified.  This is conditionally executed based on the value of the `CUTLASS_NVCC_ARCHS` variable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/76_blackwell_conv/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  76_blackwell_conv_fprop\n  76_blackwell_conv_fprop.cu\n)\n\ncutlass_example_add_executable(\n  76_blackwell_conv_dgrad\n  76_blackwell_conv_dgrad.cu\n)\n\ncutlass_example_add_executable(\n  76_blackwell_conv_wgrad\n  76_blackwell_conv_wgrad.cu\n)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CLayout with Thread Strides for HMMA\nDESCRIPTION: This code snippet defines the CLayout with specific strides to map thread IDs to (m, n) coordinates. It represents the layout considering the positions of threads T0 to T7 in the C matrix.  The strides are calculated based on the column-major encoding of (m, n) coordinates.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n  using CLayout = Layout<Shape <Shape <_2,  _2, _2>, _8>,\n                         Stride<Stride<_1, _16, _4>, _?>;\n```\n\n----------------------------------------\n\nTITLE: Blackwell Dynamic Persistent Kernel Definition C++\nDESCRIPTION: This is a pseudo-code representation of a dynamic persistent kernel using Cluster Launch Control (CLC). It sets up common data structures and then performs coordinate-specific compute based on coordinates fetched by a CLC tile scheduler. Requires `clcTileScheduler` object.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/blackwell_cluster_launch_control.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n// Dynamic Persistent Kernel\n__device__ clc_dynamic_persistent_kernel(...) {\n  setup_common_data_structures(...);\n  dim3 workCoordinates = blockIdx;\n  dim3 newClcID;\n  bool isValidId;\n  do {\n    coordinate_specific_compute(workCoordinates);\n    std::tie(isValidId, newClcID) = clcTileScheduler.fetch_next_work();\n    workCoordinates = newClcID;\n  } while (isValidId);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a WMMA-like TiledMMA with 2x2 Atom Layout in C++\nDESCRIPTION: This snippet creates a TiledMMA that resembles a WMMA by using four quadpair MMAs arranged in a 2x2 layout. The resulting TiledMMA replicates the MMA_Atom across threads, creating a 16x16x4 MMA.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nTiledMMA mma = make_tiled_mma(SM70_8x8x4_F32F16F16F32_NT{},\n                                  Layout<Shape <_2,_2>,\n                                         Stride<_2,_1>>{});   // 2x2 n-major layout of Atoms\n    print_latex(mma);\n```\n\n----------------------------------------\n\nTITLE: Declaring Executable for CUDA Conv2D Fprop Example\nDESCRIPTION: This snippet declares an executable named '16_ampere_tensorop_conv2dfprop', using 'ampere_tensorop_conv2dfprop.cu' as the source file. It leverages the 'cutlass_example_add_executable' function, which is likely defined within the CUTLASS build system, to configure the build process for this specific CUDA example, targeting Ampere's tensor core capabilities for convolution forward propagation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/16_ampere_tensorop_conv2dfprop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  16_ampere_tensorop_conv2dfprop\n  ampere_tensorop_conv2dfprop.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Profiler Sources in CMake\nDESCRIPTION: Defines a list of source files required to build the CUTLASS profiler tool.  These sources include both C++ and CUDA files.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_policy(SET CMP0112 NEW)\nset(CUTLASS_TOOLS_PROFILER_SOURCES\n  src/main.cpp\n  src/cutlass_profiler.cu\n  src/options.cu\n  src/performance_report.cpp\n  src/enumerated_types.cpp\n  src/gpu_timer.cpp\n  src/device_allocation.cu\n  src/device_context.cu\n  src/cublas_helpers.cu             \n  src/cudnn_helpers.cpp                   \n  src/problem_space.cpp\n  src/operation_profiler.cu\n  src/gemm_operation_profiler.cu\n  src/grouped_gemm_operation_profiler.cu\n  src/block_scaled_gemm_operation_profiler.cu\n  src/blockwise_gemm_operation_profiler.cu\n  src/rank_k_operation_profiler.cu\n  src/rank_2k_operation_profiler.cu\n  src/trmm_operation_profiler.cu\n  src/symm_operation_profiler.cu\n  src/conv2d_operation_profiler.cu          \n  src/conv3d_operation_profiler.cu          \n  src/sparse_gemm_operation_profiler.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Performance Test Options with Large Groups (CMake)\nDESCRIPTION: Defines a CMake variable for large test groups, setting the number of groups and iterations for performance benchmarking.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_RANDOM_PERF_LARGE_GROUP --groups=100 --iterations=10)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable with CUTLASS: turing_tensorop_conv2dfprop CUDA\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` function to define an executable named `09_turing_tensorop_conv2dfprop`. It specifies the source file `turing_tensorop_conv2dfprop.cu` which contains the CUDA implementation. This function likely configures the build process to create an executable from the given source file using the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/09_turing_tensorop_conv2dfprop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  09_turing_tensorop_conv2dfprop\n  turing_tensorop_conv2dfprop.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Cutlass Example Executable in CMake\nDESCRIPTION: This snippet adds a Cutlass example executable using the `cutlass_example_add_executable` macro.  It specifies the target name `54_hopper_fp8_warp_specialized_gemm` and the source file `54_hopper_fp8_warp_specialized_gemm.cu`. This macro is likely defined elsewhere in the Cutlass build system to handle the necessary dependencies and compiler flags for building Cutlass examples.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/54_hopper_fp8_warp_specialized_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  54_hopper_fp8_warp_specialized_gemm\n  54_hopper_fp8_warp_specialized_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Copy Tensor Elements (Generic)\nDESCRIPTION: Copies elements from a source Tensor to a destination Tensor. This overload takes the source and destination Tensors as input, and it dispatches to optimized implementations based on their types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/04_algorithms.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <class SrcEngine, class SrcLayout,\n          class DstEngine, class DstLayout>\nCUTE_HOST_DEVICE\nvoid\ncopy(Tensor<SrcEngine, SrcLayout> const& src,\n     Tensor<DstEngine, DstLayout>      & dst);\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target for CUTLASS Convolution Dgrad Device Tests (CMake)\nDESCRIPTION: This snippet defines a custom target `cutlass_test_unit_conv_dgrad_device` in CMake. It depends on the executable target `cutlass_test_unit_conv_dgrad_device_tensorop_sm90`. This target is used to group and manage the build process for these specific unit tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/dgrad/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_conv_dgrad_device\n  DEPENDS\n  cutlass_test_unit_conv_dgrad_device_tensorop_sm90\n)\n```\n\n----------------------------------------\n\nTITLE: Generic Tensor Copy Implementation\nDESCRIPTION: A generic implementation of the copy algorithm that iterates through the destination Tensor and assigns each element the corresponding value from the source Tensor. It addresses tensors with 1-D logical coordinates, thus traversing both tensors in a logical column-major order.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/04_algorithms.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <class TA, class ALayout,\n          class TB, class BLayout>\nCUTE_HOST_DEVICE\nvoid\ncopy(Tensor<TA, ALayout> const& src,  // Any logical shape\n     Tensor<TB, BLayout>      & dst)  // Any logical shape\n{\n  for (int i = 0; i < size(dst); ++i) {\n    dst(i) = src(i);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Define Thread Layout (C++)\nDESCRIPTION: This snippet defines a thread layout for partitioning a tile of data. It creates a m-major 16x16 layout of threads, `tC`, which is used to partition a 128x128 tile of `C`-data. Each thread then computes its own 8x8 subtensor of `gC`. The code assumes the existence of a `make_layout` function and `Int` type.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n// Define thread layouts (static)\n  auto tC = make_layout(make_shape(Int<16>{}, Int<16>{}));   // (m,n) -> thr_idx; m-major\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License Text\nDESCRIPTION: This snippet presents the full text of the BSD-3-Clause license, which is a permissive open-source license. It specifies conditions for redistribution, modification, and use, including the requirement to retain copyright notices and disclaimers of warranty and liability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_24\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Building for NVIDIA Maxwell Architecture\nDESCRIPTION: Configures the build to target the NVIDIA Maxwell GPU architecture using CMake. The `CUTLASS_NVCC_ARCHS` flag specifies the target architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"50;53\"          # compiles for NVIDIA Maxwell GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Example Executable with CMake\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` CMake macro to define an executable. The macro takes the name of the executable and the corresponding CUDA source file as arguments. This configures CMake to build the specified CUDA file into an executable linked against the necessary CUTLASS libraries.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/51_hopper_gett/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  51_hopper_gett\n  51_hopper_gett.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Tensor4DCoord Structure Definition C++\nDESCRIPTION: This code snippet defines a `Tensor4DCoord` structure that inherits from `Coord<4>`. It provides named accessors for the n, h, w, and c indices, improving code readability when working with 4D tensor coordinates.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nstruct Tensor4DCoord : public Coord<4> {\n  Index & n();\n  Index & h();\n  Index & w();\n  Index & c();\n};\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Example Executables in CMake\nDESCRIPTION: This CMake code snippet conditionally defines multiple CUTLASS example executables using the `cutlass_example_add_executable` macro. The `CUTLASS_NVCC_ARCHS` variable is checked to match '100a' before adding the executables. Each executable is defined with a name and a corresponding CUDA source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/72_blackwell_narrow_precision_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_ARCHS MATCHES 100a)\ncutlass_example_add_executable(\n  72a_blackwell_nvfp4_bf16_gemm\n  72a_blackwell_nvfp4_bf16_gemm.cu\n  )\n\ncutlass_example_add_executable(\n  72b_blackwell_nvfp4_nvfp4_gemm\n  72b_blackwell_nvfp4_nvfp4_gemm.cu\n  )\n\ncutlass_example_add_executable(\n  72c_blackwell_mixed_mxfp8_bf16_gemm\n  72c_blackwell_mixed_mxfp8_bf16_gemm.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for FP8 Blockwise GEMM in CUTLASS (CUDA)\nDESCRIPTION: This code snippet defines an executable named '67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling' using the 'cutlass_example_add_executable' macro. It specifies the corresponding CUDA source file '67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu'.  This executable showcases the use of Hopper architecture with FP8 precision and warp-specialized GEMM using blockwise scaling in CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling\n  67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining NVRTC Header Addition Macro (CMake)\nDESCRIPTION: Defines a CMake macro `add_nvrtc_headers` that takes a base directory and a list of files as input.  It converts each file's content into a C++ string using `bin2hex.cmake` and generates a header file containing the string. It also appends the generated header info to NVRTC_INCLUDES variables for building the final environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(add_nvrtc_headers BASE_DIR FILES)\n  foreach(CUTLASS_FILE ${FILES})\n    set(OUTPUT_FILE \"${CMAKE_CURRENT_BINARY_DIR}/nvrtc/${CUTLASS_FILE}\")\n\n    string(REPLACE \"/\" \"_\" VARIABLE_NAME ${CUTLASS_FILE})\n    string(REPLACE \".\" \"_\" VARIABLE_NAME ${VARIABLE_NAME})\n\n    add_custom_command(OUTPUT ${OUTPUT_FILE}\n      COMMAND ${CMAKE_COMMAND}\n      -DFILE_IN=\"${BASE_DIR}/${CUTLASS_FILE}\"\n      -DFILE_OUT=\"${OUTPUT_FILE}\"\n      -DVARIABLE_NAME=\"${VARIABLE_NAME}\"\n      -P ${PROJECT_SOURCE_DIR}/bin2hex.cmake\n      DEPENDS ${BASE_DIR}/${CUTLASS_FILE}\n    )\n\n    list(APPEND GENERATED_HEADER_FILES \"${OUTPUT_FILE}\")\n\n    string(APPEND NVRTC_INCLUDES_HEADERS \"#include <${OUTPUT_FILE}>\\n\")\n    string(APPEND NVRTC_INCLUDES_STRINGS \"  ${VARIABLE_NAME},\\n\")\n    string(APPEND NVRTC_INCLUDES_NAMES \"  \\\"${CUTLASS_FILE}\\\",\\n\")\n  endforeach()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f8xf6f4 GEMM Test\nDESCRIPTION: This CMake command adds an executable named `cutlass_test_unit_gemm_device_tensorop_sm100_f8xf6f4` for testing GEMM operations with f8xf6f4 precision on sm100 architecture. It configures the build with batch sources, a batch size of 1, and includes CUDA source files to implement different data layouts.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_sm100_f8xf6f4\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  f8_f6f4_void_f32_tn_layout.cu\n  f8_f6f4_void_f32_nt_layout.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License\nDESCRIPTION: This code snippet represents the BSD 3-Clause License. It outlines the terms and conditions for redistribution and use of the software, including requirements for retaining copyright notices, disclaimers, and restrictions on using the copyright holder's name for endorsement.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/59_ampere_gather_scatter_conv/README.md#_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Targets for Unit Tests (CMake)\nDESCRIPTION: This snippet adds two custom targets, `cutlass_test_unit_conv` and `test_unit_conv`, which are likely used as umbrella targets to trigger the compilation and execution of unit tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(cutlass_test_unit_conv)\nadd_custom_target(test_unit_conv)\n```\n\n----------------------------------------\n\nTITLE: Run GEMM with Identity Activation - Python\nDESCRIPTION: This snippet demonstrates how to run a GEMM operation using the CUTLASS Python interface with an identity activation function. It defines a GEMM plan with the element type and layout, and then executes the GEMM using the input tensors A, B, C, and D. The identity activation is the default and doesn't need explicit specification.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs/externals/01_epilogue.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nplan = cutlass.op.Gemm(element=np.float16, layout=cutlass.LayoutType.RowMajor)\nplan.run(tensor_A, tensor_B, tensor_C, tensor_D, print_module=print_module)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target for FMHA in CMake\nDESCRIPTION: This code snippet defines a custom target named `41_fused_multi_head_attention` that depends on the three executable targets created earlier: fixed sequence length, variable sequence length, and backward pass examples. This ensures that these executables are built when the custom target is built.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/41_fused_multi_head_attention/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(41_fused_multi_head_attention\nDEPENDS 41_fused_multi_head_attention_fixed_seqlen\n        41_fused_multi_head_attention_variable_seqlen\n        41_fused_multi_head_attention_backward\n)\n```\n\n----------------------------------------\n\nTITLE: Conv2D Dgrad Descriptor\nDESCRIPTION: This string defines a 2D convolution backward data gradient (dgrad) operation. It specifies input and filter dimensions, padding, stride, dilation, correlation type, and alpha/beta scaling factors. Parameters indicate input and filter shapes, padding amounts (padl, padu for each dimension), stride, dilation (dil), and correlation settings. The 'h_h_h_h' indicates the data layout, followed by potential hash values.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_dgrad_device_tensorop_sm90.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(1,8,8,64)_(64,1,1,64)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1260538290 3426051534 1298679529 885253342\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(1,8,8,64)_(64,1,1,16)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1260538290 4139873761 2377544542 1796715273\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(2,8,8,64)_(64,1,1,96)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 913918101 1204062629 2917884534 3849387550\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(7,8,8,64)_(64,1,1,256)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1794462925 3542030567 2009277002 1494590621\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(2,6,6,64)_(64,3,3,256)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 3783509692 1508460534 184863710 2539209373\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(2,8,8,32)_(32,3,3,256)_padl(1,1)_padu(1,1)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1260538290 19124920 184863710 310844165\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(2,8,5,64)_(64,2,5,256)_padl(1,1)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 h_h_h_h 1829721897 2657646585 184863710 1493692231\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d dgrad_(2,15,5,256)_(256,2,5,64)_padl(1,1)_padu(0,0)_str(1,1)_dil(2,3)_corr_alpha1_beta0 h_h_h_h 2554054572 2657646585 184863710 732073700\n```\n\n----------------------------------------\n\nTITLE: Adding SM100 Sparse F4F8xF4F8 GEMM Test Executable (CMake)\nDESCRIPTION: This snippet utilizes a CMake function `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for the `f4f8xf4f8` sparse GEMM test on the SM100 architecture. It specifies the source files to be compiled and linked into the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_sparse_f4f8xf4f8\n\n  sm100_sp_gemm_f4_f8_f32_f16_f16_tn.cu\n  sm100_sp_gemm_f8_f4_f32_f16_f16_tn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Small Problem Size Test Options in CMake\nDESCRIPTION: This snippet defines CMake variables for setting up test options for GEMM (General Matrix Multiplication) examples using small, fixed problem sizes.  `--m`, `--n`, and `--k` likely define the dimensions of the matrices, and `--iterations=0` specifies a single iteration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/81_blackwell_gemm_blockwise/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_SMALL --m=256 --n=128 --k=128 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf8xmxf8)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf8_mxf8_void_f8_tn_layout.cu`, `mxf8_mxf8_void_f8_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf8xmxf8\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf8_mxf8_void_f8_tn_layout.cu\n  mxf8_mxf8_void_f8_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Define CUTLASS executable\nDESCRIPTION: This snippet defines a CUTLASS executable using the `cutlass_example_add_executable` macro.  It takes two arguments: the name of the executable and the source file. This creates an executable from the provided CUDA source file using CUTLASS build system conventions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/25_ampere_fprop_mainloop_fusion/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUTLASS\nCODE:\n```\ncutlass_example_add_executable(\n  25_ampere_fprop_mainloop_fusion\n  ampere_fprop_mainloop_fusion.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Return Statements - First Form C++\nDESCRIPTION: This code snippet demonstrates a function with an `auto` return type where all returns are within an `if constexpr` ... `else` statement. GCC 10 may incorrectly report a \"missing return statement\" warning. This form ensures all possibilities are covered by the conditional branches.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<class T>\nconstexpr auto first_form(T t) {\n  if constexpr (some_condition_v<T>) {\n    return some_function(t);\n  }\n  else if constexpr (another_condition_v<T>) {\n    return another_function(t);\n  }\n  else {\n    return yet_another_function(t);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf8_mxf8_f32_f16_f16_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f16_f16_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f16_f16_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_f16_q_tnt.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_void_f16_q_tnt.cu\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_f16_q_tnn.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_void_f16_q_tnn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License Text\nDESCRIPTION: This snippet represents the full text of the BSD-3-Clause license under which CUTLASS is distributed. It includes the conditions for redistribution, disclaimers of warranty, and limitations of liability.  It allows for both source and binary redistribution with proper attribution.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/overview.md#_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: C++ Macro Definition Example\nDESCRIPTION: This code snippet shows an example of how to define a macro in CUTLASS. Macros should be namespaced by starting them with the module name (e.g., `CUTLASS_`) and should use all capital letters with underscores between words. In this case, a macro named `CUTLASS_MACROS_USE_ALL_CAPS` is defined.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\n#define CUTLASS_MACROS_USE_ALL_CAPS inline __host__ __device__\n```\n\n----------------------------------------\n\nTITLE: Setting Large Performance Check Parameters in CMake\nDESCRIPTION: Defines CMake variables `TEST_LARGE_PERFCHECK` which sets the GEMM dimensions to 4096x3456x4096 and specifies performance check. This parameter is later used to specify the gemm size for the test executable and enable performance check.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/23_ampere_gemm_operand_reduction_fusion/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_LARGE_PERFCHECK --m=4096 --n=3456 --k=4096 --perf-check)\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Hopper STSM Unit Test Executable with CMake\nDESCRIPTION: This snippet defines an executable for the STSM (Shared Tensor Store Matrix) unit test using the `cutlass_test_unit_add_executable` CMake function. The executable's name is `cutlass_test_unit_cute_hopper_stsm` and it is built from the source file `stsm.cu`.  It is part of the CUTLASS Hopper architecture's unit testing suite.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/hopper/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_hopper_stsm\n  stsm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Constructing Layouts with make_layout in C++\nDESCRIPTION: This snippet demonstrates various ways to construct `Layout` objects using the `make_layout` function with combinations of static and dynamic integers for shapes and strides. It illustrates the creation of layouts with different shapes, including those with compile-time (static) and run-time (dynamic) dimensions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nLayout s8 = make_layout(Int<8>{});\nLayout d8 = make_layout(8);\n\nLayout s2xs4 = make_layout(make_shape(Int<2>{},Int<4>{}));\nLayout s2xd4 = make_layout(make_shape(Int<2>{},4));\n\nLayout s2xd4_a = make_layout(make_shape (Int< 2>{},4),\n                             make_stride(Int<12>{},Int<1>{}));\nLayout s2xd4_col = make_layout(make_shape(Int<2>{},4),\n                               LayoutLeft{});\nLayout s2xd4_row = make_layout(make_shape(Int<2>{},4),\n                               LayoutRight{});\n\nLayout s2xh4 = make_layout(make_shape (2,make_shape (2,2)),\n                           make_stride(4,make_stride(2,1)));\nLayout s2xh4_col = make_layout(shape(s2xh4),\n                               LayoutLeft{});\n```\n\n----------------------------------------\n\nTITLE: Setting Test Command Options (CMake)\nDESCRIPTION: Defines a CMake variable to set test command options. This example sets the number of iterations to 0, effectively disabling performance benchmarking and enabling only correctness checks. This is a common setup step to define the overall test parameters.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_RANDOM --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Enable Unity Build during CMake configuration\nDESCRIPTION: This CMake command enables the 'unity build' option during the CMake configuration, reducing the binary size and potentially avoiding linker limitations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_27\n\nLANGUAGE: Bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS=\"70;75;80\" -DCUTLASS_LIBRARY_KERNELS=all -DCUTLASS_UNITY_BUILD_ENABLED=ON\n```\n\n----------------------------------------\n\nTITLE: Setting Pipeline Sources CMake\nDESCRIPTION: This snippet defines a CMake variable `PIPELINE_SOURCES` which lists the source files related to pipeline implementations. These files include various asynchronous and warp-specialized versions of the pipeline.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/pipeline/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(PIPELINE_SOURCES\n  pipeline_tma_async.cu\n  pipeline_tma_async_warp_specialized.cu\n  pipeline_tma_async_warp_specialized_persistent.cu\n  pipeline_async.cu\n  sequence_barrier.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Define CUTLASS GEMM Executable in CMake\nDESCRIPTION: This code snippet defines an executable named '12_gemm_bias_relu' using the `cutlass_example_add_executable` CMake macro.  It specifies that the executable should be built from the CUDA source file 'gemm_bias_relu.cu'.  The macro likely handles linking necessary CUTLASS libraries and setting up the build environment.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/12_gemm_bias_relu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  12_gemm_bias_relu\n  gemm_bias_relu.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet uses the 'add_subdirectory' command in CMake to include the specified directories in the build process. This includes the source code and CMake configurations found within those directories, allowing them to be compiled and linked as part of the overall project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(threadblock)\nadd_subdirectory(device)\nadd_subdirectory(kernel)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf4mxf6_mxf4mxf6_f32_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4mxf6_mxf4mxf6_f32_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4mxf6_mxf4mxf6_f32_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf4_mxf6_f32_f16_f16_q_tnt.cu\n  sm100_bssp_gemm_mxf6_mxf4_f32_f16_f16_q_tnt.cu\n)\n```\n\n----------------------------------------\n\nTITLE: BSD 3-Clause License\nDESCRIPTION: Standard BSD 3-Clause License for the CUTLASS Project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/dependent_kernel_launch.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Alignment of Reference and Pointer Types in C++\nDESCRIPTION: This snippet demonstrates the preferred alignment of reference and pointer symbols in CUTLASS, where the '&' or '*' is flush against the type it modifies (left alignment).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nint const& var;\nint const* var;\n```\n\n----------------------------------------\n\nTITLE: Profiling One CUDA Core GEMM Kernel\nDESCRIPTION: This command line profiles a single SGEMM CUDA kernel using the CUTLASS profiler.  It specifies the `sgemm` kernel and problem dimensions (m, n, k). The output shows the performance metrics like runtime, memory bandwidth, and achieved GFLOP/s for this specific kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=3456 --n=4096 --k=4096\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sublayouts with take<ModeBegin, ModeEnd> in CuTe (C++)\nDESCRIPTION: This snippet demonstrates how to retrieve sublayouts from a parent layout using the `take<ModeBegin, ModeEnd>` syntax, specifying the beginning and ending indices of the dimensions to be included in the sublayout. Layout `a` is defined as `Layout<Shape<_2,_3,_5,_7>>`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a   = Layout<Shape<_2,_3,_5,_7>>{};     // (2,3,5,7):(1,2,6,30)\nLayout a13 = take<1,3>(a);                     // (3,5):(2,6)\nLayout a14 = take<1,4>(a);                     // (3,5,7):(2,6,30)\n// take<1,1> not allowed. Empty layouts not allowed.\n```\n\n----------------------------------------\n\nTITLE: Blackwell SM120 Blockscaled MMA TMA CUDA\nDESCRIPTION: This header file defines collective mainloops targeting blockscaled datatypes with support for dense GEMM on Blackwell SM120 architecture within the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_2\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/gemm/collective/sm120_blockscaled_mma_tma.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf8mxf6_mxf6mxf8_f32_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8mxf6_mxf6mxf8_f32_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8mxf6_mxf6mxf8_f32_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf6_mxf8_f32_f16_f16_q_tnt.cu\n  sm100_bssp_gemm_mxf8_mxf6_f32_f16_f16_q_tnt.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Targets for GEMM and CONV Fusion in CMake\nDESCRIPTION: These commands define custom targets for fused two-GEMM and fused two-CONV examples. Custom targets are used to group related build actions. The DEPENDS clause specifies dependencies between targets.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/13_two_tensor_op_fusion/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(13_fused_two_gemms)\n\nadd_custom_target(13_fused_two_convs)\n\nadd_custom_target(13_two_tensor_op_fusion\nDEPENDS 13_fused_two_gemms\n        13_fused_two_convs\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable with Batch Sources using CMake\nDESCRIPTION: This CMake code uses the 'cutlass_test_unit_gemm_device_add_executable' macro to create an executable. It enables batch sources using the 'BATCH_SOURCES ON' option and sets the batch size to 1. It then includes a list of CUDA source files (.cu) specific to the SM120 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_gemm_device_tensorop_epilogue_fusion_sm120\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm120_bs_gemm_nvf4_nvf4_f32_f32_epilogue_fusion.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_nvf4_epilogue_fusion.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_bf16_epilogue_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable with CMake\nDESCRIPTION: This CMake code snippet uses the `cutlass_example_add_executable` macro to define an executable named `33_ampere_3xtf32_tensorop_symm`. It specifies `ampere_3xtf32_tensorop_symm.cu` as the source file. The macro likely handles the compilation and linking process for the CUTLASS example.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/33_ampere_3xtf32_tensorop_symm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  33_ampere_3xtf32_tensorop_symm\n  ampere_3xtf32_tensorop_symm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Executable in CMake\nDESCRIPTION: This CMake function call defines a CUTLASS example executable named '06_splitK_gemm'. It takes the name of the executable and the source file 'splitk_gemm.cu' as arguments.  The function likely handles the compilation and linking process, setting up necessary compiler flags and dependencies for the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/06_splitK_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_example_add_executable(\n  06_splitK_gemm\n  splitk_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: synclog Output Example\nDESCRIPTION: This shows example synclog output lines.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/utilities.md#_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\nsynclog start\nsynclog at 1: cluster_barrier_init line=281 time=1725400116233388736 thread=0,0,0 block=0,0,0 smem_addr=197632 arrive_count=1\nsynclog at 13: fence_barrier_init line=583 time=1725400116233388768 thread=32,0,0 block=0,0,0 \n...\n```\n\n----------------------------------------\n\nTITLE: Slicing Tensor Example C++\nDESCRIPTION: This code demonstrates how to slice a `Tensor` using the `operator()` and the `_` (underscore) character to retain modes. It creates a tensor `A` and then slices it to create subtensors `B`, `C`, `D`, `E`, and `F`, showcasing different slicing techniques.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// ((_3,2),(2,_5,_2)):((4,1),(_2,13,100))\nTensor A = make_tensor(ptr, make_shape (make_shape (Int<3>{},2), make_shape (       2,Int<5>{},Int<2>{})),\n                            make_stride(make_stride(       4,1), make_stride(Int<2>{},      13,     100)));\n\n// ((2,_5,_2)):((_2,13,100))\nTensor B = A(2,_);\n\n// ((_3,_2)):((4,1))\nTensor C = A(_,5);\n\n// (_3,2):(4,1)\nTensor D = A(make_coord(_,_),5);\n\n// (_3,_5):(4,13)\nTensor E = A(make_coord(_,1),make_coord(0,_,1));\n\n// (2,2,_2):(1,_2,100)\nTensor F = A(make_coord(2,_),make_coord(_,3,_));\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Copyright and Redistribution Notice\nDESCRIPTION: This snippet contains the copyright notice and redistribution terms for the CUTLASS library. It specifies the conditions under which the software can be redistributed in source and binary forms, including the retention of the copyright notice and disclaimer.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/grouped_scheduler.md#_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Setting NVRTC Enable Flag\nDESCRIPTION: This snippet conditionally enables NVRTC support based on the presence of the `nvidia::nvrtc` and `nvidia::cuda_driver` targets. It sets the `CUTLASS_NVRTC_ENABLE` cache variable, which can be used to control whether NVRTC-related code is included in the build.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET nvidia::nvrtc AND TARGET nvidia::cuda_driver)\n  set(CUTLASS_NVRTC_ENABLE_INIT ON)\nelse()\n  set(CUTLASS_NVRTC_ENABLE_INIT OFF)\nendif()  \n\nset(CUTLASS_NVRTC_ENABLE ${CUTLASS_NVRTC_ENABLE_INIT} CACHE BOOL \"Enable NVRTC support\")\n```\n\n----------------------------------------\n\nTITLE: AlignedArray Template Definition in CUTLASS\nDESCRIPTION: This snippet shows the template definition of `AlignedArray` in CUTLASS. It inherits from `Array` and adds an alignment requirement. Pointers to `AlignedArray` objects ensure vectorized memory accesses. Requires the `cutlass` library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <\n  typename T,          // element type\n  int N,               // number of elements\n  int Alignment        // alignment requirement in bytes\n>\nclass AlignedArray;\n```\n\n----------------------------------------\n\nTITLE: Formatting function declaration with a long parameter list in C++\nDESCRIPTION: Illustrates the formatting style for function declarations when the function name and its parameters are too long to fit on one line.  Breaks the line immediately after the opening parenthesis that starts the parameter list and double-indents the parameters.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nvoid indeed_my_fellowbeings_this_function_name_is_unusually_long(\n    std::uint32_t foo, // parameters are double-indented\n    std::uint32_t const* bar,\n    TypeA a,\n    TypeB b,\n    TypeC c) { // the ) and { go on the same line still\n  auto d = body_of_the_function(a, b, c); // body is single-indented\n  // ... more code ...\n}\n```\n\n----------------------------------------\n\nTITLE: Sorting and Reversing CUDA Architectures\nDESCRIPTION: This snippet sorts the list of enabled CUDA architectures and reverses a copy of it to determine the maximum architecture.  This value is used in later conditional logic.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nlist(SORT CUTLASS_NVCC_ARCHS_ENABLED)\nset(CUTLASS_NVCC_ARCHS_ENABLED_REVERSED ${CUTLASS_NVCC_ARCHS_ENABLED})\nlist(REVERSE CUTLASS_NVCC_ARCHS_ENABLED_REVERSED)\nlist(GET CUTLASS_NVCC_ARCHS_ENABLED_REVERSED 0 CUTLASS_NVCC_MAX_ARCH)\n```\n\n----------------------------------------\n\nTITLE: Setting CUTLASS_BUILD_MONO_LIBRARY Option\nDESCRIPTION: This snippet defines a CMake option, `CUTLASS_BUILD_MONO_LIBRARY`, which determines whether the CUTLASS library is built as a single file or multiple files. The default value is OFF.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/library/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUTLASS_BUILD_MONO_LIBRARY OFF CACHE BOOL \n  \"Determines whether the cutlass library is generated as a single file or multiple files.\")\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target in CMake\nDESCRIPTION: This snippet defines a custom target named `cutlass_test_unit_cute` using the `add_custom_target` command. This target depends on other targets related to unit tests in different subdirectories, which are specified using the `DEPENDS` keyword. When the `cutlass_test_unit_cute` target is built, CMake will ensure that all of its dependencies are built first. This creates a meta-target that allows for easy building and execution of all unit tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_cute\n  DEPENDS\n  cutlass_test_unit_cute_layout\n  cutlass_test_unit_cute_core\n  cutlass_test_unit_cute_volta\n  cutlass_test_unit_cute_turing\n  cutlass_test_unit_cute_ampere\n  cutlass_test_unit_cute_hopper\n  cutlass_test_unit_cute_msvc_compilation\n  )\n```\n\n----------------------------------------\n\nTITLE: Define S8-S32 Convolution Sources (SM80)\nDESCRIPTION: This CMake code defines the source files for the S8-S32 convolution test executable on SM80 architecture using `cutlass_target_sources`. It specifies the source files for forward propagation with both S8 and S4 input types as private sources.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 80)\n    cutlass_target_sources(\n      cutlass_test_unit_conv_device_tensorop_s32\n      PRIVATE\n      conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu\n      conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Example Executable in CMake\nDESCRIPTION: This CMake snippet defines an executable named '19_tensorop_canonical' using the `cutlass_example_add_executable` function.  It links the 'tensorop_canonical.cu' source file to create the executable. This function is assumed to be defined in the CMake context of the CUTLASS project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/19_tensorop_canonical/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  19_tensorop_canonical\n  tensorop_canonical.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Building Executables for FMHA and MLA Kernels in CMake\nDESCRIPTION: This code block uses a loop to build multiple executables for different precisions (fp8, fp16) of FMHA and MLA kernels. It leverages the `cutlass_example_add_executable` function, sets include directories, defines compiler definitions (e.g., PREC_MACRO, CPASYNC, B2B), and specifies compiler options for each target. It configures tests for forward and backward passes of FMHA, and several variants of MLA kernels, under different compilation flags.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/77_blackwell_fmha/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT WIN32 AND (NOT (CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")) AND (CUTLASS_NVCC_ARCHS MATCHES 100a))\n\n  foreach(PREC fp8 fp16)\n    string(TOUPPER \"${PREC}\" PREC_MACRO)\n\n    cutlass_example_add_executable(\n        77_blackwell_fmha_${PREC}\n        77_blackwell_fmha.cu\n        TEST_COMMAND_OPTIONS\n        TEST_BASIC\n        # TEST_CAUSAL\n        # TEST_VARLEN\n        # TEST_HDIM64\n        # TEST_GQA)\n        )\n    target_include_directories(77_blackwell_fmha_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_fmha_${PREC} PRIVATE ${PREC_MACRO})\n  \n    cutlass_example_add_executable(\n        77_blackwell_fmha_gen_${PREC}\n        77_blackwell_fmha_gen.cu\n        TEST_COMMAND_OPTIONS\n        TEST_GEN_BASIC\n        # TEST_GEN_VARLEN\n        # TEST_GEN_HDIM64\n        # TEST_GEN_GQA\n        # TEST_GEN_REMAP\n        # TEST_GEN_CACHEONLY)\n        )\n    target_include_directories(77_blackwell_fmha_gen_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_fmha_gen_${PREC} PRIVATE ${PREC_MACRO})\n\n    cutlass_example_add_executable(\n        77_blackwell_mla_2sm_${PREC}\n        77_blackwell_mla.cu\n        TEST_COMMAND_OPTIONS\n        TEST_MLA_BASIC\n        )\n    target_include_directories(77_blackwell_mla_2sm_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_mla_2sm_${PREC} PRIVATE ${PREC_MACRO})\n    target_compile_options(77_blackwell_mla_2sm_${PREC} PRIVATE -Xptxas -v)\n\n    cutlass_example_add_executable(\n        77_blackwell_mla_2sm_cpasync_${PREC}\n        77_blackwell_mla.cu\n        TEST_COMMAND_OPTIONS\n        TEST_MLA_BASIC\n        )\n    target_include_directories(77_blackwell_mla_2sm_cpasync_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_mla_2sm_cpasync_${PREC} PRIVATE ${PREC_MACRO} CPASYNC)\n    target_compile_options(77_blackwell_mla_2sm_cpasync_${PREC} PRIVATE -Xptxas -v)\n\n    cutlass_example_add_executable(\n        77_blackwell_mla_b2b_2sm_${PREC}\n        77_blackwell_mla.cu\n        TEST_COMMAND_OPTIONS\n        TEST_MLA_BASIC\n        )\n    target_include_directories(77_blackwell_mla_b2b_2sm_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_mla_b2b_2sm_${PREC} PRIVATE ${PREC_MACRO} B2B)\n    target_compile_options(77_blackwell_mla_b2b_2sm_${PREC} PRIVATE -Xptxas -v)\n\n    cutlass_example_add_executable(\n        77_blackwell_fmha_bwd_${PREC}\n        77_blackwell_fmha_bwd.cu\n        TEST_COMMAND_OPTIONS\n        TEST_BASIC\n        # TEST_GEN_VARLEN\n        # TEST_GEN_HDIM64\n        # TEST_GEN_GQA\n        # TEST_GEN_REMAP\n        # TEST_GEN_CACHEONLY)\n        )\n    target_include_directories(77_blackwell_fmha_bwd_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_fmha_bwd_${PREC} PRIVATE ${PREC_MACRO})\n    target_compile_options(77_blackwell_fmha_bwd_${PREC} PRIVATE -Xptxas -v)\n\n    cutlass_example_add_executable(\n        77_blackwell_fmha_bwd_sat_${PREC}\n        77_blackwell_fmha_bwd.cu\n        TEST_COMMAND_OPTIONS\n        TEST_BASIC\n        # TEST_GEN_VARLEN\n        TEST_GEN_HDIM64\n        # TEST_GEN_GQA\n        # TEST_GEN_REMAP\n        # TEST_GEN_CACHEONLY)\n        )\n    target_include_directories(77_blackwell_fmha_bwd_sat_${PREC} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n    target_compile_definitions(77_blackwell_fmha_bwd_sat_${PREC} PRIVATE ${PREC_MACRO} SKIP_ATOMIC)\n    target_compile_options(77_blackwell_fmha_bwd_sat_${PREC} PRIVATE -Xptxas -v)\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Copyright and License Notice\nDESCRIPTION: This snippet contains the copyright notice, licensing terms (BSD-3-Clause), and disclaimer for the CUTLASS project. It outlines the conditions under which the software can be redistributed and used, both in source and binary forms.  It's crucial for understanding the legal terms associated with using and contributing to CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs_src/source/contribute.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable\nDESCRIPTION: This snippet uses the `cutlass_example_add_executable` CMake macro to create an executable. It takes the target executable name (`22_quaternion_conv`) and the source file (`quaternion_conv.cu`) as arguments. The macro handles the compilation and linking process to produce the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/22_quaternion_conv/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  22_quaternion_conv\n  quaternion_conv.cu \n  )\n```\n\n----------------------------------------\n\nTITLE: Setting Test Command Options based on CUDA Version in CMake\nDESCRIPTION: Conditionally sets the `CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_GEMM` variable based on the CUDA version and enabled architectures. This is used to configure the testing environment for the Gemm operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/profiler/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUDA_VERSION VERSION_GREATER_EQUAL 12.3 AND CUDA_VERSION VERSION_LESS 12.4 AND (90a IN_LIST CUTLASS_NVCC_ARCHS_ENABLED OR (90 IN_LIST CUTLASS_NVCC_ARCHS_ENABLED)))\n  set(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_GEMM   --operation=Gemm       --providers=cutlass --verification-providers=cublas,host      --junit-output=test_cutlass_profiler_gemm    --print-kernel-before-running=true)\nelse()\n    set(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_GEMM   --operation=Gemm       --providers=cutlass --verification-providers=cublas,device      --junit-output=test_cutlass_profiler_gemm    --print-kernel-before-running=true)\n    set(CUTLASS_PROFILER_TEST_COMMAND_OPTIONS_GEMM   --operation=BlockScaledGemm --providers=cutlass --verification-providers=cublas,device      --junit-output=test_cutlass_profiler_gemm    --print-kernel-before-running=true)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Examples Directory Structure\nDESCRIPTION: This code lists the directories under the `examples/` folder in the CUTLASS repository. Each directory demonstrates a specific use case of the CUTLASS library, such as basic GEMM operations, batched GEMM, mixed precision GEMM, and fused operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nexamples/\n  00_basic_gemm/             # launches a basic GEMM with single precision inputs and outputs\n\n  01_cutlass_utilities/      # demonstrates CUTLASS Utilities for allocating and initializing tensors\n  \n  02_dump_reg_smem/          # debugging utilities for printing register and shared memory contents\n  \n  03_visualize_layout/       # utility for visualizing all layout functions in CUTLASS\n\n  04_tile_iterator/          # example demonstrating an iterator over tiles in memory\n\n  05_batched_gemm/           # example demonstrating CUTLASS's batched strided GEMM operation\n\n  06_splitK_gemm/            # exmaple demonstrating CUTLASS's Split-K parallel reduction kernel\n\n  07_volta_tensorop_gemm/    # example demonstrating mixed precision GEMM using Volta Tensor Cores\n\n  08_turing_tensorop_gemm/   # example demonstrating integer GEMM using Turing Tensor Cores\n\n  10_planar_complex/         # example demonstrating planar complex GEMM kernels\n\n  11_planar_complex_array/   # example demonstrating planar complex kernels with batch-specific problem sizes\n\n  12_gemm_bias_relu/         # example demonstrating GEMM fused with bias and relu activation function\n\n  13_fused_two_gemms/        # example demonstrating two GEMMs fused into one kernel\n```\n\n----------------------------------------\n\nTITLE: End Conditional Block - CMake\nDESCRIPTION: This `endif()` statement closes the conditional block that defines the custom target and executables based on the `CUTLASS_NVCC_ARCHS` variable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Executable with CMake\nDESCRIPTION: This CMake command adds an executable named 'cutlass_test_unit_reduction_kernel' to the project. It specifies that the executable should be built from the 'reduce_splitk.cu' CUDA source file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/reduction/kernel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_reduction_kernel\n  reduce_splitk.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target with Dependencies (test_unit)\nDESCRIPTION: This snippet defines a custom target named 'test_unit_reduction'. It depends on the 'test_unit_reduction_thread', 'test_unit_reduction_kernel', and 'test_unit_reduction_device' targets.  This ensures that these dependencies are built before 'test_unit_reduction'.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/reduction/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  test_unit_reduction \n  DEPENDS\n  test_unit_reduction_thread\n  test_unit_reduction_kernel\n  test_unit_reduction_device\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Example Executable in CMake\nDESCRIPTION: This CMake code defines an executable named `27_ampere_3xtf32_fast_accurate_tensorop_gemm` and specifies the source file `27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu` to build it. This indicates that it is part of the CUTLASS library. This is for a specific GPU architecture (Ampere) and uses TF32 data type for Tensor Core GEMM operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  27_ampere_3xtf32_fast_accurate_tensorop_gemm\n  27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Copyright and Redistribution Notice\nDESCRIPTION: This snippet contains the copyright notice and the terms of redistribution for the CUTLASS library. It specifies the conditions under which the software can be redistributed in source and binary forms and disclaims warranties.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Setting Swizzling Stride in CUTLASS Python\nDESCRIPTION: This snippet showcases how to configure the swizzling stride to improve L2 locality in the CUTLASS Python interface.  A stride value of 4 is set for the plan. The convolution operation is then executed with this stride, and the result is compared against a PyTorch output for verification.  Swizzling stride affects how tiles are mapped to threadblocks, which directly impacts memory access patterns.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nplan.swizzling_stride = 4\nplan.run(input, weight, tensor_C, output, stride, padding, dilation, alpha, beta, print_module=print_module)\nassert torch.equal(output_torch, output)\n```\n\n----------------------------------------\n\nTITLE: Blackwell SM120 Callbacks TMA WarpSpecialized CUDA\nDESCRIPTION: This header file defines the callbacks for full set of EVT fusions, using TMA (Tensor Memory Accelerator) and warp-specialized optimizations for Blackwell SM120 within CUTLASS.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_5\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/epilogue/fusion/sm120_callbacks_tma_warpspecialized.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Executable in CMake\nDESCRIPTION: This CMake function adds an executable named 'cutlass_test_unit_layout' to the project. It compiles 'matrix.cu', 'tensor.cu', and 'tensor_nhwc.cu' to create the executable.  The function handles creating test binaries within the CUTLASS build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/layout/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_layout\n  matrix.cu\n  tensor.cu\n  tensor_nhwc.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: AlignedBuffer Template Definition in CUTLASS\nDESCRIPTION: This snippet shows the template definition for `AlignedBuffer` in CUTLASS. This class provides a way to define aligned memory allocations, especially within shared memory. Elements within `AlignedBuffer<>` are uninitialized. Requires the `cutlass` library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <\n  typename T,          // element type\n  int N,               // number of elements\n  int Alignment        // alignment requirement in bytes\n>\nclass AlignedBuffer;\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (nvf4_nvf4_f32_f32_f32_o)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM.  It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f32_f32_o` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f32_f32_o\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f32_f32_o_tnt.cu\n  sm100_bssp_gemm_nvf4_nvf4_f32_void_f32_o_tnt.cu\n\n  sm100_bssp_gemm_nvf4_nvf4_f32_f32_f32_o_tnn.cu\n  sm100_bssp_gemm_nvf4_nvf4_f32_void_f32_o_tnn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Target for Convolution Weight Gradient Test\nDESCRIPTION: This CMake code adds a custom target named `cutlass_test_unit_conv_wgrad_device` which depends on the executable `cutlass_test_unit_conv_wgrad_device_tensorop_sm90`. This establishes a dependency relationship for build ordering.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/wgrad/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_conv_wgrad_device\n  DEPENDS\n  cutlass_test_unit_conv_wgrad_device_tensorop_sm90\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Test Configurations (CMake)\nDESCRIPTION: These lines define CMake variables that set the parameters for various tests. These variables configure the matrix dimensions (m, n, k), batch size (l), alpha and beta values for the epilogue, and the number of iterations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/56_hopper_ptr_array_batched_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_SQUARE --m=2048 --n=2048 --k=2048 -l=10 --iterations=1)\nset(TEST_SQUARE_LARGE_BATCH --m=2048 --n=2048 --k=2048 -l=500 --iterations=1)\n\nset(TEST_EPILOGUE --alpha=0.5 --beta=0.5 --iterations=1)\nset(TEST_EPILOGUE_LARGE_BATCH --alpha=1.5 --beta=2.0 -l=500 --iterations=1)\n\nset(TEST_EPILOGUE_OP --beta=0.5 --iterations=1)\nset(TEST_EPILOGUE_OP_LARGE_BATCH --alpha=1.5 -l=500 --iterations=1)\n\nset(TEST_SMALLK --m=2048 --n=5120 --k=128 --l=5 --iterations=1)\nset(TEST_SMALLK_LARGE_BATCH --m=1024 --n=512 --k=64 --l=500 --iterations=1)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf4_mxf4_f32_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4_mxf4_f32_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4_mxf4_f32_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf4_mxf4_f32_f16_mxf8_q_tnt.cu\n  sm100_bssp_gemm_mxf4_mxf4_f32_f16_f16_q_tnt.cu\n  sm100_bssp_gemm_mxf4_mxf4_f32_f32_f32_q_tnt.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable (CMake)\nDESCRIPTION: This CMake code snippet adds an executable named `18_ampere_fp64_tensorop_affine2_gemm` using the custom CMake function `cutlass_example_add_executable`.  It takes the executable name and the CUDA source file (`ampere_fp64_tensorop_affine2_gemm.cu`) as input. This function likely handles the necessary build steps for CUDA executables within the CUTLASS project.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  18_ampere_fp64_tensorop_affine2_gemm\n  ampere_fp64_tensorop_affine2_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License Text\nDESCRIPTION: This snippet contains the full text of the BSD-3-Clause license under which the Cutlass library is distributed. It outlines the conditions for redistribution, modification, and use, and includes a disclaimer of warranties and limitation of liability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_13\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Defining Ampere Group Convolution Executable (CMake)\nDESCRIPTION: This snippet defines an executable named `42_ampere_tensorop_group_conv` using the `cutlass_example_add_executable` macro. This macro likely handles the build process for CUTLASS examples. The source file for this executable is `ampere_tensorop_group_conv.cu`, which is expected to contain CUDA code.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/42_ampere_tensorop_group_conv/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  42_ampere_tensorop_group_conv\n  ampere_tensorop_group_conv.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Building CUDA Library in CMake\nDESCRIPTION: Uses the `cutlass_add_library` CMake function to compile all the generated CUDA source files in the `_gen_source_files` list into a single library named `test_self_contained_includes`. The library is created as a module, indicating it is likely intended to be dynamically linked or loaded.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/self_contained_includes/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_add_library(test_self_contained_includes MODULE ${_gen_source_files})\n```\n\n----------------------------------------\n\nTITLE: Defining Another Custom Target in CMake\nDESCRIPTION: This snippet defines another custom target named `test_unit_cute` using the `add_custom_target` command. Similar to the previous snippet, this target depends on other targets related to unit tests located in various subdirectories, as specified by the `DEPENDS` keyword. This target is likely used for a different set of unit tests or a different configuration of the tests. It also acts as a meta-target, simplifying the building and execution of specific sets of unit tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  test_unit_cute\n  DEPENDS\n  test_unit_cute_layout\n  test_unit_cute_core\n  test_unit_cute_volta\n  test_unit_cute_ampere\n  test_unit_cute_turing\n  test_unit_cute_hopper\n  test_unit_cute_msvc_compilation\n  )\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policy\nDESCRIPTION: Sets the CMake policy CMP0112 to 'NEW', ensuring consistent behavior regarding include directories with system prefixes.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/tools/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_policy(SET CMP0112 NEW)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf8_mxf8_f32_f16_mxf8_q)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f16_mxf8_q` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f16_mxf8_q\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_tnt_sfd.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_void_mxf8_q_tnt_sfd.cu\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_tnn_sfd.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_void_mxf8_q_tnn_sfd.cu\n\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_ttt_sfd.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_ttn_sfd.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_nnt_sfd.cu\n  sm100_bssp_gemm_mxf8_mxf8_f32_f16_mxf8_q_nnn_sfd.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Advancing Optimized Iterator in C++\nDESCRIPTION: This C++ code snippet shows the `advance()` function implementation for the optimized `Conv2dFpropActivationTileAccessIteratorOptimized` class. It uses precomputed increments to update the pointer to the next tile. A simple lookup from a delta table (`params_.inc_next`) is performed in device code, making the advancement process more efficient than the analytic version.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n// cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h\nCUTLASS_HOST_DEVICE\nvoid advance() { \n\n  int next_idx = 0;\n \n  // moves to the next tile\n  ++filter_s_;\n  if (filter_s_ == problem_size_.S) {\n    filter_s_ = 0;\n    ++filter_r_;\n \n    if (filter_r_ < problem_size_.R) {\n      next_idx = 1;\n    }\n    else {\n      filter_r_ = 0;\n      next_idx = 2;\n    }\n  }\n  \n  add_byte_offset_(params_.inc_next[next_idx]); // in addition to Conv2dFpropActivationTileAccessIteratorAnalytic::advance()\n\n  if (next_idx == 2) {  \n    filter_c_ += params_.filter_c_delta;\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Adding Executable with Source Files using CMake\nDESCRIPTION: This CMake code uses the 'cutlass_test_unit_gemm_device_add_executable' macro to create another executable, directly specifying the CUDA source files (.cu) to be included in the build. These sources cover various GEMM configurations for SM120, including different data types (f32, bf16, f16) and epilogue functions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_bs_gemm_device_tensorop_sm120\n\n  sm120_bs_gemm_nvf4_nvf4_f32_bf16.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_f16.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_f32.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_f32_narrow_output.cu\n  sm120_bs_gemm_nvf4_nvf4_f32_epilogue.cu\n  sm120_bs_gemm_mxf4_mxf4_f32_f32.cu\n  sm120_bs_gemm_mxf6_mxf8_f32_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Add Executable for s8xs8 TensorOp GEMM Test (sm100)\nDESCRIPTION: This CMake snippet defines an executable for testing s8xs8 GEMM operations with Tensor Cores on sm100 using the `cutlass_test_unit_gemm_device_add_executable` function. It enables batching, sets the batch size to 1, and includes the necessary source files, including a fusion kernel.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_sm100_s8xs8\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  s8_s8_void_s32.cu\n  s8_s8_s32_s32_fusion.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Naive Matrix Multiplication with Single Thread (C++)\nDESCRIPTION: This code snippet demonstrates a trivial way to compute a matrix product using a single thread. It iterates over all elements of the output matrix `gC` and accumulates the result of multiplying corresponding elements from input matrices `sA` and `sB`. While functional, this approach does not utilize the available parallelism of a CUDA thread block.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nif (thread0()) {\n  for (int m = 0; m < size<0>(gC); ++m) {\n    for (int n = 0; n < size<1>(gC); ++n) {\n      for (int k = 0; k < size<1>(sA); ++k) {\n        gC(m,n) += sA(m,k) * sB(n,k);\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Coalesce Usage\nDESCRIPTION: This example demonstrates the usage of the `coalesce` function to simplify a multi-mode layout into a simpler form with fewer modes.  The original layout is transformed into a single mode layout while preserving the total size.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nauto layout = Layout<Shape <_2,Shape <_1,_6>>,\n                     Stride<_1,Stride<_6,_2>>>{};\nauto result = coalesce(layout);    // _12:_1\n```\n\n----------------------------------------\n\nTITLE: Defining sgemm_2 Executable (CMake/CUDA)\nDESCRIPTION: This command uses a custom CMake macro `cutlass_example_add_executable` to define an executable named 'cute_tutorial_sgemm_2'. The source file for this executable is 'sgemm_2.cu', indicating a CUDA implementation.  It creates an executable target within the Cutlass build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_sgemm_2\n  sgemm_2.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Configuration for 3D Convolution Forward Propagation\nDESCRIPTION: This snippet defines configurations for 3D convolution forward propagation (fprop) operations.  It specifies input and output tensor dimensions (depth, height, width, channels), kernel size, padding, stride, dilation, correlation type (corr), alpha, beta, and numerical seeds for initializing the data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_fprop_device_tensorop_sm90.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(1,1,8,8,64)_(64,1,1,1,64)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 3453982556 1628467649 592544256 1519969106\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(1,1,8,8,64)_(16,1,1,1,64)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 3453982556 3090183904 930148554 3904697107\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,1,8,8,64)_(96,1,1,1,64)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 854950536 3677150560 1408652759 1475171676\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,3,5,8,64)_(96,3,3,3,64)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 1304210401 3269133702 2815004824 1363667010\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,3,5,8,32)_(96,3,3,3,32)_padl(1,1,1)_padu(1,1,1)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 1689793983 2561137151 3947985101 3903813688\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,3,5,8,64)_(96,3,4,5,64)_padl(1,1,1)_padu(1,1,1)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 1304210401 2276316058 1262249481 1546938459\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,3,5,8,64)_(96,3,4,5,64)_padl(1,0,1)_padu(0,2,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 1304210401 2276316058 288977156 2430714665\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,16,10,16,64)_(96,3,4,5,64)_padl(1,0,1)_padu(0,2,0)_str(2,2,3)_dil(1,1,1)_corr_alpha1_beta0 f_f_f_f 4289052901 2276316058 1081688479 1271132150\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,16,10,16,64)_(96,3,4,5,64)_padl(1,0,1)_padu(0,2,0)_str(1,1,1)_dil(2,2,3)_corr_alpha1_beta0 f_f_f_f 4289052901 2276316058 3305425723 749517833\n```\n\nLANGUAGE: text\nCODE:\n```\nconv3d fprop_(2,16,10,16,64)_(96,3,4,5,64)_padl(1,0,1)_padu(0,2,0)_str(2,2,3)_dil(2,2,3)_corr_alpha1_beta0 f_f_f_f 4289052901 2276316058 304972512 1722195827\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for f32 SM120 TensorOp GEMM Tests\nDESCRIPTION: This CMake function adds an executable named `cutlass_test_unit_gemm_device_tensorop_f32_sm120`. It compiles several CUDA files (`.cu`) that implement different GEMM configurations with f32 data type and Tensor Cores specifically designed for the SM120 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm120_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_tensorop_f32_sm120\n\n  sm120_gemm_f4_f6_f32_tensor_op_narrow_output.cu\n  sm120_gemm_f4_f6_f32_tensor_op.cu\n  sm120_gemm_f4_f8_f32_tensor_op.cu\n  sm120_gemm_f6_f8_f32_tensor_op.cu\n  sm120_gemm_f4_f4_f32_tensor_op.cu\n  sm120_gemm_f6_f6_f32_tensor_op.cu\n  sm120_gemm_f8_f8_f32_tensor_op.cu\n)\n```\n\n----------------------------------------\n\nTITLE: C++ Namespace Structure\nDESCRIPTION: This snippet illustrates the recommended namespace structure in CUTLASS. Namespaces are all lowercase, with a top-level `cutlass::` namespace, a second level for the general category of operation (e.g., `gemm::`), and a third level for the operation's position in the conceptual hierarchy (e.g., `kernel::`). The code also shows how to add comments on the closing brace to indicate the namespace being closed.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/programming_guidelines.md#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nnamespace cutlass {\nnamespace gemm {\nnamespace kernel {\n\nstruct AnotherGemmKernel {\n  // ... contents ...\n};\n\n} // namespace kernel\n} // namespace gemm\n} // namespace cutlass\n```\n\n----------------------------------------\n\nTITLE: Coalesce Layout\nDESCRIPTION: The `coalesce` operation simplifies a layout by reducing the number of modes without changing its function. This snippet shows the post-conditions for the coalesce function, ensuring the size remains the same and the depth is reduced.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// @post size(@a result) == size(@a layout)\n// @post depth(@a result) <= 1\n// @post for all i, 0 <= i < size(@a layout), @a result(i) == @a layout(i)\nLayout coalesce(Layout const& layout)\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Test Executable with CMake\nDESCRIPTION: This CMake command creates an executable named 'cutlass_test_unit_transform_threadblock' and links the provided CUDA source files into it. It is used for building a unit test within the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/transform/threadblock/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_transform_threadblock\n  regular_tile_iterator_tensor_op.cu\n  predicated_tile_iterator.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating Identity Tensors for Predication\nDESCRIPTION: This snippet demonstrates how to create identity tensors that map coordinates to themselves, which are then used for predication. It shows the creation of identity tensors for shared memory tiles of A and B, and how they are partitioned in the same way the data was partitioned.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0y_predication.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nTensor cA = make_identity_tensor(make_shape(size<0>(sA), size<1>(sA)));   // (BLK_M,BLK_K) -> (blk_m,blk_k)\nTensor cB = make_identity_tensor(make_shape(size<0>(sB), size<1>(sB)));   // (BLK_N,BLK_K) -> (blk_n,blk_k)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Target for Blockscaled Sparse GEMM Tests (SM100)\nDESCRIPTION: This CMake code adds a custom target `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse`. This target depends on a list of executables generated from the subsequent `cutlass_test_unit_gemm_device_add_executable_split_file` calls. These executables represent various configurations of blockscaled sparse GEMM tests for the SM100 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse\n  DEPENDS\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f32_f32_o\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f16_f16_o\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_nvf4_nvf4_f32_f16_nvf4_o\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f32_f32_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f16_f16_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8_mxf8_f32_f16_mxf8_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8mxf4_mxf4mxf8_f32_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf8mxf6_mxf6mxf8_f32_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4mxf6_mxf4mxf6_f32_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf6_mxf6_f32_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4_mxf4_f32_q\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4_mxf4_f32_o\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_streamk\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for Blockscaled Sparse GEMM Test (mxf4_mxf4_f32_o)\nDESCRIPTION: This CMake code uses a custom macro `cutlass_test_unit_gemm_device_add_executable_split_file` to add an executable for a specific configuration of blockscaled sparse GEMM. It configures the test executable `cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4_mxf4_f32_o` with batch compilation enabled and specifies the source files to be compiled.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable_split_file(\n  cutlass_test_unit_gemm_device_sm100_blockscaled_sparse_mxf4_mxf4_f32_o\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm100_bssp_gemm_mxf4_mxf4_f32_f16_f16_o_tnn.cu\n  sm100_bssp_gemm_mxf4_mxf4_f32_f16_f16_o_tnt.cu\n\n  sm100_bssp_gemm_mxf4_mxf4_f32_f32_f32_o_tnt.cu\n  sm100_bssp_gemm_mxf4_mxf4_f32_f32_f32_o_tnn.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Cutlass Executable using CMake\nDESCRIPTION: This CMake snippet defines an executable named '58_ada_fp8_gemm' using the 'cutlass_example_add_executable' function. It takes the executable name and the source file 'ada_fp8_gemm.cu' as input. This function is likely a custom CMake function provided by the Cutlass project to simplify the creation of example executables.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/58_ada_fp8_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  58_ada_fp8_gemm\n  ada_fp8_gemm.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Setting Generated Source Files (CMake)\nDESCRIPTION: Sets the `GENERATED_SOURCE_FILES` variable to the path of the generated `environment.cpp` file.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset(GENERATED_SOURCE_FILES \"${CMAKE_CURRENT_BINARY_DIR}/cutlass/nvrtc/environment.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Compiler Flag Based on Device Reference (CMake)\nDESCRIPTION: This snippet conditionally appends a compiler flag, `-DCUTLASS_CONV_TEST_UNIT_REFERENCE_DEVICE_ENABLED=1`, to the `CUTLASS_CUDA_NVCC_FLAGS` list if `CUTLASS_CONV_TEST_UNIT_REFERENCE_DEVICE_ENABLED` is true. This flag enables device reference verification during compilation by defining a preprocessor macro.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(CUTLASS_CONV_TEST_UNIT_REFERENCE_DEVICE_ENABLED)\n  message(STATUS \"Enable device reference verification in conv unit tests\")\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_CONV_TEST_UNIT_REFERENCE_DEVICE_ENABLED=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CLayout for GMMA (T128,V64) -> (M64,N128) in C++\nDESCRIPTION: This code snippet defines the CLayout for a 64x128 accumulator with 16 copies of the 64x8 tile. The layout maps a thread block of size 128x64 to a matrix of size 64x128, specifying the shape and stride along both dimensions.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n// (T128,V64) -> (M64,N128)\nusing CLayout = Layout<Shape <Shape <  _4, _8,  _4>, Shape < _2, _2,  _16>>,\n                       Stride<Stride<_128, _1, _16>, Stride<_64, _8, _512>>>;\n```\n\n----------------------------------------\n\nTITLE: Defining CUTLASS Test Executable with CUDA Files\nDESCRIPTION: This snippet uses a CMake function, `cutlass_test_unit_add_executable`, to define a test executable named `cutlass_test_unit_epilogue_warp`. It includes three CUDA source files: `fragment_iterator_tensor_op.cu`, `fragment_iterator_volta_tensor_op.cu`, and `fragment_iterator_wmma_tensor_op.cu`. This executable likely tests the epilogue warp functionality with various fragment iterator implementations, optimized for different hardware architectures.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/epilogue/warp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_epilogue_warp\n  fragment_iterator_tensor_op.cu\n  fragment_iterator_volta_tensor_op.cu\n  fragment_iterator_wmma_tensor_op.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (nvf4xnvf4)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`nvf4_nvf4_bf16_bf16.cu`, `nvf4_nvf4_bf16_bf16_features.cu`, `nvf4_nvf4_f16_nvfp4_epilogue.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_nvf4xnvf4\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  nvf4_nvf4_bf16_bf16.cu\n  nvf4_nvf4_bf16_bf16_features.cu\n  nvf4_nvf4_f16_nvfp4_epilogue.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf4xmxf4)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf4_mxf4_void_f16_tn_layout.cu`, `mxf4_mxf4_void_f16_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf4xmxf4\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf4_mxf4_void_f16_tn_layout.cu\n  mxf4_mxf4_void_f16_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Trivial Memory Copy C++\nDESCRIPTION: This code shows a basic (inefficient) way to copy a tile of global memory `gA0` to shared memory `sA` using a single thread. It iterates through each element of the shared memory tensor and assigns the corresponding element from the global memory tensor.  This approach does not utilize thread-level parallelism.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nif (thread0()) {\n  Tensor gA0 = gA(_,_,0);  // (BLK_M,BLK_K), the 0th tile\n  for (int i = 0; i < size(sA); ++i) {\n    sA(i) = gA0(i);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Build CUTLASS Python Interface Documentation\nDESCRIPTION: These commands generate the CUTLASS Python interface documentation using Sphinx. It requires the interface to be installed first and uses the configuration in the `docs_src` directory.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-apidoc -o docs_src/source/ cutlass/ cutlass/backend*\ncd docs_src\nmake html\nmv _build/* ../docs\n```\n\n----------------------------------------\n\nTITLE: Adding a CUDA executable for GEMM device tests (mxf6xmxf6)\nDESCRIPTION: This CMake code uses the `cutlass_test_unit_gemm_device_add_executable` macro to create a CUDA executable for a GEMM device test. It specifies the source files (`mxf6_mxf6_void_bf16_tn_layout.cu`, `mxf6_mxf6_void_bf16_nt_layout.cu`) and configuration options like `BATCH_SOURCES` and `BATCH_SIZE`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_gemm_device_add_executable(\n  cutlass_test_unit_gemm_device_bstensorop_sm100_mxf6xmxf6\n\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  mxf6_mxf6_void_bf16_tn_layout.cu\n  mxf6_mxf6_void_bf16_nt_layout.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Running the Example with Benchmark - CUDA\nDESCRIPTION: This snippet runs the compiled `09_turing_tensorop_conv2dfprop` example with the `--benchmark` flag. This executes the application with a predefined set of convolution layers and batch sizes to measure performance.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/implicit_gemm_convolution.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nbuild$ ./examples/09_turing_tensorop_conv2dfprop/09_turing_tensorop_conv2dfprop --benchmark\n```\n\n----------------------------------------\n\nTITLE: Convolution 1D Weight Gradient Configuration\nDESCRIPTION: This configuration defines parameters for a 1D convolution weight gradient calculation. It specifies input and output dimensions, padding, stride, dilation, alpha/beta values, and hash values.  The parameters are formatted as conv1d wgrad_(input_dims)_(output_dims)_padl(pad_left)_padu(pad_up)_str(stride)_dil(dilation)_corr_alpha1_beta0 h_h_h_h hash1 hash2 hash3 hash4\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_wgrad_device_tensorop_sm90.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconv1d wgrad_(1,8,64)_(1,8,128)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2903348270 4139873761 1911791555 2094311858\nconv1d wgrad_(1,8,16)_(1,8,128)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 3472074346 4139873761 626177484 4014899436\nconv1d wgrad_(2,8,96)_(2,8,128)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 2790635024 2105873592 2917884534 4063149047\nconv1d wgrad_(7,8,256)_(7,8,128)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 37621077 58406568 184863710 3825831917\nconv1d wgrad_(2,6,256)_(2,8,128)_padl(0)_padu(0)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 3284272669 2105873592 1352114402 170044063\nconv1d wgrad_(2,8,256)_(2,8,128)_padl(1)_padu(1)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 1260538290 2105873592 1352114402 1054386746\nconv1d wgrad_(2,6,256)_(2,8,128)_padl(0)_padu(1)_str(1)_dil(1)_corr_alpha1_beta0 h_h_h_h 3284272669 2105873592 4149848551 3241398662\nconv1d wgrad_(2,4,256)_(2,8,128)_padl(0)_padu(1)_str(2)_dil(1)_corr_alpha1_beta0 h_h_h_h 2501680779 2105873592 1352114402 2277261527\nconv1d wgrad_(2,5,256)_(2,8,128)_padl(0)_padu(1)_str(1)_dil(2)_corr_alpha1_beta0 h_h_h_h 3472351235 2105873592 1352114402 1619048523\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target for 2D Conv Fprop Device Tests (CMake)\nDESCRIPTION: This CMake code adds an executable target named `cutlass_test_unit_conv2d_fprop_device_tensorop_sm90`. It disables source batching to control compiler memory usage and sets the batch size to 1. It then specifies the CUDA source files for different data types (s8, f16, tf32) and tensor operations used in the 2D convolution forward propagation tests on the SM90 architecture.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/fprop/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv2d_fprop_device_tensorop_sm90\n\n  # No batching of source to control compiler memory usage\n  BATCH_SOURCES ON\n  BATCH_SIZE 1\n\n  sm90_conv2d_fprop_implicit_gemm_s8_s8_s32_tensorop_s32.cu\n  sm90_conv2d_fprop_implicit_gemm_f16_f16_f32_tensorop_f16.cu\n  sm90_conv2d_fprop_implicit_gemm_f16_f16_f32_tensorop_f32.cu\n  sm90_conv2d_fprop_implicit_gemm_tf32_tf32_f32_tensorop_f32.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Array with Sub-Byte Data Types Example in CUTLASS\nDESCRIPTION: This example shows how to use `Array` to store sub-byte data types like `int4b_t`. The elements are densely packed. Accessing elements is done using usual indirection and assignment operators. Requires the `cutlass` library and assumes `int4b_t` is defined.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/fundamental_types.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nArray<int4b_t, 2> packed_integers;\n\nstatic_assert(\n  sizeof(packed_integers) == 1,\n \"Packed storage of sub-byte data types is compact.\");\n\n// Access array elements using usual indirection and assignment operators\npacked_integers[0] = 2_s4;\npacked_integers[1] = 3_s4;\n\nCUTLASS_PRAGMA_UNROLL\nfor (auto x : elements) {\n  printf(\"%d\", int(x));       // access elements normally\n}\n```\n\n----------------------------------------\n\nTITLE: Running Warp-Level GEMM Tests\nDESCRIPTION: Executes only the warp-level GEMM tests. This command allows for focused testing of specific components within the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/quickstart.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ make test_unit_gemm_warp -j\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Redistribution License (BSD-3-Clause)\nDESCRIPTION: This text block defines the license for redistribution and use of the CUTLASS software, specifying conditions for source and binary forms.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/python/docs_src/source/install.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Add F16-F32 Convolution Test Executable (SM75)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with F16 input, F32 output, and F32 accumulation on SM75 architecture. It specifies the source files for forward propagation, data gradient, and weight gradient, as well as special cases with broadcast and reduction.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_conv_device_tensorop_f32_sm75\n\n  conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu\n  conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu\n  conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu\n\n  conv2d_fprop_with_broadcast_sm75.cu\n  conv2d_fprop_with_reduction_sm75.cu\n\n  conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu\n  conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining sgemm_1 Executable (CMake/CUDA)\nDESCRIPTION: This command uses a custom CMake macro `cutlass_example_add_executable` to define an executable named 'cute_tutorial_sgemm_1'. The source file for this executable is 'sgemm_1.cu', indicating a CUDA implementation.  It creates an executable target within the Cutlass build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_sgemm_1\n  sgemm_1.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sublayouts with select<I...> in CuTe (C++)\nDESCRIPTION: This snippet demonstrates how to retrieve sublayouts from a parent layout using the `select<I...>` syntax. The code selects specific dimensions from the layout `a` which is initialized as `Layout<Shape<_2,_3,_5,_7>>`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/01_layout.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nLayout a   = Layout<Shape<_2,_3,_5,_7>>{};     // (2,3,5,7):(1,2,6,30)\nLayout a13 = select<1,3>(a);                   // (3,7):(2,30)\nLayout a01 = select<0,1,3>(a);                 // (2,3,7):(1,2,30)\nLayout a2  = select<2>(a);                     // (5):(6)\n```\n\n----------------------------------------\n\nTITLE: Creating Executable in CMake for CUTLASS Example\nDESCRIPTION: This CMake command creates an executable named '28_ampere_3xtf32_fast_accurate_tensorop_fprop' from the source file 'ampere_3xtf32_fast_accurate_tensorop_fprop.cu'. It utilizes a custom CMake function 'cutlass_example_add_executable', provided within the CUTLASS project, to handle the build process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  28_ampere_3xtf32_fast_accurate_tensorop_fprop\n  ampere_3xtf32_fast_accurate_tensorop_fprop.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory (CMake)\nDESCRIPTION: Adds the `thread` subdirectory to the build.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thread)\n```\n\n----------------------------------------\n\nTITLE: Linking NVRTC Library to Test Executable\nDESCRIPTION: This CMake command links the 'cutlass_nvrtc' library to the 'cutlass_test_unit_nvrtc_thread' executable.  This ensures that the executable has access to the necessary NVRTC functionality.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/nvrtc/thread/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(cutlass_test_unit_nvrtc_thread PRIVATE cutlass_nvrtc)\n```\n\n----------------------------------------\n\nTITLE: Creating CUTLASS Unit Test Executable with CMake\nDESCRIPTION: This CMake code adds a new executable target named `cutlass_test_unit_cute_msvc_compilation` to the CUTLASS project. The source file `tuple.cpp` is included in the compilation of this executable. This executable is meant to test the CUDA Template Library for Linear Algebra Subroutines (CUTLASS).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/cute/msvc_compilation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_test_unit_add_executable(\n  cutlass_test_unit_cute_msvc_compilation\n\n  tuple.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Add S8 Interleaved Convolution Test Executable (SM75)\nDESCRIPTION: This CMake code adds an executable for testing convolution operations with S8 interleaved input and output, and S32 accumulation on SM75 architecture, conditionally compiled based on the `CUTLASS_NVCC_MAX_ARCH` variable.  It includes source files for forward propagation with both S8 and S4 interleaved input types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif (CUTLASS_NVCC_MAX_ARCH GREATER_EQUAL 75)\n  # Conv2d - S8 interleaved input, S8 interleaved output, S32 accumulation\n  cutlass_test_unit_add_executable(\n    cutlass_test_unit_conv_device_tensorop_s32_interleaved  \n    conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu\n    conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Test Unit Library (Infra Lib)\nDESCRIPTION: This snippet defines another CUTLASS object library named 'cutlass_test_unit_infra_lib'. It includes the 'test_unit.cpp' source file. This library provides additional infrastructure specific to unit testing.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncutlass_add_library(\n  cutlass_test_unit_infra_lib\n  OBJECT\n  test_unit.cpp\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Test Command\nDESCRIPTION: This snippet defines a CMake variable 'TEST_COMMAND_00' that specifies the execution command for a row-major layout test with extent 16x16. This command likely sets up parameters for the test executable related to memory layout and data size.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/03_visualize_layout/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_COMMAND_00 RowMajor --extent=16,16)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories with CMake\nDESCRIPTION: This snippet uses the CMake command `add_subdirectory` to include the fprop, wgrad, and dgrad subdirectories in the build process. This command instructs CMake to process the CMakeLists.txt file located in each of the specified subdirectories. Each subdirectory presumably contains source code and build instructions for a specific part of the CUTLASS project related to gradient computation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/device_3x/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fprop)\nadd_subdirectory(wgrad)\nadd_subdirectory(dgrad)\n```\n\n----------------------------------------\n\nTITLE: Partitioning Tensors with TiledCopy (C++)\nDESCRIPTION: This code demonstrates how to use a `TiledCopy` instance to partition global and shared memory tensors. It retrieves a thread copy slice using `copy_a.get_slice(threadIdx.x)`, then partitions the global memory tensor `gA` using `partition_S` (source-tensor partitioning) and the shared memory tensor `sA` using `partition_D` (destination-tensor partitioning).  A fragment tensor `tArA` is allocated to hold the copied data.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/0x_gemm_tutorial.md#_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\n  ThrCopy thr_copy_a = copy_a.get_slice(threadIdx.x);\n  Tensor tAgA = thr_copy_a.partition_S(gA);            // (CPY,CPY_M,CPY_K,k)\n  Tensor tAsA = thr_copy_a.partition_D(sA);            // (CPY,CPY_M,CPY_K)\n  // Allocate registers same shape/layout as partitioned data\n  Tensor tArA = make_fragment_like(tAsA);              // (CPY,CPY_M,CPY_K)\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License\nDESCRIPTION: This snippet is the BSD 3-Clause license, a permissive free software license. It details the conditions for redistribution and use of the software, including requirements for retaining copyright notices and disclaimers, and restrictions on using the copyright holder's name for endorsement.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cutlass_3x_design.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Defining tiled_copy Executable (CMake/CUDA)\nDESCRIPTION: This command uses a custom CMake macro `cutlass_example_add_executable` to define an executable named 'cute_tutorial_tiled_copy'. The source file for this executable is 'tiled_copy.cu', indicating a CUDA implementation.  It creates an executable target within the Cutlass build system.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/tutorial/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  cute_tutorial_tiled_copy\n  tiled_copy.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Building Quaternion GEMM Executable with CMake\nDESCRIPTION: This CMake code snippet uses the `cutlass_example_add_executable` macro to define how to build the `21_quaternion_gemm` executable. It specifies the target name and the source file (`quaternion_gemm.cu`). The macro handles the details of compiling and linking the source code to create the executable.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/21_quaternion_gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncutlass_example_add_executable(\n  21_quaternion_gemm\n  quaternion_gemm.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Install CUTLASS Python interface (Colab)\nDESCRIPTION: This snippet shows how to install the nvidia-cutlass package using pip. It is typically used in a Colab environment to install the CUTLASS Python interface before running the rest of the example. The `#` at the start of the line means the code is commented out, indicating it should be uncommented to run.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/python/04_epilogue_visitor.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!#pip install nvidia-cutlass\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This command adds the 'tutorial' subdirectory to the current CMake project. This allows CMake to process the CMakeLists.txt file located within the tutorial directory and include it in the overall build process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/cute/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(tutorial)\n```\n\n----------------------------------------\n\nTITLE: Setting Header Files to Check in CMake\nDESCRIPTION: Defines a CMake variable `header_files_to_check` which is a list of header files from the CUTLASS and CUTE libraries to verify for self-containment. This list includes headers from various subdirectories like `cutlass/gemm/kernel`, `cute/config.hpp`, `cute/algorithm`, `cute/container`, `cute/numeric`, `cute/util`, `cute/arch`, `cute/atom`, `cutlass/platform`, `cutlass/pipeline`, `cutlass/detail`, and `cutlass/layout`.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/self_contained_includes/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(header_files_to_check\n    # cutlass\n\n    # cutlass/gemm/kernel\n    cutlass/gemm/kernel/default_gemm.h\n    cutlass/gemm/kernel/default_gemm_complex.h\n    cutlass/gemm/kernel/gemm_universal_decl.h\n    # cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp\n\n    # cute\n    cute/config.hpp\n    cute/int_tuple.hpp\n    cute/layout.hpp\n    cute/layout_composed.hpp\n    cute/pointer.hpp\n    cute/pointer_base.hpp\n    cute/pointer_flagged.hpp\n    cute/pointer_swizzle.hpp\n    cute/stride.hpp\n    cute/swizzle.hpp\n    cute/swizzle_layout.hpp\n    cute/tensor.hpp\n    cute/tensor_impl.hpp\n    cute/tensor_predicate.hpp\n    cute/underscore.hpp\n    # cute/algorithm\n    cute/algorithm/axpby.hpp\n    cute/algorithm/clear.hpp\n    # cute/algorithm/cooperative_copy.hpp\n    cute/algorithm/cooperative_gemm.hpp\n    # cute/algorithm/copy.hpp\n    cute/algorithm/fill.hpp\n    cute/algorithm/functional.hpp\n    # cute/algorithm/gemm.hpp\n    cute/algorithm/prefer.hpp\n    # cute/algorithm/prefetch.hpp\n    cute/algorithm/tensor_algorithms.hpp\n    cute/algorithm/tuple_algorithms.hpp\n\n    # cute/container\n    cute/container/alignment.hpp\n    cute/container/array.hpp\n    cute/container/array_aligned.hpp\n    cute/container/array_subbyte.hpp\n    cute/container/bit_field.hpp\n    cute/container/cuda_types.hpp\n    cute/container/tuple.hpp\n    cute/container/type_list.hpp\n\n    # cute/numeric\n    cute/numeric/arithmetic_tuple.hpp\n    cute/numeric/complex.hpp\n    cute/numeric/int.hpp\n    cute/numeric/integer_sequence.hpp\n    cute/numeric/integral_ratio.hpp\n    cute/numeric/math.hpp\n    cute/numeric/numeric_types.hpp\n    cute/numeric/real.hpp\n    cute/numeric/integral_constant.hpp\n\n    # cute/util\n    cute/util/debug.hpp\n    cute/util/print.hpp\n    cute/util/type_traits.hpp\n    # cute/arch\n    cute/arch/cluster_sm90.hpp\n    cute/arch/copy.hpp\n    cute/arch/copy_sm50.hpp\n    cute/arch/copy_sm75.hpp\n    cute/arch/copy_sm80.hpp\n    cute/arch/copy_sm90.hpp\n    cute/arch/copy_sm90_desc.hpp\n    cute/arch/copy_sm90_tma.hpp\n    cute/arch/mma_sm61.hpp\n    cute/arch/mma_sm70.hpp\n    cute/arch/mma_sm75.hpp\n    cute/arch/mma_sm80.hpp\n    cute/arch/mma_sm90.hpp\n    cute/arch/mma_sm90_desc.hpp\n    cute/arch/mma_sm90_gmma.hpp\n    cute/arch/mma.hpp\n    cute/arch/util.hpp\n\n    cute/arch/cluster_sm100.hpp\n    cute/arch/copy_sm100.hpp\n    cute/arch/copy_sm100_tma.hpp\n    cute/arch/mma_sm100.hpp\n    cute/arch/mma_sm100_desc.hpp\n    cute/arch/mma_sm100_umma.hpp\n    # cute/arch/tmem_allocator_sm100.hpp\n\n    # cute/atom\n    # cute/atom/copy_atom.hpp\n    # cute/atom/copy_traits.hpp\n    # cute/atom/copy_traits_sm50.hpp\n    # cute/atom/copy_traits_sm75.hpp\n    # cute/atom/copy_traits_sm80.hpp\n    # cute/atom/copy_traits_sm90.hpp\n    # cute/atom/copy_traits_sm90_im2col.hpp\n    # cute/atom/copy_traits_sm90_tma.hpp\n    # cute/atom/copy_traits_sm90_tma_swizzle.hpp\n    cute/atom/mma_atom.hpp\n    cute/atom/mma_traits.hpp\n    cute/atom/mma_traits_sm61.hpp\n    cute/atom/mma_traits_sm70.hpp\n    cute/atom/mma_traits_sm75.hpp\n    cute/atom/mma_traits_sm80.hpp\n    cute/atom/mma_traits_sm90.hpp\n    cute/atom/mma_traits_sm90_gmma.hpp\n\n    cute/atom/mma_traits_sm100.hpp\n    cute/atom/partitioner.hpp\n\n    # cutlass\n    cutlass/aligned_buffer.h\n    cutlass/array.h\n    cutlass/array_planar_complex.h\n    cutlass/array_subbyte.h\n    cutlass/barrier.h\n    cutlass/bfloat16.h\n    cutlass/blas3.h\n    cutlass/blas3_types.h\n    cutlass/block_striped.h\n    cutlass/cluster_launch.hpp\n    cutlass/complex.h\n    cutlass/constants.h\n    cutlass/coord.h\n    cutlass/core_io.h\n    cutlass/cuda_host_adapter.hpp\n    cutlass/cutlass.h\n    cutlass/device_kernel.h\n    cutlass/fast_math.h\n    cutlass/float8.h\n    # cutlass/floating_point_nvrtc.h\n    cutlass/functional.h\n    cutlass/gemm_coord.h\n    cutlass/gemm_coord.hpp\n    cutlass/half.h\n    cutlass/integer_subbyte.h\n    cutlass/kernel_hardware_info.h\n    cutlass/kernel_hardware_info.hpp\n    cutlass/kernel_launch.h\n    cutlass/matrix.h\n    cutlass/matrix_coord.h\n    cutlass/matrix_shape.h\n    cutlass/numeric_conversion.h\n    cutlass/numeric_size.h\n    cutlass/numeric_types.h\n    cutlass/pitch_linear_coord.h\n    cutlass/predicate_vector.h\n    cutlass/quaternion.h\n    cutlass/real.h\n    cutlass/relatively_equal.h\n    cutlass/semaphore.h\n    cutlass/subbyte_reference.h\n    cutlass/tensor_coord.h\n    cutlass/tensor_ref.h\n    cutlass/tensor_ref_planar_complex.h\n    cutlass/tensor_view.h\n    cutlass/tensor_view_planar_complex.h\n    cutlass/tfloat32.h\n    cutlass/trace.h\n    cutlass/uint128.h\n    cutlass/version.h\n    cutlass/wmma_array.h\n    cutlass/workspace.h\n    cutlass/exmy_base.h\n    cutlass/float_subbyte.h\n\n    # cutlass/platform\n    cutlass/platform/platform.h\n\n    # cutlass/pipeline\n    cutlass/pipeline/pipeline.hpp\n    cutlass/pipeline/sm90_pipeline.hpp\n\n    cutlass/pipeline/sm100_pipeline.hpp\n\n\n    # cutlass/detail\n    cutlass/detail/cluster.hpp\n    cutlass/detail/collective.hpp\n    cutlass/detail/dependent_false.hpp\n    cutlass/detail/helper_macros.hpp\n    cutlass/detail/layout.hpp\n    cutlass/detail/mma.hpp\n\n    cutlass/detail/sm100_blockscaled_layout.hpp\n\n\n    # cutlass/arch\n    cutlass/arch/arch.h\n    cutlass/arch/barrier.h\n    cutlass/arch/cache_operation.h\n    cutlass/arch/config.h\n    cutlass/arch/grid_dependency_control.h\n    cutlass/arch/memory.h\n    # cutlass/arch/memory_sm75.h\n    # cutlass/arch/memory_sm80.h\n    cutlass/arch/mma.h\n    # cutlass/arch/mma_sm50.h\n    # cutlass/arch/mma_sm60.h\n    # cutlass/arch/mma_sm61.h\n    # cutlass/arch/mma_sm70.h\n    # cutlass/arch/mma_sm75.h\n    # cutlass/arch/mma_sm80.h\n    # cutlass/arch/mma_sm89.h\n    # cutlass/arch/mma_sm90.h\n    cutlass/arch/mma_sparse_sm80.h\n    cutlass/arch/mma_sparse_sm89.h\n    # cutlass/arch/simd.h\n    # cutlass/arch/simd_sm60.h\n    # cutlass/arch/simd_sm61.h\n    cutlass/arch/reg_reconfig.h\n    cutlass/arch/wmma.h\n    # cutlass/arch/wmma_sm70.h\n    # cutlass/arch/wmma_sm72.h\n    # cutlass/arch/wmma_sm75.h\n    # cutlass/arch/wmma_sm80.h\n     # cutlass/layout\n    cutlass/layout/layout.h\n    cutlass/layout/matrix.h\n    cutlass/layout/permute.h\n    cutlass/layout/pitch_linear.h\n    cutlass/layout/tensor.h\n    cutlass/layout/tensor_op_multiplicand_sm70.h\n    cutlass/layout/tensor_op_multiplicand_sm75.h\n    cutlass/layout/tensor_op_multiplicand_sm80.h\n    cutlass/layout/vector.h\n)\n```\n\n----------------------------------------\n\nTITLE: Convolution 3D Weight Gradient Configuration\nDESCRIPTION: This configuration defines parameters for a 3D convolution weight gradient calculation. It specifies input and output dimensions, padding, stride, dilation, alpha/beta values, and hash values.  The parameters are formatted as conv3d wgrad_(input_dims)_(output_dims)_padl(pad_left)_padu(pad_up)_str(stride)_dil(dilation)_corr_alpha1_beta0 h_h_h_h hash1 hash2 hash3 hash4\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_wgrad_device_tensorop_sm90.txt#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nconv3d wgrad_(1,1,8,8,16)_(1,1,8,8,128)_padl(0,0,0)_padu(0,0,0)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 1021860383 2039847788 626177484 1510320256\nconv3d wgrad_(2,3,4,6,96)_(2,3,5,8,128)_padl(1,1,1)_padu(1,1,1)_str(1,1,1)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 2885338846 4129534099 955078878 481515417\nconv3d wgrad_(2,8,5,5,96)_(2,16,10,16,128)_padl(1,0,1)_padu(0,2,0)_str(2,2,3)_dil(1,1,1)_corr_alpha1_beta0 h_h_h_h 2554054572 1879264901 955078878 3349975418\nconv3d wgrad_(2,13,6,5,96)_(2,16,10,16,128)_padl(1,0,1)_padu(0,2,0)_str(1,1,1)_dil(2,2,3)_corr_alpha1_beta0 h_h_h_h 698054658 1879264901 955078878 4113825945\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectory for Narrow Precision Tests\nDESCRIPTION: This CMake command adds the `narrow_precision` subdirectory to the build process. This likely contains further tests or configurations related to computations using narrow precision data types.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/device/sm100_tensorop_gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(narrow_precision)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories and Dependencies (CMake)\nDESCRIPTION: This loop iterates through a list of subdirectories (`device`), adds each subdirectory, and adds dependencies to the custom targets (`cutlass_test_unit_conv` and `test_unit_conv`).  It essentially integrates unit tests present in specified subdirectories into the overall build process.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/conv/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(SUBDIR\n  device\n  )\n\n  add_subdirectory(${SUBDIR})\n  add_dependencies(cutlass_test_unit_conv cutlass_test_unit_conv_${SUBDIR})\n  add_dependencies(test_unit_conv test_unit_conv_${SUBDIR})\n\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Executable for SYR2K Grouped Operation - CUDA\nDESCRIPTION: This code snippet demonstrates how to add an executable named `38_syr2k_grouped` using the `cutlass_example_add_executable` macro. It compiles `syr2k_grouped.cu` to build the executable. This showcases how to integrate a SYR2K grouped computation within the CUTLASS framework and create a runnable CUDA application.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/38_syr2k_grouped/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ncutlass_example_add_executable(\n  38_syr2k_grouped\n  syr2k_grouped.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: CUTLASS Test Directory Structure\nDESCRIPTION: This snippet illustrates the hierarchical directory structure of the CUTLASS test suite. The tests mirror the organization of source files within the CUTLASS Template Library. Understanding the structure is key to locating and running specific tests.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/code_organization.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ntest/                        # unit tests for CUTLASS Template Library\n  unit/\n    arch/\n    core/\n    gemm/\n      device/\n      kernel/\n      thread/\n      threadblock/\n      warp/\n    reduction/\n      kernel/\n      thread/\n    transform/\n      threadblock/\n      *\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target with Dependencies in CMake\nDESCRIPTION: This snippet shows how to define a custom target in CMake and specify its dependencies. The `add_custom_target` command creates a target named `cutlass_test_unit_gemm`, which depends on the specified targets (`cutlass_test_unit_gemm_thread`, `cutlass_test_unit_gemm_warp`, `cutlass_test_unit_gemm_threadblock`, `cutlass_test_unit_gemm_device`).\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n  cutlass_test_unit_gemm\n  DEPENDS\n  cutlass_test_unit_gemm_thread\n  cutlass_test_unit_gemm_warp\n  cutlass_test_unit_gemm_threadblock\n  cutlass_test_unit_gemm_device\n  )\n```\n\n----------------------------------------\n\nTITLE: Copyright and Redistribution License\nDESCRIPTION: This snippet provides the copyright notice and the BSD-3-Clause license under which CUTLASS is distributed. It outlines the conditions for redistribution and use of the software in both source and binary forms, including the required disclaimers.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/CONTRIBUTORS.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Blockscaled GEMM Dispatch Policy CUDA\nDESCRIPTION: This header file defines the GEMM dispatch policies for collectives, kernel layers, and builders.  It's part of the CUTLASS 3.9 update and targets the Blackwell SM120 architecture with blockscaled data types, providing support for dense GEMM.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_0\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/gemm/dispatch_policy.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Blackwell SM120 Epilogue TMA WarpSpecialized CUDA\nDESCRIPTION: This header file defines the Blackwell SM120 epilogue using TMA (Tensor Memory Accelerator) with warp-specialized optimizations, supporting full EVT fusions within the CUTLASS library.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_4\n\nLANGUAGE: CUDA C++\nCODE:\n```\n#include \"cutlass/epilogue/fusion/sm120_visitor_store_tma_warpspecialized.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_4x4x5x128_3x3_256x3x6_pad_h0w0_stride_h1w1_dil_h1w1_corr_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1527509174 2779255425 1217669626 2313445327\n```\n\n----------------------------------------\n\nTITLE: Defining Conv2D Fprop Kernel Configuration\nDESCRIPTION: This snippet defines a Conv2D forward propagation (fprop) kernel configuration.  It specifies the input/output tensor layouts as hnhwc, kernel parameters like pad, stride, and dilation, and alpha/beta values for scaling. The subsequent numerical values represent specific configuration identifiers or parameters.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_tensorop_f32_sm80.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_1x224x224x8_111x111_64x5x5_pad_h1w1_stride_h2w2_dil_h1w1_conv_alpha1_beta0 hnhwc_hnhwc_hnhwc_f_f 961566038 259130347 2177645918 3715581640\n```\n\n----------------------------------------\n\nTITLE: Copy Arbitrary Subtile C++\nDESCRIPTION: This code copies an arbitrary subtile of a tensor from global memory to register memory. It utilizes tiling to divide the global memory tensor into subtiles, then iterates through the tiles to copy each one to a register tensor and performs an operation.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/cute/03_tensor.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nTensor gmem = make_tensor(ptr, make_shape(24, 16));         // (24,16)\n\nauto tiler         = Shape<_8,_4>{};                        // 8x4 tiler\n//auto tiler       = Tile<Layout<_8,_3>, Layout<_4,_2>>{};  // 8x4 tiler with stride-3 and stride-2\nTensor gmem_tiled  = zipped_divide(gmem, tiler);            // ((_8,_4),Rest)\nTensor rmem        = make_tensor_like(gmem_tiled(_, 0));    // ((_8,_4))\nfor (int j = 0; j < size<1>(gmem_tiled); ++j) {\n  copy(gmem_tiled(_, j), rmem);\n  do_something(rmem);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_1x233x35x48_233x35_24x7x5_pad_h3w2_stride_h1w1_dil_h1w1_corr_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 3916058745 443607763 3443985888 4252958697\n```\n\n----------------------------------------\n\nTITLE: Configuration String for conv2d wgrad (CUTLASS)\nDESCRIPTION: This line represents the configuration for a conv2d backward pass (wgrad) operation in CUTLASS.  It specifies parameters like input/output dimensions, kernel size, padding, stride, dilation, scaling factors (alpha/beta), data layout (fnhwc), and identifiers. The numerical values likely represent hash codes or identifiers associated with this particular configuration.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_device_simt.txt#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nconv2d wgrad_32x24x32x32_24x31_32x1x2_pad_h0w0_stride_h1w1_dil_h1w1_corr_alpha1_beta0 fnhwc_fnhwc_fnhwc_f_f 1089703540 1191155361 852881505 1729883415\n```\n\n----------------------------------------\n\nTITLE: Defining Small Problem Sizes (CMake)\nDESCRIPTION: Defines CMake variables for small problem sizes, specifying matrix dimensions (m, n) and setting iterations to 0 for correctness.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_SMALL --m=256 --n=128 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Configuration for 2D Convolution Forward Propagation\nDESCRIPTION: This snippet defines a configuration for a 2D convolution forward propagation (fprop) operation. It specifies input and output dimensions, kernel size, padding, stride, dilation, and numerical identifiers, along with alpha and beta parameters. The configuration seems to be used for testing or benchmarking convolution operations.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/test/unit/data/hashes/cached_results_cutlass_test_unit_conv_fprop_device_tensorop_sm90.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,8,8,64)_(96,1,1,64)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 f_f_f_f 854950536 3677150560 1408652759 1475171676\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(7,8,8,64)_(256,1,1,64)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 f_f_f_f 1066744638 2875614321 2252791511 3400916910\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,8,8,64)_(256,3,3,64)_padl(0,0)_padu(0,0)_str(1,1)_dil(1,1)_corr_alpha1_beta0 f_f_f_f 854950536 795739645 1545596299 4043117388\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,8,8,32)_(256,3,3,32)_padl(1,1)_padu(1,1)_str(1,1)_dil(1,1)_corr_alpha1_beta0 f_f_f_f 3453982556 2561845304 1899218706 1185670554\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,8,8,64)_(256,2,5,64)_padl(1,1)_padu(2,2)_str(1,1)_dil(1,1)_corr_alpha1_beta0 f_f_f_f 854950536 4046266669 893484124 4189127440\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,8,8,64)_(256,2,5,64)_padl(1,1)_padu(0,0)_str(2,3)_dil(1,1)_corr_alpha1_beta0 f_f_f_f 854950536 4046266669 592544256 2322571289\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,16,16,64)_(256,2,5,64)_padl(1,1)_padu(0,0)_str(1,1)_dil(2,3)_corr_alpha1_beta0 f_f_f_f 191074901 4046266669 1081688479 1745547693\n```\n\nLANGUAGE: text\nCODE:\n```\nconv2d fprop_(2,16,16,64)_(256,2,5,64)_padl(1,1)_padu(0,0)_str(2,3)_dil(2,3)_corr_alpha1_beta0 f_f_f_f 191074901 4046266669 1031384344 1016941631\n```\n\n----------------------------------------\n\nTITLE: Defining Scale Per Column Test Parameters (CMake)\nDESCRIPTION: Defines CMake variables for per-column scaling tests, specifying matrix dimensions (m, n, k, c), scaling mode, and setting iterations to 0.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/69_hopper_mixed_dtype_grouped_gemm/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_SCALE_PERCOL --m=4096 --n=5120 --k=8192 --c=8192 --mode=1 --iterations=0)\n```\n\n----------------------------------------\n\nTITLE: Copyright License Text\nDESCRIPTION: This snippet contains the BSD-3-Clause license text that governs the redistribution and use of the CUTLASS software. It outlines the conditions for using, modifying, and distributing the software in source and binary forms.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/40_cutlass_py/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License\nDESCRIPTION: This code snippet shows the BSD 3-Clause License used by NVIDIA CORPORATION & AFFILIATES for the CUTLASS project. It defines the terms for redistribution and use of the software, including conditions for source and binary forms, disclaimers, and limitations of liability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/examples/README.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n```\n\n----------------------------------------\n\nTITLE: BSD-3-Clause License Text\nDESCRIPTION: This snippet provides the full text of the BSD-3-Clause license, detailing the conditions for redistribution and use of the software. It includes clauses regarding copyright notice retention, disclaimer of warranties, and limitations of liability.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/media/docs/cpp/tile_iterator_concept.md#_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Building Subset Tensor Core GEMM Kernels\nDESCRIPTION: These commands demonstrate how to build a subset of Tensor Core GEMM kernels.  It targets NVIDIA Ampere and Turing architectures. The cmake command configures the build, specifying the target architectures and a wildcard-based kernel name pattern using `CUTLASS_LIBRARY_KERNELS`. The second command builds the profiler.\nSOURCE: https://github.com/nvidia/cutlass/blob/main/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*gemm_f16_*_nt_align8\n...\n$ make cutlass_profiler -j16\n```"
  }
]
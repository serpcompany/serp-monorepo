[
  {
    "owner": "apache",
    "repo": "druid-website",
    "content": "TITLE: Example groupBy Query Structure in Apache Druid\nDESCRIPTION: A complete example of a groupBy query in Apache Druid showing the structure with filtering, aggregations, and post-aggregations. This query groups data by country and device dimensions with specific filtering conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"dimensions\": [\"country\", \"device\"],\n  \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] },\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" },\n      { \"type\": \"or\", \n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" },\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" },\n    { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"avg_usage\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" },\n        { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"having\": {\n    \"type\": \"greaterThan\",\n    \"aggregation\": \"total_usage\",\n    \"value\": 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DataSchema for Wikipedia Data Ingestion in Apache Druid\nDESCRIPTION: This detailed example shows how to configure the dataSchema for ingesting Wikipedia data. It includes specifications for the data source, parser, metrics, granularity, and transformations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Raw Data Format for Druid Ingestion\nDESCRIPTION: This snippet shows an example of raw data representing packet/byte counts between source and destination IPs. It illustrates the format of data before ingestion into Druid, with timestamp, dimensions (srcIP, dstIP), and metrics (packets, bytes).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Ingestion Spec Structure in JSON for Apache Druid\nDESCRIPTION: This snippet shows the basic structure of an Apache Druid ingestion specification, which consists of dataSchema, ioConfig, and tuningConfig components. The dataSchema defines the structure of incoming data, ioConfig specifies data source and destination, and tuningConfig allows for performance tuning.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Example groupBy Query Structure in Druid\nDESCRIPTION: A complete example of a groupBy query showing all major components including dimensions, filters, aggregations, and post-aggregations. The query analyzes mobile carrier and device data with filters for AT&T carrier and Apple/Samsung devices.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"dimensions\": [\"country\", \"device\"],\n  \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] },\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" },\n      { \"type\": \"or\", \n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" },\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" },\n    { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"avg_usage\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" },\n        { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"having\": {\n    \"type\": \"greaterThan\",\n    \"aggregation\": \"total_usage\",\n    \"value\": 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Raw Data Format for Druid Ingestion\nDESCRIPTION: This snippet shows an example of raw data representing packet/byte counts between source and destination IPs. It illustrates the format of data before ingestion into Druid, with timestamp, dimensions (srcIP, dstIP), and metrics (packets, bytes).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Sample Network Traffic Data Format\nDESCRIPTION: Example raw data showing packet/byte counts between source and destination IPs, demonstrating the structure before rollup aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Sample Network Traffic Data Format\nDESCRIPTION: Example raw data showing packet/byte counts between source and destination IPs, demonstrating the structure before rollup aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Configuring a Native Batch Ingestion Task for Wikipedia Data in Druid\nDESCRIPTION: JSON configuration for Druid's native batch ingestion task that loads Wikipedia page edit data. The spec defines the data schema with dimensions, timestamp format, granularity settings, and input source configuration using a local file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Apache Druid\nDESCRIPTION: Complete example of an index_parallel task specification for batch data ingestion in Apache Druid. This configuration demonstrates how to set up parallel batch indexing for Wikipedia data, specifying data schema, metrics, granularity, parser configuration, I/O settings, and task tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    },\n    \"tuningconfig\": {\n        \"type\": \"index_parallel\",\n        \"maxNumSubTasks\": 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Ingestion Specification with tuningConfig in Apache Druid\nDESCRIPTION: The final and complete JSON ingestion specification for Apache Druid, including dataSchema, ioConfig, and tuningConfig sections. This configuration processes netflow data with defined dimensions, metrics, and segment parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Druid SQL\nDESCRIPTION: Demonstrates the full syntax for a Druid SQL query, including optional clauses like WITH, WHERE, GROUP BY, HAVING, ORDER BY, and LIMIT. This structure is used to query Druid datasources, which appear as tables in the 'druid' schema.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Executing groupBy Query in Apache Druid\nDESCRIPTION: Example of a groupBy query object in Apache Druid. It demonstrates how to structure the query with various components like dataSource, dimensions, filters, aggregations, and post-aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/groupbyquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"dimensions\": [\"country\", \"device\"],\n  \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] },\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" },\n      { \"type\": \"or\", \n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" },\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" },\n    { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"avg_usage\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" },\n        { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"having\": {\n    \"type\": \"greaterThan\",\n    \"aggregation\": \"total_usage\",\n    \"value\": 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Aggregator Example in Druid\nDESCRIPTION: Example implementation of JavaScript aggregator computing sum(log(x)*y) + 10 with specific aggregate, combine, and reset functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"sum(log(x)*y) + 10\",\n  \"fieldNames\": [\"x\", \"y\"],\n  \"fnAggregate\" : \"function(current, a, b)      { return current + (Math.log(a) * b); }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return partialA + partialB; }\",\n  \"fnReset\"     : \"function()                   { return 10; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Bloom Filter in Druid SQL\nDESCRIPTION: Uses the BLOOM_FILTER_TEST function to check if a value is contained in a base64 serialized bloom filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nBLOOM_FILTER_TEST(<expr>, <serialized-filter>)\n```\n\n----------------------------------------\n\nTITLE: Complete Druid Ingestion Task Specification with Tuning\nDESCRIPTION: A comprehensive JSON specification for a Druid index task that includes dataSchema, ioConfig, and tuningConfig. This complete specification defines dimensions, metrics, time granularity, input source, and segment size parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Configuration Directory Structure\nDESCRIPTION: Shows the recommended organization of Druid configuration files across different service directories and components.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: Configuring a Hadoop-based Batch Ingestion Task in Druid\nDESCRIPTION: A sample JSON configuration for a Hadoop-based batch ingestion task in Druid. This task processes Wikipedia data, specifying dimensions, metrics, granularity settings, and input paths. The configuration demonstrates how to set up a complete index_hadoop task with dataSchema, ioConfig, and tuningConfig sections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"hadoopyString\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"paths\" : \"/MyDirectory/example/wikipedia_data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\": \"hadoop\"\n    }\n  },\n  \"hadoopDependencyCoordinates\": <my_hadoop_version>\n}\n```\n\n----------------------------------------\n\nTITLE: TSV (Delimited) parseSpec Configuration for Druid Ingestion\nDESCRIPTION: Configuration for the parseSpec section in a Druid ingestion spec when using TSV or other delimited formats. Specifies the timestamp column, delimiter, column order, and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Native Batch Ingestion for Wikipedia Data in Apache Druid\nDESCRIPTION: This JSON specification defines a native batch ingestion task for loading Wikipedia page edit data into Apache Druid. It specifies the data source, parser configuration, dimensions, timestamp format, granularity settings, and input file location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Ingestion Spec Structure in JSON for Apache Druid\nDESCRIPTION: This snippet outlines the basic structure of an Apache Druid ingestion specification. It includes placeholders for the three main components: dataSchema, ioConfig, and tuningConfig.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Complete DataSchema for Druid Ingestion in JSON\nDESCRIPTION: A comprehensive example of a dataSchema configuration for Wikipedia data ingestion, including parser specification, metrics definition, and granularity settings. This shows how to define dimensions with different data types and various aggregation metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Index Task in Druid\nDESCRIPTION: Example configuration for a Hadoop-based indexing task in Druid. The task ingests Wikipedia data with specified dimensions and metrics, using a daily segment granularity. It includes data schema definition, input/output configuration, and Hadoop dependency settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"hadoopyString\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"paths\" : \"/MyDirectory/example/wikipedia_data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\": \"hadoop\"\n    }\n  },\n  \"hadoopDependencyCoordinates\": <my_hadoop_version>\n}\n```\n\n----------------------------------------\n\nTITLE: Rollup Operation Pseudocode\nDESCRIPTION: Pseudocode demonstrating the rollup operation that groups data by truncated timestamp, source IP, and destination IP while summing packets and bytes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/index.md#2025-04-09_snippet_3\n\nLANGUAGE: pseudocode\nCODE:\n```\nGROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Druid SQL via JDBC\nDESCRIPTION: Java code example demonstrating how to connect to Druid's SQL API using the Avatica JDBC driver. Shows connection establishment, query execution, and result processing with proper resource management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Complete DataSchema for Druid Ingestion in JSON\nDESCRIPTION: A comprehensive example of a dataSchema configuration for Wikipedia data ingestion, including parser specification, metrics definition, and granularity settings. This shows how to define dimensions with different data types and various aggregation metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Context Parameters in Apache Druid\nDESCRIPTION: This markdown table lists common query context parameters applicable to all queries in Apache Druid. It includes parameters for timeout, priority, caching, result formatting, and resource management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/query-context.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default                                 | description          |\n|-----------------|----------------------------------------|----------------------|\n|timeout          | `druid.server.http.defaultQueryTimeout`| Query timeout in millis, beyond which unfinished queries will be cancelled. 0 timeout means `no timeout`. To set the default timeout, see [Broker configuration](../configuration/index.html#broker) |\n|priority         | `0`                                    | Query Priority. Queries with higher priority get precedence for computational resources.|\n|queryId          | auto-generated                         | Unique identifier given to this query. If a query ID is set or known, this can be used to cancel the query |\n|useCache         | `true`                                 | Flag indicating whether to leverage the query cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Apache Druid (incubating) uses druid.broker.cache.useCache or druid.historical.cache.useCache to determine whether or not to read from the query cache |\n|populateCache    | `true`                                 | Flag indicating whether to save the results of the query to the query cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache or druid.historical.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|useResultLevelCache         | `true`                      | Flag indicating whether to leverage the result level cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useResultLevelCache to determine whether or not to read from the result-level query cache |\n|populateResultLevelCache    | `true`                      | Flag indicating whether to save the results of the query to the result level cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateResultLevelCache to determine whether or not to save the results of this query to the result-level query cache |\n|bySegment        | `false`                                | Return \"by segment\" results. Primarily used for debugging, setting it to `true` returns results associated with the data segment they came from |\n|finalize         | `true`                                 | Flag indicating whether to \"finalize\" aggregation results. Primarily used for debugging. For instance, the `hyperUnique` aggregator will return the full HyperLogLog sketch instead of the estimated cardinality when this flag is set to `false` |\n|chunkPeriod      | `P0D` (off)                            | At the Broker process level, long interval queries (of any type) may be broken into shorter interval queries to parallelize merging more than normal. Broken up queries will use a larger share of cluster resources, but, if you use groupBy \"v1, it may be able to complete faster as a result. Use ISO 8601 periods. For example, if this property is set to `P1M` (one month), then a query covering a year would be broken into 12 smaller queries. The broker uses its query processing executor service to initiate processing for query chunks, so make sure \"druid.processing.numThreads\" is configured appropriately on the broker. [groupBy queries](groupbyquery.html) do not support chunkPeriod by default, although they do if using the legacy \"v1\" engine. This context is deprecated since it's only useful for groupBy \"v1\", and will be removed in the future releases.|\n|maxScatterGatherBytes| `druid.server.http.maxScatterGatherBytes` | Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. This parameter can be used to further reduce `maxScatterGatherBytes` limit at query time. See [Broker configuration](../configuration/index.html#broker) for more details.|\n|maxQueuedBytes       | `druid.broker.http.maxQueuedBytes`        | Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to `maxScatterGatherBytes`, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled.|\n|serializeDateTimeAsLong| `false`       | If true, DateTime is serialized as long in the result returned by Broker and the data transportation between Broker and compute process|\n|serializeDateTimeAsLongInner| `false`  | If true, DateTime is serialized as long in the data transportation between Broker and compute process|\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Index Task in Apache Druid with JSON\nDESCRIPTION: A complete configuration example for the Local Index Task in Druid. This task is designed for smaller data sets and executes within the indexing service. It includes dataSchema configuration with parser, metrics, and granularity specifications, as well as ioConfig for data source definition and tuningConfig for performance optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"examples/indexing/\",\n        \"filter\" : \"wikipedia_data.json\"\n       }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 1000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Raw TSV (Tab-Delimited) Data Format for Druid\nDESCRIPTION: Example of tab-separated value (TSV) data for ingestion into Druid. Each line contains a complete record with values separated by tab characters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n2013-08-31T01:02:33Z\t\"Gypsy Danger\"\t\"en\"\t\"nuclear\"\t\"true\"\t\"true\"\t\"false\"\t\"false\"\t\"article\"\t\"North America\"\t\"United States\"\t\"Bay Area\"\t\"San Francisco\"\t57\t200\t-143\n2013-08-31T03:32:45Z\t\"Striker Eureka\"\t\"en\"\t\"speed\"\t\"false\"\t\"true\"\t\"true\"\t\"false\"\t\"wikipedia\"\t\"Australia\"\t\"Australia\"\t\"Cantebury\"\t\"Syndey\"\t459\t129\t330\n2013-08-31T07:11:21Z\t\"Cherno Alpha\"\t\"ru\"\t\"masterYi\"\t\"false\"\t\"true\"\t\"true\"\t\"false\"\t\"article\"\t\"Asia\"\t\"Russia\"\t\"Oblast\"\t\"Moscow\"\t123\t12\t111\n2013-08-31T11:58:39Z\t\"Crimson Typhoon\"\t\"zh\"\t\"triplets\"\t\"true\"\t\"false\"\t\"true\"\t\"false\"\t\"wikipedia\"\t\"Asia\"\t\"China\"\t\"Shanxi\"\t\"Taiyuan\"\t905\t5\t900\n2013-08-31T12:41:27Z\t\"Coyote Tango\"\t\"ja\"\t\"cancer\"\t\"true\"\t\"false\"\t\"true\"\t\"false\"\t\"wikipedia\"\t\"Asia\"\t\"Japan\"\t\"Kanto\"\t\"Tokyo\"\t1\t10\t-9\n```\n\n----------------------------------------\n\nTITLE: Rollup Operation Pseudocode\nDESCRIPTION: Pseudocode demonstrating the rollup operation that groups data by truncated timestamp, source IP, and destination IP while summing packets and bytes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/index.md#2025-04-09_snippet_3\n\nLANGUAGE: pseudocode\nCODE:\n```\nGROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)\n```\n\n----------------------------------------\n\nTITLE: Complete Druid Ingestion Task Specification in JSON\nDESCRIPTION: This snippet shows a complete Druid ingestion task specification, including dataSchema, ioConfig, and tuningConfig. It defines dimensions, metrics, granularity, and input source for processing netflow data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: TSV (Delimited) parseSpec Configuration for Druid Ingestion\nDESCRIPTION: Configuration for the parseSpec section in a Druid ingestion spec when using TSV or other delimited formats. Specifies the timestamp column, delimiter, column order, and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Query response JSON format for top Wikipedia pages\nDESCRIPTION: Example JSON response from a Druid SQL query showing the top 10 Wikipedia pages by edit count. Each record contains the page name and the number of edits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"page\": \"Wikipedia:Vandalismusmeldung\",\n    \"Edits\": 33\n  },\n  {\n    \"page\": \"User:Cyde/List of candidates for speedy deletion/Subpage\",\n    \"Edits\": 28\n  },\n  {\n    \"page\": \"Jeremy Corbyn\",\n    \"Edits\": 27\n  },\n  {\n    \"page\": \"Wikipedia:Administrators' noticeboard/Incidents\",\n    \"Edits\": 21\n  },\n  {\n    \"page\": \"Flavia Pennetta\",\n    \"Edits\": 20\n  },\n  {\n    \"page\": \"Total Drama Presents: The Ridonculous Race\",\n    \"Edits\": 18\n  },\n  {\n    \"page\": \"User talk:Dudeperson176123\",\n    \"Edits\": 18\n  },\n  {\n    \"page\": \"Wikipdia:Le Bistro/12 septembre 2015\",\n    \"Edits\": 18\n  },\n  {\n    \"page\": \"Wikipedia:In the news/Candidates\",\n    \"Edits\": 17\n  },\n  {\n    \"page\": \"Wikipedia:Requests for page protection\",\n    \"Edits\": 17\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Avro Hadoop Parser Configuration\nDESCRIPTION: Configuration for Hadoop-based batch ingestion using Avro format. Includes dataSchema, ioConfig, and tuningConfig settings with custom schema file specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",  \n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"\",\n      \"parser\" : {\n        \"type\" : \"avro_hadoop\",\n        \"parseSpec\" : {\n          \"format\": \"avro\",\n          \"timestampSpec\": <standard timestampSpec>,\n          \"dimensionsSpec\": <standard dimensionsSpec>,\n          \"flattenSpec\": <optional>\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\",\n        \"paths\" : \"\"\n      }\n    },\n    \"tuningConfig\" : {\n       \"jobProperties\" : {\n          \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending a Basic HTTP Query to Druid\nDESCRIPTION: Example of how to send a query to Druid servers using curl with JSON content. This sends an HTTP POST request to the queryable Druid processes with a JSON query file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/querying.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: JSON Format Parse Specification\nDESCRIPTION: Configuration for parsing JSON formatted data, specifying timestamp and dimension specifications\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"json\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"dimensionSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Query Structure\nDESCRIPTION: Demonstrates the complete syntax structure for Druid SQL queries, including optional clauses for WITH, SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY, LIMIT, and UNION ALL operations. Shows the basic query template that can be used for querying Druid datasources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Using Filtered Aggregation in Druid SQL\nDESCRIPTION: Demonstrates the syntax for filtered aggregation in Druid SQL, where aggregators can have filters applied to only aggregate specific rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nAGG(expr) FILTER(WHERE whereExpr)\n```\n\n----------------------------------------\n\nTITLE: Executing Native TopN Query in Apache Druid using JSON\nDESCRIPTION: This snippet demonstrates a native TopN query in Druid's JSON format. It retrieves the top 10 Wikipedia pages with the most edits on 2015-09-12.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using CASE Statement in Druid SQL\nDESCRIPTION: Illustrates the syntax for both simple and searched CASE statements in Druid SQL for conditional logic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCASE expr WHEN value1 THEN result1 [ WHEN value2 THEN result2 ... ] [ ELSE resultN ] END\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCASE WHEN boolean_expr1 THEN result1 [ WHEN boolean_expr2 THEN result2 ... ] [ ELSE resultN ] END\n```\n\n----------------------------------------\n\nTITLE: Executing Native TopN Query in Apache Druid using JSON\nDESCRIPTION: This snippet demonstrates a native TopN query in Druid's JSON format. It retrieves the top 10 Wikipedia pages with the most edits on 2015-09-12.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sending a Basic HTTP Query to Druid\nDESCRIPTION: Example of how to send a query to Druid servers using curl with JSON content. This sends an HTTP POST request to the queryable Druid processes with a JSON query file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/querying.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Index Task in Apache Druid\nDESCRIPTION: Complete configuration example for a Local Index Task that ingests Wikipedia data. Includes dataSchema configuration with parser settings, metrics specifications, granularity settings, IO configuration for local file ingestion, and tuning parameters for performance optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"examples/indexing/\",\n        \"filter\" : \"wikipedia_data.json\"\n       }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 1000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Services with Supervise Script\nDESCRIPTION: Command to start Druid services using the supervise script, which launches Zookeeper and all required Druid components.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n```\n\n----------------------------------------\n\nTITLE: Druid DataSource Configuration\nDESCRIPTION: DataSchema configuration specifying the datasource name for the ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Segment Statistics per Datasource in Druid SQL\nDESCRIPTION: SQL query that aggregates segment data by datasource, calculating total size, average size, average number of rows, and segment count. The query filters out zero values when calculating averages and orders results by total size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    datasource,\n    SUM(\"size\") AS total_size,\n    CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size,\n    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,\n    COUNT(*) AS num_segments\nFROM sys.segments\nGROUP BY 1\nORDER BY 2 DESC\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Flatten Spec in Apache Druid ParseSpec\nDESCRIPTION: A comprehensive example of configuring the JSON Flatten Spec within a parseSpec for Apache Druid. It demonstrates various field types, expressions, and the use of field discovery.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parseSpec\": {\n  \"format\": \"json\",\n  \"flattenSpec\": {\n    \"useFieldDiscovery\": true,\n    \"fields\": [\n      {\n        \"type\": \"root\",\n        \"name\": \"dim1\"\n      },\n      \"dim2\",\n      {\n        \"type\": \"path\",\n        \"name\": \"foo.bar\",\n        \"expr\": \"$.foo.bar\"\n      },\n      {\n        \"type\": \"root\",\n        \"name\": \"foo.bar\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"path-metric\",\n        \"expr\": \"$.nestmet.val\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-0\",\n        \"expr\": \"$.hello[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-4\",\n        \"expr\": \"$.hello[4]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"world-hey\",\n        \"expr\": \"$.world[0].hey\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"worldtree\",\n        \"expr\": \"$.world[1].tree\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"first-food\",\n        \"expr\": \"$.thing.food[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"second-food\",\n        \"expr\": \"$.thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"first-food-by-jq\",\n        \"expr\": \".thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"hello-total\",\n        \"expr\": \".hello | sum\"\n      }\n    ]\n  },\n  \"dimensionsSpec\" : {\n   \"dimensions\" : [],\n   \"dimensionsExclusions\": [\"ignore_me\"]\n  },\n  \"timestampSpec\" : {\n   \"format\" : \"auto\",\n   \"column\" : \"timestamp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Batch Ingestion Task in Druid\nDESCRIPTION: Example configuration for a Hadoop-based batch ingestion task in Druid. The task specification includes dataSchema for defining data source and parsing, ioConfig for input configuration, and tuningConfig for performance settings. This example ingests Wikipedia edit data with specified dimensions and metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"hadoopyString\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"paths\" : \"/MyDirectory/example/wikipedia_data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\": \"hadoop\"\n    }\n  },\n  \"hadoopDependencyCoordinates\": <my_hadoop_version>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Scan Query in Apache Druid\nDESCRIPTION: This JSON structure defines a Scan query in Apache Druid. It specifies the data source, time interval, result format, and limits for the query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/scan-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"scan\",\n   \"dataSource\": \"wikipedia\",\n   \"resultFormat\": \"list\",\n   \"columns\":[],\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"batchSize\":20480,\n   \"limit\":5\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Native Batch Ingestion Task for Wikipedia Data in Apache Druid\nDESCRIPTION: This JSON configuration specifies a native batch ingestion task for loading Wikipedia page edit data into Apache Druid. It defines the data source, parser settings, dimensions, timestamp column, granularity, and input file details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying SYS Schema for Segment Information (SQL)\nDESCRIPTION: Shows how to query the sys.segments table to retrieve information about segments for a specific datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Druid Ingestion Specification Structure in JSON\nDESCRIPTION: The basic structure of a Druid ingestion specification consisting of three main components: dataSchema (specifies schema), ioConfig (specifies data source/destination), and tuningConfig (specifies ingestion parameter tuning).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: JDBC Connection Example in Java\nDESCRIPTION: Code example showing how to connect to Druid using the Avatica JDBC driver, execute a query, and process the results. Includes connection property setup and proper resource management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: String Concatenation in Apache Druid SQL\nDESCRIPTION: Demonstrates various methods for concatenating strings in Druid SQL, including the || operator and the CONCAT function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nx || y\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCONCAT(expr, expr...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Compaction in Apache Druid\nDESCRIPTION: Example configuration for setting up compaction on a 'wikiticker' datasource. This represents the minimal required configuration for enabling compaction, where only the datasource name is specified.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSource\": \"wikiticker\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Task Progress Response JSON Structure\nDESCRIPTION: JSON response structure for the progress endpoint showing running, succeeded, failed, complete, and total task counts along with expected successes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"running\":10,\n  \"succeeded\":0,\n  \"failed\":0,\n  \"complete\":0,\n  \"total\":10,\n  \"expectedSucceeded\":10\n}\n```\n\n----------------------------------------\n\nTITLE: Kinesis Supervisor Specification Example\nDESCRIPTION: Sample JSON configuration for a Kinesis supervisor that defines the data schema, tuning parameters, and I/O configuration. This specification controls how data is ingested from a Kinesis stream into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kinesis\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kinesis\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kinesis\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"stream\": \"metrics\",\n    \"endpoint\": \"kinesis.us-east-1.amazonaws.com\",\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\",\n    \"recordsPerFetch\": 2000,\n    \"fetchDelayMillis\": 1000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Druid Ingestion Specification Structure in JSON\nDESCRIPTION: The basic structure of a Druid ingestion specification consisting of three main components: dataSchema (specifies schema), ioConfig (specifies data source/destination), and tuningConfig (specifies ingestion parameter tuning).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Segments for a Specific Datasource in Druid SQL\nDESCRIPTION: SQL query to retrieve all segments for a specific datasource (wikipedia) from the sys.segments table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Query in Apache Druid using cURL\nDESCRIPTION: This snippet demonstrates how to cancel a query in Apache Druid using its unique identifier. It sends a DELETE request to the Druid Broker or Router with the query ID in the URL path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/querying.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://host:port/druid/v2/abc123\"\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Query in Apache Druid using cURL\nDESCRIPTION: This snippet demonstrates how to cancel a query in Apache Druid using its unique identifier. It sends a DELETE request to the Druid Broker or Router with the query ID in the URL path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/querying.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://host:port/druid/v2/abc123\"\n```\n\n----------------------------------------\n\nTITLE: Executing TopN Query in Apache Druid\nDESCRIPTION: A complete example of a TopN query in Druid that returns the top 5 values from sample_dim dimension, sorted by count. The query includes filters, aggregations, and post-aggregations to calculate an average metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/topnquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_data\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"count\",\n  \"granularity\": \"all\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim1\",\n        \"value\": \"some_value\"\n      },\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim2\",\n        \"value\": \"some_other_val\"\n      }\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\": \"longSum\",\n      \"name\": \"count\",\n      \"fieldName\": \"count\"\n    },\n    {\n      \"type\": \"doubleSum\",\n      \"name\": \"some_metric\",\n      \"fieldName\": \"some_metric\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"arithmetic\",\n      \"name\": \"average\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"some_metric\",\n          \"fieldName\": \"some_metric\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"count\",\n          \"fieldName\": \"count\"\n        }\n      ]\n    }\n  ],\n  \"intervals\": [\n    \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Druid Compaction Task in JSON\nDESCRIPTION: JSON structure for defining a compaction task in Apache Druid. This template shows all available configuration parameters including type, id, dataSource, interval, dimensions, granularity settings, target size, tuning configuration, and context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"compact\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\": <interval to specify segments to be merged>,\n    \"dimensions\" <custom dimensionsSpec>,\n    \"keepSegmentGranularity\": <true or false>,\n    \"segmentGranularity\": <segment granularity after compaction>,\n    \"targetCompactionSizeBytes\": <target size of compacted segments>\n    \"tuningConfig\" <index task tuningConfig>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Druid Compaction Task in JSON\nDESCRIPTION: JSON structure for defining a compaction task in Apache Druid. This template shows all available configuration parameters including type, id, dataSource, interval, dimensions, granularity settings, target size, tuning configuration, and context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"compact\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\": <interval to specify segments to be merged>,\n    \"dimensions\" <custom dimensionsSpec>,\n    \"keepSegmentGranularity\": <true or false>,\n    \"segmentGranularity\": <segment granularity after compaction>,\n    \"targetCompactionSizeBytes\": <target size of compacted segments>\n    \"tuningConfig\" <index task tuningConfig>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Kafka Supervisor\nDESCRIPTION: JSON configuration for the Druid Kafka supervisor, specifying data schema, tuning config, and IO config for ingesting data from the 'wikipedia' Kafka topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"wikipedia\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"time\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"channel\",\n            \"cityName\",\n            \"comment\",\n            \"countryIsoCode\",\n            \"countryName\",\n            \"isAnonymous\",\n            \"isMinor\",\n            \"isNew\",\n            \"isRobot\",\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\",\n            { \"name\": \"added\", \"type\": \"long\" },\n            { \"name\": \"deleted\", \"type\": \"long\" },\n            { \"name\": \"delta\", \"type\": \"long\" }\n          ]\n        }\n      }\n    },\n    \"metricsSpec\" : [],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"DAY\",\n      \"queryGranularity\": \"NONE\",\n      \"rollup\": false\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"reportParseExceptions\": false\n  },\n  \"ioConfig\": {\n    \"topic\": \"wikipedia\",\n    \"replicas\": 2,\n    \"taskDuration\": \"PT10M\",\n    \"completionTimeout\": \"PT20M\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete DataSchema Configuration Example in JSON\nDESCRIPTION: A comprehensive example of a dataSchema configuration for ingesting Wikipedia data. Includes parser configuration, metrics specification, granularity settings, and dimension definitions with different data types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL TopN Query in Apache Druid\nDESCRIPTION: This snippet shows an SQL query equivalent to the native JSON TopN query. It retrieves the top 10 Wikipedia pages with the most edits on 2015-09-12 using Druid SQL syntax.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8082/druid/v2/sql\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop-based Batch Ingestion Task in Apache Druid\nDESCRIPTION: This JSON configuration defines a Hadoop-based batch ingestion task for Apache Druid. It specifies the data schema, input configuration, and tuning parameters for ingesting Wikipedia data into Druid using Hadoop.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"hadoopyString\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"paths\" : \"/MyDirectory/example/wikipedia_data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\": \"hadoop\"\n    }\n  },\n  \"hadoopDependencyCoordinates\": <my_hadoop_version>\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with Virtual Columns in Apache Druid (JSON)\nDESCRIPTION: This snippet demonstrates a scan query in Apache Druid that utilizes virtual columns. It shows how to define expression-based virtual columns that can be used as dimensions or inputs to aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/virtual-columns.md#2025-04-09_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n \"queryType\": \"scan\",\n \"dataSource\": \"page_data\",\n \"columns\":[],\n \"virtualColumns\": [\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\",\n      \"outputType\": \"STRING\"\n    },\n    {\n      \"type\": \"expression\",\n      \"name\": \"tripleWordCount\",\n      \"expression\": \"wordCount * 3\",\n      \"outputType\": \"LONG\"\n    }\n  ],\n \"intervals\": [\n   \"2013-01-01/2019-01-02\"\n ] \n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries in Apache Druid\nDESCRIPTION: This SQL snippet demonstrates the structure of a SELECT query in Druid SQL, including optional clauses for explanation, common table expressions, filtering, grouping, ordering, and limiting results. It also shows how to use UNION ALL to combine multiple queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Flags for Apache Druid\nDESCRIPTION: Recommended JVM configuration flags for Apache Druid instances, including memory settings, garbage collection logging, error handling for Out of Memory conditions, and encoding settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>\n-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\n-Dorg.jboss.logging.provider=slf4j\n-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger\n-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown\n-Dlog4j.shutdownHookEnabled=true\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCApplicationStoppedTime\n-XX:+PrintGCApplicationConcurrentTime\n-Xloggc:/var/logs/druid/historical.gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=50\n-XX:GCLogFileSize=10m\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/logs/druid/historical.hprof\n-XX:MaxDirectMemorySize=10240g\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Task Specification with Roll-up Configuration\nDESCRIPTION: Index task specification that defines the schema, roll-up settings, and ingestion parameters for processing the network flow data with minute-level granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"rollup-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"srcIP\",\n              \"dstIP\"\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"rollup-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with Virtual Columns in Apache Druid\nDESCRIPTION: This snippet demonstrates a scan query in Apache Druid that uses virtual columns. It creates two virtual columns: 'fooPage' concatenating 'foo' with the 'page' column, and 'tripleWordCount' multiplying the 'wordCount' column by 3.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/virtual-columns.md#2025-04-09_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n \"queryType\": \"scan\",\n \"dataSource\": \"page_data\",\n \"columns\":[],\n \"virtualColumns\": [\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\",\n      \"outputType\": \"STRING\"\n    },\n    {\n      \"type\": \"expression\",\n      \"name\": \"tripleWordCount\",\n      \"expression\": \"wordCount * 3\",\n      \"outputType\": \"LONG\"\n    }\n  ],\n \"intervals\": [\n   \"2013-01-01/2019-01-02\"\n ] \n}\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Ingestion Specification Structure in JSON\nDESCRIPTION: Demonstrates the top-level structure of a Druid ingestion specification with its three main components: dataSchema, ioConfig, and tuningConfig.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Supervisor Spec in JSON\nDESCRIPTION: Sample JSON configuration for a Kafka supervisor specification in Druid, including dataSchema, tuningConfig, and ioConfig sections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy Query in Apache Druid\nDESCRIPTION: This snippet describes various configuration properties for GroupBy queries in Apache Druid, including settings for merging dictionary size, disk storage, and strategy selection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_46\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxMergingDictionarySize`|Maximum amount of heap space (approximately) to use for the string dictionary during merging. When the dictionary exceeds this size, a spill to disk will be triggered.|100000000|\n|`druid.query.groupBy.maxOnDiskStorage`|Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling.|0 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Process Specification for Druid Stream Pull Ingestion\nDESCRIPTION: Example configuration for Druid's Realtime process specification file showing complete setup including dataSchema, ioConfig, and tuningConfig. This configuration demonstrates integration with Kafka 0.8 as a firehose source for Wikipedia data ingestion with specific metrics and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [{\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"added\",\n        \"fieldName\" : \"added\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"deleted\",\n        \"fieldName\" : \"deleted\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"delta\",\n        \"fieldName\" : \"delta\"\n      }],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\"\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"realtime\",\n      \"firehose\": {\n        \"type\": \"kafka-0.8\",\n        \"consumerProps\": {\n          \"zookeeper.connect\": \"localhost:2181\",\n          \"zookeeper.connection.timeout.ms\" : \"15000\",\n          \"zookeeper.session.timeout.ms\" : \"15000\",\n          \"zookeeper.sync.time.ms\" : \"5000\",\n          \"group.id\": \"druid-example\",\n          \"fetch.message.max.bytes\" : \"1048586\",\n          \"auto.offset.reset\": \"largest\",\n          \"auto.commit.enable\": \"false\"\n        },\n        \"feed\": \"wikipedia\"\n      },\n      \"plumber\": {\n        \"type\": \"realtime\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\" : \"realtime\",\n      \"maxRowsInMemory\": 1000000,\n      \"intermediatePersistPeriod\": \"PT10M\",\n      \"windowPeriod\": \"PT10M\",\n      \"basePersistDirectory\": \"\\/tmp\\/realtime\\/basePersist\",\n      \"rejectionPolicy\": {\n        \"type\": \"serverTime\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Flags for Apache Druid\nDESCRIPTION: Recommended JVM flags for Druid processes that optimize garbage collection, logging, memory management, and error handling. These flags help with timezone consistency, memory limit enforcement, and proper logging of GC events and crashes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>\n-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\n-Dorg.jboss.logging.provider=slf4j\n-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger\n-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown\n-Dlog4j.shutdownHookEnabled=true\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCApplicationStoppedTime\n-XX:+PrintGCApplicationConcurrentTime\n-Xloggc:/var/logs/druid/historical.gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=50\n-XX:GCLogFileSize=10m\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/logs/druid/historical.hprof\n-XX:MaxDirectMemorySize=10240g\n```\n\n----------------------------------------\n\nTITLE: Filtering on Numeric Range with Bound Filter in Apache Druid\nDESCRIPTION: This example demonstrates filtering on a numeric column to select values where 10  myFloatColumn < 20, using string representations of numbers with the numeric ordering option.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"bound\",\n  \"dimension\": \"myFloatColumn\",\n  \"ordering\": \"numeric\",\n  \"lower\": \"10\",\n  \"lowerStrict\": false,\n  \"upper\": \"20\",\n  \"upperStrict\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Lookup in Apache Druid\nDESCRIPTION: Example JSON configuration for a JDBC lookup that polls a database table for key-value pairs. Includes connection configuration, table definition, polling period, and timestamp column for incremental updates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"jdbc\",\n  \"namespace\":\"some_lookup\",\n  \"connectorConfig\":{\n    \"createTables\":true,\n    \"connectURI\":\"jdbc:mysql://localhost:3306/druid\",\n    \"user\":\"druid\",\n    \"password\":\"diurd\"\n  },\n  \"table\":\"some_lookup_table\",\n  \"keyColumn\":\"the_old_dim_value\",\n  \"valueColumn\":\"the_new_dim_value\",\n  \"tsColumn\":\"timestamp_column\",\n  \"pollPeriod\":600000\n}\n```\n\n----------------------------------------\n\nTITLE: Final Druid Ingestion Task Specification with Tuning Config in JSON\nDESCRIPTION: This snippet presents the complete Druid ingestion task specification, including dataSchema, ioConfig, and tuningConfig with maxRowsPerSegment parameter set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Regex parseSpec Configuration for Druid Ingestion\nDESCRIPTION: Configuration for the parseSpec section in a Druid ingestion spec when using a custom Regex parser. Specifies the timestamp column, pattern for parsing, and dimensions to extract.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"regex\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [<your_list_of_dimensions>]\n    },\n    \"columns\" : [<your_columns_here>],\n    \"pattern\" : <regex pattern for partitioning data>\n  }\n```\n\n----------------------------------------\n\nTITLE: Selecting Raw Data with Time Range in Druid SQL\nDESCRIPTION: SQL query that retrieves raw user and page data from Wikipedia edits within a specific one-hour time window, limited to 5 results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT user, page\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00'\nLIMIT 5\n```\n\n----------------------------------------\n\nTITLE: Sample Kinesis Supervisor Specification JSON\nDESCRIPTION: This JSON object represents a sample Kinesis supervisor specification. It includes dataSchema, tuningConfig, and ioConfig sections that define how Druid should ingest data from a Kinesis stream.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kinesis\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kinesis\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kinesis\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"stream\": \"metrics\",\n    \"endpoint\": \"kinesis.us-east-1.amazonaws.com\",\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\",\n    \"recordsPerFetch\": 2000,\n    \"fetchDelayMillis\": 1000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Datasource in Druid\nDESCRIPTION: Defines a query datasource used for nested groupBy queries. This type is specifically designed for groupBy operations and supports subquery-like functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/datasource.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"query\",\n\t\"query\": {\n\t\t\"type\": \"groupBy\",\n\t\t...\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Compaction Task Specification for Daily Granularity\nDESCRIPTION: This JSON specification defines a compaction task that changes the segment granularity to daily, further reducing the number of segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"segmentGranularity\": \"DAY\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Supervisor Spec in JSON\nDESCRIPTION: JSON configuration for a Druid Kafka supervisor that defines the schema, tuning parameters, and connection details for ingesting data from a Kafka topic named 'wikipedia'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"wikipedia\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"time\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"channel\",\n            \"cityName\",\n            \"comment\",\n            \"countryIsoCode\",\n            \"countryName\",\n            \"isAnonymous\",\n            \"isMinor\",\n            \"isNew\",\n            \"isRobot\",\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\",\n            { \"name\": \"added\", \"type\": \"long\" },\n            { \"name\": \"deleted\", \"type\": \"long\" },\n            { \"name\": \"delta\", \"type\": \"long\" }\n          ]\n        }\n      }\n    },\n    \"metricsSpec\" : [],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"DAY\",\n      \"queryGranularity\": \"NONE\",\n      \"rollup\": false\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"reportParseExceptions\": false\n  },\n  \"ioConfig\": {\n    \"topic\": \"wikipedia\",\n    \"replicas\": 2,\n    \"taskDuration\": \"PT10M\",\n    \"completionTimeout\": \"PT20M\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Apache Druid\nDESCRIPTION: This JSON configuration specifies a parallel index task for ingesting Wikipedia data into Apache Druid. It defines the data schema, input source, and tuning parameters for parallel processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    },\n    \"tuningconfig\": {\n        \"type\": \"index_parallel\",\n        \"maxNumSubTasks\": 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Native Batch Ingestion Task Specification in Druid\nDESCRIPTION: Full JSON specification for a Druid native batch ingestion task (index type). This combines the dataSchema with task-specific configuration to define a complete ingestion job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic TopN Query Structure in Druid\nDESCRIPTION: Example of a complete TopN query showing all major components including datasource, dimension, threshold, metrics, filters, aggregations and post-aggregations. This query returns top 5 values from sample_dim ordered by count metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_data\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"count\",\n  \"granularity\": \"all\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim1\",\n        \"value\": \"some_value\"\n      },\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim2\",\n        \"value\": \"some_other_val\"\n      }\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\": \"longSum\",\n      \"name\": \"count\",\n      \"fieldName\": \"count\"\n    },\n    {\n      \"type\": \"doubleSum\",\n      \"name\": \"some_metric\",\n      \"fieldName\": \"some_metric\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"arithmetic\",\n      \"name\": \"average\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"some_metric\",\n          \"fieldName\": \"some_metric\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"count\",\n          \"fieldName\": \"count\"\n        }\n      ]\n    }\n  ],\n  \"intervals\": [\n    \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query: Unique Users for Product A\nDESCRIPTION: Sample Druid query using thetaSketch aggregator to count unique users who visited product A.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"aggregations\": [\n    { \"type\": \"thetaSketch\", \"name\": \"unique_users\", \"fieldName\": \"user_id_sketch\" }\n  ],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\" },\n  \"intervals\": [ \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Ingested Data with Druid SQL in Apache Druid\nDESCRIPTION: A bash session showing how to use the Druid SQL command-line client (dsql) to query ingested data. The example demonstrates the result of a SELECT query on the ingested netflow data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"ingestion-tutorial\";\n\n\n __time                    bytes  cost  count  dstIP    dstPort  packets  protocol  srcIP    srcPort \n\n 2018-01-01T01:01:00.000Z   6000   4.9      3  2.2.2.2     3000       60  6         1.1.1.1     2000 \n 2018-01-01T01:02:00.000Z   9000  18.1      2  2.2.2.2     7000       90  6         1.1.1.1     5000 \n 2018-01-01T01:03:00.000Z   6000   4.3      1  2.2.2.2     7000       60  6         1.1.1.1     5000 \n 2018-01-01T02:33:00.000Z  30000  56.9      2  8.8.8.8     5000      300  17        7.7.7.7     4000 \n 2018-01-01T02:35:00.000Z  30000  46.3      1  8.8.8.8     5000      300  17        7.7.7.7     4000 \n\nRetrieved 5 rows in 0.12s.\n\ndsql> \n```\n\n----------------------------------------\n\nTITLE: Example groupBy Query Structure in Apache Druid\nDESCRIPTION: A complete example of a groupBy query object, demonstrating filtering by carrier and device make, with aggregations and post-aggregations for calculating total usage, data transfer, and average usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"dimensions\": [\"country\", \"device\"],\n  \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] },\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" },\n      { \"type\": \"or\", \n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" },\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" },\n    { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"avg_usage\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" },\n        { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"having\": {\n    \"type\": \"greaterThan\",\n    \"aggregation\": \"total_usage\",\n    \"value\": 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Flags for Apache Druid\nDESCRIPTION: Essential JVM flags configuration for optimal Druid performance, including timezone settings, memory management, garbage collection logging, and error handling parameters. These settings help with debugging, memory management, and system stability.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>\n-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\n-Dorg.jboss.logging.provider=slf4j\n-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger\n-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown\n-Dlog4j.shutdownHookEnabled=true\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCApplicationStoppedTime\n-XX:+PrintGCApplicationConcurrentTime\n-Xloggc:/var/logs/druid/historical.gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=50\n-XX:GCLogFileSize=10m\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/logs/druid/historical.hprof\n-XX:MaxDirectMemorySize=10240g\n```\n\n----------------------------------------\n\nTITLE: Complete Druid Ingestion Task Specification\nDESCRIPTION: Final complete ingestion task specification combining data schema, input configuration, and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Specification JSON Configuration\nDESCRIPTION: Sample JSON configuration for a Kafka supervisor that defines how data should be ingested from a Kafka topic into Druid. The specification includes data schema, metrics, granularity, tuning parameters, and Kafka connection details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Rolled-up Data Using Druid SQL\nDESCRIPTION: SQL query to retrieve the ingested and rolled-up data from the 'rollup-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"rollup-tutorial\";\n\n __time                    bytes   count  dstIP    packets  srcIP   \n\n 2018-01-01T01:01:00.000Z   35937      3  2.2.2.2      286  1.1.1.1 \n 2018-01-01T01:02:00.000Z  366260      2  2.2.2.2      415  1.1.1.1 \n 2018-01-01T01:03:00.000Z   10204      1  2.2.2.2       49  1.1.1.1 \n 2018-01-02T21:33:00.000Z  100288      2  8.8.8.8      161  7.7.7.7 \n 2018-01-02T21:35:00.000Z    2818      1  8.8.8.8       12  7.7.7.7 \n\nRetrieved 5 rows in 1.18s.\n```\n\n----------------------------------------\n\nTITLE: Basic TopN Query Structure in Apache Druid\nDESCRIPTION: Example of a complete TopN query in Apache Druid that returns the top 5 values of 'sample_dim' ordered by 'count'. The query includes filters, aggregations, post-aggregations, and specifies time intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_data\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"count\",\n  \"granularity\": \"all\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim1\",\n        \"value\": \"some_value\"\n      },\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim2\",\n        \"value\": \"some_other_val\"\n      }\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\": \"longSum\",\n      \"name\": \"count\",\n      \"fieldName\": \"count\"\n    },\n    {\n      \"type\": \"doubleSum\",\n      \"name\": \"some_metric\",\n      \"fieldName\": \"some_metric\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"arithmetic\",\n      \"name\": \"average\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"some_metric\",\n          \"fieldName\": \"some_metric\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"count\",\n          \"fieldName\": \"count\"\n        }\n      ]\n    }\n  ],\n  \"intervals\": [\n    \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Kafka Supervisor Specification JSON for Druid\nDESCRIPTION: This JSON represents a complete Kafka supervisor specification for Druid ingestion. It includes dataSchema configuration with parsing rules, metrics specifications, and granularity settings, as well as tuning and IO configurations for Kafka connectivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive DataSchema Configuration Example in JSON\nDESCRIPTION: Provides a complete example of a dataSchema configuration for Wikipedia data, including parser settings, metrics specification, and granularity configuration. This demonstrates how to define dimensions, metrics, and time granularity for ingested data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantile Post-Aggregator in Druid\nDESCRIPTION: Illustrates the JSON configuration for the quantilesDoublesSketchToQuantile post-aggregator, which returns an approximation of a value at a given fraction.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantile\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fraction\" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive DataSchema Configuration Example\nDESCRIPTION: Shows a complete example of a dataSchema configuration including parser settings, metrics specifications, and granularity settings for Wikipedia data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Process Spec File for Stream Pull Ingestion in Apache Druid\nDESCRIPTION: This JSON configuration defines a Realtime process spec file for ingesting Wikipedia data from Kafka into Apache Druid. It includes dataSchema, ioConfig, and tuningConfig sections to specify data structure, input/output, and performance settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-pull.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [{\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"added\",\n        \"fieldName\" : \"added\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"deleted\",\n        \"fieldName\" : \"deleted\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"delta\",\n        \"fieldName\" : \"delta\"\n      }],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\"\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"realtime\",\n      \"firehose\": {\n        \"type\": \"kafka-0.8\",\n        \"consumerProps\": {\n          \"zookeeper.connect\": \"localhost:2181\",\n          \"zookeeper.connection.timeout.ms\" : \"15000\",\n          \"zookeeper.session.timeout.ms\" : \"15000\",\n          \"zookeeper.sync.time.ms\" : \"5000\",\n          \"group.id\": \"druid-example\",\n          \"fetch.message.max.bytes\" : \"1048586\",\n          \"auto.offset.reset\": \"largest\",\n          \"auto.commit.enable\": \"false\"\n        },\n        \"feed\": \"wikipedia\"\n      },\n      \"plumber\": {\n        \"type\": \"realtime\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\" : \"realtime\",\n      \"maxRowsInMemory\": 1000000,\n      \"intermediatePersistPeriod\": \"PT10M\",\n      \"windowPeriod\": \"PT10M\",\n      \"basePersistDirectory\": \"\\/tmp\\/realtime\\/basePersist\",\n      \"rejectionPolicy\": {\n        \"type\": \"serverTime\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's loadList property. This is required to use the datasketches aggregators in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-extension.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache License 2.0 header included as an HTML comment block\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Defining SegmentWriteOutMediumFactory Configuration\nDESCRIPTION: Table documenting the configuration fields for SegmentWriteOutMediumFactory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|See [Additional Peon Configuration: SegmentWriteOutMediumFactory](../../configuration/index.html#segmentwriteoutmediumfactory) for explanation and available options.|yes|\n```\n\n----------------------------------------\n\nTITLE: Sample Wikipedia Page Edit Event in JSON\nDESCRIPTION: Example JSON object representing a Wikipedia page edit event from the sample dataset. The event contains metadata about the edit, including timestamp, user, page, and geographical information of the editor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/index.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0,\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Completion Report Endpoint in Apache Druid\nDESCRIPTION: HTTP GET request to retrieve the completion report for a finished ingestion task. The report contains statistics on processed rows and any parse exceptions that occurred.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop-based Batch Ingestion Task in Druid\nDESCRIPTION: A sample JSON configuration for a Hadoop-based ingestion task in Apache Druid. This configuration specifies a task that ingests Wikipedia data with defined dimensions, metrics, and granularity settings. It includes dataSchema, ioConfig and tuningConfig sections required for the task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"hadoopyString\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"paths\" : \"/MyDirectory/example/wikipedia_data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\": \"hadoop\"\n    }\n  },\n  \"hadoopDependencyCoordinates\": <my_hadoop_version>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Compaction Task in Apache Druid\nDESCRIPTION: This JSON structure defines the configuration for a compaction task in Apache Druid. It specifies various parameters such as the data source, interval, dimensions, and tuning options for segment merging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"compact\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\": <interval to specify segments to be merged>,\n    \"dimensions\" <custom dimensionsSpec>,\n    \"keepSegmentGranularity\": <true or false>,\n    \"segmentGranularity\": <segment granularity after compaction>,\n    \"targetCompactionSizeBytes\": <target size of compacted segments>\n    \"tuningConfig\" <index task tuningConfig>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Configuration Spec\nDESCRIPTION: Sample JSON configuration for a Kafka supervisor that defines the ingestion schema, tuning parameters, and IO configuration for Kafka data ingestion. Includes settings for data source, parser, metrics, granularity, and Kafka connection properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL TopN Query in Apache Druid\nDESCRIPTION: This SQL query is equivalent to the native JSON TopN query. It selects the top 10 Wikipedia pages with the most edits on 2015-09-12.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Example Data for Granularity Demonstration in Apache Druid\nDESCRIPTION: Sample JSON data with millisecond ingestion granularity used to demonstrate different aggregation granularities in Apache Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"AAA\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-01T01:02:33Z\", \"page\": \"BBB\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"}\n```\n\n----------------------------------------\n\nTITLE: Constructing TopN Query in Apache Druid\nDESCRIPTION: Example of a complete TopN query structure including data source specification, dimension selection, threshold setting, metrics, filters, aggregations and post-aggregations. Shows how to query for top 5 results with complex filtering and calculations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/topnquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_data\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"count\",\n  \"granularity\": \"all\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim1\",\n        \"value\": \"some_value\"\n      },\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim2\",\n        \"value\": \"some_other_val\"\n      }\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\": \"longSum\",\n      \"name\": \"count\",\n      \"fieldName\": \"count\"\n    },\n    {\n      \"type\": \"doubleSum\",\n      \"name\": \"some_metric\",\n      \"fieldName\": \"some_metric\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"arithmetic\",\n      \"name\": \"average\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"some_metric\",\n          \"fieldName\": \"some_metric\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"count\",\n          \"fieldName\": \"count\"\n        }\n      ]\n    }\n  ],\n  \"intervals\": [\n    \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing GroupBy Query with Daily Period in Pacific Timezone in Apache Druid\nDESCRIPTION: This query demonstrates a GroupBy operation with a daily period granularity set to Pacific timezone. It counts events grouped by language dimension across a wide time interval. The query shows how to properly format the period granularity parameter with timezone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Druid\nDESCRIPTION: Example configuration for a parallel batch indexing task in Druid. The spec includes dataSchema configuration with metrics, granularity settings, parser configuration for JSON data, and IO configuration for local file ingestion. The tuning config specifies maximum number of subtasks for parallel processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    },\n    \"tuningconfig\": {\n        \"type\": \"index_parallel\",\n        \"maxNumSubTasks\": 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Druid\nDESCRIPTION: Example configuration for a parallel batch indexing task in Druid. The spec includes dataSchema configuration with metrics, granularity settings, parser configuration for JSON data, and IO configuration for local file ingestion. The tuning config specifies maximum number of subtasks for parallel processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    },\n    \"tuningconfig\": {\n        \"type\": \"index_parallel\",\n        \"maxNumSubTasks\": 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Native Batch Ingestion Task Specification for Apache Druid\nDESCRIPTION: This snippet provides a complete native batch ingestion task specification ('index' type) that includes all necessary components: dataSchema, parser, metrics, and granularity specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DimensionsSpec with Custom Types in Apache Druid\nDESCRIPTION: This example demonstrates how to configure a dimensionsSpec with custom column types and bitmap indexing settings. It includes string, long, and float type dimensions, as well as disabling bitmap indexing for a specific column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Stream Parser with Schema Repo\nDESCRIPTION: Example configuration for Avro stream parser using schema repository for decoding Avro bytes. Includes parser configuration with schema repo decoder and parse specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/avro.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parser\" : {\n    \"type\" : \"avro_stream\",\n    \"avroBytesDecoder\" : {\n      \"type\" : \"schema_repo\",\n      \"subjectAndIdConverter\" : {\n        \"type\" : \"avro_1124\",\n        \"topic\" : \"${YOUR_TOPIC}\"\n      },\n      \"schemaRepository\" : {\n        \"type\" : \"avro_1124_rest_client\",\n        \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\"\n      }\n    },\n    \"parseSpec\" : {\n      \"format\": \"avro\",\n      \"timestampSpec\": <standard timestampSpec>,\n      \"dimensionsSpec\": <standard dimensionsSpec>,\n      \"flattenSpec\": <optional>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Authentication Chain\nDESCRIPTION: Example configuration showing how to enable Kerberos and HTTP Basic authenticators in the authentication chain.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/auth.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"kerberos\", \"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Configuration Spec\nDESCRIPTION: Sample JSON configuration for a Kafka supervisor that defines the ingestion schema, tuning parameters, and IO configuration for Kafka data ingestion. Includes settings for data source, parser, metrics, granularity, and Kafka connection properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Complete Native Batch Ingestion Task in Druid\nDESCRIPTION: This snippet shows a complete native batch ingestion task specification for Druid. It includes the task type, dataSchema, and all necessary configurations for ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Regex ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing data using regular expressions in Druid, including pattern matching and column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"regex\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [<your_list_of_dimensions>]\n    },\n    \"columns\" : [<your_columns_here>],\n    \"pattern\" : <regex pattern for partitioning data>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache License 2.0 header included as an HTML comment block\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Simple Consumer Firehose in Druid\nDESCRIPTION: JSON configuration for setting up a Kafka Simple Consumer firehose in Druid. Defines connection parameters including broker list, queue properties, partition settings, and topic information. This configuration enables data ingestion from Kafka topics into Druid's realtime processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/kafka-simple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"firehoseV2\": {\n    \"type\" : \"kafka-0.8-v2\",\n    \"brokerList\" :  [\"localhost:4443\"],\n    \"queueBufferLength\":10001,\n    \"resetOffsetToEarliest\":\"true\",\n    \"partitionIdList\" : [\"0\"],\n    \"clientId\" : \"localclient\",\n    \"feed\": \"wikipedia\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Worker Task Spec State in Apache Druid (JSON)\nDESCRIPTION: This JSON snippet demonstrates the response structure when querying the state of a specific worker task spec in Apache Druid. It includes the task specification, current status, and task history. The spec contains detailed information about the ingestion process, including data schema, input configuration, and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"spec\": {\n    \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\",\n    \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"context\": null,\n    \"inputSplit\": {\n      \"split\": \"/path/to/data/lineitem.tbl.5\"\n    },\n    \"ingestionSpec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"lineitem\",\n        \"parser\": {\n          \"type\": \"hadoopyString\",\n          \"parseSpec\": {\n            \"format\": \"tsv\",\n            \"delimiter\": \"|\",\n            \"timestampSpec\": {\n              \"column\": \"l_shipdate\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"l_orderkey\",\n                \"l_partkey\",\n                \"l_suppkey\",\n                \"l_linenumber\",\n                \"l_returnflag\",\n                \"l_linestatus\",\n                \"l_shipdate\",\n                \"l_commitdate\",\n                \"l_receiptdate\",\n                \"l_shipinstruct\",\n                \"l_shipmode\",\n                \"l_comment\"\n              ]\n            },\n            \"columns\": [\n              \"l_orderkey\",\n              \"l_partkey\",\n              \"l_suppkey\",\n              \"l_linenumber\",\n              \"l_quantity\",\n              \"l_extendedprice\",\n              \"l_discount\",\n              \"l_tax\",\n              \"l_returnflag\",\n              \"l_linestatus\",\n              \"l_shipdate\",\n              \"l_commitdate\",\n              \"l_receiptdate\",\n              \"l_shipinstruct\",\n              \"l_shipmode\",\n              \"l_comment\"\n            ]\n          }\n        },\n        \"metricsSpec\": [\n          {\n            \"type\": \"count\",\n            \"name\": \"count\"\n          },\n          {\n            \"type\": \"longSum\",\n            \"name\": \"l_quantity\",\n            \"fieldName\": \"l_quantity\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_extendedprice\",\n            \"fieldName\": \"l_extendedprice\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_discount\",\n            \"fieldName\": \"l_discount\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_tax\",\n            \"fieldName\": \"l_tax\",\n            \"expression\": null\n          }\n        ],\n        \"granularitySpec\": {\n          \"type\": \"uniform\",\n          \"segmentGranularity\": \"YEAR\",\n          \"queryGranularity\": {\n            \"type\": \"none\"\n          },\n          \"rollup\": true,\n          \"intervals\": [\n            \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n          ]\n        },\n        \"transformSpec\": {\n          \"filter\": null,\n          \"transforms\": []\n        }\n      },\n      \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"/path/to/data/\",\n          \"filter\": \"lineitem.tbl.5\",\n          \"parser\": null\n        },\n        \"appendToExisting\": false\n      },\n      \"tuningConfig\": {\n        \"type\": \"index_parallel\",\n        \"maxRowsPerSegment\": 5000000,\n        \"maxRowsInMemory\": 1000000,\n        \"maxTotalRows\": 20000000,\n        \"numShards\": null,\n        \"indexSpec\": {\n          \"bitmap\": {\n            \"type\": \"concise\"\n          },\n          \"dimensionCompression\": \"lz4\",\n          \"metricCompression\": \"lz4\",\n          \"longEncoding\": \"longs\"\n        },\n        \"maxPendingPersists\": 0,\n        \"reportParseExceptions\": false,\n        \"pushTimeout\": 0,\n        \"segmentWriteOutMediumFactory\": null,\n        \"maxNumSubTasks\": 4,\n        \"maxRetry\": 3,\n        \"taskStatusCheckPeriodMs\": 1000,\n        \"chatHandlerTimeout\": \"PT10S\",\n        \"chatHandlerNumRetries\": 5,\n        \"logParseExceptions\": false,\n        \"maxParseExceptions\": 2147483647,\n        \"maxSavedParseExceptions\": 0,\n        \"forceGuaranteedRollup\": false,\n        \"buildV9Directly\": true\n      }\n    }\n  },\n  \"currentStatus\": {\n    \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\",\n    \"type\": \"index_sub\",\n    \"createdTime\": \"2018-04-20T22:16:29.925Z\",\n    \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\",\n    \"statusCode\": \"RUNNING\",\n    \"duration\": -1,\n    \"location\": {\n      \"host\": null,\n      \"port\": -1,\n      \"tlsPort\": -1\n    },\n    \"dataSource\": \"lineitem\",\n    \"errorMsg\": null\n  },\n  \"taskHistory\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Using EXPLAIN PLAN FOR with a Druid SQL Query\nDESCRIPTION: Demonstrates how to use the EXPLAIN PLAN FOR prefix to see what native Druid query will be generated from a SQL query. The example shows a GROUP BY query with a time filter being translated into a TopN query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Ingestion Specification with Schema and Input Source\nDESCRIPTION: A complete ingestion specification that includes dataSchema with parser configuration, metrics specification, and granularity settings, along with the ioConfig for data input source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Complete Native Batch Ingestion Task in Druid\nDESCRIPTION: This snippet shows a complete native batch ingestion task specification for Druid. It includes the task type, dataSchema, and all necessary configurations for ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Ingestion Task for Wikipedia Data in JSON\nDESCRIPTION: This JSON configuration specifies the ingestion task for loading Wikipedia page edits data into Druid. It defines the data schema, input source, and tuning parameters for the ingestion process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Flatten Spec in Apache Druid ParseSpec\nDESCRIPTION: This snippet demonstrates how to configure the JSON Flatten Spec within the parseSpec of an Apache Druid ingestion task. It includes field discovery settings and detailed field specifications for flattening nested JSON structures.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/flatten-json.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parseSpec\": {\n  \"format\": \"json\",\n  \"flattenSpec\": {\n    \"useFieldDiscovery\": true,\n    \"fields\": [\n      {\n        \"type\": \"root\",\n        \"name\": \"dim1\"\n      },\n      \"dim2\",\n      {\n        \"type\": \"path\",\n        \"name\": \"foo.bar\",\n        \"expr\": \"$.foo.bar\"\n      },\n      {\n        \"type\": \"root\",\n        \"name\": \"foo.bar\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"path-metric\",\n        \"expr\": \"$.nestmet.val\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-0\",\n        \"expr\": \"$.hello[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-4\",\n        \"expr\": \"$.hello[4]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"world-hey\",\n        \"expr\": \"$.world[0].hey\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"worldtree\",\n        \"expr\": \"$.world[1].tree\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"first-food\",\n        \"expr\": \"$.thing.food[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"second-food\",\n        \"expr\": \"$.thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"first-food-by-jq\",\n        \"expr\": \".thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"hello-total\",\n        \"expr\": \".hello | sum\"\n      }\n    ]\n  },\n  \"dimensionsSpec\" : {\n   \"dimensions\" : [],\n   \"dimensionsExclusions\": [\"ignore_me\"]\n  },\n  \"timestampSpec\" : {\n   \"format\" : \"auto\",\n   \"column\" : \"timestamp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Segment Statistics in Druid System Schema\nDESCRIPTION: SQL query to analyze segment statistics including average number of rows, size, and total counts from Druid's system schema. Helps determine if segment compaction is necessary by examining published segments for a specific datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/segment-optimization.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  \"start\",\n  \"end\",\n  version,\n  COUNT(*) AS num_segments,\n  AVG(\"num_rows\") AS avg_num_rows,\n  SUM(\"num_rows\") AS total_num_rows,\n  AVG(\"size\") AS avg_size,\n  SUM(\"size\") AS total_size\nFROM\n  sys.segments A\nWHERE\n  datasource = 'your_dataSource' AND\n  is_published = 1\nGROUP BY 1, 2, 3\nORDER BY 1, 2, 3 DESC;\n```\n\n----------------------------------------\n\nTITLE: Example JSON Data Structure in Druid\nDESCRIPTION: A sample JSON event structure showing various data types including nested objects, arrays, and mixed data types that can be flattened during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/flatten-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"timestamp\": \"2015-09-12T12:10:53.155Z\",\n \"dim1\": \"qwerty\",\n \"dim2\": \"asdf\",\n \"dim3\": \"zxcv\",\n \"ignore_me\": \"ignore this\",\n \"metrica\": 9999,\n \"foo\": {\"bar\": \"abc\"},\n \"foo.bar\": \"def\",\n \"nestmet\": {\"val\": 42},\n \"hello\": [1.0, 2.0, 3.0, 4.0, 5.0],\n \"mixarray\": [1.0, 2.0, 3.0, 4.0, {\"last\": 5}],\n \"world\": [{\"hey\": \"there\"}, {\"tree\": \"apple\"}],\n \"thing\": {\"food\": [\"sandwich\", \"pizza\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Index Task in Apache Druid JSON\nDESCRIPTION: A complete JSON configuration example for a Local Index Task in Apache Druid. This task is designed for smaller data sets and executes within the indexing service. The example shows how to ingest Wikipedia data with specific schema, metrics, and tuning configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"examples/indexing/\",\n        \"filter\" : \"wikipedia_data.json\"\n       }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 1000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV Lookup in Apache Druid\nDESCRIPTION: Example of a namespaceParseSpec configuration for TSV lookup in Druid. It defines the format, columns, key column, value column, and custom delimiter for parsing TSV data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"tsv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\",\n  \"delimiter\": \"|\"\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries with curl to Druid API\nDESCRIPTION: Example of how to execute a SQL query against Druid's REST API using curl. The query is first defined in a JSON file and then sent via HTTP POST to the Druid broker endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ cat query.json\n{\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"}\n\n$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json\n[{\"TheCount\":24433}]\n```\n\n----------------------------------------\n\nTITLE: Constructing a Data Source Metadata Query in Druid\nDESCRIPTION: JSON structure for creating a Data Source Metadata query that retrieves information about the latest ingested event timestamp for a specific dataSource in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"dataSourceMetadata\",\n    \"dataSource\": \"sample_datasource\"\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting a Search Query in Apache Druid\nDESCRIPTION: Shows the structure of a search query that returns dimension values matching a search specification. This example searches for values containing 'Ke' in dimensions 'dim1' and 'dim2' with lexicographic sorting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/searchquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"search\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"searchDimensions\": [\n    \"dim1\",\n    \"dim2\"\n  ],\n  \"query\": {\n    \"type\": \"insensitive_contains\",\n    \"value\": \"Ke\"\n  },\n  \"sort\" : {\n    \"type\": \"lexicographic\"\n  },\n  \"intervals\": [\n    \"2013-01-01T00:00:00.000/2013-01-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Caching Properties\nDESCRIPTION: Configuration properties for enabling and controlling caching behavior on the Broker node, including cache population, result-level caching, and cache limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_35\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.cache.useCache=false\ndruid.broker.cache.populateCache=false\ndruid.broker.cache.useResultLevelCache=false\ndruid.broker.cache.populateResultLevelCache=false\ndruid.broker.cache.resultLevelCacheLimit=Integer.MAX_VALUE\ndruid.broker.cache.unCacheable=[\"groupBy\", \"select\"]\ndruid.broker.cache.cacheBulkMergeLimit=Integer.MAX_VALUE\n```\n\n----------------------------------------\n\nTITLE: Executing topN Query with Multiple Aggregations in Apache Druid\nDESCRIPTION: This JSON query demonstrates a topN query in Apache Druid with multiple aggregations, filters, and a specific dimension. It includes sum aggregations for various fields, count aggregation, and an OR filter for specific order keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_TAX_doubleSum\",\n                 \"name\": \"L_TAX_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_DISCOUNT_doubleSum\",\n                 \"name\": \"L_DISCOUNT_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\",\n                 \"name\": \"L_EXTENDEDPRICE_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             },\n             {\n                 \"name\": \"count\",\n                 \"type\": \"count\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"filter\": {\n        \"fields\": [\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"103136\"\n            },\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"1648672\"\n            }\n        ],\n        \"type\": \"or\"\n    },\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Asynchronous Logging with Log4j2 for Apache Druid\nDESCRIPTION: This XML configuration reduces log verbosity by setting certain chatty Druid classes to write asynchronously. It creates AsyncLogger configurations for high-volume logging components like CuratorInventoryManager, BatchServerInventoryView, and ServerInventoryView to prevent performance degradation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/performance-faq.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <AsyncLogger name=\"org.apache.druid.curator.inventory.CuratorInventoryManager\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name=\"org.apache.druid.client.BatchServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->\n    <AsyncLogger name=\"org.apache.druid.client.ServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name =\"org.apache.druid.java.util.http.client.pool.ChannelResourceFactory\" level=\"info\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Example Expression Transform in Apache Druid\nDESCRIPTION: An example expression transform that prepends \"foo\" to the values of a page column and creates a new column named fooPage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Apache Druid TuningConfig Properties Table\nDESCRIPTION: Markdown table detailing TuningConfig properties, their descriptions, default values, and whether they are required. Includes parameters for controlling segment size, memory usage, persistence behavior, and task management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|property|description|default|required?|\n|--------|-----------|-------|----------|\n|type|The task type, this should always be `index_parallel`.|none|yes|\n|maxRowsPerSegment|Used in sharding. Determines how many rows are in each segment.|5000000|no|\n|maxRowsInMemory|Used in determining when intermediate persists to disk should occur. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set.|1000000|no|\n|maxBytesInMemory|Used in determining when intermediate persists to disk should occur. Normally this is computed internally and user does not need to set it. This value represents number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists)|1/6 of max JVM memory|no|\n|maxTotalRows|Total number of rows in segments waiting for being pushed. Used in determining when intermediate pushing should occur.|20000000|no|\n|numShards|Directly specify the number of shards to create. If this is specified and 'intervals' is specified in the granularitySpec, the index task can skip the determine intervals/partitions pass through the data. numShards cannot be specified if maxRowsPerSegment is set.|null|no|\n|indexSpec|defines segment storage format options to be used at indexing time, see [IndexSpec](#indexspec)|null|no|\n|maxPendingPersists|Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists).|0 (meaning one persist can be running concurrently with ingestion, and none can be queued up)|no|\n|forceExtendableShardSpecs|Forces use of extendable shardSpecs. Experimental feature intended for use with the [Kafka indexing service extension](../development/extensions-core/kafka-ingestion.html).|false|no|\n|reportParseExceptions|If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped.|false|no|\n|pushTimeout|Milliseconds to wait for pushing segments. It must be >= 0, where 0 means to wait forever.|0|no|\n|segmentWriteOutMediumFactory|Segment write-out medium to use when creating segments. See [SegmentWriteOutMediumFactory](#segmentWriteOutMediumFactory).|Not specified, the value from `druid.peon.defaultSegmentWriteOutMediumFactory.type` is used|no|\n|maxNumSubTasks|Maximum number of tasks which can be run at the same time. The supervisor task would spawn worker tasks up to `maxNumSubTasks` regardless of the available task slots. If this value is set to 1, the supervisor task processes data ingestion on its own instead of spawning worker tasks. If this value is set to too large, too many worker tasks can be created which might block other ingestion. Check [Capacity Planning](#capacity-planning) for more details.|1|no|\n|maxRetry|Maximum number of retries on task failures.|3|no|\n|taskStatusCheckPeriodMs|Polling period in milleseconds to check running task statuses.|1000|no|\n|chatHandlerTimeout|Timeout for reporting the pushed segments in worker tasks.|PT10S|no|\n|chatHandlerNumRetries|Retries for reporting the pushed segments in worker tasks.|5|no|\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose in Apache Druid\nDESCRIPTION: HttpFirehose configuration for reading data from remote sites via HTTP. This splittable firehose can be used by native parallel index tasks with each worker task reading a file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"http\",\n    \"uris\"  : [\"http://example.com/uri1\", \"http://example2.com/uri2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Scan Query Structure in Druid\nDESCRIPTION: Example of a basic scan query structure in Druid, showing essential parameters like queryType, dataSource, resultFormat, columns, intervals, batchSize and limit.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/scan-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"scan\",\n   \"dataSource\": \"wikipedia\",\n   \"resultFormat\": \"list\",\n   \"columns\":[],\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"batchSize\":20480,\n   \"limit\":5\n }\n```\n\n----------------------------------------\n\nTITLE: Retention Analysis Query Using Theta Sketches\nDESCRIPTION: Complex query demonstrating how to use Theta sketches for retention analysis, finding users who visited a product in one time period and also in another time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_1\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-08T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" : {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_2\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_1\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_2\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\"2014-10-01T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Segment Metadata Query in Druid JSON\nDESCRIPTION: A basic segment metadata query to retrieve information about segments in a specific date range. This query targets the 'sample_datasource' and specifies an interval covering the year 2013.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\":\"segmentMetadata\",\n  \"dataSource\":\"sample_datasource\",\n  \"intervals\":[\"2013-01-01/2014-01-01\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Process Specification File for Kafka Ingestion in Apache Druid\nDESCRIPTION: This JSON configuration example demonstrates how to set up a Realtime process in Apache Druid to ingest data from a Kafka stream. It includes the complete specification with dataSchema, ioConfig, and tuningConfig sections required for pulling data from a Kafka topic named 'wikipedia'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [{\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"added\",\n        \"fieldName\" : \"added\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"deleted\",\n        \"fieldName\" : \"deleted\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"delta\",\n        \"fieldName\" : \"delta\"\n      }],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\"\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"realtime\",\n      \"firehose\": {\n        \"type\": \"kafka-0.8\",\n        \"consumerProps\": {\n          \"zookeeper.connect\": \"localhost:2181\",\n          \"zookeeper.connection.timeout.ms\" : \"15000\",\n          \"zookeeper.session.timeout.ms\" : \"15000\",\n          \"zookeeper.sync.time.ms\" : \"5000\",\n          \"group.id\": \"druid-example\",\n          \"fetch.message.max.bytes\" : \"1048586\",\n          \"auto.offset.reset\": \"largest\",\n          \"auto.commit.enable\": \"false\"\n        },\n        \"feed\": \"wikipedia\"\n      },\n      \"plumber\": {\n        \"type\": \"realtime\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\" : \"realtime\",\n      \"maxRowsInMemory\": 1000000,\n      \"intermediatePersistPeriod\": \"PT10M\",\n      \"windowPeriod\": \"PT10M\",\n      \"basePersistDirectory\": \"\\/tmp\\/realtime\\/basePersist\",\n      \"rejectionPolicy\": {\n        \"type\": \"serverTime\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring OrderByColumnSpec in Druid GroupBy Queries\nDESCRIPTION: Specifies how to configure ordering conditions for columns in groupBy query results. Supports different dimension ordering types including lexicographic, alphanumeric, strlen, and numeric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/limitspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dimension\" : \"<Any dimension or metric name>\",\n    \"direction\" : <\"ascending\"|\"descending\">,\n    \"dimensionOrder\" : <\"lexicographic\"(default)|\"alphanumeric\"|\"strlen\"|\"numeric\">\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Basic Timeseries Query in Apache Druid\nDESCRIPTION: Example of a complete timeseries query showing all major components including datasource specification, granularity, filters, aggregations, and post-aggregations. The query processes data between 2012-01-01 and 2012-01-03 with day-level granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/timeseriesquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"descending\": \"true\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" },\n      { \"type\": \"or\",\n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" },\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"sample_divide\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" },\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Native Queries in Druid using curl with JSON Content-Type\nDESCRIPTION: Example of how to execute a native Druid query by making a POST request to a queryable host with the JSON content type. The query is loaded from a JSON file and the response is formatted for readability.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/querying.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: CSV Format Parse Specification\nDESCRIPTION: Configuration for parsing CSV formatted data, including column specifications and dimension details\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Network Flow Data Structure in JSON\nDESCRIPTION: Example of network flow data structure including fields like source/destination IP, ports, protocol, packets, bytes, and cost.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Compaction Task in Apache Druid (JSON)\nDESCRIPTION: JSON structure for specifying a compaction task in Apache Druid. It includes fields for task type, data source, interval, and various optional configurations such as custom dimension specifications and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"compact\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\": <interval to specify segments to be merged>,\n    \"dimensions\" <custom dimensionsSpec>,\n    \"keepSegmentGranularity\": <true or false>,\n    \"segmentGranularity\": <segment granularity after compaction>,\n    \"targetCompactionSizeBytes\": <target size of compacted segments>\n    \"tuningConfig\" <index task tuningConfig>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Rollup Pseudocode\nDESCRIPTION: SQL-like pseudocode showing the grouping and aggregation logic used in Druid's rollup operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/index.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nGROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose in Apache Druid\nDESCRIPTION: HttpFirehose configuration for reading data from remote sites via HTTP. This splittable firehose can be used by native parallel index tasks with each worker task reading a file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"http\",\n    \"uris\"  : [\"http://example.com/uri1\", \"http://example2.com/uri2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Joining Servers and Segments Tables\nDESCRIPTION: SQL query demonstrating how to join servers and segments tables to count segments per server for a specific datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(segments.segment_id) as num_segments from sys.segments as segments\nINNER JOIN sys.server_segments as server_segments\nON segments.segment_id  = server_segments.segment_id\nINNER JOIN sys.servers as servers\nON servers.server = server_segments.server\nWHERE segments.datasource = 'wikipedia'\nGROUP BY servers.server;\n```\n\n----------------------------------------\n\nTITLE: Extraction DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for extraction dimension specification that transforms dimension values using an extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : <dimension>,\n  \"outputName\" :  <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">,\n  \"extractionFn\" : <extraction_function>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Segment Identifier Example\nDESCRIPTION: Example showing the format of a Druid segment identifier with all components including datasource name, time interval, version number, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Configuration\nDESCRIPTION: Complete Supervisor specification JSON for configuring Kafka ingestion with Protobuf parsing in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka2\",\n    \"parser\": {\n      \"type\": \"protobuf\",\n      \"descriptor\": \"file:///tmp/metrics.desc\",\n      \"protoMessageType\": \"Metrics\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"unit\",\n            \"http_method\",\n            \"http_code\",\n            \"page\",\n            \"metricType\",\n            \"server\"\n          ],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics_pb\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Schema Avro Bytes Decoder in Druid\nDESCRIPTION: This snippet shows the configuration for an inline schema-based Avro bytes decoder. It includes the decoder type and a sample Avro schema definition within the configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"avroBytesDecoder\": {\n  \"type\": \"schema_inline\",\n  \"schema\": {\n    \"namespace\": \"org.apache.druid.data\",\n    \"name\": \"User\",\n    \"type\": \"record\",\n    \"fields\": [\n      { \"name\": \"FullName\", \"type\": \"string\" },\n      { \"name\": \"Country\", \"type\": \"string\" }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Timeseries Query in Apache Druid (JSON)\nDESCRIPTION: A complete example of a timeseries query object in JSON format. It includes specification of datasource, granularity, filtering conditions, aggregations, post-aggregations, and time intervals to query over.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"descending\": \"true\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" },\n      { \"type\": \"or\",\n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" },\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"sample_divide\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" },\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Input Structure\nDESCRIPTION: Example of a nested JSON document structure that can be flattened during ingestion, containing various data types including nested objects, arrays, and mixed-type arrays.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"timestamp\": \"2015-09-12T12:10:53.155Z\",\n \"dim1\": \"qwerty\",\n \"dim2\": \"asdf\",\n \"dim3\": \"zxcv\",\n \"ignore_me\": \"ignore this\",\n \"metrica\": 9999,\n \"foo\": {\"bar\": \"abc\"},\n \"foo.bar\": \"def\",\n \"nestmet\": {\"val\": 42},\n \"hello\": [1.0, 2.0, 3.0, 4.0, 5.0],\n \"mixarray\": [1.0, 2.0, 3.0, 4.0, {\"last\": 5}],\n \"world\": [{\"hey\": \"there\"}, {\"tree\": \"apple\"}],\n \"thing\": {\"food\": [\"sandwich\", \"pizza\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Configuration for Druid Brokers\nDESCRIPTION: Settings that control how Brokers process nested groupBy queries and long-interval queries. These configurations affect memory allocation, thread management, and parallel processing capabilities of the Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_40\n\nLANGUAGE: properties\nCODE:\n```\ndruid.processing.buffer.sizeBytes=auto (max 1GB)\ndruid.processing.buffer.poolCacheMaxCount=Integer.MAX_VALUE\ndruid.processing.formatString=processing-%s\ndruid.processing.numMergeBuffers=max(2, druid.processing.numThreads / 4)\ndruid.processing.numThreads=Number of cores - 1 (or 1)\ndruid.processing.columnCache.sizeBytes=0\ndruid.processing.fifo=false\ndruid.processing.tmpDir=path represented by java.io.tmpdir\n```\n\n----------------------------------------\n\nTITLE: Complex Query for Intersection of Unique Users Across Products\nDESCRIPTION: Advanced GroupBy query that uses filtered aggregators and post aggregations to find unique users who visited both product A and product B during a time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"},\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"B\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"A\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"B\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"B_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"B_unique_users\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\n    \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Regular Expression Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Regular Expression Extraction Function in Apache Druid. It allows extracting matching groups from dimension values using regex patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"regex\",\n  \"expr\" : <regular_expression>,\n  \"index\" : <group to extract, default 1>\n  \"replaceMissingValue\" : true,\n  \"replaceMissingValueWith\" : \"foobar\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ioConfig for Native Batch Ingestion in Apache Druid\nDESCRIPTION: This JSON snippet defines the ioConfig for a native batch ingestion task in Apache Druid. It specifies a local firehose to read input data from a file named 'ingestion-tutorial-data.json' in the 'quickstart/' directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"firehose\" : {\n    \"type\" : \"local\",\n    \"baseDir\" : \"quickstart/\",\n    \"filter\" : \"ingestion-tutorial-data.json\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with URI Extraction\nDESCRIPTION: JSON configuration for a globally cached lookup using a URI extraction namespace. This example shows how to set up a lookup that pulls data from a file source with CSV format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"uri\",\n       \"uri\": \"file:/tmp/prefix/\",\n       \"namespaceParseSpec\": {\n         \"format\": \"csv\",\n         \"columns\": [\n           \"key\",\n           \"value\"\n         ]\n       },\n       \"pollPeriod\": \"PT5M\"\n     },\n     \"firstCacheTimeout\": 0\n }\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Data Format for Druid Ingestion\nDESCRIPTION: Example of JSON-formatted data ready for ingestion by Apache Druid. Each line represents a separate event with timestamp and various dimensions and metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143\n}\n{\"timestamp\": \"2013-08-31T03:32:45Z\", \"page\": \"Striker Eureka\", \"language\" : \"en\", \"user\" : \"speed\", \"unpatrolled\" : \"false\", \"newPage\" : \"true\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Australia\", \"country\":\"Australia\", \"region\":\"Cantebury\", \"city\":\"Syndey\", \"added\": 459, \"deleted\": 129, \"delta\": 330\n}\n{\"timestamp\": \"2013-08-31T07:11:21Z\", \"page\": \"Cherno Alpha\", \"language\" : \"ru\", \"user\" : \"masterYi\", \"unpatrolled\" : \"false\", \"newPage\" : \"true\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"Asia\", \"country\":\"Russia\", \"region\":\"Oblast\", \"city\":\"Moscow\", \"added\": 123, \"deleted\": 12, \"delta\": 111\n}\n{\"timestamp\": \"2013-08-31T11:58:39Z\", \"page\": \"Crimson Typhoon\", \"language\" : \"zh\", \"user\" : \"triplets\", \"unpatrolled\" : \"true\", \"newPage\" : \"false\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Asia\", \"country\":\"China\", \"region\":\"Shanxi\", \"city\":\"Taiyuan\", \"added\": 905, \"deleted\": 5, \"delta\": 900\n}\n{\"timestamp\": \"2013-08-31T12:41:27Z\", \"page\": \"Coyote Tango\", \"language\" : \"ja\", \"user\" : \"cancer\", \"unpatrolled\" : \"true\", \"newPage\" : \"false\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Asia\", \"country\":\"Japan\", \"region\":\"Kanto\", \"city\":\"Tokyo\", \"added\": 1, \"deleted\": 10, \"delta\": -9\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL GroupBy Query in Apache Druid\nDESCRIPTION: This SQL query performs a GroupBy operation, summing the added lines for each Wikipedia channel and ordering the results by the sum in descending order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT channel, SUM(added) FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY channel ORDER BY SUM(added) DESC LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Constructing a Timeseries Query in Apache Druid\nDESCRIPTION: This JSON object demonstrates the structure of a timeseries query in Apache Druid. It includes various query parameters such as dataSource, granularity, filter, aggregations, and postAggregations. The query is designed to retrieve data from a sample datasource with specific filtering and aggregation criteria.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/timeseriesquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"descending\": \"true\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" },\n      { \"type\": \"or\",\n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" },\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"sample_divide\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" },\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Compaction Task in Apache Druid (JSON)\nDESCRIPTION: JSON structure for specifying a compaction task in Apache Druid. It includes fields for task type, data source, interval, and various optional configurations such as custom dimension specifications and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"compact\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\": <interval to specify segments to be merged>,\n    \"dimensions\" <custom dimensionsSpec>,\n    \"keepSegmentGranularity\": <true or false>,\n    \"segmentGranularity\": <segment granularity after compaction>,\n    \"targetCompactionSizeBytes\": <target size of compacted segments>\n    \"tuningConfig\" <index task tuningConfig>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Segment Identifier Example\nDESCRIPTION: Example showing the format of a Druid segment identifier with all components including datasource name, time interval, version number, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Task Specification with Roll-up Enabled\nDESCRIPTION: JSON configuration for ingesting the sample data with roll-up enabled, defining dimensions, metrics, and granularity settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"rollup-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"srcIP\",\n              \"dstIP\"\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"rollup-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Filter in Apache Druid Transform Spec\nDESCRIPTION: An example of a filter that selects only rows where the 'country' column has the value 'United States'. This filter is applied after any transforms have been processed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"country\",\n  \"value\": \"United States\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing an Expression Transform in Apache Druid\nDESCRIPTION: Syntax for defining an expression transform in Apache Druid. Expression transforms allow for creating new fields in the input rows using mathematical or string operations defined in an expression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <output field name>,\n  \"expression\": <expr>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing an Expression Transform in Apache Druid\nDESCRIPTION: Syntax for defining an expression transform in Apache Druid. Expression transforms allow for creating new fields in the input rows using mathematical or string operations defined in an expression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <output field name>,\n  \"expression\": <expr>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Index Task in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates how to set up a local index task in Apache Druid. It includes specifications for the data schema, IO configuration, and tuning parameters. The task is designed for ingesting smaller datasets directly within the indexing service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"examples/indexing/\",\n        \"filter\" : \"wikipedia_data.json\"\n       }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 1000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Native JSON TopN Query for Apache Druid\nDESCRIPTION: A native JSON query that retrieves the top 10 Wikipedia pages with the most edits on a specific date. It uses the topN query type with page dimension and count metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Response for a Segment Metadata Query in Apache Druid\nDESCRIPTION: This JSON snippet shows an example response from a segment metadata query in Apache Druid. It includes information about the segment ID, time intervals, column details, aggregators, query granularity, size, and number of rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"id\" : \"some_id\",\n  \"intervals\" : [ \"2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z\" ],\n  \"columns\" : {\n    \"__time\" : { \"type\" : \"LONG\", \"hasMultipleValues\" : false, \"size\" : 407240380, \"cardinality\" : null, \"errorMessage\" : null },\n    \"dim1\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : 1944, \"errorMessage\" : null },\n    \"dim2\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : true, \"size\" : 100000, \"cardinality\" : 1504, \"errorMessage\" : null },\n    \"metric1\" : { \"type\" : \"FLOAT\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : null, \"errorMessage\" : null }\n  },\n  \"aggregators\" : {\n    \"metric1\" : { \"type\" : \"longSum\", \"name\" : \"metric1\", \"fieldName\" : \"metric1\" }\n  },\n  \"queryGranularity\" : {\n    \"type\": \"none\"\n  },\n  \"size\" : 300000,\n  \"numRows\" : 5000000\n} ]\n```\n\n----------------------------------------\n\nTITLE: Search Query Response Format in Apache Druid\nDESCRIPTION: Sample response from a Druid search query showing results grouped by timestamp. Each result contains matching dimension values, the dimension name, and the count of occurrences for each value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/searchquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"Ke$ha\",\n        \"count\": 3\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"Ke$haForPresident\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"SomethingThatContainsKe\",\n        \"count\": 1\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"SomethingElseThatContainsKe\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with JDBC Extraction\nDESCRIPTION: JSON configuration for a globally cached lookup using a JDBC extraction namespace. This setup connects to a MySQL database to fetch lookup values, with options for filtering and timeout settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"jdbc\",\n       \"connectorConfig\": {\n         \"createTables\": true,\n         \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n         \"user\": \"druid\",\n         \"password\": \"diurd\"\n       },\n       \"table\": \"lookupTable\",\n       \"keyColumn\": \"mykeyColumn\",\n       \"valueColumn\": \"myValueColumn\",\n       \"filter\" : \"myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)\",\n       \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Injective Lookup in JSON\nDESCRIPTION: Example of an injective lookup configuration that maps unique keys to unique values. This type of lookup allows Druid to optimize query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n1 -> Foo\n2 -> Bar\n3 -> Billy\n}\n```\n\n----------------------------------------\n\nTITLE: Example GroupBy Query using Timestamp Min/Max Aggregators in Druid\nDESCRIPTION: Complete JSON query example showing how to use timeMin and timeMax aggregators in a GroupBy query to retrieve the precise min and max timestamps for each dimension value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"timeMinMax\",\n  \"granularity\": \"DAY\",\n  \"dimensions\": [\"product\"],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    },\n    {\n      \"type\": \"timeMin\",\n      \"name\": \"<output_name of timeMin>\",\n      \"fieldName\": \"tmin\"\n    },\n    {\n      \"type\": \"timeMax\",\n      \"name\": \"<output_name of timeMax>\",\n      \"fieldName\": \"tmax\"\n    }\n  ],\n  \"intervals\": [\n    \"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Druid GroupBy Query Example with Variance and Standard Deviation\nDESCRIPTION: Complete example of a Druid GroupBy query that calculates both variance and standard deviation metrics grouped by a dimension, demonstrating the use of the variance aggregator with the stddev post-aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Dimension Column Data Structures in Druid\nDESCRIPTION: Example showing the three data structures that represent a dimension column: a dictionary mapping values to integer IDs, the encoded column data, and bitmaps for each unique value indicating which rows contain that value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/segments.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   0,\n   1,\n   1]\n\n3: Bitmaps - one for each unique value of the column\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,0,1,1]\n```\n```\n\n----------------------------------------\n\nTITLE: Submitting Druid Kafka Supervisor Spec in Bash\nDESCRIPTION: cURL command to submit a Kafka supervisor specification to Druid's overlord, enabling Kafka ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Lookup Extractor in Apache Druid\nDESCRIPTION: JSON configuration for setting up a Kafka-based lookup extractor in Apache Druid. It specifies the Kafka topic and Zookeeper connection properties required for the lookup to function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"kafka\",\n  \"kafkaTopic\":\"testTopic\",\n  \"kafkaProperties\":{\"zookeeper.connect\":\"somehost:2181/kafka\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Moving Average Query in Apache Druid\nDESCRIPTION: This JSON query calculates a 7-bucket moving average for Wikipedia edit deltas with 30-minute granularity. It uses the longMean averager to compute the trailing average of changes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"movingAverage\",\n  \"dataSource\": \"wikipedia\",\n  \"granularity\": {\n    \"type\": \"period\",\n    \"period\": \"PT30M\"\n  },\n  \"intervals\": [\n    \"2015-09-12T00:00:00Z/2015-09-13T00:00:00Z\"\n  ],\n  \"aggregations\": [\n    {\n      \"name\": \"delta30Min\",\n      \"fieldName\": \"delta\",\n      \"type\": \"longSum\"\n    }\n  ],\n  \"averagers\": [\n    {\n      \"name\": \"trailing30MinChanges\",\n      \"fieldName\": \"delta30Min\",\n      \"type\": \"longMean\",\n      \"buckets\": 7\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a TopN Query in Apache Druid\nDESCRIPTION: This example demonstrates a basic TopN query in Apache Druid that finds the top 5 values for 'sample_dim' dimension based on the 'count' metric. The query includes filters, aggregations, and post-aggregations to calculate an average.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/topnquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_data\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"count\",\n  \"granularity\": \"all\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim1\",\n        \"value\": \"some_value\"\n      },\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim2\",\n        \"value\": \"some_other_val\"\n      }\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\": \"longSum\",\n      \"name\": \"count\",\n      \"fieldName\": \"count\"\n    },\n    {\n      \"type\": \"doubleSum\",\n      \"name\": \"some_metric\",\n      \"fieldName\": \"some_metric\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"arithmetic\",\n      \"name\": \"average\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"some_metric\",\n          \"fieldName\": \"some_metric\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"count\",\n          \"fieldName\": \"count\"\n        }\n      ]\n    }\n  ],\n  \"intervals\": [\n    \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting raw Wikipedia edit data in Druid SQL\nDESCRIPTION: SQL query that retrieves raw user and page data from Wikipedia edits within a one-hour time window. The LIMIT clause restricts the output to 5 records.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT user, page\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00'\nLIMIT 5\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Parser in Druid\nDESCRIPTION: JSON configuration example showing how to set up a JavaScript parser in Druid with timestamp specification and dimension handling. The parser includes a custom JavaScript function to split strings and return key-value pairs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"javascript\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    },\n    \"function\" : \"function(str) { var parts = str.split(\\\"-\\\"); return { one: parts[0], two: parts[1] } }\"\n  }\n```\n\n----------------------------------------\n\nTITLE: CSV parseSpec Configuration for Druid Ingestion\nDESCRIPTION: Configuration for the parseSpec section in a Druid ingestion spec when using CSV format. Specifies the timestamp column, all columns in order, and dimensions to extract.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Joining Servers and Segments Tables in Druid SQL\nDESCRIPTION: SQL query demonstrating how to join the servers, server_segments, and segments tables to count segments for a specific datasource grouped by server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(segments.segment_id) as num_segments from sys.segments as segments \nINNER JOIN sys.server_segments as server_segments \nON segments.segment_id  = server_segments.segment_id \nINNER JOIN sys.servers as servers \nON servers.server = server_segments.server\nWHERE segments.datasource = 'wikipedia' \nGROUP BY servers.server;\n```\n\n----------------------------------------\n\nTITLE: User Information Response with Full Role Details\nDESCRIPTION: Extended JSON response format when using the ?full flag, showing complete role and permission information\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"druid2\",\n  \"roles\": [\n    {\n      \"name\": \"druidRole\",\n      \"permissions\": [\n        {\n          \"resourceAction\": {\n            \"resource\": {\n              \"name\": \"A\",\n              \"type\": \"DATASOURCE\"\n            },\n            \"action\": \"READ\"\n          },\n          \"resourceNamePattern\": \"A\"\n        },\n        {\n          \"resourceAction\": {\n            \"resource\": {\n              \"name\": \"C\",\n              \"type\": \"CONFIG\"\n            },\n            \"action\": \"WRITE\"\n          },\n          \"resourceNamePattern\": \"C\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Requesting Header in Druid SQL Query Result (JSON)\nDESCRIPTION: Shows how to request a header in the query result by setting the 'header' parameter to true in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"arrayLines\",\n  \"header\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Requesting Header in Druid SQL Query Result (JSON)\nDESCRIPTION: Shows how to request a header in the query result by setting the 'header' parameter to true in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"arrayLines\",\n  \"header\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Grand Totals in Apache Druid Timeseries Query\nDESCRIPTION: This JSON object shows how to enable the grand totals feature in a timeseries query. By adding 'grandTotal': true to the context, Druid will include an extra row with the grand totals as the last row of the result set. This query retrieves data from a sample datasource and performs two aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/timeseriesquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Spatial Dimensions in Apache Druid JSON Data Spec\nDESCRIPTION: This snippet demonstrates how to specify spatial dimensions in a JSON data spec for Apache Druid. It shows the configuration for a Hadoop-based ingestion task with spatial dimensions defined in the dimensionsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/geo.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"hadoop\",\n\t\"dataSchema\": {\n\t\t\"dataSource\": \"DatasourceName\",\n\t\t\"parser\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"parseSpec\": {\n\t\t\t\t\"format\": \"json\",\n\t\t\t\t\"timestampSpec\": {\n\t\t\t\t\t\"column\": \"timestamp\",\n\t\t\t\t\t\"format\": \"auto\"\n\t\t\t\t},\n\t\t\t\t\"dimensionsSpec\": {\n\t\t\t\t\t\"dimensions\": [],\n\t\t\t\t\t\"spatialDimensions\": [{\n\t\t\t\t\t\t\"dimName\": \"coordinates\",\n\t\t\t\t\t\t\"dims\": [\"lat\", \"long\"]\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV ParseSpec in Apache Druid\nDESCRIPTION: This configuration snippet shows how to set up the parseSpec for ingesting TSV (tab-separated values) data in Druid. It specifies the format, timestamp column, column order, delimiter, and dimensions to be extracted from the data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring dataSource InputSpec for Hadoop Reindexing in Druid\nDESCRIPTION: Example configuration of a dataSource inputSpec within ioConfig for reindexing data from existing Druid segments using Hadoop. This allows processing data already stored in Druid for reindexing purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"dataSource\",\n    \"ingestionSpec\" : {\n      \"dataSource\": \"wikipedia\",\n      \"intervals\": [\"2014-10-20T00:00:00Z/P2W\"]\n    }\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Stream Parser with Schema Repo Decoder in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the Avro stream parser with a schema repo Avro bytes decoder. It includes settings for the parser type, bytes decoder, and parse specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"avro_stream\",\n  \"avroBytesDecoder\" : {\n    \"type\" : \"schema_repo\",\n    \"subjectAndIdConverter\" : {\n      \"type\" : \"avro_1124\",\n      \"topic\" : \"${YOUR_TOPIC}\"\n    },\n    \"schemaRepository\" : {\n      \"type\" : \"avro_1124_rest_client\",\n      \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\",\n    }\n  },\n  \"parseSpec\" : {\n    \"format\": \"avro\",\n    \"timestampSpec\": <standard timestampSpec>,\n    \"dimensionsSpec\": <standard dimensionsSpec>,\n    \"flattenSpec\": <optional>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Requesting Header in Druid SQL Query Result (JSON)\nDESCRIPTION: Shows how to request a header in the query result by setting the 'header' parameter to true in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"arrayLines\",\n  \"header\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Ingestion Task with Roll-up Enabled\nDESCRIPTION: A JSON specification for a Druid batch ingestion task that enables roll-up functionality. The task defines dimensions (srcIP, dstIP), metrics (count, packets, bytes), and sets minute-level query granularity with roll-up enabled to summarize data at ingestion time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"rollup-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"srcIP\",\n              \"dstIP\"\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"rollup-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV ParseSpec in Apache Druid\nDESCRIPTION: This configuration snippet shows how to set up the parseSpec for ingesting TSV (tab-separated values) data in Druid. It specifies the format, timestamp column, column order, delimiter, and dimensions to be extracted from the data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling On-heap Lookup Cache in JSON\nDESCRIPTION: Example configuration for a polling cache that updates its on-heap cache every 10 minutes. This configuration specifies a JDBC data fetcher connecting to a MySQL database with the relevant table and column information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"pollPeriod\":\"PT10M\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"onHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Simple Consumer Firehose in Apache Druid\nDESCRIPTION: JSON configuration for the Kafka Simple Consumer firehose (kafka-0.8-v2). This defines how Druid connects to Kafka brokers, handles message queues, manages offsets, and specifies partitions to consume from.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/kafka-simple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehoseV2\": {\n  \"type\" : \"kafka-0.8-v2\",\n  \"brokerList\" :  [\"localhost:4443\"],\n  \"queueBufferLength\":10001,\n  \"resetOffsetToEarliest\":\"true\",\n  \"partitionIdList\" : [\"0\"],\n  \"clientId\" : \"localclient\",\n  \"feed\": \"wikipedia\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics in Druid Data Schema\nDESCRIPTION: Demonstrates how to specify metrics within a Druid dataSchema using metricsSpec. This configuration defines count, longSum, and doubleSum aggregations for different columns during data rollup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }   \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Injective Lookup in JSON\nDESCRIPTION: Example of an injective lookup configuration that maps unique keys to unique values. This type of lookup allows Druid to optimize query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n1 -> Foo\n2 -> Bar\n3 -> Billy\n}\n```\n\n----------------------------------------\n\nTITLE: Using TIME_EXTRACT Function in Druid SQL\nDESCRIPTION: Extracts a time part from a timestamp expression, returning it as a number. The function accepts optional unit and timezone parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nTIME_EXTRACT(__time, 'HOUR', 'America/Los_Angeles')\n```\n\n----------------------------------------\n\nTITLE: JSON Format for SQL Queries with Context Parameters\nDESCRIPTION: Example JSON structure for submitting SQL queries to Druid with additional context parameters. This example shows how to set the time zone for SQL time functions via the sqlTimeZone context parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"context\" : {\n    \"sqlTimeZone\" : \"America/Los_Angeles\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Select Query Structure in Apache Druid\nDESCRIPTION: An example of a basic Select query in Druid that retrieves raw data from the 'wikipedia' datasource for a specific time interval with pagination threshold set to 5.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/select-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n {\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: KafkaSupervisorIOConfig Configuration Parameters\nDESCRIPTION: Configuration object that defines Kafka ingestion parameters including topic, consumer properties, polling timeouts, task counts, and message handling settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"topic\": \"kafka-topic-name\",\n  \"consumerProperties\": {\n    \"bootstrap.servers\": \"broker1:9092,broker2:9092\"\n  },\n  \"pollTimeout\": 100,\n  \"replicas\": 1,\n  \"taskCount\": 1,\n  \"taskDuration\": \"PT1H\",\n  \"startDelay\": \"PT5S\",\n  \"period\": \"PT30S\",\n  \"useEarliestOffset\": false,\n  \"completionTimeout\": \"PT30M\",\n  \"lateMessageRejectionPeriod\": null,\n  \"earlyMessageRejectionPeriod\": null,\n  \"skipOffsetGaps\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Uniform Granularity Spec Configuration Fields\nDESCRIPTION: JSON configuration fields for Uniform Granularity Spec which generates segments with uniform intervals. Includes fields for segment granularity, query granularity, rollup settings and intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"segmentGranularity\": \"string\",\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": \"JSON string array\"\n}\n```\n\n----------------------------------------\n\nTITLE: IN Filter Example in Druid\nDESCRIPTION: Filter for matching dimension values against a set of specified values, equivalent to SQL IN clause.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"in\",\n    \"dimension\": \"outlaw\",\n    \"values\": [\"Good\", \"Bad\", \"Ugly\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Redundancy in Druid Real-Time Processing\nDESCRIPTION: Demonstrates how to configure the shardSpec with type 'linear' and identical partitionNum values on different processes to achieve redundancy in Druid real-time ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: Group by multiple columns in Druid SQL\nDESCRIPTION: SQL query that groups Wikipedia data by both channel and page, calculating the sum of added content. Results are ordered by the sum of added content in descending order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT channel, page, SUM(added)\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'\nGROUP BY channel, page\nORDER BY SUM(added) DESC\n```\n\n----------------------------------------\n\nTITLE: Configuring Regex ParseSpec in Apache Druid\nDESCRIPTION: This configuration snippet demonstrates how to set up the parseSpec for ingesting data using a regex pattern in Druid. It specifies the format, timestamp column, dimensions, columns, and the regex pattern for parsing the data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"regex\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [<your_list_of_dimensions>]\n    },\n    \"columns\" : [<your_columns_here>],\n    \"pattern\" : <regex pattern for partitioning data>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ListFiltered DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a ListFiltered DimensionSpec in Apache Druid. It acts as a whitelist or blacklist for values in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"listFiltered\", \"delegate\" : <dimensionSpec>, \"values\": <array of strings>, \"isWhitelist\": <optional attribute for true/false, default is true> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Druid\nDESCRIPTION: Example configuration for a parallel index task in Druid that processes Wikipedia data. Demonstrates the task specification including dataSchema, ioConfig, and metrics configuration. The task is configured to process JSON data with multiple dimensions and metrics over a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Druid X-Large Single Server Deployment\nDESCRIPTION: Command to start Druid in x-large configuration, designed for machines with 64 CPU and 512GB RAM (equivalent to an i3.16xlarge instance).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/single-server.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-xlarge\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Spec with Transform Rules\nDESCRIPTION: Ingestion specification that defines how to transform and filter the input data, including expression transforms to modify the animal column and create a triple-number column, plus filtering rules.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"transform-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"animal\",\n              { \"name\": \"location\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"number\", \"fieldName\" : \"number\" },\n        { \"type\" : \"longSum\", \"name\" : \"triple-number\", \"fieldName\" : \"triple-number\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      },\n      \"transformSpec\": {\n        \"transforms\": [\n          {\n            \"type\": \"expression\",\n            \"name\": \"animal\",\n            \"expression\": \"concat('super-', animal)\"\n          },\n          {\n            \"type\": \"expression\",\n            \"name\": \"triple-number\",\n            \"expression\": \"number * 3\"\n          }\n        ],\n        \"filter\": {\n          \"type\":\"or\",\n          \"fields\": [\n            { \"type\": \"selector\", \"dimension\": \"animal\", \"value\": \"super-mongoose\" },\n            { \"type\": \"selector\", \"dimension\": \"triple-number\", \"value\": \"300\" },\n            { \"type\": \"selector\", \"dimension\": \"location\", \"value\": \"3\" }\n          ]\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"transform-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Type and Complete Specification\nDESCRIPTION: Demonstrates the complete task specification including dataSchema, parser configuration, metrics, and granularity settings for native batch ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Protobuf Message Definition\nDESCRIPTION: Protocol Buffer message definition for the metrics data structure using proto3 syntax.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\nmessage Metrics {\n  string unit = 1;\n  string http_method = 2;\n  int32 value = 3;\n  string timestamp = 4;\n  string http_code = 5;\n  string page = 6;\n  string metricType = 7;\n  string server = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to Druid SQL via JDBC in Java\nDESCRIPTION: Example Java code demonstrating how to connect to Druid's SQL API via the Avatica JDBC driver. The code establishes a connection, executes a query, and processes the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Ingesting JSON Data with Apache Druid\nDESCRIPTION: Example of raw JSON data format that Druid can natively ingest. Each line represents a complete record with timestamp, dimensions, and metrics in a denormalized format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143}\n{\"timestamp\": \"2013-08-31T03:32:45Z\", \"page\": \"Striker Eureka\", \"language\" : \"en\", \"user\" : \"speed\", \"unpatrolled\" : \"false\", \"newPage\" : \"true\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Australia\", \"country\":\"Australia\", \"region\":\"Cantebury\", \"city\":\"Syndey\", \"added\": 459, \"deleted\": 129, \"delta\": 330}\n{\"timestamp\": \"2013-08-31T07:11:21Z\", \"page\": \"Cherno Alpha\", \"language\" : \"ru\", \"user\" : \"masterYi\", \"unpatrolled\" : \"false\", \"newPage\" : \"true\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"Asia\", \"country\":\"Russia\", \"region\":\"Oblast\", \"city\":\"Moscow\", \"added\": 123, \"deleted\": 12, \"delta\": 111}\n{\"timestamp\": \"2013-08-31T11:58:39Z\", \"page\": \"Crimson Typhoon\", \"language\" : \"zh\", \"user\" : \"triplets\", \"unpatrolled\" : \"true\", \"newPage\" : \"false\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Asia\", \"country\":\"China\", \"region\":\"Shanxi\", \"city\":\"Taiyuan\", \"added\": 905, \"deleted\": 5, \"delta\": 900}\n{\"timestamp\": \"2013-08-31T12:41:27Z\", \"page\": \"Coyote Tango\", \"language\" : \"ja\", \"user\" : \"cancer\", \"unpatrolled\" : \"true\", \"newPage\" : \"false\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Asia\", \"country\":\"Japan\", \"region\":\"Kanto\", \"city\":\"Tokyo\", \"added\": 1, \"deleted\": 10, \"delta\": -9}\n```\n\n----------------------------------------\n\nTITLE: Protobuf Message Definition\nDESCRIPTION: Protocol Buffer message definition for metrics data structure using proto3 syntax\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\nmessage Metrics {\n  string unit = 1;\n  string http_method = 2;\n  int32 value = 3;\n  string timestamp = 4;\n  string http_code = 5;\n  string page = 6;\n  string metricType = 7;\n  string server = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Kafka Supervisor Specification in JSON\nDESCRIPTION: A complete example of a Kafka supervisor specification JSON, including dataSchema, tuningConfig, and ioConfig sections for ingesting metrics data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Data Source Metadata Query in Apache Druid\nDESCRIPTION: This JSON structure defines a Data Source Metadata query in Apache Druid. It specifies the query type and the data source to be queried. This query returns metadata information about the latest ingested event timestamp for the specified data source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"dataSourceMetadata\",\n    \"dataSource\": \"sample_datasource\"\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Small Configuration in Bash\nDESCRIPTION: Command to start Druid in the small configuration, which is designed for machines with 8 CPU cores and 64GB RAM, roughly equivalent to an Amazon i3.2xlarge instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/single-server.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-small\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving Average with Post Averager in Druid\nDESCRIPTION: Query that calculates a 7-bucket moving average of Wikipedia edit deltas over 30-minute periods, plus a ratio between current period and the moving average. Uses post averagers to perform arithmetic operations on calculated averages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"movingAverage\",\n  \"dataSource\": \"wikipedia\",\n  \"granularity\": {\n    \"type\": \"period\",\n    \"period\": \"PT30M\"\n  },\n  \"intervals\": [\n    \"2015-09-12T22:00:00Z/2015-09-13T00:00:00Z\"\n  ],\n  \"aggregations\": [\n    {\n      \"name\": \"delta30Min\",\n      \"fieldName\": \"delta\",\n      \"type\": \"longSum\"\n    }\n  ],\n  \"averagers\": [\n    {\n      \"name\": \"trailing30MinChanges\",\n      \"fieldName\": \"delta30Min\",\n      \"type\": \"longMean\",\n      \"buckets\": 7\n    }\n  ],\n  \"postAveragers\" : [\n    {\n      \"name\": \"ratioTrailing30MinChanges\",\n      \"type\": \"arithmetic\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"fieldName\": \"delta30Min\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"fieldName\": \"trailing30MinChanges\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Scan Query in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to structure a basic Scan query in Apache Druid. It includes essential properties such as queryType, dataSource, resultFormat, columns, intervals, batchSize, and limit.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/scan-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"scan\",\n   \"dataSource\": \"wikipedia\",\n   \"resultFormat\": \"list\",\n   \"columns\":[],\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"batchSize\":20480,\n   \"limit\":5\n }\n```\n\n----------------------------------------\n\nTITLE: Using Filtered Aggregation in Apache Druid SQL\nDESCRIPTION: Demonstrates how to apply a filter to an aggregation function in Druid SQL queries. This allows for conditional aggregation based on specified criteria.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nAGG(expr) FILTER(WHERE whereExpr)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Inline Schemas Avro Bytes Decoder in JSON\nDESCRIPTION: This JSON snippet demonstrates the configuration for a multiple inline schemas-based Avro bytes decoder. It includes the decoder type and a map of schema IDs to Avro schema definitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"multiple_schemas_inline\",\n  \"schemas\": {\n    \"1\": {\n      \"namespace\": \"org.apache.druid.data\",\n      \"name\": \"User\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"FullName\", \"type\": \"string\" },\n        { \"name\": \"Country\", \"type\": \"string\" }\n      ]\n    },\n    \"2\": {\n      \"namespace\": \"org.apache.druid.otherdata\",\n      \"name\": \"UserIdentity\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"Name\", \"type\": \"string\" },\n        { \"name\": \"Location\", \"type\": \"string\" }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Long Sum Aggregator in Druid\nDESCRIPTION: Aggregator that computes sum of values as 64-bit signed integers. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Extraction Filter Example in Druid\nDESCRIPTION: Extraction filter using lookup function to match specific product values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example groupBy Query with subtotalsSpec in Druid\nDESCRIPTION: A groupBy query example demonstrating the use of subtotalsSpec feature, which allows computation of multiple sub-groupings in a single query by specifying sets of dimensions to group by.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\": \"groupBy\",\n ...\n ...\n\"dimensions\": [\n  {\n  \"type\" : \"default\",\n  \"dimension\" : \"d1col\",\n  \"outputName\": \"D1\"\n  },\n  {\n  \"type\" : \"extraction\",\n  \"dimension\" : \"d2col\",\n  \"outputName\" :  \"D2\",\n  \"extractionFn\" : extraction_func\n  },\n  {\n  \"type\":\"lookup\",\n  \"dimension\":\"d3col\",\n  \"outputName\":\"D3\",\n  \"name\":\"my_lookup\"\n  }\n],\n...\n...\n\"subtotalsSpec\":[ [\"D1\", \"D2\", D3\"], [\"D1\", \"D3\"], [\"D3\"]],\n..\n\n}\n```\n\n----------------------------------------\n\nTITLE: Explaining query execution plan in Druid SQL\nDESCRIPTION: SQL query that retrieves the execution plan for a query. The EXPLAIN PLAN FOR prefix causes Druid to return the query plan instead of executing the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Loading Data Using Druid Helper Script\nDESCRIPTION: Command to execute the Druid batch ingestion helper script that loads the Wikipedia dataset. The script handles task submission and monitors the ingestion progress.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Apache Druid\nDESCRIPTION: Example ingestion spec for a parallel index task in Apache Druid. It defines the data schema, input source, and tuning configuration for parallel batch ingestion of Wikipedia data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    },\n    \"tuningconfig\": {\n        \"type\": \"index_parallel\",\n        \"maxNumSubTasks\": 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV (Delimited) ParseSpec in Druid\nDESCRIPTION: Configuration for the parseSpec to ingest TSV or other delimited data in Druid. Specifies the format, delimiter, timestamp column, column names in order, and dimensions to extract.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Submitting Kinesis Supervisor Specification via HTTP POST\nDESCRIPTION: Demonstrates how to submit a supervisor specification to the Druid Overlord using curl. This command starts a supervisor for a specific dataSource that will manage Kinesis indexing tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Using TIME_EXTRACT Function in Druid SQL\nDESCRIPTION: Examples of using the TIME_EXTRACT function to extract time parts from timestamp expressions with optional timezone parameters. This function allows extracting various units like EPOCH, HOUR, DAY, etc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nTIME_EXTRACT(__time, 'HOUR')\nTIME_EXTRACT(__time, 'HOUR', 'America/Los_Angeles')\n```\n\n----------------------------------------\n\nTITLE: Implementing QuantilesToQuantiles Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToQuantiles post-aggregator which returns an array of quantiles corresponding to specified fractions in the distribution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantiles\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fractions\" : <array of fractional positions in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Lookup in Apache Druid\nDESCRIPTION: Example of a JDBC lookup configuration in Druid. It specifies the lookup type, namespace, connector config, table details, and polling period for fetching data from a MySQL database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"jdbc\",\n  \"namespace\":\"some_lookup\",\n  \"connectorConfig\":{\n    \"createTables\":true,\n    \"connectURI\":\"jdbc:mysql://localhost:3306/druid\",\n    \"user\":\"druid\",\n    \"password\":\"diurd\"\n  },\n  \"table\":\"some_lookup_table\",\n  \"keyColumn\":\"the_old_dim_value\",\n  \"valueColumn\":\"the_new_dim_value\",\n  \"tsColumn\":\"timestamp_column\",\n  \"pollPeriod\":600000\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV (Delimited) ParseSpec in Druid\nDESCRIPTION: Configuration for the parseSpec to ingest TSV or other delimited data in Druid. Specifies the format, delimiter, timestamp column, column names in order, and dimensions to extract.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose with Environment Variable Password\nDESCRIPTION: Example of HttpFirehose configuration that uses the EnvironmentVariablePasswordProvider, which retrieves the password from an environment variable instead of embedding it in the spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"http\",\n    \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"],\n    \"httpAuthenticationUsername\": \"username\",\n    \"httpAuthenticationPassword\": {\n        \"type\": \"environment\",\n        \"variable\": \"HTTP_FIREHOSE_PW\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Coordinator Configuration JSON\nDESCRIPTION: Example JSON configuration object showing various Coordinator settings including deletion wait time, merge limits, replication parameters, and decommissioning settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"millisToWaitBeforeDeleting\": 900000,\n  \"mergeBytesLimit\": 100000000,\n  \"mergeSegmentsLimit\" : 1000,\n  \"maxSegmentsToMove\": 5,\n  \"replicantLifetime\": 15,\n  \"replicationThrottleLimit\": 10,\n  \"emitBalancingStats\": false,\n  \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"],\n  \"decommissioningNodes\": [\"localhost:8182\", \"localhost:8282\"],\n  \"decommissioningMaxPercentOfMaxSegmentsToMove\": 70\n}\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query Response Format in Druid\nDESCRIPTION: Example of the response format from a Druid timeseries query, showing the structure with timestamp and result fields. Each result contains the aggregated values for the specified metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": { \"sample_name1\": <some_value>, \"sample_name2\": <some_value>, \"sample_divide\": <some_value> } \n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": { \"sample_name1\": <some_value>, \"sample_name2\": <some_value>, \"sample_divide\": <some_value> }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Kinesis Supervisor Configuration\nDESCRIPTION: Complete example of a Kinesis supervisor specification JSON configuration that defines the data schema, tuning parameters, and IO configuration for Kinesis stream ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kinesis\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kinesis\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kinesis\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"stream\": \"metrics\",\n    \"endpoint\": \"kinesis.us-east-1.amazonaws.com\",\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\",\n    \"recordsPerFetch\": 2000,\n    \"fetchDelayMillis\": 1000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Apache Druid GroupBy Query with Variance and Standard Deviation\nDESCRIPTION: Example of a GroupBy query that calculates variance of the index field and derives standard deviation using a post-aggregator, grouped by the alias dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy Query for Multi-value Dimensions\nDESCRIPTION: Example of a basic GroupBy query without filtering on a multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Bulk Lookup Configuration in JSON\nDESCRIPTION: Example of a bulk lookup configuration JSON object that defines multiple lookups across different tiers. This configuration can be posted to the Druid coordinator to update lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__default\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupTable\",\n          \"keyColumn\": \"country_id\",\n          \"valueColumn\": \"country_name\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  },\n  \"realtime_customer1\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    }\n  },\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the cardinality aggregator which uses HyperLogLog to estimate the cardinality of a set of dimensions. The 'byRow' parameter determines whether to count distinct values or distinct combinations, and 'round' controls whether to round estimates to whole numbers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/hll-old.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"<output_name>\",\n  \"fields\": [ <dimension1>, <dimension2>, ... ],\n  \"byRow\": <false | true> # (optional, defaults to false),\n  \"round\": <false | true> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: ORC Parser Configuration with Explicit Dimension Specification in JSON\nDESCRIPTION: Configuration for ORC parser with field discovery disabled and explicit dimension specification. This example shows how to manually specify dimensions and extract nested fields using flatten expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/orc.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"orc\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": false,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"millis\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim3\",\n              \"nestedDim\",\n              \"listDimFirstItem\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Filtered Aggregator in Druid\nDESCRIPTION: Demonstrates how to configure a filtered aggregator that wraps another aggregator and only processes values matching a specified dimension filter. This allows simultaneous computation of filtered and unfiltered aggregations without multiple queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"filtered\",\n  \"filter\" : {\n    \"type\" : \"selector\",\n    \"dimension\" : <dimension>,\n    \"value\" : <dimension value>\n  }\n  \"aggregator\" : <aggregation>\n}\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Direct File Reference\nDESCRIPTION: JSON configuration for a URI-based lookup that references a specific file. This setup fetches lookup data from an S3 bucket with CSV format and polls for updates every 5 minutes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uri\": \"s3://bucket/some/key/prefix/renames-0003.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Dimensions without Rollup in Druid\nDESCRIPTION: Shows how to configure dimensionsSpec when not using rollup, where all columns including metrics are specified as dimensions with their appropriate data types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n},\n```\n\n----------------------------------------\n\nTITLE: Loading Data Using Druid Helper Script\nDESCRIPTION: Command to execute the Druid batch ingestion helper script that loads the Wikipedia dataset. The script handles task submission and monitors the ingestion progress.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index.json\n```\n\n----------------------------------------\n\nTITLE: TopN Query for Exact Aggregates in Apache Druid\nDESCRIPTION: This example shows a first query in a two-query approach for getting exact aggregates with approximate rank in TopN operations. It retrieves the top 2 values for 'l_orderkey' dimension based on the 'L_QUANTITY_' metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/topnquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Example Complete Lookup Configuration with Tier\nDESCRIPTION: A complete JSON configuration example showing how to set up a globally cached lookup within a tier. This example configures a country code lookup using JDBC extraction for a specific customer tier.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupValues\",\n          \"keyColumn\": \"value_id\",\n          \"valueColumn\": \"value_text\",\n          \"filter\": \"value_type='country'\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Client in Apache Druid Broker\nDESCRIPTION: These properties define the HTTP client settings for the Broker to communicate with data servers, including connection pool size, compression, timeouts, and backpressure limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_44\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.http.numConnections=20\ndruid.broker.http.compressionCodec=gzip\ndruid.broker.http.readTimeout=PT15M\ndruid.broker.http.unusedConnectionTimeout=PT4M\ndruid.broker.http.maxQueuedBytes=0\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Post-Aggregator Implementation in Druid\nDESCRIPTION: Demonstrates the structure of an arithmetic post-aggregator that applies mathematical functions to aggregated fields. Supports operations like +, -, *, /, and quotient with optional ordering specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arithmetic\",\n  \"name\"  : <output_name>,\n  \"fn\"    : <arithmetic_function>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...],\n  \"ordering\" : <null (default), or \"numericFirst\">\n}\n```\n\n----------------------------------------\n\nTITLE: Sample InfluxDB Line Protocol Format\nDESCRIPTION: An example of the InfluxDB Line Protocol format showing a CPU metrics entry with tags, measurements and timestamp.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000\n```\n\n----------------------------------------\n\nTITLE: Querying Overwritten Data in Druid\nDESCRIPTION: SQL query showing the results after overwriting data in the 'updates-tutorial' datasource, with changed values for existing rows and a new row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          1     100 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n\nRetrieved 3 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Defining AND Logical Expression Filter in Apache Druid JSON Query\nDESCRIPTION: The AND filter combines multiple filters using logical AND. All specified filters must match for a row to be included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Filter in Druid\nDESCRIPTION: Filter that matches dimensions using Java regular expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"regex\", \"dimension\": <dimension_string>, \"pattern\": <pattern_string> }\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Worker Selection Strategy in Apache Druid (JSON)\nDESCRIPTION: This JSON configuration demonstrates how to implement a custom JavaScript-based worker selection strategy in Apache Druid. The function assigns batch index tasks to specific workers and other tasks to available workers based on capacity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\":\"javascript\",\n\"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"middleManager1_hostname:8091\\\");\\nbatch_workers.add(\\\"middleManager2_hostname:8091\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i < workers.length; i++){\\n sortedWorkers[i] = workers[i];\\n}\\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\\nvar minWorkerVer = config.getMinWorkerVersion();\\nfor (var i = 0; i < sortedWorkers.length; i++) {\\n var worker = sortedWorkers[i];\\n  var zkWorker = zkWorkers.get(worker);\\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\\n      return worker;\\n    } else {\\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\\n        return worker;\\n      }\\n    }\\n  }\\n}\\nreturn null;\\n}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Bound Filter for Numeric Range in Apache Druid\nDESCRIPTION: This example shows how to use a bound filter to express the condition 21 <= age <= 31 using numeric ordering. Bound filters can be used to filter on ranges of dimension values with options for strict or non-strict comparisons.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: TopN Query for Most Edited Wikipedia Pages in Druid Console\nDESCRIPTION: A native Druid TopN query that retrieves the 10 Wikipedia pages with the most edits on 2015-09-12. This query is meant to be pasted into the Druid console's Query view.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Search Query in Apache Druid\nDESCRIPTION: Example of a basic search query that looks for dimension values containing 'Ke' in a sample datasource over a specified time interval. The query uses an insensitive_contains search type with lexicographic sorting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/searchquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"search\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"searchDimensions\": [\n    \"dim1\",\n    \"dim2\"\n  ],\n  \"query\": {\n    \"type\": \"insensitive_contains\",\n    \"value\": \"Ke\"\n  },\n  \"sort\" : {\n    \"type\": \"lexicographic\"\n  },\n  \"intervals\": [\n    \"2013-01-01T00:00:00.000/2013-01-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Arithmetic Post-Aggregator in Druid Query JSON\nDESCRIPTION: Demonstrates the structure of an arithmetic post-aggregator in a Druid query. It includes fields for type, name, function, fields to operate on, and optional ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arithmetic\",\n  \"name\"  : <output_name>,\n  \"fn\"    : <arithmetic_function>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...],\n  \"ordering\" : <null (default), or \"numericFirst\">\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Ingestion Task with Roll-up\nDESCRIPTION: Druid ingestion task specification that enables roll-up, defines dimensions, metrics, and query granularity for the network flow data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"rollup-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"srcIP\",\n              \"dstIP\"\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"rollup-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"targetPartitionSize\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Manual Segment Disable Request in Druid\nDESCRIPTION: CURL command to manually disable a specific segment in Druid by sending a DELETE request to the coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XDELETE http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/segments/{SEGMENT-ID}\n```\n\n----------------------------------------\n\nTITLE: Configuring multi InputSpec for Delta Ingestion in Druid\nDESCRIPTION: Example of a multi inputSpec configuration that combines data from existing Druid segments with new data for delta ingestion. This approach allows combining multiple data sources in a single ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"multi\",\n    \"children\": [\n      {\n        \"type\" : \"dataSource\",\n        \"ingestionSpec\" : {\n          \"dataSource\": \"wikipedia\",\n          \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"],\n          \"segments\": [\n            {\n              \"dataSource\": \"test1\",\n              \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\",\n              \"version\": \"v2\",\n              \"loadSpec\": {\n                \"type\": \"local\",\n                \"path\": \"/tmp/index1.zip\"\n              },\n              \"dimensions\": \"host\",\n              \"metrics\": \"visited_sum,unique_hosts\",\n              \"shardSpec\": {\n                \"type\": \"none\"\n              },\n              \"binaryVersion\": 9,\n              \"size\": 2,\n              \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\"\n            }\n          ]\n        }\n      },\n      {\n        \"type\" : \"static\",\n        \"paths\": \"/path/to/more/wikipedia/data/\"\n      }\n    ]  \n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch Estimate With Bounds Post-Aggregator\nDESCRIPTION: JSON configuration for post-aggregator that estimates cardinality with error bounds from HLL sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchEstimateWithBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>,\n  \"numStdDev\" : <number of standard deviations: 1 (default), 2 or 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Native TopN Query in Apache Druid\nDESCRIPTION: This snippet demonstrates a native JSON TopN query to retrieve the 10 Wikipedia pages with the most edits on 2015-09-12. It includes the query structure and the curl command to submit the query to the Druid Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8082/druid/v2?pretty\n```\n\n----------------------------------------\n\nTITLE: HLLSketchBuild Aggregator Configuration in Druid\nDESCRIPTION: This snippet demonstrates the configuration for the HLLSketchBuild aggregator, which creates HLL sketch objects during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchBuild\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: HLLSketchBuild Aggregator Configuration in Druid\nDESCRIPTION: This snippet demonstrates the configuration for the HLLSketchBuild aggregator, which creates HLL sketch objects during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchBuild\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Retrieving Progress of a Parallel Indexing Task in Apache Druid\nDESCRIPTION: This JSON example shows the response format from the progress endpoint when checking a parallel indexing task. It includes counts of running, succeeded, failed, and completed tasks, along with the total and expected count of succeeded tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"running\":10,\n  \"succeeded\":0,\n  \"failed\":0,\n  \"complete\":0,\n  \"total\":10,\n  \"expectedSucceeded\":10\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query Log Example - TSV Format\nDESCRIPTION: Example of an SQL query log entry showing timestamp, remote address, performance metrics and the SQL query details in TSV format. The log shows a SELECT query with aggregation and ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_6\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1       {\"sqlQuery/time\":100,\"sqlQuery/bytes\":600,\"success\":true,\"identity\":\"user1\"}  {\"query\":\"SELECT page, COUNT(*) AS Edits FROM wikiticker WHERE __time BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\",\"context\":{\"sqlQueryId\":\"c9d035a0-5ffd-4a79-a865-3ffdadbb5fdd\",\"nativeQueryIds\":\"[490978e4-f5c7-4cf6-b174-346e63cf8863]\"}}\n```\n\n----------------------------------------\n\nTITLE: Comparison Operators in Apache Druid SQL\nDESCRIPTION: Examples of comparison operators available in Druid SQL, including equality, inequality, greater/less than, BETWEEN, LIKE, NULL checks, boolean operations, and IN clauses.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nx = y\nx <> y\nx > y\nx >= y\nx < y\nx <= y\nx BETWEEN y AND z\nx NOT BETWEEN y AND z\nx LIKE pattern ESCAPE esc\nx NOT LIKE pattern ESCAPE esc\nx IS NULL\nx IS NOT NULL\nx IS TRUE\nx IS NOT TRUE\nx IS FALSE\nx IS NOT FALSE\nx IN (values)\nx NOT IN (values)\nx IN (subquery)\nx NOT IN (subquery)\nx AND y\nx OR y\nNOT x\n```\n\n----------------------------------------\n\nTITLE: Creating Lower Bound Filter in Druid JSON\nDESCRIPTION: Shows how to create a lower bound filter to select values where age >= 18, using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"18\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Parser Configuration in Druid\nDESCRIPTION: JSON configuration for setting up the InfluxDB Line Protocol parser in Druid. Includes timestamp specification, dimension exclusions, and optional measurement whitelist.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"influx\",\n        \"timestampSpec\": {\n          \"column\": \"__ts\",\n          \"format\": \"millis\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensionExclusions\": [\n            \"__ts\"\n          ]\n        },\n        \"whitelistMeasurements\": [\n          \"cpu\"\n        ]\n      }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Connection in Apache Druid\nDESCRIPTION: These properties configure the Zookeeper connection for Druid, including host, authentication, and behavior settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.service.host=none\ndruid.zk.service.user=none\ndruid.zk.service.pwd=none\ndruid.zk.service.authScheme=digest\ndruid.zk.service.terminateDruidProcessOnConnectFail=false\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch Estimate With Bounds Post-Aggregator\nDESCRIPTION: JSON configuration for post-aggregator that estimates cardinality with error bounds from HLL sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchEstimateWithBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>,\n  \"numStdDev\" : <number of standard deviations: 1 (default), 2 or 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a Segment Metadata Query in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates the basic structure of a segment metadata query in Apache Druid. It specifies the query type, data source, and time interval for the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\":\"segmentMetadata\",\n  \"dataSource\":\"sample_datasource\",\n  \"intervals\":[\"2013-01-01/2014-01-01\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Aggregations with TopN in Apache Druid\nDESCRIPTION: A TopN query that aggregates multiple line item metrics (tax, discount, price, quantity) filtered by specific order keys. The query includes sum aggregations for numeric fields, a count aggregation, and uses an OR filter to match specific order keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_TAX_doubleSum\",\n                 \"name\": \"L_TAX_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_DISCOUNT_doubleSum\",\n                 \"name\": \"L_DISCOUNT_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\",\n                 \"name\": \"L_EXTENDEDPRICE_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             },\n             {\n                 \"name\": \"count\",\n                 \"type\": \"count\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"filter\": {\n        \"fields\": [\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"103136\"\n            },\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"1648672\"\n            }\n        ],\n        \"type\": \"or\"\n    },\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Compaction Task with Preserved Granularity\nDESCRIPTION: JSON specification for a compaction task that combines segments while maintaining the original hourly granularity. This reduces the total number of segments from 51 to 24.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS as Deep Storage in Druid\nDESCRIPTION: Essential configuration properties for setting up HDFS as the deep storage layer in Druid. Includes the storage type definition, directory specification, and optional Kerberos authentication parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/path/to/storage/directory\ndruid.hadoop.security.kerberos.principal=druid@EXAMPLE.COM\ndruid.hadoop.security.kerberos.keytab=/etc/security/keytabs/druid.headlessUser.keytab\n```\n\n----------------------------------------\n\nTITLE: Implementing LongSum Aggregator in Apache Druid\nDESCRIPTION: Shows the configuration for a longSum aggregator in Druid. This aggregator computes the sum of values as a 64-bit signed integer.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid Console URL\nDESCRIPTION: Base URL pattern for accessing the main Druid Console through the Router process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/management-uis.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Segment Metadata Query Response Format in Druid JSON\nDESCRIPTION: Example response from a segment metadata query showing the detailed information returned about a segment. Includes information about column types, cardinality, size estimates, intervals, and aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"id\" : \"some_id\",\n  \"intervals\" : [ \"2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z\" ],\n  \"columns\" : {\n    \"__time\" : { \"type\" : \"LONG\", \"hasMultipleValues\" : false, \"size\" : 407240380, \"cardinality\" : null, \"errorMessage\" : null },\n    \"dim1\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : 1944, \"errorMessage\" : null },\n    \"dim2\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : true, \"size\" : 100000, \"cardinality\" : 1504, \"errorMessage\" : null },\n    \"metric1\" : { \"type\" : \"FLOAT\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : null, \"errorMessage\" : null }\n  },\n  \"aggregators\" : {\n    \"metric1\" : { \"type\" : \"longSum\", \"name\" : \"metric1\", \"fieldName\" : \"metric1\" }\n  },\n  \"queryGranularity\" : {\n    \"type\": \"none\"\n  },\n  \"size\" : 300000,\n  \"numRows\" : 5000000\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Coordinator Operation Properties\nDESCRIPTION: Core configuration properties for the Druid Coordinator component that control operational behavior including run periods, timeouts, and segment management flags.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\ndruid.coordinator.period=PT60S\ndruid.coordinator.period.indexingPeriod=PT1800S\ndruid.coordinator.startDelay=PT300S\ndruid.coordinator.merge.on=false\ndruid.coordinator.load.timeout=PT15M\ndruid.coordinator.kill.pendingSegments.on=false\ndruid.coordinator.kill.on=false\ndruid.coordinator.kill.period=P1D\ndruid.coordinator.kill.durationToRetain=PT-1S\ndruid.coordinator.kill.maxSegments=0\ndruid.coordinator.balancer.strategy=cost\ndruid.coordinator.loadqueuepeon.repeatDelay=PT0.050S\ndruid.coordinator.asOverlord.enabled=false\ndruid.coordinator.asOverlord.overlordService=NULL\n```\n\n----------------------------------------\n\nTITLE: Sample Response for Segment Metadata Query in Apache Druid\nDESCRIPTION: This JSON snippet shows the structure of a response to a segment metadata query. It includes details about the segment such as intervals, column information, aggregators, and size statistics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"id\" : \"some_id\",\n  \"intervals\" : [ \"2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z\" ],\n  \"columns\" : {\n    \"__time\" : { \"type\" : \"LONG\", \"hasMultipleValues\" : false, \"size\" : 407240380, \"cardinality\" : null, \"errorMessage\" : null },\n    \"dim1\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : 1944, \"errorMessage\" : null },\n    \"dim2\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : true, \"size\" : 100000, \"cardinality\" : 1504, \"errorMessage\" : null },\n    \"metric1\" : { \"type\" : \"FLOAT\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : null, \"errorMessage\" : null }\n  },\n  \"aggregators\" : {\n    \"metric1\" : { \"type\" : \"longSum\", \"name\" : \"metric1\", \"fieldName\" : \"metric1\" }\n  },\n  \"queryGranularity\" : {\n    \"type\": \"none\"\n  },\n  \"size\" : 300000,\n  \"numRows\" : 5000000\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop DataSource InputSpec\nDESCRIPTION: JSON configuration example showing how to set up a dataSource inputSpec for reindexing data from an existing Druid dataSource using Hadoop batch ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"dataSource\",\n    \"ingestionSpec\" : {\n      \"dataSource\": \"wikipedia\",\n      \"intervals\": [\"2014-10-20T00:00:00Z/P2W\"]\n    }\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Example groupBy Query Response Format\nDESCRIPTION: Sample response format for a groupBy query showing the structure of returned results including version, timestamp and event data with dimension values and aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:00.000Z\",\n    \"event\" : {\n      \"country\" : <some_dim_value_one>,\n      \"device\" : <some_dim_value_two>,\n      \"total_usage\" : <some_value_one>,\n      \"data_transfer\" :<some_value_two>,\n      \"avg_usage\" : <some_avg_usage_value>\n    }\n  }, \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:12.000Z\",\n    \"event\" : {\n      \"dim1\" : <some_other_dim_value_one>,\n      \"dim2\" : <some_other_dim_value_two>,\n      \"sample_name1\" : <some_other_value_one>,\n      \"sample_name2\" :<some_other_value_two>,\n      \"avg_usage\" : <some_other_avg_usage_value>\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Output of Post-Index-Task Script in Druid\nDESCRIPTION: Example output from running the batch ingestion helper script, showing task creation, progress monitoring, and successful completion of the Wikipedia data indexing process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBeginning indexing data for wikipedia\nTask started: index_wikipedia_2018-07-27T06:37:44.323Z\nTask log:     http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/log\nTask status:  http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/status\nTask index_wikipedia_2018-07-27T06:37:44.323Z still running...\nTask index_wikipedia_2018-07-27T06:37:44.323Z still running...\nTask finished with status: SUCCESS\nCompleted indexing data for wikipedia. Now loading indexed data onto the cluster...\nwikipedia loading complete! You may now query your data\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Index Job for ORC Ingestion in Druid\nDESCRIPTION: Example configuration for setting up a Hadoop index job to ingest ORC format data into Druid. Demonstrates the complete specification including ioConfig, dataSchema, and tuningConfig with ORC-specific parser settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/orc.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat\",\n        \"paths\": \"/data/path/in/HDFS/\"\n      },\n      \"metadataUpdateSpec\": {\n        \"type\": \"postgresql\",\n        \"connectURI\": \"jdbc:postgresql://localhost/druid\",\n        \"user\" : \"druid\",\n        \"password\" : \"asdf\",\n        \"segmentTable\": \"druid_segments\"\n      },\n      \"segmentOutputPath\": \"tmp/segments\"\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"no_metrics\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"name\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        },\n        \"typeString\": \"struct<time:string,name:string>\",\n        \"mapFieldNameFormat\": \"<PARENT>_<CHILD>\"\n      },\n      \"metricsSpec\": [{\n        \"type\": \"count\",\n        \"name\": \"count\"\n      }],\n      \"granularitySpec\": {\n        \"type\": \"uniform\",\n        \"segmentGranularity\": \"DAY\",\n        \"queryGranularity\": \"ALL\",\n        \"intervals\": [\"2015-12-31/2016-01-02\"]\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"workingPath\": \"tmp/working_path\",\n      \"partitionsSpec\": {\n        \"targetPartitionSize\": 5000000\n      },\n      \"jobProperties\" : {},\n      \"leaveIntermediate\": true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Filtered Aggregator in Druid\nDESCRIPTION: Demonstrates how to set up a filtered aggregator that wraps another aggregator and only processes values matching a specified dimension filter. This allows simultaneous computation of filtered and unfiltered aggregations in a single query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"filtered\",\n  \"filter\" : {\n    \"type\" : \"selector\",\n    \"dimension\" : <dimension>,\n    \"value\" : <dimension value>\n  }\n  \"aggregator\" : <aggregation>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Hadoop Parser for Batch Ingestion in Apache Druid\nDESCRIPTION: This snippet shows how to configure the Avro Hadoop parser for batch ingestion using HadoopDruidIndexer. It includes the full indexing task specification with dataSchema, ioConfig, and tuningConfig sections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",  \n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"\",\n      \"parser\" : {\n        \"type\" : \"avro_hadoop\",\n        \"parseSpec\" : {\n          \"format\": \"avro\",\n          \"timestampSpec\": <standard timestampSpec>,\n          \"dimensionsSpec\": <standard dimensionsSpec>,\n          \"flattenSpec\": <optional>\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\",\n        \"paths\" : \"\"\n      }\n    },\n    \"tuningConfig\" : {\n       \"jobProperties\" : {\n          \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing groupBy Query in Apache Druid\nDESCRIPTION: Example of a complete groupBy query object that demonstrates filtering, aggregations, post-aggregations, and having conditions. The query analyzes data transfer and usage metrics grouped by country and device dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"dimensions\": [\"country\", \"device\"],\n  \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] },\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" },\n      { \"type\": \"or\", \n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" },\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" },\n    { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"avg_usage\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" },\n        { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"having\": {\n    \"type\": \"greaterThan\",\n    \"aggregation\": \"total_usage\",\n    \"value\": 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchBuild Aggregator Configuration in Druid\nDESCRIPTION: This snippet demonstrates the configuration for the HLLSketchBuild aggregator, which creates HLL sketch objects during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchBuild\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring LongFirst Aggregator in Apache Druid\nDESCRIPTION: Illustrates how to set up a longFirst aggregator in Druid. This aggregator computes the metric value with the minimum timestamp or 0 if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"longFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchMerge Aggregator in Druid\nDESCRIPTION: JSON configuration for the HLLSketchMerge aggregator. This aggregator merges HLL sketches during query time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchMerge\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Defining OR Logical Expression Filter in Apache Druid JSON\nDESCRIPTION: Shows the JSON structure for an OR logical expression filter in Apache Druid. This filter combines multiple other filters with OR logic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS as Deep Storage in Apache Druid\nDESCRIPTION: Configuration properties for setting up HDFS as deep storage in Druid. Includes storage type, directory path, and optional Kerberos authentication parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type: hdfs\ndruid.storage.storageDirectory: <directory-path>\ndruid.hadoop.security.kerberos.principal: druid@EXAMPLE.COM\ndruid.hadoop.security.kerberos.keytab: /etc/security/keytabs/druid.headlessUser.keytab\n```\n\n----------------------------------------\n\nTITLE: Configuring SimpleJSON Lookup ParseSpec in Apache Druid\nDESCRIPTION: Example JSON configuration for a simpleJson lookup parseSpec. This format expects a line-delimited JSON file where each line contains a single key-value pair object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\":{\n  \"format\": \"simpleJson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Timeseries Query in Apache Druid\nDESCRIPTION: This SQL query performs a timeseries analysis, grouping Wikipedia edits by hour and calculating the sum of deleted lines for each hour on 2015-09-12.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);\n```\n\n----------------------------------------\n\nTITLE: Output of Post-Index-Task Script in Druid\nDESCRIPTION: Example output from running the batch ingestion helper script, showing task creation, progress monitoring, and successful completion of the Wikipedia data indexing process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBeginning indexing data for wikipedia\nTask started: index_wikipedia_2018-07-27T06:37:44.323Z\nTask log:     http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/log\nTask status:  http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/status\nTask index_wikipedia_2018-07-27T06:37:44.323Z still running...\nTask index_wikipedia_2018-07-27T06:37:44.323Z still running...\nTask finished with status: SUCCESS\nCompleted indexing data for wikipedia. Now loading indexed data onto the cluster...\nwikipedia loading complete! You may now query your data\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Spec for Protobuf Ingestion\nDESCRIPTION: Complete Kafka Supervisor specification for ingesting Protobuf data into Druid. Defines the data schema, parser configuration with Protobuf descriptor details, metrics specifications, and Kafka connection properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka2\",\n    \"parser\": {\n      \"type\": \"protobuf\",\n      \"descriptor\": \"file:///tmp/metrics.desc\",\n      \"protoMessageType\": \"Metrics\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"unit\",\n            \"http_method\",\n            \"http_code\",\n            \"page\",\n            \"metricType\",\n            \"server\"\n          ],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics_pb\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hash-based Partitioning in Druid\nDESCRIPTION: Example of configuring hash-based partitioning for Druid indexing. This partitioning strategy distributes rows across segments based on the hash of all dimensions, targeting a specific partition size for optimal segment distribution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"hashed\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure for Druid Tasks\nDESCRIPTION: A markdown document that provides comprehensive overview of Apache Druid tasks, including different task types like segment creation, compaction, kill tasks and their purposes. The document includes section headers, descriptions, and informational notes about task types and their usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Tasks Overview\"\n---\n\n# Tasks Overview\n\nApache Druid (incubating) tasks are run on MiddleManagers and always operate on a single data source.\n\nTasks are submitted using POST requests to the Overlord. Please see [Overlord Task API](../operations/api-reference.html#overlord-tasks) for API details.\n\nThere are several different types of tasks.\n\n## Segment Creation Tasks\n\n### Hadoop Index Task\n\nSee [batch ingestion](../ingestion/hadoop.html).\n\n### Native Index Tasks\n\nDruid provides a native index task which doesn't need any dependencies on other systems.\nSee [native index tasks](./native_tasks.html) for more details.\n\n<div class=\"note info\">\nPlease check [Hadoop-based Batch Ingestion VS Native Batch Ingestion](./hadoop-vs-native-batch.html) for differences between native batch ingestion and Hadoop-based ingestion.\n</div>\n\n### Kafka Indexing Tasks\n\nKafka Indexing tasks are automatically created by a Kafka Supervisor and are responsible for pulling data from Kafka streams. These tasks are not meant to be created/submitted directly by users. See [Kafka Indexing Service](../development/extensions-core/kafka-ingestion.html) for more details.\n\n### Stream Push Tasks (Tranquility)\n\nTranquility Server automatically creates \"realtime\" tasks that receive events over HTTP using an [EventReceiverFirehose](../ingestion/firehose.html#eventreceiverfirehose). These tasks are not meant to be created/submitted directly by users. See [Tranquility Stream Push](../ingestion/stream-push.html) for more info.\n\n## Compaction Tasks\n\nCompaction tasks merge all segments of the given interval. Please see [Compaction](../ingestion/compaction.html) for details.\n\n## Segment Merging Tasks\n\n<div class=\"note info\">\nThe documentation for the Append Task, Merge Task, and Same Interval Merge Task has been moved to <a href=\"../ingestion/misc-tasks.html\">Miscellaneous Tasks</a>.\n</div>\n\n## Kill Task\n\nKill tasks delete all information about a segment and removes it from deep storage. \n\nPlease see [Deleting Data](../ingestion/delete-data.html) for details.\n\n## Misc. Tasks\n\nPlease see [Miscellaneous Tasks](../ingestion/misc-tasks.html).\n\n## Task Locking and Priority\n\nPlease see [Task Locking and Priority](../ingestion/locking-and-priority.html).\n```\n\n----------------------------------------\n\nTITLE: Configuring doubleSum Aggregator in Apache Druid\nDESCRIPTION: The doubleSum aggregator computes and stores the sum of values as 64-bit floating point values. Similar to longSum, it requires an output name and the metric field name to sum over.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Direct Parser with Parquet ParseSpec in Druid\nDESCRIPTION: JSON configuration for ingesting Parquet files using the direct 'parquet' parser with a 'parquet' parseSpec. This example shows how to configure flattening of nested Parquet data with field discovery and JSON path expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"parquet\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Rolled-up Data in Druid\nDESCRIPTION: This snippet shows the result of Druid's roll-up operation on the raw data, with a queryGranularity of 'minute'. It demonstrates how Druid aggregates data by truncating timestamps to minutes and summing the metric values for each unique combination of dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/index.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000\n2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000\n2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Ingestion for Thrift Data in Apache Druid\nDESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. Demonstrates how to configure the inputFormat for both SequenceFileInputFormat and LzoThriftBlockInputFormat with necessary job properties for Thrift classes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"book\",\n      \"parser\": {\n        \"type\": \"thrift\",\n        \"jarPath\": \"book.jar\",\n        \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n        \"protocol\": \"compact\",\n        \"parseSpec\": {\n          \"format\": \"json\",\n          ...\n        }\n      },\n      \"metricsSpec\": [],\n      \"granularitySpec\": {}\n    },\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\",\n        // \"inputFormat\": \"com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat\",\n        \"paths\": \"/user/to/some/book.seq\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"jobProperties\": {\n        \"tmpjars\":\"/user/h_user_profile/du00/druid/test/book.jar\",\n        // \"elephantbird.class.for.MultiInputFormat\" : \"${YOUR_THRIFT_CLASS_NAME}\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi InputSpec for Delta Ingestion in Druid\nDESCRIPTION: A JSON configuration example for delta ingestion using the 'multi' inputSpec in Hadoop batch ingestion. This example combines data from an existing Druid dataSource (with explicitly defined segments) with additional data from a static file location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"multi\",\n    \"children\": [\n      {\n        \"type\" : \"dataSource\",\n        \"ingestionSpec\" : {\n          \"dataSource\": \"wikipedia\",\n          \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"],\n          \"segments\": [\n            {\n              \"dataSource\": \"test1\",\n              \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\",\n              \"version\": \"v2\",\n              \"loadSpec\": {\n                \"type\": \"local\",\n                \"path\": \"/tmp/index1.zip\"\n              },\n              \"dimensions\": \"host\",\n              \"metrics\": \"visited_sum,unique_hosts\",\n              \"shardSpec\": {\n                \"type\": \"none\"\n              },\n              \"binaryVersion\": 9,\n              \"size\": 2,\n              \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\"\n            }\n          ]\n        }\n      },\n      {\n        \"type\" : \"static\",\n        \"paths\": \"/path/to/more/wikipedia/data/\"\n      }\n    ]  \n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Ingestion Specification with Schema Definition in Apache Druid\nDESCRIPTION: A JSON configuration showing a complete ingestion specification with dataSchema and ioConfig. It defines the data source, parser, dimensions, metrics, and granularity settings for processing netflow data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Compaction Task for Wikipedia Data in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates a simple compaction task configuration for the Wikipedia dataset in Apache Druid. It specifies the data source and the time interval for which segments should be compacted.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"compact\",\n  \"dataSource\" : \"wikipedia\",\n  \"interval\" : \"2017-01-01/2018-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Core Components in Druid Java Codebase\nDESCRIPTION: Key Java classes that implement Druid's main functionality including Column.java for storage format, IncrementalIndex.java and IndexMerger.java for segment creation, IndexIO.java for storage engine, QueryResource.java for query processing, DruidCoordinator.java for coordination, and FirehoseFactory.java for data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/overview.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nColumn.java                              // Storage format implementation\nIncrementalIndex.java                     // Raw data ingestion\nIndexMerger.java                         // Segment creation\nIndexIO.java                             // Memory-mapped storage\nQueryResource.java                        // Query processing entry point\nDruidCoordinator.java                    // Historical process coordination\nOverlordResource.java                    // Ingestion coordination\nFirehoseFactory.java                     // Data loading implementation\nRealtimeManager.java                     // Real-time management\nRealtimePlumber.java                     // Persist and hand-off logic\nHadoopDruidDetermineConfigurationJob.java // Hadoop segment planning\nHadoopDruidIndexerJob.java               // Hadoop segment creation\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Buckets Histogram Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the fixed buckets histogram aggregator in Apache Druid. This aggregator builds a histogram on a numeric column with evenly-sized buckets across a specified value range.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"fixedBucketsHistogram\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <double>,\n  \"upperLimit\" : <double>,\n  \"outlierHandlingMode\": <mode>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS as Deep Storage in Apache Druid\nDESCRIPTION: Configuration properties for setting up HDFS as deep storage in Apache Druid. Includes parameters for basic configuration and Kerberos authentication.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.storage.type`|hdfs||Must be set.|\n|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|\n|`druid.hadoop.security.kerberos.principal`|`druid@EXAMPLE.COM`| Principal user name |empty|\n|`druid.hadoop.security.kerberos.keytab`|`/etc/security/keytabs/druid.headlessUser.keytab`|Path to keytab file|empty|\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON ParseSpec in Druid\nDESCRIPTION: Configuration for the parseSpec to ingest JSON data in Druid. Specifies the format, timestamp column, and dimensions to be extracted from the raw JSON data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"json\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"dimensionSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Joining Tables to Count Segments by Server for a Datasource\nDESCRIPTION: SQL query that joins sys.segments, sys.server_segments, and sys.servers tables to count segments for a specific datasource grouped by server. This helps identify segment distribution across the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(segments.segment_id) as num_segments from sys.segments as segments \nINNER JOIN sys.server_segments as server_segments \nON segments.segment_id  = server_segments.segment_id \nINNER JOIN sys.servers as servers \nON servers.server = server_segments.server\nWHERE segments.datasource = 'wikipedia' \nGROUP BY servers.server;\n```\n\n----------------------------------------\n\nTITLE: Querying Segments for a Specific Datasource in Druid\nDESCRIPTION: SQL query to retrieve all segments from the sys.segments table for a specific datasource (wikipedia). This query returns metadata about all segments including size, version, and availability status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n```\n\n----------------------------------------\n\nTITLE: Ingesting Raw Data and Creating Segments in Java\nDESCRIPTION: Raw data is ingested using IncrementalIndex.java, while segments are created using IndexMerger.java.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/overview.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nIncrementalIndex.java\nIndexMerger.java\n```\n\n----------------------------------------\n\nTITLE: Hourly Compaction Task Specification\nDESCRIPTION: JSON configuration for compacting segments while preserving hourly granularity, with maxRowsPerSegment set to 5000000.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ioConfig in JSON for Hadoop Indexer\nDESCRIPTION: This JSON snippet shows the configuration for the ioConfig section of the Hadoop Indexer specification file. It includes the metadataUpdateSpec for MySQL connection details and the segmentOutputPath for specifying where to output the segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  ...\n  \"metadataUpdateSpec\" : {\n    \"type\":\"mysql\",\n    \"connectURI\" : \"jdbc:mysql://localhost:3306/druid\",\n    \"password\" : \"diurd\",\n    \"segmentTable\" : \"druid_segments\",\n    \"user\" : \"druid\"\n  },\n  \"segmentOutputPath\" : \"/MyDirectory/data/index/output\"\n}\n```\n\n----------------------------------------\n\nTITLE: Trimming Strings in Apache Druid SQL\nDESCRIPTION: Illustrates various ways to trim characters from strings in Druid SQL, including TRIM, BTRIM, LTRIM, and RTRIM functions. These functions allow for flexible string manipulation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nTRIM([BOTH | LEADING | TRAILING] [<chars> FROM] expr)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nBTRIM(expr[, chars])\n```\n\nLANGUAGE: SQL\nCODE:\n```\nLTRIM(expr[, chars])\n```\n\nLANGUAGE: SQL\nCODE:\n```\nRTRIM(expr[, chars])\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticS3Firehose for Ingesting S3 Objects in Apache Druid\nDESCRIPTION: JSON configuration for the StaticS3Firehose which allows Apache Druid to ingest events from a predefined list of S3 objects. This firehose supports parallel ingestion tasks where each worker processes a separate S3 object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/s3.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-s3\",\n    \"uris\": [\"s3://foo/bar/file.gz\", \"s3://bar/foo/file2.gz\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Data Format for Druid Ingestion\nDESCRIPTION: Example of JSON-formatted data records showing various fields including timestamp, page information, and metrics that can be ingested into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143}\n```\n\n----------------------------------------\n\nTITLE: Sample groupBy Query Result in Apache Druid\nDESCRIPTION: Example of the JSON output format for a groupBy query in Apache Druid, showing the structure of individual result objects with timestamp and event data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:00.000Z\",\n    \"event\" : {\n      \"country\" : <some_dim_value_one>,\n      \"device\" : <some_dim_value_two>,\n      \"total_usage\" : <some_value_one>,\n      \"data_transfer\" :<some_value_two>,\n      \"avg_usage\" : <some_avg_usage_value>\n    }\n  }, \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:12.000Z\",\n    \"event\" : {\n      \"dim1\" : <some_other_dim_value_one>,\n      \"dim2\" : <some_other_dim_value_two>,\n      \"sample_name1\" : <some_other_value_one>,\n      \"sample_name2\" :<some_other_value_two>,\n      \"avg_usage\" : <some_other_avg_usage_value>\n    }\n  },\n...\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Storage Properties in Druid\nDESCRIPTION: Configuration properties for Druid's metadata storage system that uses JDBC for database connectivity. Includes settings for database type, connection details, and table names.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://localhost:5432/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=password123\n```\n\n----------------------------------------\n\nTITLE: Defining JavaScript Filter in Apache Druid JSON Query\nDESCRIPTION: The JavaScript filter allows custom filtering logic using a JavaScript function. It matches dimension values for which the function returns true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : <dimension_string>,\n  \"function\" : \"function(value) { <...> }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Constant Post-Aggregator Definition\nDESCRIPTION: Defines a constant post-aggregator that always returns a specified numerical value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Trimming Strings in Apache Druid SQL\nDESCRIPTION: Illustrates various ways to trim characters from strings in Druid SQL, including TRIM, BTRIM, LTRIM, and RTRIM functions. These functions allow for flexible string manipulation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nTRIM([BOTH | LEADING | TRAILING] [<chars> FROM] expr)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nBTRIM(expr[, chars])\n```\n\nLANGUAGE: SQL\nCODE:\n```\nLTRIM(expr[, chars])\n```\n\nLANGUAGE: SQL\nCODE:\n```\nRTRIM(expr[, chars])\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Timeseries Query\nDESCRIPTION: An SQL query that performs a timeseries analysis of deleted lines by hour. It uses the FLOOR function to group timestamps by hour and aggregates the deleted lines with SUM.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy v2 Parameters in Apache Druid\nDESCRIPTION: Configuration table showing runtime properties for GroupBy v2, including dictionary size and disk storage limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/groupbyquery.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxMergingDictionarySize`|Maximum amount of heap space (approximately) to use for the string dictionary during merging. When the dictionary exceeds this size, a spill to disk will be triggered.|100000000|\n|`druid.query.groupBy.maxOnDiskStorage`|Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling.|0 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Constant Post-Aggregator Definition\nDESCRIPTION: Defines a constant post-aggregator that always returns a specified numerical value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Using curl to Execute SQL Queries in Druid\nDESCRIPTION: This example demonstrates how to send SQL queries to Druid's HTTP endpoint using curl. The query is defined in a JSON file and sent as a POST request, returning the count of records from a data source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cat query.json\n{\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"}\n\n$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json\n[{\"TheCount\":24433}]\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Firehose Implementation in Jackson Module\nDESCRIPTION: Shows how to register a new Firehose implementation using Jackson modules. This allows Druid to recognize and use your custom Firehose type when specified in configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/modules.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Select Query Structure in Apache Druid\nDESCRIPTION: Demonstrates the basic structure of a Select query in Druid, including required fields like queryType, dataSource, intervals and pagingSpec. This query retrieves data from the wikipedia datasource for a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/select-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantiles Doubles Sketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketch aggregator which builds sketches from raw data during ingestion or combines pre-existing sketches. The k parameter controls size and accuracy of the sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"quantilesDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"k\": <parameter that controls size and accuracy>\n }\n```\n\n----------------------------------------\n\nTITLE: Setting Up Constant Post-Aggregator in Druid Query\nDESCRIPTION: Illustrates the JSON structure for a constant post-aggregator, which always returns a specified numerical value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON ParseSpec in Druid\nDESCRIPTION: Configuration for the parseSpec to ingest JSON data in Druid. Specifies the format, timestamp column, and dimensions to be extracted from the raw JSON data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"json\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"dimensionSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Configuration\nDESCRIPTION: Druid supervisor specification for ingesting Protobuf data from Kafka, including parser configuration and metrics specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka2\",\n    \"parser\": {\n      \"type\": \"protobuf\",\n      \"descriptor\": \"file:///tmp/metrics.desc\",\n      \"protoMessageType\": \"Metrics\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"unit\",\n            \"http_method\",\n            \"http_code\",\n            \"page\",\n            \"metricType\",\n            \"server\"\n          ],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics_pb\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Server in Apache Druid Broker\nDESCRIPTION: These properties define the HTTP server settings for the Broker, including thread configuration, queue size, timeouts, and request limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_43\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.http.numThreads=max(10, (Number of cores * 17) / 16 + 2) + 30\ndruid.server.http.queueSize=Unbounded\ndruid.server.http.maxIdleTime=PT5M\ndruid.server.http.enableRequestLimit=false\ndruid.server.http.defaultQueryTimeout=300000\ndruid.server.http.maxScatterGatherBytes=Long.MAX_VALUE\ndruid.server.http.gracefulShutdownTimeout=PT0S\ndruid.server.http.unannouncePropagationDelay=PT0S\ndruid.server.http.maxQueryTimeout=Long.MAX_VALUE\ndruid.server.http.maxRequestHeaderSize=8 * 1024\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Behavior Settings in Druid\nDESCRIPTION: Behavioral configuration properties for Zookeeper including session timeout, compression, and ACL security settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.service.sessionTimeoutMs=30000\ndruid.zk.service.compress=true\ndruid.zk.service.acl=false\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV ParseSpec in Druid\nDESCRIPTION: Configuration for the parseSpec to ingest CSV data in Druid. Specifies the format, timestamp column, column names in order, and dimensions to be extracted from the raw CSV data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantiles Doubles Sketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketch aggregator which builds sketches from raw data during ingestion or combines pre-existing sketches. The k parameter controls size and accuracy of the sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"quantilesDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"k\": <parameter that controls size and accuracy>\n }\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Log Output Example\nDESCRIPTION: Example of the log output when Tranquility Server starts up successfully.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRunning command[tranquility-server], logging to[/stage/apache-druid-0.15.1-incubating/var/sv/tranquility-server.log]: tranquility/bin/tranquility server -configFile conf/tranquility/server.json -Ddruid.extensions.loadList=[]\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Data Format for Druid Ingestion\nDESCRIPTION: Example of JSON-formatted data records showing various fields including timestamp, page information, and metrics that can be ingested into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143}\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Query Response Format in JSON\nDESCRIPTION: Example showing how to specify the result format for a Druid SQL query using the resultFormat parameter. The query counts records matching specific conditions and requests results in object format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Timestamp Min/Max Aggregations in Druid\nDESCRIPTION: Example JSON query for retrieving results using timeMin and timeMax aggregators in Druid. The query groups by day and product, and includes count, minimum timestamp, and maximum timestamp.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"timeMinMax\",\n  \"granularity\": \"DAY\",\n  \"dimensions\": [\"product\"],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    },\n    {\n      \"type\": \"timeMin\",\n      \"name\": \"<output_name of timeMin>\",\n      \"fieldName\": \"tmin\"\n    },\n    {\n      \"type\": \"timeMax\",\n      \"name\": \"<output_name of timeMax>\",\n      \"fieldName\": \"tmax\"\n    }\n  ],\n  \"intervals\": [\n    \"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a Search Query in Apache Druid\nDESCRIPTION: Example of a search query that returns dimension values matching the search specification. This query searches for values containing 'Ke' (case insensitive) in dimensions 'dim1' and 'dim2' with lexicographic sorting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"search\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"searchDimensions\": [\n    \"dim1\",\n    \"dim2\"\n  ],\n  \"query\": {\n    \"type\": \"insensitive_contains\",\n    \"value\": \"Ke\"\n  },\n  \"sort\" : {\n    \"type\": \"lexicographic\"\n  },\n  \"intervals\": [\n    \"2013-01-01T00:00:00.000/2013-01-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Select Query Structure in Druid\nDESCRIPTION: Example of a basic Select query structure in Druid, showing required and optional parameters for retrieving data from a datasource with pagination support.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/select-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid SQL Server Properties\nDESCRIPTION: Configuration properties for enabling and controlling various aspects of the Druid SQL server, including JDBC querying, HTTP querying, and planner options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_41\n\nLANGUAGE: properties\nCODE:\n```\ndruid.sql.enable=false\ndruid.sql.avatica.enable=true\ndruid.sql.avatica.maxConnections=50\ndruid.sql.avatica.maxRowsPerFrame=5000\ndruid.sql.avatica.maxStatementsPerConnection=1\ndruid.sql.avatica.connectionIdleTimeout=PT5M\ndruid.sql.http.enable=true\ndruid.sql.planner.awaitInitializationOnStart=true\ndruid.sql.planner.maxQueryCount=8\ndruid.sql.planner.maxSemiJoinRowsInMemory=100000\ndruid.sql.planner.maxTopNLimit=100000\ndruid.sql.planner.metadataRefreshPeriod=PT1M\ndruid.sql.planner.selectThreshold=1000\ndruid.sql.planner.useApproximateCountDistinct=true\ndruid.sql.planner.useApproximateTopN=true\ndruid.sql.planner.useFallback=false\ndruid.sql.planner.requireTimeCondition=false\ndruid.sql.planner.sqlTimeZone=UTC\ndruid.sql.planner.serializeComplexValues=true\n```\n\n----------------------------------------\n\nTITLE: Example JSON for Setting User Password in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates the structure of a password request object used to assign a password for HTTP basic authentication to a user in Druid's authentication API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"password\": \"helloworld\"\n}\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch ToString Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for converting HLL sketch to human-readable string format for debugging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: TopN Query Result Format\nDESCRIPTION: Example of the JSON response format for a TopN query, showing timestamp and results array containing dimension values with their corresponding metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2013-08-31T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dim1\": \"dim1_val\",\n        \"count\": 111,\n        \"some_metrics\": 10669,\n        \"average\": 96.11711711711712\n      },\n      {\n        \"dim1\": \"another_dim1_val\",\n        \"count\": 88,\n        \"some_metrics\": 28344,\n        \"average\": 322.09090909090907\n      },\n      {\n        \"dim1\": \"dim1_val3\",\n        \"count\": 70,\n        \"some_metrics\": 871,\n        \"average\": 12.442857142857143\n      },\n      {\n        \"dim1\": \"dim1_val4\",\n        \"count\": 62,\n        \"some_metrics\": 815,\n        \"average\": 13.14516129032258\n      },\n      {\n        \"dim1\": \"dim1_val5\",\n        \"count\": 60,\n        \"some_metrics\": 2787,\n        \"average\": 46.45\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting up LongLast Aggregator in Apache Druid\nDESCRIPTION: Demonstrates the configuration for a longLast aggregator in Druid. This aggregator computes the metric value with the maximum timestamp or 0 if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"longLast\",\n  \"name\" : <output_name>, \n  \"fieldName\" : <metric_name>,\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Timestamp Min/Max Aggregations in Druid\nDESCRIPTION: Example JSON query for retrieving results using timeMin and timeMax aggregators in Druid. The query groups by day and product, and includes count, minimum timestamp, and maximum timestamp.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"timeMinMax\",\n  \"granularity\": \"DAY\",\n  \"dimensions\": [\"product\"],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    },\n    {\n      \"type\": \"timeMin\",\n      \"name\": \"<output_name of timeMin>\",\n      \"fieldName\": \"tmin\"\n    },\n    {\n      \"type\": \"timeMax\",\n      \"name\": \"<output_name of timeMax>\",\n      \"fieldName\": \"tmax\"\n    }\n  ],\n  \"intervals\": [\n    \"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring String Format Extraction Function in Apache Druid\nDESCRIPTION: Shows the configuration of a String Format extraction function, which formats dimension values according to a specified format string. This example demonstrates the basic structure and null handling options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"stringFormat\", \"format\" : <sprintf_expression>, \"nullHandling\" : <optional attribute for handling null value> }\n```\n\n----------------------------------------\n\nTITLE: Quantiles Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the quantiles post-aggregator, which computes an array of quantiles based on the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantiles\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probabilities\" : [ <quantile>, <quantile>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading On-heap Guava Cache in JSON\nDESCRIPTION: Example configuration for a loading lookup with Guava cache implementation. The reverse lookup cache has specified maximum size and expiration settings, while the primary cache uses default settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"guava\"},\n   \"reverseLoadingCacheSpec\":{\"type\":\"guava\", \"maximumSize\":500000, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expression Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A filter that matches dimensions based on Java regular expression patterns, allowing for more complex pattern matching than simple equality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"regex\", \"dimension\": <dimension_string>, \"pattern\": <pattern_string> }\n```\n\n----------------------------------------\n\nTITLE: Executing Batch Ingestion Task in Bash\nDESCRIPTION: This Bash command uses the Druid helper script to submit the ingestion task and monitor its progress. It loads the Wikipedia data specified in the JSON configuration file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index.json\n```\n\n----------------------------------------\n\nTITLE: Submitting SQL Query via HTTP POST in Druid\nDESCRIPTION: Curl command for submitting a SQL query to Druid via HTTP POST, reading the query from a JSON file and sending it to the Druid Broker endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8888/druid/v2/sql\n```\n\n----------------------------------------\n\nTITLE: Configuring Derivative DataSource Supervisor in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure a derivativeDataSource supervisor in Apache Druid. It specifies the base dataSource, dimensions, metrics, and tuning configuration for creating a derived dataSource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\": \"derivativeDataSource\",\n   \"baseDataSource\": \"wikiticker\",\n   \"dimensionsSpec\": {\n       \"dimensions\": [\n           \"isUnpatrolled\",\n           \"metroCode\",\n           \"namespace\",\n           \"page\",\n           \"regionIsoCode\",\n           \"regionName\",\n           \"user\"\n       ]\n   },\n   \"metricsSpec\": [\n       {\n           \"name\": \"count\",\n           \"type\": \"count\"\n       },\n       {\n           \"name\": \"added\",\n           \"type\": \"longSum\",\n           \"fieldName\": \"added\"\n       }\n   ],\n   \"tuningConfig\": {\n       \"type\": \"hadoop\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Server in Apache Druid Broker\nDESCRIPTION: These properties define the HTTP server settings for the Broker, including thread configuration, queue size, timeouts, and request limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_43\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.http.numThreads=max(10, (Number of cores * 17) / 16 + 2) + 30\ndruid.server.http.queueSize=Unbounded\ndruid.server.http.maxIdleTime=PT5M\ndruid.server.http.enableRequestLimit=false\ndruid.server.http.defaultQueryTimeout=300000\ndruid.server.http.maxScatterGatherBytes=Long.MAX_VALUE\ndruid.server.http.gracefulShutdownTimeout=PT0S\ndruid.server.http.unannouncePropagationDelay=PT0S\ndruid.server.http.maxQueryTimeout=Long.MAX_VALUE\ndruid.server.http.maxRequestHeaderSize=8 * 1024\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Hadoop Parser for Batch Ingestion in JSON\nDESCRIPTION: This JSON snippet shows the configuration for using the Avro Hadoop Parser in a batch ingestion task. It includes settings for the data schema, input format, and Avro schema path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",  \n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"\",\n      \"parser\" : {\n        \"type\" : \"avro_hadoop\",\n        \"parseSpec\" : {\n          \"format\": \"avro\",\n          \"timestampSpec\": <standard timestampSpec>,\n          \"dimensionsSpec\": <standard dimensionsSpec>,\n          \"flattenSpec\": <optional>\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\",\n        \"paths\" : \"\"\n      }\n    },\n    \"tuningConfig\" : {\n       \"jobProperties\" : {\n          \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bucket Extraction Function in Druid\nDESCRIPTION: Configuration for bucketing numerical values into ranges of specified size with an offset. Converts values to base bucket values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bucket\",\n  \"size\" : 5,\n  \"offset\" : 2\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Failed Tasks in Druid SQL\nDESCRIPTION: SQL query to retrieve information about failed tasks from the sys.tasks table. This query filters tasks by their status to identify and inspect failures.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.tasks WHERE status='FAILED';\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Connection Properties in Apache Druid\nDESCRIPTION: Defines properties for connecting to Zookeeper, including host, authentication, and behavior settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.service.host=none\ndruid.zk.service.user=none\ndruid.zk.service.pwd=none\ndruid.zk.service.authScheme=digest\ndruid.zk.service.terminateDruidProcessOnConnectFail=false\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Post-Aggregators in Apache Druid JSON Query\nDESCRIPTION: Provides two complete examples of using post-aggregators in Druid queries, including aggregations and arithmetic operations to calculate percentages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n  \"aggregations\" : [\n    { \"type\" : \"count\", \"name\" : \"rows\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n         ]\n  }]\n  ...\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n  \"aggregations\" : [\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"part\", \"fieldName\" : \"part\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"part_percentage\",\n    \"fn\"     : \"*\",\n    \"fields\" : [\n       { \"type\"   : \"arithmetic\",\n         \"name\"   : \"ratio\",\n         \"fn\"     : \"/\",\n         \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"part\", \"fieldName\" : \"part\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" }\n         ]\n       },\n       { \"type\" : \"constant\", \"name\": \"const\", \"value\" : 100 }\n    ]\n  }]\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: TopN Query Result Format in Apache Druid\nDESCRIPTION: Example of the response format for a TopN query in Apache Druid. The result contains a timestamp and an array of dimension values with their associated metrics ordered by the specified sort metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2013-08-31T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dim1\": \"dim1_val\",\n        \"count\": 111,\n        \"some_metrics\": 10669,\n        \"average\": 96.11711711711712\n      },\n      {\n        \"dim1\": \"another_dim1_val\",\n        \"count\": 88,\n        \"some_metrics\": 28344,\n        \"average\": 322.09090909090907\n      },\n      {\n        \"dim1\": \"dim1_val3\",\n        \"count\": 70,\n        \"some_metrics\": 871,\n        \"average\": 12.442857142857143\n      },\n      {\n        \"dim1\": \"dim1_val4\",\n        \"count\": 62,\n        \"some_metrics\": 815,\n        \"average\": 13.14516129032258\n      },\n      {\n        \"dim1\": \"dim1_val5\",\n        \"count\": 60,\n        \"some_metrics\": 2787,\n        \"average\": 46.45\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy v2 Query in Druid\nDESCRIPTION: Runtime properties for GroupBy v2 queries in Druid. These settings control memory and disk usage for dictionary merging and result set spilling during query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_57\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.maxMergingDictionarySize=100000000\ndruid.query.groupBy.maxOnDiskStorage=0\n```\n\n----------------------------------------\n\nTITLE: Retrieving Column Metadata via INFORMATION_SCHEMA (SQL)\nDESCRIPTION: Illustrates how to query the INFORMATION_SCHEMA.COLUMNS table to retrieve metadata for columns in a specific Druid datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'\n```\n\n----------------------------------------\n\nTITLE: Executing HTTP POST Query in Apache Druid using Bash\nDESCRIPTION: This snippet demonstrates how to send a POST request to queryable Druid processes (Broker, Historical, Peons) to execute a query. It includes headers for Content-Type and Accept, and references an external JSON file for the query body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/querying.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Retained Entries Count from ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to get the number of retained entries from an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToNumEntries\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup in Apache Druid (JSON)\nDESCRIPTION: This snippet shows how to configure a cached namespace lookup using a URI extraction namespace. It specifies the lookup type, data source, parsing format, and polling period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"uri\",\n       \"uri\": \"file:/tmp/prefix/\",\n       \"namespaceParseSpec\": {\n         \"format\": \"csv\",\n         \"columns\": [\n           \"key\",\n           \"value\"\n         ]\n       },\n       \"pollPeriod\": \"PT5M\"\n     },\n     \"firstCacheTimeout\": 0\n }\n```\n\n----------------------------------------\n\nTITLE: Defining String Functions Table in Markdown\nDESCRIPTION: This markdown table lists and describes the string manipulation functions available in Druid's expression language, including concat, like, lookup, regexp_extract, and various string transformation functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/misc/math-expr.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|name|description|\n|----|-----------|\n|concat|concatenate a list of strings|\n|like|like(expr, pattern[, escape]) is equivalent to SQL `expr LIKE pattern`|\n|lookup|lookup(expr, lookup-name) looks up expr in a registered [query-time lookup](../querying/lookups.html)|\n|regexp_extract|regexp_extract(expr, pattern[, index]) applies a regular expression pattern and extracts a capture group index, or null if there is no match. If index is unspecified or zero, returns the substring that matched the pattern.|\n|replace|replace(expr, pattern, replacement) replaces pattern with replacement|\n|substring|substring(expr, index, length) behaves like java.lang.String's substring|\n|strlen|strlen(expr) returns length of a string in UTF-16 code units|\n|strpos|strpos(haystack, needle) returns the position of the needle within the haystack, with indexes starting from 0. If the needle is not found then the function returns -1.|\n|trim|trim(expr[, chars]) remove leading and trailing characters from `expr` if they are present in `chars`. `chars` defaults to ' ' (space) if not provided.|\n|ltrim|ltrim(expr[, chars]) remove leading characters from `expr` if they are present in `chars`. `chars` defaults to ' ' (space) if not provided.|\n|rtrim|rtrim(expr[, chars]) remove trailing characters from `expr` if they are present in `chars`. `chars` defaults to ' ' (space) if not provided.|\n|lower|lower(expr) converts a string to lowercase|\n|upper|upper(expr) converts a string to uppercase|\n```\n\n----------------------------------------\n\nTITLE: Numeric Filter Having Specifications\nDESCRIPTION: Examples of numeric filters including equalTo, greaterThan, and lessThan operations for filtering aggregate values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/having.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\": \"greaterThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\": \"equalTo\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\": \"lessThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Druid Column Data Structures\nDESCRIPTION: This code snippet demonstrates the three basic data structures used to represent a dimension column in Druid: a dictionary mapping values to integer IDs, a list of encoded column values, and bitmaps for each unique value indicating which rows contain that value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/segments.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   0,\n   1,\n   1]\n\n3: Bitmaps - one for each unique value of the column\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,0,1,1]\n```\n\n----------------------------------------\n\nTITLE: Default Partition Segment Identifier\nDESCRIPTION: Shows the format of a Druid segment identifier for partition number 0, which omits the partition number from the identifier string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_1\n\nLANGUAGE: plain\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z\n```\n\n----------------------------------------\n\nTITLE: JVM Metrics Table in Markdown\nDESCRIPTION: Table documenting JVM-related metrics including memory pools, buffer pools, and garbage collection statistics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`jvm/pool/committed`|Committed pool.|poolKind, poolName.|close to max pool|\n|`jvm/pool/init`|Initial pool.|poolKind, poolName.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Directory and Regex Pattern\nDESCRIPTION: JSON configuration for a URI-based lookup that references a directory with a regex pattern to select files. This setup searches for matching files in an S3 bucket and polls for updates every 5 minutes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uriPrefix\": \"s3://bucket/some/key/prefix/\",\n  \"fileRegex\":\"renames-[0-9]*\\\\.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop-based Batch Ingestion Task in Apache Druid\nDESCRIPTION: This JSON configuration specifies a Hadoop-based batch ingestion task for Apache Druid. It defines the data schema, input configuration, and tuning parameters for ingesting Wikipedia edit data into Druid using Hadoop.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"hadoopyString\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"paths\" : \"/MyDirectory/example/wikipedia_data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\": \"hadoop\"\n    }\n  },\n  \"hadoopDependencyCoordinates\": <my_hadoop_version>\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Data Format for Druid Ingestion\nDESCRIPTION: Sample JSON data format showing event logs with timestamp, page information, and various metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143}\n```\n\n----------------------------------------\n\nTITLE: Time Boundary Query Response Format\nDESCRIPTION: Example response format from a time boundary query showing the result structure with minimum and maximum timestamps in ISO-8601 format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"minTime\" : \"2013-05-09T18:24:00.000Z\",\n    \"maxTime\" : \"2013-05-09T18:37:00.000Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Aggregator in Druid\nDESCRIPTION: Configures custom JavaScript aggregation functions with aggregate, combine, and reset capabilities. Requires field names and JavaScript functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"javascript\",\n  \"name\": \"<output_name>\",\n  \"fieldNames\"  : [ <column1>, <column2>, ... ],\n  \"fnAggregate\" : \"function(current, column1, column2, ...) {\n                     <updates partial aggregate (current) based on the current row values>\n                     return <updated partial aggregate>\n                   }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return <combined partial results>; }\",\n  \"fnReset\"     : \"function()                   { return <initial value>; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Store Firehose in Druid\nDESCRIPTION: JSON configuration for setting up a static Azure blob store firehose in Druid. This configuration enables ingesting data from multiple Azure blob containers and paths, with support for caching and prefetching features.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/azure.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"static-azure-blobstore\",\n    \"blobs\": [\n        {\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Deep Storage in Apache Druid\nDESCRIPTION: Properties for setting up deep storage in Druid. This includes configurations for local, S3, and other storage types to manage segment data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_17\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=s3\ndruid.s3.accessKey=YOUR_ACCESS_KEY\ndruid.s3.secretKey=YOUR_SECRET_KEY\ndruid.storage.bucket=your-bucket\ndruid.storage.baseKey=druid/segments\n```\n\n----------------------------------------\n\nTITLE: Custom DimensionsSpec with Mixed Data Types in JSON\nDESCRIPTION: Example of a dimensionsSpec configuration that demonstrates how to define dimensions with different data types (string, long, float) and disable bitmap indexes for specific string columns. This shows advanced dimension schema configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Timestamp Min/Max Aggregations in Druid\nDESCRIPTION: Example of a groupBy query using timeMin and timeMax aggregators to retrieve precise timestamp ranges for events. This query groups results by day and product dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"timeMinMax\",\n  \"granularity\": \"DAY\",\n  \"dimensions\": [\"product\"],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    },\n    {\n      \"type\": \"timeMin\",\n      \"name\": \"<output_name of timeMin>\",\n      \"fieldName\": \"tmin\"\n    },\n    {\n      \"type\": \"timeMax\",\n      \"name\": \"<output_name of timeMax>\",\n      \"fieldName\": \"tmax\"\n    }\n  ],\n  \"intervals\": [\n    \"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Data Format for Druid Ingestion\nDESCRIPTION: Sample JSON data format showing event logs with timestamp, page information, and various metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Eight Firehose in Druid\nDESCRIPTION: Sample configuration specification for setting up a Kafka 0.8.x consumer in Druid. Includes essential consumer properties like Zookeeper connection settings, group ID, and message handling parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kafka-eight-firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"firehose\": {\n    \"type\": \"kafka-0.8\",\n    \"consumerProps\": {\n      \"zookeeper.connect\": \"localhost:2181\",\n      \"zookeeper.connection.timeout.ms\" : \"15000\",\n      \"zookeeper.session.timeout.ms\" : \"15000\",\n      \"zookeeper.sync.time.ms\" : \"5000\",\n      \"group.id\": \"druid-example\",\n      \"fetch.message.max.bytes\" : \"1048586\",\n      \"auto.offset.reset\": \"largest\",\n      \"auto.commit.enable\": \"false\"\n    },\n    \"feed\": \"wikipedia\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Search Filter Example in Druid\nDESCRIPTION: Example of search filter using case-insensitive contains matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"search\",\n        \"dimension\": \"product\",\n        \"query\": {\n          \"type\": \"insensitive_contains\",\n          \"value\": \"foo\" \n        }        \n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticAzureBlobStoreFirehose in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticAzureBlobStoreFirehose for ingesting data from Azure Blob Storage. It specifies the firehose type and a list of blobs to ingest.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/azure.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-azure-blobstore\",\n    \"blobs\": [\n        {\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Regex Filtered DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Regex Filtered DimensionSpec in Apache Druid. It retains only the values matching the specified regex pattern in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"regexFiltered\", \"delegate\" : <dimensionSpec>, \"pattern\": <java regex pattern> }\n```\n\n----------------------------------------\n\nTITLE: Querying Ingested Data using Druid SQL\nDESCRIPTION: This snippet demonstrates how to use the Druid SQL command-line client to query the ingested data from the 'ingestion-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"ingestion-tutorial\";\n\n\n __time                    bytes  cost  count  dstIP    dstPort  packets  protocol  srcIP    srcPort \n\n 2018-01-01T01:01:00.000Z   6000   4.9      3  2.2.2.2     3000       60  6         1.1.1.1     2000 \n 2018-01-01T01:02:00.000Z   9000  18.1      2  2.2.2.2     7000       90  6         1.1.1.1     5000 \n 2018-01-01T01:03:00.000Z   6000   4.3      1  2.2.2.2     7000       60  6         1.1.1.1     5000 \n 2018-01-01T02:33:00.000Z  30000  56.9      2  8.8.8.8     5000      300  17        7.7.7.7     4000 \n 2018-01-01T02:35:00.000Z  30000  46.3      1  8.8.8.8     5000      300  17        7.7.7.7     4000 \n\nRetrieved 5 rows in 0.12s.\n\ndsql>\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function for Time Dimension\nDESCRIPTION: A JavaScript extraction function example that works with the __time dimension, transforming the timestamp (in milliseconds) to show the seconds portion with a 'Second' prefix.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Derivative DataSource Supervisor in Druid\nDESCRIPTION: JSON configuration for creating a derivative dataSource supervisor. Specifies dimensions, metrics, and tuning configuration for maintaining materialized views based on a base dataSource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"derivativeDataSource\",\n    \"baseDataSource\": \"wikiticker\",\n    \"dimensionsSpec\": {\n        \"dimensions\": [\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\"\n        ]\n    },\n    \"metricsSpec\": [\n        {\n            \"name\": \"count\",\n            \"type\": \"count\"\n        },\n        {\n            \"name\": \"added\",\n            \"type\": \"longSum\",\n            \"fieldName\": \"added\"\n        }\n    ],\n    \"tuningConfig\": {\n        \"type\": \"hadoop\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Time Boundaries in Apache Druid with JSON\nDESCRIPTION: A time boundary query that returns the earliest and latest data points in a Druid dataset. The query can be configured to return either the minimum time, maximum time, or both by setting the optional 'bound' parameter accordingly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"timeBoundary\",\n    \"dataSource\": \"sample_datasource\",\n    \"bound\"     : < \"maxTime\" | \"minTime\" > # optional, defaults to returning both timestamps if not set \n    \"filter\"    : { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] } # optional\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos and HTTP Basic Authentication Chain in Apache Druid\nDESCRIPTION: Example configuration for enabling a multi-authenticator chain that includes Kerberos and HTTP Basic authentication methods from the druid-kerberos and druid-basic-security extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/auth.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"kerberos\", \"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Object-based Numeric TopNMetricSpec in Druid\nDESCRIPTION: Detailed numeric metric specification using JSON object format for sorting topN results by a specific metric name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnmetricspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"numeric\",\n    \"metric\": \"<metric_name>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Less Than Having Filter in Druid\nDESCRIPTION: Shows how to use a less than filter in Having clause to match rows with aggregate values below a threshold.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"lessThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the arrayOfDoublesSketch aggregator in Druid. This aggregator creates sketches that extend the functionality of count-distinct Theta sketches by adding arrays of double values associated with unique keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"arrayOfDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"nominalEntries\": <number>,\n  \"numberOfValues\" : <number>,\n  \"metricColumns\" : <array of strings>\n }\n```\n\n----------------------------------------\n\nTITLE: Extracting Druid Distribution with Bash\nDESCRIPTION: Commands to download and extract the Apache Druid distribution package. This downloads version 0.15.0-incubating and extracts it to begin the installation process for a clustered deployment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.15.0-incubating-bin.tar.gz\ncd apache-druid-0.15.0-incubating\n```\n\n----------------------------------------\n\nTITLE: Illustrating Druid Column Data Structures\nDESCRIPTION: This code snippet demonstrates the three basic data structures used to represent a dimension column in Druid: a dictionary mapping values to integer IDs, a list of encoded column values, and bitmaps for each unique value indicating which rows contain that value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/segments.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   0,\n   1,\n   1]\n\n3: Bitmaps - one for each unique value of the column\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,0,1,1]\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query with Skip Empty Buckets in Druid (JSON)\nDESCRIPTION: A timeseries query that includes the 'skipEmptyBuckets' context flag to omit time buckets with no data. Without this flag, Druid would zero-fill empty interior time buckets in the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-04T00:00:00.000\" ],\n  \"context\" : {\n    \"skipEmptyBuckets\": \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Scan Query in Apache Druid\nDESCRIPTION: Example of a Scan query object in Apache Druid. It specifies the query type, data source, result format, columns, time intervals, batch size, and result limit.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/scan-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"scan\",\n   \"dataSource\": \"wikipedia\",\n   \"resultFormat\": \"list\",\n   \"columns\":[],\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"batchSize\":20480,\n   \"limit\":3\n }\n```\n\n----------------------------------------\n\nTITLE: Submitting Ingestion Task via cURL in Bash\nDESCRIPTION: This cURL command manually submits the ingestion task to Druid's Indexer service. It POSTs the JSON configuration file to the Druid Overlord's task endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Query Structure\nDESCRIPTION: Demonstrates the basic structure of a SQL query in Druid, including optional clauses for EXPLAIN PLAN, WITH statements, and various SQL operations like SELECT, WHERE, GROUP BY, HAVING, ORDER BY, LIMIT, and UNION ALL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Task Logging Properties\nDESCRIPTION: Properties for configuring task log storage and retention across various backends like S3, Azure, GCS, HDFS and local filesystem.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_21\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.type=file\ndruid.indexer.logs.kill.enabled=false\ndruid.indexer.logs.kill.durationToRetain=\ndruid.indexer.logs.kill.initialDelay=300000\ndruid.indexer.logs.kill.delay=21600000\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Statistical Testing with Druid Post Aggregators\nDESCRIPTION: A complete JSON query example demonstrating how to use zscore2sample to calculate a z-score and then feed that result into pvalue2tailedZtest to determine the p-value. The example uses constant values for simplicity but real queries would typically reference field aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n    \"postAggregations\" : {\n    \"type\"   : \"pvalue2tailedZtest\",\n    \"name\"   : \"pvalue\",\n    \"zScore\" : \n    {\n     \"type\"   : \"zscore2sample\",\n     \"name\"   : \"zscore\",\n     \"successCount1\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation1Sample\",\n         \"value\"  : 300\n       },\n     \"sample1Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation1\",\n         \"value\"  : 500\n       },\n     \"successCount2\":\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation2Sample\",\n         \"value\"  : 450\n       },\n     \"sample2Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation2\",\n         \"value\"  : 600\n       }\n     }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Interval Filter in Druid Query (JSON)\nDESCRIPTION: This example shows how to set up an interval filter in a Druid query. It filters for time ranges in October and November 2014.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Scan Query Results in List Format\nDESCRIPTION: This JSON structure shows the result format when 'resultFormat' is set to 'list'. It includes segment information, column names, and individual event data with named fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/scan-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\",\n      \"robot\",\n      \"namespace\",\n      \"anonymous\",\n      \"unpatrolled\",\n      \"page\",\n      \"language\",\n      \"newpage\",\n      \"user\",\n      \"count\",\n      \"added\",\n      \"delta\",\n      \"variation\",\n      \"deleted\"\n    ],\n    \"events\" : [ {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._243\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 77.0,\n        \"delta\" : 77.0,\n        \"variation\" : 77.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._73\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._756\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 68.0,\n        \"delta\" : 68.0,\n        \"variation\" : 68.0,\n        \"deleted\" : 0.0\n    } ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: AND Logical Filter in Druid\nDESCRIPTION: Logical AND filter that combines multiple filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Druid Query via HTTP DELETE\nDESCRIPTION: Example of cancelling a running Druid query by its query ID using curl. This sends an HTTP DELETE request to the Broker or Router to terminate a specific query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/querying.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://host:port/druid/v2/abc123\"\n```\n\n----------------------------------------\n\nTITLE: Constructing a Follow-up Paginated Select Query in Apache Druid\nDESCRIPTION: This JSON structure shows how to construct a follow-up paginated Select query in Apache Druid. It includes the pagingIdentifiers from the previous query result, with the offset manually incremented when 'fromNext' is set to false.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/select-query.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {\"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 5}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Implementing zscore2sample Post Aggregator in Druid\nDESCRIPTION: This JSON snippet demonstrates how to define a zscore2sample post aggregator in Druid for calculating the z-score using two-sample z-test. It converts binary variables (success or not) to continuous variables (conversion rate).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"zscore2sample\",\n  \"name\": \"<output_name>\",\n  \"successCount1\": <post_aggregator> success count of sample 1,\n  \"sample1Size\": <post_aggregaror> sample 1 size,\n  \"successCount2\": <post_aggregator> success count of sample 2,\n  \"sample2Size\" : <post_aggregator> sample 2 size\n}\n```\n\n----------------------------------------\n\nTITLE: Max Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the max post-aggregator, which returns the maximum value of the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"max\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring log4j2 XML for Apache Druid\nDESCRIPTION: Example log4j2.xml configuration file for Apache Druid that sets up console logging with ISO8601 timestamp formatting. It includes the basic logging structure and a commented section showing how to enable HTTP request logging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/logging.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n\n    <!-- Uncomment to enable logging of all HTTP requests\n    <Logger name=\"org.apache.druid.jetty.RequestLog\" additivity=\"false\" level=\"DEBUG\">\n        <AppenderRef ref=\"Console\"/>\n    </Logger>\n    -->\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Paginated Select Query with fromNext Parameter\nDESCRIPTION: Example of a Select query using the fromNext parameter for backwards compatibility mode, where manual offset incrementation is required for pagination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/select-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring General TLS Settings in Apache Druid\nDESCRIPTION: These properties control the enabling and disabling of HTTP and HTTPS connectors in Apache Druid. They allow for configuring the use of plaintext (HTTP) and TLS (HTTPS) ports.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/tls-support.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid PostgreSQL Connection Properties\nDESCRIPTION: Configuration properties for connecting Druid to PostgreSQL metadata storage, including extension loading, connection URI, and authentication credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\"]\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Segment Cache in Druid\nDESCRIPTION: This code snippet shows how to configure the segment cache locations and maximum size for Druid Historical processes. It sets the cache location to '/tmp/druid/storageLocation' with a maximum size of 500GB.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/faq.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n-Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}]\n-Ddruid.server.maxSize=500000000000\n```\n\n----------------------------------------\n\nTITLE: Submitting Ingestion Task via cURL in Bash\nDESCRIPTION: This cURL command manually submits the ingestion task to Druid's Indexer service. It POSTs the JSON configuration file to the Druid Overlord's task endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Simple Compaction Task Example in Apache Druid\nDESCRIPTION: A minimal example of a compaction task that merges all segments for the year 2017. This task uses default settings for segment granularity, preserving the original granularity of the source segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"compact\",\n  \"dataSource\" : \"wikipedia\",\n  \"interval\" : \"2017-01-01/2018-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Task Logging Configuration Properties\nDESCRIPTION: Settings for configuring task log storage in various backends including S3, Azure, Google Cloud Storage, HDFS and local filesystem. Includes retention and cleanup settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_19\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.type=file\ndruid.indexer.logs.kill.enabled=false\ndruid.indexer.logs.directory=log\ndruid.indexer.logs.s3Bucket=none\ndruid.indexer.logs.s3Prefix=none\ndruid.indexer.logs.container=none\ndruid.indexer.logs.prefix=none\ndruid.indexer.logs.bucket=none\n```\n\n----------------------------------------\n\nTITLE: Connecting to Druid SQL using JDBC (Java)\nDESCRIPTION: Java code snippet showing how to connect to Druid SQL using the Avatica JDBC driver and execute a query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Prism License Comment\nDESCRIPTION: License comment for the Prism syntax highlighting library by Lea Verou, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Defining Input Source for Druid Ingestion Task in JSON\nDESCRIPTION: This snippet defines the ioConfig object for a Druid native batch task, specifying a local firehose to read input data from a JSON file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n```\n\n----------------------------------------\n\nTITLE: Other SQL Functions in Druid\nDESCRIPTION: Examples of other SQL functions available in Druid, including type casting, CASE statements, and special functions like BLOOM_FILTER_TEST.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCAST(value AS TYPE)\nCASE WHEN boolean_expr1 THEN result1 ELSE resultN END\nNULLIF(value1, value2)\nBLOOM_FILTER_TEST(expr, serialized-filter)\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Specification in Apache Druid JSON Schema\nDESCRIPTION: This snippet demonstrates how to define a metrics specification within the dataSchema. It shows configuring count, longSum, and doubleSum aggregators for different metric columns during data rollup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }   \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: groupBy Query with Subtotals in Apache Druid\nDESCRIPTION: Example of a groupBy query using the subtotals feature in Apache Druid. It demonstrates how to structure the query with dimensions and subtotalsSpec to compute multiple sub-groupings in a single query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/groupbyquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\": \"groupBy\",\n ...\n ...\n\"dimensions\": [\n  {\n  \"type\" : \"default\",\n  \"dimension\" : \"d1col\",\n  \"outputName\": \"D1\"\n  },\n  {\n  \"type\" : \"extraction\",\n  \"dimension\" : \"d2col\",\n  \"outputName\" :  \"D2\",\n  \"extractionFn\" : extraction_func\n  },\n  {\n  \"type\":\"lookup\",\n  \"dimension\":\"d3col\",\n  \"outputName\":\"D3\",\n  \"name\":\"my_lookup\"\n  }\n],\n...\n...\n\"subtotalsSpec\":[ [\"D1\", \"D2\", D3], [\"D1\", \"D3\"], [\"D3\"]],\n..\n\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FragmentSearchQuerySpec in Apache Druid\nDESCRIPTION: This snippet shows the JSON configuration for a FragmentSearchQuerySpec. It matches if a dimension value contains all specified fragments, with optional case sensitivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"fragment\",\n  \"case_sensitive\" : false,\n  \"values\" : [\"fragment1\", \"fragment2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Cycle Size Moving Average Query\nDESCRIPTION: Shows how to calculate an average of first 10-minute intervals over a 3-hour period using cycle size. The query uses 10-minute granularity and includes a cycle size of 6 to group data hourly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"movingAverage\",\n  \"dataSource\": \"wikipedia\",\n  \"granularity\": {\n    \"type\": \"period\",\n    \"period\": \"PT10M\"\n  },\n  \"intervals\": [\n    \"2015-09-12T00:00:00Z/2015-09-13T00:00:00Z\"\n  ],\n  \"aggregations\": [\n    {\n      \"name\": \"delta10Min\",\n      \"fieldName\": \"delta\",\n      \"type\": \"doubleSum\"\n    }\n  ],\n  \"averagers\": [\n    {\n      \"name\": \"trailing10MinPerHourChanges\",\n      \"fieldName\": \"delta10Min\",\n      \"type\": \"doubleMeanNoNulls\",\n      \"buckets\": 18,\n      \"cycleSize\": 6\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authentication for Hadoop in Druid\nDESCRIPTION: Kerberos security configuration properties for enabling authenticated access to Hadoop from Druid. This specifies the Kerberos principal and keytab file location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kerberos-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.hadoop.security.kerberos.principal\ndruid.hadoop.security.kerberos.keytab\n```\n\n----------------------------------------\n\nTITLE: Configuring LongLast Aggregator in Druid JSON\nDESCRIPTION: Defines a longLast aggregator to compute the metric value with the maximum timestamp or 0 if no row exists. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"longLast\",\n  \"name\" : <output_name>, \n  \"fieldName\" : <metric_name>,\n}\n```\n\n----------------------------------------\n\nTITLE: Example GroupBy Query with Variance and Standard Deviation in Druid\nDESCRIPTION: Sample GroupBy query demonstrating variance aggregation and standard deviation calculation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/stats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Tool Help Output in Druid\nDESCRIPTION: The help output for the ResetCluster tool, showing the command name, synopsis, and all available options with their descriptions. This provides a comprehensive reference for all possible ways to use the tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nNAME\n        druid tools reset-cluster - Cleanup all persisted state from metadata\n        and deep storage.\n\nSYNOPSIS\n        druid tools reset-cluster [--all] [--hadoopWorkingPath]\n                [--metadataStore] [--segmentFiles] [--taskLogs]\n\nOPTIONS\n        --all\n            delete all state stored in metadata and deep storage\n\n        --hadoopWorkingPath\n            delete hadoopWorkingPath\n\n        --metadataStore\n            delete all records in metadata storage\n\n        --segmentFiles\n            delete all segment files from deep storage\n\n        --taskLogs\n            delete all tasklogs\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Parser Configuration in Druid\nDESCRIPTION: JSON configuration for setting up the InfluxDB Line Protocol parser in Druid. Includes settings for timestamp handling, dimension exclusions, and measurement whitelisting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"influx\",\n        \"timestampSpec\": {\n          \"column\": \"__ts\",\n          \"format\": \"millis\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensionExclusions\": [\n            \"__ts\"\n          ]\n        },\n        \"whitelistMeasurements\": [\n          \"cpu\"\n        ]\n      }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid SQL Server Properties\nDESCRIPTION: Configuration properties for Druid's SQL server functionality, including connection settings, query limits, and behavior controls for the Broker component.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_34\n\nLANGUAGE: properties\nCODE:\n```\ndruid.sql.enable=false\ndruid.sql.avatica.enable=true\ndruid.sql.avatica.maxConnections=50\ndruid.sql.avatica.maxRowsPerFrame=5,000\ndruid.sql.avatica.maxStatementsPerConnection=1\ndruid.sql.avatica.connectionIdleTimeout=PT5M\ndruid.sql.http.enable=true\ndruid.sql.planner.awaitInitializationOnStart=true\ndruid.sql.planner.maxQueryCount=8\ndruid.sql.planner.maxSemiJoinRowsInMemory=100000\ndruid.sql.planner.maxTopNLimit=100000\ndruid.sql.planner.metadataRefreshPeriod=PT1M\ndruid.sql.planner.selectThreshold=1000\ndruid.sql.planner.useApproximateCountDistinct=true\ndruid.sql.planner.useApproximateTopN=true\ndruid.sql.planner.useFallback=false\ndruid.sql.planner.requireTimeCondition=false\ndruid.sql.planner.sqlTimeZone=UTC\ndruid.sql.planner.serializeComplexValues=true\n```\n\n----------------------------------------\n\nTITLE: Submitting Coordinator Dynamic Configuration in JSON (HTTP POST)\nDESCRIPTION: Example of submitting a JSON object to configure dynamic behavior of the Druid Coordinator via HTTP POST request. Includes parameters for segment management, replication, and decommissioning.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"millisToWaitBeforeDeleting\": 900000,\n  \"mergeBytesLimit\": 100000000,\n  \"mergeSegmentsLimit\" : 1000,\n  \"maxSegmentsToMove\": 5,\n  \"replicantLifetime\": 15,\n  \"replicationThrottleLimit\": 10,\n  \"emitBalancingStats\": false,\n  \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"],\n  \"decommissioningNodes\": [\"localhost:8182\", \"localhost:8282\"],\n  \"decommissioningMaxPercentOfMaxSegmentsToMove\": 70\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Timestamp Filter in Druid\nDESCRIPTION: Example of filtering on timestamp values using selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"124457387532\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Storage Parameters in Druid\nDESCRIPTION: Configuration settings for adjusting segment cache locations and maximum server size in Druid Historical processes. These parameters control the storage capacity for segment downloads.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/faq.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n-Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}]\n-Ddruid.server.maxSize=500000000000\n```\n\n----------------------------------------\n\nTITLE: Defining Common Fields for Apache Druid Alerts in JSON\nDESCRIPTION: This JSON structure outlines the common fields present in all Druid alerts. It includes timestamp, service name, host name, severity, description, and exception data if applicable.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/alerts.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"<time of alert creation>\",\n  \"service\": \"<name of service emitting the alert>\",\n  \"host\": \"<name of host emitting the alert>\",\n  \"severity\": \"<severity level e.g. anomaly, component-failure, service-failure>\",\n  \"description\": \"<description of the alert>\",\n  \"data\": {\n    \"exceptionType\": \"<type of exception if applicable>\",\n    \"exceptionMessage\": \"<exception message if applicable>\",\n    \"exceptionStackTrace\": \"<exception stack trace if applicable>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos and HTTP Basic Authenticators in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to enable the Kerberos and HTTP Basic authenticators in the authentication chain for Apache Druid. It uses the 'druid-kerberos' and 'druid-basic-security' core extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/auth.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authenticatorChain\":[\"kerberos\", \"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Example Result of Scan Query with List Format in Apache Druid\nDESCRIPTION: This JSON snippet shows an example result of a Scan query when resultFormat is set to 'list'. It includes the segmentId, columns, and events with detailed information for each row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/scan-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\",\n      \"robot\",\n      \"namespace\",\n      \"anonymous\",\n      \"unpatrolled\",\n      \"page\",\n      \"language\",\n      \"newpage\",\n      \"user\",\n      \"count\",\n      \"added\",\n      \"delta\",\n      \"variation\",\n      \"deleted\"\n    ],\n    \"events\" : [ {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._243\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 77.0,\n        \"delta\" : 77.0,\n        \"variation\" : 77.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._73\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._756\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 68.0,\n        \"delta\" : 68.0,\n        \"variation\" : 68.0,\n        \"deleted\" : 0.0\n    } ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Submitting Native TopN Query to Druid Broker using curl\nDESCRIPTION: This bash command submits the native TopN query to the Druid Broker using curl. It sends a POST request with the JSON query file as the request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8082/druid/v2?pretty\n```\n\n----------------------------------------\n\nTITLE: Post Averager Moving Average Query\nDESCRIPTION: Demonstrates calculating a 7-bucket moving average for Wikipedia edit deltas with a ratio calculation. The query uses 30-minute granularity and includes post-averaging to compute the ratio between current period and moving average values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"movingAverage\",\n  \"dataSource\": \"wikipedia\",\n  \"granularity\": {\n    \"type\": \"period\",\n    \"period\": \"PT30M\"\n  },\n  \"intervals\": [\n    \"2015-09-12T22:00:00Z/2015-09-13T00:00:00Z\"\n  ],\n  \"aggregations\": [\n    {\n      \"name\": \"delta30Min\",\n      \"fieldName\": \"delta\",\n      \"type\": \"longSum\"\n    }\n  ],\n  \"averagers\": [\n    {\n      \"name\": \"trailing30MinChanges\",\n      \"fieldName\": \"delta30Min\",\n      \"type\": \"longMean\",\n      \"buckets\": 7\n    }\n  ],\n  \"postAveragers\" : [\n    {\n      \"name\": \"ratioTrailing30MinChanges\",\n      \"type\": \"arithmetic\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"fieldName\": \"delta30Min\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"fieldName\": \"trailing30MinChanges\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up FloatMin Aggregator in Apache Druid\nDESCRIPTION: Demonstrates how to configure a floatMin aggregator in Druid. This aggregator computes the minimum of all metric values and Float.POSITIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Querying Segment Statistics in Apache Druid SQL\nDESCRIPTION: This SQL query retrieves statistics about published segments in a Druid dataSource, including average number of rows, average size, and total size. It helps identify if segment compaction is necessary.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/segment-optimization.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  \"start\",\n  \"end\",\n  version,\n  COUNT(*) AS num_segments,\n  AVG(\"num_rows\") AS avg_num_rows,\n  SUM(\"num_rows\") AS total_num_rows,\n  AVG(\"size\") AS avg_size,\n  SUM(\"size\") AS total_size\nFROM\n  sys.segments A\nWHERE\n  datasource = 'your_dataSource' AND\n  is_published = 1\nGROUP BY 1, 2, 3\nORDER BY 1, 2, 3 DESC;\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Credentials for Kinesis in Druid\nDESCRIPTION: Example of setting AWS access key and secret key for Kinesis API requests using runtime properties in Druid. If not provided, the service will look for credentials in environment variables, default profile configuration file, and EC2 instance profile provider.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\n-Ddruid.kinesis.accessKey=123 -Ddruid.kinesis.secretKey=456\n```\n\n----------------------------------------\n\nTITLE: Calculating Complex Percentage using Nested Post-Aggregations in Druid Query JSON\nDESCRIPTION: Illustrates a more complex percentage calculation using nested arithmetic post-aggregations in a Druid query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/post-aggregations.md#2025-04-09_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  ...\n  \"aggregations\" : [\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"part\", \"fieldName\" : \"part\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"part_percentage\",\n    \"fn\"     : \"*\",\n    \"fields\" : [\n       { \"type\"   : \"arithmetic\",\n         \"name\"   : \"ratio\",\n         \"fn\"     : \"/\",\n         \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"part\", \"fieldName\" : \"part\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" }\n         ]\n       },\n       { \"type\" : \"constant\", \"name\": \"const\", \"value\" : 100 }\n    ]\n  }]\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Before Rule in Apache Druid\nDESCRIPTION: The Period Drop Before Rule drops segments that fall before a specified time period. This is useful for retaining only recent data by dropping segments whose intervals are before the specified ISO-8601 period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropBeforeByPeriod\",\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Supervisor Configuration in Druid\nDESCRIPTION: Use this POST endpoint to update an existing Kafka supervisor configuration. This allows for seamless schema migrations by instructing current tasks to publish and creating new tasks with the updated configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\nPOST /druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Setting Zookeeper Behavior Properties in Apache Druid\nDESCRIPTION: Configures Zookeeper session timeout, compression, and ACL security settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.service.sessionTimeoutMs=30000\ndruid.zk.service.compress=true\ndruid.zk.service.acl=false\n```\n\n----------------------------------------\n\nTITLE: Configuring String Format Extraction in Druid\nDESCRIPTION: Demonstrates string format extraction function configuration with sprintf expression and null handling options. Supports nullString, emptyString, or returnNull handling methods.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"stringFormat\", \"format\" : <sprintf_expression>, \"nullHandling\" : <optional attribute for handling null value> }\n```\n\n----------------------------------------\n\nTITLE: CSV ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing CSV data in Druid, including column definitions and dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Ingestion Spec Count Metric Configuration\nDESCRIPTION: Configuration snippet showing how to set up a count metric in the ingestion specification for tracking the number of ingested events.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n...\n\"metricsSpec\" : [\n      {\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      },\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Buckets Histogram Aggregator in JSON\nDESCRIPTION: JSON configuration for the fixed buckets histogram aggregator. Specifies the type, output name, metric name, number of buckets, lower/upper limits, and outlier handling mode.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"fixedBucketsHistogram\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <double>,\n  \"upperLimit\" : <double>,\n  \"outlierHandlingMode\": <mode>\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Druid Console via Router\nDESCRIPTION: Example URL format to access the Druid Console through the Router process. Replace the Router IP and port with your specific deployment values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/druid-console.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: JavaScript Aggregator Example in Druid\nDESCRIPTION: A practical example of using the JavaScript aggregator to compute a custom formula: sum(log(x)*y) + 10. It demonstrates how to implement the three required JavaScript functions for custom aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"sum(log(x)*y) + 10\",\n  \"fieldNames\": [\"x\", \"y\"],\n  \"fnAggregate\" : \"function(current, a, b)      { return current + (Math.log(a) * b); }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return partialA + partialB; }\",\n  \"fnReset\"     : \"function()                   { return 10; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Demonstrates how to use a query filter HavingSpec in a groupBy query. This allows all Druid query filters to be used in the Having part of the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : <any Druid query filter>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Filtered Aggregation in Apache Druid SQL\nDESCRIPTION: Demonstrates how to apply a filter to an aggregation function in a Druid SQL query. This allows for conditional aggregation based on a WHERE clause.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nAGG(expr) FILTER(WHERE whereExpr)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hadoop-based Batch Ingestion in Druid using Java\nDESCRIPTION: HadoopDruidDetermineConfigurationJob.java determines the number of Druid segments to create, while HadoopDruidIndexerJob.java handles the actual creation of Druid segments in Hadoop-based batch ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/overview.md#2025-04-09_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHadoopDruidDetermineConfigurationJob.java\nHadoopDruidIndexerJob.java\n```\n\n----------------------------------------\n\nTITLE: Executing Native Queries in Druid using curl with Smile Format\nDESCRIPTION: Example of how to execute a native Druid query by making a POST request using the Jackson Smile binary JSON format. This can be more efficient for large queries or when network bandwidth is a concern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/querying.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor API Endpoint - Get Status\nDESCRIPTION: API endpoint to retrieve the current state of tasks managed by a Kafka supervisor, including partition offsets and consumer lag metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/supervisor/<supervisorId>/status\n```\n\n----------------------------------------\n\nTITLE: Filtering Tasks by Status in Druid\nDESCRIPTION: SQL query to retrieve information about tasks with a specific status (FAILED) from the sys.tasks table. This helps monitoring and troubleshooting ingestion tasks in the Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.tasks WHERE status='FAILED';\n```\n\n----------------------------------------\n\nTITLE: Executing TopN Query with Multiple Aggregations in Druid\nDESCRIPTION: This JSON query demonstrates a topN query in Druid with multiple aggregations, filters, and a specific dimension. It includes sum aggregations for various fields, count aggregation, and an OR filter for specific order keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/topnquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_TAX_doubleSum\",\n                 \"name\": \"L_TAX_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_DISCOUNT_doubleSum\",\n                 \"name\": \"L_DISCOUNT_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\",\n                 \"name\": \"L_EXTENDEDPRICE_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             },\n             {\n                 \"name\": \"count\",\n                 \"type\": \"count\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"filter\": {\n        \"fields\": [\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"103136\"\n            },\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"1648672\"\n            }\n        ],\n        \"type\": \"or\"\n    },\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Using IN Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A filter that matches dimension values against a set of specified values, equivalent to SQL's IN operator. This example filters for outlaws that are 'Good', 'Bad', or 'Ugly'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"in\",\n    \"dimension\": \"outlaw\",\n    \"values\": [\"Good\", \"Bad\", \"Ugly\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Deep Storage Properties in Druid\nDESCRIPTION: Configuration properties for setting up HDFS as deep storage in Druid's common.runtime.properties file. This defines the storage type and directories for segments and indexing logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kerberos-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n# Deep storage\n#\n# For HDFS:\n druid.storage.type=hdfs\n druid.storage.storageDirectory=/druid/segments\n# OR\n# druid.storage.storageDirectory=/apps/druid/segments\n\n#\n# Indexing service logs\n#\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n# OR\n# druid.storage.storageDirectory=/apps/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Native JSON Query Log Example - TSV Format\nDESCRIPTION: Example of a TSV-formatted request log entry for a native JSON query, showing timestamp, remote address, query details, and context information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_12\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1   {\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikiticker\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"LegacyDimensionSpec\",\"dimension\":\"page\",\"outputName\":\"page\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"LegacyTopNMetricSpec\",\"metric\":\"count\"},\"threshold\":10,\"intervals\":{\"type\":\"LegacySegmentSpec\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"count\"}],\"postAggregations\":[],\"context\":{\"queryId\":\"74c2d540-d700-4ebd-b4a9-3d02397976aa\"},\"descending\":false}    {\"query/time\":100,\"query/bytes\":800,\"success\":true,\"identity\":\"user1\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring QuantilesDoublesSketchToString Post Aggregator\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToString post aggregator, which returns a summary of the sketch that can be used for debugging purposes by calling the toString() method.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid SQL Server Properties\nDESCRIPTION: These properties configure the Druid SQL server on the broker, including enabling SQL, JDBC querying, connection limits, and query planning options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_20\n\nLANGUAGE: properties\nCODE:\n```\ndruid.sql.enable=false\ndruid.sql.avatica.enable=true\ndruid.sql.avatica.maxConnections=50\ndruid.sql.avatica.maxRowsPerFrame=5000\ndruid.sql.avatica.maxStatementsPerConnection=1\ndruid.sql.avatica.connectionIdleTimeout=PT5M\ndruid.sql.http.enable=true\ndruid.sql.planner.maxQueryCount=8\ndruid.sql.planner.maxSemiJoinRowsInMemory=100000\ndruid.sql.planner.maxTopNLimit=100000\ndruid.sql.planner.metadataRefreshPeriod=PT1M\ndruid.sql.planner.selectPageSize=1000\ndruid.sql.planner.useApproximateCountDistinct=true\ndruid.sql.planner.useApproximateTopN=true\ndruid.sql.planner.useFallback=false\ndruid.sql.planner.requireTimeCondition=false\ndruid.sql.planner.sqlTimeZone=UTC\n```\n\n----------------------------------------\n\nTITLE: RegexFiltered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering multi-value dimensions using regex pattern matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"regexFiltered\", \"delegate\" : <dimensionSpec>, \"pattern\": <java regex pattern> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Task in Apache Druid\nDESCRIPTION: JSON configuration for a Merge task that combines multiple segments with common timestamps. Includes options for controlling rollup behavior during merging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"segments\": <JSON list of DataSegment objects to merge>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Substring Extraction Function without Length in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Substring Extraction Function without specifying length in Apache Druid. It returns the remainder of the dimension value starting from the given index.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 3 }\n```\n\n----------------------------------------\n\nTITLE: Setting Zookeeper Behavior Properties in Apache Druid\nDESCRIPTION: Configures Zookeeper session timeout, compression, and ACL security settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.service.sessionTimeoutMs=30000\ndruid.zk.service.compress=true\ndruid.zk.service.acl=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Storage Parameters in Druid\nDESCRIPTION: Configuration settings for adjusting segment cache locations and maximum server size in Druid Historical processes. These parameters control the storage capacity for segment downloads.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/faq.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n-Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}]\n-Ddruid.server.maxSize=500000000000\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function with Length\nDESCRIPTION: Configuration for substring extraction function that returns a substring of specified length starting from given index.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 1, \"length\" : 4 }\n```\n\n----------------------------------------\n\nTITLE: Setting Up One-Sided Bound Filter in Druid (JSON)\nDESCRIPTION: This example shows how to set up a one-sided bound filter. It filters for age values less than 31.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Appended Data in Apache Druid\nDESCRIPTION: This SQL query retrieves all rows from the 'updates-tutorial' datasource after appending new data, showing the combined results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"updates-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: CSV ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing CSV data in Druid, including column definitions and dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration Properties\nDESCRIPTION: Configuration properties for HDFS and Cassandra deep storage backends in Druid. Includes required extensions and connection settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_18\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.storageDirectory=none\ndruid.storage.host=none\ndruid.storage.keyspace=none\n```\n\n----------------------------------------\n\nTITLE: Using Column Comparison Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A filter that compares two dimensions to each other, equivalent to SQL's WHERE clause with an equality condition between columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"columnComparison\", \"dimensions\": [<dimension_a>, <dimension_b>] }\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: MIT license declaration for react.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Implementing Upper Bound Age Filter in Druid\nDESCRIPTION: Example of a one-sided bound filter that checks if age is strictly less than 31.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expression Filter in Druid Queries\nDESCRIPTION: The regex filter matches a dimension against a Java regular expression pattern. It provides more flexible matching than the selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"regex\", \"dimension\": <dimension_string>, \"pattern\": <pattern_string> }\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultLimitSpec for Sorting groupBy Results in Druid\nDESCRIPTION: This JSON snippet shows the structure of the DefaultLimitSpec used to limit and sort results from a groupBy query. It includes a limit value and a list of columns for ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/limitspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"default\",\n    \"limit\"   : <integer_value>,\n    \"columns\" : [list of OrderByColumnSpec],\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Coordinator Dynamic Configuration JSON\nDESCRIPTION: Example JSON configuration object showing various Coordinator settings including deletion wait time, merge limits, replication parameters, and decommissioning settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"millisToWaitBeforeDeleting\": 900000,\n  \"mergeBytesLimit\": 100000000,\n  \"mergeSegmentsLimit\" : 1000,\n  \"maxSegmentsToMove\": 5,\n  \"replicantLifetime\": 15,\n  \"replicationThrottleLimit\": 10,\n  \"emitBalancingStats\": false,\n  \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"],\n  \"decommissioningNodes\": [\"localhost:8182\", \"localhost:8282\"],\n  \"decommissioningMaxPercentOfMaxSegmentsToMove\": 70\n}\n```\n\n----------------------------------------\n\nTITLE: Time Boundary Query Response Format in Apache Druid\nDESCRIPTION: This snippet shows the expected response format for a time boundary query in Apache Druid. It includes the timestamp and the result object containing the minTime and maxTime values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"minTime\" : \"2013-05-09T18:24:00.000Z\",\n    \"maxTime\" : \"2013-05-09T18:37:00.000Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Approximate Histogram Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the approximate histogram aggregator in Druid. This defines parameters like resolution, number of buckets, and upper/lower limits. The aggregator can be used in both ingestion and query time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"resolution\" : <integer>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <float>,\n  \"upperLimit\" : <float>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid for MySQL Metadata Storage\nDESCRIPTION: Properties to add to the Druid configuration for using MySQL as the metadata storage. This includes specifying the MySQL extension, connection URI, and credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"mysql-metadata-storage\"]\ndruid.metadata.storage.type=mysql\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Defining Selector Filter in Apache Druid JSON Query\nDESCRIPTION: The selector filter matches a specific dimension with a specific value. It is equivalent to the WHERE clause in SQL for exact matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"selector\", \"dimension\": <dimension_string>, \"value\": <dimension_value_string> }\n```\n\n----------------------------------------\n\nTITLE: TSV ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing TSV (tab-separated values) data in Druid, including delimiter and column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Default DimensionSpec in Apache Druid. It allows renaming dimensions and specifying output types for numeric columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"default\",\n  \"dimension\" : <dimension>,\n  \"outputName\": <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Timeseries Query with DistinctCount in Druid\nDESCRIPTION: Example of a Timeseries query using distinctCount aggregator to calculate unique visitor counts. The query runs on daily granularity over a specified time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example JavaScript Aggregator in Apache Druid\nDESCRIPTION: Provides an example of a JavaScript aggregator in Druid that computes sum(log(x)*y) + 10.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"sum(log(x)*y) + 10\",\n  \"fieldNames\": [\"x\", \"y\"],\n  \"fnAggregate\" : \"function(current, a, b)      { return current + (Math.log(a) * b); }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return partialA + partialB; }\",\n  \"fnReset\"     : \"function()                   { return 10; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Task Completion Report Endpoint\nDESCRIPTION: HTTP endpoint for retrieving completion reports after a task finishes execution\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/reports.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Firehose in Apache Druid\nDESCRIPTION: JSON configuration for setting up a RabbitMQ firehose in Apache Druid. Includes connection details and queue configuration parameters. Requires amqp-client-3.2.1.jar in the Druid lib directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/rabbitmq.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\" : \"rabbitmq\",\n   \"connection\" : {\n     \"host\": \"localhost\",\n     \"port\": \"5672\",\n     \"username\": \"test-dude\",\n     \"password\": \"test-word\",\n     \"virtualHost\": \"test-vhost\",\n     \"uri\": \"amqp://mqserver:1234/vhost\"\n   },\n   \"config\" : {\n     \"exchange\": \"test-exchange\",\n     \"queue\" : \"druidtest\",\n     \"routingKey\": \"#\",\n     \"durable\": \"true\",\n     \"exclusive\": \"false\",\n     \"autoDelete\": \"false\",\n     \"maxRetries\": \"10\",\n     \"retryIntervalSeconds\": \"1\",\n     \"maxDurationSeconds\": \"300\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid HTTP Server Settings\nDESCRIPTION: Jetty server configuration properties for handling HTTP requests including threads, queues, timeouts and limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_33\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.http.numThreads=max(10, (Number of cores * 17) / 16 + 2) + 30\ndruid.server.http.queueSize=Unbounded\ndruid.server.http.maxIdleTime=PT5M\ndruid.server.http.enableRequestLimit=false\ndruid.server.http.defaultQueryTimeout=300000\n```\n\n----------------------------------------\n\nTITLE: Using OR Logical Expression Filter in Druid Queries\nDESCRIPTION: The OR filter combines multiple filters with a logical OR operation. Rows matching any of the specified filters will be included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Bloom Filter Aggregator in Druid\nDESCRIPTION: Shows how to use the BLOOM_FILTER aggregator in a SQL SELECT statement for creating Bloom filters in Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT BLOOM_FILTER(<expression>, <max number of entries>) FROM druid.foo WHERE dim2 = 'abc'\n```\n\n----------------------------------------\n\nTITLE: Configuring Log4j2 for Apache Druid\nDESCRIPTION: This XML snippet demonstrates a sample log4j2.xml configuration file for Apache Druid. It sets up console logging with a specific pattern layout and configures the root logger level to 'info'. It also includes a commented-out section for enabling logging of all HTTP requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/logging.md#2025-04-09_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n\n    <!-- Uncomment to enable logging of all HTTP requests\n    <Logger name=\"org.apache.druid.jetty.RequestLog\" additivity=\"false\" level=\"DEBUG\">\n        <AppenderRef ref=\"Console\"/>\n    </Logger>\n    -->\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hash-based Partitioning in Druid\nDESCRIPTION: This JSON configuration specifies a hash-based partitioning strategy for Druid ingestion. It sets a target partition size of 5 million rows, which Druid uses to automatically determine the number of segments for optimal performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"hashed\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Filtering on Timestamp Column in Apache Druid\nDESCRIPTION: This example shows how to filter on the timestamp column (__time) using a selector filter to match a specific millisecond value expressed as a string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"124457387532\"\n}\n```\n\n----------------------------------------\n\nTITLE: Executing HTTP POST Query with Jackson Smile in Apache Druid using Bash\nDESCRIPTION: This snippet shows how to send a POST request to Druid queryable processes using the Jackson Smile format for Content-Type and Accept headers. It demonstrates an alternative serialization format for Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/querying.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties for Classloader Isolation in JSON\nDESCRIPTION: This JSON snippet shows how to set Hadoop job properties in a Druid indexing task to enable classloader isolation. It sets 'mapreduce.job.classloader' to true and specifies which classes should be loaded from the system classpath vs. job-supplied JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\": {\n  \"mapreduce.job.classloader\": \"true\",\n  \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\"\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Selector Filter for Multi-value Dimension in Druid\nDESCRIPTION: Shows a GroupBy query with a selector filter on the 'tags' dimension. This query demonstrates how filtering is applied before dimension explosion in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading Off-heap MapDB Cache in Apache Druid\nDESCRIPTION: JSON configuration for a loading off-heap MapDB cache with reverse lookup. It uses a JDBC data fetcher and specifies cache parameters such as maximum entries, store size, and expiration times.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"mapDb\", \"maxEntriesSize\":100000},\n   \"reverseLoadingCacheSpec\":{\"type\":\"mapDb\", \"maxStoreSize\":5, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Using Bloom Filter Aggregator in SQL\nDESCRIPTION: Example of using the BLOOM_FILTER aggregator function in a Druid SQL query to compute a Bloom filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT BLOOM_FILTER(<expression>, <max number of entries>) FROM druid.foo WHERE dim2 = 'abc'\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties for Classloader Isolation in JSON\nDESCRIPTION: This JSON snippet shows how to set Hadoop job properties in a Druid indexing task to enable classloader isolation. It sets 'mapreduce.job.classloader' to true and specifies which classes should be loaded from the system classpath vs. job-supplied JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\": {\n  \"mapreduce.job.classloader\": \"true\",\n  \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalFirehose in Apache Druid\nDESCRIPTION: This example shows how to configure a LocalFirehose to read data from files on local disk. The firehose is splittable and can be used by native parallel index tasks, where each worker task will read one file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"local\",\n    \"filter\"   : \"*.csv\",\n    \"baseDir\"  : \"/data/directory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Cache in Druid\nDESCRIPTION: Configuration options for using Memcached as a cache backend in Druid. This allows all processes to share the same cache. Settings include expiration time, timeout, hosts, object size, and connection parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_50\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.expiration=2592000\ndruid.cache.timeout=500\ndruid.cache.hosts=none\ndruid.cache.maxObjectSize=52428800\ndruid.cache.memcachedPrefix=druid\ndruid.cache.numConnections=1\ndruid.cache.protocol=binary\ndruid.cache.locator=consistent\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch to Estimate Post Aggregator\nDESCRIPTION: Post aggregator that returns a distinct count estimate from an ArrayOfDoublesSketch. Used to extract the estimated number of distinct keys from a pre-computed sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: TSV ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing TSV (tab-separated values) data in Druid, including delimiter and column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Aggregators for Flattened Metrics in Druid\nDESCRIPTION: Examples of how to set up aggregators that use metric column names defined in the flattenSpec. This snippet shows longSum and doubleSum aggregators referencing both auto-discovered and explicitly defined fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/flatten-json.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metricsSpec\" : [ \n{\n  \"type\" : \"longSum\",\n  \"name\" : \"path-metric-sum\",\n  \"fieldName\" : \"path-metric\"\n}, \n{\n  \"type\" : \"doubleSum\",\n  \"name\" : \"hello-0-sum\",\n  \"fieldName\" : \"hello-0\"\n},\n{\n  \"type\" : \"longSum\",\n  \"name\" : \"metrica-sum\",\n  \"fieldName\" : \"metrica\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Implementing Time Interval Filter in Druid\nDESCRIPTION: Example of an interval filter for time ranges in October and November 2014.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Management Properties in Apache Druid\nDESCRIPTION: This markdown table defines configuration properties for segment management in Druid, including segment discovery method and load queue implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.serverview.type`|batch or http|Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper.|batch|\n|`druid.coordinator.loadqueuepeon.type`|curator or http|Whether to use \"http\" or \"curator\" implementation to assign segment loads/drops to Historical|curator|\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter in Druid Queries\nDESCRIPTION: JSON structure for specifying a bloom filter in Druid queries, including required fields like dimension and bloomKFilter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bloom\",\n  \"dimension\" : <dimension_name>,\n  \"bloomKFilter\" : <serialized_bytes_for_BloomKFilter>,\n  \"extractionFn\" : <extraction_fn>\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Selector Filter for Multi-value Dimension in Druid\nDESCRIPTION: Shows a GroupBy query with a selector filter on the 'tags' dimension. This query demonstrates how filtering is applied before dimension explosion in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering on Numeric Column in Druid JSON\nDESCRIPTION: Shows how to filter on a numeric column 'myFloatColumn' for the specific value 10.1 using a selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"myFloatColumn\",\n  \"value\": \"10.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Group By Query with Multiple Dimensions in Druid SQL\nDESCRIPTION: SQL query that groups Wikipedia data by both channel and page, calculating the sum of added lines and ordering by that sum in descending order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT channel, page, SUM(added)\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'\nGROUP BY channel, page\nORDER BY SUM(added) DESC\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Parser with TimeAndDims ParseSpec in Druid\nDESCRIPTION: JSON configuration example for ingesting Parquet files using the 'parquet' parser with a 'timeAndDims' parseSpec. This approach specifies dimensions explicitly without using field discovery or flattening.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Group By Query with Multiple Dimensions in Druid SQL\nDESCRIPTION: SQL query that groups Wikipedia data by both channel and page, calculating the sum of added lines and ordering by that sum in descending order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT channel, page, SUM(added)\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'\nGROUP BY channel, page\nORDER BY SUM(added) DESC\n```\n\n----------------------------------------\n\nTITLE: Theta Sketch Set Operations Post Aggregator\nDESCRIPTION: Configuration for Theta sketch set operations (union, intersection, not) between multiple sketches. This enables combining multiple sketches to perform operations like finding overlapping unique users.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchSetOp\",\n  \"name\": <output name>,\n  \"func\": <UNION|INTERSECT|NOT>,\n  \"fields\"  : <array of fieldAccess type post aggregators to access the thetaSketch aggregators or thetaSketchSetOp type post aggregators to allow arbitrary combination of set operations>,\n  \"size\": <16384 by default, must be max of size from sketches in fields input>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Arbitrary Granularity Spec in Druid\nDESCRIPTION: JSON configuration for arbitrary granularity specification that creates evenly sized segments with arbitrary intervals. Includes queryGranularity, rollup, and intervals parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": [\"string\"]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter in Druid Queries\nDESCRIPTION: JSON structure for specifying a bloom filter in Druid queries, including required fields like dimension and bloomKFilter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bloom\",\n  \"dimension\" : <dimension_name>,\n  \"bloomKFilter\" : <serialized_bytes_for_BloomKFilter>,\n  \"extractionFn\" : <extraction_fn>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing pull-deps Tool with Default Version in Apache Druid\nDESCRIPTION: This command shows how to use the pull-deps tool with a default version specified. It downloads the same extensions and Hadoop clients as the previous example, but uses a default version for the Druid extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/pull-deps.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.15.0-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Caching in Druid\nDESCRIPTION: These properties control caching behavior on the Druid broker, including enabling/disabling cache, result level caching, and cache limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_21\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.cache.useCache=false\ndruid.broker.cache.populateCache=false\ndruid.broker.cache.useResultLevelCache=false\ndruid.broker.cache.populateResultLevelCache=false\ndruid.broker.cache.resultLevelCacheLimit=Integer.MAX_VALUE\ndruid.broker.cache.unCacheable=[\"groupBy\", \"select\"]\ndruid.broker.cache.cacheBulkMergeLimit=Integer.MAX_VALUE\n```\n\n----------------------------------------\n\nTITLE: Submitting Native JSON Query to Druid via curl\nDESCRIPTION: A curl command that submits the native JSON query to the Druid Broker endpoint. It uses POST method with appropriate content type header.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8082/druid/v2?pretty\n```\n\n----------------------------------------\n\nTITLE: Defining Protobuf Message Structure\nDESCRIPTION: Protobuf message definition for the metrics data structure, saved as metrics.proto.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\nmessage Metrics {\n  string unit = 1;\n  string http_method = 2;\n  int32 value = 3;\n  string timestamp = 4;\n  string http_code = 5;\n  string page = 6;\n  string metricType = 7;\n  string server = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Exhibitor Integration in Apache Druid\nDESCRIPTION: These properties set up integration with Exhibitor for dynamic Zookeeper cluster management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\ndruid.exhibitor.service.hosts=none\ndruid.exhibitor.service.port=8080\ndruid.exhibitor.service.restUriPath=/exhibitor/v1/cluster/list\ndruid.exhibitor.service.useSsl=false\ndruid.exhibitor.service.pollingMs=10000\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Source Hadoop InputSpec\nDESCRIPTION: JSON configuration for combining multiple input sources using the multi inputSpec type, which enables delta ingestion and combining data from multiple dataSources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"multi\",\n    \"children\": [\n      {\n        \"type\" : \"dataSource\",\n        \"ingestionSpec\" : {\n          \"dataSource\": \"wikipedia\",\n          \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"],\n          \"segments\": [\n            {\n              \"dataSource\": \"test1\",\n              \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\",\n              \"version\": \"v2\",\n              \"loadSpec\": {\n                \"type\": \"local\",\n                \"path\": \"/tmp/index1.zip\"\n              },\n              \"dimensions\": \"host\",\n              \"metrics\": \"visited_sum,unique_hosts\",\n              \"shardSpec\": {\n                \"type\": \"none\"\n              },\n              \"binaryVersion\": 9,\n              \"size\": 2,\n              \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\"\n            }\n          ]\n        }\n      },\n      {\n        \"type\" : \"static\",\n        \"paths\": \"/path/to/more/wikipedia/data/\"\n      }\n    ]  \n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query Log Example - TSV Format\nDESCRIPTION: Example of a TSV-formatted request log entry for an SQL query, showing timestamp, remote address, query metrics, and SQL query details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_13\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1       {\"sqlQuery/time\":100,\"sqlQuery/bytes\":600,\"success\":true,\"identity\":\"user1\"}  {\"query\":\"SELECT page, COUNT(*) AS Edits FROM wikiticker WHERE __time BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\",\"context\":{\"sqlQueryId\":\"c9d035a0-5ffd-4a79-a865-3ffdadbb5fdd\",\"nativeQueryIds\":\"[490978e4-f5c7-4cf6-b174-346e63cf8863]\"}}\n```\n\n----------------------------------------\n\nTITLE: Downloading and extracting Apache Kafka in Bash\nDESCRIPTION: Commands to download Kafka 0.10.2.0, extract the archive, and navigate to the Kafka directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz\ntar -xzf kafka_2.11-0.10.2.0.tgz\ncd kafka_2.11-0.10.2.0\n```\n\n----------------------------------------\n\nTITLE: Setting Historical General Configuration in YAML\nDESCRIPTION: YAML configuration for general Historical settings, including max size, tier, and priority.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_37\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.server.maxSize: 0\ndruid.server.tier: _default_tier\ndruid.server.priority: 0\n```\n\n----------------------------------------\n\nTITLE: Marking Segments Used/Unused in Druid Coordinator API\nDESCRIPTION: POST endpoints to mark segments as used or unused for a datasource. Accepts interval or segment IDs in the request payload.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_2\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/datasources/{dataSourceName}/markUsed\nPOST /druid/coordinator/v1/datasources/{dataSourceName}/markUnused\n```\n\n----------------------------------------\n\nTITLE: Querying Initial Data in Apache Druid\nDESCRIPTION: This SQL query retrieves all rows from the 'updates-tutorial' datasource, showing the initial data loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"updates-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalFirehose in Apache Druid\nDESCRIPTION: This example shows how to configure a LocalFirehose to read data from files on local disk. The firehose is splittable and can be used by native parallel index tasks, where each worker task will read one file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"local\",\n    \"filter\"   : \"*.csv\",\n    \"baseDir\"  : \"/data/directory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Backwards Compatible Pagination Query\nDESCRIPTION: Example of a Select query using the fromNext parameter for backwards compatibility with older Druid versions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/select-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Metadata Query in Apache Druid\nDESCRIPTION: This snippet shows the configuration properties for Segment Metadata queries in Apache Druid, including default history and analysis types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_45\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.segmentMetadata.defaultHistory`|When no interval is specified in the query, use a default interval of defaultHistory before the end time of the most recent segment, specified in ISO8601 format. This property also controls the duration of the default interval used by GET /druid/v2/datasources/{dataSourceName} interactions for retrieving datasource dimensions/metrics.|P1W|\n|`druid.query.segmentMetadata.defaultAnalysisTypes`|This can be used to set the Default Analysis Types for all segment metadata queries, this can be overridden when making the query|[\"cardinality\", \"interval\", \"minmax\"]|\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy Query on Multi-value Dimensions in Druid\nDESCRIPTION: A GroupBy query that groups results by the 'tags' dimension without any filtering. When used with multi-value dimensions, this will explode each value into its own group in the results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lower Bound Filter in Druid Query (JSON)\nDESCRIPTION: This snippet demonstrates how to configure a lower bound filter in a Druid query. It filters for age values greater than or equal to 18.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"18\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Direct File Reference\nDESCRIPTION: JSON configuration for a URI-based lookup that references a specific file in S3. This example shows how to configure a cached lookup using a direct file URI with CSV format parsing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uri\": \"s3://bucket/some/key/prefix/renames-0003.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Query with Header in JSON\nDESCRIPTION: Example demonstrating how to request a header in the query results by setting the header parameter to true, while using arrayLines format for the results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"arrayLines\",\n  \"header\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring InsensitiveContainsSearchQuerySpec in Apache Druid\nDESCRIPTION: This snippet demonstrates the JSON configuration for an InsensitiveContainsSearchQuerySpec. It matches if any part of a dimension value contains the specified value, regardless of case.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"insensitive_contains\",\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Mixed Version Segments in Druid\nDESCRIPTION: Demonstrates a scenario where the cluster contains a mixture of v1 and v2 segments during the update process. This example shows how different versions can coexist for different time intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v2_1\nfoo_2015-01-03/2015-01-04_v1_2\n``` \n```\n\n----------------------------------------\n\nTITLE: Configuring arbitrary granularity specifications in Apache Druid\nDESCRIPTION: The arbitrary granularity spec is used to generate segments with arbitrary intervals, attempting to create evenly sized segments. This specification includes query granularity, rollup behavior, and intervals but does not support real-time processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryGranularity\": \"NONE\",\n  \"rollup\": true,\n  \"intervals\": [\"interval1\", \"interval2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring arbitrary granularity specifications in Apache Druid\nDESCRIPTION: The arbitrary granularity spec is used to generate segments with arbitrary intervals, attempting to create evenly sized segments. This specification includes query granularity, rollup behavior, and intervals but does not support real-time processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryGranularity\": \"NONE\",\n  \"rollup\": true,\n  \"intervals\": [\"interval1\", \"interval2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Bound Filter in Druid JSON\nDESCRIPTION: Demonstrates how to create a bound filter in Druid to filter values between 21 and 31 for the 'age' dimension using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query Results Format in Apache Druid\nDESCRIPTION: Example showing the JSON format of results returned from a timeseries query. Each result contains a timestamp and values for the requested aggregations and post-aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": { \"sample_name1\": <some_value>, \"sample_name2\": <some_value>, \"sample_divide\": <some_value> } \n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": { \"sample_name1\": <some_value>, \"sample_name2\": <some_value>, \"sample_divide\": <some_value> }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Submitting a Kill Task to permanently delete disabled segments\nDESCRIPTION: cURL command to submit a Kill Task JSON specification to the Druid Overlord API. This task permanently removes disabled segments from both metadata storage and deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Configuring Compaction Task in Apache Druid\nDESCRIPTION: Example JSON configuration for setting up a compaction task for the 'wikiticker' datasource in Apache Druid. This basic configuration specifies only the required dataSource property, relying on default values for other configuration options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSource\": \"wikiticker\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initial Data Loading Task\nDESCRIPTION: Bash command to submit the initial indexing task that creates multiple segments per hour from Wikipedia edits data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticAzureBlobStoreFirehose in JSON for Apache Druid\nDESCRIPTION: Sample JSON configuration for the StaticAzureBlobStoreFirehose, which ingests events from Azure Blob Store. This firehose is splittable and can be used by native parallel index tasks, with each worker task reading a separate object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/azure.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-azure-blobstore\",\n    \"blobs\": [\n        {\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using LIKE Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A filter for wildcard searches equivalent to SQL's LIKE operator. This example filters for last names that start with 'D'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"like\",\n    \"dimension\": \"last_name\",\n    \"pattern\": \"D%\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Test Stats Aggregators in Druid Query\nDESCRIPTION: This JSON example demonstrates a complete implementation of test statistics in a Druid query. It calculates a z-score using the zscore2sample post aggregator with constant values and then feeds that z-score to the pvalue2tailedZtest post aggregator to calculate the p-value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n    \"postAggregations\" : {\n    \"type\"   : \"pvalue2tailedZtest\",\n    \"name\"   : \"pvalue\",\n    \"zScore\" : \n    {\n     \"type\"   : \"zscore2sample\",\n     \"name\"   : \"zscore\",\n     \"successCount1\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation1Sample\",\n         \"value\"  : 300\n       },\n     \"sample1Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation1\",\n         \"value\"  : 500\n       },\n     \"successCount2\":\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation2Sample\",\n         \"value\"  : 450\n       },\n     \"sample2Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation2\",\n         \"value\"  : 600\n       }\n     }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: AND Filter Query for Multi-value Dimensions\nDESCRIPTION: Example of an AND filter query that requires multiple values to be present in a multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"and\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Indexer-Specific Zookeeper Paths in Apache Druid\nDESCRIPTION: Configures Zookeeper paths specific to the indexing service in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.indexer.base=${druid.zk.paths.base}/indexer\ndruid.zk.paths.indexer.announcementsPath=${druid.zk.paths.indexer.base}/announcements\ndruid.zk.paths.indexer.tasksPath=${druid.zk.paths.indexer.base}/tasks\ndruid.zk.paths.indexer.statusPath=${druid.zk.paths.indexer.base}/status\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter Aggregator\nDESCRIPTION: JSON structure for using the Bloom filter aggregator in Druid queries, specifying the aggregator type, output name, maximum number of entries, and dimension specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"type\": \"bloom\",\n      \"name\": <output_field_name>,\n      \"maxNumEntries\": <maximum_number_of_elements_for_BloomKFilter>\n      \"field\": <dimension_spec>\n    }\n```\n\n----------------------------------------\n\nTITLE: Using OR Logical Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A logical OR filter that combines multiple other filters, requiring at least one condition to be true for a row to match.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Querying Segments Table for a Specific Datasource in Druid SQL\nDESCRIPTION: SQL query to retrieve all segments for a specific datasource (wikipedia) from the sys.segments table. This provides information about all segments belonging to the wikipedia datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n```\n\n----------------------------------------\n\nTITLE: Numeric Range Filter in Druid JSON\nDESCRIPTION: Demonstrates how to create a bound filter for a numeric range on 'myFloatColumn' where 10 <= myFloatColumn < 20.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"bound\",\n  \"dimension\": \"myFloatColumn\",\n  \"ordering\": \"numeric\",\n  \"lower\": \"10\",\n  \"lowerStrict\": false,\n  \"upper\": \"20\",\n  \"upperStrict\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Segments Table for a Specific Datasource in Druid SQL\nDESCRIPTION: SQL query to retrieve all segments for a specific datasource (wikipedia) from the sys.segments table. This provides information about all segments belonging to the wikipedia datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n```\n\n----------------------------------------\n\nTITLE: Filtering on Numeric Columns with Selector in Apache Druid\nDESCRIPTION: This example shows how to filter on a floating-point column by using a selector filter with the value expressed as a string. Druid automatically converts this to a numeric predicate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"myFloatColumn\",\n  \"value\": \"10.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter Aggregator\nDESCRIPTION: JSON structure for the bloom filter aggregator that creates bloom filters from query results. It includes configuration for output field name, the dimension to process, and maximum number of distinct elements the filter can represent.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"type\": \"bloom\",\n      \"name\": <output_field_name>,\n      \"maxNumEntries\": <maximum_number_of_elements_for_BloomKFilter>\n      \"field\": <dimension_spec>\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring a URI-based Cached Namespace Lookup in Druid\nDESCRIPTION: Example configuration for a globally cached lookup using the URI type. This configuration specifies a file-based lookup with CSV format, defining key-value columns and a polling period for updates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\": \"cachedNamespace\",\n   \"extractionNamespace\": {\n      \"type\": \"uri\",\n      \"uri\": \"file:/tmp/prefix/\",\n      \"namespaceParseSpec\": {\n        \"format\": \"csv\",\n        \"columns\": [\n          \"key\",\n          \"value\"\n        ]\n      },\n      \"pollPeriod\": \"PT5M\"\n    },\n    \"firstCacheTimeout\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Lookup Extractor Factory in Druid\nDESCRIPTION: JSON configuration for setting up a Kafka lookup extractor factory. Specifies the Kafka topic and required Kafka properties for connection. Requires zookeeper connection details and supports optional timeout and mapping type settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"kafka\",\n  \"kafkaTopic\":\"testTopic\",\n  \"kafkaProperties\":{\"zookeeper.connect\":\"somehost:2181/kafka\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Emitter Properties in Druid\nDESCRIPTION: Configuration properties for the Logging Emitter module in Druid that handles metrics emission through logging. Includes settings for logger class selection and log level configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=logging\ndruid.emitter.logging.loggerClass=LoggingEmitter\ndruid.emitter.logging.logLevel=info\n```\n\n----------------------------------------\n\nTITLE: Sample groupBy Query Response Format\nDESCRIPTION: Example of the response format returned by a groupBy query, showing the structure of result objects including version, timestamp, and event data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:00.000Z\",\n    \"event\" : {\n      \"country\" : <some_dim_value_one>,\n      \"device\" : <some_dim_value_two>,\n      \"total_usage\" : <some_value_one>,\n      \"data_transfer\" :<some_value_two>,\n      \"avg_usage\" : <some_avg_usage_value>\n    }\n  }, \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:12.000Z\",\n    \"event\" : {\n      \"dim1\" : <some_other_dim_value_one>,\n      \"dim2\" : <some_other_dim_value_two>,\n      \"sample_name1\" : <some_other_value_one>,\n      \"sample_name2\" :<some_other_value_two>,\n      \"avg_usage\" : <some_other_avg_usage_value>\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example Timeseries Query with Variance\nDESCRIPTION: Sample timeseries query demonstrating the use of variance aggregator over a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"testing\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Execute SQL Query via cURL\nDESCRIPTION: Example showing how to execute a Druid SQL query using cURL from the command line. Demonstrates sending a simple COUNT query as JSON over HTTP.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cat query.json\n{\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"}\n\n$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json\n[{\"TheCount\":24433}]\n```\n\n----------------------------------------\n\nTITLE: Running Tranquility with Thrift Extensions\nDESCRIPTION: Command-line example for running Tranquility with the Thrift extensions. This snippet shows how to specify the Druid extensions directory and load the Thrift extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka \\\n  -configFile $jsonConfig \\\n  -Ddruid.extensions.directory=/path/to/extensions \\\n  -Ddruid.extensions.loadList='[\"druid-thrift-extensions\"]'\n```\n\n----------------------------------------\n\nTITLE: Executing Druid SQL Query via JDBC (Java)\nDESCRIPTION: Demonstrates how to execute a Druid SQL query using the Avatica JDBC driver in Java, including connection setup and result set processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Druid SQL Query via JDBC (Java)\nDESCRIPTION: Demonstrates how to execute a Druid SQL query using the Avatica JDBC driver in Java, including connection setup and result set processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Count Aggregator in Druid\nDESCRIPTION: Basic count aggregator that computes the number of Druid rows matching specified filters. Takes an output name parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"count\", \"name\" : <output_name> }\n```\n\n----------------------------------------\n\nTITLE: Data Source Metadata Query Result Format in Apache Druid\nDESCRIPTION: This JSON structure shows the format of the result returned by a Data Source Metadata query in Apache Druid. It includes the timestamp of the query execution and the maxIngestedEventTime, which represents the timestamp of the latest ingested event for the queried dataSource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"maxIngestedEventTime\" : \"2013-05-09T18:24:09.007Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Access for Hadoop Indexing Tasks\nDESCRIPTION: Job properties configuration for loading data from S3 using EMR with Druid. Includes AWS credentials, S3 filesystem implementation, and compression codec settings required for the tuningConfig section of a Hadoop indexing task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\" : {\n   \"fs.s3.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"fs.s3n.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3n.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3n.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Segment Statistics using SQL in Apache Druid\nDESCRIPTION: This SQL query retrieves statistics about published segments in a specified dataSource, including average number of rows, average size, and total size. It helps identify segments that may need compaction.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/segment-optimization.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  \"start\",\n  \"end\",\n  version,\n  COUNT(*) AS num_segments,\n  AVG(\"num_rows\") AS avg_num_rows,\n  SUM(\"num_rows\") AS total_num_rows,\n  AVG(\"size\") AS avg_size,\n  SUM(\"size\") AS total_size\nFROM\n  sys.segments A\nWHERE\n  datasource = 'your_dataSource' AND\n  is_published = 1\nGROUP BY 1, 2, 3\nORDER BY 1, 2, 3 DESC;\n```\n\n----------------------------------------\n\nTITLE: Field Accessor Post-Aggregator Implementation\nDESCRIPTION: Shows two types of field accessor post-aggregators that return either raw aggregation objects or finalized values from specified aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"fieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"finalizingFieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authenticator in Apache Druid\nDESCRIPTION: This snippet shows how to configure a Basic authenticator in Druid's common runtime properties file. It sets up an authenticator chain, specifies the authenticator type, and sets initial passwords for admin and internal client users.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyBasicAuthenticator\"]\n\ndruid.auth.authenticator.MyBasicAuthenticator.type=basic\ndruid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1\ndruid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2\ndruid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring SqlFirehose in Apache Druid\nDESCRIPTION: SqlFirehose configuration for ingesting events from an RDBMS. Requires MySQL or PostgreSQL Metadata Store extension. The example shows a MySQL connection with multiple SQL queries to fetch data for indexing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"sql\",\n    \"database\": {\n        \"type\": \"mysql\",\n        \"connectorConfig\" : {\n            \"connectURI\" : \"jdbc:mysql://host:port/schema\",\n            \"user\" : \"user\",\n            \"password\" : \"password\"\n        }\n     },\n    \"sqls\" : [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining MomentSketch Post-Aggregator for Quantiles\nDESCRIPTION: JSON configuration for the momentSketchSolveQuantiles post-aggregator that calculates specific quantiles from a moment sketch. It requires the field reference and an array of fractions representing the desired quantiles.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"momentSketchSolveQuantiles\",\n  \"name\" : <output_name>,\n  \"field\" : <reference to moment sketch>,\n  \"fractions\" : <array of doubles in [0,1]>\n}\n```\n\n----------------------------------------\n\nTITLE: IndexSpec Configuration Table in Markdown\nDESCRIPTION: Configuration table defining the available parameters for IndexSpec including bitmap, dimensionCompression, metricCompression, and longEncoding options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|bitmap|Object|Compression format for bitmap indexes. Should be a JSON object; see below for options.|no (defaults to Concise)|\n|dimensionCompression|String|Compression format for dimension columns. Choose from `LZ4`, `LZF`, or `uncompressed`.|no (default == `LZ4`)|\n|metricCompression|String|Compression format for metric columns. Choose from `LZ4`, `LZF`, `uncompressed`, or `none`.|no (default == `LZ4`)|\n|longEncoding|String|Encoding format for metric and dimension columns with type long. Choose from `auto` or `longs`. `auto` encodes the values using offset or lookup table depending on column cardinality, and store them with variable size. `longs` stores the value as is with 8 bytes each.|no (default == `longs`)|\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Micro-Quickstart Single Server Deployment\nDESCRIPTION: Command to start Druid in micro-quickstart configuration, designed for small machines with 4 CPU and 16GB RAM, primarily for evaluation purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/single-server.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-micro-quickstart\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Druid Multi-value Column Encoding\nDESCRIPTION: Example illustrating how Druid handles multi-value columns with modified dictionary encoding, array-based column data, and bitmap structures that support multiple values per row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/segments.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   [0,1],  <--Row value of multi-value column can have array of values\n   1,\n   1]\n\n3: Bitmaps - one for each unique value\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,1,1,1]\n                            ^\n                            |\n                            |\n    Multi-value column has multiple non-zero entries\n```\n\n----------------------------------------\n\nTITLE: Submitting Kafka Supervisor to Druid via API in Bash\nDESCRIPTION: Command to submit a Kafka supervisor specification to the Druid overlord using curl, which configures Druid to ingest data from the Kafka topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8081/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Retrieving Datasource Information from Broker using GET HTTP Requests\nDESCRIPTION: Endpoints for retrieving metadata about datasources, including lists of queryable datasources, dimensions, metrics, and segment information with server locations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_14\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/v2/datasources\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/v2/datasources/{dataSourceName}\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/v2/datasources/{dataSourceName}/dimensions\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/v2/datasources/{dataSourceName}/metrics\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/v2/datasources/{dataSourceName}/candidates?intervals={comma-separated-intervals-in-ISO8601-format}&numCandidates={numCandidates}\n```\n\n----------------------------------------\n\nTITLE: Event Count Metrics Specification\nDESCRIPTION: Example showing metrics specification for counting ingested events at ingestion time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metricsSpec\" : [\n      {\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }\n```\n\n----------------------------------------\n\nTITLE: Querying transformed data in Apache Druid using SQL\nDESCRIPTION: This SQL query selects all columns from the 'transform-tutorial' datasource to view the ingested and transformed data in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"transform-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Initial Data Loading Command\nDESCRIPTION: Command to load the Wikipedia edits sample data using a pre-defined ingestion spec that creates multiple segments per hour.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Complete Druid Ingestion Task Specification JSON\nDESCRIPTION: The final ingestion specification with complete dataSchema, ioConfig, and tuningConfig. This specification defines how network flow data will be parsed, transformed, and stored in Druid segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Druid SQL Query via JDBC (Java)\nDESCRIPTION: Demonstrates how to execute a Druid SQL query using the Avatica JDBC driver in Java, including connection setup and result set processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your Broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Segment Metadata Query Response Format in Druid\nDESCRIPTION: Example response format showing segment metadata including intervals, column information, aggregators, query granularity, and size metrics for a segment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"id\" : \"some_id\",\n  \"intervals\" : [ \"2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z\" ],\n  \"columns\" : {\n    \"__time\" : { \"type\" : \"LONG\", \"hasMultipleValues\" : false, \"size\" : 407240380, \"cardinality\" : null, \"errorMessage\" : null },\n    \"dim1\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : 1944, \"errorMessage\" : null },\n    \"dim2\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : true, \"size\" : 100000, \"cardinality\" : 1504, \"errorMessage\" : null },\n    \"metric1\" : { \"type\" : \"FLOAT\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : null, \"errorMessage\" : null }\n  },\n  \"aggregators\" : {\n    \"metric1\" : { \"type\" : \"longSum\", \"name\" : \"metric1\", \"fieldName\" : \"metric1\" }\n  },\n  \"queryGranularity\" : {\n    \"type\": \"none\"\n  },\n  \"size\" : 300000,\n  \"numRows\" : 5000000\n} ]\n```\n\n----------------------------------------\n\nTITLE: Extraction DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for transforming dimension values using extraction functions with support for output type conversion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : <dimension>,\n  \"outputName\" :  <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">,\n  \"extractionFn\" : <extraction_function>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet-Avro Ingestion with Avro ParseSpec in Druid\nDESCRIPTION: Configuration example for ingesting Parquet files using the 'parquet-avro' parser type with an 'avro' parseSpec format. This setup converts Parquet data to Avro records and uses the Avro extensions module for parsing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet-avro\",\n        \"parseSpec\": {\n          \"format\": \"avro\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Compaction in Apache Druid\nDESCRIPTION: A minimal example of compaction configuration for a Druid datasource named 'wikiticker'. This configuration demonstrates the basic required settings for initiating compaction tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSource\": \"wikiticker\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Storage Type\nDESCRIPTION: Configuration to specify the type of metadata storage backend to use. Options include 'mysql', 'postgresql', or 'derby' (default).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=mysql\n```\n\n----------------------------------------\n\nTITLE: Creating a Derivative DataSource Supervisor in Apache Druid\nDESCRIPTION: This JSON configuration specifies a 'derivativeDataSource' supervisor that creates and maintains a derived dataSource from the 'wikiticker' base dataSource. It selects a subset of dimensions and metrics from the base dataSource to improve query performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"derivativeDataSource\",\n    \"baseDataSource\": \"wikiticker\",\n    \"dimensionsSpec\": {\n        \"dimensions\": [\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\"\n        ]\n    },\n    \"metricsSpec\": [\n        {\n            \"name\": \"count\",\n            \"type\": \"count\"\n        },\n        {\n            \"name\": \"added\",\n            \"type\": \"longSum\",\n            \"fieldName\": \"added\"\n        }\n    ],\n    \"tuningConfig\": {\n        \"type\": \"hadoop\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Derivative DataSource Supervisor in Apache Druid\nDESCRIPTION: This JSON configuration specifies a 'derivativeDataSource' supervisor that creates and maintains a derived dataSource from the 'wikiticker' base dataSource. It selects a subset of dimensions and metrics from the base dataSource to improve query performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"derivativeDataSource\",\n    \"baseDataSource\": \"wikiticker\",\n    \"dimensionsSpec\": {\n        \"dimensions\": [\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\"\n        ]\n    },\n    \"metricsSpec\": [\n        {\n            \"name\": \"count\",\n            \"type\": \"count\"\n        },\n        {\n            \"name\": \"added\",\n            \"type\": \"longSum\",\n            \"fieldName\": \"added\"\n        }\n    ],\n    \"tuningConfig\": {\n        \"type\": \"hadoop\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Task Submission Response Example\nDESCRIPTION: Shows the expected JSON response from Druid when successfully submitting an ingestion task, containing the task ID.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n{\"task\":\"index_wikipedia_2018-06-09T21:30:32.802Z\"}\n```\n\n----------------------------------------\n\nTITLE: Selector Filter Having Clause in Druid\nDESCRIPTION: Shows how to use a selector filter within a Having clause to filter results based on specific dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : {\n              \"type\": \"selector\",\n              \"dimension\" : \"<dimension>\",\n              \"value\" : \"<dimension_value>\"\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining DefaultLimitSpec for Sorting groupBy Results in Druid\nDESCRIPTION: This JSON structure defines the DefaultLimitSpec used to sort and limit groupBy query results. It includes a limit and a list of columns for ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/limitspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"default\",\n    \"limit\"   : <integer_value>,\n    \"columns\" : [list of OrderByColumnSpec],\n}\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Configuration for Protobuf Ingestion\nDESCRIPTION: Complete Supervisor specification JSON for configuring Kafka ingestion with Protobuf parsing in Druid\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka2\",\n    \"parser\": {\n      \"type\": \"protobuf\",\n      \"descriptor\": \"file:///tmp/metrics.desc\",\n      \"protoMessageType\": \"Metrics\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"unit\",\n            \"http_method\",\n            \"http_code\",\n            \"page\",\n            \"metricType\",\n            \"server\"\n          ],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics_pb\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring EventReceiverFirehose in Apache Druid\nDESCRIPTION: EventReceiverFirehose ingests events using an HTTP endpoint. It's used in stream-pull ingestion and tasks automatically generated by Tranquility stream push.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/firehose.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"receiver\",\n  \"serviceName\": \"eventReceiverServiceName\",\n  \"bufferSize\": 10000\n}\n```\n\n----------------------------------------\n\nTITLE: Query Filter Having Clause in Druid\nDESCRIPTION: Demonstrates how to use query filters in Having clauses to filter groupBy results using any Druid query filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : <any Druid query filter>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query Using Bloom Filter Aggregator in Druid\nDESCRIPTION: This SQL snippet demonstrates how to use the BLOOM_FILTER aggregator function in a Druid SQL query to compute a Bloom Filter based on an expression and maximum number of entries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT BLOOM_FILTER(<expression>, <max number of entries>) FROM druid.foo WHERE dim2 = 'abc'\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Aggregator\nDESCRIPTION: JSON configuration for setting up a Theta sketch aggregator with customizable parameters including output name, field name, input type, and sketch size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"thetaSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,  \n  \"isInputThetaSketch\": false,\n  \"size\": 16384\n }\n```\n\n----------------------------------------\n\nTITLE: Requesting Header in Druid SQL Query Result (JSON)\nDESCRIPTION: Shows how to request a header in the query result by setting the 'header' parameter to true in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"arrayLines\",\n  \"header\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Time Interval Filter in Druid\nDESCRIPTION: Creates an interval filter for specific date ranges using ISO 8601 time intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Protobuf Parser for Druid Ingestion\nDESCRIPTION: JSON configuration for the Protobuf parser in Druid. It specifies the parser type, descriptor file, message type, and parse specification for ingesting Protobuf data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"protobuf\",\n  \"descriptor\": \"file:///tmp/metrics.desc\",\n  \"protoMessageType\": \"Metrics\",\n  \"parseSpec\": {\n    \"format\": \"json\",\n    \"timestampSpec\": {\n      \"column\": \"timestamp\",\n      \"format\": \"auto\"\n    },\n    \"dimensionsSpec\": {\n      \"dimensions\": [\n        \"unit\",\n        \"http_method\",\n        \"http_code\",\n        \"page\",\n        \"metricType\",\n        \"server\"\n      ],\n      \"dimensionExclusions\": [\n        \"timestamp\",\n        \"value\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Count Aggregator in Druid JSON\nDESCRIPTION: Defines a count aggregator to compute the number of Druid rows matching filters. The aggregator takes an output name parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"count\", \"name\" : <output_name> }\n```\n\n----------------------------------------\n\nTITLE: Defining Expression Virtual Column in Apache Druid (JSON)\nDESCRIPTION: This snippet shows the syntax for defining an expression virtual column in Apache Druid. It includes properties for specifying the column name, expression, and output type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/virtual-columns.md#2025-04-09_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <name of the virtual column>,\n  \"expression\": <row expression>,\n  \"outputType\": <output value type of expression>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-dimension Partitioning in Apache Druid\nDESCRIPTION: This snippet demonstrates how to set up single-dimension partitioning for Druid ingestion. It specifies the partitioning type as 'dimension' and sets a target partition size, allowing Druid to partition data based on ranges of a single dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"dimension\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Setting Up Interval Load Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines an interval load rule that specifies how many replicas of a segment should exist in different server tiers for a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByInterval\",\n  \"interval\": \"2012-01-01/2013-01-01\",\n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Task Logs in Druid\nDESCRIPTION: Configuration for storing task logs in HDFS. Requires the druid-hdfs-storage extension to be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_18\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.directory=none\n```\n\n----------------------------------------\n\nTITLE: Querying Data from Druid using DSQL\nDESCRIPTION: SQL query to select all data from the 'updates-tutorial' datasource, showing the initial dataset with animal dimension and number metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\"; \n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  tiger         1     100 \n 2018-01-01T03:01:00.000Z  aardvark      1      42 \n 2018-01-01T03:01:00.000Z  giraffe       1   14124 \n\nRetrieved 3 rows in 1.42s.\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Selector Filter for Multi-value Dimensions\nDESCRIPTION: Example of a GroupBy query with a selector filter that selects rows where 'tags' contains 't3'. This demonstrates how filters are applied before dimension values are exploded into multiple rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Extraction DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up an Extraction DimensionSpec in Apache Druid. It allows transforming dimension values using an extraction function and specifying output types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : <dimension>,\n  \"outputName\" :  <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">,\n  \"extractionFn\" : <extraction_function>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Buckets Histogram Aggregator in Druid\nDESCRIPTION: JSON configuration for the fixed buckets histogram aggregator in Apache Druid. This aggregator builds a histogram with evenly-sized buckets across a specified value range and provides different outlier handling modes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"fixedBucketsHistogram\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <double>,\n  \"upperLimit\" : <double>,\n  \"outlierHandlingMode\": <mode>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Filtered Aggregator in Druid\nDESCRIPTION: Demonstrates how to set up a filtered aggregator that wraps another aggregator and only processes values matching a specified dimension filter. This allows simultaneous computation of filtered and unfiltered aggregations without multiple queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"filtered\",\n  \"filter\" : {\n    \"type\" : \"selector\",\n    \"dimension\" : <dimension>,\n    \"value\" : <dimension value>\n  }\n  \"aggregator\" : <aggregation>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with JDBC\nDESCRIPTION: Example configuration for a globally cached lookup using JDBC connection to MySQL database with table and column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"jdbc\",\n       \"connectorConfig\": {\n         \"createTables\": true,\n         \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n         \"user\": \"druid\",\n         \"password\": \"diurd\"\n       },\n       \"table\": \"lookupTable\",\n       \"keyColumn\": \"mykeyColumn\",\n       \"valueColumn\": \"myValueColumn\",\n       \"filter\" : \"myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)\",\n       \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Coordinator Intervals API\nDESCRIPTION: GET requests to retrieve interval information for datasources. Includes endpoints for all intervals, specific interval details, and aggregated interval data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_9\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/intervals\nGET /druid/coordinator/v1/intervals/{interval}\nGET /druid/coordinator/v1/intervals/{interval}?simple\nGET /druid/coordinator/v1/intervals/{interval}?full\n```\n\n----------------------------------------\n\nTITLE: Sample Result Format for Segment Metadata Query in Apache Druid\nDESCRIPTION: This JSON snippet illustrates the format of the result returned by a segment metadata query in Apache Druid. It includes information about the segment ID, time intervals, column details, aggregators, query granularity, size, and number of rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"id\" : \"some_id\",\n  \"intervals\" : [ \"2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z\" ],\n  \"columns\" : {\n    \"__time\" : { \"type\" : \"LONG\", \"hasMultipleValues\" : false, \"size\" : 407240380, \"cardinality\" : null, \"errorMessage\" : null },\n    \"dim1\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : 1944, \"errorMessage\" : null },\n    \"dim2\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : true, \"size\" : 100000, \"cardinality\" : 1504, \"errorMessage\" : null },\n    \"metric1\" : { \"type\" : \"FLOAT\", \"hasMultipleValues\" : false, \"size\" : 100000, \"cardinality\" : null, \"errorMessage\" : null }\n  },\n  \"aggregators\" : {\n    \"metric1\" : { \"type\" : \"longSum\", \"name\" : \"metric1\", \"fieldName\" : \"metric1\" }\n  },\n  \"queryGranularity\" : {\n    \"type\": \"none\"\n  },\n  \"size\" : 300000,\n  \"numRows\" : 5000000\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring pvalue2tailedZtest Post Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the pvalue2tailedZtest post aggregator which calculates the p-value of a two-sided z-test from a given z-score. This is typically used after calculating a z-score with the zscore2sample aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"pvalue2tailedZtest\",\n  \"name\": \"<output_name>\",\n  \"zScore\": <zscore post_aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Filtered Aggregator in Druid JSON\nDESCRIPTION: Configuration for a filtered aggregator that wraps any given aggregator and only processes values matching a specified dimension filter. This allows simultaneous computation of filtered and unfiltered aggregations without multiple queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"filtered\",\n  \"filter\" : {\n    \"type\" : \"selector\",\n    \"dimension\" : <dimension>,\n    \"value\" : <dimension value>\n  }\n  \"aggregator\" : <aggregation>\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Role Permissions in Druid Authorization Database\nDESCRIPTION: JSON structure for defining permissions for roles in Druid's basic security authorization system. This example shows how to grant READ access to all resources matching 'wiki.*' and WRITE access to the 'wikiticker' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n{\n  \"resource\": {\n    \"name\": \"wiki.*\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"READ\"\n},\n{\n  \"resource\": {\n    \"name\": \"wikiticker\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"WRITE\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-dimension Partitioning in Apache Druid\nDESCRIPTION: This snippet demonstrates how to set up single-dimension partitioning for Druid ingestion. It specifies the partitioning type as 'dimension' and sets a target partition size, allowing Druid to partition data based on ranges of a single dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"dimension\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Complex Percentage Calculation Query\nDESCRIPTION: Shows an advanced query implementation calculating percentages using multiple aggregations and nested post-aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\" : [\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"part\", \"fieldName\" : \"part\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"part_percentage\",\n    \"fn\"     : \"*\",\n    \"fields\" : [\n       { \"type\"   : \"arithmetic\",\n         \"name\"   : \"ratio\",\n         \"fn\"     : \"/\",\n         \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"part\", \"fieldName\" : \"part\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" }\n         ]\n       },\n       { \"type\" : \"constant\", \"name\": \"const\", \"value\" : 100 }\n    ]\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Coordinator Leadership Status in JSON\nDESCRIPTION: This endpoint returns a JSON object indicating whether the current server is the leader Coordinator of the cluster. It returns HTTP 200 if the server is the leader and HTTP 404 if not, making it suitable for load balancer health checks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"leader\": true\n}\n```\n\n----------------------------------------\n\nTITLE: DimensionsSpec Configuration Example\nDESCRIPTION: Illustrates a detailed dimensionsSpec configuration with mixed data types and bitmap indexing settings for various dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Concatenating Strings in Apache Druid SQL\nDESCRIPTION: Shows two methods for concatenating strings in Druid SQL: using the || operator and the CONCAT function. Both achieve string concatenation but with different syntax.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nx || y\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCONCAT(expr, expr...)\n```\n\n----------------------------------------\n\nTITLE: Expression Transform Syntax in JSON\nDESCRIPTION: The syntax for an expression transform which creates a new field based on an expression applied to input rows. It requires a name for the output field and an expression that defines the transformation logic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <output field name>,\n  \"expression\": <expr>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LongSum Aggregator in Druid JSON\nDESCRIPTION: Defines a longSum aggregator to compute the sum of values as a 64-bit signed integer. It requires an output name and the field name to sum over.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring TwitterSpritzerFirehose in Apache Druid\nDESCRIPTION: Configuration specification for the TwitterSpritzerFirehose extension that connects to Twitter's spritzer data stream. Includes parameters for controlling event count limits and runtime duration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/examples.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"twitzer\",\n    \"maxEventCount\": -1,\n    \"maxRunMinutes\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Spec with Transform and Filter\nDESCRIPTION: JSON configuration for ingesting data into Druid with transform specs to modify data and apply filters. It includes transforms to prepend 'super-' to animal names and triple the 'number' value, along with a filter to select specific rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"transform-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"animal\",\n              { \"name\": \"location\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"number\", \"fieldName\" : \"number\" },\n        { \"type\" : \"longSum\", \"name\" : \"triple-number\", \"fieldName\" : \"triple-number\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      },\n      \"transformSpec\": {\n        \"transforms\": [\n          {\n            \"type\": \"expression\",\n            \"name\": \"animal\",\n            \"expression\": \"concat('super-', animal)\"\n          },\n          {\n            \"type\": \"expression\",\n            \"name\": \"triple-number\",\n            \"expression\": \"number * 3\"\n          }\n        ],\n        \"filter\": {\n          \"type\":\"or\",\n          \"fields\": [\n            { \"type\": \"selector\", \"dimension\": \"animal\", \"value\": \"super-mongoose\" },\n            { \"type\": \"selector\", \"dimension\": \"triple-number\", \"value\": \"300\" },\n            { \"type\": \"selector\", \"dimension\": \"location\", \"value\": \"3\" }\n          ]\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"transform-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Constant Post-Aggregator Implementation in Druid\nDESCRIPTION: Defines a constant post-aggregator that always returns a specified numerical value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor Spec for Protobuf Ingestion\nDESCRIPTION: Complete Supervisor spec JSON for submitting to Druid's Overlord. It configures Kafka ingestion for Protobuf data, including data schema, parser settings, and Kafka connection details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka2\",\n    \"parser\": {\n      \"type\": \"protobuf\",\n      \"descriptor\": \"file:///tmp/metrics.desc\",\n      \"protoMessageType\": \"Metrics\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"unit\",\n            \"http_method\",\n            \"http_code\",\n            \"page\",\n            \"metricType\",\n            \"server\"\n          ],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics_pb\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Segment Storage Configuration in YAML\nDESCRIPTION: Configuration for storing intermediate segments in Druid's Realtime Process. This setting determines where temporary segment data is stored during processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/realtime.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.segmentCache.locations: none\n```\n\n----------------------------------------\n\nTITLE: Submitting a Druid Ingestion Task Using Bash Command\nDESCRIPTION: Command to submit the ingestion task specification to Druid. This uses the post-index-task script included in the Druid distribution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/ingestion-tutorial-index.json \n```\n\n----------------------------------------\n\nTITLE: Example of Partially Updated Segment Set in Druid\nDESCRIPTION: Shows a mixed state during a multi-interval update where only some intervals have been updated to v2. This demonstrates how the cluster can contain a mixture of different segment versions during updates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v2_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Kafka in Bash\nDESCRIPTION: Commands to download Apache Kafka 0.10.2.2 and extract it to a local directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz\ntar -xzf kafka_2.12-0.10.2.2.tgz\ncd kafka_2.12-0.10.2.2\n```\n\n----------------------------------------\n\nTITLE: Overwriting Data in Apache Druid\nDESCRIPTION: This command submits a task to overwrite the existing data in the 'updates-tutorial' datasource with new data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Resuming All Supervisors POST Endpoint\nDESCRIPTION: REST endpoint to resume all supervisors simultaneously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/resumeAll\n```\n\n----------------------------------------\n\nTITLE: Final Ingestion Spec with tuningConfig for Apache Druid\nDESCRIPTION: This JSON configuration represents the complete and final ingestion spec for a native batch task in Apache Druid. It includes dataSchema, ioConfig, and tuningConfig with all necessary configurations for ingesting netflow data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"targetPartitionSize\" : 5000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query with cURL\nDESCRIPTION: Example showing how to execute a Druid SQL query using cURL from command line by sending a JSON POST request to the SQL endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ cat query.json\n{\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"}\n\n$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json\n[{\"TheCount\":24433}]\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Connection Context\nDESCRIPTION: Example of a Druid SQL query JSON payload that includes both a SQL query and a context object specifying the time zone. This demonstrates how to pass execution parameters along with queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"context\" : {\n    \"sqlTimeZone\" : \"America/Los_Angeles\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authorizer in Apache Druid\nDESCRIPTION: This snippet shows how to set up a Basic authorizer in Druid. It adds the authorizer to the list of authorizers and specifies its type as 'basic'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authorizers=[\"MyBasicAuthorizer\"]\n\ndruid.auth.authorizer.MyBasicAuthorizer.type=basic\n```\n\n----------------------------------------\n\nTITLE: EXPLAIN PLAN Output in Druid Shell\nDESCRIPTION: Terminal output showing the native Druid query plan generated from the SQL query, displayed in the Druid SQL shell (dsql). The plan shows a TopN query with various parameters including datasource, dimensions, intervals, and aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n\n PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\n DruidQueryRel(query=[{\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"page\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":10,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}],\"postAggregations\":[],\"context\":{},\"descending\":false}], signature=[{d0:STRING, a0:LONG}]) \n\nRetrieved 1 row in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topic\nDESCRIPTION: Command to create a Kafka topic named 'wikipedia' with single partition and replication factor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia\n```\n\n----------------------------------------\n\nTITLE: Enabling Rollup in Druid Ingestion Spec\nDESCRIPTION: Adds a granularitySpec to the dataSchema to enable rollup during data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query Using Bloom Filter Aggregator\nDESCRIPTION: This JSON snippet provides a complete example of a Druid query using the Bloom Filter aggregator. It demonstrates how to configure the aggregator within a timeseries query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"wikiticker\",\n  \"intervals\": [ \"2015-09-12T00:00:00.000/2015-09-13T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"bloom\",\n      \"name\": \"userBloom\",\n      \"maxNumEntries\": 100000,\n      \"field\": {\n        \"type\":\"default\",\n        \"dimension\":\"user\",\n        \"outputType\": \"STRING\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Paginated Select Query with fromNext Option in Apache Druid\nDESCRIPTION: This JSON snippet shows a paginated Select query in Apache Druid with the fromNext option set to false. This option is used for backwards compatibility with older versions of Druid where manual offset incrementation was required.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/select-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Search Query Configuration in YAML\nDESCRIPTION: Configuration for search queries in Druid's Realtime Process, setting the maximum number of search results to return. This controls query result size limitation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/realtime.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.query.search.maxSearchLimit: 1000\n```\n\n----------------------------------------\n\nTITLE: Startup Logging Configuration JSON\nDESCRIPTION: Properties for configuring startup logging behavior including property logging and masking sensitive information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"druid.startup.logging.logProperties\": false,\n  \"druid.startup.logging.maskProperties\": [\"password\"]\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Aggregator Configuration\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator defining sketch parameters and metrics\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"arrayOfDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"nominalEntries\": <number>,\n  \"numberOfValues\" : <number>,\n  \"metricColumns\" : <array of strings>\n }\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Aggregator Configuration\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator defining sketch parameters and metrics\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"arrayOfDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"nominalEntries\": <number>,\n  \"numberOfValues\" : <number>,\n  \"metricColumns\" : <array of strings>\n }\n```\n\n----------------------------------------\n\nTITLE: HTTP Client Configuration for Druid Brokers\nDESCRIPTION: Configuration options for the HTTP client used by Brokers to communicate with Historical servers and real-time tasks. These settings control connection pooling, compression, timeouts, and backpressure mechanisms.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_38\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.http.numConnections=20\ndruid.broker.http.compressionCodec=gzip\ndruid.broker.http.readTimeout=PT15M\ndruid.broker.http.unusedConnectionTimeout=PT4M\ndruid.broker.http.maxQueuedBytes=0\n```\n\n----------------------------------------\n\nTITLE: SQL Query with EXPLAIN PLAN FOR\nDESCRIPTION: SQL query demonstrating how to use EXPLAIN PLAN FOR to view the execution plan of a TopN query filtering Wikipedia edits by timestamp, grouped by page and ordered by edit count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Configuring TopN Query Context in Apache Druid\nDESCRIPTION: A markdown table showing the specific query context parameter for TopN queries in Apache Druid. It includes the parameter name, default value, and description.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/query-context.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default              | description          |\n|-----------------|---------------------|----------------------|\n|minTopNThreshold | `1000`              | The top minTopNThreshold local results from each segment are returned for merging to determine the global topN. |\n```\n\n----------------------------------------\n\nTITLE: Configuring Hybrid Cache in Druid\nDESCRIPTION: Configuration properties for a two-level L1/L2 hybrid cache implementation in Druid. These settings allow combining different cache types, such as an in-memory cache with a remote Memcached cache.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_53\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.l1.type=caffeine\ndruid.cache.l2.type=caffeine\ndruid.cache.useL2=true\ndruid.cache.populateL2=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid SQL Server Properties in Markdown\nDESCRIPTION: This snippet presents a table of configuration properties for the Druid SQL server. It includes property names, descriptions, and default values for various settings related to SQL querying, Avatica server, HTTP endpoints, query planning, and metadata management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_20\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.sql.enable`|Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely.|false|\n|`druid.sql.avatica.enable`|Whether to enable JDBC querying at `/druid/v2/sql/avatica/`.|true|\n|`druid.sql.avatica.maxConnections`|Maximum number of open connections for the Avatica server. These are not HTTP connections, but are logical client connections that may span multiple HTTP connections.|25|\n|`druid.sql.avatica.maxRowsPerFrame`|Maximum number of rows to return in a single JDBC frame. Setting this property to -1 indicates that no row limit should be applied. Clients can optionally specify a row limit in their requests; if a client specifies a row limit, the lesser value of the client-provided limit and `maxRowsPerFrame` will be used.|5,000|\n|`druid.sql.avatica.maxStatementsPerConnection`|Maximum number of simultaneous open statements per Avatica client connection.|4|\n|`druid.sql.avatica.connectionIdleTimeout`|Avatica client connection idle timeout.|PT5M|\n|`druid.sql.http.enable`|Whether to enable JSON over HTTP querying at `/druid/v2/sql/`.|true|\n|`druid.sql.planner.maxQueryCount`|Maximum number of queries to issue, including nested queries. Set to 1 to disable sub-queries, or set to 0 for unlimited.|8|\n|`druid.sql.planner.maxSemiJoinRowsInMemory`|Maximum number of rows to keep in memory for executing two-stage semi-join queries like `SELECT * FROM Employee WHERE DeptName IN (SELECT DeptName FROM Dept)`.|100000|\n|`druid.sql.planner.maxTopNLimit`|Maximum threshold for a [TopN query](../querying/topnquery.html). Higher limits will be planned as [GroupBy queries](../querying/groupbyquery.html) instead.|100000|\n|`druid.sql.planner.metadataRefreshPeriod`|Throttle for metadata refreshes.|PT1M|\n|`druid.sql.planner.selectThreshold`|Page size threshold for [Select queries](../querying/select-query.html). Select queries for larger resultsets will be issued back-to-back using pagination.|1000|\n|`druid.sql.planner.useApproximateCountDistinct`|Whether to use an approximate cardinalty algorithm for `COUNT(DISTINCT foo)`.|true|\n|`druid.sql.planner.useApproximateTopN`|Whether to use approximate [TopN queries](../querying/topnquery.html) when a SQL query could be expressed as such. If false, exact [GroupBy queries](../querying/groupbyquery.html) will be used instead.|true|\n|`druid.sql.planner.useFallback`|Whether to evaluate operations on the Broker when they cannot be expressed as Druid queries. This option is not recommended for production since it can generate unscalable query plans. If false, SQL queries that cannot be translated to Druid queries will fail.|false|\n|`druid.sql.planner.requireTimeCondition`|Whether to require SQL to have filter conditions on __time column so that all generated native queries will have user specified intervals. If true, all queries wihout filter condition on __time column will fail|false|\n|`druid.sql.planner.sqlTimeZone`|Sets the default time zone for the server, which will affect how time functions and timestamp literals behave. Should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\".|UTC|\n|`druid.sql.planner.metadataSegmentCacheEnable`|Whether to keep a cache of published segments in broker. If true, broker polls coordinator in background to get segments from metadata store and maintains a local cache. If false, coordinator's REST api will be invoked when broker needs published segments info.|false|\n|`druid.sql.planner.metadataSegmentPollPeriod`|How often to poll coordinator for published segments list if `druid.sql.planner.metadataSegmentCacheEnable` is set to true. Poll period is in milliseconds. |60000|\n```\n\n----------------------------------------\n\nTITLE: Accessing Recent Coordinator Configuration History via HTTP\nDESCRIPTION: HTTP endpoint for retrieving a specified number of recent entries from the audit history of coordinator dynamic configuration changes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_11\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Defining StatsD Metric Mapping in JSON\nDESCRIPTION: JSON structure for mapping Druid metrics to StatsD types and dimensions. This configuration defines how Druid metrics are converted and sent to StatsD, including metric types and relevant dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query/time\": { \n    \"dimensions\": [\"dataSource\", \"type\"], \n    \"type\": \"timer\"\n  },\n  \"coordinator-segment/count\": { \n    \"dimensions\": [\"dataSource\"], \n    \"type\": \"gauge\" \n  },\n  \"historical-segment/count\": { \n    \"dimensions\": [\"dataSource\", \"tier\", \"priority\"], \n    \"type\": \"gauge\" \n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookup DimensionSpec with External Lookup in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Lookup DimensionSpec using an external lookup table in Apache Druid. It references a pre-registered lookup table by name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"name\":\"lookupName\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling On-heap Lookup Cache in Apache Druid\nDESCRIPTION: JSON configuration for a polling on-heap lookup cache that updates every 10 minutes. It uses a JDBC data fetcher to retrieve data from a MySQL database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"pollPeriod\":\"PT10M\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"onHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Event Count Metrics Specification in Druid Ingestion\nDESCRIPTION: JSON configuration snippet showing how to specify a count metric in a Druid ingestion spec. This creates a count column that can be used to track the total number of ingested events, which is useful when rollup is enabled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-design.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n...\n\"metricsSpec\" : [\n      {\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      },\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Query Datasource for Nested GroupBy in Apache Druid JSON\nDESCRIPTION: Illustrates the JSON structure for a query datasource in Apache Druid, used for nested groupBy queries. This type is only supported for groupBy queries and allows for subquery-like functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/datasource.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"query\",\n\t\"query\": {\n\t\t\"type\": \"groupBy\",\n\t\t...\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Behavior in Apache Druid Overlord (JSON)\nDESCRIPTION: This JSON configuration specifies worker selection strategy, affinity settings, and autoscaling parameters for EC2 instances in Apache Druid's Overlord component.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"selectStrategy\": {\n    \"type\": \"fillCapacity\",\n    \"affinityConfig\": {\n      \"affinity\": {\n        \"datasource1\": [\"host1:port\", \"host2:port\"],\n        \"datasource2\": [\"host3:port\"]\n      }\n    }\n  },\n  \"autoScaler\": {\n    \"type\": \"ec2\",\n    \"minNumWorkers\": 2,\n    \"maxNumWorkers\": 12,\n    \"envConfig\": {\n      \"availabilityZone\": \"us-east-1a\",\n      \"nodeData\": {\n        \"amiId\": \"${AMI}\",\n        \"instanceType\": \"c3.8xlarge\",\n        \"minInstances\": 1,\n        \"maxInstances\": 1,\n        \"securityGroupIds\": [\"${IDs}\"],\n        \"keyName\": \"${KEY_NAME}\"\n      },\n      \"userData\": {\n        \"impl\": \"string\",\n        \"data\": \"${SCRIPT_COMMAND}\",\n        \"versionReplacementString\": \":VERSION:\",\n        \"version\": null\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Ingestion Metrics Table in Markdown\nDESCRIPTION: This code snippet shows a markdown table listing various ingestion metrics for the Realtime process in Apache Druid. It includes metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/thrownAway`|Number of events rejected because they are outside the windowPeriod.|dataSource, taskId, taskType.|0|\n|`ingest/events/unparseable`|Number of events rejected because the events are unparseable.|dataSource, taskId, taskType.|0|\n|`ingest/events/duplicate`|Number of events rejected because the events are duplicated.|dataSource, taskId, taskType.|0|\n|`ingest/events/processed`|Number of events successfully processed per emission period.|dataSource, taskId, taskType.|Equal to your # of events per emission period|\n|`ingest/rows/output`|Number of Druid rows persisted.|dataSource, taskId, taskType.|Your # of events with rollup|\n|`ingest/persists/count`|Number of times persist occurred.|dataSource, taskId, taskType.|Depends on configuration|\n|`ingest/persists/time`|Milliseconds spent doing intermediate persist.|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/persists/cpu`|Cpu time in Nanoseconds spent on doing intermediate persist.|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/persists/backPressure`|Milliseconds spent creating persist tasks and blocking waiting for them to finish.|dataSource, taskId, taskType.|0 or very low|\n|`ingest/persists/failed`|Number of persists that failed.|dataSource, taskId, taskType.|0|\n|`ingest/handoff/failed`|Number of handoffs that failed.|dataSource, taskId, taskType.|0|\n|`ingest/merge/time`|Milliseconds spent merging intermediate segments|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/merge/cpu`|Cpu time in Nanoseconds spent on merging intermediate segments.|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/handoff/count`|Number of handoffs that happened.|dataSource, taskId, taskType.|Varies. Generally greater than 0 once every segment granular period if cluster operating normally|\n|`ingest/sink/count`|Number of sinks not handoffed.|dataSource, taskId, taskType.|1~3|\n|`ingest/events/messageGap`|Time gap between the data time in event and current system time.|dataSource, taskId, taskType.|Greater than 0, depends on the time carried in event|\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD Emitter in Druid\nDESCRIPTION: JSON configuration structure showing the mapping between Druid metrics and StatsD metrics. It demonstrates how to specify metric types, dimensions, and range conversion settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"<druid metric name>\" : { \"dimensions\" : <dimension list>, \"type\" : <StatsD type>, \"convertRange\" : true/false}\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topic\nDESCRIPTION: Command to create a Kafka topic named 'wikipedia' with single partition and replication factor of 1.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi InputSpec for Delta Ingestion in Druid\nDESCRIPTION: JSON configuration for the 'multi' inputSpec type in Druid's Hadoop batch ingestion. This spec allows combining multiple inputSpecs for delta ingestion and combining data from multiple dataSources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"multi\",\n    \"children\": [\n      {\n        \"type\" : \"dataSource\",\n        \"ingestionSpec\" : {\n          \"dataSource\": \"wikipedia\",\n          \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"],\n          \"segments\": [\n            {\n              \"dataSource\": \"test1\",\n              \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\",\n              \"version\": \"v2\",\n              \"loadSpec\": {\n                \"type\": \"local\",\n                \"path\": \"/tmp/index1.zip\"\n              },\n              \"dimensions\": \"host\",\n              \"metrics\": \"visited_sum,unique_hosts\",\n              \"shardSpec\": {\n                \"type\": \"none\"\n              },\n              \"binaryVersion\": 9,\n              \"size\": 2,\n              \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\"\n            }\n          ]\n        }\n      },\n      {\n        \"type\" : \"static\",\n        \"paths\": \"/path/to/more/wikipedia/data/\"\n      }\n    ]  \n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Select Query Structure in Druid\nDESCRIPTION: Example of a basic Select query structure for Apache Druid. It includes essential parameters like queryType, dataSource, dimensions, metrics, granularity, intervals and pagingSpec for retrieving paginated results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/select-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Using Bloom Filter in SQL WHERE Clause\nDESCRIPTION: Example of using a Bloom filter in a Druid SQL query's WHERE clause using the bloom_filter_test operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')\n```\n\n----------------------------------------\n\nTITLE: Filtering on Day of Week with Time Format Extraction in Apache Druid\nDESCRIPTION: This example demonstrates filtering for rows where the timestamp falls on a Friday, using a time format extraction function to format the timestamp according to day of week with specific timezone and locale.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"Friday\",\n  \"extractionFn\": {\n    \"type\": \"timeFormat\",\n    \"format\": \"EEEE\",\n    \"timeZone\": \"America/New_York\",\n    \"locale\": \"en\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Bloom Filter in SQL WHERE Clause\nDESCRIPTION: Example of using a Bloom filter in a Druid SQL query's WHERE clause using the bloom_filter_test operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')\n```\n\n----------------------------------------\n\nTITLE: Cardinality Aggregator for Counting Distinct Countries\nDESCRIPTION: Example JSON configuration for using the cardinality aggregator to count distinct countries that people are living in or have come from across two different dimension fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/hll-old.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_countries\",\n  \"fields\": [ \"country_of_origin\", \"country_of_residence\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript ParseSpec in Druid Ingestion\nDESCRIPTION: This snippet shows how to configure a JavaScript parser in Druid's ingestion specification. The example includes a custom parsing function that splits a string by hyphen and returns a key-value object. The configuration also specifies timestamp and dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"javascript\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    },\n    \"function\" : \"function(str) { var parts = str.split(\\\"-\\\"); return { one: parts[0], two: parts[1] } }\"\n  }\n```\n\n----------------------------------------\n\nTITLE: OR Filter Example for Multi-value Dimensions in Druid\nDESCRIPTION: Example of an OR filter that matches rows where the 'tags' dimension contains either 't1' or 't3'. This would match row1 and row2 from the dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"or\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Parser with Auto Field Discovery and Flatten Expressions\nDESCRIPTION: JSON configuration example for the ORC parser using the 'orc' parseSpec format with auto field discovery enabled and custom flatten expressions to extract nested fields from the ORC data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/orc.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"orc\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"millis\"\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: TopN Query Example with Variance Aggregator and Standard Deviation Post-Aggregator in Apache Druid\nDESCRIPTION: Example of a TopN query using both the variance aggregator and standard deviation post-aggregator in Apache Druid. Shows how to combine these aggregators in a query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"threshold\": 5,\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing DoubleFirst Aggregator in Apache Druid\nDESCRIPTION: Shows how to configure a doubleFirst aggregator in Druid. This aggregator computes the metric value with the minimum timestamp or 0 if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQFirehose for Apache Druid\nDESCRIPTION: A sample configuration specification for setting up a RabbitMQ firehose in Apache Druid. This configuration defines connection parameters and queue settings to ingest data from a RabbitMQ broker into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/rabbitmq.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n   \"type\" : \"rabbitmq\",\n   \"connection\" : {\n     \"host\": \"localhost\",\n     \"port\": \"5672\",\n     \"username\": \"test-dude\",\n     \"password\": \"test-word\",\n     \"virtualHost\": \"test-vhost\",\n     \"uri\": \"amqp://mqserver:1234/vhost\"\n   },\n   \"config\" : {\n     \"exchange\": \"test-exchange\",\n     \"queue\" : \"druidtest\",\n     \"routingKey\": \"#\",\n     \"durable\": \"true\",\n     \"exclusive\": \"false\",\n     \"autoDelete\": \"false\",\n     \"maxRetries\": \"10\",\n     \"retryIntervalSeconds\": \"1\",\n     \"maxDurationSeconds\": \"300\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with JDBC Extraction\nDESCRIPTION: JSON configuration for a globally cached lookup using a JDBC extraction namespace. This example demonstrates how to set up a lookup that pulls data from a MySQL database with filtering options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"jdbc\",\n       \"connectorConfig\": {\n         \"createTables\": true,\n         \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n         \"user\": \"druid\",\n         \"password\": \"diurd\"\n       },\n       \"table\": \"lookupTable\",\n       \"keyColumn\": \"mykeyColumn\",\n       \"valueColumn\": \"myValueColumn\",\n       \"filter\" : \"myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)\",\n       \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticS3Firehose for Data Ingestion\nDESCRIPTION: JSON configuration for the StaticS3Firehose, which enables ingesting events from predefined S3 objects. This configuration can be used with native parallel index tasks and supports caching and prefetching features.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/s3.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"firehose\" : {\n        \"type\" : \"static-s3\",\n        \"uris\": [\"s3://foo/bar/file.gz\", \"s3://bar/foo/file2.gz\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Storage for Druid Cluster\nDESCRIPTION: Updates the common.runtime.properties file to use S3 for deep storage and indexing logs. Requires adding the S3 extension and configuring bucket, access key, and secret key.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-s3-extensions\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=s3\ndruid.storage.bucket=your-bucket\ndruid.storage.baseKey=druid/segments\ndruid.s3.accessKey=...\ndruid.s3.secretKey=...\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=s3\ndruid.indexer.logs.s3Bucket=your-bucket\ndruid.indexer.logs.s3Prefix=druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Broadcast Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a forever broadcast rule, which specifies how segments of different data sources should be co-located in Historical processes indefinitely.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastForever\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Time Parsing Extraction Function in Druid JSON\nDESCRIPTION: Parses dimension values as timestamps using a specified input format and returns them formatted using the given output format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"time\",\n  \"timeFormat\" : <input_format>,\n  \"resultFormat\" : <output_format>,\n  \"joda\" : <true, false> }\n```\n\n----------------------------------------\n\nTITLE: Implementing QuantilesToString Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToString post-aggregator which returns a debug-friendly summary of the sketch by calling its toString() method.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-dimension Partitioning in Druid\nDESCRIPTION: This snippet shows the configuration for single-dimension partitioning in Druid's partitionsSpec. It sets the partitioning type to 'dimension' and specifies a target partition size, allowing Druid to partition data based on a single dimension's values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"dimension\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Creating a Spatial Filter with Rectangular Bounds in Apache Druid\nDESCRIPTION: This snippet shows how to define a spatial filter in Apache Druid queries using rectangular bounds. The filter specifies minimum and maximum coordinates to create a bounding box for filtering spatial data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/geo.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\" : {\n    \"type\": \"spatial\",\n    \"dimension\": \"spatialDim\",\n    \"bound\": {\n        \"type\": \"rectangular\",\n        \"minCoords\": [10.0, 20.0],\n        \"maxCoords\": [30.0, 40.0]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up FloatFirst Aggregator in Apache Druid\nDESCRIPTION: Demonstrates how to configure a floatFirst aggregator in Druid. This aggregator computes the metric value with the minimum timestamp or 0 if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticS3Firehose for Data Ingestion\nDESCRIPTION: JSON configuration for the StaticS3Firehose, which enables ingesting events from predefined S3 objects. This configuration can be used with native parallel index tasks and supports caching and prefetching features.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/s3.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"firehose\" : {\n        \"type\" : \"static-s3\",\n        \"uris\": [\"s3://foo/bar/file.gz\", \"s3://bar/foo/file2.gz\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator in Druid\nDESCRIPTION: Configuration for cardinality aggregator that computes the cardinality of Druid dimensions using HyperLogLog estimation. Supports both single and multiple dimensions with options for byRow calculation and rounding.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/hll-old.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"<output_name>\",\n  \"fields\": [ <dimension1>, <dimension2>, ... ],\n  \"byRow\": <false | true> # (optional, defaults to false),\n  \"round\": <false | true> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Properties for Router Process\nDESCRIPTION: Example runtime.properties configuration for the Router process, including service name, broker mappings, and HTTP settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/router.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=#{IP_ADDR}:8080\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.defaultBrokerServiceName=druid:broker-cold\ndruid.router.coordinatorServiceName=druid:coordinator\ndruid.router.tierToBrokerMap={\"hot\":\"druid:broker-hot\",\"_default_tier\":\"druid:broker-cold\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the Router proxy http client\ndruid.router.http.numMaxThreads=100\n\ndruid.server.http.numThreads=100\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function without Length\nDESCRIPTION: Configuration for substring extraction function that returns remainder of string from specified index.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 3 }\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Coordinator and Overlord Services\nDESCRIPTION: Commands to start the Druid Coordinator and Overlord services on the coordination server. These services manage cluster coordination and task management respectively.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator\njava `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord\n```\n\n----------------------------------------\n\nTITLE: Configuring SegmentWriteOutMediumFactory in Apache Druid\nDESCRIPTION: This snippet shows the configuration structure for SegmentWriteOutMediumFactory in Apache Druid. It requires specifying the type, with additional options available in the linked documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|type|String|See [Additional Peon Configuration: SegmentWriteOutMediumFactory](../configuration/index.html#segmentwriteoutmediumfactory) for explanation and available options.|yes|\n```\n\n----------------------------------------\n\nTITLE: Creating Hadoop XML Directory in Druid Configuration\nDESCRIPTION: Commands to create the Hadoop XML directory in Druid's configuration directory and copy the Hadoop configuration files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p {PATH_TO_DRUID}/quickstart/tutorial/conf/druid/_common/hadoop-xml\ncp /tmp/shared/hadoop_xml/*.xml {PATH_TO_DRUID}/quickstart/tutorial/conf/druid/_common/hadoop-xml/\n```\n\n----------------------------------------\n\nTITLE: Implementing Hadoop-based Batch Ingestion in Druid using Java\nDESCRIPTION: Hadoop indexing in Druid is primarily handled by HadoopDruidDetermineConfigurationJob.java for determining segment count and HadoopDruidIndexerJob.java for creating segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/overview.md#2025-04-09_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHadoopDruidDetermineConfigurationJob.java\nHadoopDruidIndexerJob.java\n```\n\n----------------------------------------\n\nTITLE: Configuring Peon Forked Properties\nDESCRIPTION: Pattern for configuring explicit child peon properties in the MiddleManager. Properties prefixed with this pattern will be passed down to child peons.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_38\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.fork.property\n```\n\n----------------------------------------\n\nTITLE: Tuning Configuration for Druid Batch Ingestion JSON\nDESCRIPTION: A tuningConfig example that sets the maximum number of rows per segment for a native batch ingestion task. This allows fine-tuning of the ingestion process for performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n```\n\n----------------------------------------\n\nTITLE: SQL Query with EXPLAIN PLAN FOR\nDESCRIPTION: SQL query demonstrating how to use EXPLAIN PLAN FOR to view the execution plan of a TopN query filtering Wikipedia edits by timestamp, grouped by page and ordered by edit count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Using AND Logical Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A logical AND filter that combines multiple other filters, requiring all conditions to be true for a row to match.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQFirehose for Apache Druid\nDESCRIPTION: A sample configuration specification for setting up a RabbitMQ firehose in Apache Druid. This configuration defines connection parameters and queue settings to ingest data from a RabbitMQ broker into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/rabbitmq.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n   \"type\" : \"rabbitmq\",\n   \"connection\" : {\n     \"host\": \"localhost\",\n     \"port\": \"5672\",\n     \"username\": \"test-dude\",\n     \"password\": \"test-word\",\n     \"virtualHost\": \"test-vhost\",\n     \"uri\": \"amqp://mqserver:1234/vhost\"\n   },\n   \"config\" : {\n     \"exchange\": \"test-exchange\",\n     \"queue\" : \"druidtest\",\n     \"routingKey\": \"#\",\n     \"durable\": \"true\",\n     \"exclusive\": \"false\",\n     \"autoDelete\": \"false\",\n     \"maxRetries\": \"10\",\n     \"retryIntervalSeconds\": \"1\",\n     \"maxDurationSeconds\": \"300\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Overwriting Data in Apache Druid\nDESCRIPTION: This command submits a task to overwrite the existing data in the 'updates-tutorial' datasource with new data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DBCP Properties for Druid Metadata Storage\nDESCRIPTION: Example of setting custom database connection pool properties for Druid metadata storage. These properties control connection lifetime and query timeout settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000\ndruid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Source Delta Ingestion in Apache Druid\nDESCRIPTION: JSON configuration example for delta ingestion in Apache Druid using the multi inputSpec type. This combines data from an existing dataSource (wikipedia) with new data from a static file path, allowing for incremental updates to existing segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"multi\",\n    \"children\": [\n      {\n        \"type\" : \"dataSource\",\n        \"ingestionSpec\" : {\n          \"dataSource\": \"wikipedia\",\n          \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"],\n          \"segments\": [\n            {\n              \"dataSource\": \"test1\",\n              \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\",\n              \"version\": \"v2\",\n              \"loadSpec\": {\n                \"type\": \"local\",\n                \"path\": \"/tmp/index1.zip\"\n              },\n              \"dimensions\": \"host\",\n              \"metrics\": \"visited_sum,unique_hosts\",\n              \"shardSpec\": {\n                \"type\": \"none\"\n              },\n              \"binaryVersion\": 9,\n              \"size\": 2,\n              \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\"\n            }\n          ]\n        }\n      },\n      {\n        \"type\" : \"static\",\n        \"paths\": \"/path/to/more/wikipedia/data/\"\n      }\n    ]  \n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing a Data Source Metadata Query in Apache Druid (JSON)\nDESCRIPTION: This snippet shows the basic structure of a Data Source Metadata query in Apache Druid. It includes the required 'queryType' and 'dataSource' fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"dataSourceMetadata\",\n    \"dataSource\": \"sample_datasource\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Deep Storage for Druid\nDESCRIPTION: Properties for setting up Amazon S3 as Druid's deep storage layer. Requires loading the 'druid-s3-extensions' extension. Includes access credentials and bucket configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_13\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=s3\ndruid.s3.accessKey=YOUR_ACCESS_KEY\ndruid.s3.secretKey=YOUR_SECRET_KEY\ndruid.storage.bucket=your-bucket\ndruid.storage.baseKey=druid/segments\n```\n\n----------------------------------------\n\nTITLE: Using Selector Filter with Extraction Function in Apache Druid\nDESCRIPTION: This example shows a selector filter combined with a lookup extraction function to match transformed dimension values. It filters for products that map to 'bar_1' in the lookup table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with TimeMin/Max Aggregators\nDESCRIPTION: Example GroupBy query configuration that includes timeMin and timeMax aggregators along with count aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"timeMinMax\",\n  \"granularity\": \"DAY\",\n  \"dimensions\": [\"product\"],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    },\n    {\n      \"type\": \"timeMin\",\n      \"name\": \"<output_name of timeMin>\",\n      \"fieldName\": \"tmin\"\n    },\n    {\n      \"type\": \"timeMax\",\n      \"name\": \"<output_name of timeMax>\",\n      \"fieldName\": \"tmax\"\n    }\n  ],\n  \"intervals\": [\n    \"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting SQL Query to Druid Broker using curl\nDESCRIPTION: This bash command submits the SQL query to the Druid Broker using curl. It sends a POST request with the JSON file containing the SQL query as the request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8082/druid/v2/sql\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Drop Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines an interval drop rule that indicates segments within the specified time interval should be dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Numeric Greater Than Filter in Druid groupBy Query\nDESCRIPTION: Demonstrates a having-clause numeric filter that matches rows where an aggregate metric is greater than a specified value, equivalent to SQL's HAVING <aggregate> > <value>.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"greaterThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Basic Authorizer\nDESCRIPTION: Properties for setting up a Basic Authorizer in Druid. Defines the authorizers list and type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authorizers=[\"MyBasicAuthorizer\"]\n\ndruid.auth.authorizer.MyBasicAuthorizer.type=basic\n```\n\n----------------------------------------\n\nTITLE: Defining Interval Filter in Druid JSON\nDESCRIPTION: Demonstrates how to create an interval filter to select data within specific time ranges using ISO 8601 interval strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Response Format for groupBy Query with subtotalsSpec\nDESCRIPTION: Sample response format for a groupBy query with subtotalsSpec, showing concatenated results from multiple groupings with different combinations of dimensions as specified in the subtotalsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t1\",\n    \"event\" : { \"D1\": \"..\", \"D2\": \"..\", \"D3\": \"..\" }\n    }\n  },\n    {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t2\",\n    \"event\" : { \"D1\": \"..\", \"D2\": \"..\", \"D3\": \"..\" }\n    }\n  },\n  ...\n  ...\n\n   {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t1\",\n    \"event\" : { \"D1\": \"..\", \"D3\": \"..\" }\n    }\n  },\n    {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t2\",\n    \"event\" : { \"D1\": \"..\", \"D3\": \"..\" }\n    }\n  },\n  ...\n  ...\n\n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t1\",\n    \"event\" : { \"D3\": \"..\" }\n    }\n  },\n    {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t2\",\n    \"event\" : { \"D3\": \"..\" }\n    }\n  },\n...\n]\n```\n\n----------------------------------------\n\nTITLE: Defining GroupBy v2 Runtime Properties in Markdown\nDESCRIPTION: This snippet defines a table of supported runtime properties for GroupBy v2 queries in Apache Druid. It includes property names, descriptions, and default values for configurations such as buffer grouper settings, hash aggregation, and parallel combining threads.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_59\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.bufferGrouperInitialBuckets`|Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024).|0|\n|`druid.query.groupBy.bufferGrouperMaxLoadFactor`|Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7).|0|\n|`druid.query.groupBy.forceHashAggregation`|Force to use hash-based aggregation.|false|\n|`druid.query.groupBy.intermediateCombineDegree`|Number of intermediate processes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful cpu cores.|8|\n|`druid.query.groupBy.numParallelCombineThreads`|Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(`druid.query.groupBy.numParallelCombineThreads`, `druid.processing.numThreads`).|1 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Configuring Common GroupBy Strategies in Druid\nDESCRIPTION: Common configuration options for all GroupBy query strategies in Druid. Settings include default strategy and threading options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_55\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.defaultStrategy=v2\ndruid.query.groupBy.singleThreaded=false\n```\n\n----------------------------------------\n\nTITLE: Druid Metrics Documentation Table Format\nDESCRIPTION: Markdown tables documenting various Druid metrics across different components including ingestion, indexing, coordination, and system health metrics. Each table contains metric names, descriptions, dimensions, and normal value ranges.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/thrownAway`|Number of events rejected because they are outside the windowPeriod.|dataSource, taskId, taskType.|0|\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Line Protocol Parser in Druid\nDESCRIPTION: JSON configuration for setting up the InfluxDB Line Protocol parser in Druid ingestion specs. This example shows how to configure the parser with timestampSpec, dimensionsSpec, and optional whitelistMeasurements to filter specific measurement types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"influx\",\n        \"timestampSpec\": {\n          \"column\": \"__ts\",\n          \"format\": \"millis\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensionExclusions\": [\n            \"__ts\"\n          ]\n        },\n        \"whitelistMeasurements\": [\n          \"cpu\"\n        ]\n      }\n\n```\n\n----------------------------------------\n\nTITLE: Configuring SqlFirehose in Apache Druid\nDESCRIPTION: This snippet demonstrates the configuration of a SqlFirehose for ingesting events from RDBMS. It includes database connection details and SQL queries for data retrieval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/firehose.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"sql\",\n    \"database\": {\n        \"type\": \"mysql\",\n        \"connectorConfig\" : {\n            \"connectURI\" : \"jdbc:mysql://host:port/schema\",\n            \"user\" : \"user\",\n            \"password\" : \"password\"\n        }\n     },\n    \"sqls\" : [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining GroupBy v2 Query Contexts in Markdown\nDESCRIPTION: This snippet defines a table of supported query contexts for GroupBy v2 queries in Apache Druid. It includes context keys, descriptions, and default values for overriding runtime properties and controlling query behavior such as sorting and limit push down.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_60\n\nLANGUAGE: markdown\nCODE:\n```\n|Key|Description|Default|\n|---|-----------|-------|\n|`bufferGrouperInitialBuckets`|Overrides the value of `druid.query.groupBy.bufferGrouperInitialBuckets` for this query.|None|\n|`bufferGrouperMaxLoadFactor`|Overrides the value of `druid.query.groupBy.bufferGrouperMaxLoadFactor` for this query.|None|\n|`forceHashAggregation`|Overrides the value of `druid.query.groupBy.forceHashAggregation`|None|\n|`intermediateCombineDegree`|Overrides the value of `druid.query.groupBy.intermediateCombineDegree`|None|\n|`numParallelCombineThreads`|Overrides the value of `druid.query.groupBy.numParallelCombineThreads`|None|\n|`sortByDimsFirst`|Sort the results first by dimension values and then by timestamp.|false|\n|`forceLimitPushDown`|When all fields in the orderby are part of the grouping key, the broker will push limit application down to the Historical processes. When the sorting order uses fields that are not in the grouping key, applying this optimization can result in approximate results with unknown accuracy, so this optimization is disabled by default in that case. Enabling this context flag turns on limit push down for limit/orderbys that contain non-grouping key columns.|false|\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Response Format with Header in Druid SQL\nDESCRIPTION: Example of requesting a header along with query results by setting the 'header' parameter to true. This example uses the 'arrayLines' format which separates JSON arrays by newlines for easier streaming.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"arrayLines\",\n  \"header\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Query Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Search Query Extraction Function in Apache Druid. It returns the dimension value unchanged if the SearchQuerySpec matches, otherwise returns null.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_9\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"searchQuery\", \"query\" : <search_query_spec> }\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultLimitSpec in Apache Druid groupBy Queries\nDESCRIPTION: This JSON snippet shows the structure of a DefaultLimitSpec used to limit and order results from a groupBy query. It includes a limit value and a list of columns for ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/limitspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"default\",\n    \"limit\"   : <integer_value>,\n    \"columns\" : [list of OrderByColumnSpec]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON ParseSpec in Apache Druid\nDESCRIPTION: This configuration snippet shows how to set up the parseSpec for ingesting JSON data in Druid. It specifies the format, timestamp column, and dimensions to be extracted from the data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"json\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"dimensionSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Druid Datasource Metadata in HTTP\nDESCRIPTION: This HTTP GET request returns full metadata for a specified datasource as stored in the metadata store.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/metadata/datasources/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Querying Appended Data in Druid\nDESCRIPTION: SQL query showing results after appending data, where new rows are added including a duplicate bear entry in a separate segment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          2     400 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n 2018-01-01T05:01:00.000Z  mongoose      1     737 \n 2018-01-01T06:01:00.000Z  snake         1    1234 \n 2018-01-01T07:01:00.000Z  octopus       1     115 \n 2018-01-01T04:01:00.000Z  bear          1     222 \n 2018-01-01T09:01:00.000Z  falcon        1    1241 \n\nRetrieved 8 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query Response Format in Druid (JSON)\nDESCRIPTION: Example of the JSON response format for a timeseries query in Druid. Each element in the array represents a time bucket with a timestamp and the requested aggregation results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": { \"sample_name1\": <some_value>, \"sample_name2\": <some_value>, \"sample_divide\": <some_value> } \n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": { \"sample_name1\": <some_value>, \"sample_name2\": <some_value>, \"sample_divide\": <some_value> }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Task for Segment Merging in Druid\nDESCRIPTION: JSON configuration for Merge tasks that combine multiple segments with common timestamps. The rollup parameter determines whether common timestamps are merged or preserved.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"segments\": <JSON list of DataSegment objects to merge>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultLimitSpec in Apache Druid groupBy Queries\nDESCRIPTION: This JSON snippet shows the structure of a DefaultLimitSpec used to limit and order results from a groupBy query. It includes a limit value and a list of columns for ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/limitspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"default\",\n    \"limit\"   : <integer_value>,\n    \"columns\" : [list of OrderByColumnSpec]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Segment Loading Status in Apache Druid\nDESCRIPTION: These endpoints retrieve information about segment loading status in the Druid cluster, including load percentage, segments left to load, and load queue details for historical nodes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_2\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/loadstatus\nGET /druid/coordinator/v1/loadstatus?simple\nGET /druid/coordinator/v1/loadstatus?full\nGET /druid/coordinator/v1/loadqueue\nGET /druid/coordinator/v1/loadqueue?simple\nGET /druid/coordinator/v1/loadqueue?full\n```\n\n----------------------------------------\n\nTITLE: Logical AND Filter in Druid\nDESCRIPTION: Combines multiple filters with AND logic to create complex filtering conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Calculating Rollup Ratio using Druid SQL\nDESCRIPTION: SQL query to measure the rollup ratio of a datasource by comparing the number of rows in Druid with the number of ingested events using a count metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- \"* 1.0\" so we get decimal rather than integer division\nSELECT SUM(\"event_count\") / COUNT(*) * 1.0 FROM datasource\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator, specifying output name, input field, accuracy, and associated values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"arrayOfDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"nominalEntries\": <number>,\n  \"numberOfValues\" : <number>,\n  \"metricColumns\" : <array of strings>\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Discovery Properties for Druid Broker\nDESCRIPTION: This snippet defines segment discovery-related configuration properties for the Druid Broker, including discovery method, tier and datasource filtering, and initialization behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_48\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.serverview.type`|batch or http|Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper.|batch|\n|`druid.broker.segment.watchedTiers`|List of strings|Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of tiers. By default, Broker would consider all tiers. This can be used to partition your dataSources in specific Historical tiers and configure brokers in partitions so that they are only queryable for specific dataSources.|none|\n|`druid.broker.segment.watchedDataSources`|List of strings|Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of dataSources. By default, Broker would consider all datasources. This can be used to configure brokers in partitions so that they are only queryable for specific dataSources.|none|\n|`druid.broker.segment.awaitInitializationOnStart`|Boolean|Whether the the Broker will wait for its view of segments to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also `druid.sql.planner.awaitInitializationOnStart`, a related setting.|true|\n```\n\n----------------------------------------\n\nTITLE: Configuring Jetty Server TLS Settings in Apache Druid\nDESCRIPTION: This configuration table shows the Jetty server TLS settings for Apache Druid, including keystore path, type, certificate alias, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/tls-support.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.server.https.keyStorePath`|The file path or URL of the TLS/SSL Key store.|none|yes|\n|`druid.server.https.keyStoreType`|The type of the key store.|none|yes|\n|`druid.server.https.certAlias`|Alias of TLS/SSL certificate for the connector.|none|yes|\n|`druid.server.https.keyStorePassword`|The [Password Provider](../operations/password-provider.html) or String password for the Key Store.|none|yes|\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Stream Specification in Druid\nDESCRIPTION: Example JSON configuration for realtime stream ingestion in Druid. Demonstrates setup of dataSchema, ioConfig, and tuningConfig for a Wikipedia data stream using Kafka 0.8 as the firehose source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [{\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"added\",\n        \"fieldName\" : \"added\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"deleted\",\n        \"fieldName\" : \"deleted\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"delta\",\n        \"fieldName\" : \"delta\"\n      }],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\"\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"realtime\",\n      \"firehose\": {\n        \"type\": \"kafka-0.8\",\n        \"consumerProps\": {\n          \"zookeeper.connect\": \"localhost:2181\",\n          \"zookeeper.connection.timeout.ms\" : \"15000\",\n          \"zookeeper.session.timeout.ms\" : \"15000\",\n          \"zookeeper.sync.time.ms\" : \"5000\",\n          \"group.id\": \"druid-example\",\n          \"fetch.message.max.bytes\" : \"1048586\",\n          \"auto.offset.reset\": \"largest\",\n          \"auto.commit.enable\": \"false\"\n        },\n        \"feed\": \"wikipedia\"\n      },\n      \"plumber\": {\n        \"type\": \"realtime\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\" : \"realtime\",\n      \"maxRowsInMemory\": 1000000,\n      \"intermediatePersistPeriod\": \"PT10M\",\n      \"windowPeriod\": \"PT10M\",\n      \"basePersistDirectory\": \"\\/tmp\\/realtime\\/basePersist\",\n      \"rejectionPolicy\": {\n        \"type\": \"serverTime\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookup DimensionSpec with Map in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Lookup DimensionSpec with a map implementation in Apache Druid. It allows defining lookup implementations directly in the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"replaceMissingValueWith\":\"missing_value\",\n  \"retainMissingValue\":false,\n  \"lookup\":{\"type\": \"map\", \"map\":{\"key\":\"value\"}, \"isOneToOne\":false}\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Kafka for Apache Druid (Deprecated)\nDESCRIPTION: Command to start Tranquility Kafka with a configuration file. This method is deprecated in favor of the Kafka Indexing Service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-push.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka -configFile <path_to_config_file>/kafka.json\n```\n\n----------------------------------------\n\nTITLE: Using Bloom Filters in SQL WHERE Clauses\nDESCRIPTION: SQL syntax for filtering Druid data using bloom filters. This example shows how to use the bloom_filter_test operator in a WHERE clause with a serialized bloom filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')\n```\n\n----------------------------------------\n\nTITLE: Defining Selector Filter in Apache Druid JSON\nDESCRIPTION: Demonstrates the JSON structure for a selector filter in Apache Druid queries. This filter matches a specific dimension with a specific value, equivalent to a WHERE clause in SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n\"filter\": { \"type\": \"selector\", \"dimension\": <dimension_string>, \"value\": <dimension_value_string> }\n```\n\n----------------------------------------\n\nTITLE: Creating Union Data Source in Druid\nDESCRIPTION: Demonstrates how to create a union data source that combines multiple table data sources. All source tables must have the same schema, and queries must be sent to a Broker/Router process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/datasource.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n       \"type\": \"union\",\n       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Emitter Properties in Druid\nDESCRIPTION: Example configuration for setting up Kafka Emitter in Druid. Demonstrates how to specify Kafka bootstrap servers, metric and alert topics, and additional producer configurations. The configuration shows essential parameters for connecting to Kafka brokers and defining topics for metrics and alerts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/kafka-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092\ndruid.emitter.kafka.metric.topic=druid-metric\ndruid.emitter.kafka.alert.topic=druid-alert\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet-Avro Parser with Avro ParseSpec in Druid\nDESCRIPTION: JSON configuration for ingesting Parquet files using the 'parquet-avro' parser with an 'avro' parseSpec. This approach converts Parquet to Avro records first, then processes using the Druid Avro extensions module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet-avro\",\n        \"parseSpec\": {\n          \"format\": \"avro\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Output Format for groupBy Queries in Druid\nDESCRIPTION: Sample response format from a Druid groupBy query showing multiple result objects, each containing a timestamp and event object with dimension values and aggregated metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:00.000Z\",\n    \"event\" : {\n      \"country\" : <some_dim_value_one>,\n      \"device\" : <some_dim_value_two>,\n      \"total_usage\" : <some_value_one>,\n      \"data_transfer\" :<some_value_two>,\n      \"avg_usage\" : <some_avg_usage_value>\n    }\n  }, \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:12.000Z\",\n    \"event\" : {\n      \"dim1\" : <some_other_dim_value_one>,\n      \"dim2\" : <some_other_dim_value_two>,\n      \"sample_name1\" : <some_other_value_one>,\n      \"sample_name2\" :<some_other_value_two>,\n      \"avg_usage\" : <some_other_avg_usage_value>\n    }\n  },\n...\n]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Jackson Dependency Conflict Error in Java\nDESCRIPTION: This snippet shows the Java VerifyError that occurs due to Jackson dependency conflicts between Druid and CDH in a MapReduce job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\njava.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n```\n\n----------------------------------------\n\nTITLE: Documenting SegmentWriteOutMediumFactory Configuration in Markdown\nDESCRIPTION: Markdown table defining configuration parameters for SegmentWriteOutMediumFactory in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|See [Additional Peon Configuration: SegmentWriteOutMediumFactory] for explanation and available options.|yes|\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Apache Druid TuningConfig Parameters\nDESCRIPTION: A markdown table documenting all available parameters in the TuningConfig, including their descriptions, default values, and whether they are required. Covers configuration options for memory management, sharding, segment storage, task management, and error handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|property|description|default|required?|\n|--------|-----------|-------|----------|\n|type|The task type, this should always be `index_parallel`.|none|yes|\n|maxRowsPerSegment|Used in sharding. Determines how many rows are in each segment.|5000000|no|\n|maxRowsInMemory|Used in determining when intermediate persists to disk should occur. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set.|1000000|no|\n|maxBytesInMemory|Used in determining when intermediate persists to disk should occur. Normally this is computed internally and user does not need to set it. This value represents number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists)|1/6 of max JVM memory|no|\n|maxTotalRows|Total number of rows in segments waiting for being pushed. Used in determining when intermediate pushing should occur.|20000000|no|\n|numShards|Directly specify the number of shards to create. If this is specified and 'intervals' is specified in the granularitySpec, the index task can skip the determine intervals/partitions pass through the data. numShards cannot be specified if maxRowsPerSegment is set.|null|no|\n|indexSpec|defines segment storage format options to be used at indexing time, see [IndexSpec](#indexspec)|null|no|\n|maxPendingPersists|Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists).|0 (meaning one persist can be running concurrently with ingestion, and none can be queued up)|no|\n|forceExtendableShardSpecs|Forces use of extendable shardSpecs. Experimental feature intended for use with the [Kafka indexing service extension](../development/extensions-core/kafka-ingestion.html).|false|no|\n|reportParseExceptions|If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped.|false|no|\n|pushTimeout|Milliseconds to wait for pushing segments. It must be >= 0, where 0 means to wait forever.|0|no|\n|segmentWriteOutMediumFactory|Segment write-out medium to use when creating segments. See [SegmentWriteOutMediumFactory](#segmentWriteOutMediumFactory).|Not specified, the value from `druid.peon.defaultSegmentWriteOutMediumFactory.type` is used|no|\n|maxNumSubTasks|Maximum number of tasks which can be run at the same time. The supervisor task would spawn worker tasks up to `maxNumSubTasks` regardless of the available task slots. If this value is set to 1, the supervisor task processes data ingestion on its own instead of spawning worker tasks. If this value is set to too large, too many worker tasks can be created which might block other ingestion. Check [Capacity Planning](#capacity-planning) for more details.|1|no|\n|maxRetry|Maximum number of retries on task failures.|3|no|\n|taskStatusCheckPeriodMs|Polling period in milleseconds to check running task statuses.|1000|no|\n|chatHandlerTimeout|Timeout for reporting the pushed segments in worker tasks.|PT10S|no|\n|chatHandlerNumRetries|Retries for reporting the pushed segments in worker tasks.|5|no|\n```\n\n----------------------------------------\n\nTITLE: Using Column Comparison Filter in Druid Queries\nDESCRIPTION: The column comparison filter compares dimensions to each other rather than to a static value. It's equivalent to comparing two columns in a SQL WHERE clause.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"columnComparison\", \"dimensions\": [<dimension_a>, <dimension_b>] }\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Cache Properties in Druid\nDESCRIPTION: Configuration properties for enabling and controlling caching behavior on Druid Historical nodes. Includes settings for enabling cache, populating cache, and specifying uncacheable query types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_41\n\nLANGUAGE: properties\nCODE:\n```\ndruid.historical.cache.useCache=false\ndruid.historical.cache.populateCache=false\ndruid.historical.cache.unCacheable=[\"groupBy\", \"select\"]\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query with Skip Empty Buckets in Apache Druid\nDESCRIPTION: Example demonstrating how to skip empty time buckets in results by setting the skipEmptyBuckets context parameter. This prevents zero-filling of missing time periods.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-04T00:00:00.000\" ],\n  \"context\" : {\n    \"skipEmptyBuckets\": \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Lookup in Apache Druid (JSON)\nDESCRIPTION: JSON configuration for setting up a Kafka lookup in Apache Druid. It specifies the lookup type, Kafka topic, and Kafka properties including the Zookeeper connection string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"kafka\",\n  \"kafkaTopic\":\"testTopic\",\n  \"kafkaProperties\":{\"zookeeper.connect\":\"somehost:2181/kafka\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Features in Druid Configuration\nDESCRIPTION: Configuration example showing how to enable experimental features in Druid by adding their artifacts to the loadList property in runtime.properties file. This setting needs to be applied to all Druid processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/experimental.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-histogram\"]\n```\n\n----------------------------------------\n\nTITLE: Sample Coordinator Dynamic Configuration JSON\nDESCRIPTION: Example JSON configuration object showing various Coordinator settings including deletion waiting period, merge limits, replication parameters, and decommissioning configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"millisToWaitBeforeDeleting\": 900000,\n  \"mergeBytesLimit\": 100000000,\n  \"mergeSegmentsLimit\" : 1000,\n  \"maxSegmentsToMove\": 5,\n  \"replicantLifetime\": 15,\n  \"replicationThrottleLimit\": 10,\n  \"emitBalancingStats\": false,\n  \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"],\n  \"decommissioningNodes\": [\"localhost:8182\", \"localhost:8282\"],\n  \"decommissioningMaxPercentOfMaxSegmentsToMove\": 70\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Coordinator Console (Version 2)\nDESCRIPTION: URL pattern for accessing the Version 2 Coordinator Console which displays cluster information and rule configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/management-uis.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>\n```\n\n----------------------------------------\n\nTITLE: Querying with LessThan Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Shows how to use a 'lessThan' filter in a HavingSpec for a groupBy query. This filter matches rows with aggregate values less than the specified value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"lessThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Coordinator Console (Version 2)\nDESCRIPTION: URL pattern for accessing the Version 2 Coordinator Console which displays cluster information and rule configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/management-uis.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Redundancy in Druid (JSON)\nDESCRIPTION: Example JSON configurations for shardSpec to create redundancy across multiple real-time processes. This allows multiple processes to store segments with the same datasource, version, time interval, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"linear\",\n    \"partitionNum\": 0\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"linear\",\n    \"partitionNum\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration in Apache Druid (JSON)\nDESCRIPTION: This snippet illustrates how to configure a URI-based lookup in Apache Druid. It specifies the S3 location of the lookup data, parsing format, and polling period for updates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uri\": \"s3://bucket/some/key/prefix/renames-0003.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Name Range Bound Filter in Druid\nDESCRIPTION: Example of a bound filter for lexicographic string comparison between 'foo' and 'hoo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Coordinator Dynamic Configuration JSON\nDESCRIPTION: Example JSON configuration for Druid Coordinator showing common dynamic configuration parameters including settings for segment deletion, merging, replication, and data source management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"millisToWaitBeforeDeleting\": 900000,\n  \"mergeBytesLimit\": 100000000,\n  \"mergeSegmentsLimit\" : 1000,\n  \"maxSegmentsToMove\": 5,\n  \"replicantLifetime\": 15,\n  \"replicationThrottleLimit\": 10,\n  \"emitBalancingStats\": false,\n  \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Manually disabling a segment in Druid via the Coordinator API\nDESCRIPTION: cURL command to send a DELETE request to the Coordinator API that marks a specific segment as 'unused', which is the first step in the permanent deletion process. The segment ID is a placeholder that needs to be replaced with the actual segment ID.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XDELETE http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/segments/{SEGMENT-ID}\n```\n\n----------------------------------------\n\nTITLE: Configuring RegexSearchQuerySpec in Druid\nDESCRIPTION: Defines a search using regular expressions that matches when any part of a dimension value matches the specified pattern. This specification provides the most flexible pattern matching capabilities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"regex\",\n  \"pattern\" : \"some_pattern\"\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting Math Functions in Markdown for Apache Druid\nDESCRIPTION: This markdown table lists various mathematical functions available in Apache Druid, including their names and descriptions. It covers a wide range of operations such as absolute value, trigonometric functions, logarithms, and more. The descriptions reference the Java Math class for detailed explanations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/misc/math-expr.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|name|description|\n|----|-----------|\n|abs|abs(x) would return the absolute value of x|\n|acos|acos(x) would return the arc cosine of x|\n|asin|asin(x) would return the arc sine of x|\n|atan|atan(x) would return the arc tangent of x|\n|atan2|atan2(y, x) would return the angle theta from the conversion of rectangular coordinates (x, y) to polar * coordinates (r, theta)|\n|cbrt|cbrt(x) would return the cube root of x|\n|ceil|ceil(x) would return the smallest (closest to negative infinity) double value that is greater than or equal to x and is equal to a mathematical integer|\n|copysign|copysign(x) would return the first floating-point argument with the sign of the second floating-point argument|\n|cos|cos(x) would return the trigonometric cosine of x|\n|cosh|cosh(x) would return the hyperbolic cosine of x|\n|div|div(x,y) is integer division of x by y|\n|exp|exp(x) would return Euler's number raised to the power of x|\n|expm1|expm1(x) would return e^x-1|\n|floor|floor(x) would return the largest (closest to positive infinity) double value that is less than or equal to x and is equal to a mathematical integer|\n|getExponent|getExponent(x) would return the unbiased exponent used in the representation of x|\n|hypot|hypot(x, y) would return sqrt(x^2+y^2) without intermediate overflow or underflow|\n|log|log(x) would return the natural logarithm of x|\n|log10|log10(x) would return the base 10 logarithm of x|\n|log1p|log1p(x) would the natural logarithm of x + 1|\n|max|max(x, y) would return the greater of two values|\n|min|min(x, y) would return the smaller of two values|\n|nextafter|nextafter(x, y) would return the floating-point number adjacent to the x in the direction of the y|\n|nextUp|nextUp(x) would return the floating-point value adjacent to x in the direction of positive infinity|\n|pow|pow(x, y) would return the value of the x raised to the power of y|\n|remainder|remainder(x, y) would return the remainder operation on two arguments as prescribed by the IEEE 754 standard|\n|rint|rint(x) would return value that is closest in value to x and is equal to a mathematical integer|\n|round|round(x) would return the closest long value to x, with ties rounding up|\n|scalb|scalb(d, sf) would return d * 2^sf rounded as if performed by a single correctly rounded floating-point multiply to a member of the double value set|\n|signum|signum(x) would return the signum function of the argument x|\n|sin|sin(x) would return the trigonometric sine of an angle x|\n|sinh|sinh(x) would return the hyperbolic sine of x|\n|sqrt|sqrt(x) would return the correctly rounded positive square root of x|\n|tan|tan(x) would return the trigonometric tangent of an angle x|\n|tanh|tanh(x) would return the hyperbolic tangent of x|\n|todegrees|todegrees(x) converts an angle measured in radians to an approximately equivalent angle measured in degrees|\n|toradians|toradians(x) converts an angle measured in degrees to an approximately equivalent angle measured in radians|\n|ulp|ulp(x) would return the size of an ulp of the argument x|\n```\n\n----------------------------------------\n\nTITLE: Object Assign License\nDESCRIPTION: MIT license declaration for object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Advanced GroupBy v2 Runtime Properties in Apache Druid\nDESCRIPTION: Detailed configuration options for GroupBy v2 including buffer settings, hash aggregation, and parallel processing parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/groupbyquery.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.bufferGrouperInitialBuckets`|Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024).|0|\n|`druid.query.groupBy.bufferGrouperMaxLoadFactor`|Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7).|0|\n|`druid.query.groupBy.forceHashAggregation`|Force to use hash-based aggregation.|false|\n|`druid.query.groupBy.intermediateCombineDegree`|Number of intermediate nodes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful cpu cores.|8|\n|`druid.query.groupBy.numParallelCombineThreads`|Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(`druid.query.groupBy.numParallelCombineThreads`, `druid.processing.numThreads`).|1 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Sample Row Output Format from DumpSegment Tool\nDESCRIPTION: Example of a single row output from the DumpSegment tool when using the default row dump option. The output is a JSON object with all column values from the segment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/dump-segment.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__time\": 1442018818771,\n  \"added\": 36,\n  \"channel\": \"#en.wikipedia\",\n  \"cityName\": null,\n  \"comment\": \"added project\",\n  \"count\": 1,\n  \"countryIsoCode\": null,\n  \"countryName\": null,\n  \"deleted\": 0,\n  \"delta\": 36,\n  \"isAnonymous\": \"false\",\n  \"isMinor\": \"false\",\n  \"isNew\": \"false\",\n  \"isRobot\": \"false\",\n  \"isUnpatrolled\": \"false\",\n  \"iuser\": \"00001553\",\n  \"metroCode\": null,\n  \"namespace\": \"Talk\",\n  \"page\": \"Talk:Oswald Tilghman\",\n  \"regionIsoCode\": null,\n  \"regionName\": null,\n  \"user\": \"GELongstreet\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator in Druid\nDESCRIPTION: JSON configuration for the cardinality aggregator that uses HyperLogLog to estimate cardinality of dimension sets. Supports both single and multiple dimensions with optional row-based computation and rounding.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/hll-old.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"<output_name>\",\n  \"fields\": [ <dimension1>, <dimension2>, ... ],\n  \"byRow\": <false | true> # (optional, defaults to false),\n  \"round\": <false | true> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Initial Data in Apache Druid\nDESCRIPTION: This SQL query retrieves all rows from the newly created 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"updates-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Configuring Histogram Post-Aggregator in Druid\nDESCRIPTION: Demonstrates the JSON configuration for the quantilesDoublesSketchToHistogram post-aggregator, which returns an approximation of a histogram given split points.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToHistogram\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"splitPoints\" : <array of split points>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DimensionsSpec with Custom Types and Bitmap Indexing in Apache Druid\nDESCRIPTION: This snippet shows how to configure a dimensionsSpec in Apache Druid, including custom dimension types (Long and Float) and disabling bitmap indexing for a specific string column. It demonstrates the flexibility in defining dimension schemas and controlling index creation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Coordinator Operation Configuration Properties Table\nDESCRIPTION: Configuration table showing properties for basic Coordinator operations including run periods, delays, timeouts and operational flags.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_25\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.coordinator.period`|The run period for the Coordinator. The Coordinator's operates by maintaining the current state of the world in memory and periodically looking at the set of segments available and segments being served to make decisions about whether any changes need to be made to the data topology. This property sets the delay between each of these runs.|PT60S|\n|`druid.coordinator.period.indexingPeriod`|How often to send compact/merge/conversion tasks to the indexing service. It's recommended to be longer than `druid.manager.segments.pollDuration`|PT1800S (30 mins)|\n|`druid.coordinator.startDelay`|The operation of the Coordinator works on the assumption that it has an up-to-date view of the state of the world when it runs, the current ZK interaction code, however, is written in a way that doesn't allow the Coordinator to know for a fact that it's done loading the current state of the world. This delay is a hack to give it enough time to believe that it has all the data.|PT300S|\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Selector Filter on Multi-value Dimensions\nDESCRIPTION: A GroupBy query that filters for rows where the 'tags' dimension contains 't3'. With multi-value dimensions, the filter matches any row where at least one value matches, then all values from matching rows are included in grouping.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the thetaSketch aggregator used during ingestion and query time. This defines how Theta sketches are created and stored in Druid segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"thetaSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,  \n  \"isInputThetaSketch\": false,\n  \"size\": 16384\n }\n```\n\n----------------------------------------\n\nTITLE: Viewing Appended Data in Druid\nDESCRIPTION: SQL query showing the result after appending data, where new rows have been added but duplicate dimensions (bear) exist in separate segments without roll-up.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          2     400 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n 2018-01-01T05:01:00.000Z  mongoose      1     737 \n 2018-01-01T06:01:00.000Z  snake         1    1234 \n 2018-01-01T07:01:00.000Z  octopus       1     115 \n 2018-01-01T04:01:00.000Z  bear          1     222 \n 2018-01-01T09:01:00.000Z  falcon        1    1241 \n\nRetrieved 8 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Configuring Expression Virtual Column in Apache Druid (JSON)\nDESCRIPTION: This snippet shows the syntax for configuring an expression virtual column in Apache Druid. It includes the required properties such as type, name, expression, and the optional outputType property.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/virtual-columns.md#2025-04-09_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <name of the virtual column>,\n  \"expression\": <row expression>,\n  \"outputType\": <output value type of expression>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Age Range Bound Filter in Druid\nDESCRIPTION: Example of a bound filter that checks if age is between 21 and 31 using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Drop Rule in Apache Druid\nDESCRIPTION: The Forever Drop Rule permanently removes matching segments from the cluster. All segments matching this rule will be dropped without any time-based conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropForever\"  \n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchToString Debug Post-Aggregator\nDESCRIPTION: JSON configuration for a debug post-aggregator that converts an HLL sketch to a human-readable string representation. This is useful for debugging and understanding the internal state of sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchToString Debug Post-Aggregator\nDESCRIPTION: JSON configuration for a debug post-aggregator that converts an HLL sketch to a human-readable string representation. This is useful for debugging and understanding the internal state of sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Flags for Apache Druid\nDESCRIPTION: This snippet provides recommended JVM flags for running Apache Druid. It includes settings for timezone, file encoding, logging, garbage collection, and memory management. These flags aim to optimize performance and enhance debugging capabilities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>\n-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\n-Dorg.jboss.logging.provider=slf4j\n-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger\n-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown\n-Dlog4j.shutdownHookEnabled=true\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCApplicationStoppedTime\n-XX:+PrintGCApplicationConcurrentTime\n-Xloggc:/var/logs/druid/historical.gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=50\n-XX:GCLogFileSize=10m\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/logs/druid/historical.hprof\n-XX:MaxDirectMemorySize=10240g\n```\n\n----------------------------------------\n\nTITLE: Defining Input Source with ioConfig in Druid JSON\nDESCRIPTION: Configures the input source for Druid ingestion using a local firehose. This snippet shows how to set up the ioConfig to read from a local JSON file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n```\n\n----------------------------------------\n\nTITLE: HDFS Deep Storage Configuration\nDESCRIPTION: Configuration settings for using HDFS as Druid's deep storage layer. Includes storage paths for segments and indexing logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-hdfs-storage\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Logical OR Filter in Druid\nDESCRIPTION: Combines multiple filters with OR logic to create complex filtering conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Retrieving Completion Report in Apache Druid\nDESCRIPTION: Example of an API endpoint to retrieve the completion report for a finished ingestion task. The report contains information about ingested rows and parse exceptions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/reports.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Aggregator in Apache Druid\nDESCRIPTION: This snippet shows how to configure a HyperUnique aggregator in Apache Druid. It uses HyperLogLog to compute the estimated cardinality of a dimension that has been aggregated as a 'hyperUnique' metric at indexing time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/hll-old.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"hyperUnique\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"isInputHyperUnique\" : false,\n  \"round\" : false\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript and Storage Configuration\nDESCRIPTION: Properties for enabling JavaScript functionality and configuring double column storage precision.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_22\n\nLANGUAGE: properties\nCODE:\n```\ndruid.javascript.enabled=false\ndruid.indexing.doubleStorage=double\n```\n\n----------------------------------------\n\nTITLE: Multiple Inline Schemas Avro Decoder Configuration\nDESCRIPTION: Configuration for multiple inline schemas-based Avro bytes decoder that supports different schemas identified by schema IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/avro.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"avroBytesDecoder\": {\n    \"type\": \"multiple_schemas_inline\",\n    \"schemas\": {\n      \"1\": {\n        \"namespace\": \"org.apache.druid.data\",\n        \"name\": \"User\",\n        \"type\": \"record\",\n        \"fields\": [\n          { \"name\": \"FullName\", \"type\": \"string\" },\n          { \"name\": \"Country\", \"type\": \"string\" }\n        ]\n      },\n      \"2\": {\n        \"namespace\": \"org.apache.druid.otherdata\",\n        \"name\": \"UserIdentity\",\n        \"type\": \"record\",\n        \"fields\": [\n          { \"name\": \"Name\", \"type\": \"string\" },\n          { \"name\": \"Location\", \"type\": \"string\" }\n        ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Issuing Native Queries to Apache Druid using cURL\nDESCRIPTION: This snippet demonstrates how to send a native query to Apache Druid using cURL. It specifies the Content-Type and Accept headers as application/json and sends the query JSON file to the specified Druid host and port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/querying.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Basic Theta Sketch Aggregator Example\nDESCRIPTION: Example showing how to create a Theta sketch aggregator during ingestion, used to count unique user IDs for different products.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"thetaSketch\", \"name\": \"user_id_sketch\", \"fieldName\": \"user_id\" }\n```\n\n----------------------------------------\n\nTITLE: Calculating Simple Percentage Using Post-Aggregators in Druid\nDESCRIPTION: Shows a query structure that uses aggregations and post-aggregations to calculate a simple percentage based on a 'total' metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n  \"aggregations\" : [\n    { \"type\" : \"count\", \"name\" : \"rows\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n         ]\n  }]\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Timestamp Column Filter in Druid\nDESCRIPTION: Examples of filtering on timestamp columns using different approaches including direct millisecond values and day of week.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"124457387532\"\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy v2 Configuration Table\nDESCRIPTION: Configuration table showing runtime properties for groupBy v2 queries, including heap space and disk storage settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxMergingDictionarySize`|Maximum amount of heap space (approximately) to use for the string dictionary during merging. When the dictionary exceeds this size, a spill to disk will be triggered.|100000000|\n|`druid.query.groupBy.maxOnDiskStorage`|Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling.|0 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Implementing Age Range Bound Filter in Druid\nDESCRIPTION: Example of a bound filter that checks if age is between 21 and 31 using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Flags for Apache Druid\nDESCRIPTION: Recommended JVM flags for Apache Druid processes that optimize performance, logging, memory management and error handling. These settings include timezone configuration, garbage collection logging, memory management, and out-of-memory error handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>\n-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\n-Dorg.jboss.logging.provider=slf4j\n-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger\n-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown\n-Dlog4j.shutdownHookEnabled=true\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCApplicationStoppedTime\n-XX:+PrintGCApplicationConcurrentTime\n-Xloggc:/var/logs/druid/historical.gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=50\n-XX:GCLogFileSize=10m\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/logs/druid/historical.hprof\n-XX:MaxDirectMemorySize=10240g\n```\n\n----------------------------------------\n\nTITLE: Implementing pvalue2tailedZtest Post Aggregator in Druid\nDESCRIPTION: JSON configuration for the pvalue2tailedZtest post aggregator which calculates the p-value of a two-sided z-test from a provided z-score. This is typically used following a zscore2sample calculation to determine statistical significance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"pvalue2tailedZtest\",\n  \"name\": \"<output_name>\",\n  \"zScore\": <zscore post_aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Timestamp Selector Filter in Druid\nDESCRIPTION: Shows how to filter on timestamp values using long millisecond representation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"124457387532\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Compaction Task in Apache Druid\nDESCRIPTION: JSON structure for creating a compaction task in Apache Druid. This configuration specifies how to merge segments for a given time interval with various optional parameters to control the compaction process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"compact\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\": <interval to specify segments to be merged>,\n    \"dimensions\" <custom dimensionsSpec>,\n    \"keepSegmentGranularity\": <true or false>,\n    \"segmentGranularity\": <segment granularity after compaction>,\n    \"targetCompactionSizeBytes\": <target size of compacted segments>\n    \"tuningConfig\" <index task tuningConfig>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Settings in YAML\nDESCRIPTION: YAML configuration for Druid Historical process, including host, port, and service name settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_36\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.host: InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost: false\ndruid.plaintextPort: 8083\ndruid.tlsPort: 8283\ndruid.service: druid/historical\n```\n\n----------------------------------------\n\nTITLE: Constructing a Search Query in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to structure a search query in Apache Druid. It includes essential parameters such as queryType, dataSource, granularity, searchDimensions, query specification, sort order, and time intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/searchquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"search\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"searchDimensions\": [\n    \"dim1\",\n    \"dim2\"\n  ],\n  \"query\": {\n    \"type\": \"insensitive_contains\",\n    \"value\": \"Ke\"\n  },\n  \"sort\" : {\n    \"type\": \"lexicographic\"\n  },\n  \"intervals\": [\n    \"2013-01-01T00:00:00.000/2013-01-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Timeseries Query Example with Variance Aggregator\nDESCRIPTION: Complete example of a Druid timeseries query that uses the variance aggregator to calculate variance of a metric over time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"testing\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring longLast Aggregator in Apache Druid for Queries\nDESCRIPTION: The longLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"longLast\",\n  \"name\" : <output_name>, \n  \"fieldName\" : <metric_name>,\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Filter Example in Apache Druid (JSON)\nDESCRIPTION: A JavaScript filter example that matches dimension values for 'name' that fall alphabetically between 'bar' and 'foo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : \"name\",\n  \"function\" : \"function(x) { return(x >= 'bar' && x <= 'foo') }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Period Load Rule Configuration in Druid\nDESCRIPTION: Configures segment replication based on a rolling time period using ISO-8601 period format. Includes options for future data inclusion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true,\n  \"tieredReplicants\": {\n      \"hot\": 1,\n      \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Set Operation Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the thetaSketchSetOp post-aggregator, used to perform set operations (UNION, INTERSECT, NOT) on Theta sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchSetOp\",\n  \"name\": <output name>,\n  \"func\": <UNION|INTERSECT|NOT>,\n  \"fields\"  : <array of fieldAccess type post aggregators to access the thetaSketch aggregators or thetaSketchSetOp type post aggregators to allow arbitrary combination of set operations>,\n  \"size\": <16384 by default, must be max of size from sketches in fields input>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Common GroupBy Query Properties in Druid\nDESCRIPTION: Common configuration properties for all GroupBy query strategies, including default strategy selection and threading options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_44\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.defaultStrategy=v2\ndruid.query.groupBy.singleThreaded=false\n```\n\n----------------------------------------\n\nTITLE: Defining Search Filter in Apache Druid JSON\nDESCRIPTION: Shows the JSON structure for a search filter in Apache Druid. This filter is used for partial string matching on dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_9\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"search\",\n        \"dimension\": \"product\",\n        \"query\": {\n          \"type\": \"insensitive_contains\",\n          \"value\": \"foo\" \n        }        \n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Full Serialization Format for Histogram in Apache Druid\nDESCRIPTION: Defines the byte-level structure for fully serializing a histogram, including version, encoding mode, limits, bucket counts, and outlier handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x01 for full\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\narray of longs: bucket counts for the histogram\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose in Apache Druid\nDESCRIPTION: This snippet demonstrates the configuration for an HttpFirehose, which is used to read data from remote sites via HTTP. It specifies the URIs to fetch data from.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/firehose.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"http\",\n    \"uris\"  : [\"http://example.com/uri1\", \"http://example2.com/uri2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Query Filter Implementation in Druid\nDESCRIPTION: Shows how to implement a basic query filter HavingSpec in a groupBy query, allowing any Druid query filter to be used in the Having clause.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/having.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : <any Druid query filter>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop JobProperties in Druid TuningConfig\nDESCRIPTION: Example of configuring Hadoop job properties within a Druid tuningConfig. This allows passing custom Hadoop configuration parameters to the MapReduce job during indexing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n   \"tuningConfig\" : {\n     \"type\": \"hadoop\",\n     \"jobProperties\": {\n       \"<hadoop-property-a>\": \"<value-a>\",\n       \"<hadoop-property-b>\": \"<value-b>\"\n     }\n   }\n```\n\n----------------------------------------\n\nTITLE: Managing Druid Coordinator Compaction Configuration\nDESCRIPTION: GET, POST, and DELETE requests to manage compaction configuration. Includes endpoints for retrieving, creating, updating, and deleting compaction configs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_10\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/config/compaction\nGET /druid/coordinator/v1/config/compaction/{dataSource}\nPOST /druid/coordinator/v1/config/compaction/taskslots?ratio={someRatio}&max={someMaxSlots}\nPOST /druid/coordinator/v1/config/compaction\nDELETE /druid/coordinator/v1/config/compaction/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring Coordinator Discovery in Druid\nDESCRIPTION: Configuration for finding the Coordinator using Curator service discovery. Used by realtime indexing processes to get segment information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_20\n\nLANGUAGE: properties\nCODE:\n```\ndruid.selectors.coordinator.serviceName=druid/coordinator\n```\n\n----------------------------------------\n\nTITLE: Displaying GroupBy v2 Runtime Properties Table in Markdown\nDESCRIPTION: This snippet shows a markdown table listing the supported runtime properties for GroupBy v2 queries in Apache Druid. It includes property names, descriptions, and default values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_56\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.bufferGrouperInitialBuckets`|Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024).|0|\n|`druid.query.groupBy.bufferGrouperMaxLoadFactor`|Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7).|0|\n|`druid.query.groupBy.forceHashAggregation`|Force to use hash-based aggregation.|false|\n|`druid.query.groupBy.intermediateCombineDegree`|Number of intermediate processes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful cpu cores.|8|\n|`druid.query.groupBy.numParallelCombineThreads`|Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(`druid.query.groupBy.numParallelCombineThreads`, `druid.processing.numThreads`).|1 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Submitting Kill Task for Permanent Deletion\nDESCRIPTION: CURL command to submit a Kill Task that permanently removes disabled segments from both metadata and deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Variances Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for post-aggregator to get variance values for each column from ArrayOfDoublesSketch\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToVariances\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: HTTP Compression Configuration Parameters - Markdown Table\nDESCRIPTION: Configuration table showing the properties, descriptions and default values for HTTP compression settings in Apache Druid. Includes compression level and buffer size parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/http-compression.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|\n|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|\n```\n\n----------------------------------------\n\nTITLE: Null Selector Filter for Multi-value Dimension in Druid\nDESCRIPTION: Illustrates a selector filter that matches rows where the 'tags' dimension is null or empty. This filter would match the fourth row of the sample dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"tags\",\n  \"value\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Min/Max Post-Aggregator Configuration\nDESCRIPTION: JSON configurations for retrieving minimum and maximum values from histogram aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"min\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"max\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Priority Router Strategy Configuration\nDESCRIPTION: JSON configuration for the priority routing strategy, which routes queries based on their priority level to different tiers of Brokers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"priority\",\n  \"minPriority\":0,\n  \"maxPriority\":1\n}\n```\n\n----------------------------------------\n\nTITLE: Null Selector Filter for Multi-value Dimension in Druid\nDESCRIPTION: Illustrates a selector filter that matches rows where the 'tags' dimension is null or empty. This filter would match the fourth row of the sample dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"tags\",\n  \"value\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Initial Segment Naming in Druid\nDESCRIPTION: Demonstrates the naming convention for Druid segments with multiple partitions for the same time interval. The example shows three segments for the same datasource, time interval, and version, but with different partition numbers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-01/2015-01-02_v1_1\nfoo_2015-01-01/2015-01-02_v1_2\n```\n\n----------------------------------------\n\nTITLE: Inline Schema Avro Decoder Configuration\nDESCRIPTION: Configuration for inline schema-based Avro bytes decoder with a fixed schema definition. Suitable for cases where all input events use the same schema.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"avroBytesDecoder\": {\n    \"type\": \"schema_inline\",\n    \"schema\": {\n      \"namespace\": \"org.apache.druid.data\",\n      \"name\": \"User\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"FullName\", \"type\": \"string\" },\n        { \"name\": \"Country\", \"type\": \"string\" }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Load Rule in Apache Druid\nDESCRIPTION: The Period Load Rule specifies how many replicas of a segment should exist in different server tiers based on a rolling time period. It retains data for the specified ISO-8601 period from past to present or future.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true,\n  \"tieredReplicants\": {\n      \"hot\": 1,\n      \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Basic Segment Metadata Query in Druid\nDESCRIPTION: Basic segment metadata query structure to retrieve information about segments within a specific time range from a data source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\":\"segmentMetadata\",\n  \"dataSource\":\"sample_datasource\",\n  \"intervals\":[\"2013-01-01/2014-01-01\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ThetaSketch Aggregator in Druid Query\nDESCRIPTION: Demonstrates the configuration of a thetaSketch aggregator in a Druid query, including parameters for output name, field name, input type, and sketch size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"thetaSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,  \n  \"isInputThetaSketch\": false,\n  \"size\": 16384\n }\n```\n\n----------------------------------------\n\nTITLE: Example Expression Transform in Druid\nDESCRIPTION: Demonstrates an example expression transform that prepends 'foo' to values in a 'page' column and stores the result in a new 'fooPage' column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Regex ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing data using regular expressions in Druid, including pattern matching and column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"regex\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [<your_list_of_dimensions>]\n    },\n    \"columns\" : [<your_columns_here>],\n    \"pattern\" : <regex pattern for partitioning data>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Response with Zero Sent Count in JSON\nDESCRIPTION: Example of a response from Tranquility Server showing that events were received but not yet sent to Druid. This may occur when Druid is still allocating resources for the ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":0}}\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to Quantile Post Aggregator\nDESCRIPTION: Post aggregator configuration for extracting a single quantile value from a DoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantile\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch>,\n  \"fraction\" : <fractional position in the hypothetical sorted stream>\n}\n```\n\n----------------------------------------\n\nTITLE: MySQL Database and User Creation\nDESCRIPTION: SQL commands to create a Druid database with UTF8MB4 encoding, create a Druid user, and grant necessary permissions\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- create a druid database, make sure to use utf8mb4 as encoding\nCREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;\n\n-- create a druid user\nCREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';\n\n-- grant the user all the permissions on the database we just created\nGRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Custom Buckets Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the custom buckets post-aggregator, which computes a visual representation with user-defined break points.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"customBuckets\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"breaks\" : [ <value>, <value>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Emitter Module in Apache Druid\nDESCRIPTION: Properties for setting up the logging emitter module in Druid. This includes specifying the logger class and log level for emitting metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_14\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.logging.loggerClass=LoggingEmitter\ndruid.emitter.logging.logLevel=info\n```\n\n----------------------------------------\n\nTITLE: Displaying Coordination Metrics Table in Markdown\nDESCRIPTION: A markdown table showing various coordination metrics for the Druid Coordinator, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/assigned/count`|Number of segments assigned to be loaded in the cluster.|tier.|Varies.|\n|`segment/moved/count`|Number of segments moved in the cluster.|tier.|Varies.|\n|`segment/dropped/count`|Number of segments dropped due to being overshadowed.|tier.|Varies.|\n|`segment/deleted/count`|Number of segments dropped due to rules.|tier.|Varies.|\n|`segment/unneeded/count`|Number of segments dropped due to being marked as unused.|tier.|Varies.|\n|`segment/cost/raw`|Used in cost balancing. The raw cost of hosting segments.|tier.|Varies.|\n|`segment/cost/normalization`|Used in cost balancing. The normalization of hosting segments.|tier.|Varies.|\n|`segment/cost/normalized`|Used in cost balancing. The normalized cost of hosting segments.|tier.|Varies.|\n|`segment/loadQueue/size`|Size in bytes of segments to load.|server.|Varies.|\n|`segment/loadQueue/failed`|Number of segments that failed to load.|server.|0|\n|`segment/loadQueue/count`|Number of segments to load.|server.|Varies.|\n|`segment/dropQueue/count`|Number of segments to drop.|server.|Varies.|\n|`segment/size`|Size in bytes of available segments.|dataSource.|Varies.|\n|`segment/count`|Number of available segments.|dataSource.|< max|\n|`segment/overShadowed/count`|Number of overShadowed segments.||Varies.|\n|`segment/unavailable/count`|Number of segments (not including replicas) left to load until segments that should be loaded in the cluster are available for queries.|datasource.|0|\n|`segment/underReplicated/count`|Number of segments (including replicas) left to load until segments that should be loaded in the cluster are available for queries.|tier, datasource.|0|\n```\n\n----------------------------------------\n\nTITLE: Sending Data to Kafka Topic in Bash\nDESCRIPTION: Commands to set encoding options and use the Kafka console producer to send the sample Wikipedia data to the Kafka topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport KAFKA_OPTS=\"-Dfile.encoding=UTF-8\"\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Compaction Configuration Example\nDESCRIPTION: Simple JSON configuration example for compacting the 'wikiticker' datasource in Druid. This represents a minimal compaction configuration that relies on default values for optional parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSource\": \"wikiticker\"\n}\n```\n\n----------------------------------------\n\nTITLE: Rendezvous Hash Balancer Configuration\nDESCRIPTION: Configuration property for enabling the Rendezvous Hash Balancer for Avatica JDBC request routing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_6\n\nLANGUAGE: java\nCODE:\n```\ndruid.router.avatica.balancer.type=rendezvousHash\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Selector Filter on Multi-value Dimensions in Apache Druid\nDESCRIPTION: Example of a groupBy query that filters and groups results by the 'tags' multi-value dimension using a selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Extraction DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up an Extraction DimensionSpec in Apache Druid. It allows transforming dimension values using an extraction function and specifying output types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : <dimension>,\n  \"outputName\" :  <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">,\n  \"extractionFn\" : <extraction_function>\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Multi-value Dimensions with OR Condition in Apache Druid\nDESCRIPTION: Example of an 'or' filter that matches rows containing either 't1' or 't3' in the 'tags' multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"or\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Realtime Process Config Example in YAML\nDESCRIPTION: Example configuration properties for Apache Druid's Realtime Process showing the host, port and service configuration options. These settings control how the Realtime Process is identified and accessed within the Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.host: InetAddress.getLocalHost().getCanonicalHostName()\ndruid.plaintextPort: 8084\ndruid.tlsPort: 8284\ndruid.service: druid/realtime\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Direct File Reference in Apache Druid\nDESCRIPTION: JSON configuration for a URI-based lookup using a direct S3 file reference. This example shows how to configure a CSV lookup with polling for updates periodically.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uri\": \"s3://bucket/some/key/prefix/renames-0003.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TwitterSpritzerFirehose in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure the TwitterSpritzerFirehose in Apache Druid. It specifies the firehose type as 'twitzer' and sets parameters for maximum event count and run time in minutes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/examples.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"twitzer\",\n    \"maxEventCount\": -1,\n    \"maxRunMinutes\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Spec File for Stream Pull Ingestion in Apache Druid\nDESCRIPTION: This JSON configuration defines the structure and settings for a Realtime process in Apache Druid. It includes data schema definition, input/output configuration, and tuning parameters for ingesting data from a Kafka stream.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [{\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"added\",\n        \"fieldName\" : \"added\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"deleted\",\n        \"fieldName\" : \"deleted\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"delta\",\n        \"fieldName\" : \"delta\"\n      }],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\"\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"realtime\",\n      \"firehose\": {\n        \"type\": \"kafka-0.8\",\n        \"consumerProps\": {\n          \"zookeeper.connect\": \"localhost:2181\",\n          \"zookeeper.connection.timeout.ms\" : \"15000\",\n          \"zookeeper.session.timeout.ms\" : \"15000\",\n          \"zookeeper.sync.time.ms\" : \"5000\",\n          \"group.id\": \"druid-example\",\n          \"fetch.message.max.bytes\" : \"1048586\",\n          \"auto.offset.reset\": \"largest\",\n          \"auto.commit.enable\": \"false\"\n        },\n        \"feed\": \"wikipedia\"\n      },\n      \"plumber\": {\n        \"type\": \"realtime\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\" : \"realtime\",\n      \"maxRowsInMemory\": 1000000,\n      \"intermediatePersistPeriod\": \"PT10M\",\n      \"windowPeriod\": \"PT10M\",\n      \"basePersistDirectory\": \"\\/tmp\\/realtime\\/basePersist\",\n      \"rejectionPolicy\": {\n        \"type\": \"serverTime\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining AND Logical Expression Filter in Apache Druid JSON\nDESCRIPTION: Demonstrates the JSON structure for an AND logical expression filter in Apache Druid. This filter combines multiple other filters with AND logic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n\"filter\": { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch to String Post Aggregator\nDESCRIPTION: Post aggregator that returns a human-readable summary of an ArrayOfDoublesSketch by calling the sketch's toString() method. Useful for debugging and inspecting sketch contents.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a Kill Task in Apache Druid\nDESCRIPTION: JSON configuration for a Kill Task that deletes information about segments and removes them from deep storage. Kill tasks can only be performed on segments that have been marked as disabled (used==0) in the Druid segment table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"kill\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\" : <all_segments_in_this_interval_will_die!>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Basic Segment Metadata Query in Druid\nDESCRIPTION: Basic segment metadata query structure to retrieve information about segments within a specific time range from a data source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\":\"segmentMetadata\",\n  \"dataSource\":\"sample_datasource\",\n  \"intervals\":[\"2013-01-01/2014-01-01\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Count Aggregator in Druid\nDESCRIPTION: The count aggregator computes the count of Druid rows that match the specified filters. This returns the number of Druid rows, which may not reflect the actual number of raw events if data rollup was enabled during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"count\", \"name\" : <output_name> }\n```\n\n----------------------------------------\n\nTITLE: LIKE Filter Example in Druid\nDESCRIPTION: Filter for pattern matching using SQL-like wildcards, supporting % and _ characters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"like\",\n    \"dimension\": \"last_name\",\n    \"pattern\": \"D%\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Zookeeper Behavior in Apache Druid\nDESCRIPTION: These properties control Zookeeper behavior, such as session timeout, compression, and ACL settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.service.sessionTimeoutMs=30000\ndruid.zk.service.compress=true\ndruid.zk.service.acl=false\n```\n\n----------------------------------------\n\nTITLE: Configuring IngestSegmentFirehose in Apache Druid\nDESCRIPTION: This snippet shows the configuration for an IngestSegmentFirehose, used to read data from existing Druid segments. It specifies the data source and time interval for ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/firehose.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"ingestSegment\",\n    \"dataSource\"   : \"wikipedia\",\n    \"interval\" : \"2013-01-01/2013-01-02\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Lexicographic Bound Filter in Druid JSON\nDESCRIPTION: Shows how to define a bound filter using lexicographic ordering to filter values between 'foo' and 'hoo' for the 'name' dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data\nDESCRIPTION: Command to extract the compressed sample Wikipedia data file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial\ngunzip -k wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Filtering on ISO 8601 Time Intervals in Apache Druid\nDESCRIPTION: This example shows how to use the interval filter to select data from specific time ranges using ISO 8601 interval notation, targeting the __time column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN for EMR with Druid\nDESCRIPTION: YARN configuration properties for setting up a long-running EMR cluster to work with Druid. Includes memory settings, Java options, and timeout configurations for mapreduce jobs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nclassification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000]\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Certificate Authentication in Apache Druid\nDESCRIPTION: This configuration table describes optional parameters for setting up client certificate authentication in Apache Druid. It includes settings for specifying the client keystore, certificate alias, and related passwords.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/simple-client-sslcontext.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.client.https.keyStorePath`|The file path or URL of the TLS/SSL Key store containing the client certificate that Druid will use when communicating with other Druid services. If this is null, the other properties in this table are ignored.|none|yes|\n|`druid.client.https.keyStoreType`|The type of the key store.|none|yes|\n|`druid.client.https.certAlias`|Alias of TLS client certificate in the keystore.|none|yes|\n|`druid.client.https.keyStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Store.|none|no|\n|`druid.client.https.keyManagerFactoryAlgorithm`|Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).|`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.client.https.keyManagerPassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Manager.|none|no|\n|`druid.client.https.validateHostnames`|Validate the hostname of the server. This should not be disabled unless you are using [custom TLS certificate checks](../../operations/tls-support.html#custom-tls-certificate-checks) and know that standard hostname validation is not needed.|true|no|\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Users per Row using Post-Aggregations in Druid Query JSON\nDESCRIPTION: Demonstrates a complex query using aggregations and post-aggregations to calculate the average number of unique users per row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/post-aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n  \"aggregations\" : [{\n    {\"type\" : \"count\", \"name\" : \"rows\"},\n    {\"type\" : \"hyperUnique\", \"name\" : \"unique_users\", \"fieldName\" : \"uniques\"}\n  }],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average_users_per_row\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n      { \"type\" : \"hyperUniqueCardinality\", \"fieldName\" : \"unique_users\" },\n      { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n    ]\n  }]\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticGoogleBlobStoreFirehose in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticGoogleBlobStoreFirehose for ingesting data from Google Cloud Storage. It specifies the firehose type and a list of blobs to ingest from different buckets and paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/google.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-google-blobstore\",\n    \"blobs\": [\n        {\n          \"bucket\": \"foo\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"bucket\": \"bar\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Druid Overlord Tasks\nDESCRIPTION: GET, POST, and DELETE requests to manage and retrieve information about Druid tasks. Includes endpoints for task lists, task details, task status, and task shutdown.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_13\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/indexer/v1/tasks\nGET /druid/indexer/v1/completeTasks\nGET /druid/indexer/v1/runningTasks\nGET /druid/indexer/v1/waitingTasks\nGET /druid/indexer/v1/pendingTasks\nGET /druid/indexer/v1/task/{taskId}\nGET /druid/indexer/v1/task/{taskId}/status\nGET /druid/indexer/v1/task/{taskId}/segments\nGET /druid/indexer/v1/task/{taskId}/reports\nPOST /druid/indexer/v1/task\nPOST /druid/indexer/v1/task/{taskId}/shutdown\nPOST /druid/indexer/v1/datasources/{dataSource}/shutdownAllTasks\nPOST /druid/indexer/v1/taskStatus\nDELETE /druid/indexer/v1/pendingSegments/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: Configuring Jetty Server TLS Properties in Apache Druid\nDESCRIPTION: Sets up TLS/SSL configuration for the embedded Jetty web server in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.https.keyStorePath=none\ndruid.server.https.keyStoreType=none\ndruid.server.https.certAlias=none\ndruid.server.https.keyStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Mount Storage in YAML for Apache Druid\nDESCRIPTION: This YAML configuration snippet sets up local mount storage for Apache Druid. It specifies the storage type as 'local' and defines the directory for storing segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type: local\ndruid.storage.storageDirectory: /path/to/storage\n```\n\n----------------------------------------\n\nTITLE: Displaying GroupBy v1 Runtime Properties Table in Markdown\nDESCRIPTION: This snippet shows a markdown table listing the supported runtime properties for GroupBy v1 queries in Apache Druid. It includes property names, descriptions, and default values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_58\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxIntermediateRows`|Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail.|50000|\n|`druid.query.groupBy.maxResults`|Maximum number of results. Queries that exceed this limit will fail.|500000|\n```\n\n----------------------------------------\n\nTITLE: Sample Kafka Supervisor Specification in JSON\nDESCRIPTION: Comprehensive example of a Kafka supervisor specification for Druid, including dataSchema, tuningConfig, and ioConfig sections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchToMeans Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchToMeans post-aggregator. This returns a list of mean values from a given ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToMeans\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: HDFS Deep Storage Configuration\nDESCRIPTION: Example configuration settings for using HDFS as Druid's deep storage system. Shows required property settings for storage and indexing service logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-hdfs-storage\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose in Apache Druid\nDESCRIPTION: This snippet demonstrates the configuration of an HttpFirehose for reading data from remote sites via HTTP. It includes URIs for data sources and optional authentication settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/firehose.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"http\",\n    \"uris\"  : [\"http://example.com/uri1\", \"http://example2.com/uri2\"]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"http\",\n    \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"],\n    \"httpAuthenticationUsername\": \"username\",\n    \"httpAuthenticationPassword\": \"password123\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"http\",\n    \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"],\n    \"httpAuthenticationUsername\": \"username\",\n    \"httpAuthenticationPassword\": {\n        \"type\": \"environment\",\n        \"variable\": \"HTTP_FIREHOSE_PW\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Filter in Transform Spec\nDESCRIPTION: An example of a selector filter in a transform spec that only ingests rows where the 'country' column has the value 'United States'. This filter is applied after any transforms.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/transform-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"country\",\n  \"value\": \"United States\"\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing a Timeseries Query in Apache Druid\nDESCRIPTION: This JSON object demonstrates the structure of a timeseries query in Apache Druid. It includes various query parameters such as dataSource, granularity, filter, aggregations, and postAggregations. The query is designed to retrieve data from a sample datasource over a specific time interval, applying filters and aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/timeseriesquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"descending\": \"true\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" },\n      { \"type\": \"or\",\n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" },\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"sample_divide\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" },\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DataSchema for Wikipedia Data Ingestion in Apache Druid\nDESCRIPTION: This snippet demonstrates a complete dataSchema configuration for ingesting Wikipedia data into Apache Druid. It includes specifications for the data source, parser, metrics, granularity, and optional transform. The parser is set to handle JSON data with specific timestamp and dimension configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"wikipedia\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"column\" : \"timestamp\",\n        \"format\" : \"auto\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"page\",\n          \"language\",\n          \"user\",\n          \"unpatrolled\",\n          \"newPage\",\n          \"robot\",\n          \"anonymous\",\n          \"namespace\",\n          \"continent\",\n          \"country\",\n          \"region\",\n          \"city\",\n          {\n            \"type\": \"long\",\n            \"name\": \"countryNum\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLatitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"userLongitude\"\n          }\n        ],\n        \"dimensionExclusions\" : [],\n        \"spatialDimensions\" : []\n      }\n    }\n  },\n  \"metricsSpec\" : [{\n    \"type\" : \"count\",\n    \"name\" : \"count\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"added\",\n    \"fieldName\" : \"added\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"deleted\",\n    \"fieldName\" : \"deleted\"\n  }, {\n    \"type\" : \"doubleSum\",\n    \"name\" : \"delta\",\n    \"fieldName\" : \"delta\"\n  }],\n  \"granularitySpec\" : {\n    \"segmentGranularity\" : \"DAY\",\n    \"queryGranularity\" : \"NONE\",\n    \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n  },\n  \"transformSpec\" : null\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Processing in Apache Druid Historical Processes\nDESCRIPTION: This snippet shows configuration options for processing in Historical processes, including buffer sizes, thread counts, and memory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.processing.buffer.sizeBytes`|This specifies a buffer size for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.|auto (max 1GB)|\n|`druid.processing.buffer.poolCacheMaxCount`|processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.|Integer.MAX_VALUE|\n|`druid.processing.formatString`|Realtime and Historical processes use this format string to name their processing threads.|processing-%s|\n|`druid.processing.numMergeBuffers`|The number of direct memory buffers available for merging query results. The buffers are sized by `druid.processing.buffer.sizeBytes`. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.|`max(2, druid.processing.numThreads / 4)`|\n|`druid.processing.numThreads`|The number of processing threads to have available for parallel processing of segments. Our rule of thumb is `num_cores - 1`, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value `1`.|Number of cores - 1 (or 1)|\n|`druid.processing.columnCache.sizeBytes`|Maximum size in bytes for the dimension value lookup cache. Any value greater than `0` enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.|`0` (disabled)|\n|`druid.processing.fifo`|If the processing queue should treat tasks of equal priority in a FIFO manner|`false`|\n|`druid.processing.tmpDir`|Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default `java.io.tmpdir` path.|path represented by `java.io.tmpdir`|\n```\n\n----------------------------------------\n\nTITLE: Configuring SqlFirehose in Apache Druid\nDESCRIPTION: This snippet demonstrates the configuration for a SqlFirehose, which is used to ingest events from RDBMS. It specifies the database connection details and SQL queries for data retrieval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/firehose.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"sql\",\n    \"database\": {\n        \"type\": \"mysql\",\n        \"connectorConfig\" : {\n        \"connectURI\" : \"jdbc:mysql://host:port/schema\",\n        \"user\" : \"user\",\n        \"password\" : \"password\"\n        }\n     },\n    \"sqls\" : [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Format Extraction Function in Druid\nDESCRIPTION: Formats dimension values according to specified date-time format, time zone, and locale. Supports various options including custom formatting, localization, and granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"timeFormat\",\n  \"format\" : <output_format> (optional),\n  \"timeZone\" : <time_zone> (optional, default UTC),\n  \"locale\" : <locale> (optional, default current locale),\n  \"granularity\" : <granularity> (optional, default none) },\n  \"asMillis\" : <true or false> (optional) }\n```\n\n----------------------------------------\n\nTITLE: Implementing Age Range Filter in Druid\nDESCRIPTION: Shows how to create a bound filter for filtering records where age is between 21 and 31 using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample CompactedList Format Result from Apache Druid Scan Query\nDESCRIPTION: This example shows the result format when 'resultFormat' is set to 'compactedList'. The response includes the segment ID, column names, and event data with each event represented as an array of values corresponding to the columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/scan-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\"\n    ],\n    \"events\" : [\n     [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._243\", \"en\", \"1\", \"MZMcBride\", 1.0, 77.0, 77.0, 77.0, 0.0]\n    ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Running Command Line Hadoop Indexer in Bash\nDESCRIPTION: Command to execute the Hadoop indexer from the command line. It sets Java options for memory, timezone, and file encoding, specifies the classpath, and runs the main class with required arguments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>\n```\n\n----------------------------------------\n\nTITLE: Calculating Rollup Ratio in Druid SQL\nDESCRIPTION: This SQL query calculates the rollup ratio of a Druid datasource by comparing the sum of event counts to the total number of rows. It uses decimal division to get a precise ratio.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- \"* 1.0\" so we get decimal rather than integer division\nSELECT SUM(\"event_count\") / COUNT(*) * 1.0 FROM datasource\n```\n\n----------------------------------------\n\nTITLE: Setting up LongMax Aggregator in Apache Druid\nDESCRIPTION: Demonstrates the configuration for a longMax aggregator in Druid. This aggregator computes the maximum of all metric values and Long.MIN_VALUE.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch to String Post-Aggregator\nDESCRIPTION: Post-aggregator that generates a human-readable summary of an ArrayOfDoublesSketch by calling the sketch's toString() method. Useful for debugging sketch state and contents.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchToMeans Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchToMeans post-aggregator. This returns a list of mean values from a given ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToMeans\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Upper Case Extraction Function with Locale in Druid\nDESCRIPTION: Configuration for converting dimension values to upper case with specific locale settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"upper\",\n  \"locale\":\"fr\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Uniform Granularity Spec in Druid Ingestion\nDESCRIPTION: This JSON snippet defines the structure of a uniform granularity spec in Druid. It specifies fields for segmentGranularity, queryGranularity, rollup, and intervals. This configuration is used to generate segments with uniform time intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"segmentGranularity\": \"string\",\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": [\"JSON string array\"]\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Filter Example in Druid\nDESCRIPTION: JavaScript-based filter that matches dimension values between 'bar' and 'foo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : \"name\",\n  \"function\" : \"function(x) { return(x >= 'bar' && x <= 'foo') }\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter in Druid Queries\nDESCRIPTION: JSON structure for using a Bloom filter in Druid queries, specifying the filter type, dimension, serialized Bloom filter, and optional extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bloom\",\n  \"dimension\" : <dimension_name>,\n  \"bloomKFilter\" : <serialized_bytes_for_BloomKFilter>,\n  \"extractionFn\" : <extraction_fn>\n}\n```\n\n----------------------------------------\n\nTITLE: Bloom Filter Expression Syntax\nDESCRIPTION: Syntax for using bloom filters in Druid expressions. This follows the same pattern as the SQL operator, taking an expression and a serialized bloom filter string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nbloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')\n```\n\n----------------------------------------\n\nTITLE: Configuring OrderByColumnSpec for Sorting in Druid groupBy Queries\nDESCRIPTION: This JSON structure defines an OrderByColumnSpec, which specifies how to perform order-by operations on dimensions or metrics in Druid groupBy queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/limitspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dimension\" : \"<Any dimension or metric name>\",\n    \"direction\" : <\"ascending\"|\"descending\">,\n    \"dimensionOrder\" : <\"lexicographic\"(default)|\"alphanumeric\"|\"strlen\"|\"numeric\">\n}\n```\n\n----------------------------------------\n\nTITLE: Prefixing Child Peon Configurations in MiddleManager\nDESCRIPTION: Demonstrates the prefix syntax used to set explicit child peon configurations in the MiddleManager. This pattern allows passing specific configurations from the MiddleManager to its child Peon processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_33\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.fork.property\n```\n\n----------------------------------------\n\nTITLE: Custom Buckets Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for creating histogram representation with custom bucket breaks. Allows specification of exact break points for histogram bins.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"customBuckets\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"breaks\" : [ <value>, <value>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Python Kafka Producer for Protobuf Messages\nDESCRIPTION: Python script that reads JSON data from stdin, converts it to Protobuf format, and publishes it to a Kafka topic. It uses the kafka-python and protobuf libraries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\nimport json\n\nfrom kafka import KafkaProducer\nfrom metrics_pb2 import Metrics\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ntopic = 'metrics_pb'\nmetrics = Metrics()\n\nfor row in iter(sys.stdin):\n    d = json.loads(row)\n    for k, v in d.items():\n        setattr(metrics, k, v)\n    pb = metrics.SerializeToString()\n    producer.send(topic, pb)\n```\n\n----------------------------------------\n\nTITLE: List Filtered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering specific values within multi-value dimensions using whitelist or blacklist functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"listFiltered\", \"delegate\" : <dimensionSpec>, \"values\": <array of strings>, \"isWhitelist\": <optional attribute for true/false, default is true> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Hybrid Cache Properties in Druid\nDESCRIPTION: Configuration properties for setting up a two-level L1/L2 hybrid cache in Druid, which combines two cache implementations to improve performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_39\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.l1.type=caffeine\ndruid.cache.l2.type=caffeine\ndruid.cache.l1.sizeInBytes=100000000\ndruid.cache.l2.*=...\ndruid.cache.useL2=true\ndruid.cache.populateL2=true\n```\n\n----------------------------------------\n\nTITLE: URI Prefix Lookup Configuration in Apache Druid (JSON)\nDESCRIPTION: This snippet shows how to configure a URI prefix-based lookup in Apache Druid. It uses a URI prefix and file regex to locate lookup data, and specifies parsing format and polling period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uriPrefix\": \"s3://bucket/some/key/prefix/\",\n  \"fileRegex\":\"renames-[0-9]*\\\\.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookup DimensionSpec with External Lookup in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Lookup DimensionSpec using an external lookup table in Apache Druid. It references a pre-registered lookup table by name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"name\":\"lookupName\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Substring Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Substring Extraction Function in Apache Druid. It returns a substring of the dimension value based on the specified index and length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 1, \"length\" : 4 }\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid to Use MySQL Metadata Storage\nDESCRIPTION: Properties configuration for connecting Druid to a MySQL metadata store, including loading the extension and setting up the connection parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"mysql-metadata-storage\"]\ndruid.metadata.storage.type=mysql\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=druid\n```\n\n----------------------------------------\n\nTITLE: Implementing TopN Query with DistinctCount in Druid\nDESCRIPTION: Example of a TopN query using distinctCount aggregator to find top 5 dimensions based on unique visitor counts. The query calculates unique visitors for a single day period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"uv\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced TLS Settings in Apache Druid\nDESCRIPTION: Advanced configuration options for TLS/SSL in Apache Druid, including key manager settings and cipher suite configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/tls-support.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.server.https.keyManagerFactoryAlgorithm`|Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).|`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.server.https.keyManagerPassword`|The [Password Provider](../operations/password-provider.html) or String password for the Key Manager.|none|no|\n|`druid.server.https.includeCipherSuites`|List of cipher suite names to include. You can either use the exact cipher suite name or a regular expression.|Jetty's default include cipher list|no|\n|`druid.server.https.excludeCipherSuites`|List of cipher suite names to exclude. You can either use the exact cipher suite name or a regular expression.|Jetty's default exclude cipher list|no|\n|`druid.server.https.includeProtocols`|List of exact protocols names to include.|Jetty's default include protocol list|no|\n|`druid.server.https.excludeProtocols`|List of exact protocols names to exclude.|Jetty's default exclude protocol list|no|\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Redundancy in Druid\nDESCRIPTION: This snippet shows how to configure the shardSpec for redundancy in Druid realtime processes. It demonstrates setting up two processes with the same partitionNum to create redundant data storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"linear\",\n    \"partitionNum\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Regular Expression Filter in Apache Druid JSON\nDESCRIPTION: Illustrates the JSON structure for a regular expression filter in Apache Druid. This filter matches a dimension against a Java regular expression pattern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n\"filter\": { \"type\": \"regex\", \"dimension\": <dimension_string>, \"pattern\": <pattern_string> }\n```\n\n----------------------------------------\n\nTITLE: Generating Summary of ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to generate a human-readable summary of an ArrayOfDoublesSketch for debugging purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV Lookup in Druid\nDESCRIPTION: Example configuration for a TSV lookup in Druid using namespaceParseSpec. It defines columns, key column, value column, and a custom delimiter for parsing TSV data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"tsv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\",\n  \"delimiter\": \"|\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Age Range Filter in Druid\nDESCRIPTION: Shows how to create a bound filter for filtering records where age is between 21 and 31 using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Terminating a Supervisor in Druid\nDESCRIPTION: Use this POST endpoint to terminate a Kafka supervisor and instruct all its managed tasks to stop and publish their segments immediately. The supervisor will remain in metadata but cannot be restarted without submitting a new spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/terminate\n```\n\n----------------------------------------\n\nTITLE: Configuring Substring Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Substring Extraction Function in Apache Druid. It returns a substring of the dimension value based on the specified index and length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_10\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 1, \"length\" : 4 }\n```\n\n----------------------------------------\n\nTITLE: Configuring ThetaSketch Estimate Post-Aggregator\nDESCRIPTION: Shows how to define a thetaSketchEstimate post-aggregator to estimate the cardinality of a Theta sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Lookup Configuration Example for Apache Druid\nDESCRIPTION: Complete example of configuring multiple lookups across different tiers. Includes examples of map lookups and cached JDBC lookups with various configurations for different customer segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__default\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupTable\",\n          \"keyColumn\": \"country_id\",\n          \"valueColumn\": \"country_name\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  },\n  \"realtime_customer1\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    }\n  },\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Worker Selection Strategy Example for Druid Overlord\nDESCRIPTION: A JavaScript function that implements custom worker selection logic for Druid tasks. This example directs batch_index_task to specific workers while sending all other tasks to different available workers based on capacity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_37\n\nLANGUAGE: javascript\nCODE:\n```\n{\n\"type\":\"javascript\",\n\"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"middleManager1_hostname:8091\\\");\\nbatch_workers.add(\\\"middleManager2_hostname:8091\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i < workers.length; i++){\\n sortedWorkers[i] = workers[i];\\n}\\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\\nvar minWorkerVer = config.getMinWorkerVersion();\\nfor (var i = 0; i < sortedWorkers.length; i++) {\\n var worker = sortedWorkers[i];\\n  var zkWorker = zkWorkers.get(worker);\\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\\n      return worker;\\n    } else {\\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\\n        return worker;\\n      }\\n    }\\n  }\\n}\\nreturn null;\\n}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Cache in Druid\nDESCRIPTION: Configuration properties for using Memcached as a cache backend in Druid. Settings include expiration time, timeout, host configuration, object size limits, and connection parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_52\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.expiration=2592000\ndruid.cache.timeout=500\ndruid.cache.hosts=none\ndruid.cache.maxObjectSize=52428800\ndruid.cache.memcachedPrefix=druid\ndruid.cache.numConnections=1\ndruid.cache.protocol=binary\ndruid.cache.locator=consistent\n```\n\n----------------------------------------\n\nTITLE: Example Segment Metadata JSON in Druid\nDESCRIPTION: Sample JSON structure representing segment metadata stored in the 'payload' column of the segments table. Includes details like data source, interval, version, and load specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"dataSource\":\"wikipedia\",\n \"interval\":\"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z\",\n \"version\":\"2012-05-24T00:10:00.046Z\",\n \"loadSpec\":{\n    \"type\":\"s3_zip\",\n    \"bucket\":\"bucket_for_segment\",\n    \"key\":\"path/to/segment/on/s3\"\n },\n \"dimensions\":\"comma-delimited-list-of-dimension-names\",\n \"metrics\":\"comma-delimited-list-of-metric-names\",\n \"shardSpec\":{\"type\":\"none\"},\n \"binaryVersion\":9,\n \"size\":size_of_segment,\n \"identifier\":\"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Aggregated Data After Rollup in Markdown\nDESCRIPTION: Shows a markdown table representing the aggregated data after Druid's rollup operation. The data is grouped by minute, with summed packet and byte counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/index.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000\n2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000\n2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Prefix Filtered DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Prefix Filtered DimensionSpec in Apache Druid. It retains only the values starting with the specified prefix in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"prefixFiltered\", \"delegate\" : <dimensionSpec>, \"prefix\": <prefix string> }\n```\n\n----------------------------------------\n\nTITLE: Enabling MiddleManager Response in JSON\nDESCRIPTION: This JSON response confirms that a MiddleManager has been enabled. The key is the combined druid.host and druid.port, with 'enabled' as the value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"enabled\"}\n```\n\n----------------------------------------\n\nTITLE: Displaying Aggregated Data After Rollup in Markdown\nDESCRIPTION: Shows a markdown table representing the aggregated data after Druid's rollup operation. The data is grouped by minute, with summed packet and byte counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/index.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000\n2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000\n2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Lower Case Extraction Function in Druid\nDESCRIPTION: Basic configuration for converting dimension values to lower case using system default locale.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"lower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring List Filtered DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a List Filtered DimensionSpec in Apache Druid. It acts as a whitelist or blacklist for values in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"listFiltered\", \"delegate\" : <dimensionSpec>, \"values\": <array of strings>, \"isWhitelist\": <optional attribute for true/false, default is true> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookups in Bulk for Apache Druid\nDESCRIPTION: JSON structure for bulk configuration of lookups across multiple tiers in Druid. This example shows the format for updating lookups at the Coordinator endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"<tierName>\": {\n        \"<lookupName>\": {\n          \"version\": \"<version>\",\n          \"lookupExtractorFactory\": {\n            \"type\": \"<someExtractorFactoryType>\",\n            \"<someExtractorField>\": \"<someExtractorValue>\"\n          }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Compaction Configuration API Endpoints\nDESCRIPTION: HTTP GET, POST, and DELETE endpoints for managing compaction configurations\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_6\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/config/compaction\nGET /druid/coordinator/v1/config/compaction/{dataSource}\nPOST /druid/coordinator/v1/config/compaction/taskslots?ratio={someRatio}&max={someMaxSlots}\nPOST /druid/coordinator/v1/config/compaction\nDELETE /druid/coordinator/v1/config/compaction/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: Moving Average Query Result for Wikipedia Edits\nDESCRIPTION: The result of the moving average query showing delta30Min (raw 30-minute edit counts), trailing30MinChanges (7-bucket moving average), and ratioTrailing30MinChanges (ratio between current period and moving average) for Wikipedia edits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T22:00:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 144269,\n    \"trailing30MinChanges\" : 204088.14285714287,\n    \"ratioTrailing30MinChanges\" : 0.7068955500319539\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T22:30:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 242860,\n    \"trailing30MinChanges\" : 214031.57142857142,\n    \"ratioTrailing30MinChanges\" : 1.134692411867141\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:00:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 119100,\n    \"trailing30MinChanges\" : 198697.2857142857,\n    \"ratioTrailing30MinChanges\" : 0.5994042624782422\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:30:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 177882,\n    \"trailing30MinChanges\" : 193890.0,\n    \"ratioTrailing30MinChanges\" : 0.9174377224199288\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Strlen Extraction Function in Druid\nDESCRIPTION: The Strlen extraction function returns the length of dimension values, measuring the number of Unicode code units present in the string as encoded in UTF-16. Null strings are treated as having zero length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"strlen\" }\n```\n\n----------------------------------------\n\nTITLE: Implementing floatFirst Aggregator in Druid Queries\nDESCRIPTION: The floatFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. This can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Lookup Configuration Example in Druid\nDESCRIPTION: A complete example showing multiple lookup configurations across different tiers including default lookups, customer-specific lookups, and different lookup types like map and jdbc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__default\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupTable\",\n          \"keyColumn\": \"country_id\",\n          \"valueColumn\": \"country_name\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  },\n  \"realtime_customer1\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    }\n  },\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Dimension Column Data Structures in Druid Segments\nDESCRIPTION: This example shows the three data structures used for dimension columns in Druid: a dictionary mapping values to integer IDs, the encoded column data, and bitmaps for each unique value indicating which rows contain that value. These structures enable efficient filtering and group-by operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/segments.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   0,\n   1,\n   1]\n\n3: Bitmaps - one for each unique value of the column\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,0,1,1]\n```\n\n----------------------------------------\n\nTITLE: Constructing a groupBy Query in Apache Druid\nDESCRIPTION: Example of a groupBy query object in Apache Druid, demonstrating various query components including datasource, dimensions, filters, aggregations, and post-aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"dimensions\": [\"country\", \"device\"],\n  \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] },\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" },\n      { \"type\": \"or\", \n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" },\n          { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" },\n    { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"avg_usage\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" },\n        { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"having\": {\n    \"type\": \"greaterThan\",\n    \"aggregation\": \"total_usage\",\n    \"value\": 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License Header\nDESCRIPTION: Copyright notice and MIT license declaration for Prism, a syntax highlighting library created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Defining Library Dependencies and Assembly Merge Strategy in SBT for Druid Project\nDESCRIPTION: This snippet defines the library dependencies for an Apache Druid project, including Druid core and extension modules, AWS SDK, Jackson libraries, and testing dependencies. It also specifies exclusion rules to avoid conflicts. Additionally, it defines an assembly merge strategy to handle conflicting files during the assembly process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/use_sbt_to_build_fat_jar.md#2025-04-09_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.9.23\" exclude(\"common-logging\", \"common-logging\"),\n  \"org.joda\" % \"joda-convert\" % \"1.7\",\n  \"joda-time\" % \"joda-time\" % \"2.7\",\n  \"org.apache.druid\" % \"druid\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-services\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-service\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-hadoop\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"mysql-metadata-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-s3-extensions\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-histogram\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-hdfs-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-guava\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-joda\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-base\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-json-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-smile-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-jaxb-annotations\" % \"2.3.0\",\n  \"com.sun.jersey\" % \"jersey-servlet\" % \"1.17.1\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.34\",\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.3\" % \"test\",\n  \"org.mockito\" % \"mockito-core\" % \"1.10.19\" % \"test\"\n)\n\nassemblyMergeStrategy in assembly := {\n  case path if path contains \"pom.\" => MergeStrategy.first\n  case path if path contains \"javax.inject.Named\" => MergeStrategy.first\n  case path if path contains \"mime.types\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog$1.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/NoOpLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogFactory.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogConfigurationException.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/Log.class\" => MergeStrategy.first\n  case path if path contains \"META-INF/jersey-module-version\" => MergeStrategy.first\n  case path if path contains \".properties\" => MergeStrategy.first\n  case path if path contains \".class\" => MergeStrategy.first\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n```\n\n----------------------------------------\n\nTITLE: External Lookup DimensionSpec in Druid\nDESCRIPTION: Configuration for lookup dimension specification using an external lookup table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"name\":\"lookupName\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Expression Transform in Apache Druid JSON Configuration\nDESCRIPTION: An example of an expression transform that prepends \"foo\" to the values of a page column, creating a new column called \"fooPage\". Uses the concat function to combine strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Constructing a Timeseries Query in Druid\nDESCRIPTION: A complete example of a timeseries query in Druid showing the main query components including queryType, dataSource, granularity, filters, aggregations, postAggregations, and intervals. This query retrieves daily aggregated data with specified filters and calculations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"descending\": \"true\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" },\n      { \"type\": \"or\",\n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" },\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"sample_divide\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" },\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Cache Locations in Druid Historical\nDESCRIPTION: Example configuration for defining where Historical processes store segment cache data on the local filesystem. The configuration includes path, maximum size, and optional free space percentage requirement.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\"path\": \"/mnt/druidSegments\", \"maxSize\": 10000, \"freeSpacePercent\": 1.0}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid PostgreSQL Extension\nDESCRIPTION: Properties configuration for connecting Druid to PostgreSQL metadata storage, including extension loading and database connection parameters\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\"]\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Lookup in Druid\nDESCRIPTION: Defines a JDBC lookup configuration that polls a database table to populate its local cache. Includes connection settings, table and column specifications, and polling period configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"jdbc\",\n  \"namespace\":\"some_lookup\",\n  \"connectorConfig\":{\n    \"createTables\":true,\n    \"connectURI\":\"jdbc:mysql://localhost:3306/druid\",\n    \"user\":\"druid\",\n    \"password\":\"diurd\"\n  },\n  \"table\":\"some_lookup_table\",\n  \"keyColumn\":\"the_old_dim_value\",\n  \"valueColumn\":\"the_new_dim_value\",\n  \"tsColumn\":\"timestamp_column\",\n  \"pollPeriod\":600000\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalFirehose in Apache Druid\nDESCRIPTION: LocalFirehose configuration for reading data from files on local disk. This firehose is splittable and can be used by native parallel index tasks, where each worker task reads a file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"local\",\n    \"filter\"   : \"*.csv\",\n    \"baseDir\"  : \"/data/directory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Segment Metrics by Datasource in Druid SQL\nDESCRIPTION: SQL query to calculate total size, average size, average number of rows, and count of segments grouped by datasource, ordered by total size descending.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    datasource,\n    SUM(\"size\") AS total_size,\n    CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size,\n    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,\n    COUNT(*) AS num_segments\nFROM sys.segments\nGROUP BY 1\nORDER BY 2 DESC\n```\n\n----------------------------------------\n\nTITLE: Defining Library Dependencies and Assembly Merge Strategy in SBT for Druid Project\nDESCRIPTION: This snippet defines the library dependencies for an Apache Druid project, including Druid core and extension modules, AWS SDK, Jackson libraries, and testing dependencies. It also specifies exclusion rules to avoid conflicts. Additionally, it defines an assembly merge strategy to handle conflicting files during the assembly process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/use_sbt_to_build_fat_jar.md#2025-04-09_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.9.23\" exclude(\"common-logging\", \"common-logging\"),\n  \"org.joda\" % \"joda-convert\" % \"1.7\",\n  \"joda-time\" % \"joda-time\" % \"2.7\",\n  \"org.apache.druid\" % \"druid\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-services\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-service\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-hadoop\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"mysql-metadata-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-s3-extensions\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-histogram\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-hdfs-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-guava\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-joda\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-base\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-json-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-smile-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-jaxb-annotations\" % \"2.3.0\",\n  \"com.sun.jersey\" % \"jersey-servlet\" % \"1.17.1\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.34\",\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.3\" % \"test\",\n  \"org.mockito\" % \"mockito-core\" % \"1.10.19\" % \"test\"\n)\n\nassemblyMergeStrategy in assembly := {\n  case path if path contains \"pom.\" => MergeStrategy.first\n  case path if path contains \"javax.inject.Named\" => MergeStrategy.first\n  case path if path contains \"mime.types\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog$1.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/NoOpLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogFactory.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogConfigurationException.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/Log.class\" => MergeStrategy.first\n  case path if path contains \"META-INF/jersey-module-version\" => MergeStrategy.first\n  case path if path contains \".properties\" => MergeStrategy.first\n  case path if path contains \".class\" => MergeStrategy.first\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Log4j2 XML for Apache Druid\nDESCRIPTION: Example log4j2.xml configuration file that sets up console logging with timestamp formatting and configurable log levels. Includes a commented section for enabling HTTP request logging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/logging.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n\n    <!-- Uncomment to enable logging of all HTTP requests\n    <Logger name=\"org.apache.druid.jetty.RequestLog\" additivity=\"false\" level=\"DEBUG\">\n        <AppenderRef ref=\"Console\"/>\n    </Logger>\n    -->\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleMax Aggregator in Apache Druid\nDESCRIPTION: Illustrates the configuration for a doubleMax aggregator in Druid. This aggregator computes the maximum of all metric values and Double.NEGATIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Defining Bitmap Type Configurations\nDESCRIPTION: Tables documenting the configuration options for Concise and Roaring bitmap types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|Must be `concise`.|yes|\n\nFor Roaring bitmaps:\n\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|Must be `roaring`.|yes|\n|`compressRunOnSerialization`|Boolean|Use a run-length encoding where it is estimated as more space efficient.|no (default == `true`)|\n```\n\n----------------------------------------\n\nTITLE: Configuring RegexFiltered DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a RegexFiltered DimensionSpec in Apache Druid. It retains only the values matching the specified regex pattern in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"regexFiltered\", \"delegate\" : <dimensionSpec>, \"pattern\": <java regex pattern> }\n```\n\n----------------------------------------\n\nTITLE: Druid TopN Query Example with Variance and Standard Deviation\nDESCRIPTION: Complete example of a Druid TopN query that calculates both variance and standard deviation of a metric, showing how to combine the variance aggregator with the stddev post-aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"threshold\": 5,\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Registering DruidModule Implementation Path\nDESCRIPTION: Example of the required META-INF/services file content that registers a custom Druid module implementation. This file should be included in the module's JAR to enable Druid to discover and load the module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\norg.apache.druid.storage.cassandra.CassandraDruidModule\n```\n\n----------------------------------------\n\nTITLE: Using curl for SQL Queries in Druid\nDESCRIPTION: Example showing how to execute a SQL query against Druid using curl with JSON over HTTP. This demonstrates sending a simple count query to the Druid broker endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ cat query.json\n{\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"}\n\n$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json\n[{\"TheCount\":24433}]\n```\n\n----------------------------------------\n\nTITLE: Aggregation Function Syntax for Druid SQL\nDESCRIPTION: Sample syntax for using aggregation functions in Druid SQL. These functions can appear in the SELECT clause of any query and can optionally include FILTER clauses to selectively aggregate data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nAGG(expr) FILTER(WHERE whereExpr)\n```\n\n----------------------------------------\n\nTITLE: Enabling DataSketches Extension in Druid Configuration\nDESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It adds the 'druid-datasketches' extension to the loadList.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-extension.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Client TLS in Apache Druid\nDESCRIPTION: These properties configure TLS for the internal HTTP client used by Druid services to communicate with each other.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\ndruid.client.https.protocol=TLSv1.2\ndruid.client.https.trustStoreType=java.security.KeyStore.getDefaultType()\ndruid.client.https.trustStorePath=none\ndruid.client.https.trustStoreAlgorithm=javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()\ndruid.client.https.trustStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Extensions and Modules\nDESCRIPTION: Defines configuration properties for managing Druid extensions and modules, including loading extensions, specifying dependencies, and controlling class loading behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.extensions.directory: extensions\ndruid.extensions.hadoopDependenciesDir: hadoop-dependencies\ndruid.extensions.loadList: null\ndruid.extensions.searchCurrentClassloader: true\ndruid.extensions.useExtensionClassloaderFirst: false\ndruid.extensions.hadoopContainerDruidClasspath: null\ndruid.extensions.addExtensionsToHadoopContainer: false\n\ndruid.modules.excludeList: []\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch to Estimate with Error Bounds Post Aggregator\nDESCRIPTION: Post aggregator that returns a distinct count estimate with error bounds from an ArrayOfDoublesSketch. Returns triple of values: estimate, lower bound, and upper bound at specified standard deviations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimateAndBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,\n  \"numStdDevs\", <number from 1 to 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Role Permissions in Druid Authorization API (JSON)\nDESCRIPTION: JSON payload example for the POST endpoint to set permissions for a role in Druid's basic-security authorization system. The example shows how to define READ access to all wiki datasources and WRITE access to the wikiticker datasource specifically.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n{\n  \"resource\": {\n    \"name\": \"wiki.*\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"READ\"\n},\n{\n  \"resource\": {\n    \"name\": \"wikiticker\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"WRITE\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: JavaScript Filter Example for Range Matching in Druid\nDESCRIPTION: This JavaScript filter example matches any dimension values for the dimension 'name' that fall between 'bar' and 'foo' alphabetically.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : \"name\",\n  \"function\" : \"function(x) { return(x >= 'bar' && x <= 'foo') }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Day of Week Filter with Extraction Function in Druid JSON\nDESCRIPTION: Demonstrates filtering on the day of the week using a selector filter with a time format extraction function on the '__time' column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"Friday\",\n  \"extractionFn\": {\n    \"type\": \"timeFormat\",\n    \"format\": \"EEEE\",\n    \"timeZone\": \"America/New_York\",\n    \"locale\": \"en\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Query Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Search Query Extraction Function in Apache Druid. It returns the dimension value unchanged if the SearchQuerySpec matches, otherwise returns null.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"searchQuery\", \"query\" : <search_query_spec> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Task in Druid\nDESCRIPTION: Defines a deprecated task that merges multiple segments, combining common timestamps unless rollup is disabled. Requires task ID, datasource, aggregations, rollup setting, and list of segments to merge.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"segments\": <JSON list of DataSegment objects to merge>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining OrderByColumnSpec for Result Sorting in Druid Queries\nDESCRIPTION: This JSON structure defines an OrderByColumnSpec, which specifies how to perform order-by operations in Druid queries. It includes the dimension to sort by, the sort direction, and the dimensionOrder for custom sorting behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/limitspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dimension\" : \"<Any dimension or metric name>\",\n    \"direction\" : <\"ascending\"|\"descending\">,\n    \"dimensionOrder\" : <\"lexicographic\"(default)|\"alphanumeric\"|\"strlen\"|\"numeric\">\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Segment Metrics by Datasource in Druid\nDESCRIPTION: SQL query that retrieves total size, average size, average number of rows, and total count of segments grouped by datasource. This query provides a summary view of segment distribution and size characteristics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    datasource,\n    SUM(\"size\") AS total_size,\n    CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size,\n    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,\n    COUNT(*) AS num_segments\nFROM sys.segments\nGROUP BY 1\nORDER BY 2 DESC\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalFirehose in Apache Druid\nDESCRIPTION: LocalFirehose configuration for reading data from files on local disk. This firehose is splittable and can be used by native parallel index tasks, where each worker task reads a file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"local\",\n    \"filter\"   : \"*.csv\",\n    \"baseDir\"  : \"/data/directory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CombiningFirehose in Apache Druid\nDESCRIPTION: CombiningFirehose configuration for combining and merging data from multiple different firehoses. This allows ingestion from various sources in a single task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"combining\",\n    \"delegates\" : [ { firehose1 }, { firehose2 }, ..... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring IngestSegmentFirehose in Apache Druid\nDESCRIPTION: This example shows the configuration for IngestSegmentFirehose, which reads data from existing Druid segments. It can be used to ingest existing segments with a new schema, changing dimensions, metrics, or other properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"ingestSegment\",\n    \"dataSource\"   : \"wikipedia\",\n    \"interval\" : \"2013-01-01/2013-01-02\"\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing a TopN Query in Apache Druid\nDESCRIPTION: This JSON object demonstrates the structure of a TopN query in Apache Druid. It includes various components such as queryType, dataSource, dimension, threshold, metric, granularity, filter, aggregations, postAggregations, and intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_data\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"count\",\n  \"granularity\": \"all\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim1\",\n        \"value\": \"some_value\"\n      },\n      {\n        \"type\": \"selector\",\n        \"dimension\": \"dim2\",\n        \"value\": \"some_other_val\"\n      }\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\": \"longSum\",\n      \"name\": \"count\",\n      \"fieldName\": \"count\"\n    },\n    {\n      \"type\": \"doubleSum\",\n      \"name\": \"some_metric\",\n      \"fieldName\": \"some_metric\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"arithmetic\",\n      \"name\": \"average\",\n      \"fn\": \"/\",\n      \"fields\": [\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"some_metric\",\n          \"fieldName\": \"some_metric\"\n        },\n        {\n          \"type\": \"fieldAccess\",\n          \"name\": \"count\",\n          \"fieldName\": \"count\"\n        }\n      ]\n    }\n  ],\n  \"intervals\": [\n    \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch to Estimate with Error Bounds Post Aggregator\nDESCRIPTION: Post aggregator that returns a distinct count estimate with error bounds from an ArrayOfDoublesSketch. Returns triple of values: estimate, lower bound, and upper bound at specified standard deviations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimateAndBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,\n  \"numStdDevs\", <number from 1 to 3>\n}\n```\n\n----------------------------------------\n\nTITLE: HyperUnique Aggregator Configuration\nDESCRIPTION: JSON configuration for the HyperUnique aggregator which uses HyperLogLog to compute estimated cardinality of dimensions that have been pre-aggregated during indexing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"hyperUnique\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"isInputHyperUnique\" : false,\n  \"round\" : false\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Segment Metrics by Datasource in Druid SQL\nDESCRIPTION: SQL query to calculate total size, average size, average number of rows, and count of segments grouped by datasource, ordered by total size descending.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    datasource,\n    SUM(\"size\") AS total_size,\n    CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size,\n    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,\n    COUNT(*) AS num_segments\nFROM sys.segments\nGROUP BY 1\nORDER BY 2 DESC\n```\n\n----------------------------------------\n\nTITLE: Configuring pvalue2tailedZtest Post Aggregator in Druid\nDESCRIPTION: JSON configuration for the pvalue2tailedZtest post aggregator that calculates the p-value of a two-sided z-test from a provided z-score input.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"pvalue2tailedZtest\",\n  \"name\": \"<output_name>\",\n  \"zScore\": <zscore post_aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Cache Properties in Apache Druid\nDESCRIPTION: These properties are used to configure the Redis cache for Druid. They should be added to the common.runtime.properties file. Additional cache-enabling properties need to be set separately for each process type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/redis-cache.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.host=None\ndruid.cache.port=None\ndruid.cache.expiration=24 * 3600 * 1000\ndruid.cache.timeout=2000\ndruid.cache.maxTotalConnections=8\ndruid.cache.maxIdleConnections=8\ndruid.cache.minIdleConnections=0\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffeine Cache in Druid\nDESCRIPTION: Configuration properties for Druid's Caffeine cache implementation, including size limits, expiration policies, and executor options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_24\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.type=caffeine\ndruid.cache.sizeInBytes=min(1GB, Runtime.maxMemory / 10)\ndruid.cache.expireAfter=None\ndruid.cache.cacheExecutorFactory=COMMON_FJP\ndruid.cache.evictOnClose=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Cassandra Deep Storage in Druid\nDESCRIPTION: Configuration properties for using Cassandra as deep storage in Druid. Requires the druid-cassandra-storage extension to be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.host=none\ndruid.storage.keyspace=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variable Password Provider in Druid\nDESCRIPTION: JSON configuration for environment variable-based password provider that retrieves passwords from system environment variables. This provider requires specifying the environment variable name that contains the password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/password-provider.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring SimpleJSON Lookup Parse Specification in Druid\nDESCRIPTION: Defines a namespaceParseSpec for simple JSON format lookups, which processes line-delimited JSON where each field is a key and its value is the lookup value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\": \"bar\"}\n{\"baz\": \"bat\"}\n{\"buck\": \"truck\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\":{\n  \"format\": \"simpleJson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Task Priority in Apache Druid JSON Configuration\nDESCRIPTION: This JSON snippet demonstrates how to override the default task priority by setting a custom priority value in the task context. The priority is set to 100, which is higher than the default priorities for most task types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/locking-and-priority.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"context\" : {\n    \"priority\" : 100\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Coordinator Endpoint Response for Lookup Configuration\nDESCRIPTION: The JSON response returned from the Coordinator endpoint for a specific lookup configuration. This shows how the lookup configuration is exposed via the API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v0\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n      \"type\": \"jdbc\",\n      \"connectorConfig\": {\n        \"createTables\": true,\n        \"connectURI\": \"jdbc:mysql://localhost:3306/druid\",\n        \"user\": \"druid\",\n        \"password\": \"diurd\"\n      },\n      \"table\": \"lookupValues\",\n      \"keyColumn\": \"value_id\",\n      \"valueColumn\": \"value_text\",\n      \"filter\": \"value_type='country'\",\n      \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Buckets Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for creating histogram buckets with specified bucket size and offset. Defines width of binning interval and alignment value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"buckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"bucketSize\": <bucket_size>,\n  \"offset\": <offset>\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Active Tasks on MiddleManager in JSON\nDESCRIPTION: GET request to /druid/worker/v1/tasks returns a JSON list of active task IDs being run on the MiddleManager. This endpoint is less preferred compared to the Overlord API for normal usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\"index_wikiticker_2019-02-11T02:20:15.316Z\"]\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Query Command and Results\nDESCRIPTION: SQL query to verify the transformed data in Druid, showing the modified animal names and new triple-number column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"transform-tutorial\";\n\n __time                    animal          count  location  number  triple-number \n\n 2018-01-01T05:01:00.000Z  super-mongoose      1         2     200            600 \n 2018-01-01T06:01:00.000Z  super-snake         1         3     300            900 \n 2018-01-01T07:01:00.000Z  super-octopus       1         1     100            300 \n\nRetrieved 3 rows in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Bloom Filter Test in Druid\nDESCRIPTION: Shows how to use the bloom_filter_test operator in a SQL WHERE clause for filtering Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')\n```\n\n----------------------------------------\n\nTITLE: Configuring Derby Metadata Storage for Druid\nDESCRIPTION: Basic configuration for setting up Derby as the metadata storage for Druid. This is the default option but not recommended for production use.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true\n```\n\n----------------------------------------\n\nTITLE: Configuring FloatSum Aggregator in Apache Druid\nDESCRIPTION: Demonstrates the configuration for a floatSum aggregator in Druid. This aggregator computes and stores the sum of values as a 32-bit floating point value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Deep Storage in Apache Druid\nDESCRIPTION: Configuration properties for using Amazon S3 as deep storage, including access credentials, bucket settings, and archiving options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_19\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=s3\ndruid.s3.accessKey=your_access_key\ndruid.s3.secretKey=your_secret_key\ndruid.storage.bucket=druid-data\ndruid.storage.baseKey=druid/segments\ndruid.storage.disableAcl=false\ndruid.storage.archiveBucket=druid-archive\ndruid.storage.archiveBaseKey=druid/archived-segments\ndruid.storage.useS3aSchema=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator in Druid\nDESCRIPTION: Configuration for cardinality aggregator that computes the cardinality of Druid dimensions using HyperLogLog. Supports both single and multiple dimensions, with options for byRow calculation and result rounding.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/hll-old.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"<output_name>\",\n  \"fields\": [ <dimension1>, <dimension2>, ... ],\n  \"byRow\": <false | true> # (optional, defaults to false),\n  \"round\": <false | true> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Configuration Directory Structure\nDESCRIPTION: Shows the recommended organization of Druid configuration files across different service directories including broker, coordinator, historical, middleManager and overlord configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: SQL GroupBy Query for Channel Statistics\nDESCRIPTION: Groups Wikipedia edits by channel and sums added lines. Shows usage of GROUP BY with ordering and limiting results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT channel, SUM(added) FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY channel ORDER BY SUM(added) DESC LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Configuring Expression Virtual Column in Apache Druid\nDESCRIPTION: This snippet shows the syntax for configuring an expression virtual column in Apache Druid. It includes the required fields such as type, name, and expression, as well as the optional outputType field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/virtual-columns.md#2025-04-09_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <name of the virtual column>,\n  \"expression\": <row expression>,\n  \"outputType\": <output value type of expression>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Deep Storage in Apache Druid\nDESCRIPTION: Configuration for using the local filesystem as deep storage for Druid segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_18\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=local\ndruid.storage.storageDirectory=/tmp/druid/localStorage\n```\n\n----------------------------------------\n\nTITLE: Configuring Real-time Thrift Ingestion in Tranquility\nDESCRIPTION: JSON configuration example for setting up real-time Thrift data ingestion using Tranquility. This snippet demonstrates how to specify the Thrift parser, including the Thrift class and protocol, within a Druid data schema.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSources\": [{\n    \"spec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"book\",\n        \"granularitySpec\": {          },\n        \"parser\": {\n          \"type\": \"thrift\",\n          \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n          \"protocol\": \"compact\",\n          \"parseSpec\": {\n            \"format\": \"json\",\n            ...\n          }\n        },\n        \"metricsSpec\": [...]\n      },\n      \"tuningConfig\": {...}\n    },\n    \"properties\": {...}\n  }],\n  \"properties\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchMerge Aggregator for Combining HLL Sketches\nDESCRIPTION: JSON configuration for the HLLSketchMerge aggregator which merges pre-existing HLL sketches during query time. This is used when the data already contains serialized HLL sketches that need to be combined.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchMerge\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Buckets Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for creating histogram buckets with specified bucket size and offset. Defines width of binning interval and alignment value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"buckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"bucketSize\": <bucket_size>,\n  \"offset\": <offset>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchToString Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchToString post-aggregator. This returns a human-readable summary of a given ArrayOfDoublesSketch, useful for debugging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Live Row Statistics in Apache Druid\nDESCRIPTION: API endpoint to retrieve live row statistics from a running ingestion task, providing moving averages and totals for processed rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/reports.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/rowStats\n```\n\n----------------------------------------\n\nTITLE: Python Kafka Producer for Protobuf Messages\nDESCRIPTION: Python script that reads JSON data from stdin and publishes it to Kafka as Protobuf messages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\nimport json\n\nfrom kafka import KafkaProducer\nfrom metrics_pb2 import Metrics\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ntopic = 'metrics_pb'\nmetrics = Metrics()\n\nfor row in iter(sys.stdin):\n    d = json.loads(row)\n    for k, v in d.items():\n        setattr(metrics, k, v)\n    pb = metrics.SerializeToString()\n    producer.send(topic, pb)\n```\n\n----------------------------------------\n\nTITLE: LongSum Aggregator Configuration in Druid\nDESCRIPTION: Computes sum of values as 64-bit signed integer. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Submitting Supervisor Spec via cURL\nDESCRIPTION: Command to submit a Kafka supervisor specification to the Druid Overlord via HTTP POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function Examples in JSON\nDESCRIPTION: Examples of JavaScript extraction functions for both regular dimensions and time dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str.substr(0, 3); }\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str + '!!!'; }\",\n  \"injective\" : true\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Insensitive Contains Search in Druid\nDESCRIPTION: Defines a case-insensitive search that matches if any part of a dimension value contains the specified search value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"insensitive_contains\",\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring pvalue2tailedZtest Post Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the pvalue2tailedZtest post aggregator that calculates the p-value of a two-sided z-test from a given z-score. The z-score input can be calculated using the zscore2sample post aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"pvalue2tailedZtest\",\n  \"name\": \"<output_name>\",\n  \"zScore\": <zscore post_aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file. This is required to use the quantiles sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing TopN Query with DistinctCount in Druid\nDESCRIPTION: Example of a TopN query using DistinctCount aggregator to find the top 5 dimensions based on unique visitor counts. The query operates over a specific day with 'all' granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"uv\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring String Format Extraction Function in Druid\nDESCRIPTION: Configuration for formatting dimension values using sprintf expression with null handling options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"stringFormat\", \"format\" : <sprintf_expression>, \"nullHandling\" : <optional attribute for handling null value> }\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Apache ZooKeeper\nDESCRIPTION: Commands to download ZooKeeper 3.4.11, extract it, and rename the directory to 'zk'. ZooKeeper is a dependency for Druid used for distributed coordination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/index.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Chrome Configuration for Kerberos\nDESCRIPTION: Commands to configure Google Chrome browser for Kerberos authentication with Druid web UI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngoogle-chrome --auth-server-whitelist=\"druid-coordinator-hostname\" --auth-negotiate-delegate-whitelist=\"druid-coordinator-hostname\"\ngoogle-chrome --auth-server-whitelist=\"druid-overlord-hostname\" --auth-negotiate-delegate-whitelist=\"druid-overlord-hostname\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Segment Identifier Format for Apache Druid\nDESCRIPTION: Example of a segment identifier in Apache Druid, showing the four-part format that includes datasource name, time interval, version number, and partition number. This example represents a segment in the clarity-cloud0 datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Maven Dependencies for Druid Extensions\nDESCRIPTION: Maven POM configuration showing required Druid extension dependencies that need to be included in the fat jar build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-avro-extensions</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-parquet-extensions</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-hdfs-storage</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>mysql-metadata-storage</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Metric in Druid's metricsSpec\nDESCRIPTION: This JSON configuration snippet shows how to set up a HyperUnique metric in Druid's metricsSpec. It defines a metric named 'devices' that uses the 'hyperUnique' type and references the 'device_id_met' field for unique count operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-design.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"hyperUnique\", \"name\" : \"devices\", \"fieldName\" : \"device_id_met\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring String Format Extraction Function in Druid\nDESCRIPTION: Configuration for formatting dimension values using sprintf expressions with optional null handling settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"stringFormat\", \"format\" : <sprintf_expression>, \"nullHandling\" : <optional attribute for handling null value> }\n```\n\n----------------------------------------\n\nTITLE: Configuring InsensitiveContainsSearchQuerySpec in Druid\nDESCRIPTION: Defines a case-insensitive search where a match occurs if any part of a dimension value contains the specified value. This query specification ignores case when matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchqueryspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"insensitive_contains\",\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying for Ingested Event Count in Druid\nDESCRIPTION: This JSON snippet demonstrates how to query for the number of ingested events in Druid using a longSum aggregator. It defines an aggregation named 'numIngestedEvents' that sums the 'count' field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n\"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"numIngestedEvents\", \"fieldName\": \"count\" },\n...\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading Off-heap MapDB Cache in Apache Druid\nDESCRIPTION: This JSON configuration shows a loading lookup using an off-heap MapDB cache. It specifies the lookup type, data fetcher, and separate cache specifications for forward and reverse lookups with size and expiration settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"mapDb\", \"maxEntriesSize\":100000},\n   \"reverseLoadingCacheSpec\":{\"type\":\"mapDb\", \"maxStoreSize\":5, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-dimension Partitioning in Druid\nDESCRIPTION: This JSON snippet configures single-dimension partitioning for Druid ingestion. It sets a target partition size of 5 million rows, allowing Druid to automatically select a dimension for partitioning and create contiguous ranges for segmentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"dimension\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: TopN Query Results Format in Apache Druid\nDESCRIPTION: This example shows the JSON format of results returned by a TopN query in Apache Druid. The results include a timestamp and an array of result objects, each containing dimension values and calculated metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/topnquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2013-08-31T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dim1\": \"dim1_val\",\n        \"count\": 111,\n        \"some_metrics\": 10669,\n        \"average\": 96.11711711711712\n      },\n      {\n        \"dim1\": \"another_dim1_val\",\n        \"count\": 88,\n        \"some_metrics\": 28344,\n        \"average\": 322.09090909090907\n      },\n      {\n        \"dim1\": \"dim1_val3\",\n        \"count\": 70,\n        \"some_metrics\": 871,\n        \"average\": 12.442857142857143\n      },\n      {\n        \"dim1\": \"dim1_val4\",\n        \"count\": 62,\n        \"some_metrics\": 815,\n        \"average\": 13.14516129032258\n      },\n      {\n        \"dim1\": \"dim1_val5\",\n        \"count\": 60,\n        \"some_metrics\": 2787,\n        \"average\": 46.45\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantile Post Aggregator for DoublesSketch\nDESCRIPTION: Post aggregator configuration to extract a single quantile value from a DoublesSketch. It returns the approximate value at a given fractional position in the theoretical sorted stream.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantile\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fraction\" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Average Calculation Example in Druid\nDESCRIPTION: Example query showing how to calculate an average using post-aggregators. This example divides the sum of 'total' by the count of rows to compute an average value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n  \"aggregations\" : [\n    { \"type\" : \"count\", \"name\" : \"rows\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n         ]\n  }]\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Dimension Column Data Structures in Druid\nDESCRIPTION: This snippet demonstrates the three data structures used to represent a dimension column in Druid: a dictionary mapping values to integer IDs, a list of encoded column values, and bitmaps for each unique value indicating which rows contain that value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/segments.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   0,\n   1,\n   1]\n\n3: Bitmaps - one for each unique value of the column\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,0,1,1]\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Extraction Function for Time Dimension\nDESCRIPTION: Uses a JavaScript function to transform the __time dimension, returning a string representation of the second within the minute.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Timeseries Query Context Parameters in Apache Druid\nDESCRIPTION: This markdown table lists query context parameters specific to Timeseries queries in Apache Druid. It includes the skipEmptyBuckets parameter for controlling zero-filling behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/query-context.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default              | description          |\n|-----------------|---------------------|----------------------|\n|skipEmptyBuckets | `false`             | Disable timeseries zero-filling behavior, so only buckets with results will be returned. |\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD Emitter Parameters in JSON\nDESCRIPTION: JSON configuration for the StatsD emitter extension in Apache Druid. It includes parameters like hostname, port, prefix, and other options to customize the emitter's behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"druid.emitter.statsd.hostname\": \"<statsD_server_hostname>\",\n  \"druid.emitter.statsd.port\": <statsD_server_port>,\n  \"druid.emitter.statsd.prefix\": \"\",\n  \"druid.emitter.statsd.separator\": \".\",\n  \"druid.emitter.statsd.includeHost\": false,\n  \"druid.emitter.statsd.dimensionMapPath\": \"<path_to_json_mapping_file>\",\n  \"druid.emitter.statsd.blankHolder\": \"-\",\n  \"druid.emitter.statsd.dogstatsd\": false,\n  \"druid.emitter.statsd.dogstatsdConstantTags\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating Supervisor in Druid Overlord API\nDESCRIPTION: POST request to create a new supervisor or update an existing one in the Druid Overlord.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchToEstimate Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchToEstimate post-aggregator. This returns a distinct count estimate from a given ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Base Settings in Druid\nDESCRIPTION: Basic Zookeeper configuration properties including connection settings, authentication, and session behavior parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.service.host=none\ndruid.zk.service.user=none\ndruid.zk.service.pwd=none\ndruid.zk.service.authScheme=digest\ndruid.zk.service.sessionTimeoutMs=30000\ndruid.zk.service.compress=true\ndruid.zk.service.acl=false\n```\n\n----------------------------------------\n\nTITLE: Sparse Histogram Serialization Format\nDESCRIPTION: Binary format specification for sparse histogram serialization that uses (bucketNum, count) pairs. Used when less than half of buckets have values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x02 for sparse\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\nint: number of following (bucketNum, count) pairs\nsequence of (int, long) pairs:\n  int: bucket number\n  count: bucket count\n```\n\n----------------------------------------\n\nTITLE: Implementing Case-Sensitive Contains Search in Druid\nDESCRIPTION: Defines a case-sensitive search that matches if any part of a dimension value contains the specified search value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"contains\",\n  \"case_sensitive\" : true,\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticS3Firehose in Apache Druid\nDESCRIPTION: JSON configuration for the StaticS3Firehose that ingests events from predefined S3 objects. This firehose is splittable and can be used by native parallel index tasks, where each worker task reads a single object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/s3.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-s3\",\n    \"uris\": [\"s3://foo/bar/file.gz\", \"s3://bar/foo/file2.gz\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleSum Aggregator in Druid JSON\nDESCRIPTION: Defines a doubleSum aggregator to compute the sum of values as a 64-bit floating point value. Similar to longSum, it requires an output name and field name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Time Zone Format Example\nDESCRIPTION: Examples of supported time zone formats in Druid's timestamp functions\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/misc/math-expr.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n\"-08:00\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Period Granularity in Apache Druid\nDESCRIPTION: Examples of specifying period granularity in Apache Druid queries. Shows how to set custom periods using ISO8601 format, with optional time zone and origin specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P2D\", \"timeZone\": \"America/Los_Angeles\"}\n\n{\"type\": \"period\", \"period\": \"P3M\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"2012-02-01T00:00:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Druid Basic Authorizer\nDESCRIPTION: Configuration properties for setting up a Basic authorizer in Apache Druid, which implements role-based access control.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authorizers=[\"MyBasicAuthorizer\"]\n\ndruid.auth.authorizer.MyBasicAuthorizer.type=basic\n```\n\n----------------------------------------\n\nTITLE: HTTP Endpoint for Viewing Recent Coordinator Configuration History in Druid\nDESCRIPTION: HTTP endpoint to retrieve the last N entries of the audit history for Coordinator dynamic configuration changes. This provides a way to limit the number of history entries returned.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_31\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Druid Parser Configuration for JSON Data\nDESCRIPTION: Parser configuration for handling JSON formatted input data using string parser type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TuningConfig in JSON for Hadoop Indexer\nDESCRIPTION: JSON configuration for the tuningConfig section of the Hadoop indexer spec file. It includes the workingPath field for specifying the directory for intermediate results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"tuningConfig\" : {\n ...\n  \"workingPath\": \"/tmp\",\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Variance Values Post-Aggregator\nDESCRIPTION: Post-aggregator that calculates variance values for each column from an ArrayOfDoublesSketch. Returns N double values representing variances where N is the number of values kept per key.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToVariances\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid MySQL metadata storage\nDESCRIPTION: Properties configuration for connecting Druid to a MySQL metadata store. Specifies the extension to load, storage type, connection URL, and authentication credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"mysql-metadata-storage\"]\ndruid.metadata.storage.type=mysql\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=druid\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleFirst Aggregator in Druid JSON\nDESCRIPTION: Defines a doubleFirst aggregator to compute the metric value with the minimum timestamp or 0 if no row exists. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Request Logging Configuration JSON\nDESCRIPTION: Configuration for HTTP request logging including file, emitter and SLF4J logging options with various property settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"druid.request.logging.type\": \"noop\",\n  \"druid.request.logging.dir\": null,\n  \"druid.request.logging.feed\": null,\n  \"druid.request.logging.setMDC\": false,\n  \"druid.request.logging.setContextMDC\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a TopN Query with Multiple Aggregations in Druid\nDESCRIPTION: This JSON query demonstrates a topN query in Druid with multiple aggregations, filters, and a specific dimension. It includes sum aggregations for various fields, count aggregation, and an OR filter for specific order keys. The query is set to return the top 2 results based on the L_QUANTITY_ metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/topnquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_TAX_doubleSum\",\n                 \"name\": \"L_TAX_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_DISCOUNT_doubleSum\",\n                 \"name\": \"L_DISCOUNT_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\",\n                 \"name\": \"L_EXTENDEDPRICE_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             },\n             {\n                 \"name\": \"count\",\n                 \"type\": \"count\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"filter\": {\n        \"fields\": [\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"103136\"\n            },\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"1648672\"\n            }\n        ],\n        \"type\": \"or\"\n    },\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Segment Identifier in Apache Druid\nDESCRIPTION: Example of a segment identifier in Apache Druid, showing the format of datasource name, time interval, version number, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator. This aggregator extends count-distinct Theta sketches by adding arrays of double values associated with unique keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"arrayOfDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"nominalEntries\": <number>,\n  \"numberOfValues\" : <number>,\n  \"metricColumns\" : <array of strings>\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Client for Druid Broker (Properties)\nDESCRIPTION: Properties for configuring the HTTP client used by the Broker to communicate with data servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_31\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.http.numConnections=20\ndruid.broker.http.compressionCodec=gzip\ndruid.broker.http.readTimeout=PT15M\ndruid.broker.http.unusedConnectionTimeout=PT4M\ndruid.broker.http.maxQueuedBytes=0\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Access for Hadoop Indexing Task\nDESCRIPTION: Job properties configuration for accessing S3 data in Hadoop indexing tasks, including AWS credentials and compression codecs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\" : {\n   \"fs.s3.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"fs.s3n.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3n.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3n.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling On-heap Lookup in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates a polling cache that will update its on-heap cache every 10 minutes. It specifies the lookup type, polling period, data fetcher, and cache factory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"pollPeriod\":\"PT10M\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"onHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Scan Query Results in List Format\nDESCRIPTION: Example of Scan query results when resultFormat is set to 'list'. It shows the segmentId, columns, and events with their respective attributes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/scan-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\",\n      \"robot\",\n      \"namespace\",\n      \"anonymous\",\n      \"unpatrolled\",\n      \"page\",\n      \"language\",\n      \"newpage\",\n      \"user\",\n      \"count\",\n      \"added\",\n      \"delta\",\n      \"variation\",\n      \"deleted\"\n    ],\n    \"events\" : [ {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._243\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 77.0,\n        \"delta\" : 77.0,\n        \"variation\" : 77.0,\n        \"deleted\" : 0.0\n    } ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Buckets Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for computing histogram representation with custom bucket size and offset alignment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"buckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"bucketSize\": <bucket_size>,\n  \"offset\": <offset>\n}\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variable Password Provider in Apache Druid (JSON)\nDESCRIPTION: This code snippet demonstrates how to configure an environment variable password provider in Apache Druid. This method allows passwords to be stored in environment variables rather than in plaintext in configuration files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/password-provider.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchToString Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the HLLSketchToString post-aggregator. This post-aggregator converts an HLL sketch to a human-readable string for debugging purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing BloomKFilter in Java for Druid Queries\nDESCRIPTION: This Java code snippet demonstrates how to create a BloomKFilter, add values to it, and serialize it to a Base64 encoded string for use in Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nBloomKFilter bloomFilter = new BloomKFilter(1500);\nbloomFilter.addString(\"value 1\");\nbloomFilter.addString(\"value 2\");\nbloomFilter.addString(\"value 3\");\nByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\nBloomKFilter.serialize(byteArrayOutputStream, bloomFilter);\nString base64Serialized = Base64.encodeBase64String(byteArrayOutputStream.toByteArray());\n```\n\n----------------------------------------\n\nTITLE: Setting up Float Sum Aggregator in Druid\nDESCRIPTION: Aggregator for computing sums as 32-bit floating point values. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring ContainsSearchQuerySpec in Apache Druid\nDESCRIPTION: This snippet illustrates the JSON configuration for a ContainsSearchQuerySpec. It matches if any part of a dimension value contains the specified value, with case sensitivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"contains\",\n  \"case_sensitive\" : true,\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Local Mount Configuration Properties for Druid Deep Storage\nDESCRIPTION: Configuration table showing required properties for setting up local mount deep storage in Druid. Defines the storage type and directory location for storing segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.storage.type`|local||Must be set.|\n|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling On-heap Lookup in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates a polling cache that will update its on-heap cache every 10 minutes. It specifies the lookup type, polling period, data fetcher, and cache factory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"pollPeriod\":\"PT10M\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"onHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Detailed Numeric TopNMetricSpec in Druid\nDESCRIPTION: This snippet demonstrates the JSON object format for specifying a numeric TopNMetricSpec in Druid. It includes the type and metric name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnmetricspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"numeric\",\n    \"metric\": \"<metric_name>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ContainsSearchQuerySpec in Druid\nDESCRIPTION: Defines a case-sensitive search where a match occurs if any part of a dimension value contains the specified value. Unlike the insensitive_contains type, this search respects case by default.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchqueryspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"contains\",\n  \"case_sensitive\" : true,\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Scan Query Results in List Format\nDESCRIPTION: Example of Scan query results when resultFormat is set to 'list'. It shows the segmentId, columns, and events with their respective attributes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/scan-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\",\n      \"robot\",\n      \"namespace\",\n      \"anonymous\",\n      \"unpatrolled\",\n      \"page\",\n      \"language\",\n      \"newpage\",\n      \"user\",\n      \"count\",\n      \"added\",\n      \"delta\",\n      \"variation\",\n      \"deleted\"\n    ],\n    \"events\" : [ {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._243\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 77.0,\n        \"delta\" : 77.0,\n        \"variation\" : 77.0,\n        \"deleted\" : 0.0\n    } ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading Off-heap MapDB Lookup in Apache Druid\nDESCRIPTION: Example configuration for a loading lookup using off-heap MapDB cache with custom settings for both forward and reverse lookup caches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"mapDb\", \"maxEntriesSize\":100000},\n   \"reverseLoadingCacheSpec\":{\"type\":\"mapDb\", \"maxStoreSize\":5, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Approximate Histogram Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the approximate histogram aggregator. This defines parameters like resolution (number of centroids), number of buckets, and upper/lower limits for the histogram calculation. This aggregator is deprecated in favor of DataSketches Quantiles.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"resolution\" : <integer>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <float>,\n  \"upperLimit\" : <float>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing GroupBy Query with DistinctCount in Druid\nDESCRIPTION: Example of a GroupBy query using distinctCount aggregator to count unique visitors grouped by sample_dim. The query operates over a specific day with 'all' granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimensions\": \"[sample_dim]\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Druid Query Logic in Java\nDESCRIPTION: The query logic in Druid is primarily found in the Query* classes. QueryResource.java is a good starting point for tracing the query logic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/overview.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nQueryResource.java\n```\n\n----------------------------------------\n\nTITLE: Coordination Metrics Table in Markdown\nDESCRIPTION: Metrics for the Druid Coordinator component, tracking segment management and load balancing statistics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/assigned/count`|Number of segments assigned to be loaded in the cluster.|tier.|Varies.|\n|`segment/moved/count`|Number of segments moved in the cluster.|tier.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Coordination Metrics Table in Markdown\nDESCRIPTION: Metrics for the Druid Coordinator component, tracking segment management and load balancing statistics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/assigned/count`|Number of segments assigned to be loaded in the cluster.|tier.|Varies.|\n|`segment/moved/count`|Number of segments moved in the cluster.|tier.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Storage Configuration\nDESCRIPTION: Configuration properties for using Google Cloud Storage through HDFS extension, specifying the GCS bucket and directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=gs://bucket/example/directory\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authorizer in Druid\nDESCRIPTION: This JSON snippet shows how to enable the 'basic' authorizer implementation from the druid-basic-security extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/auth.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authorizers\":[\"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffeine Cache in Druid\nDESCRIPTION: Configuration properties for the Caffeine cache implementation in Druid, including size limits, expiration, and executor settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_36\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.type=caffeine\ndruid.cache.sizeInBytes=1GB\ndruid.cache.expireAfter=None\ndruid.cache.cacheExecutorFactory=COMMON_FJP\ndruid.cache.evictOnClose=false\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Production Architecture Image in Markdown\nDESCRIPTION: This code snippet embeds an image showing the Druid production architecture, illustrating the integration of Druid with streaming technologies and data processing systems.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/integrating-druid-with-other-technologies.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<img src=\"../../img/druid-production.png\" width=\"800\"/>\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling On-heap Lookup in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates a polling cache that will update its on-heap cache every 10 minutes. It specifies the lookup type, polling period, data fetcher, and cache factory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"pollPeriod\":\"PT10M\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"onHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Extraction Function with Injectivity Flag\nDESCRIPTION: This JavaScript extraction function appends three exclamation marks to the input string and specifies that the function preserves uniqueness through the injective property.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str + '!!!'; }\",\n  \"injective\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Approximate Histogram Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the approximate histogram aggregator. This defines parameters like resolution (number of centroids), number of buckets, and upper/lower limits for the histogram calculation. This aggregator is deprecated in favor of DataSketches Quantiles.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"resolution\" : <integer>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <float>,\n  \"upperLimit\" : <float>\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Datasources in Apache Druid\nDESCRIPTION: These endpoints provide various operations for managing datasources in Druid, including listing datasources, retrieving metadata, querying intervals and segments, and enabling/disabling datasources and segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/datasources\nGET /druid/coordinator/v1/datasources?simple\nGET /druid/coordinator/v1/datasources?full\nGET /druid/coordinator/v1/datasources/{dataSourceName}\nGET /druid/coordinator/v1/datasources/{dataSourceName}?full\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals?simple\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals?full\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}?simple\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}?full\nGET /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}/serverview\nGET /druid/coordinator/v1/datasources/{dataSourceName}/segments\nGET /druid/coordinator/v1/datasources/{dataSourceName}/segments?full\nGET /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId}\nGET /druid/coordinator/v1/datasources/{dataSourceName}/tiers\nPOST /druid/coordinator/v1/datasources/{dataSourceName}\nPOST /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId}\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId}\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Lookup Extraction Function with Map in Druid\nDESCRIPTION: This example configures an inline lookup extraction function with a map type lookup for value replacement. It retains missing values and specifies that the lookup is injective (preserves uniqueness).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":true,\n  \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Spec with Transform and Filter (JSON)\nDESCRIPTION: Ingestion specification for Druid that includes transform specs to modify data during ingestion. It demonstrates expression transforms and filtering based on transformed values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"transform-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"animal\",\n              { \"name\": \"location\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"number\", \"fieldName\" : \"number\" },\n        { \"type\" : \"longSum\", \"name\" : \"triple-number\", \"fieldName\" : \"triple-number\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      },\n      \"transformSpec\": {\n        \"transforms\": [\n          {\n            \"type\": \"expression\",\n            \"name\": \"animal\",\n            \"expression\": \"concat('super-', animal)\"\n          },\n          {\n            \"type\": \"expression\",\n            \"name\": \"triple-number\",\n            \"expression\": \"number * 3\"\n          }\n        ],\n        \"filter\": {\n          \"type\":\"or\",\n          \"fields\": [\n            { \"type\": \"selector\", \"dimension\": \"animal\", \"value\": \"super-mongoose\" },\n            { \"type\": \"selector\", \"dimension\": \"triple-number\", \"value\": \"300\" },\n            { \"type\": \"selector\", \"dimension\": \"location\", \"value\": \"3\" }\n          ]\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"transform-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Union Datasource in Druid\nDESCRIPTION: Specifies a union datasource that combines multiple table datasources. All source tables must have the same schema. Must be sent to Broker/Router process and not directly to Historical processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/datasource.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n       \"type\": \"union\",\n       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Storage Properties for HDFS\nDESCRIPTION: Configuration settings to modify in Druid's common.runtime.properties file to enable HDFS for deep storage instead of local disk.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS:\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n```\n\n----------------------------------------\n\nTITLE: Executing GroupBy Query with Daily Granularity in Pacific Timezone\nDESCRIPTION: This snippet shows a groupBy query in Apache Druid that aggregates data on a daily basis using the Pacific timezone. It demonstrates how to set up the query with a specific granularity, dimension, and aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/granularities.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring zscore2sample Post Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the zscore2sample post aggregator that calculates z-score using two-sample z-test. It takes success counts and sample sizes for two samples to calculate the z-score between two population proportions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"zscore2sample\",\n  \"name\": \"<output_name>\",\n  \"successCount1\": <post_aggregator> success count of sample 1,\n  \"sample1Size\": <post_aggregaror> sample 1 size,\n  \"successCount2\": <post_aggregator> success count of sample 2,\n  \"sample2Size\" : <post_aggregator> sample 2 size\n}\n```\n\n----------------------------------------\n\nTITLE: Executing HTTP POST Query with Jackson Smile in Druid\nDESCRIPTION: This snippet shows how to send a query to Druid using curl, specifying the Jackson Smile format for the Accept header. This alternative format can be used instead of JSON.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/querying.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:x-jackson-smile' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: TopN Query Result Format in Apache Druid\nDESCRIPTION: This JSON object illustrates the format of results returned by a TopN query in Apache Druid. It includes a timestamp and an array of result objects, each containing dimension values and aggregated metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2013-08-31T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dim1\": \"dim1_val\",\n        \"count\": 111,\n        \"some_metrics\": 10669,\n        \"average\": 96.11711711711712\n      },\n      {\n        \"dim1\": \"another_dim1_val\",\n        \"count\": 88,\n        \"some_metrics\": 28344,\n        \"average\": 322.09090909090907\n      },\n      {\n        \"dim1\": \"dim1_val3\",\n        \"count\": 70,\n        \"some_metrics\": 871,\n        \"average\": 12.442857142857143\n      },\n      {\n        \"dim1\": \"dim1_val4\",\n        \"count\": 62,\n        \"some_metrics\": 815,\n        \"average\": 13.14516129032258\n      },\n      {\n        \"dim1\": \"dim1_val5\",\n        \"count\": 60,\n        \"some_metrics\": 2787,\n        \"average\": 46.45\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Using Selector Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A simple selector filter that matches a specific dimension with a specific value, equivalent to SQL's WHERE clause with an equality condition.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"selector\", \"dimension\": <dimension_string>, \"value\": <dimension_value_string> }\n```\n\n----------------------------------------\n\nTITLE: Basic String Metric Specification in Druid TopN Queries\nDESCRIPTION: The simplest form of TopNMetricSpec, using a string value to indicate the metric name for sorting topN results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/topnmetricspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": \"<metric_name>\"\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in the Druid loadList. This is required to use the quantiles sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading On-heap Guava Cache in Apache Druid\nDESCRIPTION: JSON configuration for a loading on-heap Guava cache with reverse lookup. It uses a JDBC data fetcher and specifies cache parameters such as maximum size and expiration times.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"guava\"},\n   \"reverseLoadingCacheSpec\":{\"type\":\"guava\", \"maximumSize\":500000, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring G1 Garbage Collector in Java for Druid\nDESCRIPTION: This snippet shows the Java VM argument to enable the G1 garbage collector, which is recommended for Druid. It also includes an argument to terminate the process on out-of-memory errors, which is useful for automatic recovery.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n-XX:+UseG1GC\n-XX:+ExitOnOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: Quantiles Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Quantiles post-aggregator, which computes an array of quantiles based on the underlying approximate histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantiles\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probabilities\" : [ <quantile>, <quantile>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Cleaning Kafka Logs for Reset\nDESCRIPTION: Command to remove Kafka log files when resetting the cluster state after completing the Kafka tutorial. This ensures a clean state for subsequent tutorial runs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/index.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /tmp/kafka-logs\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in the Druid loadList. This is required to use the quantiles sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Extraction Function Filter in Druid\nDESCRIPTION: Example of a selector filter with lookup extraction function for product matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Cardinality Aggregation for Distinct Countries\nDESCRIPTION: JSON configuration for a cardinality aggregation that counts distinct countries across two fields: country of origin and country of residence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_countries\",\n  \"fields\": [ \"country_of_origin\", \"country_of_residence\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Coordinator Leadership Information in Apache Druid\nDESCRIPTION: These endpoints provide information about the current leader coordinator in the Druid cluster, including identifying the leader and checking if a specific coordinator is the leader.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_1\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/leader\nGET /druid/coordinator/v1/isLeader\n```\n\n----------------------------------------\n\nTITLE: Bulk Lookup Configuration JSON Structure in Druid\nDESCRIPTION: The JSON structure for configuring multiple lookups across different tiers in a single bulk update operation via the Coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"<tierName>\": {\n        \"<lookupName>\": {\n          \"version\": \"<version>\",\n          \"lookupExtractorFactory\": {\n            \"type\": \"<someExtractorFactoryType>\",\n            \"<someExtractorField>\": \"<someExtractorValue>\"\n          }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authenticator in Druid\nDESCRIPTION: Basic configuration to set up a Kerberos authenticator in the authentication chain. Defines the authenticator type as 'kerberos'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyKerberosAuthenticator\"]\n\ndruid.auth.authenticator.MyKerberosAuthenticator.type=kerberos\n```\n\n----------------------------------------\n\nTITLE: Implementing Contains Search in Druid\nDESCRIPTION: Defines a case-sensitive search query that matches if any part of a dimension value contains the specified search value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"contains\",\n  \"case_sensitive\" : true,\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring White-list Converter for Ambari Metrics Emitter in Druid\nDESCRIPTION: JSON configuration for the 'whiteList' event converter that sends only white-listed metrics and dimensions to Ambari Metrics. This example shows how to specify a custom map file for the white list.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"appName\":\"druid\", \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring QuantilesDoublesSketchToQuantile Post Aggregator\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToQuantile post aggregator, which returns an approximation of the value at a given fractional position in a hypothetical sorted version of the input stream.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantile\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fraction\" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Thrift Ingestion Using Hadoop in Apache Druid\nDESCRIPTION: A JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. It shows how to configure inputFormat for either SequenceFileInputFormat or LzoThriftBlockInputFormat with the necessary jar paths and job properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"book\",\n      \"parser\": {\n        \"type\": \"thrift\",\n        \"jarPath\": \"book.jar\",\n        \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n        \"protocol\": \"compact\",\n        \"parseSpec\": {\n          \"format\": \"json\",\n          ...\n        }\n      },\n      \"metricsSpec\": [],\n      \"granularitySpec\": {}\n    },\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\",\n        // \"inputFormat\": \"com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat\",\n        \"paths\": \"/user/to/some/book.seq\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"jobProperties\": {\n        \"tmpjars\":\"/user/h_user_profile/du00/druid/test/book.jar\",\n        // \"elephantbird.class.for.MultiInputFormat\" : \"${YOUR_THRIFT_CLASS_NAME}\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Timeseries Query with DistinctCount in Druid\nDESCRIPTION: Example of a Timeseries query using the DistinctCount aggregator to count unique visitor IDs over a specified time interval. The query is configured with daily granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Selector Filter in Druid\nDESCRIPTION: Basic selector filter for matching specific dimension values. Equivalent to SQL WHERE clause for exact matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"selector\", \"dimension\": <dimension_string>, \"value\": <dimension_value_string> }\n```\n\n----------------------------------------\n\nTITLE: Configuring SqlFirehose in Apache Druid\nDESCRIPTION: SqlFirehose configuration for ingesting events from RDBMS sources. The database connection information is provided as part of the ingestion spec, and results are fetched locally and indexed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"sql\",\n    \"database\": {\n        \"type\": \"mysql\",\n        \"connectorConfig\" : {\n        \"connectURI\" : \"jdbc:mysql://host:port/schema\",\n        \"user\" : \"user\",\n        \"password\" : \"password\"\n        }\n     },\n    \"sqls\" : [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Response Format for subtotalsSpec Query in Apache Druid\nDESCRIPTION: Shows the structure of results returned when using subtotalsSpec, with multiple sections representing different dimension groupings as specified in the query configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t1\",\n    \"event\" : { \"D1\": \"..\", \"D2\": \"..\", \"D3\": \"..\" }\n    }\n  },\n    {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t2\",\n    \"event\" : { \"D1\": \"..\", \"D2\": \"..\", \"D3\": \"..\" }\n    }\n  },\n  ...\n  ...\n\n   {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t1\",\n    \"event\" : { \"D1\": \"..\", \"D3\": \"..\" }\n    }\n  },\n    {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t2\",\n    \"event\" : { \"D1\": \"..\", \"D3\": \"..\" }\n    }\n  },\n  ...\n  ...\n\n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t1\",\n    \"event\" : { \"D3\": \"..\" }\n    }\n  },\n    {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"t2\",\n    \"event\" : { \"D3\": \"..\" }\n    }\n  },\n...\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Field Accessor Post-Aggregators in Druid\nDESCRIPTION: Shows two types of field accessor post-aggregators: 'fieldAccess' for raw aggregation objects and 'finalizingFieldAccess' for finalized values like estimated cardinality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"fieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"finalizingFieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Defining Dimensions in Druid Ingestion Spec\nDESCRIPTION: Adding a dimensionsSpec inside the parseSpec to define the dimensions for the dataset. This includes IP addresses, ports, and protocol, with appropriate data types specified for each dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing a Select Query in Apache Druid\nDESCRIPTION: This JSON structure defines a Select query in Apache Druid. It specifies the data source, time interval, and pagination settings. The query retrieves raw data rows from the 'wikipedia' datasource for a specific date range.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/select-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Executing EXPLAIN PLAN FOR with a TopN query in Druid SQL\nDESCRIPTION: This snippet shows how to use the EXPLAIN PLAN FOR command with a SQL query that selects the top 10 pages by edit count from a Wikipedia dataset within a specific time range. The result displays the native Druid query plan.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\nLANGUAGE: bash\nCODE:\n```\ndsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n\n PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\n DruidQueryRel(query=[{\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"page\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":10,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}],\"postAggregations\":[],\"context\":{},\"descending\":false}], signature=[{d0:STRING, a0:LONG}]) \n\nRetrieved 1 row in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: Querying Failed Tasks in Druid\nDESCRIPTION: SQL query to retrieve information about failed tasks from the sys.tasks table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.tasks where status='FAILED';\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticAzureBlobStoreFirehose for Apache Druid\nDESCRIPTION: JSON configuration for the StaticAzureBlobStoreFirehose to ingest data from Azure Blob Storage. This defines how Druid reads JSON objects from specified Azure blobs, with support for multiple containers and paths. The configuration includes optional caching and prefetching parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/azure.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-azure-blobstore\",\n    \"blobs\": [\n        {\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Combining and Overwriting Data in Apache Druid\nDESCRIPTION: This command submits a task that combines existing data with new data and overwrites the original data in the 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Compaction Task Example\nDESCRIPTION: A minimal example of a Druid compaction task that compacts all segments in the 'wikipedia' dataSource for the year 2017. This task uses default settings for segment granularity, which maintains the original granularity of the segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"compact\",\n  \"dataSource\" : \"wikipedia\",\n  \"interval\" : \"2017-01-01/2018-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid to Use PostgreSQL Metadata Storage\nDESCRIPTION: Configuration properties to enable and configure PostgreSQL as the metadata storage for Druid. These settings specify the connection details including URI, username and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\"]\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Max Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the max post-aggregator, which returns the maximum value of the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"max\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Greatest Post-Aggregator Implementation\nDESCRIPTION: Implements a post-aggregator that computes the maximum value across multiple specified fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"doubleGreatest\",\n  \"name\"  : <output_name>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Ingestion with Parquet Parser\nDESCRIPTION: This JSON configuration demonstrates how to set up Hadoop-based indexing to ingest Parquet files using the 'parquet' parser and 'parquet' parseSpec. It includes settings for input format, flatten spec, timestamp spec, and dimensions spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"parquet\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Spatial Dimensions in Druid JSON Data Spec\nDESCRIPTION: This snippet demonstrates how to define spatial dimensions in a Hadoop-based JSON data specification. It configures a spatial dimension named 'coordinates' that is constructed from 'lat' and 'long' dimensions in the original data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/geo.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"hadoop\",\n\t\"dataSchema\": {\n\t\t\"dataSource\": \"DatasourceName\",\n\t\t\"parser\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"parseSpec\": {\n\t\t\t\t\"format\": \"json\",\n\t\t\t\t\"timestampSpec\": {\n\t\t\t\t\t\"column\": \"timestamp\",\n\t\t\t\t\t\"format\": \"auto\"\n\t\t\t\t},\n\t\t\t\t\"dimensionsSpec\": {\n\t\t\t\t\t\"dimensions\": [],\n\t\t\t\t\t\"spatialDimensions\": [{\n\t\t\t\t\t\t\"dimName\": \"coordinates\",\n\t\t\t\t\t\t\"dims\": [\"lat\", \"long\"]\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining HyperUnique metric in Druid metricsSpec\nDESCRIPTION: Configuration snippet for the metricsSpec that defines a hyperUnique aggregator. This enables fast unique count operations on the field while maintaining the ability to filter on the dimension version of the same data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"hyperUnique\", \"name\" : \"devices\", \"fieldName\" : \"device_id_met\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Metadata Queries in Druid\nDESCRIPTION: Configuration properties for Segment Metadata queries in Druid, controlling default history interval and analysis types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.segmentMetadata.defaultHistory=P1W\ndruid.query.segmentMetadata.defaultAnalysisTypes=[\"cardinality\", \"interval\", \"minmax\"]\n```\n\n----------------------------------------\n\nTITLE: Duration Granularity Specification in Apache Druid\nDESCRIPTION: Examples of specifying duration granularities in Apache Druid queries. These granularities allow for custom time buckets based on millisecond durations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 7200000}\n\n{\"type\": \"duration\", \"duration\": 3600000, \"origin\": \"2012-01-01T00:30:00Z\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Lookup Extraction Function in Druid\nDESCRIPTION: Specifies an inline lookup map for dimension value replacement without registering in cluster-wide configuration. Includes options for handling missing values and injectivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":true,\n  \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Equal Buckets Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the equal buckets post-aggregator, which computes a visual representation of the histogram with equal-sized bins.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"equalBuckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"numBuckets\": <count>\n}\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Response in JSON\nDESCRIPTION: Example of a successful response from Tranquility Server after sending Wikipedia data. The response shows that 39,244 events were received and sent to Druid for ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":39244}}\n```\n\n----------------------------------------\n\nTITLE: SQL Equivalent for Multi-Dimension Cardinality\nDESCRIPTION: SQL query showing the equivalent operation for cardinality aggregator with multiple dimensions using UNION.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/hll-old.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(DISTINCT(value)) FROM (\n  SELECT dim_1 as value FROM <datasource>\n  UNION\n  SELECT dim_2 as value FROM <datasource>\n  UNION\n  SELECT dim_3 as value FROM <datasource>\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Data Source Metadata in Apache Druid\nDESCRIPTION: Basic query structure for retrieving metadata information from a Druid dataSource. The query requires specifying the queryType as 'dataSourceMetadata' and the target dataSource name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"dataSourceMetadata\",\n    \"dataSource\": \"sample_datasource\"\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Apache Kafka in Bash\nDESCRIPTION: Commands to download Apache Kafka 2.1.0, extract the archive, and navigate to the Kafka directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://archive.apache.org/dist/kafka/2.1.0/kafka_2.12-2.1.0.tgz\ntar -xzf kafka_2.12-2.1.0.tgz\ncd kafka_2.12-2.1.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Escalator in Apache Druid\nDESCRIPTION: Configuration for the Druid escalator which handles internal system user requests. This includes username, password, and authorizer settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# Escalator\ndruid.escalator.type=basic\ndruid.escalator.internalClientUsername=druid_system\ndruid.escalator.internalClientPassword=password2\ndruid.escalator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Compaction Task Example\nDESCRIPTION: A minimal example of a Druid compaction task that compacts all segments in the 'wikipedia' dataSource for the year 2017. This task uses default settings for segment granularity, which maintains the original granularity of the segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"compact\",\n  \"dataSource\" : \"wikipedia\",\n  \"interval\" : \"2017-01-01/2018-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Lookup in Apache Druid\nDESCRIPTION: Example configuration for JDBC-based lookup including database connection details, polling period, and column mappings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"jdbc\",\n  \"namespace\":\"some_lookup\",\n  \"connectorConfig\":{\n    \"createTables\":true,\n    \"connectURI\":\"jdbc:mysql://localhost:3306/druid\",\n    \"user\":\"druid\",\n    \"password\":\"diurd\"\n  },\n  \"table\":\"some_lookup_table\",\n  \"keyColumn\":\"the_old_dim_value\",\n  \"valueColumn\":\"the_new_dim_value\",\n  \"tsColumn\":\"timestamp_column\",\n  \"pollPeriod\":600000\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with Selector Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Shows how to use a selector filter in a HavingSpec for a groupBy query. This allows filtering based on a specific dimension and value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : {\n              \"type\": \"selector\",\n              \"dimension\" : \"<dimension>\",\n              \"value\" : \"<dimension_value>\"\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Metrics Table Structure - Query Metrics for Historical\nDESCRIPTION: Markdown table showing metrics collected by Historical nodes, including query processing times, segment scanning metrics, and CPU usage statistics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`query/time`|Milliseconds taken to complete a query.|Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension.|< 1s|\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Rule in Druid\nDESCRIPTION: JSON configuration for a period drop rule that removes segments based on a rolling time window.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring longMax Aggregator in Apache Druid\nDESCRIPTION: The longMax aggregator computes the maximum of all metric values and Long.MIN_VALUE. It requires an output name and the field name of the metric to find the maximum value of.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Druid Metrics Table Structure - Query Metrics for Historical\nDESCRIPTION: Markdown table showing metrics collected by Historical nodes, including query processing times, segment scanning metrics, and CPU usage statistics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`query/time`|Milliseconds taken to complete a query.|Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension.|< 1s|\n```\n\n----------------------------------------\n\nTITLE: Configuring Variance Fold Aggregator for Querying in Apache Druid\nDESCRIPTION: JSON configuration for using the variance fold aggregator during querying in Apache Druid. Specifies the aggregator type, output name, field name, and estimator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"varianceFold\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Select Query Result Format\nDESCRIPTION: Example of the response format from a Select query, showing the pagingIdentifiers and events structure containing the raw data rows with their corresponding segment information and offsets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/select-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n  \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n  \"result\" : {\n    \"pagingIdentifiers\" : {\n      \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 4\n    },\n    \"events\" : [ {\n      \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n      \"offset\" : 0,\n      \"event\" : {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n      }\n    }] \n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Uniform Granularity Spec in Druid\nDESCRIPTION: JSON configuration for uniform granularity specification that generates segments with uniform time intervals. Defines parameters like segmentGranularity, queryGranularity, rollup, and intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"segmentGranularity\": \"string\",\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": [\"string\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Overlord Console URL in Markdown\nDESCRIPTION: Provides the URL format for accessing the Overlord console, which allows viewing pending tasks, running tasks, available workers, and recent worker activity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/overlord.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\nhttp://<OVERLORD_IP>:<port>/console.html\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Response Format in Druid SQL\nDESCRIPTION: Example of specifying a custom result format in a Druid SQL query using the 'resultFormat' parameter. The query counts records in a data source with specific filter conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Log Storage for HDFS\nDESCRIPTION: Configuration settings to modify in Druid's common.runtime.properties file to enable HDFS for indexing service logs instead of local disk.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n#\n# Indexing service logs\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Querying with Virtual Columns in Apache Druid\nDESCRIPTION: This example demonstrates a scan query in Apache Druid that includes virtual columns. It shows how to define expression-based virtual columns that create new queryable views based on existing data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/virtual-columns.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"queryType\": \"scan\",\n \"dataSource\": \"page_data\",\n \"columns\":[],\n \"virtualColumns\": [\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\",\n      \"outputType\": \"STRING\"\n    },\n    {\n      \"type\": \"expression\",\n      \"name\": \"tripleWordCount\",\n      \"expression\": \"wordCount * 3\",\n      \"outputType\": \"LONG\"\n    }\n  ],\n \"intervals\": [\n   \"2013-01-01/2019-01-02\"\n ] \n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Statistical Testing Query in Apache Druid\nDESCRIPTION: A comprehensive example showing how to combine the zscore2sample and pvalue2tailedZtest post aggregators in a Druid query. This example calculates a z-score from two samples with different success rates and then derives the p-value from that z-score.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n    \"postAggregations\" : {\n    \"type\"   : \"pvalue2tailedZtest\",\n    \"name\"   : \"pvalue\",\n    \"zScore\" : \n    {\n     \"type\"   : \"zscore2sample\",\n     \"name\"   : \"zscore\",\n     \"successCount1\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation1Sample\",\n         \"value\"  : 300\n       },\n     \"sample1Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation1\",\n         \"value\"  : 500\n       },\n     \"successCount2\":\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation2Sample\",\n         \"value\"  : 450\n       },\n     \"sample2Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation2\",\n         \"value\"  : 600\n       }\n     }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling a Middle Manager for Update in Apache Druid\nDESCRIPTION: Send a POST request to disable a Middle Manager, preventing it from receiving new tasks and allowing current tasks to complete before updating.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rolling-updates.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/disable\n```\n\n----------------------------------------\n\nTITLE: Column Comparison Filter in Druid\nDESCRIPTION: Filter that compares two dimensions to each other, equivalent to WHERE dimension_a = dimension_b in SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"columnComparison\", \"dimensions\": [<dimension_a>, <dimension_b>] }\n```\n\n----------------------------------------\n\nTITLE: License Comment for React use-sync-external-store-shim\nDESCRIPTION: Copyright and license information for the React use-sync-external-store-shim production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Credentials in Druid Properties\nDESCRIPTION: Example showing how to set AWS access key and secret key credentials in Druid's runtime.properties for Kinesis API authentication. If not provided, credentials will be looked up from environment variables, default profile config, or EC2 instance profile.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\n-Ddruid.kinesis.accessKey=123 -Ddruid.kinesis.secretKey=456\n```\n\n----------------------------------------\n\nTITLE: Search Query Response Format in Apache Druid\nDESCRIPTION: Example of the response format for a search query. The response contains a timestamp and result array with matching dimension values, including the dimension name, matching value, and count of occurrences.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"Ke$ha\",\n        \"count\": 3\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"Ke$haForPresident\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"SomethingThatContainsKe\",\n        \"count\": 1\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"SomethingElseThatContainsKe\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Datasource Metadata in JSON\nDESCRIPTION: This endpoint returns full metadata for a specific datasource as stored in the metadata store. It provides comprehensive information about the datasource configuration and properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"name\": \"example_datasource\",\n  \"properties\": {\n    \"segments\": 1000,\n    \"size\": 1073741824,\n    \"minTime\": \"2020-01-01T00:00:00.000Z\",\n    \"maxTime\": \"2020-12-31T23:59:59.999Z\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Kafka Supervisor Spec via HTTP POST in Bash\nDESCRIPTION: Example curl command to submit a Kafka supervisor specification to the Druid Overlord via HTTP POST.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Configuring General TLS Settings in Apache Druid\nDESCRIPTION: Basic configuration properties for enabling/disabling HTTP and HTTPS connectors in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/tls-support.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.enablePlaintextPort`|Enable/Disable HTTP connector.|`true`|\n|`druid.enableTlsPort`|Enable/Disable HTTPS connector.|`false`|\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension TopNMetricSpec in Druid\nDESCRIPTION: This snippet shows how to configure a Dimension TopNMetricSpec in Druid. It sorts TopN results by dimension value and includes options for specifying sorting order and pagination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnmetricspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"dimension\",\n    \"ordering\": \"lexicographic\",\n    \"previousStop\": \"<previousStop_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Broker Process\nDESCRIPTION: Command to start the Druid Broker process. This is the main entry point for launching a Broker node in a Druid cluster using the Main class CLI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/broker.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple Numeric TopNMetricSpec in Druid\nDESCRIPTION: This snippet shows the simplest metric specification as a String value indicating the metric to sort topN results by in a Druid topN query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnmetricspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": \"<metric_name>\"\n```\n\n----------------------------------------\n\nTITLE: Defining Aggregators for Flattened JSON Fields in Apache Druid\nDESCRIPTION: An example of how to define aggregators using the metric column names specified in the flattenSpec for Apache Druid. It includes longSum and doubleSum aggregator types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metricsSpec\" : [ \n{\n  \"type\" : \"longSum\",\n  \"name\" : \"path-metric-sum\",\n  \"fieldName\" : \"path-metric\"\n}, \n{\n  \"type\" : \"doubleSum\",\n  \"name\" : \"hello-0-sum\",\n  \"fieldName\" : \"hello-0\"\n},\n{\n  \"type\" : \"longSum\",\n  \"name\" : \"metrica-sum\",\n  \"fieldName\" : \"metrica\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Broker Process\nDESCRIPTION: Command to start the Druid Broker process. This is the main entry point for launching a Broker node in a Druid cluster using the Main class CLI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/broker.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Segment Registry in Druid\nDESCRIPTION: The ZooKeeper path where Historical and Realtime processes register the permanent znodes to track which segments they are serving. This creates the base path for segment announcements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/zookeeper.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Selector Filter in Druid\nDESCRIPTION: Basic selector filter that matches a specific dimension with a specific value, equivalent to SQL WHERE clause with equality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"selector\", \"dimension\": <dimension_string>, \"value\": <dimension_value_string> }\n```\n\n----------------------------------------\n\nTITLE: Example Query Using MomentSketch for Quantile Analysis\nDESCRIPTION: Complete example of a query that uses momentSketchMerge aggregator to combine pre-computed sketches and post-aggregators to calculate specific quantiles and minimum value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\": [{\n    \"type\": \"momentSketchMerge\",\n    \"name\": \"sketch\",\n    \"fieldName\": \"sketch\",\n    \"k\": 10,\n    \"compress\": true\n  }],\n  \"postAggregations\": [\n  {\n    \"type\": \"momentSketchSolveQuantiles\",\n    \"name\": \"quantiles\",\n    \"fractions\": [0.1, 0.5, 0.9],\n    \"field\": {\n      \"type\": \"fieldAccess\",\n      \"fieldName\": \"sketch\"\n    }\n  },\n  {\n    \"type\": \"momentSketchMin\",\n    \"name\": \"min\",\n    \"field\": {\n      \"type\": \"fieldAccess\",\n      \"fieldName\": \"sketch\"\n    }\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling a Specific Segment in Druid Coordinator API\nDESCRIPTION: DELETE endpoint to disable a specific segment for a datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_6\n\nLANGUAGE: HTTP\nCODE:\n```\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId}\n```\n\n----------------------------------------\n\nTITLE: Configuration History Count API Endpoint\nDESCRIPTION: HTTP endpoint for retrieving the last N entries of Coordinator configuration audit history.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Defining Coordinator Leader Election Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path used for Coordinator leader election using Curator LeadershipLatch recipe.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.coordinatorPath}/_COORDINATOR\n```\n\n----------------------------------------\n\nTITLE: Sample Data Format for Multi-value Dimensions in Druid\nDESCRIPTION: Example of data rows with a multi-value dimension called 'tags' that contains arrays of string values. This represents the structure of data stored in a Druid segment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2011-01-12T00:00:00.000Z\", \"tags\": [\"t1\",\"t2\",\"t3\"]}  #row1\n{\"timestamp\": \"2011-01-13T00:00:00.000Z\", \"tags\": [\"t3\",\"t4\",\"t5\"]}  #row2\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": [\"t5\",\"t6\",\"t7\"]}  #row3\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": []}                #row4\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Compression Parameters in Apache Druid\nDESCRIPTION: This table outlines the configuration properties for HTTP compression in Apache Druid. It includes settings for compression level and inflate buffer size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/http-compression.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|\n|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|\n```\n\n----------------------------------------\n\nTITLE: SQL Query Using Bloom Filter Aggregator\nDESCRIPTION: SQL query example demonstrating how to use the BLOOM_FILTER aggregator function to compute bloom filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT BLOOM_FILTER(<expression>, <max number of entries>) FROM druid.foo WHERE dim2 = 'abc'\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator for Distinct Countries in Apache Druid\nDESCRIPTION: This example demonstrates how to use the Cardinality aggregator to determine the number of distinct countries people are living in or have come from. It uses two fields: country_of_origin and country_of_residence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/hll-old.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_countries\",\n  \"fields\": [ \"country_of_origin\", \"country_of_residence\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing MySQL Metadata Tables for Druid\nDESCRIPTION: This command initializes the metadata tables for Druid in a MySQL database. It uses the metadata-init tool, specifying the MySQL extension, connection URI, user credentials, and base table name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metadata-migration.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ${DRUID_ROOT}\njava -classpath \"lib/*\" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory=\"extensions\" -Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\"] -Ddruid.metadata.storage.type=mysql org.apache.druid.cli.Main tools metadata-init --connectURI=\"<mysql-uri>\" --user <user> --password <pass> --base druid\n```\n\n----------------------------------------\n\nTITLE: Configuring Asynchronous Logging in Apache Druid using Log4j2\nDESCRIPTION: This XML configuration for Log4j2 sets up asynchronous logging for specific chatty classes in Apache Druid. It defines console appenders and configures async loggers for various Druid components to improve logging performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/performance-faq.md#2025-04-09_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <AsyncLogger name=\"org.apache.druid.curator.inventory.CuratorInventoryManager\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name=\"org.apache.druid.client.BatchServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->\n    <AsyncLogger name=\"org.apache.druid.client.ServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name =\"org.apache.druid.java.util.http.client.pool.ChannelResourceFactory\" level=\"info\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Broadcast Rule in Druid\nDESCRIPTION: JSON configuration for a forever broadcast rule that defines segment co-location across data sources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastForever\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Constant Post-Aggregator in Druid Query JSON\nDESCRIPTION: Illustrates the structure of a constant post-aggregator that always returns a specified numerical value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Simple Consumer Firehose in Apache Druid\nDESCRIPTION: JSON configuration for the Kafka Simple Consumer firehose in Apache Druid. This configuration connects to Kafka brokers, specifies the topic to consume from, and sets parameters like queue buffer length and partition IDs. It uses 'firehoseV2' instead of 'firehose' with type 'kafka-0.8-v2'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/kafka-simple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"firehoseV2\": {\n    \"type\" : \"kafka-0.8-v2\",\n    \"brokerList\" :  [\"localhost:4443\"],\n    \"queueBufferLength\":10001,\n    \"resetOffsetToEarliest\":\"true\",\n    \"partitionIdList\" : [\"0\"],\n    \"clientId\" : \"localclient\",\n    \"feed\": \"wikipedia\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Compaction in Apache Druid\nDESCRIPTION: A minimal example of a compaction configuration for the 'wikiticker' datasource. This basic configuration initiates compaction using default values for all optional parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSource\": \"wikiticker\"\n}\n```\n\n----------------------------------------\n\nTITLE: React Core Production License\nDESCRIPTION: MIT license declaration for React's react.production.min.js core module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining JavaScript Post-Aggregator in Druid Query JSON\nDESCRIPTION: Demonstrates how to define a JavaScript post-aggregator that applies a custom JavaScript function to specified fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/post-aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"absPercent\",\n  \"fieldNames\": [\"delta\", \"total\"],\n  \"function\": \"function(delta, total) { return 100 * Math.abs(delta) / total; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling MiddleManager Response in JSON\nDESCRIPTION: POST request to /druid/worker/v1/disable 'disables' a MiddleManager, stopping it from accepting new tasks but allowing completion of existing ones. Returns a JSON object with the combined druid.host and druid.port as the key.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"disabled\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Redis Cache Properties\nDESCRIPTION: These properties are used to configure the Redis cache for Druid. They should be added to the common.runtime.properties file. The configuration includes Redis server details, cache entry expiration, timeout, and connection pool settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/redis-cache.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.host=None\ndruid.cache.port=None\ndruid.cache.expiration=24 * 3600 * 1000\ndruid.cache.timeout=2000\ndruid.cache.maxTotalConnections=8\ndruid.cache.maxIdleConnections=8\ndruid.cache.minIdleConnections=0\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Parser with Disabled Auto Discovery\nDESCRIPTION: JSON configuration example for the ORC parser using the 'orc' parseSpec format with field discovery explicitly disabled and manually specified dimensions and flatten expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/orc.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"orc\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": false,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"millis\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim3\",\n              \"nestedDim\",\n              \"listDimFirstItem\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Inverted TopNMetricSpec in Druid\nDESCRIPTION: A specification that inverts the order of a delegate metric spec, typically used to achieve ascending sort order in topN query results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/topnmetricspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"inverted\",\n    \"metric\": <delegate_top_n_metric_spec>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Parser with Parquet ParseSpec in Druid\nDESCRIPTION: This JSON configuration demonstrates how to set up the Parquet parser with a Parquet parseSpec in Druid. It includes settings for input format, flattenSpec, timestampSpec, and dimensionsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"parquet\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: HDFS Deep Storage Basic Configuration\nDESCRIPTION: Configuration properties for setting up HDFS as deep storage in Druid, including storage type and directory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=<hdfs-directory>\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous Authenticator with Basic Security Extension\nDESCRIPTION: Example configuration that enables an anonymous authenticator after basic authentication, creating a fallback path for authentication. The anonymous user will be identified as 'defaultUser' and authorized by 'myBasicAuthorizer'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/auth.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"basic\", \"anonymous\"]\n\ndruid.auth.authenticator.anonymous.type=anonymous\ndruid.auth.authenticator.anonymous.identity=defaultUser\ndruid.auth.authenticator.anonymous.authorizerName=myBasicAuthorizer\n\n# ... usual configs for basic authentication would go here ...\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Metric for Dual-Purpose Columns in Druid (JSON)\nDESCRIPTION: This snippet shows how to configure the 'metricsSpec' in Druid to use a 'hyperUnique' metric type. It specifies the metric name, type, and the field name to be used for unique counting, corresponding to the metric version of the dual-purpose column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"hyperUnique\", \"name\" : \"devices\", \"fieldName\" : \"device_id_met\" }\n```\n\n----------------------------------------\n\nTITLE: Include All Columns Configuration in Druid Metadata Query\nDESCRIPTION: Configuration to include all columns in the segment metadata query result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"toInclude\": { \"type\": \"all\"}\n```\n\n----------------------------------------\n\nTITLE: Using NOT Logical Expression Filter in Druid groupBy Query\nDESCRIPTION: Demonstrates how to use a NOT logical expression filter in a having clause to negate a condition, showing results that do not match the specified filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n        \"type\": \"not\",\n        \"havingSpec\":         \n            {\n                \"type\": \"equalTo\",\n                \"aggregation\": \"<aggregate_metric>\",\n                \"value\": <numeric_value>\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query on Multi-value Dimensions without Filtering in Apache Druid\nDESCRIPTION: Example of a groupBy query that groups results by the 'tags' multi-value dimension without any filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with EqualTo Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Demonstrates how to use an 'equalTo' filter in a HavingSpec for a groupBy query. This filter matches rows with a specific aggregate value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"equalTo\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Same Interval Merge Task in Apache Druid (Deprecated)\nDESCRIPTION: JSON configuration for a Same Interval Merge task in Apache Druid. This deprecated task is a shortcut of the merge task, merging all segments within a specified interval. It includes options for aggregations, rollup, and task context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"same_interval_merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"interval\": <DataSegment objects in this interval are going to be merged>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Installing ZooKeeper\nDESCRIPTION: Commands to download and set up Apache ZooKeeper, which is required for Druid's distributed coordination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/index.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Thrift Ingestion with Hadoop\nDESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. Includes settings for input format, jar paths, and Thrift class specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"book\",\n      \"parser\": {\n        \"type\": \"thrift\",\n        \"jarPath\": \"book.jar\",\n        \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n        \"protocol\": \"compact\",\n        \"parseSpec\": {\n          \"format\": \"json\",\n          \"...\": \"...\"\n        }\n      },\n      \"metricsSpec\": [],\n      \"granularitySpec\": {}\n    },\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\",\n        \"paths\": \"/user/to/some/book.seq\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"jobProperties\": {\n        \"tmpjars\":\"/user/h_user_profile/du00/druid/test/book.jar\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Payload for Marking Segments Used/Unused in Druid\nDESCRIPTION: JSON payload structure for marking segments as used or unused, specifying either an interval or a set of segment IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"interval\": \"2015-09-12T03:00:00.000Z/2015-09-12T05:00:00.000Z\",\n  \"segmentIds\": [\"segmentId1\", \"segmentId2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Subtotals Specification in groupBy Query\nDESCRIPTION: Example showing how to use subtotalsSpec in a groupBy query to compute multiple sub-groupings in a single query, with custom dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\": \"groupBy\",\n ...\n ...\n\"dimensions\": [\n  {\n  \"type\" : \"default\",\n  \"dimension\" : \"d1col\",\n  \"outputName\": \"D1\"\n  },\n  {\n  \"type\" : \"extraction\",\n  \"dimension\" : \"d2col\",\n  \"outputName\" :  \"D2\",\n  \"extractionFn\" : extraction_func\n  },\n  {\n  \"type\":\"lookup\",\n  \"dimension\":\"d3col\",\n  \"outputName\":\"D3\",\n  \"name\":\"my_lookup\"\n  }\n],\n...\n...\n\"subtotalsSpec\":[ [\"D1\", \"D2\", D3\"], [\"D1\", \"D3\"], [\"D3\"]],\n..\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Equivalent for Single Dimension Cardinality\nDESCRIPTION: SQL query demonstrating the equivalent operation for cardinality aggregator with a single dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/hll-old.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(DISTINCT(dimension)) FROM <datasource>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Live Row Stats in Apache Druid\nDESCRIPTION: Shows the API endpoint for retrieving live row statistics from a running task on a Peon. This is supported by non-parallel Native Batch Tasks, Hadoop batch tasks, and Kafka Indexing Service tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/rowStats\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Certificate Authentication in Apache Druid\nDESCRIPTION: This configuration table shows the settings for client certificate authentication in Apache Druid, including options for trust store configuration and certificate validation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/tls-support.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.server.https.requireClientCertificate`|If set to true, clients must identify themselves by providing a TLS certificate.  If `requireClientCertificate` is false, the rest of the options in this table are ignored.|false|no|\n|`druid.server.https.trustStoreType`|The type of the trust store containing certificates used to validate client certificates. Not needed if `requireClientCertificate` is false.|`java.security.KeyStore.getDefaultType()`|no|\n|`druid.server.https.trustStorePath`|The file path or URL of the trust store containing certificates used to validate client certificates. Not needed if `requireClientCertificate` is false.|none|yes, only if `requireClientCertificate` is true|\n|`druid.server.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate client certificate chains. Not needed if `requireClientCertificate` is false.|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.server.https.trustStorePassword`|The [Password Provider](../operations/password-provider.html) or String password for the Trust Store.  Not needed if `requireClientCertificate` is false.|none|no|\n|`druid.server.https.validateHostnames`|If set to true, check that the client's hostname matches the CN/subjectAltNames in the client certificate.  Not used if `requireClientCertificate` is false.|true|no|\n|`druid.server.https.crlPath`|Specifies a path to a file containing static [Certificate Revocation Lists](https://en.wikipedia.org/wiki/Certificate_revocation_list), used to check if a client certificate has been revoked. Not used if `requireClientCertificate` is false.|null|no|\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffeine Cache Properties\nDESCRIPTION: Configuration for the recommended Caffeine cache implementation, providing high-performance local caching with advanced eviction policies. Requires JRE8u60 or higher for optimal performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_39\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.type=caffeine\ndruid.cache.sizeInBytes=min(1GB, Runtime.maxMemory / 10)\ndruid.cache.expireAfter=None\ndruid.cache.cacheExecutorFactory=COMMON_FJP\ndruid.cache.evictOnClose=false\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Lookup ParseSpec in Apache Druid\nDESCRIPTION: Example JSON configuration for a CSV lookup parseSpec. This defines how to parse a CSV file where columns are specified along with key and value column mappings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"csv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Interval Load Rule Configuration in Druid\nDESCRIPTION: Defines segment replication for a specific time interval using ISO-8601 format. Specifies tier names and replica counts for the given interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByInterval\",\n  \"interval\": \"2012-01-01/2013-01-01\",\n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: FloatSum Aggregator Configuration in Druid\nDESCRIPTION: Computes sum of values as 32-bit floating point. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Performing Student's t-test on ArrayOfDoublesSketch Instances\nDESCRIPTION: Post-aggregator configuration to perform Student's t-test on two ArrayOfDoublesSketch instances and return p-values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchTTest\",\n  \"name\": <output name>,\n  \"fields\"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Field Accessor Post-Aggregator in Druid Query JSON\nDESCRIPTION: Shows two variations of field accessor post-aggregators: 'fieldAccess' for raw aggregation objects and 'finalizingFieldAccess' for finalized values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/post-aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"fieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"finalizingFieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kinesis Indexing Tasks in Druid\nDESCRIPTION: Formula for calculating the minimum worker capacity needed to handle concurrent reading and publishing tasks for Kinesis ingestion in Druid. This ensures sufficient resources are available for both task types to run simultaneously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Setting Task Priority in Druid Context Configuration\nDESCRIPTION: Demonstrates how to override the default task priority by setting a custom priority value in the task context. Priority values determine lock acquisition precedence, with higher numbers indicating higher priority.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/locking-and-priority.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"context\" : {\n  \"priority\" : 100\n}\n```\n\n----------------------------------------\n\nTITLE: Explaining Query Plan in Druid SQL\nDESCRIPTION: Example of using the EXPLAIN PLAN FOR command in Druid SQL to understand how a query will be executed, showing the internal query plan for a topN query on Wikipedia data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n\n PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\n DruidQueryRel(query=[{\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"page\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":10,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}],\"postAggregations\":[],\"context\":{},\"descending\":false}], signature=[{d0:STRING, a0:LONG}]) \n\nRetrieved 1 row in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: HDFS Deep Storage Basic Configuration\nDESCRIPTION: Configuration properties for setting up HDFS as deep storage in Druid, including storage type and directory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=<hdfs-directory>\n```\n\n----------------------------------------\n\nTITLE: Registered Lookup Extraction Function Configuration\nDESCRIPTION: Configuration for using registered cluster-wide lookups with options for handling missing values and optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"registeredLookup\",\n  \"lookup\":\"some_lookup_name\",\n  \"retainMissingValue\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Quantiles Sketch from ArrayOfDoublesSketch Column\nDESCRIPTION: Post-aggregator configuration to construct a quantiles DoublesSketch from a column of an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToQuantilesSketch\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"column\" : <number>,\n  \"k\" : <parameter that determines the accuracy and size of the quantiles sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Filtered GroupBy Query for Multi-value Dimensions\nDESCRIPTION: GroupBy query with a selector filter on the tags dimension to filter specific values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Selector Filter in Druid\nDESCRIPTION: Basic selector filter that matches a specific dimension with a specific value, equivalent to SQL WHERE clause with equality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"selector\", \"dimension\": <dimension_string>, \"value\": <dimension_value_string> }\n```\n\n----------------------------------------\n\nTITLE: SQL Query Syntax Structure for Apache Druid\nDESCRIPTION: The basic structure of a Druid SQL query, including optional EXPLAIN PLAN, WITH clause for common table expressions, and standard SELECT query components. This structure is translated into native Druid JSON queries on execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: SQL Query Syntax Structure for Apache Druid\nDESCRIPTION: The basic structure of a Druid SQL query, including optional EXPLAIN PLAN, WITH clause for common table expressions, and standard SELECT query components. This structure is translated into native Druid JSON queries on execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Spatial Dimensions in JSON Data Spec\nDESCRIPTION: Demonstrates how to specify spatial dimensions in a Hadoop-based JSON data specification. The example shows how to define a spatial dimension named 'coordinates' composed of 'lat' and 'long' dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/geo.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"hadoop\",\n\t\"dataSchema\": {\n\t\t\"dataSource\": \"DatasourceName\",\n\t\t\"parser\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"parseSpec\": {\n\t\t\t\t\"format\": \"json\",\n\t\t\t\t\"timestampSpec\": {\n\t\t\t\t\t\"column\": \"timestamp\",\n\t\t\t\t\t\"format\": \"auto\"\n\t\t\t\t},\n\t\t\t\t\"dimensionsSpec\": {\n\t\t\t\t\t\"dimensions\": [],\n\t\t\t\t\t\"spatialDimensions\": [{\n\t\t\t\t\t\t\"dimName\": \"coordinates\",\n\t\t\t\t\t\t\"dims\": [\"lat\", \"long\"]\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Spatial Dimensions in JSON Data Spec\nDESCRIPTION: Demonstrates how to specify spatial dimensions in a Hadoop-based JSON data specification. The example shows how to define a spatial dimension named 'coordinates' composed of 'lat' and 'long' dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/geo.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"hadoop\",\n\t\"dataSchema\": {\n\t\t\"dataSource\": \"DatasourceName\",\n\t\t\"parser\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"parseSpec\": {\n\t\t\t\t\"format\": \"json\",\n\t\t\t\t\"timestampSpec\": {\n\t\t\t\t\t\"column\": \"timestamp\",\n\t\t\t\t\t\"format\": \"auto\"\n\t\t\t\t},\n\t\t\t\t\"dimensionsSpec\": {\n\t\t\t\t\t\"dimensions\": [],\n\t\t\t\t\t\"spatialDimensions\": [{\n\t\t\t\t\t\t\"dimName\": \"coordinates\",\n\t\t\t\t\t\t\"dims\": [\"lat\", \"long\"]\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Float Max Aggregator in Druid\nDESCRIPTION: Computes maximum of metric values and Float.NEGATIVE_INFINITY. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Using Extraction Function with Selector Filter in Druid JSON\nDESCRIPTION: Illustrates how to combine a selector filter with an extraction function using a lookup map to transform input values before applying the filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Maven Build with Profiles\nDESCRIPTION: Complete Maven build command that generates source and binary distributions with signatures and checksums, runs license audits, and skips unit tests using multiple profiles.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/build.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Papache-release,dist,rat -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Selector Filter Example in Druid\nDESCRIPTION: Demonstrates how to use a selector filter within a Having clause to filter results based on specific dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/having.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : {\n              \"type\": \"selector\",\n              \"dimension\" : \"<dimension>\",\n              \"value\" : \"<dimension_value>\"\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container\nDESCRIPTION: Command to start the Hadoop container with necessary port mappings and volume mounts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Configuring Same Interval Merge Task in Druid\nDESCRIPTION: Defines a deprecated task that merges all segments within a specified interval. Simplifies the merge task by using an interval instead of explicitly listing segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"same_interval_merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"interval\": <DataSegment objects in this interval are going to be merged>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Raw CSV Data Format for Druid Ingestion\nDESCRIPTION: Example of raw CSV data that can be ingested into Druid. Each line contains a complete record with values in a specific order, without column headers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n2013-08-31T01:02:33Z,\"Gypsy Danger\",\"en\",\"nuclear\",\"true\",\"true\",\"false\",\"false\",\"article\",\"North America\",\"United States\",\"Bay Area\",\"San Francisco\",57,200,-143\n2013-08-31T03:32:45Z,\"Striker Eureka\",\"en\",\"speed\",\"false\",\"true\",\"true\",\"false\",\"wikipedia\",\"Australia\",\"Australia\",\"Cantebury\",\"Syndey\",459,129,330\n2013-08-31T07:11:21Z,\"Cherno Alpha\",\"ru\",\"masterYi\",\"false\",\"true\",\"true\",\"false\",\"article\",\"Asia\",\"Russia\",\"Oblast\",\"Moscow\",123,12,111\n2013-08-31T11:58:39Z,\"Crimson Typhoon\",\"zh\",\"triplets\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"China\",\"Shanxi\",\"Taiyuan\",905,5,900\n2013-08-31T12:41:27Z,\"Coyote Tango\",\"ja\",\"cancer\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"Japan\",\"Kanto\",\"Tokyo\",1,10,-9\n```\n\n----------------------------------------\n\nTITLE: Raw CSV Data Format for Druid Ingestion\nDESCRIPTION: Example of raw CSV data that can be ingested into Druid. Each line contains a complete record with values in a specific order, without column headers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n2013-08-31T01:02:33Z,\"Gypsy Danger\",\"en\",\"nuclear\",\"true\",\"true\",\"false\",\"false\",\"article\",\"North America\",\"United States\",\"Bay Area\",\"San Francisco\",57,200,-143\n2013-08-31T03:32:45Z,\"Striker Eureka\",\"en\",\"speed\",\"false\",\"true\",\"true\",\"false\",\"wikipedia\",\"Australia\",\"Australia\",\"Cantebury\",\"Syndey\",459,129,330\n2013-08-31T07:11:21Z,\"Cherno Alpha\",\"ru\",\"masterYi\",\"false\",\"true\",\"true\",\"false\",\"article\",\"Asia\",\"Russia\",\"Oblast\",\"Moscow\",123,12,111\n2013-08-31T11:58:39Z,\"Crimson Typhoon\",\"zh\",\"triplets\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"China\",\"Shanxi\",\"Taiyuan\",905,5,900\n2013-08-31T12:41:27Z,\"Coyote Tango\",\"ja\",\"cancer\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"Japan\",\"Kanto\",\"Tokyo\",1,10,-9\n```\n\n----------------------------------------\n\nTITLE: Setting Up Forever Drop Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a forever drop rule that indicates segments matching this rule should be permanently dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropForever\"  \n}\n```\n\n----------------------------------------\n\nTITLE: Starting the Apache Druid Historical Process Server\nDESCRIPTION: Command to launch the Historical process server in Apache Druid. This command starts the Historical process which is responsible for loading and serving segments from deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/historical.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server historical\n```\n\n----------------------------------------\n\nTITLE: Implementing Float Min Aggregator in Druid\nDESCRIPTION: Computes minimum of metric values and Float.POSITIVE_INFINITY. Takes output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Getting Specific Supervisor History GET Endpoint\nDESCRIPTION: REST endpoint to retrieve audit history of specifications for a specific supervisor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_18\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/supervisor/<supervisorId>/history\n```\n\n----------------------------------------\n\nTITLE: Results of a Basic GroupBy Query on Multi-value Dimensions\nDESCRIPTION: Example results from a GroupBy query on multi-value dimensions, showing how the original rows are exploded into multiple rows (one per tag value) and counts are aggregated accordingly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t1\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t2\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t3\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t4\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t5\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t6\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t7\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Lexicographic Bound Filter in Druid (JSON)\nDESCRIPTION: This example shows how to set up a bound filter using lexicographic ordering. It filters for name values between 'foo' and 'hoo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Data Set for Timestamp Min/Max Aggregation in Druid\nDESCRIPTION: Sample data set showing timestamp, dimension, and metric value format for demonstrating timestamp min/max aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T01:00:00.000Z  A  1\n2015-07-28T02:00:00.000Z  A  1\n2015-07-28T03:00:00.000Z  A  1\n2015-07-28T04:00:00.000Z  B  1\n2015-07-28T05:00:00.000Z  A  1\n2015-07-28T06:00:00.000Z  B  1\n2015-07-29T01:00:00.000Z  C  1\n2015-07-29T02:00:00.000Z  C  1\n2015-07-29T03:00:00.000Z  A  1\n2015-07-29T04:00:00.000Z  A  1\n```\n\n----------------------------------------\n\nTITLE: T-Test Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for post-aggregator to perform Student's t-test on two ArrayOfDoublesSketch instances\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchTTest\",\n  \"name\": <output name>,\n  \"fields\"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>,\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Password Provider Configuration Template in Druid\nDESCRIPTION: Template for configuring custom password provider implementations in Druid. Shows the basic structure required for custom provider configuration including type and additional Jackson properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/password-provider.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"<registered_password_provider_name>\", \"<jackson_property>\": \"<value>\", ... }\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy Query with Pacific Timezone\nDESCRIPTION: Example of a Druid GroupBy query using P1D (1 day) period granularity in Pacific timezone. Groups data by language and counts occurrences.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: TimeMax Aggregator Configuration\nDESCRIPTION: JSON configuration for timeMax aggregator during ingestion. Specifies the output name and source field for maximum timestamp calculation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMax\",\n    \"name\": \"tmax\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Task Logs in Druid\nDESCRIPTION: Configuration for storing task logs in Amazon S3. Requires the druid-s3-extensions extension to be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_15\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.s3Bucket=none\ndruid.indexer.logs.s3Prefix=none\n```\n\n----------------------------------------\n\nTITLE: Query Result Example\nDESCRIPTION: Sample response showing the results of a moving average query, including both the raw 30-minute delta and the calculated trailing average.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n   \"version\" : \"v1\",\n   \"timestamp\" : \"2015-09-12T00:30:00.000Z\",\n   \"event\" : {\n     \"delta30Min\" : 30490,\n     \"trailing30MinChanges\" : 4355.714285714285\n   }\n }, {\n   \"version\" : \"v1\",\n   \"timestamp\" : \"2015-09-12T01:00:00.000Z\",\n   \"event\" : {\n     \"delta30Min\" : 96526,\n     \"trailing30MinChanges\" : 18145.14285714286\n   }\n }]\n```\n\n----------------------------------------\n\nTITLE: Example Task Completion Report Response\nDESCRIPTION: Sample JSON response showing ingestion statistics including row counts and error information across different ingestion phases\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Approximate Quantile Calculation in Apache Druid SQL\nDESCRIPTION: Shows the usage of the APPROX_QUANTILE function for computing approximate quantiles on numeric or approxHistogram expressions. This function requires the approximate histogram extension to be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nAPPROX_QUANTILE(expr, probability, [resolution])\n```\n\n----------------------------------------\n\nTITLE: Disabling a Datasource in Druid Coordinator API\nDESCRIPTION: DELETE endpoint to disable an entire datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_4\n\nLANGUAGE: HTTP\nCODE:\n```\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Router Runtime Properties Configuration\nDESCRIPTION: Example runtime properties for a Druid Router, including service configuration, tier-to-broker mapping, connection settings, and thread configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\ndruid.host=#{IP_ADDR}:8080\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.defaultBrokerServiceName=druid:broker-cold\ndruid.router.coordinatorServiceName=druid:coordinator\ndruid.router.tierToBrokerMap={\"hot\":\"druid:broker-hot\",\"_default_tier\":\"druid:broker-cold\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the Router proxy http client\ndruid.router.http.numMaxThreads=100\n\ndruid.server.http.numThreads=100\n```\n\n----------------------------------------\n\nTITLE: Sending a Query to Druid Broker with Kerberos Authentication\nDESCRIPTION: Example command showing how to send a query stored in a JSON file to a Druid Broker using curl with Kerberos authentication. It uses cookie-based authentication for efficiency.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Caching in Apache Druid Historical Processes\nDESCRIPTION: This snippet presents configuration options for caching in Historical processes, including enabling/disabling cache and specifying uncacheable query types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_30\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.historical.cache.useCache`|true, false|Enable the cache on the Historical.|false|\n|`druid.historical.cache.populateCache`|true, false|Populate the cache on the Historical.|false|\n|`druid.historical.cache.unCacheable`|All druid query types|All query types to not cache.|[\"groupBy\", \"select\"]|\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Task Priority in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to override the default task priority by setting a custom priority value in the task context. The priority is set to 100, which is higher than the default priorities for most task types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/locking-and-priority.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"context\" : {\n  \"priority\" : 100\n}\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function with Length in Druid\nDESCRIPTION: Configuration for extracting a substring of specified length from dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 1, \"length\" : 4 }\n```\n\n----------------------------------------\n\nTITLE: DimensionsSpec Configuration with Type Definitions in JSON\nDESCRIPTION: Shows how to configure a dimensionsSpec with mixed data types including String, Long, and Float dimensions. This example also demonstrates how to disable bitmap indexing for specific string columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Caching in Apache Druid Historical Processes\nDESCRIPTION: This snippet presents configuration options for caching in Historical processes, including enabling/disabling cache and specifying uncacheable query types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_30\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.historical.cache.useCache`|true, false|Enable the cache on the Historical.|false|\n|`druid.historical.cache.populateCache`|true, false|Populate the cache on the Historical.|false|\n|`druid.historical.cache.unCacheable`|All druid query types|All query types to not cache.|[\"groupBy\", \"select\"]|\n```\n\n----------------------------------------\n\nTITLE: Configuring Exhibitor Integration\nDESCRIPTION: Properties for integrating Druid with Exhibitor, a supervisor system for ZooKeeper that enables dynamic scaling of ZooKeeper clusters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.exhibitor.service.hosts=none\ndruid.exhibitor.service.port=8080\ndruid.exhibitor.service.restUriPath=/exhibitor/v1/cluster/list\ndruid.exhibitor.service.useSsl=false\ndruid.exhibitor.service.pollingMs=10000\n```\n\n----------------------------------------\n\nTITLE: Defining Spatial Filter in Apache Druid Query\nDESCRIPTION: This snippet shows the JSON structure for defining a spatial filter in an Apache Druid query. It illustrates how to specify a rectangular bound for filtering spatial data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/geo.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\" : {\n    \"type\": \"spatial\",\n    \"dimension\": \"spatialDim\",\n    \"bound\": {\n        \"type\": \"rectangular\",\n        \"minCoords\": [10.0, 20.0],\n        \"maxCoords\": [30.0, 40.0]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Selector Filter in Query Filter HavingSpec\nDESCRIPTION: Demonstrates how to use a selector filter within a query filter HavingSpec to filter groupBy query results based on specific dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : {\n              \"type\": \"selector\",\n              \"dimension\" : \"<dimension>\",\n              \"value\" : \"<dimension_value>\"\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Supervisor Ingestion Stats in Druid\nDESCRIPTION: Use this GET endpoint to retrieve ingestion statistics for tasks managed by a supervisor, including current row counters and moving averages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nGET /druid/indexer/v1/supervisor/<supervisorId>/stats\n```\n\n----------------------------------------\n\nTITLE: Rendering Markdown Navigation Links for Druid Documentation\nDESCRIPTION: Markdown formatted list of links to miscellaneous Druid documentation sections including expressions language, papers and talks, and thanks pages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/toc.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Misc\n  * [Druid Expressions Language](/docs/VERSION/misc/math-expr.html)\n  * [Papers & Talks](/docs/VERSION/misc/papers-and-talks.html)\n  * [Thanks](/thanks.html)\n```\n\n----------------------------------------\n\nTITLE: IN Filter Example in Druid\nDESCRIPTION: IN filter for matching multiple possible dimension values, similar to SQL IN clause.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"in\",\n    \"dimension\": \"outlaw\",\n    \"values\": [\"Good\", \"Bad\", \"Ugly\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Grand Totals in Druid Timeseries Queries\nDESCRIPTION: Example of how to enable the grand totals feature in a Druid timeseries query by setting the 'grandTotal' property to true in the query context. This adds an additional row with aggregated totals across all time buckets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Filtered DimensionSpec on Multi-value Dimensions\nDESCRIPTION: A GroupBy query that combines a selector filter with a filtered dimensionSpec to only include specific values in the output. This technique is more efficient than using HAVING clauses for filtering multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"listFiltered\",\n      \"delegate\": {\n        \"type\": \"default\",\n        \"dimension\": \"tags\",\n        \"outputName\": \"tags\"\n      },\n      \"values\": [\"t3\"]\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Filter Example in Druid\nDESCRIPTION: Example of JavaScript filter that matches dimension values between 'bar' and 'foo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : \"name\",\n  \"function\" : \"function(x) { return(x >= 'bar' && x <= 'foo') }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Lookup DimensionSpec with Map Implementation\nDESCRIPTION: Configuration for lookup dimension specification using map implementation passed at query time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"replaceMissingValueWith\":\"missing_value\",\n  \"retainMissingValue\":false,\n  \"lookup\":{\"type\": \"map\", \"map\":{\"key\":\"value\"}, \"isOneToOne\":false}\n}\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: This JSON snippet shows how to include the DataSketches extension in the Druid configuration file. It specifies the extension to be loaded in the 'druid.extensions.loadList' array.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-extension.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Behavior Settings\nDESCRIPTION: Settings that control Zookeeper behavior including session timeout, compression, and ACL security options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.service.sessionTimeoutMs=30000\ndruid.zk.service.compress=true\ndruid.zk.service.acl=false\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React Scheduler\nDESCRIPTION: Copyright and license declaration for the React Scheduler production minified file, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Segment Announcement Configuration\nDESCRIPTION: Settings for configuring how segments are announced and unannounced in ZooKeeper using Curator. Controls segment batching and node sizes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_21\n\nLANGUAGE: properties\nCODE:\n```\ndruid.announcer.segmentsPerNode=50\ndruid.announcer.maxBytesPerNode=524288\ndruid.announcer.skipDimensionsAndMetrics=false\ndruid.announcer.skipLoadSpec=false\n```\n\n----------------------------------------\n\nTITLE: Configuring FloatMin Aggregator in Druid JSON\nDESCRIPTION: Defines a floatMin aggregator to compute the minimum of all metric values and Float.POSITIVE_INFINITY. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Nested JSON Data Structure Example\nDESCRIPTION: Example showing nested JSON data structure that needs to be flattened before ingestion into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\":{\"bar\": 3}}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter in Druid Queries\nDESCRIPTION: Defines the JSON structure for using a Bloom filter in Druid queries, including required and optional parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bloom\",\n  \"dimension\" : <dimension_name>,\n  \"bloomKFilter\" : <serialized_bytes_for_BloomKFilter>,\n  \"extractionFn\" : <extraction_fn>\n}\n```\n\n----------------------------------------\n\nTITLE: Nested JSON Data Structure Example\nDESCRIPTION: Example showing nested JSON data structure that needs to be flattened before ingestion into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\":{\"bar\": 3}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Intervals for Batch Ingestion in Druid\nDESCRIPTION: JSON configuration for time intervals in a Druid batch ingestion task. The intervals property defines the time range for data ingestion, where rows with timestamps outside this range will not be processed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"queryGranularity\" : \"MINUTE\",\n    \"intervals\" : [\"2018-01-01/2018-01-02\"],\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for Prism Syntax Highlighter\nDESCRIPTION: License declaration for Prism, a lightweight syntax highlighting library created by Lea Verou under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Querying for Ingested Event Count with LongSum Aggregator\nDESCRIPTION: JSON configuration showing how to properly query for the total number of ingested events using a longSum aggregator on the count field. This is important when using rollup, as a standard count aggregator would return the number of rows after rollup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-design.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n...\n\"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"numIngestedEvents\", \"fieldName\": \"count\" },\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Storage Properties\nDESCRIPTION: Essential configuration properties for connecting Druid to PostgreSQL metadata storage, including extension loading, connection URI, and authentication details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\"]\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Lookup DimensionSpec with External Lookup\nDESCRIPTION: Configuration for lookup-based dimension transformation using external lookup table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"name\":\"lookupName\"\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Number of Entries Post Aggregator\nDESCRIPTION: Post aggregator that returns the number of retained entries from an ArrayOfDoublesSketch. This metric indicates how many unique entries are currently stored in the sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToNumEntries\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing floatMax Aggregator in Druid\nDESCRIPTION: The floatMax aggregator computes the maximum of all metric values and Float.NEGATIVE_INFINITY. It requires name for the output and fieldName to specify which metric column to evaluate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Implementing Spatial Filter with Rectangular Bounds\nDESCRIPTION: Example of a spatial filter configuration using rectangular boundaries. The filter specifies minimum and maximum coordinates to define a rectangular geographic region.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/geo.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\" : {\n    \"type\": \"spatial\",\n    \"dimension\": \"spatialDim\",\n    \"bound\": {\n        \"type\": \"rectangular\",\n        \"minCoords\": [10.0, 20.0],\n        \"maxCoords\": [30.0, 40.0]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Native Batch Ingestion Task Configuration in Druid\nDESCRIPTION: A full example of an index task specification that combines all the previously shown components into a complete batch ingestion task configuration for Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Statistical Test Query Example in Druid\nDESCRIPTION: Comprehensive example showing how to combine zscore2sample and pvalue2tailedZtest post aggregators in a single query to perform statistical analysis.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n    \"postAggregations\" : {\n    \"type\"   : \"pvalue2tailedZtest\",\n    \"name\"   : \"pvalue\",\n    \"zScore\" : \n    {\n     \"type\"   : \"zscore2sample\",\n     \"name\"   : \"zscore\",\n     \"successCount1\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation1Sample\",\n         \"value\"  : 300\n       },\n     \"sample1Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation1\",\n         \"value\"  : 500\n       },\n     \"successCount2\":\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation2Sample\",\n         \"value\"  : 450\n       },\n     \"sample2Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation2\",\n         \"value\"  : 600\n       }\n     }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Lookup Parse Specification in Druid\nDESCRIPTION: JSON configuration for CSV-based lookup parsing in Druid, defining columns, key column, and value column for the lookup data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"csv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Day of Week Extraction Example in Druid JSON\nDESCRIPTION: Example extraction function that returns the day of the week for Montral in French by extracting from the __time dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : \"__time\",\n  \"outputName\" :  \"dayOfWeek\",\n  \"extractionFn\" : {\n    \"type\" : \"timeFormat\",\n    \"format\" : \"EEEE\",\n    \"timeZone\" : \"America/Montreal\",\n    \"locale\" : \"fr\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Inline Lookup Extraction Function Examples\nDESCRIPTION: Examples of inline lookup configurations with different settings for missing value handling and injectivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":true,\n  \"injective\":true\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":false,\n  \"injective\":false,\n  \"replaceMissingValueWith\":\"MISSING\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FloatMax Aggregator in Druid JSON\nDESCRIPTION: Defines a floatMax aggregator to compute the maximum of all metric values and Float.NEGATIVE_INFINITY. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring White-list Based Converter for Ambari Metrics Emitter in Druid\nDESCRIPTION: Configuration example for the 'whiteList' event converter implementation that selectively sends only white-listed metrics and dimensions to Ambari Metrics. This example specifies a custom map file path for the white list definition.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"appName\":\"druid\", \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple JSON Lookup in Druid\nDESCRIPTION: Example configuration for a simple JSON lookup in Druid using namespaceParseSpec. It uses the simpleJson format for parsing line-delimited JSON data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\":{\n  \"format\": \"simpleJson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy Query for Multi-value Dimensions\nDESCRIPTION: Example of a basic GroupBy query that aggregates data across the tags dimension with no filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Substrings with Regular Expressions in Druid SQL\nDESCRIPTION: Shows how to use REGEXP_EXTRACT function to apply a regular expression pattern and extract a capture group from a string expression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nREGEXP_EXTRACT(expr, pattern, [index])\n```\n\n----------------------------------------\n\nTITLE: Defining IN Filter in Apache Druid JSON Query\nDESCRIPTION: The IN filter matches a dimension against a set of values. It is equivalent to the SQL IN operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"in\",\n    \"dimension\": \"outlaw\",\n    \"values\": [\"Good\", \"Bad\", \"Ugly\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It adds the 'druid-datasketches' extension to the loadList.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-extension.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authenticator in Druid\nDESCRIPTION: Configuration properties for setting up a basic authenticator with initial admin and internal client passwords\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyBasicAuthenticator\"]\n\ndruid.auth.authenticator.MyBasicAuthenticator.type=basic\ndruid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1\ndruid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2\ndruid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring FloatFirst Aggregator in Druid JSON\nDESCRIPTION: Defines a floatFirst aggregator to compute the metric value with the minimum timestamp or 0 if no row exists. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Multi-Interval Segment Set in Druid\nDESCRIPTION: Illustrates a set of segments spanning multiple time intervals, each with their own version and partition numbers. This example demonstrates the base state before a partial update.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-changes.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v1_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Combining and Overwriting Data in Druid\nDESCRIPTION: Command to submit a task that combines existing data with new data and then overwrites the original data in the 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Config\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's configuration file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Time Boundary Router Strategy Configuration\nDESCRIPTION: JSON configuration for the timeBoundary router strategy which routes all timeBoundary queries to the highest priority Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"timeBoundary\"\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Segment Load Status in JSON (Historical API)\nDESCRIPTION: This JSON response indicates whether all segments in the local cache have been loaded. The 'cacheInitialized' key has a boolean value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\"cacheInitialized\":<value>}\n```\n\n----------------------------------------\n\nTITLE: Filtering for Null Values in Multi-value Dimensions in Apache Druid\nDESCRIPTION: Example of a selector filter that matches rows with null values in the 'tags' multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"tags\",\n  \"value\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Log Output from Azure Storage DataSegmentPuller in Druid\nDESCRIPTION: Example log output showing the segment loading process in a Druid Historical node when using Azure for deep storage. Shows the sequence of events when pulling a segment from Azure storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n2015-04-14T02:42:33,450 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.ZkCoordinator - New request[LOAD: dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00\n.000Z_2015-04-14T02:41:09.484Z] with zNode[/druid/dev/loadQueue/192.168.33.104:8081/dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.\n484Z].\n2015-04-14T02:42:33,451 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.ZkCoordinator - Loading segment dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.0\n00Z_2015-04-14T02:41:09.484Z\n2015-04-14T02:42:33,463 INFO [ZkCoordinator-0] org.apache.druid.guice.JsonConfigurator - Loaded class[class org.apache.druid.storage.azure.AzureAccountConfig] from props[drui\nd.azure.] as [org.apache.druid.storage.azure.AzureAccountConfig@759c9ad9]\n2015-04-14T02:49:08,275 INFO [ZkCoordinator-0] org.apache.druid.java.util.common.CompressionUtils - Unzipping file[/opt/druid/tmp/compressionUtilZipCache1263964429587449785.z\nip] to [/opt/druid/zk_druid/dde/2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z/2015-04-14T02:41:09.484Z/0]\n2015-04-14T02:49:08,276 INFO [ZkCoordinator-0] org.apache.druid.storage.azure.AzureDataSegmentPuller - Loaded 1196 bytes from [dde/2015-01-02T00:00:00.000Z_2015-01-03\nT00:00:00.000Z/2015-04-14T02:41:09.484Z/0/index.zip] to [/opt/druid/zk_druid/dde/2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z/2015-04-14T02:41:09.484Z/0]\n2015-04-14T02:49:08,277 WARN [ZkCoordinator-0] org.apache.druid.segment.loading.SegmentLoaderLocalCacheManager - Segment [dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.484Z] is different than expected size. Expected [0] found [1196]\n2015-04-14T02:49:08,282 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.BatchDataSegmentAnnouncer - Announcing segment[dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.484Z] at path[/druid/dev/segments/192.168.33.104:8081/192.168.33.104:8081_historical__default_tier_2015-04-14T02:49:08.282Z_7bb87230ebf940188511dd4a53ffd7351]\n2015-04-14T02:49:08,292 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.ZkCoordinator - Completed request [LOAD: dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.484Z]\n```\n\n----------------------------------------\n\nTITLE: Defining LIKE Filter in Apache Druid JSON Query\nDESCRIPTION: The LIKE filter supports basic wildcard searches, equivalent to the SQL LIKE operator. It uses '%' for any number of characters and '_' for a single character.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"like\",\n    \"dimension\": \"last_name\",\n    \"pattern\": \"D%\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variable Password Provider in Apache Druid\nDESCRIPTION: This JSON configuration shows how to use the environment variable password provider to securely fetch passwords from system environment variables rather than storing them in plaintext in configuration files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/password-provider.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" }\n```\n\n----------------------------------------\n\nTITLE: Creating a BloomKFilter in Java for Druid Queries\nDESCRIPTION: Example Java code to construct a BloomKFilter externally, serialize it to bytes, and encode it as a Base64 string for use in Druid queries. The filter is initialized with a capacity of 1500 elements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nBloomKFilter bloomFilter = new BloomKFilter(1500);\nbloomFilter.addString(\"value 1\");\nbloomFilter.addString(\"value 2\");\nbloomFilter.addString(\"value 3\");\nByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\nBloomKFilter.serialize(byteArrayOutputStream, bloomFilter);\nString base64Serialized = Base64.encodeBase64String(byteArrayOutputStream.toByteArray());\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Settings\nDESCRIPTION: TLS/SSL configuration properties for Druid's Jetty server and internal client communication, including keystore settings and cipher suite configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\ndruid.server.https.keyStorePath=none\ndruid.server.https.keyStoreType=none\ndruid.server.https.certAlias=none\ndruid.server.https.keyStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Thrift Ingestion with Tranquility in Apache Druid\nDESCRIPTION: JSON configuration for setting up realtime ingestion of Thrift data using tranquility in Apache Druid. The parser is configured for the Thrift format with a specific Thrift class and compact protocol.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSources\": [{\n    \"spec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"book\",\n        \"granularitySpec\": {          },\n        \"parser\": {\n          \"type\": \"thrift\",\n          \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n          \"protocol\": \"compact\",\n          \"parseSpec\": {\n            \"format\": \"json\",\n            ...\n          }\n        },\n        \"metricsSpec\": [...]\n      },\n      \"tuningConfig\": {...}\n    },\n    \"properties\": {...}\n  }],\n  \"properties\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Load Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period load rule, which specifies how many replicas of a segment should exist in different server tiers for a rolling time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true,\n  \"tieredReplicants\": {\n      \"hot\": 1,\n      \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Regex Extraction Function Configuration\nDESCRIPTION: Configuration for regex-based extraction function that returns the first matching group with optional missing value handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"regex\",\n  \"expr\" : <regular_expression>,\n  \"index\" : <group to extract, default 1>\n  \"replaceMissingValue\" : true,\n  \"replaceMissingValueWith\" : \"foobar\"\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a Search Query in Apache Druid\nDESCRIPTION: Example of a basic search query in Apache Druid that searches for dimension values containing 'Ke' (case insensitive) across the 'dim1' and 'dim2' dimensions. The query is filtered to a specific time range and uses lexicographic sorting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/searchquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"search\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"searchDimensions\": [\n    \"dim1\",\n    \"dim2\"\n  ],\n  \"query\": {\n    \"type\": \"insensitive_contains\",\n    \"value\": \"Ke\"\n  },\n  \"sort\" : {\n    \"type\": \"lexicographic\"\n  },\n  \"intervals\": [\n    \"2013-01-01T00:00:00.000/2013-01-03T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Emitter Module in Apache Druid\nDESCRIPTION: Properties for setting up the HTTP emitter module in Druid. This includes configurations for flushing data, authentication, batching strategy, and SSL/TLS settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_15\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.http.flushMillis=60000\ndruid.emitter.http.flushCount=500\ndruid.emitter.http.recipientBaseUrl=http://example.com/endpoint\ndruid.emitter.http.ssl.trustStorePath=/path/to/truststore\ndruid.emitter.http.ssl.trustStorePassword=password\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Variance Values Post Aggregator\nDESCRIPTION: Post aggregator that returns a list of variance values from an ArrayOfDoublesSketch. The result will be N double values corresponding to the number of values kept per key in the sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToVariances\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Historical and MiddleManager on Data Server\nDESCRIPTION: Launches the Druid Historical and MiddleManager processes on each Data server using Java commands.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical\njava `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Case Expression Format\nDESCRIPTION: Shows the syntax for case_searched and case_simple expressions in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/misc/math-expr.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncase_searched(expr1, result1, [[expr2, result2, ...], else-result])\ncase_simple(expr, value1, result1, [[value2, result2, ...], else-result])\n```\n\n----------------------------------------\n\nTITLE: Retrieving Supervisor Status with GET API in Druid\nDESCRIPTION: Use this GET endpoint to retrieve status report of a Kafka supervisor, including latest offsets, consumer lag per partition, and aggregate lag information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nGET /druid/indexer/v1/supervisor/<supervisorId>/status\n```\n\n----------------------------------------\n\nTITLE: Registering a Druid Module Example\nDESCRIPTION: Example of contents for the META-INF/services/org.apache.druid.initialization.DruidModule file needed to register a custom Druid module. This file should contain the fully qualified class name of your module implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\norg.apache.druid.storage.cassandra.CassandraDruidModule\n```\n\n----------------------------------------\n\nTITLE: Configuring Lower Case Extraction Function in Druid\nDESCRIPTION: Basic configuration for lower case extraction function without locale specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"lower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Grand Totals in Apache Druid Timeseries Query\nDESCRIPTION: This JSON query demonstrates how to enable the 'grand totals' feature in a timeseries query by adding the 'grandTotal' flag to the query context. This will include an extra row with totals across all time buckets in the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Forever Load Rule Configuration in Druid\nDESCRIPTION: Configures how many replicas of a segment should exist indefinitely in different server tiers. Requires specification of tier names and their replica counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Kinesis Supervisor Configuration JSON\nDESCRIPTION: Complete example of a Kinesis supervisor specification JSON that defines the ingestion schema, tuning configuration, and IO configuration for Kinesis data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kinesis\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kinesis\",\n    \"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kinesis\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"stream\": \"metrics\",\n    \"endpoint\": \"kinesis.us-east-1.amazonaws.com\",\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\",\n    \"recordsPerFetch\": 2000,\n    \"fetchDelayMillis\": 1000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Cyclical Moving Average in Druid\nDESCRIPTION: Query that demonstrates the cycle size parameter to calculate an average of every first 10-minutes of the last 3 hours. Uses 18 buckets with a cycle size of 6 to select specific time periods within each hour.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"movingAverage\",\n  \"dataSource\": \"wikipedia\",\n  \"granularity\": {\n    \"type\": \"period\",\n    \"period\": \"PT10M\"\n  },\n  \"intervals\": [\n    \"2015-09-12T00:00:00Z/2015-09-13T00:00:00Z\"\n  ],\n  \"aggregations\": [\n    {\n      \"name\": \"delta10Min\",\n      \"fieldName\": \"delta\",\n      \"type\": \"doubleSum\"\n    }\n  ],\n  \"averagers\": [\n    {\n      \"name\": \"trailing10MinPerHourChanges\",\n      \"fieldName\": \"delta10Min\",\n      \"type\": \"doubleMeanNoNulls\",\n      \"buckets\": 18,\n      \"cycleSize\": 6\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookup DimensionSpec with Map in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Lookup DimensionSpec with a map implementation in Apache Druid. It allows defining a lookup directly in the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"replaceMissingValueWith\":\"missing_value\",\n  \"retainMissingValue\":false,\n  \"lookup\":{\"type\": \"map\", \"map\":{\"key\":\"value\"}, \"isOneToOne\":false}\n}\n```\n\n----------------------------------------\n\nTITLE: ListFiltered DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for filtering multi-value dimensions using a whitelist or blacklist approach.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"listFiltered\", \"delegate\" : <dimensionSpec>, \"values\": <array of strings>, \"isWhitelist\": <optional attribute for true/false, default is true> }\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It adds the 'druid-datasketches' extension to the loadList.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-extension.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Coordinator Rules API\nDESCRIPTION: GET requests to retrieve rules for datasources in the Druid cluster. Includes endpoints for all rules, specific datasource rules, and rule history.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_7\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules\nGET /druid/coordinator/v1/rules/{dataSourceName}\nGET /druid/coordinator/v1/rules/{dataSourceName}?full\nGET /druid/coordinator/v1/rules/history?interval=<interval>\nGET /druid/coordinator/v1/rules/history?count=<n>\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?interval=<interval>\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Defining Greatest Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Shows the structure of a 'doubleGreatest' post-aggregator that computes the maximum of all specified fields and Double.NEGATIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"doubleGreatest\",\n  \"name\"  : <output_name>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Greatest Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Shows the structure of a 'doubleGreatest' post-aggregator that computes the maximum of all specified fields and Double.NEGATIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"doubleGreatest\",\n  \"name\"  : <output_name>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File Task Logs in Druid\nDESCRIPTION: Configuration for storing task logs in the local filesystem, specifying the directory path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_14\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.directory=log\n```\n\n----------------------------------------\n\nTITLE: Declaring React-DOM License in JavaScript\nDESCRIPTION: This snippet provides license information for the React-DOM production module version 17.0.2, which is under the MIT license and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Forever Load Rule Configuration in Druid\nDESCRIPTION: Configures how many replicas of a segment should exist indefinitely in different server tiers. Requires specification of tier names and their replica counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing stringLast Aggregator in Druid Queries\nDESCRIPTION: The stringLast aggregator computes the string metric value with the maximum timestamp or null if no row exists. Includes optional maxStringBytes parameter to limit string size and filterNullValues to exclude null values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It adds the 'druid-datasketches' extension to the loadList.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-extension.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Parametrized HTTP Emitter in Apache Druid\nDESCRIPTION: Configuration properties for the Parametrized HTTP Emitter module, which allows dynamic URL patterns based on event feeds.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_15\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=parametrized\ndruid.emitter.parametrized.httpEmitting.flushMillis=60000\ndruid.emitter.parametrized.httpEmitting.flushCount=500\ndruid.emitter.parametrized.recipientBaseUrlPattern=http://foo.bar/{feed}\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Settings\nDESCRIPTION: TLS/SSL configuration properties for Druid's Jetty server and internal client communication, including keystore settings and cipher suite configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\ndruid.server.https.keyStorePath=none\ndruid.server.https.keyStoreType=none\ndruid.server.https.certAlias=none\ndruid.server.https.keyStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleLast Aggregator in Druid JSON\nDESCRIPTION: Defines a doubleLast aggregator to compute the metric value with the maximum timestamp or 0 if no row exists. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Standard Deviation Post-Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for using the standard deviation post-aggregator in Apache Druid queries. Specifies the aggregator type, output name, field name, and estimator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"stddev\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"estimator\": <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Search Query Result Format in Apache Druid\nDESCRIPTION: Example of the result format returned by a search query in Apache Druid. The results are grouped by timestamp and contain dimension values that match the search criteria along with their count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/searchquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"Ke$ha\",\n        \"count\": 3\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"Ke$haForPresident\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"SomethingThatContainsKe\",\n        \"count\": 1\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"SomethingElseThatContainsKe\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Database for Druid\nDESCRIPTION: Command to create a PostgreSQL database named 'druid' that is owned by the previously created 'druid' user.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncreatedb druid -O druid\n```\n\n----------------------------------------\n\nTITLE: Enabling Grand Totals in Apache Druid Timeseries Query\nDESCRIPTION: This JSON query demonstrates how to enable the 'grand totals' feature in a timeseries query by adding the 'grandTotal' flag to the query context. This will include an extra row with totals across all time buckets in the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Greatest Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Shows the structure of a 'doubleGreatest' post-aggregator that computes the maximum of all specified fields and Double.NEGATIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"doubleGreatest\",\n  \"name\"  : <output_name>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing longMax Aggregator in Druid\nDESCRIPTION: The longMax aggregator computes the maximum of all metric values and Long.MIN_VALUE. It requires name for the output and fieldName to specify which metric column to evaluate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Response from Successful Druid Task Submission\nDESCRIPTION: JSON response returned by the Druid Overlord API after successfully submitting an ingestion task, containing the unique task ID.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n{\"task\":\"index_wikipedia_2018-06-09T21:30:32.802Z\"}\n```\n\n----------------------------------------\n\nTITLE: Including druid-rocketmq Extension in Druid\nDESCRIPTION: Instructions for including the druid-rocketmq extension in a Druid installation to enable integration with Apache RocketMQ.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/rocketmq.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo use this extension, make sure to [include](../../operations/including-extensions.html) `druid-rocketmq` extension.\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffeine Cache in Druid\nDESCRIPTION: Configuration options for the Caffeine cache implementation in Druid. This cache type is the default and offers better performance than the deprecated local cache. Settings include cache size, expiration, executor factory, and eviction behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_49\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.type=caffeine\ndruid.cache.sizeInBytes=min(1GB, Runtime.maxMemory / 10)\ndruid.cache.expireAfter=None (no time limit)\ndruid.cache.cacheExecutorFactory=COMMON_FJP\ndruid.cache.evictOnClose=false\n```\n\n----------------------------------------\n\nTITLE: Configuring StringLast Aggregator in Druid JSON\nDESCRIPTION: Defines a stringLast aggregator to compute the metric value with the maximum timestamp or null if no row exists. It includes optional parameters for maximum string bytes and null value filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Including Cassandra Storage Extension in Druid Configuration\nDESCRIPTION: Instructions for including the Cassandra storage extension in Apache Druid. This is a prerequisite for using Cassandra as deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/cassandra.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo use this Apache Druid (incubating) extension, make sure to [include](../../operations/including-extensions.html) `druid-cassandra-storage` extension.\n```\n\n----------------------------------------\n\nTITLE: Configuring IngestSegmentFirehose in Apache Druid\nDESCRIPTION: IngestSegmentFirehose reads data from existing Druid segments. It can be used to ingest existing segments using a new schema and change the name, dimensions, metrics, rollup, etc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/firehose.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"ingestSegment\",\n    \"dataSource\"   : \"wikipedia\",\n    \"interval\" : \"2013-01-01/2013-01-02\"\n}\n```\n\n----------------------------------------\n\nTITLE: Forever Drop Rule Configuration in Druid\nDESCRIPTION: Simple configuration to permanently drop segments that match this rule from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropForever\"  \n}\n```\n\n----------------------------------------\n\nTITLE: Druid Query Error Response Format\nDESCRIPTION: Example JSON structure returned when a Druid query fails. The response includes error code, message, exception class, and the host where the error occurred.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/querying.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\" : \"Query timeout\",\n  \"errorMessage\" : \"Timeout waiting for task.\",\n  \"errorClass\" : \"java.util.concurrent.TimeoutException\",\n  \"host\" : \"druid1.example.com:8083\"\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Segment Metrics by Datasource\nDESCRIPTION: SQL query to calculate total size, average size, average number of rows and segment count grouped by datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    datasource,\n    SUM(\"size\") AS total_size,\n    CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size,\n    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,\n    COUNT(*) AS num_segments\nFROM sys.segments\nGROUP BY 1\nORDER BY 2 DESC\n```\n\n----------------------------------------\n\nTITLE: Druid Query Error Response Format\nDESCRIPTION: Example JSON structure returned when a Druid query fails. The response includes error code, message, exception class, and the host where the error occurred.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/querying.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\" : \"Query timeout\",\n  \"errorMessage\" : \"Timeout waiting for task.\",\n  \"errorClass\" : \"java.util.concurrent.TimeoutException\",\n  \"host\" : \"druid1.example.com:8083\"\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Segments by Time Interval in Druid\nDESCRIPTION: cURL command to disable segments within a specific time interval using the Coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d '{ \"interval\" : \"2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z\" }' http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused\n```\n\n----------------------------------------\n\nTITLE: Configuring QuantilesDoublesSketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketch aggregator which builds a sketch from raw data or reads existing sketches from segments. The k parameter controls accuracy and size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"quantilesDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"k\": <parameter that controls size and accuracy>\n }\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Configuration for Druid Coordinator\nDESCRIPTION: Example JSON configuration object for Druid Coordinator. This includes settings for segment deletion, merging limits, replication parameters, and node decommissioning options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"millisToWaitBeforeDeleting\": 900000,\n  \"mergeBytesLimit\": 100000000,\n  \"mergeSegmentsLimit\" : 1000,\n  \"maxSegmentsToMove\": 5,\n  \"replicantLifetime\": 15,\n  \"replicationThrottleLimit\": 10,\n  \"emitBalancingStats\": false,\n  \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"],\n  \"decommissioningNodes\": [\"localhost:8182\", \"localhost:8282\"],\n  \"decommissioningMaxPercentOfMaxSegmentsToMove\": 70\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Client Properties in Druid\nDESCRIPTION: HTTP client configuration for Druid Broker to communicate with Historical servers and real-time tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_44\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.http.numConnections=20\ndruid.broker.http.compressionCodec=gzip\ndruid.broker.http.readTimeout=PT15M\ndruid.broker.http.unusedConnectionTimeout=PT4M\ndruid.broker.http.maxQueuedBytes=0\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Ingestion Spec Structure\nDESCRIPTION: Demonstrates the basic structure of a Druid ingestion specification with its three main components: dataSchema, ioConfig, and tuningConfig.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Router Process\nDESCRIPTION: Command for starting the Druid Router process from the command line.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server router\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid HTTP Endpoints with Curl and Kerberos Authentication\nDESCRIPTION: curl command that uses SPNEGO negotiation to authenticate with Kerberos-protected Druid endpoints. This command stores authentication cookies for subsequent requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json' <HTTP_END_POINT>\n```\n\n----------------------------------------\n\nTITLE: Submitting Druid Ingestion Task via Command Line\nDESCRIPTION: Command to submit an ingestion task specification that creates a datasource called 'retention-tutorial' with hourly segments from Wikipedia edits data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-retention.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/retention-index.json\n```\n\n----------------------------------------\n\nTITLE: Sample InfluxDB Line Protocol Format\nDESCRIPTION: Example of the InfluxDB Line Protocol format showing the structure with measurement, tags, field values, and timestamp. The format includes cpu metrics with application and host tags, along with usage metrics and a nanosecond timestamp.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000\n```\n\n----------------------------------------\n\nTITLE: Internal System User Interface\nDESCRIPTION: Java interface showing the required methods for implementing internal system user authentication handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/auth.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic HttpClient createEscalatedClient(HttpClient baseClient);\n\npublic org.eclipse.jetty.client.HttpClient createEscalatedJettyClient(org.eclipse.jetty.client.HttpClient baseClient);\n\npublic AuthenticationResult createEscalatedAuthenticationResult();\n```\n\n----------------------------------------\n\nTITLE: Complete Maven Build Command with Distribution and Release Profiles\nDESCRIPTION: Advanced Maven build command that generates source and binary distributions with signatures and checksums, runs license audits, and skips unit tests for faster build times.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/build.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Papache-release,dist,rat -DskipTests\n```\n\n----------------------------------------\n\nTITLE: POST Request for Rules Management\nDESCRIPTION: HTTP POST endpoint for updating rules with optional audit header parameters\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/coordinator/v1/rules/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Using Greater Than Filter in Druid groupBy Query\nDESCRIPTION: Shows how to use a greaterThan filter in a having clause to match rows with aggregate values greater than the given value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"greaterThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CombiningFirehose in Apache Druid\nDESCRIPTION: CombiningFirehose configuration for combining and merging data from multiple firehoses. This allows data to be ingested from different sources and merged together in a single ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"combining\",\n    \"delegates\" : [ { firehose1 }, { firehose2 }, ..... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Jetty Server TLS Properties in Apache Druid\nDESCRIPTION: Provides advanced TLS/SSL configuration options for the Jetty server in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.https.keyManagerFactoryAlgorithm=javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()\ndruid.server.https.keyManagerPassword=none\ndruid.server.https.includeCipherSuites=Jetty's default include cipher list\ndruid.server.https.excludeCipherSuites=Jetty's default exclude cipher list\ndruid.server.https.includeProtocols=Jetty's default include protocol list\ndruid.server.https.excludeProtocols=Jetty's default exclude protocol list\n```\n\n----------------------------------------\n\nTITLE: Arbitrary Granularity Spec Configuration Fields\nDESCRIPTION: JSON configuration fields for Arbitrary Granularity Spec which generates segments with arbitrary intervals. Includes fields for query granularity, rollup settings and intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": \"JSON string array\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Regex ParseSpec in Druid\nDESCRIPTION: Configuration for the parseSpec to ingest data using a regular expression pattern in Druid. Specifies the format, timestamp column, pattern for matching, columns, and dimensions to extract.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"regex\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [<your_list_of_dimensions>]\n    },\n    \"columns\" : [<your_columns_here>],\n    \"pattern\" : <regex pattern for partitioning data>\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring OrderByColumnSpec in Druid GroupBy Queries\nDESCRIPTION: Defines the structure for OrderByColumnSpec which specifies how to order results by columns. Supports dimension name, sort direction, and dimension ordering type configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/limitspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dimension\" : \"<Any dimension or metric name>\",\n    \"direction\" : <\"ascending\"|\"descending\">,\n    \"dimensionOrder\" : <\"lexicographic\"(default)|\"alphanumeric\"|\"strlen\"|\"numeric\">\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Time Boundary Query in Druid JSON\nDESCRIPTION: A JSON query template for retrieving the earliest and latest timestamps from a Druid dataset. This query supports optional parameters for filtering and returning only the minimum or maximum timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"timeBoundary\",\n    \"dataSource\": \"sample_datasource\",\n    \"bound\"     : < \"maxTime\" | \"minTime\" > # optional, defaults to returning both timestamps if not set \n    \"filter\"    : { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] } # optional\n}\n```\n\n----------------------------------------\n\nTITLE: Broker Process Configuration Properties in Druid\nDESCRIPTION: Basic configuration properties for Druid Broker processes including host, port, and service settings. These configurations define how the Broker advertises itself and what ports it listens on.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_35\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8082\ndruid.tlsPort=8282\ndruid.service=druid/broker\n```\n\n----------------------------------------\n\nTITLE: Theta Sketch Set Operations Post Aggregator\nDESCRIPTION: Post aggregator that performs set operations (UNION, INTERSECT, NOT) on Theta sketches. This allows for complex cardinality calculations like unique users across multiple conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchSetOp\",\n  \"name\": <output name>,\n  \"func\": <UNION|INTERSECT|NOT>,\n  \"fields\"  : <array of fieldAccess type post aggregators to access the thetaSketch aggregators or thetaSketchSetOp type post aggregators to allow arbitrary combination of set operations>,\n  \"size\": <16384 by default, must be max of size from sketches in fields input>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ThetaSketch Set Operation Post-Aggregator\nDESCRIPTION: Illustrates the configuration of a thetaSketchSetOp post-aggregator for performing set operations (union, intersect, not) on Theta sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchSetOp\",\n  \"name\": <output name>,\n  \"func\": <UNION|INTERSECT|NOT>,\n  \"fields\"  : <array of fieldAccess type post aggregators to access the thetaSketch aggregators or thetaSketchSetOp type post aggregators to allow arbitrary combination of set operations>,\n  \"size\": <16384 by default, must be max of size from sketches in fields input>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OrderByColumnSpec in Druid GroupBy Queries\nDESCRIPTION: Defines the structure for OrderByColumnSpec which specifies how to order results by columns. Supports dimension name, sort direction, and dimension ordering type configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/limitspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dimension\" : \"<Any dimension or metric name>\",\n    \"direction\" : <\"ascending\"|\"descending\">,\n    \"dimensionOrder\" : <\"lexicographic\"(default)|\"alphanumeric\"|\"strlen\"|\"numeric\">\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Logic with CASE in Druid SQL\nDESCRIPTION: Demonstrates the use of simple and searched CASE statements for conditional logic in Druid SQL queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCASE expr WHEN value1 THEN result1 [ WHEN value2 THEN result2 ... ] [ ELSE resultN ] END\nCASE WHEN boolean_expr1 THEN result1 [ WHEN boolean_expr2 THEN result2 ... ] [ ELSE resultN ] END\n```\n\n----------------------------------------\n\nTITLE: Executing Timeseries Query in Apache Druid SQL\nDESCRIPTION: This SQL query demonstrates a timeseries analysis, calculating the sum of deleted lines per hour for Wikipedia edits between 2015-09-12 and 2015-09-13.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY FLOOR(__time to HOUR);\n```\n\n----------------------------------------\n\nTITLE: Sample Data Dump Output Format in JSON\nDESCRIPTION: Example of a single row output from the DumpSegment tool showing various fields and their values in JSON format. This represents one line of the dump output when using the default row dump mode.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/dump-segment.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__time\": 1442018818771,\n  \"added\": 36,\n  \"channel\": \"#en.wikipedia\",\n  \"cityName\": null,\n  \"comment\": \"added project\",\n  \"count\": 1,\n  \"countryIsoCode\": null,\n  \"countryName\": null,\n  \"deleted\": 0,\n  \"delta\": 36,\n  \"isAnonymous\": \"false\",\n  \"isMinor\": \"false\",\n  \"isNew\": \"false\",\n  \"isRobot\": \"false\",\n  \"isUnpatrolled\": \"false\",\n  \"iuser\": \"00001553\",\n  \"metroCode\": null,\n  \"namespace\": \"Talk\",\n  \"page\": \"Talk:Oswald Tilghman\",\n  \"regionIsoCode\": null,\n  \"regionName\": null,\n  \"user\": \"GELongstreet\"\n}\n```\n\n----------------------------------------\n\nTITLE: Buckets Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the buckets post-aggregator, which computes a visual representation with specified bucket size and offset. Not supported for fixed buckets histogram.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"buckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"bucketSize\": <bucket_size>,\n  \"offset\": <offset>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimensions Without Rollup in Druid\nDESCRIPTION: This snippet shows how to configure dimensions when not using rollup in Druid. All columns are specified in the dimensionsSpec, including metric columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Virtual Columns in a Druid Scan Query\nDESCRIPTION: This snippet demonstrates how to include virtual columns in a Druid scan query. It defines two virtual columns: 'fooPage' concatenating 'foo' with the 'page' column, and 'tripleWordCount' multiplying the 'wordCount' column by 3.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/virtual-columns.md#2025-04-09_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n \"queryType\": \"scan\",\n \"dataSource\": \"page_data\",\n \"columns\":[],\n \"virtualColumns\": [\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\",\n      \"outputType\": \"STRING\"\n    },\n    {\n      \"type\": \"expression\",\n      \"name\": \"tripleWordCount\",\n      \"expression\": \"wordCount * 3\",\n      \"outputType\": \"LONG\"\n    }\n  ],\n \"intervals\": [\n   \"2013-01-01/2019-01-02\"\n ] \n}\n```\n\n----------------------------------------\n\nTITLE: Prefix Filtered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering dimension values that start with a specific prefix within multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"prefixFiltered\", \"delegate\" : <dimensionSpec>, \"prefix\": <prefix string> }\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function with Injective Property\nDESCRIPTION: A JavaScript extraction function that appends '!!!' to a string with the injective property set to true, indicating that the function preserves uniqueness in the transformed values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str + '!!!'; }\",\n  \"injective\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Router JavaScript Strategy Configuration\nDESCRIPTION: Example of a JavaScript-based routing strategy that routes queries with 3+ aggregators to low priority brokers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying System Schema for Segment Metrics in Apache Druid SQL\nDESCRIPTION: SQL query that retrieves metrics about published segments from the system schema, including average row count, segment size, and total counts. This query helps determine if segments need compaction by analyzing their current characteristics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/segment-optimization.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  \"start\",\n  \"end\",\n  version,\n  COUNT(*) AS num_segments,\n  AVG(\"num_rows\") AS avg_num_rows,\n  SUM(\"num_rows\") AS total_num_rows,\n  AVG(\"size\") AS avg_size,\n  SUM(\"size\") AS total_size\nFROM\n  sys.segments A\nWHERE\n  datasource = 'your_dataSource' AND\n  is_published = 1\nGROUP BY 1, 2, 3\nORDER BY 1, 2, 3 DESC;\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Logging in Druid\nDESCRIPTION: Basic configuration properties for task logging in Druid, including the storage type selection for task logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.type=file\n```\n\n----------------------------------------\n\nTITLE: Configuring timeMin Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for including a timeMin aggregator during data ingestion. The fieldName parameter typically references the column specified in the timestamp spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMin\",\n    \"name\": \"tmin\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Jetty Server for Druid Broker (Properties)\nDESCRIPTION: HTTP server configuration properties for the Broker, including thread counts, queue size, and timeout settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_30\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.http.numThreads=max(10, (Number of cores * 17) / 16 + 2) + 30\ndruid.server.http.queueSize=Unbounded\ndruid.server.http.maxIdleTime=PT5M\ndruid.server.http.enableRequestLimit=false\ndruid.server.http.defaultQueryTimeout=300000\ndruid.server.http.maxScatterGatherBytes=Long.MAX_VALUE\ndruid.server.http.gracefulShutdownTimeout=PT0S\ndruid.server.http.unannouncePropagationDelay=PT0S\ndruid.server.http.maxQueryTimeout=Long.MAX_VALUE\ndruid.server.http.maxRequestHeaderSize=8 * 1024\n```\n\n----------------------------------------\n\nTITLE: Defining Arithmetic Post-Aggregator in Druid\nDESCRIPTION: JSON structure for an arithmetic post-aggregator that applies a mathematical function to given fields. It supports functions like +, -, *, /, and quotient. The optional 'ordering' parameter defines the sort order of resulting values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arithmetic\",\n  \"name\"  : <output_name>,\n  \"fn\"    : <arithmetic_function>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...],\n  \"ordering\" : <null (default), or \"numericFirst\">\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Load Queue Peon Properties in Apache Druid\nDESCRIPTION: This markdown table defines additional configuration properties for the HTTP load queue peon implementation in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.coordinator.loadqueuepeon.http.batchSize`|Number of segment load/drop requests to batch in one HTTP request. Note that it must be smaller than `druid.segmentCache.numLoadingThreads` config on Historical process.|1|\n```\n\n----------------------------------------\n\nTITLE: Day of Week Filter in Druid\nDESCRIPTION: Demonstrates filtering on day of week using time format extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"Friday\",\n  \"extractionFn\": {\n    \"type\": \"timeFormat\",\n    \"format\": \"EEEE\",\n    \"timeZone\": \"America/New_York\",\n    \"locale\": \"en\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Query Result for GroupBy with Daily Period in Pacific Time Zone\nDESCRIPTION: This snippet shows the result of the groupBy query with daily period granularity in Pacific time zone. It demonstrates how the data is aggregated into daily buckets, with timestamps adjusted to the specified time zone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 2,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for NProgress\nDESCRIPTION: Specifies the MIT license for the NProgress library created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Quantile Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the quantile post-aggregator, which computes a single quantile based on the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantile\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probability\" : <quantile> }\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL User for Druid\nDESCRIPTION: Command to create a PostgreSQL user named 'druid' with password prompt for Druid metadata storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncreateuser druid -P\n```\n\n----------------------------------------\n\nTITLE: Strlen Extraction Function Implementation in JSON\nDESCRIPTION: A simple extraction function that returns the length of dimension values measured in Unicode code units. Handles null strings as zero length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"strlen\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring LongMax Aggregator in Druid JSON\nDESCRIPTION: Defines a longMax aggregator to compute the maximum of all metric values and Long.MIN_VALUE. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Constant Post-Aggregator in Druid\nDESCRIPTION: JSON structure for a constant post-aggregator that always returns the specified numerical value, useful in calculations that require fixed values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Basic Theta Sketch Aggregator Example\nDESCRIPTION: Example configuration of a Theta sketch aggregator for unique user counting. This is the basic setup for indexing data with the sketch aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"thetaSketch\", \"name\": \"user_id_sketch\", \"fieldName\": \"user_id\" }\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Declaration\nDESCRIPTION: License declaration for React's scheduler.production.min.js under MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Processing Properties for Druid Broker\nDESCRIPTION: This snippet defines various processing-related configuration properties for the Druid Broker, including buffer sizes, thread counts, and caching options. It also includes a note about direct memory requirements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_45\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.processing.buffer.sizeBytes`|This specifies a buffer size for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.|auto (max 1GB)|\n|`druid.processing.buffer.poolCacheMaxCount`|processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.|Integer.MAX_VALUE|\n|`druid.processing.formatString`|Realtime and Historical processes use this format string to name their processing threads.|processing-%s|\n|`druid.processing.numMergeBuffers`|The number of direct memory buffers available for merging query results. The buffers are sized by `druid.processing.buffer.sizeBytes`. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.|`max(2, druid.processing.numThreads / 4)`|\n|`druid.processing.numThreads`|The number of processing threads to have available for parallel processing of segments. Our rule of thumb is `num_cores - 1`, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value `1`.|Number of cores - 1 (or 1)|\n|`druid.processing.columnCache.sizeBytes`|Maximum size in bytes for the dimension value lookup cache. Any value greater than `0` enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.|`0` (disabled)|\n|`druid.processing.fifo`|If the processing queue should treat tasks of equal priority in a FIFO manner|`false`|\n|`druid.processing.tmpDir`|Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default `java.io.tmpdir` path.|path represented by `java.io.tmpdir`|\n```\n\n----------------------------------------\n\nTITLE: Configuring SimpleJSON Lookup Parse Specification in Druid\nDESCRIPTION: JSON configuration for simple JSON lookup parsing in Druid, which processes line-delimited JSON objects where field name is the key and field value is the value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\":{\n  \"format\": \"simpleJson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Router JavaScript Strategy Configuration\nDESCRIPTION: Example of a JavaScript-based routing strategy that routes queries with 3+ aggregators to low-priority brokers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/router.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Caching Configuration for Realtime Queries in YAML\nDESCRIPTION: Configuration properties for enabling and controlling caching in Druid's Realtime Process. These settings determine if and how query results are cached, which query types can use caching, and cache entry size limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/realtime.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.realtime.cache.useCache: false\ndruid.realtime.cache.populateCache: false\ndruid.realtime.cache.unCacheable: [\"select\"]\ndruid.realtime.cache.maxEntrySize: 1000000\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function without Length in Druid\nDESCRIPTION: Configuration for extracting a substring from a specified index to the end of the dimension value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 3 }\n```\n\n----------------------------------------\n\nTITLE: Interval Filter in Druid\nDESCRIPTION: Example of an interval filter for filtering time ranges in October and November 2014.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a View Query in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates a 'view' query type that wraps a groupBy query. The Druid engine will optimize the execution by selecting the most efficient materialized view that satisfies the query requirements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"view\",\n    \"query\": {\n        \"queryType\": \"groupBy\",\n        \"dataSource\": \"wikiticker\",\n        \"granularity\": \"all\",\n        \"dimensions\": [\n            \"user\"\n        ],\n        \"limitSpec\": {\n            \"type\": \"default\",\n            \"limit\": 1,\n            \"columns\": [\n                {\n                    \"dimension\": \"added\",\n                    \"direction\": \"descending\",\n                    \"dimensionOrder\": \"numeric\"\n                }\n            ]\n        },\n        \"aggregations\": [\n            {\n                \"type\": \"longSum\",\n                \"name\": \"added\",\n                \"fieldName\": \"added\"\n            }\n        ],\n        \"intervals\": [\n            \"2015-09-12/2015-09-13\"\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Storage for Historical in YAML\nDESCRIPTION: YAML configuration for segment storage in Historical processes, including cache locations and deletion settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_38\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.segmentCache.locations:\n  - path: \"/mnt/druidSegments\"\n    maxSize: 10000\n    freeSpacePercent: 1.0\ndruid.segmentCache.deleteOnRemove: true\ndruid.segmentCache.dropSegmentDelayMillis: 30000\ndruid.segmentCache.infoDir: ${first_location}/info_dir\ndruid.segmentCache.announceIntervalMillis: 5000\ndruid.segmentCache.numLoadingThreads: ${Number of cores}\ndruid.coordinator.loadqueuepeon.curator.numCallbackThreads: 2\n```\n\n----------------------------------------\n\nTITLE: Querying INFORMATION_SCHEMA in Druid SQL\nDESCRIPTION: Example SQL query demonstrating how to retrieve metadata for a specific Druid datasource using the INFORMATION_SCHEMA tables. This query returns column information for a table named 'foo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'\n```\n\n----------------------------------------\n\nTITLE: Scan Query Results in Compacted List Format\nDESCRIPTION: This JSON structure demonstrates the result format when 'resultFormat' is set to 'compactedList'. It includes segment information, column names, and event data as arrays of values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/scan-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\"\n    ],\n    \"events\" : [\n     [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._243\", \"en\", \"1\", \"MZMcBride\", 1.0, 77.0, 77.0, 77.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._73\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._756\", \"en\", \"1\", \"MZMcBride\", 1.0, 68.0, 68.0, 68.0, 0.0]\n    ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Worker History Audit API Endpoint\nDESCRIPTION: HTTP endpoint for retrieving worker configuration audit history with interval parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_32\n\nLANGUAGE: text\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker/history?interval=<interval>\n```\n\n----------------------------------------\n\nTITLE: Multi-Interval Segment Example - Version 1\nDESCRIPTION: Illustrates segment naming across multiple time intervals with version 1.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v1_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Running Druid Batch Ingestion Using Helper Script\nDESCRIPTION: Command to execute the post-index-task helper script that submits the ingestion task to Druid overlord and monitors its progress until completion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index.json\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Header\nDESCRIPTION: Copyright notice and MIT license declaration for React's scheduler.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Origin for Granularity in GroupBy Query\nDESCRIPTION: This snippet demonstrates how to set a custom origin for the granularity in a groupBy query. It shows the modification to the granularity object to include an origin timestamp, which affects how the data is bucketed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Using Bound Filter for String Range in Apache Druid\nDESCRIPTION: This example demonstrates a bound filter that expresses the condition foo <= name <= hoo using the default lexicographic sorting order for string comparison.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Coordinator and Overlord Unified Process\nDESCRIPTION: Configuration property to enable running Coordinator and Overlord as a combined unified process. This setting allows for simpler deployment in smaller clusters where resource contention is not a concern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/processes.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.coordinator.asOverlord.enabled\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Historical Node Properties\nDESCRIPTION: These properties configure Druid historical nodes, including network settings, service name, and general configuration options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_23\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.plaintextPort=8083\ndruid.tlsPort=8283\ndruid.service=druid/historical\ndruid.server.maxSize=0\ndruid.server.tier=_default_tier\ndruid.server.priority=0\n```\n\n----------------------------------------\n\nTITLE: Expression Transform Syntax in Druid\nDESCRIPTION: Shows the syntax for an expression transform in Druid, which allows creating new fields based on expressions applied to input rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <output field name>,\n  \"expression\": <expr>\n}\n```\n\n----------------------------------------\n\nTITLE: Running Druid Peon CLI Command\nDESCRIPTION: Command line syntax for running a Peon instance directly. Takes a task file containing JSON task specification and a status file path for output. This is primarily used for development purposes rather than production.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/peons.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main internal peon <task_file> <status_file>\n```\n\n----------------------------------------\n\nTITLE: Configuring Scan Query Context Parameters in Druid\nDESCRIPTION: Example JSON configuration showing how to override default scan query settings. Demonstrates setting custom values for maxRowsQueuedForOrdering and maxSegmentPartitionsOrderedInMemory parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/scan-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"maxRowsQueuedForOrdering\": 100001,\n  \"maxSegmentPartitionsOrderedInMemory\": 100\t\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring zscore2sample Post Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the zscore2sample post aggregator which calculates z-score using two-sample z-test. It converts binary variables to continuous variables and compares proportions between two samples.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"zscore2sample\",\n  \"name\": \"<output_name>\",\n  \"successCount1\": <post_aggregator> success count of sample 1,\n  \"sample1Size\": <post_aggregaror> sample 1 size,\n  \"successCount2\": <post_aggregator> success count of sample 2,\n  \"sample2Size\" : <post_aggregator> sample 2 size\n}\n```\n\n----------------------------------------\n\nTITLE: Strict Bound Filter in Druid\nDESCRIPTION: Example of a bound filter with strict comparisons expressing 21 < age < 31.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"lowerStrict\": true,\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Strlen Extraction Function in Druid JSON\nDESCRIPTION: Returns the length of dimension values in Unicode code units. Null strings are considered as having zero length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"strlen\" }\n```\n\n----------------------------------------\n\nTITLE: Implementing FloatMax Aggregator in Apache Druid\nDESCRIPTION: Shows the setup for a floatMax aggregator in Druid. This aggregator computes the maximum of all metric values and Float.NEGATIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Viewing Combined Data in Druid\nDESCRIPTION: SQL query showing the result after combining existing and new data, demonstrating how roll-up has occurred for the 'lion' row which now has a count of 2.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          2     400 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n 2018-01-01T05:01:00.000Z  mongoose      1     737 \n 2018-01-01T06:01:00.000Z  snake         1    1234 \n 2018-01-01T07:01:00.000Z  octopus       1     115 \n\nRetrieved 6 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: ISO8601 Period Pattern\nDESCRIPTION: Examples of ISO8601 period patterns used in timestamp manipulation functions\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/misc/math-expr.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nPT12H\n```\n\n----------------------------------------\n\nTITLE: Quantile Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the quantile post-aggregator, which computes a single quantile based on the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantile\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probability\" : <quantile> }\n```\n\n----------------------------------------\n\nTITLE: Including the DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in Apache Druid. This needs to be added to your Druid configuration file to enable the ArrayOfDoublesSketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Basic Scan Query Structure in Druid\nDESCRIPTION: Example of a basic scan query that retrieves data from the Wikipedia dataset. The query specifies batch size, limit, and time interval parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/scan-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"scan\",\n   \"dataSource\": \"wikipedia\",\n   \"resultFormat\": \"list\",\n   \"columns\":[],\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"batchSize\":20480,\n   \"limit\":5\n }\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Extraction in Apache Druid SQL\nDESCRIPTION: Shows how to use regular expressions to extract substrings from a string expression. The function can return a specific capture group or the entire matched substring.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nREGEXP_EXTRACT(expr, pattern, [index])\n```\n\n----------------------------------------\n\nTITLE: External Lookup DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for lookup-based dimension transformation using external lookup tables.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"name\":\"lookupName\"\n}\n```\n\n----------------------------------------\n\nTITLE: TopN Query Results - JSON\nDESCRIPTION: Sample response from a Druid TopN query showing the 10 most edited Wikipedia pages, including edit counts and page titles. Results are sorted by edit count in descending order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2015-09-12T00:46:58.771Z\",\n  \"result\" : [ {\n    \"count\" : 33,\n    \"page\" : \"Wikipedia:Vandalismusmeldung\"\n  }, {\n    \"count\" : 28,\n    \"page\" : \"User:Cyde/List of candidates for speedy deletion/Subpage\"\n  }, {\n    \"count\" : 27,\n    \"page\" : \"Jeremy Corbyn\"\n  }, {\n    \"count\" : 21,\n    \"page\" : \"Wikipedia:Administrators' noticeboard/Incidents\"\n  }, {\n    \"count\" : 20,\n    \"page\" : \"Flavia Pennetta\"\n  }, {\n    \"count\" : 18,\n    \"page\" : \"Total Drama Presents: The Ridonculous Race\"\n  }, {\n    \"count\" : 18,\n    \"page\" : \"User talk:Dudeperson176123\"\n  }, {\n    \"count\" : 18,\n    \"page\" : \"Wikipdia:Le Bistro/12 septembre 2015\"\n  }, {\n    \"count\" : 17,\n    \"page\" : \"Wikipedia:In the news/Candidates\"\n  }, {\n    \"count\" : 17,\n    \"page\" : \"Wikipedia:Requests for page protection\"\n  } ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Extraction in Apache Druid SQL\nDESCRIPTION: Shows how to use regular expressions to extract substrings from a string expression. The function can return a specific capture group or the entire matched substring.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nREGEXP_EXTRACT(expr, pattern, [index])\n```\n\n----------------------------------------\n\nTITLE: Get Lookups Response Example\nDESCRIPTION: Example JSON response from the GET /druid/listen/v1/lookups endpoint showing active lookups on a process with their extractor factories\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/lookups.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"site_id_customer2\": {\n    \"version\": \"v1\",\n    \"lookupExtractorFactory\": {\n      \"type\": \"map\",\n      \"map\": {\n        \"AHF77\": \"Home\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring floatLast Aggregator in Apache Druid for Queries\nDESCRIPTION: The floatLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Lookup Configuration Example\nDESCRIPTION: Comprehensive example showing a multi-tier lookup configuration with different lookup types including map-based and JDBC-based lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__default\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupTable\",\n          \"keyColumn\": \"country_id\",\n          \"valueColumn\": \"country_name\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  },\n  \"realtime_customer1\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    }\n  },\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Settings in Druid\nDESCRIPTION: TLS/SSL configuration properties for Jetty server and internal client communication security.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\ndruid.server.https.keyStorePath=none\ndruid.server.https.keyStoreType=none\ndruid.server.https.certAlias=none\ndruid.server.https.keyStorePassword=none\ndruid.client.https.protocol=TLSv1.2\ndruid.client.https.trustStoreType=java.security.KeyStore.getDefaultType()\ndruid.client.https.trustStorePath=none\ndruid.client.https.trustStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Field Accessor Post-Aggregator Implementation in Druid\nDESCRIPTION: Shows two types of field accessor post-aggregators that return values from specified aggregators. Includes both basic fieldAccess and finalizingFieldAccess types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"fieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"finalizingFieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Retry Policy Configuration for Druid Brokers\nDESCRIPTION: Configuration for query retry policies in Druid Brokers to handle transient errors. This controls how many times a query will be retried before failing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_39\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.retryPolicy.numTries=1\n```\n\n----------------------------------------\n\nTITLE: String Concatenation in Apache Druid SQL\nDESCRIPTION: Demonstrates various ways to concatenate strings in Druid SQL, including the || operator and the CONCAT function for multiple arguments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nx || y\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCONCAT(expr, expr...)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nTEXTCAT(expr, expr)\n```\n\n----------------------------------------\n\nTITLE: Querying Segments for Wikipedia Dataset in Druid\nDESCRIPTION: SQL query to retrieve all segments from the sys.segments table for the Wikipedia datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.segments WHERE datasource = 'wikipedia'\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Broadcast Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines an interval broadcast rule that specifies how segments of different data sources should be co-located in Historical processes for a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByInterval\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Scan Query Results in Compacted List Format\nDESCRIPTION: This JSON structure demonstrates the result format when 'resultFormat' is set to 'compactedList'. It includes segment information, column names, and event data as arrays of values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/scan-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\"\n    ],\n    \"events\" : [\n     [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._243\", \"en\", \"1\", \"MZMcBride\", 1.0, 77.0, 77.0, 77.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._73\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._756\", \"en\", \"1\", \"MZMcBride\", 1.0, 68.0, 68.0, 68.0, 0.0]\n    ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Querying Segment Information in Apache Druid using SQL\nDESCRIPTION: SQL query that retrieves segment statistics from Druid's system schema to analyze average segment size, row counts, and other metrics. This helps determine if segment compaction is necessary by showing how data is distributed across segments for a specific datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/segment-optimization.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  \"start\",\n  \"end\",\n  version,\n  COUNT(*) AS num_segments,\n  AVG(\"num_rows\") AS avg_num_rows,\n  SUM(\"num_rows\") AS total_num_rows,\n  AVG(\"size\") AS avg_size,\n  SUM(\"size\") AS total_size\nFROM\n  sys.segments A\nWHERE\n  datasource = 'your_dataSource' AND\n  is_published = 1\nGROUP BY 1, 2, 3\nORDER BY 1, 2, 3 DESC;\n```\n\n----------------------------------------\n\nTITLE: Defining Table Data Source in Druid\nDESCRIPTION: Shows how to define a basic table data source in Druid, which is the most common type of data source representing a database table. The structure includes a type identifier and name property.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/datasource.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"table\",\n\t\"name\": \"<string_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Cardinality Aggregation for Distinct People by Row\nDESCRIPTION: JSON configuration for a cardinality aggregation that counts distinct combinations of first and last names using byRow=true to count unique people.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_people\",\n  \"fields\": [ \"first_name\", \"last_name\" ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: RegexFiltered DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for filtering multi-value dimensions using regex pattern matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"regexFiltered\", \"delegate\" : <dimensionSpec>, \"pattern\": <java regex pattern> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Overlord Worker Blacklist Properties\nDESCRIPTION: Configuration properties for managing worker blacklisting behavior in Druid Overlord. These settings control thresholds for blacklisting MiddleManagers, blacklist duration, cleanup periods, and maximum percentage of workers that can be blacklisted.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/overlord.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.runner.maxRetriesBeforeBlacklist\ndruid.indexer.runner.workerBlackListBackoffTime\ndruid.indexer.runner.workerBlackListCleanupPeriod\ndruid.indexer.runner.maxPercentageBlacklistWorkers\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for object-assign\nDESCRIPTION: Copyright declaration for the object-assign library, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Performing Set Operations on ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to perform set operations (union, intersection, difference) on ArrayOfDoublesSketch instances.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchSetOp\",\n  \"name\": <output name>,\n  \"operation\": <\"UNION\"|\"INTERSECT\"|\"NOT\">,\n  \"fields\"  : <array of post aggregators to access sketch aggregators or post aggregators to allow arbitrary combination of set operations>,\n  \"nominalEntries\" : <parameter that determines the accuracy and size of the sketch>,\n  \"numberOfValues\" : <number of values associated with each distinct key>\n}\n```\n\n----------------------------------------\n\nTITLE: HTTP Endpoint for Coordinator Configuration\nDESCRIPTION: The API endpoint URL pattern for submitting Coordinator configuration via HTTP POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_25\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config\n```\n\n----------------------------------------\n\nTITLE: Example TopN Query for Exact Aggregates in Apache Druid\nDESCRIPTION: Example of a first query when implementing a two-query approach to get exact aggregates with approximate ranking for high-cardinality dimensions. This query finds the top 2 'l_orderkey' values ordered by 'L_QUANTITY_'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Coordinator Leader Election Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path used for Druid Coordinator leader election using Curator LeadershipLatch recipe.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.coordinatorPath}/_COORDINATOR\n```\n\n----------------------------------------\n\nTITLE: Querying Transformed Data in Druid using SQL\nDESCRIPTION: SQL query to retrieve all columns from the transformed data in the 'transform-tutorial' datasource. This query demonstrates the results of the applied transformations and filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"transform-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data in Druid\nDESCRIPTION: Command to load Wikipedia edits data using a pre-defined indexing specification file that creates hourly segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/deletion-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Period Broadcast Rule Configuration in Druid\nDESCRIPTION: Configures segment co-location based on a rolling time period using ISO-8601 period format. Includes options for future data and target data sources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByPeriod\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Quantile Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for computing single quantile value from histogram aggregator. Requires probability value for desired quantile.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantile\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probability\" : <quantile> }\n```\n\n----------------------------------------\n\nTITLE: JavaScript Router Strategy Configuration\nDESCRIPTION: Example JSON configuration for a JavaScript-based router strategy that routes queries with 3+ aggregators to the lowest priority Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Overlord Worker Configuration API Endpoint\nDESCRIPTION: HTTP endpoint for submitting worker configuration changes to the Overlord via POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_30\n\nLANGUAGE: text\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's config file to enable Tuple Sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Daily Compaction Task Execution\nDESCRIPTION: Command to submit the daily granularity compaction task to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json\n```\n\n----------------------------------------\n\nTITLE: Theta Sketch Estimate Post-Aggregator for Druid\nDESCRIPTION: JSON configuration for the thetaSketchEstimate post-aggregator which returns the estimated cardinality of a sketch produced by a thetaSketch aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Datasource in Druid\nDESCRIPTION: Demonstrates the structure for nested groupBy queries. This type is specifically designed for use with groupBy operations and enables subquery-like functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/datasource.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"query\",\n\t\"query\": {\n\t\t\"type\": \"groupBy\",\n\t\t...\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Custom Origin and Pacific Timezone in Apache Druid\nDESCRIPTION: This JavaScript snippet shows the granularity configuration with both a custom origin time and the Pacific timezone. The origin parameter defines the starting point for the first granularity bucket, shifting all bucket boundaries accordingly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Client TLS Properties in Apache Druid\nDESCRIPTION: Sets up TLS configuration for internal HTTP client communication between Druid services.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\ndruid.client.https.protocol=TLSv1.2\ndruid.client.https.trustStoreType=java.security.KeyStore.getDefaultType()\ndruid.client.https.trustStorePath=none\ndruid.client.https.trustStoreAlgorithm=javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()\ndruid.client.https.trustStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Large Single Server Deployment\nDESCRIPTION: Command to start Druid in large configuration, designed for machines with 32 CPU and 256GB RAM (equivalent to an i3.8xlarge instance).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/single-server.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-large\n```\n\n----------------------------------------\n\nTITLE: Implementing TopN Query with DistinctCount in Druid\nDESCRIPTION: Example of a TopN query using DistinctCount aggregator to find top 5 dimensions based on unique visitor counts. The query uses 'all' granularity for a specific day interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"uv\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Query Granularity in Apache Druid Schema\nDESCRIPTION: This snippet shows how to set the query granularity in the granularitySpec, which determines the time bucket size for aggregating events. This example uses minute granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"queryGranularity\" : \"MINUTE\"\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependencies in Druid Indexing Task JSON\nDESCRIPTION: This JSON snippet demonstrates how to specify Hadoop dependencies in a Druid Hadoop indexing task. It instructs Druid to load hadoop-client version 2.4.0 when processing the task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Flatten Spec in Apache Druid ParseSpec\nDESCRIPTION: This JSON configuration demonstrates how to set up the flattenSpec within a parseSpec to flatten the nested JSON structure. It includes various field types and expressions for accessing nested data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parseSpec\": {\n  \"format\": \"json\",\n  \"flattenSpec\": {\n    \"useFieldDiscovery\": true,\n    \"fields\": [\n      {\n        \"type\": \"root\",\n        \"name\": \"dim1\"\n      },\n      \"dim2\",\n      {\n        \"type\": \"path\",\n        \"name\": \"foo.bar\",\n        \"expr\": \"$.foo.bar\"\n      },\n      {\n        \"type\": \"root\",\n        \"name\": \"foo.bar\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"path-metric\",\n        \"expr\": \"$.nestmet.val\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-0\",\n        \"expr\": \"$.hello[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-4\",\n        \"expr\": \"$.hello[4]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"world-hey\",\n        \"expr\": \"$.world[0].hey\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"worldtree\",\n        \"expr\": \"$.world[1].tree\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"first-food\",\n        \"expr\": \"$.thing.food[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"second-food\",\n        \"expr\": \"$.thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"first-food-by-jq\",\n        \"expr\": \".thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"hello-total\",\n        \"expr\": \".hello | sum\"\n      }\n    ]\n  },\n  \"dimensionsSpec\" : {\n   \"dimensions\" : [],\n   \"dimensionsExclusions\": [\"ignore_me\"]\n  },\n  \"timestampSpec\" : {\n   \"format\" : \"auto\",\n   \"column\" : \"timestamp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Segment Management Configuration Properties Table\nDESCRIPTION: Configuration settings for segment management including discovery method and load queue implementation options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_26\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.serverview.type`|batch or http|Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper.|batch|\n|`druid.coordinator.loadqueuepeon.type`|curator or http|Whether to use \"http\" or \"curator\" implementation to assign segment loads/drops to Historical|curator|\n```\n\n----------------------------------------\n\nTITLE: HyperUnique Cardinality Post-Aggregator Setup\nDESCRIPTION: Configures a HyperUnique cardinality post-aggregator for use with HyperUnique objects in calculations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"hyperUniqueCardinality\",\n  \"name\": <output name>,\n  \"fieldName\"  : <the name field value of the hyperUnique aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchUnion Post-Aggregator\nDESCRIPTION: JSON configuration for the post-aggregator that performs union operations on multiple HLL sketches. This allows combining distinct count approximations from different sketch columns in the same row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchUnion\",\n  \"name\": <output name>,\n  \"fields\"  : <array of post aggregators that return HLL sketches>,\n  \"lgK\": <log2 of K for the target sketch>,\n  \"tgtHllType\" : <target HLL type>\n}\n```\n\n----------------------------------------\n\nTITLE: Using AND Logical Expression Filter in Druid groupBy Query\nDESCRIPTION: Demonstrates how to use an AND logical expression filter in a having clause to combine multiple conditions that must all be satisfied.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"and\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"lessThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing SQL query using HTTP POST request\nDESCRIPTION: Command for submitting a SQL query to Druid via HTTP POST using curl. The query is contained in a JSON file and sent to the Druid Broker endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8888/druid/v2/sql\n```\n\n----------------------------------------\n\nTITLE: Configuring Count Aggregator in Apache Druid\nDESCRIPTION: The count aggregator computes the count of Druid rows that match specified filters. Note that this counts Druid rows, not necessarily raw events if rollup is enabled during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"count\", \"name\" : <output_name> }\n```\n\n----------------------------------------\n\nTITLE: Executing a Select Query in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates the structure of a basic Select query in Apache Druid. It includes essential parameters such as queryType, dataSource, intervals, and pagingSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/select-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Java String Format Pattern\nDESCRIPTION: Reference to Java's String.format pattern syntax used in Druid's format function\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/misc/math-expr.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nString.format(\"pattern\", args...)\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch to Quantiles Sketch Post Aggregator\nDESCRIPTION: Post aggregator that creates a quantiles DoublesSketch from a specific column of values from an ArrayOfDoublesSketch. Takes optional parameters for column number and accuracy parameter k.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToQuantilesSketch\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"column\" : <number>,\n  \"k\" : <parameter that determines the accuracy and size of the quantiles sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Get Lookup Response Example in JSON\nDESCRIPTION: Example JSON response for a GET request to retrieve a specific lookup configuration from the Druid Coordinator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependencies in Druid Indexing Task JSON\nDESCRIPTION: This JSON snippet demonstrates how to specify Hadoop dependencies in a Druid Hadoop indexing task. It instructs Druid to load hadoop-client version 2.4.0 when processing the task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Data Source Metadata Query Response Format in Druid\nDESCRIPTION: Example JSON response from a Data Source Metadata query in Apache Druid, showing the timestamp of the query and the maxIngestedEventTime representing the latest event timestamp in the dataSource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"maxIngestedEventTime\" : \"2013-05-09T18:24:09.007Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Executing a Time Boundary Query in Apache Druid\nDESCRIPTION: A JSON query structure for retrieving time boundaries from a Druid datasource. The query allows optional filtering and can return either minimum time, maximum time, or both time boundaries depending on the 'bound' parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"timeBoundary\",\n    \"dataSource\": \"sample_datasource\",\n    \"bound\"     : < \"maxTime\" | \"minTime\" > # optional, defaults to returning both timestamps if not set \n    \"filter\"    : { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] } # optional\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring doubleMin Aggregator in Apache Druid\nDESCRIPTION: The doubleMin aggregator computes the minimum of all metric values and Double.POSITIVE_INFINITY. It takes an output name and the field name of the metric to find the minimum value of.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Executing pull-deps Tool for Apache Druid in Java\nDESCRIPTION: This command demonstrates how to use the pull-deps tool to download specific Druid extensions and Hadoop client versions. It cleans existing directories, downloads mysql-metadata-storage and druid-rabbitmq extensions, and fetches two versions of the Hadoop client.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/pull-deps.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.15.0-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.15.0-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Storage Settings in Druid\nDESCRIPTION: Configuration settings for adjusting segment cache locations and maximum server size in Druid Historical processes. These settings control how much data Historical nodes can store and manage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/faq.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}]\n-Ddruid.server.maxSize=500000000000\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to Quantile Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for computing a single quantile value from a DoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantile\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fraction\" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Object-based Numeric TopNMetricSpec in Apache Druid\nDESCRIPTION: Shows how to specify a Numeric TopNMetricSpec as a JSON object, which sorts dimension values by a numeric metric value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"numeric\",\n    \"metric\": \"<metric_name>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Process Settings for Apache Druid Realtime Node\nDESCRIPTION: YAML configuration for setting up the basic process parameters of a Druid realtime node, including host, ports, and service name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.host: InetAddress.getLocalHost().getCanonicalHostName()\ndruid.plaintextPort: 8084\ndruid.tlsPort: 8284\ndruid.service: druid/realtime\n```\n\n----------------------------------------\n\nTITLE: Configuring TopN Query in Druid\nDESCRIPTION: Configuration options for TopN queries in Druid. This setting controls the minimum threshold for TopN aliasing behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_51\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.topN.minTopNThreshold=1000\n```\n\n----------------------------------------\n\nTITLE: Disable MiddleManager Response - JSON\nDESCRIPTION: Example JSON response when disabling a MiddleManager, showing status keyed by host:port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"disabled\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing QuantilesToHistogram Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToHistogram post-aggregator which generates a histogram based on specified split points that define the histogram bins.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToHistogram\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"splitPoints\" : <array of split points>\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Metrics Table Structure\nDESCRIPTION: Markdown table structure showing metric details including descriptions, dimensions and normal values for different Druid components.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`query/time`|Milliseconds taken to complete a query.|Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension.|< 1s|\n```\n\n----------------------------------------\n\nTITLE: Scan Query List Format Results\nDESCRIPTION: Example of scan query results when resultFormat is set to 'list'. Shows the detailed structure with segmentId, columns definitions, and event data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/scan-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\",\n      \"robot\",\n      \"namespace\",\n      \"anonymous\",\n      \"unpatrolled\",\n      \"page\",\n      \"language\",\n      \"newpage\",\n      \"user\",\n      \"count\",\n      \"added\",\n      \"delta\",\n      \"variation\",\n      \"deleted\"\n    ],\n    \"events\" : [ {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }]\n```\n\n----------------------------------------\n\nTITLE: Implementing longLast Aggregator in Druid Queries\nDESCRIPTION: The longLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. This can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"longLast\",\n  \"name\" : <output_name>, \n  \"fieldName\" : <metric_name>,\n}\n```\n\n----------------------------------------\n\nTITLE: AND Filter Query for Multi-value Dimensions\nDESCRIPTION: Example of an AND filter that requires multiple values to be present in the tags dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"and\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid JavaScript and Storage Properties\nDESCRIPTION: Properties for enabling JavaScript functionality and configuring double column storage precision.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_23\n\nLANGUAGE: properties\nCODE:\n```\ndruid.javascript.enabled=false\ndruid.indexing.doubleStorage=double\n```\n\n----------------------------------------\n\nTITLE: Quantile Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for computing single and multiple quantiles from histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantile\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probability\" : <quantile> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantiles\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probabilities\" : [ <quantile>, <quantile>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: AND Logical Expression Having Filter in Druid\nDESCRIPTION: Demonstrates using an AND logical expression to combine multiple Having filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"and\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"lessThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting ZooKeeper in Bash\nDESCRIPTION: Commands to download and set up Apache ZooKeeper, which is a dependency for Druid's distributed coordination. The script downloads, extracts, and renames the ZooKeeper directory for use with Druid's startup scripts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/index.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Granularity in Druid\nDESCRIPTION: This snippet demonstrates how to set the segment granularity in the granularitySpec of a Druid ingestion task. It configures hourly segments and includes the complete dataSchema.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Mean Values Post Aggregator\nDESCRIPTION: Post aggregator that returns a list of mean values from an ArrayOfDoublesSketch. The result will be N double values corresponding to the number of values kept per key in the sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToMeans\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Segment Load Queue Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path where Coordinator writes instructions for Historical nodes to load or drop segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.loadQueuePath}/_host_of_historical_node/_segment_identifier\n```\n\n----------------------------------------\n\nTITLE: Result from Basic GroupBy Query on Multi-value Dimensions\nDESCRIPTION: The response from a GroupBy query on multi-value dimensions shows how rows are 'exploded' into multiple result rows. Each unique tag value becomes its own group with a count showing how many rows contained that tag.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t1\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t2\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t3\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t4\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t5\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t6\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t7\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension TopNMetricSpec in Apache Druid\nDESCRIPTION: Shows how to create a Dimension TopNMetricSpec that sorts by dimension values using lexicographic ordering, with optional pagination support via previousStop parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"dimension\",\n    \"ordering\": \"lexicographic\",\n    \"previousStop\": \"<previousStop_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool to Reset All Components in Apache Druid\nDESCRIPTION: Command to run the ResetCluster tool with the --all flag to reset the entire Druid cluster state by wiping all metadata and deep storage components at once.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster --all\n```\n\n----------------------------------------\n\nTITLE: Extracting Apache Druid Distribution\nDESCRIPTION: Commands to download and extract the Apache Druid 0.13.0-incubating release package.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.13.0-incubating-bin.tar.gz\ncd apache-druid-0.13.0-incubating\n```\n\n----------------------------------------\n\nTITLE: Search Query Response Format in Apache Druid\nDESCRIPTION: Illustrates the response format of a search query in Apache Druid. The result is organized by timestamp and contains matching dimension values with their occurrence counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/searchquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"Ke$ha\",\n        \"count\": 3\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"Ke$haForPresident\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"SomethingThatContainsKe\",\n        \"count\": 1\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"SomethingElseThatContainsKe\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Implementing doubleMax Aggregator in Druid\nDESCRIPTION: The doubleMax aggregator computes the maximum of all metric values and Double.NEGATIVE_INFINITY. It requires name for the output and fieldName to specify which metric column to evaluate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Select Query Response Format in Apache Druid\nDESCRIPTION: Shows the structure of a response from a Select query, including the pagingIdentifiers and events array containing the raw data rows with their timestamps and attributes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/select-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"pagingIdentifiers\" : {\n   \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 4\n }\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Response JSON\nDESCRIPTION: Example JSON response from Tranquility Server after successfully receiving and sending events to Druid. It shows the number of events received and sent.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":39244}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cache Type and Local Cache Properties\nDESCRIPTION: Configuration properties for setting the cache type and configuring the deprecated local cache.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_44\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.type=caffeine\ndruid.cache.sizeInBytes=0\ndruid.cache.initialSize=500000\ndruid.cache.logEvictionCount=0\n```\n\n----------------------------------------\n\nTITLE: Configuring StringFirst Aggregator in Druid JSON\nDESCRIPTION: Defines a stringFirst aggregator to compute the metric value with the minimum timestamp or null if no row exists. It includes optional parameters for maximum string bytes and null value filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Rollup in Druid Ingestion Spec\nDESCRIPTION: Adding a granularitySpec with rollup enabled to the dataSchema. When rollup is enabled, input columns are separated into dimensions for grouping and metrics for aggregation during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Firehose Factory in Java\nDESCRIPTION: Example of how to register a custom FirehoseFactory implementation (StaticS3FirehoseFactory) with Jackson's polymorphic serialization/deserialization system. This allows the firehose to be specified in JSON configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV Lookup in Apache Druid\nDESCRIPTION: Example of TSV data format with custom delimiter and its corresponding namespaceParseSpec configuration for Druid lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_7\n\nLANGUAGE: tsv\nCODE:\n```\nbar|something,1|foo\nbat|something,2|baz\ntruck|something,3|buck\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"tsv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\",\n  \"delimiter\": \"|\"\n}\n```\n\n----------------------------------------\n\nTITLE: Compaction Task Specification for Hourly Granularity\nDESCRIPTION: This JSON specification defines a compaction task that maintains hourly granularity while reducing the number of segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TopN Queries in Druid\nDESCRIPTION: Configuration properties for TopN queries in Druid, controlling the behavior of query processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_27\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.topN.minTopNThreshold=1000\n```\n\n----------------------------------------\n\nTITLE: True Filter in Druid JSON\nDESCRIPTION: Demonstrates the use of a true filter, which matches all values and can be used to temporarily disable other filters without removing them.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"true\" }\n```\n\n----------------------------------------\n\nTITLE: Using curl to Execute SQL Queries Against Druid\nDESCRIPTION: Example of how to execute SQL queries against Druid using curl with the JSON over HTTP API. The example shows a simple COUNT query being sent to the Druid broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ cat query.json\n{\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"}\n\n$ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json\n[{\"TheCount\":24433}]\n```\n\n----------------------------------------\n\nTITLE: Configuring StringFirst Aggregator in Druid JSON\nDESCRIPTION: Defines a stringFirst aggregator to compute the metric value with the minimum timestamp or null if no row exists. It includes optional parameters for maximum string bytes and null value filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the hyperUnique aggregator which computes the estimated cardinality using HyperLogLog. The 'isInputHyperUnique' flag enables working with pre-computed HLL values, and 'round' controls whether to round estimates to whole numbers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/hll-old.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"hyperUnique\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"isInputHyperUnique\" : false,\n  \"round\" : false\n}\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Firehose Factory in Java\nDESCRIPTION: Example of how to register a custom FirehoseFactory implementation (StaticS3FirehoseFactory) with Jackson's polymorphic serialization/deserialization system. This allows the firehose to be specified in JSON configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Dependencies and Assembly Merge Strategy in SBT\nDESCRIPTION: A complete build.sbt configuration for Scala projects using Apache Druid 0.8.1. It defines all necessary library dependencies including Druid core, services, extensions, and supporting libraries. The configuration also handles dependency conflicts through exclusion rules and custom assembly merge strategies for creating fat JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/use_sbt_to_build_fat_jar.md#2025-04-09_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.9.23\" exclude(\"common-logging\", \"common-logging\"),\n  \"org.joda\" % \"joda-convert\" % \"1.7\",\n  \"joda-time\" % \"joda-time\" % \"2.7\",\n  \"org.apache.druid\" % \"druid\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-services\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-service\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-hadoop\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"mysql-metadata-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-s3-extensions\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-histogram\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-hdfs-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-guava\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-joda\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-base\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-json-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-smile-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-jaxb-annotations\" % \"2.3.0\",\n  \"com.sun.jersey\" % \"jersey-servlet\" % \"1.17.1\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.34\",\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.3\" % \"test\",\n  \"org.mockito\" % \"mockito-core\" % \"1.10.19\" % \"test\"\n)\n\nassemblyMergeStrategy in assembly := {\n  case path if path contains \"pom.\" => MergeStrategy.first\n  case path if path contains \"javax.inject.Named\" => MergeStrategy.first\n  case path if path contains \"mime.types\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog$1.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/NoOpLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogFactory.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogConfigurationException.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/Log.class\" => MergeStrategy.first\n  case path if path contains \"META-INF/jersey-module-version\" => MergeStrategy.first\n  case path if path contains \".properties\" => MergeStrategy.first\n  case path if path contains \".class\" => MergeStrategy.first\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for Prism Syntax Highlighting Library\nDESCRIPTION: This snippet provides the copyright and license information for the Prism syntax highlighting library, which is used for code highlighting on the website. It is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Querying INFORMATION_SCHEMA in SQL\nDESCRIPTION: SQL query example showing how to retrieve metadata for a specific Druid datasource using the INFORMATION_SCHEMA.COLUMNS system table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffeine Cache in Druid\nDESCRIPTION: Configuration properties for the Caffeine cache implementation in Druid. The configuration sets cache type, size limitations, expiration policies, and executor factory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_50\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.type=caffeine\ndruid.cache.sizeInBytes=min(1GB, Runtime.maxMemory / 10)\ndruid.cache.expireAfter=None (no time limit)\ndruid.cache.cacheExecutorFactory=ForkJoinPool common pool (`COMMON_FJP`)\ndruid.cache.evictOnClose=false\n```\n\n----------------------------------------\n\nTITLE: CSV ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing CSV data in Druid, including column definitions and dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Firehose Factory in Java\nDESCRIPTION: Example of how to register a custom FirehoseFactory implementation (StaticS3FirehoseFactory) with Jackson's polymorphic serialization/deserialization system. This allows the firehose to be specified in JSON configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Timestamp Column Filter in Druid JSON\nDESCRIPTION: Shows how to filter on the timestamp column '__time' for a specific long millisecond value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"124457387532\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Theta Sketch Aggregator in Druid Indexing\nDESCRIPTION: Example of using the thetaSketch aggregator during data indexing to create a sketch of unique user IDs that can be queried later for cardinality analysis.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"thetaSketch\", \"name\": \"user_id_sketch\", \"fieldName\": \"user_id\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring Inverted TopNMetricSpec in Druid\nDESCRIPTION: Inverted metric specification that reverses the sort order of a delegate metric spec, useful for ascending order sorting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnmetricspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"inverted\",\n    \"metric\": <delegate_top_n_metric_spec>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FloatLast Aggregator in Druid JSON\nDESCRIPTION: Defines a floatLast aggregator to compute the metric value with the maximum timestamp or 0 if no row exists. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Storage for Microsoft SQLServer\nDESCRIPTION: Essential configuration properties for connecting Apache Druid to a Microsoft SQLServer database for metadata storage. These parameters define the storage type, connection URI, username, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/sqlserver.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=sqlserver\ndruid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Querying Live Row Stats in Apache Druid\nDESCRIPTION: HTTP GET request to retrieve live row statistics from a running ingestion task. This endpoint provides real-time updates on the progress of data processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/rowStats\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the thetaSketch aggregator, including options for output name, field name, input type, and sketch size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"thetaSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,  \n  \"isInputThetaSketch\": false,\n  \"size\": 16384\n }\n```\n\n----------------------------------------\n\nTITLE: Displaying EventReceiverFirehose Metrics Table in Markdown\nDESCRIPTION: This code snippet presents a markdown table listing metrics for the EventReceiverFirehose in Apache Druid when the EventReceiverFirehoseMonitor module is included. It includes metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metrics.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/buffered`|Number of events queued in the EventReceiverFirehose's buffer|serviceName, dataSource, taskId, taskType, bufferCapacity.|Equal to current # of events in the buffer queue.|\n|`ingest/bytes/received`|Number of bytes received by the EventReceiverFirehose.|serviceName, dataSource, taskId, taskType.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Implementing floatSum Aggregator in Druid\nDESCRIPTION: The floatSum aggregator computes and stores the sum of values as 32-bit floating point values. Similar to longSum and doubleSum, requiring name and fieldName parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Example JSON Mapping for Druid to StatsD Metrics\nDESCRIPTION: JSON examples showing how Druid metrics are mapped to StatsD format. Each metric defines its dimensions, StatsD type (timer, counter, gauge), and whether value range conversion is needed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"query/time\" : { \"dimensions\" : [\"dataSource\", \"type\"], \"type\" : \"timer\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"coordinator-segment/count\" : { \"dimensions\" : [\"dataSource\"], \"type\" : \"gauge\" },\n\"historical-segment/count\" : { \"dimensions\" : [\"dataSource\", \"tier\", \"priority\"], \"type\" : \"gauge\" }\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Druid Query via HTTP DELETE\nDESCRIPTION: Shows how to cancel a running query using its unique identifier. The example demonstrates the HTTP DELETE request format for the query cancellation endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/querying.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://host:port/druid/v2/abc123\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchToVariances Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchToVariances post-aggregator. This returns a list of variance values from a given ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToVariances\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Context Parameters in Apache Druid\nDESCRIPTION: A markdown table listing various query context parameters, their default values, and descriptions. These parameters control aspects like timeout, priority, caching, and result formatting for Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/query-context.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default                                 | description          |\n|-----------------|----------------------------------------|----------------------|\n|timeout          | `druid.server.http.defaultQueryTimeout`| Query timeout in millis, beyond which unfinished queries will be cancelled. 0 timeout means `no timeout`. To set the default timeout, see [Broker configuration](../configuration/index.html#broker) |\n|priority         | `0`                                    | Query Priority. Queries with higher priority get precedence for computational resources.|\n|queryId          | auto-generated                         | Unique identifier given to this query. If a query ID is set or known, this can be used to cancel the query |\n|useCache         | `true`                                 | Flag indicating whether to leverage the query cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Apache Druid (incubating) uses druid.broker.cache.useCache or druid.historical.cache.useCache to determine whether or not to read from the query cache |\n|populateCache    | `true`                                 | Flag indicating whether to save the results of the query to the query cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache or druid.historical.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|useResultLevelCache         | `false`                                 | Flag indicating whether to leverage the result level cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useResultLevelCache to determine whether or not to read from the query cache |\n|populateResultLevelCache    | `false`                                 | Flag indicating whether to save the results of the query to the result level cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|bySegment        | `false`                                | Return \"by segment\" results. Primarily used for debugging, setting it to `true` returns results associated with the data segment they came from |\n|finalize         | `true`                                 | Flag indicating whether to \"finalize\" aggregation results. Primarily used for debugging. For instance, the `hyperUnique` aggregator will return the full HyperLogLog sketch instead of the estimated cardinality when this flag is set to `false` |\n|chunkPeriod      | `P0D` (off)                            | At the Broker process level, long interval queries (of any type) may be broken into shorter interval queries to parallelize merging more than normal. Broken up queries will use a larger share of cluster resources, but, if you use groupBy \"v1, it may be able to complete faster as a result. Use ISO 8601 periods. For example, if this property is set to `P1M` (one month), then a query covering a year would be broken into 12 smaller queries. The broker uses its query processing executor service to initiate processing for query chunks, so make sure \"druid.processing.numThreads\" is configured appropriately on the broker. [groupBy queries](groupbyquery.html) do not support chunkPeriod by default, although they do if using the legacy \"v1\" engine. This context is deprecated since it's only useful for groupBy \"v1\", and will be removed in the future releases.|\n|maxScatterGatherBytes| `druid.server.http.maxScatterGatherBytes` | Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. This parameter can be used to further reduce `maxScatterGatherBytes` limit at query time. See [Broker configuration](../configuration/index.html#broker) for more details.|\n|maxQueuedBytes       | `druid.broker.http.maxQueuedBytes`        | Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to `maxScatterGatherBytes`, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled.|\n|serializeDateTimeAsLong| `false`       | If true, DateTime is serialized as long in the result returned by Broker and the data transportation between Broker and compute process|\n|serializeDateTimeAsLongInner| `false`  | If true, DateTime is serialized as long in the data transportation between Broker and compute process|\n```\n\n----------------------------------------\n\nTITLE: Example Post-Aggregation Query Implementation in Druid\nDESCRIPTION: Provides complete example queries demonstrating practical usage of post-aggregations for calculating averages and percentages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\" : [\n    { \"type\" : \"count\", \"name\" : \"rows\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n         ]\n  }]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\" : [\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"part\", \"fieldName\" : \"part\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"part_percentage\",\n    \"fn\"     : \"*\",\n    \"fields\" : [\n       { \"type\"   : \"arithmetic\",\n         \"name\"   : \"ratio\",\n         \"fn\"     : \"/\",\n         \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"part\", \"fieldName\" : \"part\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" }\n         ]\n       },\n       { \"type\" : \"constant\", \"name\": \"const\", \"value\" : 100 }\n    ]\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Field Accessor Post-Aggregators in Apache Druid JSON Query\nDESCRIPTION: Shows two types of field accessor post-aggregators: 'fieldAccess' for raw aggregation objects and 'finalizingFieldAccess' for finalized values like estimated cardinality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"fieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"finalizingFieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Scan Query CompactedList Format Results\nDESCRIPTION: Example of scan query results when resultFormat is set to 'compactedList'. Shows the more concise array-based structure of the results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/scan-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\"\n    ],\n    \"events\" : [\n     [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0]\n    ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Example of Time Format Extraction Function for Day of Week\nDESCRIPTION: This example shows how to use the Time Format extraction function to return the day of the week for Montral in French, demonstrating locale and timezone configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : \"__time\",\n  \"outputName\" :  \"dayOfWeek\",\n  \"extractionFn\" : {\n    \"type\" : \"timeFormat\",\n    \"format\" : \"EEEE\",\n    \"timeZone\" : \"America/Montreal\",\n    \"locale\" : \"fr\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query with Skip Empty Buckets in Apache Druid\nDESCRIPTION: Example of a timeseries query that uses the 'skipEmptyBuckets' context parameter to exclude time buckets with no data from the results, disabling the default zero-filling behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-04T00:00:00.000\" ],\n  \"context\" : {\n    \"skipEmptyBuckets\": \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD Emitter Properties in YAML\nDESCRIPTION: YAML configuration block for setting up StatsD Emitter in Apache Druid. It includes properties for specifying the StatsD server, metric prefixes, and additional options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.emitter.statsd.hostname: <StatsD server hostname>\ndruid.emitter.statsd.port: <StatsD server port>\ndruid.emitter.statsd.prefix: \"\"\ndruid.emitter.statsd.separator: \".\"\ndruid.emitter.statsd.includeHost: false\ndruid.emitter.statsd.dimensionMapPath: <path to JSON mapping file>\ndruid.emitter.statsd.blankHolder: \"-\"\ndruid.emitter.statsd.dogstatsd: false\ndruid.emitter.statsd.dogstatsdConstantTags: []\n```\n\n----------------------------------------\n\nTITLE: Downloading and extracting Tranquility Server for Apache Druid\nDESCRIPTION: These bash commands download the Tranquility distribution, extract it, and move it to the correct location for the tutorial setup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz\ntar -xzf tranquility-distribution-0.8.3.tgz\nmv tranquility-distribution-0.8.3 tranquility\n```\n\n----------------------------------------\n\nTITLE: Initial Empty Druid DataSchema\nDESCRIPTION: Basic empty dataSchema structure that serves as the starting point for the ingestion specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {}\n```\n\n----------------------------------------\n\nTITLE: Multi-value Column Structure in Druid Segments\nDESCRIPTION: This example demonstrates how Druid handles multi-value columns within segment files. It shows how the dictionary remains the same, but column data can contain arrays of values for a single row, and bitmaps reflect multiple non-zero entries for rows with multiple values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/segments.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   [0,1],  <--Row value of multi-value column can have array of values\n   1,\n   1]\n\n3: Bitmaps - one for each unique value\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,1,1,1]\n                            ^\n                            |\n                            |\n    Multi-value column has multiple non-zero entries\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Time Boundary Query Result Format in Apache Druid\nDESCRIPTION: The response format for a time boundary query showing the minimum and maximum timestamps in the dataset. If only one bound was requested, only that value will appear in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"minTime\" : \"2013-05-09T18:24:00.000Z\",\n    \"maxTime\" : \"2013-05-09T18:37:00.000Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Dimension Selector Having Filter in Druid\nDESCRIPTION: Example of using a dimension selector filter in Having clause to match rows based on dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n       {\n            \"type\": \"dimSelector\",\n            \"dimension\": \"<dimension>\",\n            \"value\": <dimension_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Hadoop Indexer with Custom Druid Jar in Bash\nDESCRIPTION: This Bash command demonstrates how to run the Hadoop indexer using the custom self-contained Druid jar. It includes necessary Java options and classpath settings to use the new jar instead of the standard Druid library.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticGoogleBlobStoreFirehose for Ingesting Data from Google Cloud Storage in Apache Druid\nDESCRIPTION: JSON specification for setting up a StaticGoogleBlobStoreFirehose in Druid to ingest data from Google Cloud Storage. This firehose can read from multiple GCS objects, supports splittable operations for parallel processing, and includes caching and prefetching capabilities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/google.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-google-blobstore\",\n    \"blobs\": [\n        {\n          \"bucket\": \"foo\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"bucket\": \"bar\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Druid Repository\nDESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/incubator-druid.git\ncd druid\n```\n\n----------------------------------------\n\nTITLE: React DOM Production License\nDESCRIPTION: MIT license declaration for React's react-dom.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties in Druid TuningConfig\nDESCRIPTION: This snippet shows how to set custom Hadoop job properties within the TuningConfig of a Druid ingestion task. It allows users to specify Hadoop-specific configuration parameters that will be added to the Hadoop job configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n   \"tuningConfig\" : {\n     \"type\": \"hadoop\",\n     \"jobProperties\": {\n       \"<hadoop-property-a>\": \"<value-a>\",\n       \"<hadoop-property-b>\": \"<value-b>\"\n     }\n   }\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Segment Load Status in HTTP\nDESCRIPTION: This HTTP GET request returns the percentage of segments loaded in the cluster versus segments that should be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/loadstatus\n```\n\n----------------------------------------\n\nTITLE: Performing Set Operations on ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to perform set operations (union, intersection, difference) on ArrayOfDoublesSketch instances.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchSetOp\",\n  \"name\": <output name>,\n  \"operation\": <\"UNION\"|\"INTERSECT\"|\"NOT\">,\n  \"fields\"  : <array of post aggregators to access sketch aggregators or post aggregators to allow arbitrary combination of set operations>,\n  \"nominalEntries\" : <parameter that determines the accuracy and size of the sketch>,\n  \"numberOfValues\" : <number of values associated with each distinct key>\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Server in Bash\nDESCRIPTION: Command to start a Kafka broker using the default server properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-server-start.sh config/server.properties\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period drop rule, which indicates that segments within a rolling time period should be dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Extensions in Druid Configuration\nDESCRIPTION: Shows how to enable experimental features in Druid by adding them to the loadList in the runtime.properties configuration file. This example demonstrates enabling the druid-histogram extension, which needs to be applied to all indexer and query nodes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/experimental.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-histogram\"]\n```\n\n----------------------------------------\n\nTITLE: PagingIdentifiers Example in Druid Select Query Results\nDESCRIPTION: An example of the pagingIdentifiers section from a Select query result, which contains the pagination state that should be used in subsequent queries to retrieve the next set of results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/select-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    \"pagingIdentifiers\" : {\n      \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 4\n    },\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in Druid's config file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Segment Payload JSON Structure in Metadata Storage (JSON)\nDESCRIPTION: Example JSON structure for a segment payload stored in the metadata storage. Contains segment metadata including data source, interval, version, load specification, and other details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"dataSource\":\"wikipedia\",\n \"interval\":\"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z\",\n \"version\":\"2012-05-24T00:10:00.046Z\",\n \"loadSpec\":{\n    \"type\":\"s3_zip\",\n    \"bucket\":\"bucket_for_segment\",\n    \"key\":\"path/to/segment/on/s3\"\n },\n \"dimensions\":\"comma-delimited-list-of-dimension-names\",\n \"metrics\":\"comma-delimited-list-of-metric-names\",\n \"shardSpec\":{\"type\":\"none\"},\n \"binaryVersion\":9,\n \"size\":size_of_segment,\n \"identifier\":\"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Variance Fold Aggregator for Queries\nDESCRIPTION: Configuration for querying variance results using the varianceFold aggregator, which is required when querying pre-aggregated variance data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"varianceFold\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Router Strategy Configuration\nDESCRIPTION: JSON configuration for a JavaScript-based routing strategy that defines custom routing logic. This example routes queries with 3+ aggregators to the lowest priority Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring doubleLast Aggregator in Apache Druid for Queries\nDESCRIPTION: The doubleLast aggregator computes the metric value with the maximum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Table Datasource in Apache Druid JSON\nDESCRIPTION: Demonstrates the JSON structure for defining a table datasource in Apache Druid. Table datasources are the most common type and can be represented by a string or a full structure with type and name properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/datasource.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"table\",\n\t\"name\": \"<string_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Segment Payload JSON Structure in Metadata Storage (JSON)\nDESCRIPTION: Example JSON structure for a segment payload stored in the metadata storage. Contains segment metadata including data source, interval, version, load specification, and other details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"dataSource\":\"wikipedia\",\n \"interval\":\"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z\",\n \"version\":\"2012-05-24T00:10:00.046Z\",\n \"loadSpec\":{\n    \"type\":\"s3_zip\",\n    \"bucket\":\"bucket_for_segment\",\n    \"key\":\"path/to/segment/on/s3\"\n },\n \"dimensions\":\"comma-delimited-list-of-dimension-names\",\n \"metrics\":\"comma-delimited-list-of-metric-names\",\n \"shardSpec\":{\"type\":\"none\"},\n \"binaryVersion\":9,\n \"size\":size_of_segment,\n \"identifier\":\"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for NProgress Library\nDESCRIPTION: This snippet contains the copyright and license information for the NProgress library, which is used for displaying progress indicators and is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Example InfluxDB Line Protocol Format\nDESCRIPTION: Example showing the structure of an InfluxDB Line Protocol message containing measurement name, tags, measurements, and timestamp in nanoseconds.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000\n```\n\n----------------------------------------\n\nTITLE: Retrieving Server Information in Druid Coordinator API\nDESCRIPTION: GET endpoints to retrieve information about Druid servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_12\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/servers\nGET /druid/coordinator/v1/servers?simple\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV ParseSpec in Apache Druid\nDESCRIPTION: This configuration snippet demonstrates how to set up the parseSpec for ingesting CSV data in Druid. It specifies the format, timestamp column, column order, and dimensions to be extracted from the data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"csv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating BloomKFilter in Java for Druid Queries\nDESCRIPTION: Demonstrates how to construct a BloomKFilter in Java, add values, and serialize it for use in Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nBloomKFilter bloomFilter = new BloomKFilter(1500);\nbloomFilter.addString(\"value 1\");\nbloomFilter.addString(\"value 2\");\nbloomFilter.addString(\"value 3\");\nByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\nBloomKFilter.serialize(byteArrayOutputStream, bloomFilter);\nString base64Serialized = Base64.encodeBase64String(byteArrayOutputStream.toByteArray());\n```\n\n----------------------------------------\n\nTITLE: Get Single Lookup Response Example in JSON\nDESCRIPTION: Example JSON response for a GET request to retrieve a specific lookup from a Druid process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing TopN Query with DistinctCount in Druid\nDESCRIPTION: Example of a TopN query using distinctCount aggregator to find top 5 dimensions based on unique visitor counts. The query runs on all granularity for a specific day.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"uv\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: API Endpoint for Limited Worker Configuration History in Druid\nDESCRIPTION: The HTTP endpoint for retrieving a specified number of the most recent worker configuration changes in Druid's audit history.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_36\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Implementing Greatest Post-Aggregator in Druid\nDESCRIPTION: Shows the structure of a 'doubleGreatest' post-aggregator, which computes the maximum of all specified fields and Double.NEGATIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"doubleGreatest\",\n  \"name\"  : <output_name>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Duplicate Column in Schema-less Dimensions - JSON\nDESCRIPTION: Example showing how to include the same column (device_id) twice in the data, once as a dimension with '_dim' suffix and once as a metric with '_met' suffix for unique counting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"device_id_dim\":123, \"device_id_met\":123}\n```\n\n----------------------------------------\n\nTITLE: Querying Unparseable Events in Apache Druid\nDESCRIPTION: HTTP GET request to retrieve the current list of unparseable events from a running ingestion task. This endpoint helps identify problematic data during the ingestion process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/unparseableEvents\n```\n\n----------------------------------------\n\nTITLE: Performing Set Operations on ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to perform set operations (union, intersection, difference) on ArrayOfDoublesSketch instances.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchSetOp\",\n  \"name\": <output name>,\n  \"operation\": <\"UNION\"|\"INTERSECT\"|\"NOT\">,\n  \"fields\"  : <array of post aggregators to access sketch aggregators or post aggregators to allow arbitrary combination of set operations>,\n  \"nominalEntries\" : <parameter that determines the accuracy and size of the sketch>,\n  \"numberOfValues\" : <number of values associated with each distinct key>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Historical Node Metrics Table in Markdown\nDESCRIPTION: This code snippet presents a markdown table listing various metrics for the Historical nodes in Apache Druid. It includes metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/max`|Maximum byte limit available for segments.||Varies.|\n|`segment/used`|Bytes used for served segments.|dataSource, tier, priority.|< max|\n|`segment/usedPercent`|Percentage of space used by served segments.|dataSource, tier, priority.|< 100%|\n|`segment/count`|Number of served segments.|dataSource, tier, priority.|Varies.|\n|`segment/pendingDelete`|On-disk size in bytes of segments that are waiting to be cleared out|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Interval Broadcast Rule Configuration in Druid\nDESCRIPTION: Defines segment co-location for a specific time interval using ISO-8601 format. Specifies target data sources for co-location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByInterval\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Coordinator Service Properties\nDESCRIPTION: Core configuration properties for the Druid Coordinator process including host, ports and service name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_13\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8081\ndruid.tlsPort=8281\ndruid.service=druid/coordinator\n```\n\n----------------------------------------\n\nTITLE: Copying Hadoop Configuration to Druid Classpath\nDESCRIPTION: Commands to copy Hadoop XML configuration files to the Druid classpath for enabling Hadoop integration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml\n```\n\n----------------------------------------\n\nTITLE: Enabling Rollup in Druid GranularitySpec\nDESCRIPTION: Adding a granularitySpec to the dataSchema to enable rollup functionality in Druid ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD Emitter Properties in Druid\nDESCRIPTION: A markdown table showing the configuration parameters available for the StatsD emitter extension. Parameters include hostname, port, prefix, separator, and other settings for controlling how metrics are sent to StatsD.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|property|description|required?|default|\n|--------|-----------|---------|-------|\n|`druid.emitter.statsd.hostname`|The hostname of the StatsD server.|yes|none|\n|`druid.emitter.statsd.port`|The port of the StatsD server.|yes|none|\n|`druid.emitter.statsd.prefix`|Optional metric name prefix.|no|\"\"|\n|`druid.emitter.statsd.separator`|Metric name separator|no|.|  \n|`druid.emitter.statsd.includeHost`|Flag to include the hostname as part of the metric name.|no|false|  \n|`druid.emitter.statsd.dimensionMapPath`|JSON file defining the StatsD type, and desired dimensions for every Druid metric|no|Default mapping provided. See below.|  \n|`druid.emitter.statsd.blankHolder`|The blank character replacement as statsD does not support path with blank character|no|\"-\"|  \n```\n\n----------------------------------------\n\nTITLE: Aggregating Segment Statistics by Datasource in Druid\nDESCRIPTION: SQL query to calculate total size, average size, average number of rows, and segment count grouped by datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    datasource,\n    SUM(\"size\") AS total_size,\n    CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size,\n    CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows,\n    COUNT(*) AS num_segments\nFROM sys.segments\nGROUP BY 1\nORDER BY 2 DESC\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Coordinator Process in Java\nDESCRIPTION: Command to start the Druid Coordinator process. The Coordinator is responsible for segment management and distribution across the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/coordinator.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server coordinator\n```\n\n----------------------------------------\n\nTITLE: Field Accessor Post-Aggregator with fieldAccess Type\nDESCRIPTION: JSON structure for a field accessor post-aggregator that returns the raw value produced by a specified aggregator. The 'fieldName' refers to the output name of the aggregator defined in the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"fieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Defining ContainsSearchQuerySpec in JSON for Apache Druid\nDESCRIPTION: This JSON snippet defines a ContainsSearchQuerySpec, which matches if any part of a dimension value contains the specified value. It includes 'type', 'case_sensitive', and 'value' fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"contains\",\n  \"case_sensitive\" : true,\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Coordinator Process in Java\nDESCRIPTION: Command to start the Druid Coordinator process. The Coordinator is responsible for segment management and distribution across the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/coordinator.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server coordinator\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Scaling\nDESCRIPTION: Example configuration showing how to set up additional realtime nodes with different partitionNum values for horizontal scaling. This allows distribution of different data segments across multiple nodes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 1\n    }\n```\n\n----------------------------------------\n\nTITLE: Hadoop Indexer Command Example\nDESCRIPTION: Example command for running the Hadoop indexer with the custom fat jar, showing the required classpath and Java options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n```\n\n----------------------------------------\n\nTITLE: Multi-Day Segment Example with Version 1\nDESCRIPTION: Example showing Druid segments across multiple consecutive days with v1 versioning. Each segment covers a one-day interval from 2015-01-01 to 2015-01-04.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-changes.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v1_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Server\nDESCRIPTION: Command to start the Kafka broker using the default server configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-server-start.sh config/server.properties\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Backpressure in Druid\nDESCRIPTION: Sets the maximum buffer size for queued, unread data when retrieving query results from Historical processes or Tasks. This setting helps control backpressure on the channel to the Historical or Tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.broker.http.maxQueuedBytes: 5MB\n```\n\n----------------------------------------\n\nTITLE: Importing Metadata into PostgreSQL Database\nDESCRIPTION: SQL commands to import exported metadata CSV files into PostgreSQL database tables. It uses the COPY statement for each Druid metadata table, specifying the CSV format and delimiter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/export-metadata.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCOPY druid_segments(id,dataSource,created_date,start,\"end\",partitioned,version,used,payload) FROM '/tmp/csv/druid_segments.csv' DELIMITER ',' CSV;\n\nCOPY druid_rules(id,dataSource,version,payload) FROM '/tmp/csv/druid_rules.csv' DELIMITER ',' CSV;\n\nCOPY druid_config(name,payload) FROM '/tmp/csv/druid_config.csv' DELIMITER ',' CSV;\n\nCOPY druid_dataSource(dataSource,created_date,commit_metadata_payload,commit_metadata_sha1) FROM '/tmp/csv/druid_dataSource.csv' DELIMITER ',' CSV;\n\nCOPY druid_supervisors(id,spec_id,created_date,payload) FROM '/tmp/csv/druid_supervisors.csv' DELIMITER ',' CSV;\n```\n\n----------------------------------------\n\nTITLE: Declaring Lunr.js Main Library Copyright and License\nDESCRIPTION: This snippet provides the copyright and license information for the main Lunr.js library. It includes a brief description of Lunr.js, version number, copyright notice, and license type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/4611.d3c34935.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9\\n * Copyright (C) 2020 Oliver Nightingale\\n * @license MIT\\n */\n```\n\n----------------------------------------\n\nTITLE: Downloading Multiple Extensions with Version Specification\nDESCRIPTION: Example command showing how to download multiple Druid extensions (mysql-metadata-storage and druid-rabbitmq) and Hadoop client dependencies with specific versions using the pull-deps tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/pull-deps.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.14.0-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.14.0-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to String Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for converting a DoublesSketch to a debug-friendly string representation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Cache with Path and Size in Druid Historical\nDESCRIPTION: Example configuration for the segment cache location in a Druid Historical node. This defines where the local cache resides, with options for maximum size and free space percentage requirements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n[{\"path\": \"/mnt/druidSegments\", \"maxSize\": 10000, \"freeSpacePercent\": 1.0}]\n```\n\n----------------------------------------\n\nTITLE: Follow-up Paginated Select Query in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates a follow-up paginated Select query in Apache Druid. It uses the pagingIdentifiers from the previous query result and increments the offset by 1 to retrieve the next set of results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/select-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {\"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 5}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Line Protocol Parser in Druid\nDESCRIPTION: JSON configuration for setting up the InfluxDB Line Protocol Parser in a Druid ingestion specification. This shows how to define the parser type, timestamp format, dimension exclusions, and optional measurement whitelist.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"influx\",\n        \"timestampSpec\": {\n          \"column\": \"__ts\",\n          \"format\": \"millis\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensionExclusions\": [\n            \"__ts\"\n          ]\n        },\n        \"whitelistMeasurements\": [\n          \"cpu\"\n        ]\n      }\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Druid Kafka ingestion via REST API in Bash\nDESCRIPTION: Command to submit a Kafka supervisor specification to the Druid overlord through a POST request, which sets up the Kafka indexing service for the Wikipedia data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Timestamp Specification Configuration\nDESCRIPTION: Configuration for parsing ISO 8601 timestamps from the 'ts' column in the input data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Spatial Filter for Geographic Queries in Apache Druid\nDESCRIPTION: This code snippet illustrates the structure of a spatial filter used in Apache Druid queries. It shows how to define a rectangular bound for filtering spatial data based on minimum and maximum coordinates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/geo.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\" : {\n    \"type\": \"spatial\",\n    \"dimension\": \"spatialDim\",\n    \"bound\": {\n        \"type\": \"rectangular\",\n        \"minCoords\": [10.0, 20.0],\n        \"maxCoords\": [30.0, 40.0]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Variance Aggregator for Ingestion\nDESCRIPTION: Configuration for using the variance aggregator during data ingestion. Supports different input types (float, long, variance) and allows specification of population vs sample variance through the estimator parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"variance\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"inputType\" : <input_type>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: HDFS Directory Setup Commands\nDESCRIPTION: Commands to create and configure HDFS directories for Druid data storage and copying sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/hadoop/bin\n./hadoop fs -mkdir /druid\n./hadoop fs -mkdir /druid/segments\n./hadoop fs -mkdir /quickstart\n./hadoop fs -chmod 777 /druid\n./hadoop fs -chmod 777 /druid/segments\n./hadoop fs -chmod 777 /quickstart\n./hadoop fs -chmod -R 777 /tmp\n./hadoop fs -chmod -R 777 /user\n./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: MiddleManager Enable Response in JSON\nDESCRIPTION: Example JSON response after enabling a MiddleManager via the /druid/worker/v1/enable endpoint, showing the host:port and enabled status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"enabled\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties in TuningConfig\nDESCRIPTION: Example showing how to specify Hadoop job properties within the tuningConfig object. This allows setting custom Hadoop configuration parameters for the indexing job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n   \"tuningConfig\" : {\n     \"type\": \"hadoop\",\n     \"jobProperties\": {\n       \"<hadoop-property-a>\": \"<value-a>\",\n       \"<hadoop-property-b>\": \"<value-b>\"\n     }\n   }\n```\n\n----------------------------------------\n\nTITLE: Select Query Result Format in Apache Druid\nDESCRIPTION: Example of a typical Select query result from Druid showing the response structure with pagingIdentifiers and events containing the raw data rows retrieved from the datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/select-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n [{\n  \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n  \"result\" : {\n    \"pagingIdentifiers\" : {\n      \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 4\n    },\n    \"events\" : [ {\n      \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n      \"offset\" : 0,\n      \"event\" : {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n      }\n    }, {\n      \"segmentId\" : \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n      \"offset\" : 1,\n      \"event\" : {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n      }\n    }, {\n      \"segmentId\" : \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n      \"offset\" : 2,\n      \"event\" : {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._243\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 77.0,\n        \"delta\" : 77.0,\n        \"variation\" : 77.0,\n        \"deleted\" : 0.0\n      }\n    }, {\n      \"segmentId\" : \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n      \"offset\" : 3,\n      \"event\" : {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._73\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n      }\n    }, {\n      \"segmentId\" : \"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n      \"offset\" : 4,\n      \"event\" : {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"113_U.S._756\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 68.0,\n        \"delta\" : 68.0,\n        \"variation\" : 68.0,\n        \"deleted\" : 0.0\n      }\n    } ]\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension Configuration\nDESCRIPTION: Configuration to load the DataSketches extension in Druid config file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Partial Extraction Function Configuration in Druid\nDESCRIPTION: Configuration for conditional value extraction based on regex matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"partial\", \"expr\" : <regular_expression> }\n```\n\n----------------------------------------\n\nTITLE: Lookup Introspection Keys Response Example\nDESCRIPTION: Example JSON response from the GET /druid/v1/lookups/introspect/{lookupId}/keys endpoint showing lookup keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[\n    \"A\",\n    \"B\",\n    \"C\",\n    \"Y\",\n    \"Z\",\n    \"-\"\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring TimedShutoffFirehose in Apache Druid\nDESCRIPTION: TimedShutoffFirehose configuration for a firehose that will shut down at a specified time. It wraps another firehose (delegate) and manages its lifecycle based on the shutoff time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"timed\",\n    \"shutoffTime\": \"2015-08-25T01:26:05.119Z\",\n    \"delegate\": {\n          \"type\": \"receiver\",\n          \"serviceName\": \"eventReceiverServiceName\",\n          \"bufferSize\": 100000\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Network Flow Sample Data in JSON\nDESCRIPTION: Example network flow data containing IP addresses, ports, protocol numbers, and traffic metrics like packets, bytes and cost.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4}\n```\n\n----------------------------------------\n\nTITLE: Cardinality Aggregation with Extraction Function\nDESCRIPTION: JSON configuration for a cardinality aggregation that uses an extraction function to count distinct first characters of last names.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_last_name_first_char\",\n  \"fields\": [\n    {\n     \"type\" : \"extraction\",\n     \"dimension\" : \"last_name\",\n     \"outputName\" :  \"last_name_first_char\",\n     \"extractionFn\" : { \"type\" : \"substring\", \"index\" : 0, \"length\" : 1 }\n    }\n  ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Mark.js MIT License Header\nDESCRIPTION: License header for the mark.js library (version 8.11.1) created by Julian Khnel, used for highlighting functionality and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Defining Regular Expression Filter in Apache Druid JSON Query\nDESCRIPTION: The regular expression filter matches a dimension against a Java regular expression pattern. It allows for more flexible matching than the selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"regex\", \"dimension\": <dimension_string>, \"pattern\": <pattern_string> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi InputSpec for Delta Ingestion in Druid\nDESCRIPTION: Example configuration for a Hadoop inputSpec of type 'multi' which combines multiple input sources for delta ingestion. This example includes both existing Druid segments from 'wikipedia' datasource and new data from a static file path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"multi\",\n    \"children\": [\n      {\n        \"type\" : \"dataSource\",\n        \"ingestionSpec\" : {\n          \"dataSource\": \"wikipedia\",\n          \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"],\n          \"segments\": [\n            {\n              \"dataSource\": \"test1\",\n              \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\",\n              \"version\": \"v2\",\n              \"loadSpec\": {\n                \"type\": \"local\",\n                \"path\": \"/tmp/index1.zip\"\n              },\n              \"dimensions\": \"host\",\n              \"metrics\": \"visited_sum,unique_hosts\",\n              \"shardSpec\": {\n                \"type\": \"none\"\n              },\n              \"binaryVersion\": 9,\n              \"size\": 2,\n              \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\"\n            }\n          ]\n        }\n      },\n      {\n        \"type\" : \"static\",\n        \"paths\": \"/path/to/more/wikipedia/data/\"\n      }\n    ]  \n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topic in Bash\nDESCRIPTION: Command to create a Kafka topic named 'wikipedia' with 1 partition and a replication factor of 1.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia\n```\n\n----------------------------------------\n\nTITLE: Overlord Leadership and Task Management\nDESCRIPTION: HTTP endpoints for overlord leadership status and task management operations\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/leader\nGET /druid/indexer/v1/isLeader\nGET /druid/indexer/v1/tasks\nPOST /druid/indexer/v1/task\nDELETE /druid/indexer/v1/pendingSegments/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: Implementing QuantilesToQuantile Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToQuantile post-aggregator which returns an approximation of a value at a specific fraction in the distribution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantile\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fraction\" : <fractional position in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Password Provider in Java\nDESCRIPTION: Example of how to register a custom PasswordProvider implementation with Jackson for JSON deserialization. This allows the password provider to be specified in configuration files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nreturn ImmutableList.of(\n    new SimpleModule(\"SomePasswordProviderModule\")\n        .registerSubtypes(\n            new NamedType(SomePasswordProvider.class, \"some\")\n        )\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Aggregator in Druid\nDESCRIPTION: The JavaScript aggregator computes arbitrary JavaScript functions over a set of columns. It requires three JavaScript functions: fnAggregate to process each row, fnCombine to combine partial results, and fnReset to initialize values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"javascript\",\n  \"name\": \"<output_name>\",\n  \"fieldNames\"  : [ <column1>, <column2>, ... ],\n  \"fnAggregate\" : \"function(current, column1, column2, ...) {\n                     <updates partial aggregate (current) based on the current row values>\n                     return <updated partial aggregate>\n                   }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return <combined partial results>; }\",\n  \"fnReset\"     : \"function()                   { return <initial value>; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Redis Cache Metrics in Markdown Table\nDESCRIPTION: Markdown table showing the additional metrics reported by the Redis cache implementation, including the metric name, description, and expected normal value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/redis-cache.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Normal value|\n|------|-----------|------------|\n|`query/cache/redis/*/requests`|Count of requests to redis cache|whatever request to redis will increase request count by 1|\n```\n\n----------------------------------------\n\nTITLE: Apache Druid Timeseries Query with Variance Aggregator\nDESCRIPTION: Example of a timeseries query using variance aggregation to calculate variance of the index_var field over daily intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"testing\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Hadoop Indexer Command\nDESCRIPTION: Command to run the Hadoop indexer with the self-contained jar\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n```\n\n----------------------------------------\n\nTITLE: Realtime Ingestion Metrics Table in Markdown\nDESCRIPTION: Table of metrics for the Realtime ingestion process, including event processing, persistence, and handoff metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/thrownAway`|Number of events rejected because they are outside the windowPeriod.|dataSource, taskId, taskType.|0|\n|`ingest/events/unparseable`|Number of events rejected because the events are unparseable.|dataSource, taskId, taskType.|0|\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Layout\nDESCRIPTION: Front matter configuration for the documentation page layout and title\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Schema Design\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffeine Cache Metrics in Apache Druid\nDESCRIPTION: This snippet shows the metrics reported by the Caffeine cache implementation in Apache Druid. It includes metrics for cache requests, load time, and eviction bytes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_40\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Normal value|\n|------|-----------|------------|\n|`query/cache/caffeine/*/requests`|Count of hits or misses|hit + miss|\n|`query/cache/caffeine/*/loadTime`|Length of time caffeine spends loading new values (unused feature)|0|\n|`query/cache/caffeine/*/evictionBytes`|Size in bytes that have been evicted from the cache|Varies, should tune cache `sizeInBytes` so that `sizeInBytes`/`evictionBytes` is approximately the rate of cache churn you desire|\n```\n\n----------------------------------------\n\nTITLE: Running a Kill Task for a Datasource Interval in Druid Coordinator API\nDESCRIPTION: DELETE endpoint to run a Kill task for a specified interval and datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: HTTP\nCODE:\n```\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}\n```\n\n----------------------------------------\n\nTITLE: Uniform Granularity Spec Configuration in Apache Druid\nDESCRIPTION: JSON configuration for uniform granularity specification that defines how to partition data into time chunks with uniform intervals. Includes settings for segment granularity, query granularity, rollup behavior and intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"segmentGranularity\": \"DAY\",\n  \"queryGranularity\": \"NONE\",\n  \"rollup\": true,\n  \"intervals\": [\"interval1\", \"interval2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration Properties Table in Markdown\nDESCRIPTION: Table defining the configuration properties for Druid SQL server including property names, descriptions and default values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.sql.enable`|Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely.|false|\n|`druid.sql.avatica.enable`|Whether to enable JDBC querying at `/druid/v2/sql/avatica/`.|true|\n```\n\n----------------------------------------\n\nTITLE: Applying Logical Expression Filters in groupBy Query Having Clause (JSON)\nDESCRIPTION: This snippet shows how to use logical expression filters (AND, OR, NOT) in the having clause of a groupBy query to create more complex filtering conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/having.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"and\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"lessThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n        \"type\": \"not\",\n        \"havingSpec\":         \n            {\n                \"type\": \"equalTo\",\n                \"aggregation\": \"<aggregate_metric>\",\n                \"value\": <numeric_value>\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: One-Sided Lower Bound Filter in Druid\nDESCRIPTION: Example of a bound filter expressing age >= 18 using only a lower bound.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"18\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining InsensitiveContainsSearchQuerySpec in JSON for Apache Druid\nDESCRIPTION: This JSON snippet defines an InsensitiveContainsSearchQuerySpec, which matches if any part of a dimension value contains the specified value, regardless of case. It requires a 'type' and 'value' field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"insensitive_contains\",\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Druid Module in META-INF services file\nDESCRIPTION: Example of how to register a custom Druid module in the META-INF/services file. This file should contain the fully qualified class name of the module implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\norg.apache.druid.storage.cassandra.CassandraDruidModule\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Query for Top Wikipedia Pages\nDESCRIPTION: An SQL query equivalent to the native JSON query that finds the top 10 Wikipedia pages by edit count for a specific date. It uses SELECT, GROUP BY, ORDER BY, and LIMIT clauses.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Query for Top Wikipedia Pages\nDESCRIPTION: An SQL query equivalent to the native JSON query that finds the top 10 Wikipedia pages by edit count for a specific date. It uses SELECT, GROUP BY, ORDER BY, and LIMIT clauses.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Configuring White-list Based Converter for Graphite Emitter in Apache Druid\nDESCRIPTION: JSON configuration for the 'whiteList' event converter which sends only white-listed metrics and dimensions to Graphite. This configuration specifies a custom file path for the white list map JSON object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true, \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Complex Druid Query for Intersection of Unique Users\nDESCRIPTION: Shows a more complex Druid query using filtered aggregations and thetaSketchSetOp to find the intersection of unique users between two products.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"},\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"B\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"A\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"B\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"B_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"B_unique_users\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\n    \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Docker Image\nDESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster from the provided Dockerfile.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:2.8.3 .\n```\n\n----------------------------------------\n\nTITLE: Configuring Same Interval Merge Task in Apache Druid\nDESCRIPTION: JSON configuration for a Same Interval Merge task that merges all segments within a specified interval. Simplified version of the merge task with interval-based segment selection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"same_interval_merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"interval\": <DataSegment objects in this interval are going to be merged>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Druid Database and User in MySQL\nDESCRIPTION: SQL commands to create a new database for Druid with UTF-8 encoding and grant permissions to a new user. This sets up the necessary database structure for Druid's metadata storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- create a druid database, make sure to use utf8mb4 as encoding\nCREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;\n\n-- create a druid user, and grant it all permission on the database we just created\nGRANT ALL ON druid.* TO 'druid'@'localhost' IDENTIFIED BY 'diurd';\n```\n\n----------------------------------------\n\nTITLE: Custom Buckets Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the customBuckets post-aggregator, which computes a visual representation with user-defined break points. Not supported for fixed buckets histogram.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"customBuckets\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"breaks\" : [ <value>, <value>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Sample CSV Data Format for Druid Ingestion\nDESCRIPTION: Example of CSV-formatted data for ingestion by Apache Druid. Each line represents an event with fields in a specific order without column headers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n2013-08-31T01:02:33Z,\"Gypsy Danger\",\"en\",\"nuclear\",\"true\",\"true\",\"false\",\"false\",\"article\",\"North America\",\"United States\",\"Bay Area\",\"San Francisco\",57,200,-143\n2013-08-31T03:32:45Z,\"Striker Eureka\",\"en\",\"speed\",\"false\",\"true\",\"true\",\"false\",\"wikipedia\",\"Australia\",\"Australia\",\"Cantebury\",\"Syndey\",459,129,330\n2013-08-31T07:11:21Z,\"Cherno Alpha\",\"ru\",\"masterYi\",\"false\",\"true\",\"true\",\"false\",\"article\",\"Asia\",\"Russia\",\"Oblast\",\"Moscow\",123,12,111\n2013-08-31T11:58:39Z,\"Crimson Typhoon\",\"zh\",\"triplets\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"China\",\"Shanxi\",\"Taiyuan\",905,5,900\n2013-08-31T12:41:27Z,\"Coyote Tango\",\"ja\",\"cancer\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"Japan\",\"Kanto\",\"Tokyo\",1,10,-9\n```\n\n----------------------------------------\n\nTITLE: One-Sided Upper Bound Filter in Druid\nDESCRIPTION: Example of a bound filter expressing age < 31 using only an upper bound.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Data Server Processes\nDESCRIPTION: Commands to start the Historical and MiddleManager processes on Data servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical\njava `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Configuring White-List Based Graphite Event Converter in JSON\nDESCRIPTION: JSON configuration for the 'whiteList' event converter type that sends only whitelisted metrics and dimensions to Graphite. Includes custom map file path configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true, \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Applying Numeric Filter in groupBy Query Having Clause (JSON)\nDESCRIPTION: This snippet shows how to use a numeric filter in the having clause of a groupBy query. It provides examples for equalTo, greaterThan, and lessThan filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/having.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"greaterThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"equalTo\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"lessThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Including DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet for including the DataSketches extension in Druid's configuration file. This is required to use the quantiles sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Spatial Dimensions in Apache Druid JSON Data Spec\nDESCRIPTION: This snippet demonstrates how to specify spatial dimensions in a JSON data spec for Apache Druid. It shows the configuration for a Hadoop-based ingestion task with spatial indexing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/geo.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"hadoop\",\n\t\"dataSchema\": {\n\t\t\"dataSource\": \"DatasourceName\",\n\t\t\"parser\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"parseSpec\": {\n\t\t\t\t\"format\": \"json\",\n\t\t\t\t\"timestampSpec\": {\n\t\t\t\t\t\"column\": \"timestamp\",\n\t\t\t\t\t\"format\": \"auto\"\n\t\t\t\t},\n\t\t\t\t\"dimensionsSpec\": {\n\t\t\t\t\t\"dimensions\": [],\n\t\t\t\t\t\"spatialDimensions\": [{\n\t\t\t\t\t\t\"dimName\": \"coordinates\",\n\t\t\t\t\t\t\"dims\": [\"lat\", \"long\"]\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Column Comparison Filter in Druid\nDESCRIPTION: Filter for comparing two dimensions against each other, similar to comparing columns in SQL WHERE clause.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"columnComparison\", \"dimensions\": [<dimension_a>, <dimension_b>] }\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultLimitSpec for Sorting groupBy Query Results in Druid\nDESCRIPTION: This JSON snippet demonstrates the structure of a DefaultLimitSpec used to sort and limit groupBy query results. It includes a limit value and a list of columns for ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/limitspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"default\",\n    \"limit\"   : <integer_value>,\n    \"columns\" : [list of OrderByColumnSpec],\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data with Post-Index-Task in Druid\nDESCRIPTION: Command to load initial data into a new datasource called 'updates-tutorial' using a predefined index specification file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Quantiles Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the quantiles post-aggregator, which computes an array of quantiles based on the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantiles\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probabilities\" : [ <quantile>, <quantile>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Sample JSON data for Apache Druid ingestion\nDESCRIPTION: This JSON snippet contains sample data for the tutorial, including timestamps, animal names, locations, and numeric values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"timestamp\":\"2018-01-01T07:01:35Z\",\"animal\":\"octopus\",  \"location\":1, \"number\":100},\n  {\"timestamp\":\"2018-01-01T05:01:35Z\",\"animal\":\"mongoose\", \"location\":2,\"number\":200},\n  {\"timestamp\":\"2018-01-01T06:01:35Z\",\"animal\":\"snake\", \"location\":3, \"number\":300},\n  {\"timestamp\":\"2018-01-01T01:01:35Z\",\"animal\":\"lion\", \"location\":4, \"number\":300}\n]\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch Merge Aggregator Configuration\nDESCRIPTION: JSON configuration for the HLLSketchMerge aggregator used to merge HLL sketches at query time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchMerge\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Using One-Sided Lower Bound Filter in Apache Druid\nDESCRIPTION: This example shows a one-sided bound filter that expresses the condition age >= 18 by specifying only the lower limit without setting a value for upper.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"18\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Regex Filtered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering dimension values using regex pattern matching within multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"regexFiltered\", \"delegate\" : <dimensionSpec>, \"pattern\": <java regex pattern> }\n```\n\n----------------------------------------\n\nTITLE: Querying Ingested Data with Druid SQL in Bash\nDESCRIPTION: Example of running a query on the ingested data using Druid SQL command-line client. This shows the command and output when querying the newly created 'ingestion-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"ingestion-tutorial\";\n\n\n __time                    bytes  cost  count  dstIP    dstPort  packets  protocol  srcIP    srcPort \n\n 2018-01-01T01:01:00.000Z   6000   4.9      3  2.2.2.2     3000       60  6         1.1.1.1     2000 \n 2018-01-01T01:02:00.000Z   9000  18.1      2  2.2.2.2     7000       90  6         1.1.1.1     5000 \n 2018-01-01T01:03:00.000Z   6000   4.3      1  2.2.2.2     7000       60  6         1.1.1.1     5000 \n 2018-01-01T02:33:00.000Z  30000  56.9      2  8.8.8.8     5000      300  17        7.7.7.7     4000 \n 2018-01-01T02:35:00.000Z  30000  46.3      1  8.8.8.8     5000      300  17        7.7.7.7     4000 \n\nRetrieved 5 rows in 0.12s.\n\ndsql> \n```\n\n----------------------------------------\n\nTITLE: Executing a View Query in Apache Druid\nDESCRIPTION: This JSON snippet shows how to structure a view query in Apache Druid. It includes the query type 'view' and a nested groupBy query, demonstrating how to leverage materialized views for query optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"view\",\n   \"query\": {\n       \"queryType\": \"groupBy\",\n       \"dataSource\": \"wikiticker\",\n       \"granularity\": \"all\",\n       \"dimensions\": [\n           \"user\"\n       ],\n       \"limitSpec\": {\n           \"type\": \"default\",\n           \"limit\": 1,\n           \"columns\": [\n               {\n                   \"dimension\": \"added\",\n                   \"direction\": \"descending\",\n                   \"dimensionOrder\": \"numeric\"\n               }\n           ]\n       },\n       \"aggregations\": [\n           {\n               \"type\": \"longSum\",\n               \"name\": \"added\",\n               \"fieldName\": \"added\"\n           }\n       ],\n       \"intervals\": [\n           \"2015-09-12/2015-09-13\"\n       ]\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Prism License Comment\nDESCRIPTION: License comment for Prism, a syntax highlighting library, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Avro Hadoop Parser Configuration\nDESCRIPTION: Configuration for batch ingestion using Avro Hadoop parser with custom schema file. Includes dataSchema, ioConfig, and tuningConfig settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",  \n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"\",\n      \"parser\" : {\n        \"type\" : \"avro_hadoop\",\n        \"parseSpec\" : {\n          \"format\": \"avro\",\n          \"timestampSpec\": <standard timestampSpec>,\n          \"dimensionsSpec\": <standard dimensionsSpec>,\n          \"flattenSpec\": <optional>\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\",\n        \"paths\" : \"\"\n      }\n    },\n    \"tuningConfig\" : {\n       \"jobProperties\" : {\n          \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Equal Buckets Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the equalBuckets post-aggregator, which computes a visual representation of the approximate histogram with equal-sized bins. Not supported for fixed buckets histogram.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"equalBuckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"numBuckets\": <count>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hash-based Partitioning in Apache Druid\nDESCRIPTION: This snippet shows how to configure hash-based partitioning for Druid ingestion. It specifies the partitioning type as 'hashed' and sets a target partition size, which determines the number of segments automatically based on data cardinality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"hashed\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Configuring Spatial Dimensions in JSON Data Spec for Apache Druid\nDESCRIPTION: This snippet demonstrates how to configure spatial dimensions in a Hadoop-based JSON data specification for Apache Druid. It shows how to define spatialDimensions that combine latitude and longitude fields into a single spatial dimension named 'coordinates'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/geo.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"hadoop\",\n\t\"dataSchema\": {\n\t\t\"dataSource\": \"DatasourceName\",\n\t\t\"parser\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"parseSpec\": {\n\t\t\t\t\"format\": \"json\",\n\t\t\t\t\"timestampSpec\": {\n\t\t\t\t\t\"column\": \"timestamp\",\n\t\t\t\t\t\"format\": \"auto\"\n\t\t\t\t},\n\t\t\t\t\"dimensionsSpec\": {\n\t\t\t\t\t\"dimensions\": [],\n\t\t\t\t\t\"spatialDimensions\": [{\n\t\t\t\t\t\t\"dimName\": \"coordinates\",\n\t\t\t\t\t\t\"dims\": [\"lat\", \"long\"]\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Segment Load/Drop Instructions in Druid\nDESCRIPTION: This snippet shows the ZooKeeper path where the Coordinator writes instructions for Historical processes to load or drop segments. The znode contains a payload with specific instructions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React Is in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the React Is production build, attributing it to Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with URI Source in Apache Druid\nDESCRIPTION: JSON configuration for a globally cached lookup using a URI source. This example shows how to configure a file-based lookup with polling for updates every 5 minutes using CSV format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\": \"cachedNamespace\",\n   \"extractionNamespace\": {\n      \"type\": \"uri\",\n      \"uri\": \"file:/tmp/prefix/\",\n      \"namespaceParseSpec\": {\n        \"format\": \"csv\",\n        \"columns\": [\n          \"key\",\n          \"value\"\n        ]\n      },\n      \"pollPeriod\": \"PT5M\"\n    },\n    \"firstCacheTimeout\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating SQL LIKE Operator in Druid Expressions\nDESCRIPTION: Shows the usage of the 'like' function in Druid expressions, which is equivalent to the SQL LIKE operator. It takes an expression, a pattern, and an optional escape character.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/misc/math-expr.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nlike(expr, pattern[, escape])\n```\n\n----------------------------------------\n\nTITLE: Daily Compaction Task Specification\nDESCRIPTION: JSON configuration for compacting segments into daily granularity, with segmentGranularity set to DAY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"segmentGranularity\": \"DAY\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Duration Granularity\nDESCRIPTION: Example of a Druid GroupBy query using duration-based granularity specification\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"duration\", \"duration\": \"86400000\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Batch Ingestion in Druid using Helper Script\nDESCRIPTION: Command to run the post-index-task helper script that submits the Wikipedia ingestion task to Druid Overlord and monitors the process until completion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Standard Deviation Post-Aggregator in Druid\nDESCRIPTION: Configuration for calculating standard deviation from variance using the stddev post-aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"stddev\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"estimator\": <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kill Task in Apache Druid\nDESCRIPTION: JSON configuration for a Kill Task that permanently deletes segment information from Druid's metadata store and deep storage. The task requires segments to be marked as disabled (used==0) before deletion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"kill\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\" : <all_segments_in_this_interval_will_die!>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Duration Granularity\nDESCRIPTION: Example of a Druid GroupBy query using duration-based granularity specification\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"duration\", \"duration\": \"86400000\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Equivalent OR of Bound Filters in Druid\nDESCRIPTION: Example showing how an interval filter is equivalent to an OR of bound filters with millisecond timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Load Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a forever load rule that specifies how many replicas of a segment should exist in different server tiers indefinitely.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Time Boundary Query in Apache Druid\nDESCRIPTION: This snippet demonstrates the structure of a time boundary query in Apache Druid. It includes optional parameters for specifying the bound type and applying filters to the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"timeBoundary\",\n    \"dataSource\": \"sample_datasource\",\n    \"bound\"     : < \"maxTime\" | \"minTime\" > # optional, defaults to returning both timestamps if not set \n    \"filter\"    : { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] } # optional\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Queries in Druid\nDESCRIPTION: Configuration properties for Search queries in Druid, controlling maximum results and search strategy.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_28\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.search.maxSearchLimit=1000\ndruid.query.search.searchStrategy=useIndexes\n```\n\n----------------------------------------\n\nTITLE: Configuring ContainsSearchQuerySpec in Druid\nDESCRIPTION: Defines a case-sensitive search that matches when any part of a dimension value contains the specified search value. This specification allows precise substring matching with case sensitivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"contains\",\n  \"case_sensitive\" : true,\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Like Filter in Apache Druid JSON\nDESCRIPTION: Demonstrates the JSON structure for a LIKE filter in Apache Druid. This filter supports basic wildcard searches, equivalent to the SQL LIKE operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_11\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"type\": \"like\",\n    \"dimension\": \"last_name\",\n    \"pattern\": \"D%\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding SBT Assembly Plugin in Scala\nDESCRIPTION: This code adds the sbt-assembly plugin to the project, which is used to create a fat jar that includes all dependencies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n```\n\n----------------------------------------\n\nTITLE: Running DumpSegment Tool in Apache Druid\nDESCRIPTION: This command demonstrates how to execute the DumpSegment tool to extract data from a Druid segment. It requires specifying a classpath with Druid libraries, the segment directory path, and an output file to write the results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/dump-segment.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools dump-segment \\\n  --directory /home/druid/path/to/segment/ \\\n  --out /home/druid/output.txt\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Coordinator Leader Status in HTTP\nDESCRIPTION: This HTTP GET request returns the current leader Coordinator of the Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/leader\n```\n\n----------------------------------------\n\nTITLE: Defining a Table Data Source in Apache Druid JSON\nDESCRIPTION: The table data source is the most common type in Druid. It can be represented by a simple string or a structured JSON object as shown in this snippet.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/datasource.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"table\",\n\t\"name\": \"<string_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Min Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for extracting minimum value from histogram aggregator. Works with both approximate and fixed buckets histograms.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"min\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Defining a Table Data Source in Apache Druid JSON\nDESCRIPTION: The table data source is the most common type in Druid. It can be represented by a simple string or a structured JSON object as shown in this snippet.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/datasource.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"table\",\n\t\"name\": \"<string_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Transform Spec in Apache Druid\nDESCRIPTION: The basic syntax for defining a transformSpec in Apache Druid, which contains transforms to be applied to input rows and an optional filter to determine which rows will be ingested.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"transformSpec\": {\n  \"transforms: <List of transforms>,\n  \"filter\": <filter>\n}\n```\n\n----------------------------------------\n\nTITLE: Appending Data with Batch Index Task in Druid\nDESCRIPTION: Submitting a task with appendToExisting set to true to add new data to existing segments without overwriting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index2.json\n```\n\n----------------------------------------\n\nTITLE: Defining a Transform Spec in Apache Druid\nDESCRIPTION: The basic syntax for defining a transformSpec in Apache Druid, which contains transforms to be applied to input rows and an optional filter to determine which rows will be ingested.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"transformSpec\": {\n  \"transforms: <List of transforms>,\n  \"filter\": <filter>\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Specific Lookup with JSON\nDESCRIPTION: Example of updating a single lookup 'site_id_customer1' in the 'realtime_customer1' tier using a POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"847632\": \"Internal Use Only\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's configuration file. This is required before using any HLL sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for Prism\nDESCRIPTION: This comment block provides license and author information for Prism, a syntax highlighting library created by Lea Verou and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's configuration file. This is required before using any HLL sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Result with Daily Granularity in Pacific Timezone\nDESCRIPTION: This snippet shows the result of the groupBy query with daily granularity in Pacific timezone. It demonstrates how the data is aggregated into daily buckets, with timestamps adjusted to the specified timezone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/granularities.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 2,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Data Source in Druid\nDESCRIPTION: Shows the structure for a query data source used for nested groupBy queries. This type is specifically designed for use with groupBy query operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/datasource.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"query\",\n\t\"query\": {\n\t\t\"type\": \"groupBy\",\n\t\t...\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Union Data Source in Apache Druid JSON\nDESCRIPTION: The union data source combines two or more table data sources with the same schema. Union queries must be sent to a Broker/Router process and are not supported directly by Historical processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/datasource.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n       \"type\": \"union\",\n       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Graphite Event Converter in JSON\nDESCRIPTION: JSON configuration for the 'all' event converter type that sends all Druid service metrics events to Graphite. This configuration ignores hostname and service name in the metric path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Live Row Stats in Apache Druid\nDESCRIPTION: This HTTP GET request retrieves live row statistics from a running task in Apache Druid. It provides real-time information about processed rows, errors, and moving averages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/reports.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/rowStats\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Statistical Testing with Apache Druid Post Aggregators\nDESCRIPTION: A complete JSON query example demonstrating how to use zscore2sample and pvalue2tailedZtest post aggregators together. The example calculates the z-score between two population samples and then derives the p-value from that z-score.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n    \"postAggregations\" : {\n    \"type\"   : \"pvalue2tailedZtest\",\n    \"name\"   : \"pvalue\",\n    \"zScore\" : \n    {\n     \"type\"   : \"zscore2sample\",\n     \"name\"   : \"zscore\",\n     \"successCount1\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation1Sample\",\n         \"value\"  : 300\n       },\n     \"sample1Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation1\",\n         \"value\"  : 500\n       },\n     \"successCount2\":\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation2Sample\",\n         \"value\"  : 450\n       },\n     \"sample2Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation2\",\n         \"value\"  : 600\n       }\n     }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependencies in Hadoop Index Task JSON\nDESCRIPTION: This JSON snippet demonstrates how to specify the Hadoop client version to be loaded by Druid when processing a Hadoop Index Task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring flattenSpec for Nested ORC Map Fields in Druid (JSON)\nDESCRIPTION: This JSON configuration shows how to use flattenSpec expressions to flatten ORC Map columns with primitive types in Druid 0.15.0+. It replaces the mapFieldNameFormat property used in the 'contrib' extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/orc.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flattenSpec\": {\n    \"fields\": [\n      {\n        \"type\": \"path\",\n        \"name\": \"nestedData_dim1\",\n        \"expr\": \"$.nestedData.dim1\"\n      }\n    ]\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Druid MySQL Metadata Storage Configuration\nDESCRIPTION: Properties configuration for connecting Druid to the MySQL metadata store. These settings specify the extension to load, storage type, connection URI, and authentication credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"mysql-metadata-storage\"]\ndruid.metadata.storage.type=mysql\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=druid\n```\n\n----------------------------------------\n\nTITLE: Aggregating Wikipedia edits by hour with Druid SQL\nDESCRIPTION: SQL query that groups Wikipedia data by hour and calculates the sum of deleted lines for each hour period. Uses the FLOOR function to truncate timestamps to hour granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'\nGROUP BY 1\n```\n\n----------------------------------------\n\nTITLE: Filtering Multi-value Dimensions with AND Logic in Druid\nDESCRIPTION: Example of an 'and' filter query that matches rows where the multi-value dimension 'tags' contains both 't1' and 't3'. This query would match only row1 from the sample dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"and\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Grouped Data in Apache Druid\nDESCRIPTION: This SQL query demonstrates how to perform a GroupBy query on the updated datasource, showing aggregated results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect __time, animal, SUM(\"count\"), SUM(\"number\") from \"updates-tutorial\" group by __time, animal;\n```\n\n----------------------------------------\n\nTITLE: Time Format Example for Day of Week\nDESCRIPTION: Example configuration showing how to extract the day of week in French for Montreal timezone from the __time dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : \"__time\",\n  \"outputName\" :  \"dayOfWeek\",\n  \"extractionFn\" : {\n    \"type\" : \"timeFormat\",\n    \"format\" : \"EEEE\",\n    \"timeZone\" : \"America/Montreal\",\n    \"locale\" : \"fr\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Noop Task for Testing in Druid\nDESCRIPTION: JSON configuration for Noop tasks used for testing purposes. These tasks start, sleep for a specified time, and can optionally test firehose connections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"noop\",\n    \"id\": <optional_task_id>,\n    \"interval\" : <optional_segment_interval>,\n    \"runTime\" : <optional_millis_to_sleep>,\n    \"firehose\": <optional_firehose_to_test_connect>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTSDB Metric Mapping in JSON\nDESCRIPTION: Example JSON configuration showing how to map Druid metrics to OpenTSDB dimensions. This snippet demonstrates mapping the query/time metric to include dataSource and type dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/opentsdb-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"query/time\": [\n    \"dataSource\",\n    \"type\"\n]\n```\n\n----------------------------------------\n\nTITLE: Defining a Union Data Source in Apache Druid JSON\nDESCRIPTION: The union data source combines two or more table data sources with the same schema. Union queries must be sent to a Broker/Router process and are not supported directly by Historical processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/datasource.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n       \"type\": \"union\",\n       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RegexSearchQuerySpec in Druid\nDESCRIPTION: Defines a regular expression-based search where a match occurs if any part of a dimension value matches the specified regex pattern. This allows for complex pattern matching operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchqueryspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"regex\",\n  \"pattern\" : \"some_pattern\"\n}\n```\n\n----------------------------------------\n\nTITLE: Query Filter Having Specification for Druid groupBy\nDESCRIPTION: Example of using query filters in having clause for groupBy queries. Demonstrates basic filter structure and selector filter usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/having.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : <any Druid query filter>\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : {\n              \"type\": \"selector\",\n              \"dimension\" : \"<dimension>\",\n              \"value\" : \"<dimension_value>\"\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: OR Filter Query for Multi-value Dimensions\nDESCRIPTION: Example of an OR filter query that matches multiple values within a multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"or\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Kerberos Authentication with Curl\nDESCRIPTION: Commands for authenticating with Kerberos using kinit and accessing Druid HTTP endpoints using curl with SPNEGO negotiation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkinit -k -t <path_to_keytab_file> user@REALM.COM\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json' <HTTP_END_POINT>\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json\n```\n\n----------------------------------------\n\nTITLE: Calculating Variance Values from ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to retrieve variance values for each column from an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToVariances\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to String Post Aggregator\nDESCRIPTION: Post aggregator configuration for converting a DoublesSketch to a debug-friendly string representation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for React\nDESCRIPTION: This comment block provides license information for the React production build, which is released under the MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Parser with TimeAndDims ParseSpec in Druid\nDESCRIPTION: Example configuration for ingesting Parquet files using the 'parquet' parser type with a timeAndDims parseSpec. This configuration explicitly defines dimensions without using field discovery or flattening.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Druid Integration\nDESCRIPTION: Markdown documentation describing integration approaches for Apache Druid with streaming and SQL-on-Hadoop technologies. Includes sections on using Tranquility library with stream processors and potential SQL-on-Hadoop integrations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/integrating-druid-with-other-technologies.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Integrating Apache Druid (incubating) With Other Technologies\n\nThis page discusses how we can integrate Druid with other technologies. \n\n## Integrating with Open Source Streaming Technologies\n\nEvent streams can be stored in a distributed message bus such as Kafka and further processed via a distributed stream  \nprocessor system such as Storm, Samza, or Spark Streaming. Data processed by the stream processor can feed into Druid using \nthe [Tranquility](https://github.com/druid-io/tranquility) library.\n\n<img src=\"../../img/druid-production.png\" width=\"800\"/>\n\n## Integrating with SQL-on-Hadoop Technologies\n\nDruid should theoretically integrate well with SQL-on-Hadoop technologies such as Apache Drill, Spark SQL, Presto, Impala, and Hive.\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties in Druid TuningConfig\nDESCRIPTION: Example showing how to specify Hadoop job properties within the tuningConfig object. Allows setting custom Hadoop configuration parameters for the indexing job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n   \"tuningConfig\" : {\n     \"type\": \"hadoop\",\n     \"jobProperties\": {\n       \"<hadoop-property-a>\": \"<value-a>\",\n       \"<hadoop-property-b>\": \"<value-b>\"\n     }\n   }\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading Off-heap MapDB Cache in JSON\nDESCRIPTION: Example configuration for a loading lookup with MapDB off-heap cache implementation. The primary cache has a maximum entries limit, while the reverse lookup cache has a maximum store size and expiration settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"mapDb\", \"maxEntriesSize\":100000},\n   \"reverseLoadingCacheSpec\":{\"type\":\"mapDb\", \"maxStoreSize\":5, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Single Lookup in Apache Druid\nDESCRIPTION: JSON format for updating a specific lookup in a particular tier. This example shows how to replace the site_id_customer1 lookup in the realtime_customer1 tier with a new version.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"847632\": \"Internal Use Only\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Initial Data from a Druid Datasource\nDESCRIPTION: SQL query to retrieve all rows from the newly created 'updates-tutorial' datasource, showing the animal dimension and number metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\"; \n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  tiger         1     100 \n 2018-01-01T03:01:00.000Z  aardvark      1      42 \n 2018-01-01T03:01:00.000Z  giraffe       1   14124 \n\nRetrieved 3 rows in 1.42s.\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Lookup in Apache Druid\nDESCRIPTION: Example of a namespaceParseSpec configuration for CSV lookup in Druid. It specifies the format, columns, key column, and value column for parsing CSV data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"csv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Daily Granularity Compaction Task Specification\nDESCRIPTION: JSON configuration for compacting segments into daily granularity, including datasource, interval, and segment granularity settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"segmentGranularity\": \"DAY\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Wikipedia edits by hour with Druid SQL\nDESCRIPTION: SQL query that groups Wikipedia data by hour and calculates the sum of deleted lines for each hour period. Uses the FLOOR function to truncate timestamps to hour granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted\nFROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'\nGROUP BY 1\n```\n\n----------------------------------------\n\nTITLE: Configuring toInclude All Option in Druid Query\nDESCRIPTION: Configuration to include all columns in the segment metadata query result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"toInclude\": { \"type\": \"all\"}\n```\n\n----------------------------------------\n\nTITLE: Exclude All Columns Configuration in Druid Metadata Query\nDESCRIPTION: Configuration to exclude all columns from the segment metadata query result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"toInclude\": { \"type\": \"none\"}\n```\n\n----------------------------------------\n\nTITLE: Kill Task Configuration in Apache Druid\nDESCRIPTION: JSON configuration for a Kill Task that permanently deletes segment data from Druid's metadata store and deep storage. The task requires segments to be marked as disabled (used==0) before deletion can occur.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"kill\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\" : <all_segments_in_this_interval_will_die!>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Before Rule in Druid\nDESCRIPTION: JSON configuration for a period drop before rule that removes segments older than a specified time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropBeforeByPeriod\",\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Timeseries Query Context in Apache Druid\nDESCRIPTION: A markdown table showing the specific query context parameter for Timeseries queries in Druid, which controls the zero-filling behavior for empty buckets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/query-context.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default              | description          |\n|-----------------|---------------------|----------------------|\n|skipEmptyBuckets | `false`             | Disable timeseries zero-filling behavior, so only buckets with results will be returned. |\n```\n\n----------------------------------------\n\nTITLE: Starting Query Server for Apache Druid\nDESCRIPTION: This command starts the Query server for Apache Druid. It should be run from the distribution root after copying the Druid distribution and edited configurations to the Query servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-cluster-query-server\n```\n\n----------------------------------------\n\nTITLE: Configuring RegexSearchQuerySpec in Druid\nDESCRIPTION: Defines a regular expression-based search where a match occurs if any part of a dimension value matches the specified regex pattern. This allows for complex pattern matching operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchqueryspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"regex\",\n  \"pattern\" : \"some_pattern\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic SSL Settings in Apache Druid\nDESCRIPTION: This table outlines the basic configuration properties for setting up SSL in Apache Druid, including protocol, trust store settings, and algorithms.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/simple-client-sslcontext.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.client.https.protocol`|SSL protocol to use.|`TLSv1.2`|no|\n|`druid.client.https.trustStoreType`|The type of the key store where trusted root certificates are stored.|`java.security.KeyStore.getDefaultType()`|no|\n|`druid.client.https.trustStorePath`|The file path or URL of the TLS/SSL Key store where trusted root certificates are stored.|none|yes|\n|`druid.client.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate certificate chains|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.client.https.trustStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Trust Store.|none|yes|\n```\n\n----------------------------------------\n\nTITLE: Defining RegexSearchQuerySpec in JSON for Apache Druid\nDESCRIPTION: This JSON snippet defines a RegexSearchQuerySpec, which matches if any part of a dimension value matches the specified regex pattern. It includes 'type' and 'pattern' fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"regex\",\n  \"pattern\" : \"some_pattern\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example of JavaScript Filter in Apache Druid JSON\nDESCRIPTION: Provides an example of a JavaScript filter that matches dimension values between 'bar' and 'foo' for the dimension 'name'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : \"name\",\n  \"function\" : \"function(x) { return(x >= 'bar' && x <= 'foo') }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Druid Segment Naming Convention (Version 1)\nDESCRIPTION: Example showing the naming convention for Druid segments with version 1. The segments share the same datasource, interval, and version, but have linearly increasing partition numbers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-01/2015-01-02_v1_1\nfoo_2015-01-01/2015-01-02_v1_2\n```\n\n----------------------------------------\n\nTITLE: Concatenating Strings in Druid SQL with || Operator\nDESCRIPTION: Shows how to concatenate two strings using the || operator in Druid SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nx || y\n```\n\n----------------------------------------\n\nTITLE: API Endpoint for Worker Configuration in Druid\nDESCRIPTION: The HTTP endpoint for submitting worker configuration to the Druid Overlord. This endpoint accepts POST requests to update worker configuration and GET requests to retrieve current configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_33\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker\n```\n\n----------------------------------------\n\nTITLE: Configuring tuningConfig for Performance Optimization in Druid\nDESCRIPTION: JSON configuration for tuning ingestion parameters in Apache Druid. This example sets maxRowsPerSegment to 5000000 to control segment size for the native batch ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n```\n\n----------------------------------------\n\nTITLE: Druid groupBy Query with Subtotals Specification\nDESCRIPTION: Example showing how to use subtotalsSpec in a groupBy query to compute multiple sub-groupings in a single query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\": \"groupBy\",\n ...\n ...\n\"dimensions\": [\n  {\n  \"type\" : \"default\",\n  \"dimension\" : \"d1col\",\n  \"outputName\": \"D1\"\n  },\n  {\n  \"type\" : \"extraction\",\n  \"dimension\" : \"d2col\",\n  \"outputName\" :  \"D2\",\n  \"extractionFn\" : extraction_func\n  },\n  {\n  \"type\":\"lookup\",\n  \"dimension\":\"d3col\",\n  \"outputName\":\"D3\",\n  \"name\":\"my_lookup\"\n  }\n],\n...\n...\n\"subtotalsSpec\":[ [\"D1\", \"D2\", \"D3\"], [\"D1\", \"D3\"], [\"D3\"]],\n..\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Row Count - Druid SQL\nDESCRIPTION: SQL query to count total rows in the compaction-tutorial datasource, demonstrating data consistency before and after compaction.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select count(*) from \"compaction-tutorial\";\n\n EXPR$0 \n\n  39244 \n\nRetrieved 1 row in 1.38s.\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function Examples\nDESCRIPTION: Examples of JavaScript-based extraction functions for both regular dimensions and __time dimension, with optional injectivity specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str.substr(0, 3); }\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str + '!!!'; }\",\n  \"injective\" : true\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Wikipedia Edit Event JSON\nDESCRIPTION: Example JSON structure showing the format of Wikipedia page edit events used in the tutorial dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/index.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0\n}\n```\n\n----------------------------------------\n\nTITLE: React Core License Comment\nDESCRIPTION: License comment for the React core production file, which is licensed under the MIT License and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6f6dba15.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Sample Wikipedia Edit Event JSON\nDESCRIPTION: Example JSON structure showing the format of Wikipedia page edit events used in the tutorial dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/index.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0\n}\n```\n\n----------------------------------------\n\nTITLE: Equivalent Bound Filter for Time Intervals in Druid\nDESCRIPTION: Shows the equivalent OR of bound filters for the interval filter example.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Equivalent Bound Filter for Time Intervals in Druid\nDESCRIPTION: Shows the equivalent OR of bound filters for the interval filter example.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Scan Query Compacted List Format Results\nDESCRIPTION: Example of scan query results when resultFormat is set to 'compactedList'. Shows the optimized array-based structure for more efficient data representation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/scan-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\"\n    ],\n    \"events\" : [\n     [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0]\n    ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Executing Timeseries Query with Variance Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for a Timeseries query using the variance aggregator. It specifies the query type, data source, granularity, aggregations, and time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"testing\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Append Task in Apache Druid\nDESCRIPTION: JSON configuration for an Append task that combines multiple segments sequentially into a single segment. Includes task ID, datasource, segment list, optional aggregations, and context settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"append\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"segments\": <JSON list of DataSegment objects to append>,\n    \"aggregations\": <optional list of aggregators>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Input Source with Local Firehose\nDESCRIPTION: Configuration for the ioConfig object that defines a local firehose input source for reading data files from the filesystem.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Aggregator\nDESCRIPTION: JSON configuration for the Theta sketch aggregator. This defines how to create sketch objects during ingestion to store approximate unique counts of values in a column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"thetaSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,  \n  \"isInputThetaSketch\": false,\n  \"size\": 16384\n }\n```\n\n----------------------------------------\n\nTITLE: GET Request for Intervals\nDESCRIPTION: HTTP GET endpoints for retrieving interval information across datasources\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/intervals\nGET /druid/coordinator/v1/intervals/{interval}\nGET /druid/coordinator/v1/intervals/{interval}?simple\nGET /druid/coordinator/v1/intervals/{interval}?full\n```\n\n----------------------------------------\n\nTITLE: Sample Completion Report JSON Structure in Apache Druid\nDESCRIPTION: Example of a completion report JSON structure, showing ingestion statistics including row counts and error information for different phases of ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom DBCP Properties for Metadata Storage in Apache Druid\nDESCRIPTION: Examples of setting custom DBCP (Database Connection Pooling) properties for metadata storage connectors in Druid. These properties control connection lifecycle and query timeout.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000\ndruid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000\n```\n\n----------------------------------------\n\nTITLE: Using One-Sided Upper Bound Filter in Apache Druid\nDESCRIPTION: This example demonstrates a one-sided bound by omitting the lower limit, expressing the condition age < 31 with strict upper bound comparison.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Wikipedia Data to Tranquility Server in Bash\nDESCRIPTION: Commands to decompress a sample Wikipedia dataset and send it to Tranquility Server using a HTTP POST request. The data is sent as JSON to the /v1/post/wikipedia endpoint on localhost port 8200.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz \ncurl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Datasource Segments in HTTP\nDESCRIPTION: This HTTP POST request returns a list of all segments, overlapping with given intervals, for a datasource as stored in the metadata store. The request body should contain an array of ISO 8601 interval strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments\n```\n\n----------------------------------------\n\nTITLE: Configuring Noop Task in Druid\nDESCRIPTION: Defines a no-operation task used for testing purposes. The task can be configured with an optional ID, segment interval, sleep duration, and test firehose connection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"noop\",\n    \"id\": <optional_task_id>,\n    \"interval\" : <optional_segment_interval>,\n    \"runTime\" : <optional_millis_to_sleep>,\n    \"firehose\": <optional_firehose_to_test_connect>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator with Extraction Function in Apache Druid\nDESCRIPTION: This example demonstrates using the Cardinality aggregator with an extraction function to determine the number of distinct starting characters of last names. It uses a substring extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/hll-old.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_last_name_first_char\",\n  \"fields\": [\n    {\n     \"type\" : \"extraction\",\n     \"dimension\" : \"last_name\",\n     \"outputName\" :  \"last_name_first_char\",\n     \"extractionFn\" : { \"type\" : \"substring\", \"index\" : 0, \"length\" : 1 }\n    }\n  ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring timeMin Aggregator at Ingestion Time in Druid\nDESCRIPTION: JSON configuration for the timeMin aggregator that should be included during data ingestion to enable calculating the minimum timestamp of events.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMin\",\n    \"name\": \"tmin\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bulk Lookup Updates in Apache Druid\nDESCRIPTION: Example JSON configuration for bulk updating lookups in Apache Druid. It demonstrates how to define multiple tiers and lookups with different extractor types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__default\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupTable\",\n          \"keyColumn\": \"country_id\",\n          \"valueColumn\": \"country_name\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  },\n  \"realtime_customer1\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    }\n  },\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading Off-heap MapDB Cache in Druid\nDESCRIPTION: Example configuration for a loading lookup using MapDB off-heap cache implementation with size and expiration settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"mapDb\", \"maxEntriesSize\":100000},\n   \"reverseLoadingCacheSpec\":{\"type\":\"mapDb\", \"maxStoreSize\":5, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Cached Namespace Lookup in Apache Druid (JSON)\nDESCRIPTION: This snippet demonstrates how to set up a cached namespace lookup using a JDBC extraction namespace. It includes database connection details, table and column specifications, and cache timeout settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"jdbc\",\n       \"connectorConfig\": {\n         \"createTables\": true,\n         \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n         \"user\": \"druid\",\n         \"password\": \"diurd\"\n       },\n       \"table\": \"lookupTable\",\n       \"keyColumn\": \"mykeyColumn\",\n       \"valueColumn\": \"myValueColumn\",\n       \"filter\" : \"myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)\",\n       \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Defining HyperUnique Cardinality Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Shows the structure of a HyperUnique Cardinality post-aggregator used to wrap a hyperUnique object for use in post-aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"hyperUniqueCardinality\",\n  \"name\": <output name>,\n  \"fieldName\"  : <the name field value of the hyperUnique aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading Off-heap MapDB Cache in Druid\nDESCRIPTION: Example configuration for a loading lookup using MapDB off-heap cache implementation with size and expiration settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"mapDb\", \"maxEntriesSize\":100000},\n   \"reverseLoadingCacheSpec\":{\"type\":\"mapDb\", \"maxStoreSize\":5, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchEstimateWithBounds Post-Aggregator\nDESCRIPTION: JSON configuration for the post-aggregator that provides cardinality estimates with error bounds from an HLL sketch. The numStdDev parameter controls the confidence interval width.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchEstimateWithBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>,\n  \"numStdDev\" : <number of standard deviations: 1 (default), 2 or 3>\n}\n```\n\n----------------------------------------\n\nTITLE: React Core MIT License Declaration\nDESCRIPTION: License declaration for react.production.min.js (v17.0.2) created by Facebook, Inc. and its affiliates under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Example GroupBy Query with Variance and Standard Deviation\nDESCRIPTION: Sample GroupBy query demonstrating variance aggregation and standard deviation calculation grouped by dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Wikipedia Data to Tranquility Server in Bash\nDESCRIPTION: Commands to decompress a sample Wikipedia dataset and send it to Tranquility Server using a HTTP POST request. The data is sent as JSON to the /v1/post/wikipedia endpoint on localhost port 8200.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz \ncurl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Index Task in Apache Druid\nDESCRIPTION: This JSON configuration defines a Local Index Task for ingesting data into Apache Druid. It specifies the data schema, input source, and various tuning parameters for optimizing the indexing process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"examples/indexing/\",\n        \"filter\" : \"wikipedia_data.json\"\n       }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 1000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom JSON Lookup in Apache Druid\nDESCRIPTION: Example of a namespaceParseSpec configuration for custom JSON lookup in Druid. It specifies the format and defines the key and value field names for parsing JSON data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"customJson\",\n  \"keyFieldName\": \"key\",\n  \"valueFieldName\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Timestamps in Druid Expressions\nDESCRIPTION: Shows how to format timestamps as strings in Druid expressions using the 'timestamp_format' function. It uses Joda DateTimeFormat patterns and can specify a timezone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/misc/math-expr.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\ntimestamp_format(expr, [pattern, [timezone]])\n```\n\n----------------------------------------\n\nTITLE: Configuring White-List Based Converter for Ambari Metrics in Apache Druid\nDESCRIPTION: Configuration for the 'whiteList' event converter implementation which only sends white-listed metrics and dimensions to Ambari Metrics. This example shows how to specify a custom whitelist map file path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"appName\":\"druid\", \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring TimedShutoffFirehose in Apache Druid (JSON)\nDESCRIPTION: JSON configuration for TimedShutoffFirehose, which wraps another firehose and shuts it down at a specified time. It includes the shutdown time and delegates to an EventReceiverFirehose configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"timed\",\n    \"shutoffTime\": \"2015-08-25T01:26:05.119Z\",\n    \"delegate\": {\n          \"type\": \"receiver\",\n          \"serviceName\": \"eventReceiverServiceName\",\n          \"bufferSize\": 100000\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Basic Authorizer in Apache Druid\nDESCRIPTION: This JSON snippet shows how to enable the 'basic' authorizer implementation from the 'druid-basic-security' extension in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/auth.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authorizers\":[\"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Storage for SQLServer\nDESCRIPTION: This configuration snippet sets up the connection parameters for using Microsoft SQLServer as Druid's metadata storage. It specifies the storage type, connection URI, username, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/sqlserver.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=sqlserver\ndruid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Logical NOT Filter in Druid\nDESCRIPTION: Negates the result of the specified filter condition.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"not\", \"field\": <filter> }\n```\n\n----------------------------------------\n\nTITLE: Scan Query List Format Results\nDESCRIPTION: Example of scan query results when resultFormat is set to 'list'. Shows the detailed object structure containing segmentId, columns and events array with individual row data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/scan-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\",\n      \"robot\",\n      \"namespace\",\n      \"anonymous\",\n      \"unpatrolled\",\n      \"page\",\n      \"language\",\n      \"newpage\",\n      \"user\",\n      \"count\",\n      \"added\",\n      \"delta\",\n      \"variation\",\n      \"deleted\"\n    ],\n    \"events\" : [ {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"1\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"11._korpus_(NOVJ)\",\n        \"language\" : \"sl\",\n        \"newpage\" : \"0\",\n        \"user\" : \"EmausBot\",\n        \"count\" : 1.0,\n        \"added\" : 39.0,\n        \"delta\" : 39.0,\n        \"variation\" : 39.0,\n        \"deleted\" : 0.0\n    }, {\n        \"timestamp\" : \"2013-01-01T00:00:00.000Z\",\n        \"robot\" : \"0\",\n        \"namespace\" : \"article\",\n        \"anonymous\" : \"0\",\n        \"unpatrolled\" : \"0\",\n        \"page\" : \"112_U.S._580\",\n        \"language\" : \"en\",\n        \"newpage\" : \"1\",\n        \"user\" : \"MZMcBride\",\n        \"count\" : 1.0,\n        \"added\" : 70.0,\n        \"delta\" : 70.0,\n        \"variation\" : 70.0,\n        \"deleted\" : 0.0\n    }]}\n]\n```\n\n----------------------------------------\n\nTITLE: Sample TSV (Tab-Delimited) Data Format for Druid Ingestion\nDESCRIPTION: Example of TSV-formatted data for ingestion by Apache Druid. Each line represents an event with fields separated by tabs without column headers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n2013-08-31T01:02:33Z\t\"Gypsy Danger\"\t\"en\"\t\"nuclear\"\t\"true\"\t\"true\"\t\"false\"\t\"false\"\t\"article\"\t\"North America\"\t\"United States\"\t\"Bay Area\"\t\"San Francisco\"\t57\t200\t-143\n2013-08-31T03:32:45Z\t\"Striker Eureka\"\t\"en\"\t\"speed\"\t\"false\"\t\"true\"\t\"true\"\t\"false\"\t\"wikipedia\"\t\"Australia\"\t\"Australia\"\t\"Cantebury\"\t\"Syndey\"\t459\t129\t330\n2013-08-31T07:11:21Z\t\"Cherno Alpha\"\t\"ru\"\t\"masterYi\"\t\"false\"\t\"true\"\t\"true\"\t\"false\"\t\"article\"\t\"Asia\"\t\"Russia\"\t\"Oblast\"\t\"Moscow\"\t123\t12\t111\n2013-08-31T11:58:39Z\t\"Crimson Typhoon\"\t\"zh\"\t\"triplets\"\t\"true\"\t\"false\"\t\"true\"\t\"false\"\t\"wikipedia\"\t\"Asia\"\t\"China\"\t\"Shanxi\"\t\"Taiyuan\"\t905\t5\t900\n2013-08-31T12:41:27Z\t\"Coyote Tango\"\t\"ja\"\t\"cancer\"\t\"true\"\t\"false\"\t\"true\"\t\"false\"\t\"wikipedia\"\t\"Asia\"\t\"Japan\"\t\"Kanto\"\t\"Tokyo\"\t1\t10\t-9\n```\n\n----------------------------------------\n\nTITLE: Running Compaction Task with Day Granularity\nDESCRIPTION: Command to submit the compaction task that changes the segment granularity from hourly to daily.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Query Results Format\nDESCRIPTION: Example of query results showing timestamp granularity, dimension values, count, and min/max timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T00:00:00.000Z A 4 2015-07-28T01:00:00.000Z 2015-07-28T05:00:00.000Z\n2015-07-28T00:00:00.000Z B 2 2015-07-28T04:00:00.000Z 2015-07-28T06:00:00.000Z\n2015-07-29T00:00:00.000Z A 2 2015-07-29T03:00:00.000Z 2015-07-29T04:00:00.000Z\n2015-07-29T00:00:00.000Z C 2 2015-07-29T01:00:00.000Z 2015-07-29T02:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Example Segment Payload JSON Structure for Druid Metadata\nDESCRIPTION: Sample JSON structure stored in the 'payload' column of the segments table in Druid's metadata storage. Contains segment metadata including data source, interval, version, and loading specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n \"dataSource\":\"wikipedia\",\n \"interval\":\"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z\",\n \"version\":\"2012-05-24T00:10:00.046Z\",\n \"loadSpec\":{\n    \"type\":\"s3_zip\",\n    \"bucket\":\"bucket_for_segment\",\n    \"key\":\"path/to/segment/on/s3\"\n },\n \"dimensions\":\"comma-delimited-list-of-dimension-names\",\n \"metrics\":\"comma-delimited-list-of-metric-names\",\n \"shardSpec\":{\"type\":\"none\"},\n \"binaryVersion\":9,\n \"size\":size_of_segment,\n \"identifier\":\"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Deep Storage in Druid\nDESCRIPTION: Configuration changes in common.runtime.properties file to set up HDFS as deep storage for Druid segments and indexing service logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-hdfs-storage\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Describing Druid Alert Structure in JSON\nDESCRIPTION: This JSON structure outlines the common fields present in all Druid alerts. It includes timestamp, service name, host name, severity, description, and additional data for exceptions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/alerts.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"<time the alert was created>\",\n  \"service\": \"<service name that emitted the alert>\",\n  \"host\": \"<host name that emitted the alert>\",\n  \"severity\": \"<severity of the alert>\",\n  \"description\": \"<description of the alert>\",\n  \"data\": {\n    \"exceptionType\": \"<type of exception>\",\n    \"exceptionMessage\": \"<exception message>\",\n    \"exceptionStackTrace\": \"<exception stack trace>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Router Runtime Properties Configuration\nDESCRIPTION: Runtime configuration properties for a Router node including service settings and HTTP connection parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=#{IP_ADDR}:8080\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.defaultBrokerServiceName=druid:broker-cold\ndruid.router.coordinatorServiceName=druid:coordinator\ndruid.router.tierToBrokerMap={\"hot\":\"druid:broker-hot\",\"_default_tier\":\"druid:broker-cold\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the router proxy http client\ndruid.router.http.numMaxThreads=100\n\ndruid.server.http.numThreads=100\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Ingestion Task Specification\nDESCRIPTION: A complete JSON specification for a Druid index task that includes dataSchema configuration with dimensions, metrics, and granularity specifications, along with the input source configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Fragment Search in Druid\nDESCRIPTION: Specifies a search that matches if a dimension value contains all specified fragments, with optional case sensitivity control.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"fragment\",\n  \"case_sensitive\" : false,\n  \"values\" : [\"fragment1\", \"fragment2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Management Proxy Routing Table\nDESCRIPTION: A markdown table showing the mapping between request routes, destinations, rewritten routes, and examples for both implicit and explicit routing patterns in Druid's management proxy.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n|Request Route|Destination|Rewritten Route|Example|\n|-------------|-----------|---------------|-------|\n|`/druid/coordinator/*`|Coordinator|`/druid/coordinator/*`|`router:8888/druid/coordinator/v1/datasources` -> `coordinator:8081/druid/coordinator/v1/datasources`|\n|`/druid/indexer/*`|Overlord|`/druid/indexer/*`|`router:8888/druid/indexer/v1/task` -> `overlord:8090/druid/indexer/v1/task`|\n|`/proxy/coordinator/*`|Coordinator|`/*`|`router:8888/proxy/coordinator/status` -> `coordinator:8081/status`|\n|`/proxy/overlord/*`|Overlord|`/*`|`router:8888/proxy/overlord/druid/indexer/v1/isLeader` -> `overlord:8090/druid/indexer/v1/isLeader`|\n```\n\n----------------------------------------\n\nTITLE: NOT Logical Filter in Druid\nDESCRIPTION: Logical NOT filter that negates another filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"not\", \"field\": <filter> }\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for Prism\nDESCRIPTION: Specifies the MIT license for the Prism syntax highlighting library created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Executing View Query in Druid\nDESCRIPTION: Example of a view query that wraps a groupBy query to leverage materialized views for optimized performance. Demonstrates querying user dimensions with aggregations and limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"view\",\n    \"query\": {\n        \"queryType\": \"groupBy\",\n        \"dataSource\": \"wikiticker\",\n        \"granularity\": \"all\",\n        \"dimensions\": [\n            \"user\"\n        ],\n        \"limitSpec\": {\n            \"type\": \"default\",\n            \"limit\": 1,\n            \"columns\": [\n                {\n                    \"dimension\": \"added\",\n                    \"direction\": \"descending\",\n                    \"dimensionOrder\": \"numeric\"\n                }\n            ]\n        },\n        \"aggregations\": [\n            {\n                \"type\": \"longSum\",\n                \"name\": \"added\",\n                \"fieldName\": \"added\"\n            }\n        ],\n        \"intervals\": [\n            \"2015-09-12/2015-09-13\"\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Numbered Sharding in Druid\nDESCRIPTION: JSON configuration for the 'numbered' sharding strategy in Druid. This approach requires sequential partition numbering and explicitly setting the total number of partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"numbered\",\n        \"partitionNum\": 0,\n        \"partitions\": 2\n    }\n```\n\n----------------------------------------\n\nTITLE: Retrieving MiddleManager Enabled Status Response in JSON\nDESCRIPTION: Example JSON response from the /druid/worker/v1/enabled endpoint showing the enabled state of a MiddleManager with its host and port as the key.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":true}\n```\n\n----------------------------------------\n\nTITLE: Filtering Multi-value Dimensions with AND Condition in Apache Druid\nDESCRIPTION: Example of an 'and' filter that matches rows containing both 't1' and 't3' in the 'tags' multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"and\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to MySQL as Root\nDESCRIPTION: Command to connect to a MySQL server with root privileges in preparation for creating a Druid database and user.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n> mysql -u root\n```\n\n----------------------------------------\n\nTITLE: Optimized Query Rewrite Example for Lookup Extraction\nDESCRIPTION: Shows how Druid optimizes a lookup-based extraction filter by rewriting it as a clause of selector filters, improving query performance for lookups that map multiple keys to the same value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"filter\":{\n      \"type\":\"or\",\n      \"fields\":[\n         {\n            \"filter\":{\n               \"type\":\"selector\",\n               \"dimension\":\"product\",\n               \"value\":\"product_1\"\n            }\n         },\n         {\n            \"filter\":{\n               \"type\":\"selector\",\n               \"dimension\":\"product\",\n               \"value\":\"product_3\"\n            }\n         }\n      ]\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Suspending Supervisor POST Endpoint\nDESCRIPTION: REST endpoint to suspend indexing tasks for a specific supervisor while keeping the supervisor operational.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/suspend\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec with Partition 0 for Redundancy in Druid\nDESCRIPTION: Configuration example for a linear shardSpec with partitionNum 0. When multiple processes use the same partitionNum, they provide redundancy as brokers will assume they contain identical data and query only one of them.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-pull.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos and HTTP Basic Authenticators in Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure the authentication chain to use Kerberos and HTTP Basic authenticators from the druid-kerberos and druid-basic-security core extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/auth.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authenticatorChain\":[\"kerberos\", \"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Downloading and Setting Up ZooKeeper in Bash\nDESCRIPTION: Commands to download ZooKeeper 3.4.11, extract it, and move it to the 'zk' directory under the Druid package root. ZooKeeper is a dependency required by Druid for distributed coordination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/index.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Configuring G1 Garbage Collector for Apache Druid in Java\nDESCRIPTION: This snippet shows the Java VM argument to enable the G1 Garbage Collector for Apache Druid. G1GC is recommended for improved performance and reduced pause times.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n-XX:+UseG1GC\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Coordinator Leader Status in HTTP\nDESCRIPTION: This HTTP GET request returns the current leader Coordinator of the Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/leader\n```\n\n----------------------------------------\n\nTITLE: Configuring String First Aggregator in Druid\nDESCRIPTION: Computes string metric value with minimum timestamp or null if no rows exist. Includes optional maxStringBytes and filterNullValues parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing floatMin Aggregator in Druid\nDESCRIPTION: The floatMin aggregator computes the minimum of all metric values and Float.POSITIVE_INFINITY. It requires name for the output and fieldName to specify which metric column to evaluate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Priority Router Strategy Configuration\nDESCRIPTION: JSON configuration for the priority router strategy which routes queries based on their priority level to different Brokers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"priority\",\n  \"minPriority\":0,\n  \"maxPriority\":1\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Timestamp Min/Max Aggregations in Druid\nDESCRIPTION: Example of a GroupBy query using timeMin and timeMax aggregators in Druid. This query groups data by day and product, calculating count and timestamp min/max for each group.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"timeMinMax\",\n  \"granularity\": \"DAY\",\n  \"dimensions\": [\"product\"],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    },\n    {\n      \"type\": \"timeMin\",\n      \"name\": \"<output_name of timeMin>\",\n      \"fieldName\": \"tmin\"\n    },\n    {\n      \"type\": \"timeMax\",\n      \"name\": \"<output_name of timeMax>\",\n      \"fieldName\": \"tmax\"\n    }\n  ],\n  \"intervals\": [\n    \"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Tier Configuration Example for Country Code Lookup\nDESCRIPTION: JSON configuration showing a complete example of a lookup tier configuration for a 'country_code' lookup in the 'realtime_customer2' tier using JDBC extraction.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupValues\",\n          \"keyColumn\": \"value_id\",\n          \"valueColumn\": \"value_text\",\n          \"filter\": \"value_type='country'\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Expression Operator Example\nDESCRIPTION: Example showing SQL-like expression operators in decreasing order of precedence, including unary, binary, comparison and logical operators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/misc/math-expr.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n!, -       // Unary NOT and Minus\n^          // Binary power op\n*, /, %    // Binary multiplicative\n+, -       // Binary additive\n<, <=, >, >=, ==, !=  // Binary Comparison\n&&, |      // Binary Logical AND, OR\n```\n\n----------------------------------------\n\nTITLE: Querying Materialized Views in Druid\nDESCRIPTION: Example view query specification showing how to query materialized views. Includes a nested groupBy query with aggregations and limitSpec configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"view\",\n    \"query\": {\n        \"queryType\": \"groupBy\",\n        \"dataSource\": \"wikiticker\",\n        \"granularity\": \"all\",\n        \"dimensions\": [\n            \"user\"\n        ],\n        \"limitSpec\": {\n            \"type\": \"default\",\n            \"limit\": 1,\n            \"columns\": [\n                {\n                    \"dimension\": \"added\",\n                    \"direction\": \"descending\",\n                    \"dimensionOrder\": \"numeric\"\n                }\n            ]\n        },\n        \"aggregations\": [\n            {\n                \"type\": \"longSum\",\n                \"name\": \"added\",\n                \"fieldName\": \"added\"\n            }\n        ],\n        \"intervals\": [\n            \"2015-09-12/2015-09-13\"\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Coordinator Endpoint Response for Country Code Lookup\nDESCRIPTION: Example JSON response from the Druid Coordinator API endpoint for the 'country_code' lookup. Shows how lookups are presented in API responses.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v0\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n      \"type\": \"jdbc\",\n      \"connectorConfig\": {\n        \"createTables\": true,\n        \"connectURI\": \"jdbc:mysql://localhost:3306/druid\",\n        \"user\": \"druid\",\n        \"password\": \"diurd\"\n      },\n      \"table\": \"lookupValues\",\n      \"keyColumn\": \"value_id\",\n      \"valueColumn\": \"value_text\",\n      \"filter\": \"value_type='country'\",\n      \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling Off-heap Lookup Cache in Druid\nDESCRIPTION: Example configuration for an off-heap lookup cache that is cached once and never swapped using JDBC data fetcher.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"offHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Segment Identifier without Partition Number in Apache Druid\nDESCRIPTION: Example of a segment identifier in Apache Druid for the first partition (partition 0), which omits the partition number from the identifier.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/index.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z\n```\n\n----------------------------------------\n\nTITLE: Inline Schema Avro Decoder Configuration\nDESCRIPTION: Configuration for inline schema-based Avro bytes decoder where all input events use the same schema. Demonstrates how to specify schema directly in the task configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"avroBytesDecoder\": {\n    \"type\": \"schema_inline\",\n    \"schema\": {\n      \"namespace\": \"org.apache.druid.data\",\n      \"name\": \"User\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"FullName\", \"type\": \"string\" },\n        { \"name\": \"Country\", \"type\": \"string\" }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Timestamps in Apache Druid Expressions\nDESCRIPTION: Illustrates the use of the timestamp_parse function to convert string expressions into timestamps using specified date-time patterns and time zones in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/misc/math-expr.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ntimestamp_parse(string expr, [pattern, [timezone]])\n```\n\n----------------------------------------\n\nTITLE: Running Tranquility with Thrift Extensions in Apache Druid\nDESCRIPTION: Bash command for running tranquility with Kafka and the Thrift extensions loaded. This command demonstrates how to specify the extensions directory and load list for the Druid Thrift extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka \\\n  -configFile $jsonConfig \\\n  -Ddruid.extensions.directory=/path/to/extensions \\\n  -Ddruid.extensions.loadList='[\"druid-thrift-extensions\"]'\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Import SQL Commands\nDESCRIPTION: SQL commands for importing CSV metadata into PostgreSQL database tables with column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/export-metadata.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCOPY druid_segments(id,dataSource,created_date,start,\"end\",partitioned,version,used,payload) FROM '/tmp/csv/druid_segments.csv' DELIMITER ',' CSV;\n\nCOPY druid_rules(id,dataSource,version,payload) FROM '/tmp/csv/druid_rules.csv' DELIMITER ',' CSV;\n\nCOPY druid_config(name,payload) FROM '/tmp/csv/druid_config.csv' DELIMITER ',' CSV;\n\nCOPY druid_dataSource(dataSource,created_date,commit_metadata_payload,commit_metadata_sha1) FROM '/tmp/csv/druid_dataSource.csv' DELIMITER ',' CSV;\n\nCOPY druid_supervisors(id,spec_id,created_date,payload) FROM '/tmp/csv/druid_supervisors.csv' DELIMITER ',' CSV;\n```\n\n----------------------------------------\n\nTITLE: HDFS Kerberos Authentication Configuration\nDESCRIPTION: Properties for configuring Kerberos authentication with HDFS, including principal and keytab file settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.hadoop.security.kerberos.principal=druid@EXAMPLE.COM\ndruid.hadoop.security.kerberos.keytab=/etc/security/keytabs/druid.headlessUser.keytab\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Results with Day Granularity in Apache Druid\nDESCRIPTION: Example results from a GroupBy query in Apache Druid using 'day' granularity. Demonstrates how data is aggregated into daily buckets, with counts for each language.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T00:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T00:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T00:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-03T00:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Segment Identifier Path in ZooKeeper\nDESCRIPTION: The ephemeral ZooKeeper path structure used to track individual segments being served by nodes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_\n```\n\n----------------------------------------\n\nTITLE: Importing Metadata into MySQL Database\nDESCRIPTION: SQL commands to import exported metadata CSV files into MySQL database tables. It uses the LOAD DATA INFILE statement for each Druid metadata table, specifying field terminators and enclosures.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/export-metadata.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nLOAD DATA INFILE '/tmp/csv/druid_segments.csv' INTO TABLE druid_segments FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,created_date,start,end,partitioned,version,used,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_rules.csv' INTO TABLE druid_rules FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,version,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_config.csv' INTO TABLE druid_config FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (name,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_dataSource.csv' INTO TABLE druid_dataSource FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (dataSource,created_date,commit_metadata_payload,commit_metadata_sha1); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_supervisors.csv' INTO TABLE druid_supervisors FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,spec_id,created_date,payload); SHOW WARNINGS;\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced TLS Settings in Apache Druid\nDESCRIPTION: This configuration table shows advanced TLS settings for Apache Druid, including options for key manager algorithm, cipher suites, and protocols.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/tls-support.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.server.https.keyManagerFactoryAlgorithm`|Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).|`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.server.https.keyManagerPassword`|The [Password Provider](../operations/password-provider.html) or String password for the Key Manager.|none|no|\n|`druid.server.https.includeCipherSuites`|List of cipher suite names to include. You can either use the exact cipher suite name or a regular expression.|Jetty's default include cipher list|no|\n|`druid.server.https.excludeCipherSuites`|List of cipher suite names to exclude. You can either use the exact cipher suite name or a regular expression.|Jetty's default exclude cipher list|no|\n|`druid.server.https.includeProtocols`|List of exact protocols names to include.|Jetty's default include protocol list|no|\n|`druid.server.https.excludeProtocols`|List of exact protocols names to exclude.|Jetty's default exclude protocol list|no|\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.df5a69b6.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring True Filter in Druid Query (JSON)\nDESCRIPTION: This example demonstrates how to configure a true filter in a Druid query. The true filter matches all values and can be used to temporarily disable other filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"true\" }\n```\n\n----------------------------------------\n\nTITLE: Implementing Regex Search in Druid\nDESCRIPTION: Specifies a regex pattern-based search that matches if any part of a dimension value matches the given pattern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/searchqueryspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"regex\",\n  \"pattern\" : \"some_pattern\"\n}\n```\n\n----------------------------------------\n\nTITLE: Compaction Task Configuration - JSON\nDESCRIPTION: JSON configuration for the compaction task that combines 24 segments into one. Specifies datasource, time interval, and tuning parameters for controlling output segment size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"targetPartitionSize\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Same Interval Merge Task in Apache Druid (Deprecated)\nDESCRIPTION: JSON configuration for a Same Interval Merge task in Apache Druid. This deprecated task is a shortcut of the merge task, merging all segments within a specified interval, with options for aggregations and rollup behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"same_interval_merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"interval\": <DataSegment objects in this interval are going to be merged>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Submitting Druid Ingestion Task with cURL\nDESCRIPTION: cURL command to manually POST a JSON ingestion task specification to the Druid Overlord API without using the helper script.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Data Source Metadata Query Response Format\nDESCRIPTION: Example response format from a Data Source Metadata query showing the timestamp and maxIngestedEventTime for the specified dataSource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"maxIngestedEventTime\" : \"2013-05-09T18:24:09.007Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring doubleMax Aggregator in Apache Druid\nDESCRIPTION: The doubleMax aggregator computes the maximum of all metric values and Double.NEGATIVE_INFINITY. It takes an output name and the field name of the metric to find the maximum value of.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Basic Log4j2 Configuration for Apache Druid\nDESCRIPTION: Basic log4j2 configuration file that sets up console logging with timestamp patterns and INFO level logging. Includes commented section for enabling HTTP request logging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/logging.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n\n    <!-- Uncomment to enable logging of all HTTP requests\n    <Logger name=\"org.apache.druid.jetty.RequestLog\" additivity=\"false\" level=\"DEBUG\">\n        <AppenderRef ref=\"Console\"/>\n    </Logger>\n    -->\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring Inverted TopNMetricSpec in Druid JSON\nDESCRIPTION: Demonstrates how to use an inverted metric specification to sort dimension values in ascending order by inverting the order of a delegate metric spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"inverted\",\n    \"metric\": <delegate_top_n_metric_spec>\n}\n```\n\n----------------------------------------\n\nTITLE: Running Druid Peon Process in Java\nDESCRIPTION: Command to run a Druid Peon process independently for development purposes. Takes a task file containing the task JSON object and a status file path for output. This should rarely be run outside of MiddleManager control.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/peons.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main internal peon <task_file> <status_file>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Selector Filter on Timestamp Column in Druid (JSON)\nDESCRIPTION: This snippet shows how to set up a selector filter on the timestamp column in a Druid query. It filters for a specific long timestamp value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"124457387532\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Maven Build Command for Druid\nDESCRIPTION: Basic Maven command to build Druid from source, which runs static analysis, unit tests, compiles classes, and packages projects into JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/build.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: Getting All Supervisor History GET Endpoint\nDESCRIPTION: REST endpoint to retrieve audit history of specifications for all supervisors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_17\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/supervisor/history\n```\n\n----------------------------------------\n\nTITLE: Checking Historical Segment Loading Status using GET HTTP Requests\nDESCRIPTION: Endpoints for checking if all segments in the local cache of a historical node have been loaded, which is useful for determining if a historical node is ready to be queried after a restart.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_17\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/historical/v1/loadstatus\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/historical/v1/readiness\n```\n\n----------------------------------------\n\nTITLE: Implementing longMin Aggregator in Druid\nDESCRIPTION: The longMin aggregator computes the minimum of all metric values and Long.MAX_VALUE. It requires name for the output and fieldName to specify which metric column to evaluate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Multi-value Column Data Structure Example\nDESCRIPTION: Example demonstrating how Druid handles multi-value columns, showing modified dictionary mapping, column data array with array values, and corresponding bitmap structures.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/segments.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   [0,1],\n   1,\n   1]\n\n3: Bitmaps - one for each unique value\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,1,1,1]\n```\n\n----------------------------------------\n\nTITLE: Enabling Management Proxy in Router Configuration\nDESCRIPTION: Configuration setting to enable the Router as a management proxy for Coordinator and Overlord processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/router.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.managementProxy.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Registered Lookup Extraction Function in Druid\nDESCRIPTION: Uses a registered lookup to replace dimension values. Includes options for handling missing values and optimizing query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"registeredLookup\",\n  \"lookup\":\"some_lookup_name\",\n  \"retainMissingValue\":true\n}\n```\n\n----------------------------------------\n\nTITLE: HTTP Endpoint for Coordinator Configuration in Druid\nDESCRIPTION: HTTP endpoint to submit dynamic configuration to the Druid Coordinator. The configuration is sent as a JSON object to this endpoint via POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_28\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data in Apache Druid\nDESCRIPTION: This command loads the initial dataset into a datasource called 'updates-tutorial' using a predefined index task specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Setting Realtime Operation Parameters in YAML for Apache Druid\nDESCRIPTION: This YAML configuration defines the publish type and specFile location for the Apache Druid Realtime Process. It determines where segments are published and where the realtime specification file is located.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/realtime.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.publish.type: metadata\ndruid.realtime.specFile: none\n```\n\n----------------------------------------\n\nTITLE: Configuring DataSource InputSpec for Hadoop Reindexing in Druid\nDESCRIPTION: Example configuration for a Hadoop inputSpec of type 'dataSource' which reads data from existing Druid segments for reindexing purposes. This configuration targets the 'wikipedia' datasource with a two-week interval starting from October 20, 2014.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"dataSource\",\n    \"ingestionSpec\" : {\n      \"dataSource\": \"wikipedia\",\n      \"intervals\": [\"2014-10-20T00:00:00Z/P2W\"]\n    }\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension TopNMetricSpec in Druid JSON\nDESCRIPTION: Shows how to specify a dimension-based metric for sorting topN results in a Druid query. This includes options for specifying the sorting order and a starting point for pagination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"dimension\",\n    \"ordering\": \"lexicographic\",\n    \"previousStop\": \"<previousStop_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage with HDFS Extension in Apache Druid\nDESCRIPTION: Configuration properties for using Google Cloud Storage (GCS) with the HDFS extension in Apache Druid as a deep storage solution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.storage.type`|hdfs||Must be set.|\n|`druid.storage.storageDirectory`||gs://bucket/example/directory|Must be set.|\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchUnion Post-Aggregator in Druid\nDESCRIPTION: This JSON configuration defines the HLLSketchUnion post-aggregator. It performs a union operation on multiple HLL sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchUnion\",\n  \"name\": <output name>,\n  \"fields\"  : <array of post aggregators that return HLL sketches>,\n  \"lgK\": <log2 of K for the target sketch>,\n  \"tgtHllType\" : <target HLL type>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultLimitSpec in Druid GroupBy Queries\nDESCRIPTION: Defines the structure for DefaultLimitSpec which allows limiting and ordering groupBy query results. Takes a limit value and list of columns for ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/limitspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"default\",\n    \"limit\"   : <integer_value>,\n    \"columns\" : [list of OrderByColumnSpec],\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CombiningFirehose in Apache Druid\nDESCRIPTION: This snippet shows how to configure a CombiningFirehose to combine and merge data from multiple firehoses. It includes a list of delegate firehoses.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/firehose.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"combining\",\n    \"delegates\" : [ { firehose1 }, { firehose2 }, ..... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring VarianceFold Aggregator for Druid Queries\nDESCRIPTION: JSON configuration for the varianceFold aggregator used in Druid queries. This aggregator merges pre-computed variance metrics with options to specify the estimator method.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"varianceFold\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Before Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period drop before rule that indicates segments before a specified rolling time period should be dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropBeforeByPeriod\",\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Submitting Druid Ingestion Task via cURL\nDESCRIPTION: Command to submit the ingestion task directly using cURL, bypassing the helper script by POSTing the JSON specification to the Druid overlord.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Configuring longFirst Aggregator in Apache Druid for Queries\nDESCRIPTION: The longFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"longFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Deep Storage in Druid\nDESCRIPTION: Properties for configuring deep storage in Druid. These settings determine how to push and pull Segments from deep storage, with options for local, S3, and HDFS storage types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=local\ndruid.storage.storageDirectory=/tmp/druid/localStorage\n```\n\n----------------------------------------\n\nTITLE: Middle Manager Disable API Request\nDESCRIPTION: HTTP POST request endpoint to disable a Middle Manager node before updating, preventing it from receiving new tasks while allowing current tasks to complete.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/disable\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Master Server With Zookeeper\nDESCRIPTION: Starts the Druid Master server processes along with a local Zookeeper instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-cluster-master-with-zk-server\n```\n\n----------------------------------------\n\nTITLE: Period Granularity Configuration in Druid\nDESCRIPTION: Examples of period granularity specifications with timezone and origin configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/granularities.md#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P2D\", \"timeZone\": \"America/Los_Angeles\"}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P3M\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"2012-02-01T00:00:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Coordinator Operation Properties in Markdown\nDESCRIPTION: A markdown table listing various configuration properties for the Coordinator's operation, including run periods, timeouts, and balancing strategies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_24\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.coordinator.period`|The run period for the Coordinator. The Coordinator's operates by maintaining the current state of the world in memory and periodically looking at the set of segments available and segments being served to make decisions about whether any changes need to be made to the data topology. This property sets the delay between each of these runs.|PT60S|\n|`druid.coordinator.period.indexingPeriod`|How often to send compact/merge/conversion tasks to the indexing service. It's recommended to be longer than `druid.manager.segments.pollDuration`|PT1800S (30 mins)|\n|`druid.coordinator.startDelay`|The operation of the Coordinator works on the assumption that it has an up-to-date view of the state of the world when it runs, the current ZK interaction code, however, is written in a way that doesn't allow the Coordinator to know for a fact that it's done loading the current state of the world. This delay is a hack to give it enough time to believe that it has all the data.|PT300S|\n|`druid.coordinator.load.timeout`|The timeout duration for when the Coordinator assigns a segment to a Historical process.|PT15M|\n|`druid.coordinator.kill.pendingSegments.on`|Boolean flag for whether or not the Coordinator clean up old entries in the `pendingSegments` table of metadata store. If set to true, Coordinator will check the created time of most recently complete task. If it doesn't exist, it finds the created time of the earlist running/pending/waiting tasks. Once the created time is found, then for all dataSources not in the `killPendingSegmentsSkipList` (see [Dynamic configuration](#dynamic-configuration)), Coordinator will ask the Overlord to clean up the entries 1 day or more older than the found created time in the `pendingSegments` table. This will be done periodically based on `druid.coordinator.period` specified.|false|\n|`druid.coordinator.kill.on`|Boolean flag for whether or not the Coordinator should submit kill task for unused segments, that is, hard delete them from metadata store and deep storage. If set to true, then for all whitelisted dataSources (or optionally all), Coordinator will submit tasks periodically based on `period` specified. These kill tasks will delete all segments except for the last `durationToRetain` period. Whitelist or All can be set via dynamic configuration `killAllDataSources` and `killDataSourceWhitelist` described later.|false|\n|`druid.coordinator.kill.period`|How often to send kill tasks to the indexing service. Value must be greater than `druid.coordinator.period.indexingPeriod`. Only applies if kill is turned on.|P1D (1 Day)|\n|`druid.coordinator.kill.durationToRetain`| Do not kill segments in last `durationToRetain`, must be greater or equal to 0. Only applies and MUST be specified if kill is turned on. Note that default value is invalid.|PT-1S (-1 seconds)|\n|`druid.coordinator.kill.maxSegments`|Kill at most n segments per kill task submission, must be greater than 0. Only applies and MUST be specified if kill is turned on. Note that default value is invalid.|0|\n|`druid.coordinator.balancer.strategy`|Specify the type of balancing strategy that the coordinator should use to distribute segments among the historicals. `cachingCost` is logically equivalent to `cost` but is more CPU-efficient on large clusters and will replace `cost` in the future versions, users are invited to try it. Use `diskNormalized` to distribute segments among processes so that the disks fill up uniformly and use `random` to randomly pick processes to distribute segments.|`cost`|\n|`druid.coordinator.balancer.cachingCost.awaitInitialization`|Whether to wait for segment view initialization before creating the `cachingCost` balancing strategy. This property is enabled only when `druid.coordinator.balancer.strategy` is `cachingCost`. If set to 'true', the Coordinator will not start to assign segments, until the segment view is initialized. If set to 'false', the Coordinator will fallback to use the `cost` balancing strategy only if the segment view is not initialized yet. Notes, it may take much time to wait for the initialization since the `cachingCost` balancing strategy involves much computing to build itself.|false|\n|`druid.coordinator.loadqueuepeon.repeatDelay`|The start and repeat delay for the loadqueuepeon , which manages the load and drop of segments.|PT0.050S (50 ms)|\n|`druid.coordinator.asOverlord.enabled`|Boolean value for whether this Coordinator process should act like an Overlord as well. This configuration allows users to simplify a druid cluster by not having to deploy any standalone Overlord processes. If set to true, then Overlord console is available at `http://coordinator-host:port/console.html` and be sure to set `druid.coordinator.asOverlord.overlordService` also. See next.|false|\n|`druid.coordinator.asOverlord.overlordService`| Required, if `druid.coordinator.asOverlord.enabled` is `true`. This must be same value as `druid.service` on standalone Overlord processes and `druid.selectors.indexing.serviceName` on Middle Managers.|NULL|\n```\n\n----------------------------------------\n\nTITLE: Configuring VarianceFold Aggregator for Druid Queries\nDESCRIPTION: JSON configuration for the varianceFold aggregator used in Druid queries. This aggregator merges pre-computed variance metrics with options to specify the estimator method.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"varianceFold\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Equal Buckets Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for computing histogram representation with equal-sized bins. Specifies output name, source aggregator, and number of buckets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"equalBuckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"numBuckets\": <count>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Aggregator in Druid\nDESCRIPTION: Configuration for hyperUnique aggregator that handles pre-computed HyperLogLog values with options for input validation and result rounding.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/hll-old.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"hyperUnique\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"isInputHyperUnique\" : false,\n  \"round\" : false\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Paths in Apache Druid\nDESCRIPTION: These properties set the Zookeeper paths for various Druid components and operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.paths.propertiesPath=${druid.zk.paths.base}/properties\ndruid.zk.paths.announcementsPath=${druid.zk.paths.base}/announcements\ndruid.zk.paths.liveSegmentsPath=${druid.zk.paths.base}/segments\ndruid.zk.paths.loadQueuePath=${druid.zk.paths.base}/loadQueue\ndruid.zk.paths.coordinatorPath=${druid.zk.paths.base}/coordinator\ndruid.zk.paths.servedSegmentsPath=${druid.zk.paths.base}/servedSegments\n```\n\n----------------------------------------\n\nTITLE: Derby Import SQL Commands\nDESCRIPTION: SQL commands for importing CSV metadata into Derby database tables.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/export-metadata.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SEGMENTS','/tmp/csv/druid_segments.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_RULES','/tmp/csv/druid_rules.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_CONFIG','/tmp/csv/druid_config.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_DATASOURCE','/tmp/csv/druid_dataSource.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SUPERVISORS','/tmp/csv/druid_supervisors.csv',',','\"',null,0);\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage with HDFS Extension in Apache Druid\nDESCRIPTION: YAML configuration for using the HDFS extension with Google Cloud Storage as deep storage in Apache Druid. Specifies the storage type and GCS bucket directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type: hdfs\ndruid.storage.storageDirectory: gs://bucket/example/directory\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Storage for Druid Cluster\nDESCRIPTION: Updates the common.runtime.properties file to use HDFS for deep storage and indexing logs. Requires adding the HDFS extension and configuring storage directories.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-hdfs-storage\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Sample Response Format for a groupBy Query in Apache Druid\nDESCRIPTION: Illustrates the JSON structure returned by a groupBy query, showing how results are formatted with timestamp, version, and event objects containing dimension values and aggregated metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:00.000Z\",\n    \"event\" : {\n      \"country\" : <some_dim_value_one>,\n      \"device\" : <some_dim_value_two>,\n      \"total_usage\" : <some_value_one>,\n      \"data_transfer\" :<some_value_two>,\n      \"avg_usage\" : <some_avg_usage_value>\n    }\n  }, \n  {\n    \"version\" : \"v1\",\n    \"timestamp\" : \"2012-01-01T00:00:12.000Z\",\n    \"event\" : {\n      \"dim1\" : <some_other_dim_value_one>,\n      \"dim2\" : <some_other_dim_value_two>,\n      \"sample_name1\" : <some_other_value_one>,\n      \"sample_name2\" :<some_other_value_two>,\n      \"avg_usage\" : <some_other_avg_usage_value>\n    }\n  },\n...\n]\n```\n\n----------------------------------------\n\nTITLE: OR Logical Expression Having Filter in Druid\nDESCRIPTION: Shows how to use an OR logical expression to combine multiple Having filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Load Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period load rule that specifies how many replicas of a segment should exist in different server tiers for a rolling time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true,\n  \"tieredReplicants\": {\n      \"hot\": 1,\n      \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Druid GroupBy Query with Hour Granularity\nDESCRIPTION: Example of a GroupBy query configuration using hour granularity for time-based aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/granularities.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":\"hour\",\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Period Granularity Specifications in Apache Druid\nDESCRIPTION: Examples of specifying period granularities in Apache Druid queries. These granularities allow for time buckets based on calendar periods and support time zone specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P2D\", \"timeZone\": \"America/Los_Angeles\"}\n\n{\"type\": \"period\", \"period\": \"P3M\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"2012-02-01T00:00:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Disabling MiddleManager Response in JSON\nDESCRIPTION: This JSON response confirms that a MiddleManager has been disabled. The key is the combined druid.host and druid.port, with 'disabled' as the value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"disabled\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Discovery in Druid\nDESCRIPTION: These properties configure how Druid discovers segments, including the discovery method and filters for tiers and dataSources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_22\n\nLANGUAGE: properties\nCODE:\n```\ndruid.announcer.type=batch\ndruid.broker.segment.watchedTiers=\ndruid.broker.segment.watchedDataSources=\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Properties to Control Hadoop Classloader Behavior\nDESCRIPTION: JSON configuration for Hadoop job properties that excludes javax.validation classes from being loaded from the system classpath while including specific Java and Apache packages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\": {\n  \"mapreduce.job.classloader\": \"true\",\n  \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Aggregator in Druid JSON\nDESCRIPTION: Defines a JavaScript aggregator to compute arbitrary JavaScript functions over a set of columns. It requires field names, aggregate function, combine function, and reset function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"javascript\",\n  \"name\": \"<output_name>\",\n  \"fieldNames\"  : [ <column1>, <column2>, ... ],\n  \"fnAggregate\" : \"function(current, column1, column2, ...) {\n                     <updates partial aggregate (current) based on the current row values>\n                     return <updated partial aggregate>\n                   }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return <combined partial results>; }\",\n  \"fnReset\"     : \"function()                   { return <initial value>; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authenticator in Apache Druid\nDESCRIPTION: Configuration example for setting up a Basic authenticator in Druid's authenticator chain, including initial admin password, internal client password, and authorizer reference.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyBasicAuthenticator\"]\n\ndruid.auth.authenticator.MyBasicAuthenticator.type=basic\ndruid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1\ndruid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2\ndruid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Combining and Overwriting Data in Apache Druid\nDESCRIPTION: This command submits a task that combines existing data with new data and overwrites the original data in the 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Lower Case Extraction Function in Druid\nDESCRIPTION: Shows how to convert dimension values to lowercase using the default locale of the Java Virtual Machine.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"lower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Services with the Supervise Script\nDESCRIPTION: Command to start all Druid services using the supervise script with a tutorial configuration. This launches ZooKeeper and all necessary Druid components as a local cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n```\n\n----------------------------------------\n\nTITLE: Equal Buckets Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Equal Buckets post-aggregator, which computes a visual representation of the approximate histogram with equal-sized bins.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"equalBuckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"numBuckets\": <count>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Transform Spec Structure in Druid\nDESCRIPTION: Defines the basic syntax structure for a transformSpec in Druid, which includes optional transforms and filter properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"transformSpec\": {\n  \"transforms: <List of transforms>,\n  \"filter\": <filter>\n}\n```\n\n----------------------------------------\n\nTITLE: Roaring Bitmap Configuration Table in Markdown\nDESCRIPTION: Configuration table defining the parameters for Roaring bitmap type specification including compression options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|type|String|Must be `roaring`.|yes|\n|compressRunOnSerialization|Boolean|Use a run-length encoding where it is estimated as more space efficient.|no (default == `true`)|\n```\n\n----------------------------------------\n\nTITLE: Accessing Overlord Console URL\nDESCRIPTION: The URL pattern for accessing the Druid Overlord console, which provides views of tasks, workers, and recent worker activity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/management-uis.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<OVERLORD_IP>:<OVERLORD_PORT>/console.html\n```\n\n----------------------------------------\n\nTITLE: JavaScript Post-Aggregator Configuration\nDESCRIPTION: Demonstrates JavaScript post-aggregator implementation with custom functions applied to specified fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"absPercent\",\n  \"fieldNames\": [\"delta\", \"total\"],\n  \"function\": \"function(delta, total) { return 100 * Math.abs(delta) / total; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Storage Configuration\nDESCRIPTION: Configuration properties for setting up HDFS as deep storage and log storage in Druid\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Example Expression Transform in Apache Druid\nDESCRIPTION: An example of an expression transform that prepends 'foo' to the values in a 'page' column and creates a new 'fooPage' column with the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Copying Hadoop Configuration\nDESCRIPTION: Commands to copy Hadoop configuration files to Druid classpath.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml\n```\n\n----------------------------------------\n\nTITLE: Python Kafka Producer for Protobuf Messages\nDESCRIPTION: Python script that reads JSON data from stdin and publishes it to Kafka as Protobuf messages\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\nimport json\n\nfrom kafka import KafkaProducer\nfrom metrics_pb2 import Metrics\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ntopic = 'metrics_pb'\nmetrics = Metrics()\n\nfor row in iter(sys.stdin):\n    d = json.loads(row)\n    for k, v in d.items():\n        setattr(metrics, k, v)\n    pb = metrics.SerializeToString()\n    producer.send(topic, pb)\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Page Layout and Title for Batch Data Ingestion Documentation\nDESCRIPTION: This snippet sets up the layout and title for the Markdown documentation page. It specifies the page layout as 'doc_page' and sets the title to 'Batch Data Ingestion'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/batch-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Batch Data Ingestion\"\n---\n```\n\n----------------------------------------\n\nTITLE: Basic Markdown Documentation Layout\nDESCRIPTION: Front matter and layout configuration for the documentation page defining the title and document type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/indexing-service.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Indexing Service\"\n---\n```\n\n----------------------------------------\n\nTITLE: Equal To Filter in Druid\nDESCRIPTION: Demonstrates implementation of an equalTo filter in the Having clause, equivalent to SQL HAVING <aggregate> = <value>.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/having.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"equalTo\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Thrift Ingestion with Hadoop\nDESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. Shows how to set up the data schema, IO configuration, and tuning configuration for batch processing of Thrift data from SequenceFile or LzoThriftBlock formats.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"book\",\n      \"parser\": {\n        \"type\": \"thrift\",\n        \"jarPath\": \"book.jar\",\n        \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n        \"protocol\": \"compact\",\n        \"parseSpec\": {\n          \"format\": \"json\",\n          ...\n        }\n      },\n      \"metricsSpec\": [],\n      \"granularitySpec\": {}\n    },\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\",\n        // \"inputFormat\": \"com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat\",\n        \"paths\": \"/user/to/some/book.seq\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"jobProperties\": {\n        \"tmpjars\":\"/user/h_user_profile/du00/druid/test/book.jar\",\n        // \"elephantbird.class.for.MultiInputFormat\" : \"${YOUR_THRIFT_CLASS_NAME}\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StringLast Aggregator in Apache Druid\nDESCRIPTION: Illustrates the setup for a stringLast aggregator in Druid. This aggregator computes the metric value with the maximum timestamp or null if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing Tasks on a Druid Middle Manager\nDESCRIPTION: Shows how to retrieve a list of all running tasks on a Middle Manager using a GET request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/tasks\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in Druid's config file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Optimized Query Rewrite for Lookup-based Selector Filter\nDESCRIPTION: This shows how Druid rewrites a selector filter with lookup extraction into an OR of simpler selector filters for optimization, assuming the lookup maps 'product_1' and 'product_3' to 'bar_1'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"filter\":{\n      \"type\":\"or\",\n      \"fields\":[\n         {\n            \"filter\":{\n               \"type\":\"selector\",\n               \"dimension\":\"product\",\n               \"value\":\"product_1\"\n            }\n         },\n         {\n            \"filter\":{\n               \"type\":\"selector\",\n               \"dimension\":\"product\",\n               \"value\":\"product_3\"\n            }\n         }\n      ]\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fixed Buckets Histogram Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the fixed buckets histogram aggregator. This defines parameters including lower and upper limits, number of buckets, and how to handle outlier values. This aggregator is designed for specific use cases where the data distribution is well understood.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"fixedBucketsHistogram\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <double>,\n  \"upperLimit\" : <double>,\n  \"outlierHandlingMode\": <mode>\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Druid Distribution Package\nDESCRIPTION: Commands to download and extract the Apache Druid distribution package to begin cluster setup\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.14.2-incubating-bin.tar.gz\ncd apache-druid-0.14.2-incubating\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Results with 'All' Granularity in Apache Druid\nDESCRIPTION: Example results from a GroupBy query in Apache Druid using 'all' granularity. Demonstrates how all data is aggregated into a single bucket.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2000-01-01T00:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 4,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Coordinator Process in Druid\nDESCRIPTION: Basic configuration properties for the Coordinator process, including host, port settings, and service name for metrics and alerts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_24\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8081\ndruid.tlsPort=8281\ndruid.service=druid/coordinator\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchUnion Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the HLLSketchUnion post-aggregator. This post-aggregator performs a union operation on multiple HLL sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchUnion\",\n  \"name\": <output name>,\n  \"fields\"  : <array of post aggregators that return HLL sketches>,\n  \"lgK\": <log2 of K for the target sketch>,\n  \"tgtHllType\" : <target HLL type>\n}\n```\n\n----------------------------------------\n\nTITLE: Inline Schema Avro Bytes Decoder Configuration\nDESCRIPTION: Configuration for inline schema-based Avro bytes decoder that uses a fixed schema specified directly in the task configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/avro.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"avroBytesDecoder\": {\n    \"type\": \"schema_inline\",\n    \"schema\": {\n      \"namespace\": \"org.apache.druid.data\",\n      \"name\": \"User\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"FullName\", \"type\": \"string\" },\n        { \"name\": \"Country\", \"type\": \"string\" }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CustomJSON Lookup in Apache Druid\nDESCRIPTION: Example of custom JSON format with specified key and value fields and its corresponding namespaceParseSpec configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"key\": \"foo\", \"value\": \"bar\", \"somethingElse\" : \"something\"}\n{\"key\": \"baz\", \"value\": \"bat\", \"somethingElse\" : \"something\"}\n{\"key\": \"buck\", \"somethingElse\": \"something\", \"value\": \"truck\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"customJson\",\n  \"keyFieldName\": \"key\",\n  \"valueFieldName\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQFirehose for Druid Data Ingestion in JSON\nDESCRIPTION: A sample configuration specification for the RabbitMQ firehose in Apache Druid. This configuration establishes a connection to a RabbitMQ broker and defines parameters for queue binding and connection management. The configuration includes connection details, exchange parameters, and retry settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/rabbitmq.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n   \"type\" : \"rabbitmq\",\n   \"connection\" : {\n     \"host\": \"localhost\",\n     \"port\": \"5672\",\n     \"username\": \"test-dude\",\n     \"password\": \"test-word\",\n     \"virtualHost\": \"test-vhost\",\n     \"uri\": \"amqp://mqserver:1234/vhost\"\n   },\n   \"config\" : {\n     \"exchange\": \"test-exchange\",\n     \"queue\" : \"druidtest\",\n     \"routingKey\": \"#\",\n     \"durable\": \"true\",\n     \"exclusive\": \"false\",\n     \"autoDelete\": \"false\",\n     \"maxRetries\": \"10\",\n     \"retryIntervalSeconds\": \"1\",\n     \"maxDurationSeconds\": \"300\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch ToString Post-Aggregator\nDESCRIPTION: JSON configuration for post-aggregator that converts HLL sketch to human-readable string format for debugging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Full Serialization Format for Histograms in Apache Druid\nDESCRIPTION: Defines the byte-level structure for fully serializing histogram data, including version, encoding mode, limits, bucket counts, and outlier handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x01 for full\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\narray of longs: bucket counts for the histogram\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running insert-segment-to-db Tool with MySQL and S3\nDESCRIPTION: This command shows how to run the insert-segment-to-db tool with MySQL as metadata storage and S3 as deep storage. It includes necessary Java system properties for S3 configuration and command-line arguments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava\n-Ddruid.metadata.storage.type=mysql \n-Ddruid.metadata.storage.connector.connectURI=jdbc\\:mysql\\://localhost\\:3306/druid \n-Ddruid.metadata.storage.connector.user=druid \n-Ddruid.metadata.storage.connector.password=diurd\n-Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\",\\\"druid-s3-extensions\\\"]\n-Ddruid.storage.type=s3\n-Ddruid.s3.accessKey=... \n-Ddruid.s3.secretKey=...\n-Ddruid.storage.bucket=your-bucket\n-Ddruid.storage.baseKey=druid/storage/wikipedia\n-Ddruid.storage.maxListingLength=1000\n-cp $DRUID_CLASSPATH\norg.apache.druid.cli.Main tools insert-segment-to-db --workingDir \"druid/storage/wikipedia\" --updateDescriptor true\n```\n\n----------------------------------------\n\nTITLE: Generating Protobuf Descriptor File\nDESCRIPTION: Command to generate the Protobuf descriptor file using the protoc compiler.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprotoc -o /tmp/metrics.desc metrics.proto\n```\n\n----------------------------------------\n\nTITLE: Basic ResetCluster Command with Options\nDESCRIPTION: Command to run ResetCluster tool with selective cleanup options. Parameters allow choosing which components to reset including metadata store, segment files, task logs, and Hadoop working path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Listing Miscellaneous Resources in Markdown\nDESCRIPTION: A markdown list of miscellaneous resources related to Apache Druid, including links to the Druid Expressions Language documentation, papers and talks about Druid, and a thanks page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/toc.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Misc\n  * [Druid Expressions Language](/docs/VERSION/misc/math-expr.html)\n  * [Papers & Talks](/docs/VERSION/misc/papers-and-talks.html)\n  * [Thanks](/thanks.html)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Kafka\nDESCRIPTION: Commands to download Apache Kafka 2.1.0 and extract it to the local filesystem.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://archive.apache.org/dist/kafka/2.1.0/kafka_2.12-2.1.0.tgz\ntar -xzf kafka_2.12-2.1.0.tgz\ncd kafka_2.12-2.1.0\n```\n\n----------------------------------------\n\nTITLE: Configuring flattenSpec for Column Renaming in Druid ORC Ingestion\nDESCRIPTION: This JSON snippet demonstrates how to use flattenSpec expressions to rename columns when ingesting ORC data in Druid. It replaces the previous typeString property used in the 'contrib' extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/orc.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flattenSpec\": {\n    \"fields\": [\n      {\n        \"type\": \"path\",\n        \"name\": \"time\",\n        \"expr\": \"$._col0\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"name\",\n        \"expr\": \"$._col1\"\n      }\n    ]\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: React Is License Header in JavaScript\nDESCRIPTION: License header for React's react-is.production.min.js file version 16.13.1. This component is used for type checking in React and is licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticAzureBlobStoreFirehose in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticAzureBlobStoreFirehose for ingesting data from Azure Blob Storage. It specifies the firehose type and a list of blobs to ingest from different containers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/azure.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"static-azure-blobstore\",\n    \"blobs\": [\n        {\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy v2 Core Runtime Properties\nDESCRIPTION: Core runtime properties for GroupBy v2 queries including memory and disk usage limits. These properties can be set in runtime.properties file on Broker, Historical, and MiddleManager processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.maxMergingDictionarySize=100000000\ndruid.query.groupBy.maxOnDiskStorage=0\n```\n\n----------------------------------------\n\nTITLE: SQL Query Response JSON Format in Druid\nDESCRIPTION: Example of the JSON response format returned by Druid when executing a SQL query via HTTP, showing the 10 Wikipedia pages with the most edits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"page\": \"Wikipedia:Vandalismusmeldung\",\n    \"Edits\": 33\n  },\n  {\n    \"page\": \"User:Cyde/List of candidates for speedy deletion/Subpage\",\n    \"Edits\": 28\n  },\n  {\n    \"page\": \"Jeremy Corbyn\",\n    \"Edits\": 27\n  },\n  {\n    \"page\": \"Wikipedia:Administrators' noticeboard/Incidents\",\n    \"Edits\": 21\n  },\n  {\n    \"page\": \"Flavia Pennetta\",\n    \"Edits\": 20\n  },\n  {\n    \"page\": \"Total Drama Presents: The Ridonculous Race\",\n    \"Edits\": 18\n  },\n  {\n    \"page\": \"User talk:Dudeperson176123\",\n    \"Edits\": 18\n  },\n  {\n    \"page\": \"Wikipdia:Le Bistro/12 septembre 2015\",\n    \"Edits\": 18\n  },\n  {\n    \"page\": \"Wikipedia:In the news/Candidates\",\n    \"Edits\": 17\n  },\n  {\n    \"page\": \"Wikipedia:Requests for page protection\",\n    \"Edits\": 17\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Querying Rolled-up Data with Druid SQL\nDESCRIPTION: Example of using Druid's SQL shell (dsql) to query the ingested data. The output demonstrates how multiple input rows have been summarized into fewer rows through the roll-up process based on dimensions and query granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"rollup-tutorial\";\n\n __time                    bytes   count  dstIP    packets  srcIP   \n\n 2018-01-01T01:01:00.000Z   35937      3  2.2.2.2      286  1.1.1.1 \n 2018-01-01T01:02:00.000Z  366260      2  2.2.2.2      415  1.1.1.1 \n 2018-01-01T01:03:00.000Z   10204      1  2.2.2.2       49  1.1.1.1 \n 2018-01-02T21:33:00.000Z  100288      2  8.8.8.8      161  7.7.7.7 \n 2018-01-02T21:35:00.000Z    2818      1  8.8.8.8       12  7.7.7.7 \n\nRetrieved 5 rows in 1.18s.\n\ndsql>\n```\n\n----------------------------------------\n\nTITLE: React-Is License Comment\nDESCRIPTION: License comment for the React-Is production file, which is licensed under the MIT License and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6f6dba15.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data with Post Index Task in Druid\nDESCRIPTION: Command to load initial dataset into Druid using post-index-task with a specification file that creates a datasource called 'updates-tutorial'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Sample Network Flow Event Data in JSON\nDESCRIPTION: Example dataset containing network flow events with timestamp, source IP, destination IP, packet count, and byte count information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":20,\"bytes\":9024}\n{\"timestamp\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":255,\"bytes\":21133}\n{\"timestamp\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":11,\"bytes\":5780}\n{\"timestamp\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":38,\"bytes\":6289}\n{\"timestamp\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":377,\"bytes\":359971}\n{\"timestamp\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":49,\"bytes\":10204}\n{\"timestamp\":\"2018-01-02T21:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":38,\"bytes\":6289}\n{\"timestamp\":\"2018-01-02T21:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":123,\"bytes\":93999}\n{\"timestamp\":\"2018-01-02T21:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":12,\"bytes\":2818}\n```\n\n----------------------------------------\n\nTITLE: Calculating Rollup Ratio with Druid SQL\nDESCRIPTION: This SQL query calculates the rollup ratio of a datasource by comparing the number of rows in Druid with the number of ingested events. It uses the SUM of event_count divided by the COUNT of rows, multiplied by 1.0 to ensure decimal division.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT SUM(\"event_count\") / COUNT(*) * 1.0 FROM datasource\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Kerberos Authenticator in Druid\nDESCRIPTION: This snippet shows the minimal configuration needed to add a Kerberos authenticator to the authenticator chain in Druid. It defines a custom authenticator named 'MyKerberosAuthenticator' of type 'kerberos'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyKerberosAuthenticator\"]\n\ndruid.auth.authenticator.MyKerberosAuthenticator.type=kerberos\n```\n\n----------------------------------------\n\nTITLE: Combining and Overwriting Data in Druid\nDESCRIPTION: Command to submit a task that combines existing data with new data and overwrites the original data with the combined results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function for __time Dimension in Druid JSON\nDESCRIPTION: Transforms __time dimension value using JavaScript. The input is passed as a number representing milliseconds since January 1, 1970 UTC.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Broker\nDESCRIPTION: Command to start a Kafka broker using the default configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-server-start.sh config/server.properties\n```\n\n----------------------------------------\n\nTITLE: Full Histogram Binary Serialization Format\nDESCRIPTION: Specification for the full histogram serialization format that includes complete bucket count array. Version 0x01 format with various metadata fields and full bucket count array.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x01 for full\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\narray of longs: bucket counts for the histogram\n```\n\n----------------------------------------\n\nTITLE: Configuring Detailed Numeric TopNMetricSpec in JSON for Apache Druid\nDESCRIPTION: Shows the JSON object format for specifying a numeric sort in a Druid topN query. It includes the type and metric properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"numeric\",\n    \"metric\": \"<metric_name>\"\n}\n```\n\n----------------------------------------\n\nTITLE: ISO 8601 Interval Filter on Timestamp in Druid JSON\nDESCRIPTION: Illustrates how to filter on a set of ISO 8601 time intervals using the interval filter on the '__time' column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Approximate Histogram Aggregator in JSON\nDESCRIPTION: JSON configuration for the approximate histogram aggregator. Specifies the type, output name, metric name, resolution, number of buckets, and optional lower/upper limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"resolution\" : <integer>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <float>,\n  \"upperLimit\" : <float>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Historical Node Metrics Table in Markdown\nDESCRIPTION: A markdown table showing various metrics for Historical nodes in Apache Druid, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/max`|Maximum byte limit available for segments.||Varies.|\n|`segment/used`|Bytes used for served segments.|dataSource, tier, priority.|< max|\n|`segment/usedPercent`|Percentage of space used by served segments.|dataSource, tier, priority.|< 100%|\n|`segment/count`|Number of served segments.|dataSource, tier, priority.|Varies.|\n|`segment/pendingDelete`|On-disk size in bytes of segments that are waiting to be cleared out|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Counting Retained Entries in ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to count the number of retained entries in an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToNumEntries\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Processing Parameters for Druid Broker (Properties)\nDESCRIPTION: Properties for configuring processing-related settings such as buffer sizes, thread counts, and caching options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_33\n\nLANGUAGE: properties\nCODE:\n```\ndruid.processing.buffer.sizeBytes=auto (max 1GB)\ndruid.processing.buffer.poolCacheMaxCount=Integer.MAX_VALUE\ndruid.processing.formatString=processing-%s\ndruid.processing.numMergeBuffers=max(2, druid.processing.numThreads / 4)\ndruid.processing.numThreads=Number of cores - 1 (or 1)\ndruid.processing.columnCache.sizeBytes=0\ndruid.processing.fifo=false\ndruid.processing.tmpDir=path represented by java.io.tmpdir\n```\n\n----------------------------------------\n\nTITLE: Initiating a Paginated Select Query in Apache Druid\nDESCRIPTION: This JSON snippet shows how to specify a PagingSpec for initiating a paginated Select query in Apache Druid. It sets a threshold for the number of rows to return and an empty pagingIdentifiers object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/select-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Segment Naming Convention in Druid\nDESCRIPTION: Shows how Druid uniquely identifies segments using datasource, interval, version, and partition number. Illustrates the naming pattern for segments with the same time interval but different versions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-01/2015-01-02_v1_1\nfoo_2015-01-01/2015-01-02_v1_2\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Processing Settings in YAML\nDESCRIPTION: YAML configuration for Historical processing, including buffer sizes, thread counts, and caching options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_40\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.processing.buffer.sizeBytes: auto\ndruid.processing.buffer.poolCacheMaxCount: Integer.MAX_VALUE\ndruid.processing.formatString: processing-%s\ndruid.processing.numMergeBuffers: max(2, druid.processing.numThreads / 4)\ndruid.processing.numThreads: Number of cores - 1\ndruid.processing.columnCache.sizeBytes: 0\ndruid.processing.fifo: false\ndruid.processing.tmpDir: ${java.io.tmpdir}\n```\n\n----------------------------------------\n\nTITLE: Testing Kafka Rename Functionality using Console Producer\nDESCRIPTION: Bash command for testing the Kafka rename functionality. It uses the Kafka console producer to send key-value pairs to a Kafka stream, allowing users to publish renames in the format 'OLD_VAL->NEW_VAL'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-console-producer.sh --property parse.key=true --property key.separator=\"->\" --broker-list localhost:9092 --topic testTopic\n```\n\n----------------------------------------\n\nTITLE: Query Aggregation for Event Counting\nDESCRIPTION: Example showing query aggregation specification for counting ingested events.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"numIngestedEvents\", \"fieldName\": \"count\" }\n```\n\n----------------------------------------\n\nTITLE: Asynchronous log4j2.xml Configuration for Apache Druid\nDESCRIPTION: An advanced log4j2.xml configuration that uses asynchronous logging for chatty Druid classes to improve performance. It configures specific loggers like CuratorInventoryManager and BatchServerInventoryView to write logs asynchronously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/logging.md#2025-04-09_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <AsyncLogger name=\"org.apache.druid.curator.inventory.CuratorInventoryManager\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name=\"org.apache.druid.client.BatchServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->\n    <AsyncLogger name=\"org.apache.druid.client.ServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name =\"org.apache.druid.java.util.http.client.pool.ChannelResourceFactory\" level=\"info\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: One-Sided Upper Bound Age Filter in Druid\nDESCRIPTION: Shows how to create a one-sided bound filter for ages less than 31.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Management Proxy in Router\nDESCRIPTION: Property configuration to enable the Router to act as a management proxy that forwards requests to the active Coordinator or Overlord process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.managementProxy.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Lookup ExtractorFactory in Druid\nDESCRIPTION: JSON configuration for setting up a Kafka-based lookup extractor. Defines the kafka topic and connection properties for dimension value mapping.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"kafka\",\n  \"kafkaTopic\":\"testTopic\",\n  \"kafkaProperties\":{\"zookeeper.connect\":\"somehost:2181/kafka\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Regular Expression Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Regular Expression Extraction Function in Apache Druid. It allows extracting matching groups from dimension values using regex patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\" : \"regex\",\n  \"expr\" : <regular_expression>,\n  \"index\" : <group to extract, default 1>\n  \"replaceMissingValue\" : true,\n  \"replaceMissingValueWith\" : \"foobar\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Specification in Druid Ingestion Spec\nDESCRIPTION: Adding a timestampSpec to the parseSpec to define how to extract the timestamp field from the input data. The specification indicates that the timestamp is in ISO 8601 format and contained in the 'ts' column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Unparseable Events Endpoint\nDESCRIPTION: HTTP endpoint for retrieving current lists of unparseable events from a running task\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/reports.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/unparseableEvents\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Services with Supervisor Script\nDESCRIPTION: This command launches all Druid services in a supervised mode using the tutorial cluster configuration. It starts ZooKeeper and Druid services including coordinator, broker, router, historical, overlord, and middleManager.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n```\n\n----------------------------------------\n\nTITLE: Querying Unique Users for a Product Using Theta Sketch in Druid\nDESCRIPTION: A sample GroupBy query that uses the thetaSketch aggregator to count unique users who visited product A. The query filters for a specific product and applies a thetaSketch aggregation on user_id_sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"aggregations\": [\n    { \"type\": \"thetaSketch\", \"name\": \"unique_users\", \"fieldName\": \"user_id_sketch\" }\n  ],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\" },\n  \"intervals\": [ \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Coordinator Process in Java\nDESCRIPTION: Command to start the Druid Coordinator process using the Main class. This initiates the coordinator server which manages segment distribution and load balancing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/coordinator.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server coordinator\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka 0.8.x Firehose in Druid\nDESCRIPTION: A sample JSON configuration for Kafka 0.8.x firehose in Apache Druid. It demonstrates how to set up consumer properties including Zookeeper connection parameters, consumer group ID, message size limits, and offset behavior. The sample is configured to ingest from a 'wikipedia' topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-eight-firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\": {\n  \"type\": \"kafka-0.8\",\n  \"consumerProps\": {\n    \"zookeeper.connect\": \"localhost:2181\",\n    \"zookeeper.connection.timeout.ms\" : \"15000\",\n    \"zookeeper.session.timeout.ms\" : \"15000\",\n    \"zookeeper.sync.time.ms\" : \"5000\",\n    \"group.id\": \"druid-example\",\n    \"fetch.message.max.bytes\" : \"1048586\",\n    \"auto.offset.reset\": \"largest\",\n    \"auto.commit.enable\": \"false\"\n  },\n  \"feed\": \"wikipedia\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Tranquility Server\nDESCRIPTION: Commands to download, extract and start Tranquility Server for stream ingestion\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz\ntar -xzf tranquility-distribution-0.8.0.tgz\ncd tranquility-distribution-0.8.0\nbin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json\n```\n\n----------------------------------------\n\nTITLE: Rollup Aggregation Pseudocode\nDESCRIPTION: SQL-like pseudocode showing the grouping and aggregation logic used during rollup operation with minute-level granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nGROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Server\nDESCRIPTION: Command to start the Kafka broker using the default server configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-server-start.sh config/server.properties\n```\n\n----------------------------------------\n\nTITLE: ORC Parser with TimeAndDims ParseSpec in JSON\nDESCRIPTION: Configuration for ORC parser using the timeAndDims parseSpec format. This approach requires explicit dimension specification and is an alternative to the orc parseSpec format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/orc.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SimpleJSON Lookup in Apache Druid\nDESCRIPTION: Example of simple JSON format where each line contains a key-value pair object and its basic namespaceParseSpec configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\": \"bar\"}\n{\"baz\": \"bat\"}\n{\"buck\": \"truck\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\":{\n  \"format\": \"simpleJson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Hourly Granularity Compaction Task Specification\nDESCRIPTION: JSON configuration for compacting segments while maintaining hourly granularity. Specifies the datasource, time interval, and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticCloudFilesFirehose for Data Ingestion\nDESCRIPTION: Configuration specification for setting up a static Cloud Files firehose in Druid. Defines how to specify multiple blobs for ingestion with region, container and path information. Supports caching and prefetching capabilities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/cloudfiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"static-cloudfiles\",\n    \"blobs\": [\n        {\n          \"region\": \"DFW\"\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"region\": \"ORD\"\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Aggregator Example in Apache Druid\nDESCRIPTION: An example of the JavaScript aggregator that computes sum(log(x)*y) + 10. It defines three functions: fnAggregate to process row values, fnCombine to merge partial results, and fnReset to establish the initial value of 10.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"sum(log(x)*y) + 10\",\n  \"fieldNames\": [\"x\", \"y\"],\n  \"fnAggregate\" : \"function(current, a, b)      { return current + (Math.log(a) * b); }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return partialA + partialB; }\",\n  \"fnReset\"     : \"function()                   { return 10; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Common GroupBy Strategy Configuration Table\nDESCRIPTION: Configuration table showing common runtime properties for all groupBy strategies, including default strategy and threading options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.defaultStrategy`|Default groupBy query strategy.|v2|\n|`druid.query.groupBy.singleThreaded`|Merge results using a single thread.|false|\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Database for Druid\nDESCRIPTION: Command to create a PostgreSQL database named 'druid' owned by the druid user.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncreatedb druid -O druid\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Credentials for Kinesis in Druid\nDESCRIPTION: Example of setting AWS access key and secret key via runtime properties for Kinesis API authentication in Druid. If not provided, the service will look for credentials in environment variables, default profile configuration file, or EC2 instance profile provider.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\n-Ddruid.kinesis.accessKey=123 -Ddruid.kinesis.secretKey=456\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Cache Executor Factory Options\nDESCRIPTION: Lists the possible values for druid.cache.cacheExecutorFactory which controls how cache maintenance tasks are run with their respective behaviors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_37\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.cacheExecutorFactory=COMMON_FJP\n```\n\n----------------------------------------\n\nTITLE: Apache Druid ingestion spec with transform and filter\nDESCRIPTION: This JSON configuration specifies the ingestion process for Druid, including data schema, transform specs for modifying columns, and a filter to select specific rows based on transformed values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"transform-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"animal\",\n              { \"name\": \"location\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"number\", \"fieldName\" : \"number\" },\n        { \"type\" : \"longSum\", \"name\" : \"triple-number\", \"fieldName\" : \"triple-number\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      },\n      \"transformSpec\": {\n        \"transforms\": [\n          {\n            \"type\": \"expression\",\n            \"name\": \"animal\",\n            \"expression\": \"concat('super-', animal)\"\n          },\n          {\n            \"type\": \"expression\",\n            \"name\": \"triple-number\",\n            \"expression\": \"number * 3\"\n          }\n        ],\n        \"filter\": {\n          \"type\":\"or\",\n          \"fields\": [\n            { \"type\": \"selector\", \"dimension\": \"animal\", \"value\": \"super-mongoose\" },\n            { \"type\": \"selector\", \"dimension\": \"triple-number\", \"value\": \"300\" },\n            { \"type\": \"selector\", \"dimension\": \"location\", \"value\": \"3\" }\n          ]\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"transform-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating Distinct Keys with Error Bounds using ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to estimate distinct keys with error bounds from an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimateAndBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,\n  \"numStdDevs\": <number from 1 to 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Get MiddleManager Enabled Status Response - JSON\nDESCRIPTION: Example JSON response from the MiddleManager's enabled status endpoint showing a boolean status keyed by host:port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":true}\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Query Limits for Apache Druid Realtime Node\nDESCRIPTION: YAML configuration for setting the maximum number of search results to return in a Druid realtime node.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/realtime.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.query.search.maxSearchLimit: 1000\n```\n\n----------------------------------------\n\nTITLE: Configuring TopN Query Context Parameters in Apache Druid\nDESCRIPTION: This markdown table lists query context parameters specific to TopN queries in Apache Druid. It includes the minTopNThreshold parameter for controlling local result merging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/query-context.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default              | description          |\n|-----------------|---------------------|----------------------|\n|minTopNThreshold | `1000`              | The top minTopNThreshold local results from each segment are returned for merging to determine the global topN. |\n```\n\n----------------------------------------\n\nTITLE: Historical Node Configuration Example\nDESCRIPTION: Example configuration for Druid Historical nodes showing processing thread and buffer settings for a specific hardware configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.processing.buffer.sizeBytes=500000000\ndruid.processing.numMergeBuffers=8\ndruid.processing.numThreads=31\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for React use-sync-external-store-shim\nDESCRIPTION: Copyright notice and MIT license information for React's use-sync-external-store-shim production module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Sample Data Input Format\nDESCRIPTION: Example of raw data format stored in Druid with millisecond granularity\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"AAA\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-01T01:02:33Z\", \"page\": \"BBB\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NOT Logical Expression Filter in Apache Druid JSON Query\nDESCRIPTION: The NOT filter negates the result of another filter. It includes rows that do not match the specified filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"not\", \"field\": <filter> }\n```\n\n----------------------------------------\n\nTITLE: Setting Query Granularity in Druid\nDESCRIPTION: Shows how to configure queryGranularity within the granularitySpec, which determines the time bucket size for timestamp flooring during ingestion, set to MINUTE in this example.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"queryGranularity\" : \"MINUTE\"\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Kerberos Authentication with Kinit\nDESCRIPTION: Command to authenticate using Kerberos keytab file via kinit\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkinit -k -t <path_to_keytab_file> user@REALM.COM\n```\n\n----------------------------------------\n\nTITLE: Setting up HDFS Directories\nDESCRIPTION: Commands to create and configure required HDFS directories for Druid data storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/hadoop/bin\n./hadoop fs -mkdir /druid\n./hadoop fs -mkdir /druid/segments\n./hadoop fs -mkdir /quickstart\n./hadoop fs -chmod 777 /druid\n./hadoop fs -chmod 777 /druid/segments\n./hadoop fs -chmod 777 /quickstart\n./hadoop fs -chmod -R 777 /tmp\n./hadoop fs -chmod -R 777 /user\n./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Extraction Function for Time Dimension\nDESCRIPTION: This JavaScript extraction function example operates on the __time dimension, converting millisecond timestamps to display the seconds portion with a descriptive prefix.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Server\nDESCRIPTION: Command to start the Kafka broker using the default server configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-server-start.sh config/server.properties\n```\n\n----------------------------------------\n\nTITLE: Specifying Result Format in Druid SQL Query (JSON)\nDESCRIPTION: Example of how to specify a result format for a Druid SQL query using the 'resultFormat' parameter in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Result Format in Druid SQL Query (JSON)\nDESCRIPTION: Example of how to specify a result format for a Druid SQL query using the 'resultFormat' parameter in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Standard Deviation Post-Aggregator Configuration\nDESCRIPTION: Configuration for the standard deviation post-aggregator that calculates standard deviation from variance results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"stddev\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"estimator\": <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Coordinator Operation Properties in Apache Druid\nDESCRIPTION: This markdown table defines various configuration properties for the Druid Coordinator's operation, including run periods, timeouts, and behavior flags.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.coordinator.period`|The run period for the Coordinator. The Coordinator's operates by maintaining the current state of the world in memory and periodically looking at the set of segments available and segments being served to make decisions about whether any changes need to be made to the data topology. This property sets the delay between each of these runs.|PT60S|\n|`druid.coordinator.period.indexingPeriod`|How often to send compact/merge/conversion tasks to the indexing service. It's recommended to be longer than `druid.manager.segments.pollDuration`|PT1800S (30 mins)|\n|`druid.coordinator.startDelay`|The operation of the Coordinator works on the assumption that it has an up-to-date view of the state of the world when it runs, the current ZK interaction code, however, is written in a way that doesn't allow the Coordinator to know for a fact that it's done loading the current state of the world. This delay is a hack to give it enough time to believe that it has all the data.|PT300S|\n|`druid.coordinator.merge.on`|Boolean flag for whether or not the Coordinator should try and merge small segments into a more optimal segment size.|false|\n|`druid.coordinator.load.timeout`|The timeout duration for when the Coordinator assigns a segment to a Historical process.|PT15M|\n|`druid.coordinator.kill.pendingSegments.on`|Boolean flag for whether or not the Coordinator clean up old entries in the `pendingSegments` table of metadata store. If set to true, Coordinator will check the created time of most recently complete task. If it doesn't exist, it finds the created time of the earlist running/pending/waiting tasks. Once the created time is found, then for all dataSources not in the `killPendingSegmentsSkipList` (see [Dynamic configuration](#dynamic-configuration)), Coordinator will ask the Overlord to clean up the entries 1 day or more older than the found created time in the `pendingSegments` table. This will be done periodically based on `druid.coordinator.period` specified.|false|\n|`druid.coordinator.kill.on`|Boolean flag for whether or not the Coordinator should submit kill task for unused segments, that is, hard delete them from metadata store and deep storage. If set to true, then for all whitelisted dataSources (or optionally all), Coordinator will submit tasks periodically based on `period` specified. These kill tasks will delete all segments except for the last `durationToRetain` period. Whitelist or All can be set via dynamic configuration `killAllDataSources` and `killDataSourceWhitelist` described later.|false|\n|`druid.coordinator.kill.period`|How often to send kill tasks to the indexing service. Value must be greater than `druid.coordinator.period.indexingPeriod`. Only applies if kill is turned on.|P1D (1 Day)|\n|`druid.coordinator.kill.durationToRetain`| Do not kill segments in last `durationToRetain`, must be greater or equal to 0. Only applies and MUST be specified if kill is turned on. Note that default value is invalid.|PT-1S (-1 seconds)|\n|`druid.coordinator.kill.maxSegments`|Kill at most n segments per kill task submission, must be greater than 0. Only applies and MUST be specified if kill is turned on. Note that default value is invalid.|0|\n|`druid.coordinator.balancer.strategy`|Specify the type of balancing strategy that the Coordinator should use to distribute segments among the Historicals. `cachingCost` is logically equivalent to `cost` but is more CPU-efficient on large clusters and will replace `cost` in the future versions, users are invited to try it. Use `diskNormalized` to distribute segments among Historical processes so that the disks fill up uniformly and use `random` to randomly pick nodes to distribute segments.|`cost`|\n|`druid.coordinator.loadqueuepeon.repeatDelay`|The start and repeat delay for the loadqueuepeon , which manages the load and drop of segments.|PT0.050S (50 ms)|\n|`druid.coordinator.asOverlord.enabled`|Boolean value for whether this Coordinator process should act like an Overlord as well. This configuration allows users to simplify a druid cluster by not having to deploy any standalone Overlord processes. If set to true, then Overlord console is available at `http://coordinator-host:port/console.html` and be sure to set `druid.coordinator.asOverlord.overlordService` also. See next.|false|\n|`druid.coordinator.asOverlord.overlordService`| Required, if `druid.coordinator.asOverlord.enabled` is `true`. This must be same value as `druid.service` on standalone Overlord processes and `druid.selectors.indexing.serviceName` on Middle Managers.|NULL|\n```\n\n----------------------------------------\n\nTITLE: TSV Example Input for Druid Lookup\nDESCRIPTION: Example TSV input data with pipe delimiter and three columns for use in Druid lookup configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nbar|something,1|foo\nbat|something,2|baz\ntruck|something,3|buck\n```\n\n----------------------------------------\n\nTITLE: Running Command Line Hadoop Indexer in Bash\nDESCRIPTION: This command executes the Command Line Hadoop Indexer for Apache Druid. It sets the maximum heap size, timezone, file encoding, and classpath before running the main class with the Hadoop index command and a specification file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>\n```\n\n----------------------------------------\n\nTITLE: Documenting KinesisSupervisorTuningConfig Parameters in Markdown\nDESCRIPTION: Markdown table defining all available configuration parameters for KinesisSupervisorTuningConfig in Apache Druid. Includes detailed descriptions of memory management, indexing behavior, threading, and performance tuning options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|The indexing task type, this should always be `kinesis`.|yes|\n|`maxRowsInMemory`|Integer|The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in.|no (default == 100000)|\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Cache Properties\nDESCRIPTION: Configuration for the deprecated local cache implementation, including size limits and eviction settings. This has been replaced by the Caffeine cache implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_38\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.sizeInBytes=0\ndruid.cache.initialSize=500000\ndruid.cache.logEvictionCount=0\n```\n\n----------------------------------------\n\nTITLE: Get MiddleManager Tasks Response - JSON\nDESCRIPTION: Example JSON response showing list of active task IDs running on a MiddleManager.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n[\"index_wikiticker_2019-02-11T02:20:15.316Z\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Certificate Authentication in Apache Druid\nDESCRIPTION: This table describes optional configuration properties for setting up client certificate authentication in Apache Druid, including key store settings and certificate alias.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/simple-client-sslcontext.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.client.https.keyStorePath`|The file path or URL of the TLS/SSL Key store containing the client certificate that Druid will use when communicating with other Druid services. If this is null, the other properties in this table are ignored.|none|yes|\n|`druid.client.https.keyStoreType`|The type of the key store.|none|yes|\n|`druid.client.https.certAlias`|Alias of TLS client certificate in the keystore.|none|yes|\n|`druid.client.https.keyStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Store.|none|no|\n|`druid.client.https.keyManagerFactoryAlgorithm`|Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).|`javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.client.https.keyManagerPassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Key Manager.|none|no|\n|`druid.client.https.validateHostnames`|Validate the hostname of the server. This should not be disabled unless you are using [custom TLS certificate checks](../../operations/tls-support.html#custom-tls-certificate-checks) and know that standard hostname validation is not needed.|true|no|\n```\n\n----------------------------------------\n\nTITLE: Defining Search Filter in Apache Druid JSON Query\nDESCRIPTION: The search filter allows filtering on partial string matches. It supports different types of search queries like case-insensitive contains.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"search\",\n        \"dimension\": \"product\",\n        \"query\": {\n          \"type\": \"insensitive_contains\",\n          \"value\": \"foo\" \n        }        \n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Hadoop Indexer with Self-Contained Jar in Bash\nDESCRIPTION: This Bash command demonstrates how to run the Hadoop indexer using the self-contained jar created by the Maven Shade plugin. It includes necessary Java options, classpath settings, and the main class to execute.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Hour Granularity in Apache Druid\nDESCRIPTION: Example of a GroupBy query in Apache Druid using 'hour' granularity. This query groups data by language and counts occurrences for each hour.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":\"hour\",\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Blacklist Settings in Apache Druid Overlord\nDESCRIPTION: This snippet shows the configuration properties for setting up worker blacklisting in the Druid Overlord. It includes settings for retry thresholds, backoff time, cleanup period, and maximum percentage of blacklisted workers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/overlord.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.runner.maxRetriesBeforeBlacklist\ndruid.indexer.runner.workerBlackListBackoffTime\ndruid.indexer.runner.workerBlackListCleanupPeriod\ndruid.indexer.runner.maxPercentageBlacklistWorkers\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Converter for Ambari Metrics in Druid\nDESCRIPTION: JSON configuration for the 'all' event converter type that sends all Druid service metrics events to Ambari metrics collector. This defines the namespace prefix and application name for the metrics path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"appName\":\"druid\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing Long Max Aggregator in Druid\nDESCRIPTION: Computes maximum of metric values and Long.MIN_VALUE. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring ioConfig in JSON for Hadoop Indexer\nDESCRIPTION: This JSON snippet shows the additional fields required in the ioConfig section of the specification file for the Command Line Hadoop Indexer. It includes metadataUpdateSpec for database connection details and segmentOutputPath for specifying the output location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  ...\n  \"metadataUpdateSpec\" : {\n    \"type\":\"mysql\",\n    \"connectURI\" : \"jdbc:mysql://localhost:3306/druid\",\n    \"password\" : \"diurd\",\n    \"segmentTable\" : \"druid_segments\",\n    \"user\" : \"druid\"\n  },\n  \"segmentOutputPath\" : \"/MyDirectory/data/index/output\"\n}\n```\n\n----------------------------------------\n\nTITLE: Memory Mapping Druid Segments in Java\nDESCRIPTION: Druid segments are memory mapped for querying using the IndexIO.java class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/overview.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nIndexIO.java\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchEstimateWithBounds Post-Aggregator in Druid\nDESCRIPTION: This JSON configuration defines the HLLSketchEstimateWithBounds post-aggregator. It estimates the cardinality with error bounds based on the number of standard deviations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchEstimateWithBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>,\n  \"numStdDev\" : <number of standard deviations: 1 (default), 2 or 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Strlen Extraction Function Configuration in JSON\nDESCRIPTION: Configuration for the strlen extraction function that returns the length of dimension values in Unicode code units.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"strlen\" }\n```\n\n----------------------------------------\n\nTITLE: Illustrating Segment Versioning in Druid\nDESCRIPTION: Demonstrates how newly created segments with a new schema have a higher version id. This example shows segments with the same time interval but an incremented version number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\nfoo_2015-01-01/2015-01-02_v2_0\nfoo_2015-01-01/2015-01-02_v2_1\nfoo_2015-01-01/2015-01-02_v2_2\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Broadcast Rule in Druid\nDESCRIPTION: A Forever Broadcast Rule configuration that instructs Druid to co-locate segments of different data sources in historical nodes. This rule ensures segments are replicated to nodes holding segments of co-located data sources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastForever\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: CustomJSON Example Input for Druid Lookup\nDESCRIPTION: Example JSON input data with key, value, and additional fields for use with customJSON parsing in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"key\": \"foo\", \"value\": \"bar\", \"somethingElse\" : \"something\"}\n{\"key\": \"baz\", \"value\": \"bat\", \"somethingElse\" : \"something\"}\n{\"key\": \"buck\", \"somethingElse\": \"something\", \"value\": \"truck\"}\n```\n\n----------------------------------------\n\nTITLE: Submit Daily Compaction Task\nDESCRIPTION: Bash command to submit the daily granularity compaction task specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json\n```\n\n----------------------------------------\n\nTITLE: Including RocketMQ Extension in Apache Druid\nDESCRIPTION: Instructions for including the druid-rocketmq extension in Apache Druid. This snippet demonstrates how to reference the extension documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/rocketmq.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo use this Apache Druid (incubating) extension, make sure to [include](../../operations/including-extensions.html) `druid-rocketmq` extension.\n```\n\n----------------------------------------\n\nTITLE: GroupBy V1 Configuration Table\nDESCRIPTION: Configuration table showing runtime properties specific to GroupBy v1, including row limits and result maximums.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/groupbyquery.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxIntermediateRows`|Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail.|50000|\n|`druid.query.groupBy.maxResults`|Maximum number of results. Queries that exceed this limit will fail.|500000|\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Storage Schema for Druid\nDESCRIPTION: SQL create statements for required Cassandra tables. Creates index_storage table for compressed segments using Chunked Object storage, and descriptor_storage table for segment metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authentication for Druid-Hadoop Integration\nDESCRIPTION: This configuration snippet demonstrates how to set up Kerberos authentication for Druid to access Hadoop. It includes properties for specifying the Kerberos principal and keytab file location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kerberos-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.hadoop.security.kerberos.principal=hdfs-test@EXAMPLE.IO\ndruid.hadoop.security.kerberos.keytab=/etc/security/keytabs/hdfs.headless.keytab\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Broker on Query Server\nDESCRIPTION: Launches the Druid Broker process on each Query server using a Java command.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/broker/jvm.config | xargs` -cp conf/druid/_common:conf/druid/broker:lib/* org.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy v2 Query in Druid\nDESCRIPTION: Runtime property configurations for GroupBy v2 queries in Druid. Settings include maximum merging dictionary size and maximum on-disk storage for spilling result sets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_54\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.maxMergingDictionarySize=100000000\ndruid.query.groupBy.maxOnDiskStorage=0\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka 0.8.x Firehose in JSON\nDESCRIPTION: Sample configuration for setting up a Kafka 0.8.x firehose in Druid. Specifies essential consumer properties including Zookeeper connection details, group ID, message size limits, and offset management settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-eight-firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"firehose\": {\n    \"type\": \"kafka-0.8\",\n    \"consumerProps\": {\n      \"zookeeper.connect\": \"localhost:2181\",\n      \"zookeeper.connection.timeout.ms\" : \"15000\",\n      \"zookeeper.session.timeout.ms\" : \"15000\",\n      \"zookeeper.sync.time.ms\" : \"5000\",\n      \"group.id\": \"druid-example\",\n      \"fetch.message.max.bytes\" : \"1048586\",\n      \"auto.offset.reset\": \"largest\",\n      \"auto.commit.enable\": \"false\"\n    },\n    \"feed\": \"wikipedia\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating Distinct Keys with ArrayOfDoublesSketch Post-Aggregator\nDESCRIPTION: Post-aggregator configuration to estimate the number of distinct keys from an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring All-Metrics Converter for Graphite Emitter in Apache Druid\nDESCRIPTION: JSON configuration for the 'all' event converter which sends all Druid service metrics events to Graphite. This configuration ignores both hostname and service name in the metrics path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true}\n```\n\n----------------------------------------\n\nTITLE: Optimized Query Rewrite Example in Druid JSON\nDESCRIPTION: Shows how Druid rewrites an optimized lookup-based extraction filter as a simpler OR query with multiple selectors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"filter\":{\n      \"type\":\"or\",\n      \"fields\":[\n         {\n            \"filter\":{\n               \"type\":\"selector\",\n               \"dimension\":\"product\",\n               \"value\":\"product_1\"\n            }\n         },\n         {\n            \"filter\":{\n               \"type\":\"selector\",\n               \"dimension\":\"product\",\n               \"value\":\"product_3\"\n            }\n         }\n      ]\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Coordination Services\nDESCRIPTION: Commands to start the Druid coordinator and overlord services on the Master server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator\njava `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production build\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0c610519.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Managing Tasks in Druid Overlord API\nDESCRIPTION: GET, POST, and DELETE endpoints to manage and retrieve information about tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_14\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/indexer/v1/tasks\nGET /druid/indexer/v1/completeTasks\nGET /druid/indexer/v1/runningTasks\nGET /druid/indexer/v1/waitingTasks\nGET /druid/indexer/v1/pendingTasks\nGET /druid/indexer/v1/task/{taskId}\nGET /druid/indexer/v1/task/{taskId}/status\nGET /druid/indexer/v1/task/{taskId}/segments\nGET /druid/indexer/v1/task/{taskId}/reports\nPOST /druid/indexer/v1/task\nPOST /druid/indexer/v1/task/{taskId}/shutdown\nPOST /druid/indexer/v1/datasources/{dataSource}/shutdownAllTasks\nPOST /druid/indexer/v1/taskStatus\nDELETE /druid/indexer/v1/pendingSegments/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: NProgress Copyright and License Information\nDESCRIPTION: Copyright notice for the NProgress library created by Rico Sta. Cruz and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Example of Optimized Lookup-based Extraction Filter\nDESCRIPTION: An example showing how a registeredLookup extraction function with optimization enabled can be used within a selector filter to transform dimension values during filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"registeredLookup\",\n            \"optimize\": true,\n            \"lookup\": \"some_lookup_name\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter in Druid\nDESCRIPTION: JSON structure for defining a bloom filter in Druid queries. It specifies the dimension to filter, the serialized bloom filter data, and an optional extraction function to apply to dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bloom\",\n  \"dimension\" : <dimension_name>,\n  \"bloomKFilter\" : <serialized_bytes_for_BloomKFilter>,\n  \"extractionFn\" : <extraction_fn>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Transform Specification in Apache Druid\nDESCRIPTION: The basic syntax for the transformSpec property in Druid ingest specs. It contains an optional list of transforms and an optional filter to apply to input rows during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"transformSpec\": {\n  \"transforms: <List of transforms>,\n  \"filter\": <filter>\n}\n```\n\n----------------------------------------\n\nTITLE: Inline Lookup Extraction Function with ReplaceMissingValue in Druid JSON\nDESCRIPTION: Specifies an inline lookup map with a replacement value for missing keys. This example replaces missing values with \"MISSING\".\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":false,\n  \"injective\":false,\n  \"replaceMissingValueWith\":\"MISSING\"\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Submitting Ingestion Task to Druid API\nDESCRIPTION: Direct method to submit a batch ingestion task to the Druid Overlord using curl. This posts a JSON specification file to the Druid indexer API endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8081/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Example of Selector Filter with Registered Lookup Extraction\nDESCRIPTION: This example shows a selector filter using a registered lookup extraction function with optimization enabled. The filter selects dimension values that map to 'bar_1' in the lookup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"registeredLookup\",\n            \"optimize\": true,\n            \"lookup\": \"some_lookup_name\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing BloomKFilter in Java for Druid Queries\nDESCRIPTION: Java code snippet demonstrating how to construct a BloomKFilter, add values, and serialize it for use in Druid queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nBloomKFilter bloomFilter = new BloomKFilter(1500);\nbloomFilter.addString(\"value 1\");\nbloomFilter.addString(\"value 2\");\nbloomFilter.addString(\"value 3\");\nByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\nBloomKFilter.serialize(byteArrayOutputStream, bloomFilter);\nString base64Serialized = Base64.encodeBase64String(byteArrayOutputStream.toByteArray());\n```\n\n----------------------------------------\n\nTITLE: Configuring Default DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Default DimensionSpec in Apache Druid. It allows renaming dimensions and specifying output types for numeric columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\" : \"default\",\n  \"dimension\" : <dimension>,\n  \"outputName\": <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Kafka Supervisor Spec via cURL\nDESCRIPTION: Example of how to submit a Kafka supervisor specification using cURL to the Druid Overlord API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kafka Indexing Tasks in Druid\nDESCRIPTION: Formula for determining the minimum worker capacity needed to support both reading and publishing tasks in a Kafka indexing service configuration. This calculation ensures sufficient resources for concurrent task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Running Hadoop Indexer with Custom Jar in Bash\nDESCRIPTION: This Bash command demonstrates how to run the Hadoop indexer using the custom self-contained jar file, without needing the 'lib' directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n```\n\n----------------------------------------\n\nTITLE: Time Boundary Query Response Format in Apache Druid\nDESCRIPTION: The JSON response format from a time boundary query, showing both the minimum and maximum timestamps for the queried dataset. The result contains the earliest and latest timestamps present in the data source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"minTime\" : \"2013-05-09T18:24:00.000Z\",\n    \"maxTime\" : \"2013-05-09T18:37:00.000Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Priority in Druid Context\nDESCRIPTION: Example of how to override the default task priority by setting a custom priority value in the task context. Higher numbers indicate higher priority, with this example setting a priority of 100.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/locking-and-priority.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"context\" : {\n  \"priority\" : 100\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Caching Properties for Druid Broker\nDESCRIPTION: This snippet defines caching-related configuration properties for the Druid Broker, including cache enablement flags and size limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_47\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.broker.cache.useCache`|true, false|Enable the cache on the Broker.|false|\n|`druid.broker.cache.populateCache`|true, false|Populate the cache on the Broker.|false|\n|`druid.broker.cache.useResultLevelCache`|true, false|Enable result level caching on the Broker.|false|\n|`druid.broker.cache.populateResultLevelCache`|true, false|Populate the result level cache on the Broker.|false|\n|`druid.broker.cache.resultLevelCacheLimit`|positive integer|Maximum size of query response that can be cached.|`Integer.MAX_VALUE`|\n|`druid.broker.cache.unCacheable`|All druid query types|All query types to not cache.|`[\"groupBy\", \"select\"]`|\n|`druid.broker.cache.cacheBulkMergeLimit`|positive integer or 0|Queries with more segments than this number will not attempt to fetch from cache at the broker level, leaving potential caching fetches (and cache result merging) to the Historicals|`Integer.MAX_VALUE`|\n```\n\n----------------------------------------\n\nTITLE: Downloading Apache Druid Source Code with Git\nDESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/incubator-druid.git\ncd druid\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Strategy for Router\nDESCRIPTION: Example of a JavaScript-based routing strategy that sends queries with 3 or more aggregators to the lowest priority Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/router.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function (config, query) { if (query.getAggregatorSpecs && query.getAggregatorSpecs().size() >= 3) { var size = config.getTierToBrokerMap().values().size(); if (size > 0) { return config.getTierToBrokerMap().values().toArray()[size-1] } else { return config.getDefaultBrokerServiceName() } } else { return null } }\"\n}\n```\n\n----------------------------------------\n\nTITLE: React-is License Header\nDESCRIPTION: Copyright notice and MIT license declaration for React's react-is.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Querying Variance in Apache Druid with VarianceFold Aggregator\nDESCRIPTION: JSON configuration for querying pre-aggregated variance data using the varianceFold aggregator. This is used to combine partial variance results from pre-aggregated data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"varianceFold\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Including Cassandra Storage Extension in Apache Druid\nDESCRIPTION: Instructions for including the Cassandra storage extension in Apache Druid. This snippet shows the extension name that needs to be included in the Druid configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/cassandra.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo use this Apache Druid (incubating) extension, make sure to [include](../../operations/including-extensions.html) `druid-cassandra-storage` extension.\n```\n\n----------------------------------------\n\nTITLE: Time Parsing Extraction Function Configuration\nDESCRIPTION: Configuration for parsing dimension values as timestamps using specified input and output formats. Supports both Joda and SimpleDateFormat patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"time\",\n  \"timeFormat\" : <input_format>,\n  \"resultFormat\" : <output_format>,\n  \"joda\" : <true, false> }\n```\n\n----------------------------------------\n\nTITLE: Default DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for default dimension specification that returns dimension values as-is with optional renaming and type conversion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"default\",\n  \"dimension\" : <dimension>,\n  \"outputName\": <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bitmap Compression for Concise Bitmap Type in Druid\nDESCRIPTION: JSON configuration for using Concise bitmaps in Druid's IndexSpec. Concise is the default bitmap compression type that can be specified in the indexSpec of the tuning configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"concise\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Derby as Metadata Storage for Apache Druid\nDESCRIPTION: Configuration properties to set up Derby as the metadata storage for Druid. Note that Derby is not recommended for production use.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical General Settings in Apache Druid\nDESCRIPTION: This snippet presents a table of general configuration properties for Historical processes in Apache Druid, including max size, tier, and priority settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_26\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.server.maxSize`|The maximum number of bytes-worth of segments that the process wants assigned to it. This is not a limit that Historical processes actually enforces, just a value published to the Coordinator process so it can plan accordingly.|0|\n|`druid.server.tier`| A string to name the distribution tier that the storage process belongs to. Many of the [rules Coordinator processes use](../operations/rule-configuration.html) to manage segments can be keyed on tiers. |  `_default_tier` |\n|`druid.server.priority`|In a tiered architecture, the priority of the tier, thus allowing control over which processes are queried. Higher numbers mean higher priority. The default (no priority) works for architecture with no cross replication (tiers that have no data-storage overlap). Data centers typically have equal priority. | 0 |\n```\n\n----------------------------------------\n\nTITLE: Bitmap Type Configuration - Roaring Format\nDESCRIPTION: JSON configuration object for specifying Roaring bitmap compression format with optional run-length encoding optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"roaring\",\n  \"compressRunOnSerialization\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Format Extraction Function in Druid\nDESCRIPTION: The Time Format extraction function formats dimension values according to specified date/time patterns. It works with __time dimension values or regular dimensions with ISO-8601 formatted strings, allowing customization of format, locale, timezone, and granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"timeFormat\",\n  \"format\" : <output_format> (optional),\n  \"timeZone\" : <time_zone> (optional, default UTC),\n  \"locale\" : <locale> (optional, default current locale),\n  \"granularity\" : <granularity> (optional, default none) },\n  \"asMillis\" : <true or false> (optional) }\n```\n\n----------------------------------------\n\nTITLE: JSON Flatten Specification Configuration\nDESCRIPTION: Complete parseSpec configuration showing how to flatten nested JSON fields using both field discovery and explicit field definitions with JsonPath and jackson-jq expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parseSpec\": {\n  \"format\": \"json\",\n  \"flattenSpec\": {\n    \"useFieldDiscovery\": true,\n    \"fields\": [\n      {\n        \"type\": \"root\",\n        \"name\": \"dim1\"\n      },\n      \"dim2\",\n      {\n        \"type\": \"path\",\n        \"name\": \"foo.bar\",\n        \"expr\": \"$.foo.bar\"\n      },\n      {\n        \"type\": \"root\",\n        \"name\": \"foo.bar\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"path-metric\",\n        \"expr\": \"$.nestmet.val\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-0\",\n        \"expr\": \"$.hello[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-4\",\n        \"expr\": \"$.hello[4]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"world-hey\",\n        \"expr\": \"$.world[0].hey\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"worldtree\",\n        \"expr\": \"$.world[1].tree\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"first-food\",\n        \"expr\": \"$.thing.food[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"second-food\",\n        \"expr\": \"$.thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"first-food-by-jq\",\n        \"expr\": \".thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"hello-total\",\n        \"expr\": \".hello | sum\"\n      }\n    ]\n  },\n  \"dimensionsSpec\" : {\n   \"dimensions\" : [],\n   \"dimensionsExclusions\": [\"ignore_me\"]\n  },\n  \"timestampSpec\" : {\n   \"format\" : \"auto\",\n   \"column\" : \"timestamp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query Using Bloom Filter Aggregator\nDESCRIPTION: Complete example of a Druid timeseries query using the Bloom filter aggregator to create a Bloom filter from the 'user' dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"wikiticker\",\n  \"intervals\": [ \"2015-09-12T00:00:00.000/2015-09-13T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"bloom\",\n      \"name\": \"userBloom\",\n      \"maxNumEntries\": 100000,\n      \"field\": {\n        \"type\":\"default\",\n        \"dimension\":\"user\",\n        \"outputType\": \"STRING\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Results with 'None' Granularity in Apache Druid\nDESCRIPTION: Example results from a GroupBy query in Apache Druid using 'none' granularity. Shows how data is returned at the original ingestion granularity (millisecond in this case).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T01:02:33.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T01:02:33.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T23:32:45.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-03T03:32:45.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Values from ArrayOfDoublesSketch\nDESCRIPTION: Post-aggregator configuration to retrieve mean values for each column from an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToMeans\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticS3Firehose in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure a StaticS3Firehose in Apache Druid. It specifies the firehose type as 'static-s3' and provides a list of S3 object URIs to ingest events from.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/s3.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-s3\",\n    \"uris\": [\"s3://foo/bar/file.gz\", \"s3://bar/foo/file2.gz\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Overwriting Existing Data in Druid\nDESCRIPTION: Command to submit a task that overwrites the existing data in the 'updates-tutorial' datasource with new data from a different file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring LongFirst Aggregator in Druid JSON\nDESCRIPTION: Defines a longFirst aggregator to compute the metric value with the minimum timestamp or 0 if no row exists. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"longFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Results of GroupBy Query with Filtered DimensionSpec\nDESCRIPTION: Example results from a GroupBy query using a filtered dimensionSpec that only includes the specific 't3' value in the output, while still counting all matching rows correctly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t3\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Derby as Metadata Storage for Apache Druid\nDESCRIPTION: Configuration properties to set up Derby as the metadata storage for Druid. Note that Derby is not recommended for production use.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Docker Image for Druid Integration\nDESCRIPTION: Commands to build a Docker image named 'druid-hadoop-demo' with Hadoop 2.8.3 that will be used for batch indexing tasks with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:2.8.3 .\n```\n\n----------------------------------------\n\nTITLE: JavaScript Aggregator Configuration in Druid\nDESCRIPTION: Configures custom JavaScript aggregation function with aggregate, combine, and reset functions. Supports both metrics and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"javascript\",\n  \"name\": \"<output_name>\",\n  \"fieldNames\"  : [ <column1>, <column2>, ... ],\n  \"fnAggregate\" : \"function(current, column1, column2, ...) {\n                     <updates partial aggregate (current) based on the current row values>\n                     return <updated partial aggregate>\n                   }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return <combined partial results>; }\",\n  \"fnReset\"     : \"function()                   { return <initial value>; }\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"sum(log(x)*y) + 10\",\n  \"fieldNames\": [\"x\", \"y\"],\n  \"fnAggregate\" : \"function(current, a, b)      { return current + (Math.log(a) * b); }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return partialA + partialB; }\",\n  \"fnReset\"     : \"function()                   { return 10; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing pvalue2tailedZtest Post Aggregator in Druid\nDESCRIPTION: This JSON snippet shows how to define a pvalue2tailedZtest post aggregator in Druid for calculating the p-value of a two-sided z-test from a z-score. The input is a z-score which can be calculated using the zscore2sample post aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"pvalue2tailedZtest\",\n  \"name\": \"<output_name>\",\n  \"zScore\": <zscore post_aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Example MomentSketch Aggregator for Ingestion\nDESCRIPTION: Example JSON configuration for using the momentSketch aggregator during data ingestion in Apache Druid. This defines how raw data is sketched during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"momentSketch\", \n  \"name\": \"sketch\", \n  \"fieldName\": \"value\", \n  \"k\": 10, \n  \"compress\": true,\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Server Information in Druid SQL\nDESCRIPTION: SQL query to retrieve information about all servers from the sys.servers table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Lookup Configuration in JSON\nDESCRIPTION: Example response from a GET request to retrieve a specific lookup configuration. This shows the structure of a lookup definition as returned by the Druid coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Apache Druid with Jackson Smile Format\nDESCRIPTION: Demonstrates how to send a query to Druid with Jackson Smile binary JSON format. This example uses the same endpoint but changes the Accept header to use the binary format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/querying.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Example SimpleJSON Input Data for Druid Lookup\nDESCRIPTION: Sample JSON input data that would be parsed by the simpleJson namespaceParseSpec configuration. Contains line-delimited JSON objects where each object has a single key-value pair.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\": \"bar\"}\n{\"baz\": \"bat\"}\n{\"buck\": \"truck\"}\n```\n\n----------------------------------------\n\nTITLE: Copying Druid Distribution to Master Server\nDESCRIPTION: Uses rsync to copy the Druid distribution and edited configurations from a local machine to the Master server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrsync -az apache-druid-0.15.0-incubating/ MASTER_SERVER:apache-druid-0.15.0-incubating/\n```\n\n----------------------------------------\n\nTITLE: Advanced Jetty Server TLS Configuration in Apache Druid\nDESCRIPTION: These are advanced TLS configuration options for the Jetty server in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.https.keyManagerFactoryAlgorithm=javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()\ndruid.server.https.keyManagerPassword=none\ndruid.server.https.includeCipherSuites=Jetty's default include cipher list\ndruid.server.https.excludeCipherSuites=Jetty's default exclude cipher list\ndruid.server.https.includeProtocols=Jetty's default include protocol list\ndruid.server.https.excludeProtocols=Jetty's default exclude protocol list\n```\n\n----------------------------------------\n\nTITLE: Defining HyperUnique Cardinality Post-Aggregator in Druid Query JSON\nDESCRIPTION: Shows the structure of a HyperUnique Cardinality post-aggregator used to wrap a hyperUnique object for use in post-aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/post-aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\"  : \"hyperUniqueCardinality\",\n  \"name\": <output name>,\n  \"fieldName\"  : <the name field value of the hyperUnique aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter Aggregator\nDESCRIPTION: JSON structure for specifying a bloom filter aggregator in Druid queries, including fields like name, maxNumEntries, and field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"type\": \"bloom\",\n      \"name\": <output_field_name>,\n      \"maxNumEntries\": <maximum_number_of_elements_for_BloomKFilter>\n      \"field\": <dimension_spec>\n    }\n```\n\n----------------------------------------\n\nTITLE: JSON Flatten Specification Configuration\nDESCRIPTION: Complete parseSpec configuration showing how to flatten nested JSON fields using both field discovery and explicit field definitions with JsonPath and jackson-jq expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parseSpec\": {\n  \"format\": \"json\",\n  \"flattenSpec\": {\n    \"useFieldDiscovery\": true,\n    \"fields\": [\n      {\n        \"type\": \"root\",\n        \"name\": \"dim1\"\n      },\n      \"dim2\",\n      {\n        \"type\": \"path\",\n        \"name\": \"foo.bar\",\n        \"expr\": \"$.foo.bar\"\n      },\n      {\n        \"type\": \"root\",\n        \"name\": \"foo.bar\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"path-metric\",\n        \"expr\": \"$.nestmet.val\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-0\",\n        \"expr\": \"$.hello[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-4\",\n        \"expr\": \"$.hello[4]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"world-hey\",\n        \"expr\": \"$.world[0].hey\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"worldtree\",\n        \"expr\": \"$.world[1].tree\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"first-food\",\n        \"expr\": \"$.thing.food[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"second-food\",\n        \"expr\": \"$.thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"first-food-by-jq\",\n        \"expr\": \".thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"hello-total\",\n        \"expr\": \".hello | sum\"\n      }\n    ]\n  },\n  \"dimensionsSpec\" : {\n   \"dimensions\" : [],\n   \"dimensionsExclusions\": [\"ignore_me\"]\n  },\n  \"timestampSpec\" : {\n   \"format\" : \"auto\",\n   \"column\" : \"timestamp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Results with 'None' Granularity in Apache Druid\nDESCRIPTION: Example results from a GroupBy query in Apache Druid using 'none' granularity. Shows how data is returned at the original ingestion granularity (millisecond in this case).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T01:02:33.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T01:02:33.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T23:32:45.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-03T03:32:45.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Loading Batch Data Command\nDESCRIPTION: Command to submit the Hadoop indexing task for loading Wikipedia sample data into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Mark.js Copyright and License Information\nDESCRIPTION: Copyright notice for the mark.js library (v8.11.1) created by Julian Khnel and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Configuring Chrome Browser for Kerberos Authentication to Druid Console\nDESCRIPTION: Commands to launch Google Chrome with proper configurations for Kerberos authentication to Druid Coordinator and Overlord consoles, enabling browser-based access to the UI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngoogle-chrome --auth-server-whitelist=\"druid-coordinator-hostname\" --auth-negotiate-delegate-whitelist=\"druid-coordinator-hostname\"\ngoogle-chrome --auth-server-whitelist=\"druid-overlord-hostname\" --auth-negotiate-delegate-whitelist=\"druid-overlord-hostname\"\n```\n\n----------------------------------------\n\nTITLE: Defining GroupBy v1 Query Contexts in Markdown\nDESCRIPTION: This snippet defines a table of supported query contexts for GroupBy v1 queries in Apache Druid. It includes context keys, descriptions, and default values for overriding runtime properties and controlling query behavior such as maximum intermediate rows, maximum results, and off-heap storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_62\n\nLANGUAGE: markdown\nCODE:\n```\n|Key|Description|Default|\n|---|-----------|-------|\n|`maxIntermediateRows`|Can be used to lower the value of `druid.query.groupBy.maxIntermediateRows` for this query.|None|\n|`maxResults`|Can be used to lower the value of `druid.query.groupBy.maxResults` for this query.|None|\n|`useOffheap`|Set to true to store aggregations off-heap when merging results.|false|\n```\n\n----------------------------------------\n\nTITLE: Query Processing Configuration in YAML\nDESCRIPTION: Configuration properties for query processing in Druid's Realtime Process. These settings control buffer sizes, threading, and memory allocation for query execution, affecting performance and resource usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/realtime.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.processing.buffer.sizeBytes: auto (max 1GB)\ndruid.processing.formatString: processing-%s\ndruid.processing.numMergeBuffers: max(2, druid.processing.numThreads / 4)\ndruid.processing.numThreads: Number of cores - 1 (or 1)\ndruid.processing.columnCache.sizeBytes: 0\ndruid.processing.tmpDir: path represented by java.io.tmpdir\n```\n\n----------------------------------------\n\nTITLE: Configuring Strict Bound Filter in Druid Query (JSON)\nDESCRIPTION: This snippet demonstrates how to configure a strict bound filter in a Druid query. It filters for age values strictly between 21 and 31.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"lowerStrict\": true,\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Combined Data in Druid\nDESCRIPTION: SQL query showing the results after combining and overwriting data in the 'updates-tutorial' datasource, with roll-up applied to duplicate rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          2     400 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n 2018-01-01T05:01:00.000Z  mongoose      1     737 \n 2018-01-01T06:01:00.000Z  snake         1    1234 \n 2018-01-01T07:01:00.000Z  octopus       1     115 \n\nRetrieved 6 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Overwriting Existing Data in Druid\nDESCRIPTION: Command to submit an overwrite task that replaces existing data in the 'updates-tutorial' datasource with new data from a different input file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Other Functions in Apache Druid SQL\nDESCRIPTION: Additional functions supported in Druid SQL, including type casting, CASE statements, NULLIF, COALESCE, and bloom filter testing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCAST(value AS TYPE)\nCASE expr WHEN value1 THEN result1 [ WHEN value2 THEN result2 ... ] [ ELSE resultN ] END\nCASE WHEN boolean_expr1 THEN result1 [ WHEN boolean_expr2 THEN result2 ... ] [ ELSE resultN ] END\nNULLIF(value1, value2)\nCOALESCE(value1, value2, ...)\nBLOOM_FILTER_TEST(<expr>, <serialized-filter>)\n```\n\n----------------------------------------\n\nTITLE: Accessing the Overlord Console\nDESCRIPTION: URL pattern for accessing the Overlord console, which provides views of pending tasks, running tasks, available workers, and recent worker activities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/management-uis.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<OVERLORD_IP>:<OVERLORD_PORT>/console.html\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Site Properties for EMR\nDESCRIPTION: YARN site configuration for EMR cluster setup, specifying memory settings, Java options, and task timeout parameters for MapReduce operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nclassification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000]\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to Histogram Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for generating a histogram from a DoublesSketch using specified split points.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToHistogram\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"splitPoints\" : <array of split points>\n}\n```\n\n----------------------------------------\n\nTITLE: Filter Example in Apache Druid Transform Spec\nDESCRIPTION: An example filter that ingests only input rows where the country column has the value \"United States\". This filter is applied after any transforms have been processed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/transform-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"country\",\n  \"value\": \"United States\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building Apache Druid Distribution with Maven in Bash\nDESCRIPTION: Advanced Maven command to build Druid source and binary distributions with signatures and checksums, audit licenses, and skip unit tests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/build.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Papache-release,dist,rat -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Configuring floatFirst Aggregator in Apache Druid for Queries\nDESCRIPTION: The floatFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Pull-deps with Multiple Extensions\nDESCRIPTION: Example command showing how to download multiple Druid extensions and Hadoop dependencies with specific versions using the pull-deps tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/pull-deps.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.14.1-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.14.1-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Granularity in Druid\nDESCRIPTION: This snippet shows how to set both segment and query granularity in the granularitySpec of a Druid ingestion task. It configures hourly segments with minute-level query granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"queryGranularity\" : \"MINUTE\"\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Arbitrary Granularity Spec Configuration Table\nDESCRIPTION: Specifies the configuration parameters for arbitrary granularity specification in Druid, used for generating segments with arbitrary intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Field | Type | Description | Required |\n|-------|------|-------------|----------|\n| queryGranularity | string | The minimum granularity to be able to query results at and the granularity of the data inside the segment. E.g. a value of \"minute\" will mean that data is aggregated at minutely granularity. That is, if there are collisions in the tuple (minute(timestamp), dimensions), then it will aggregate values together using the aggregators instead of storing individual rows. A granularity of 'NONE' means millisecond granularity.| no (default == 'NONE') |\n| rollup | boolean | rollup or not | no (default == true) |\n| intervals | string | A list of intervals for the raw data being ingested. Ignored for real-time ingestion. | no. If specified, batch ingestion tasks may skip determining partitions phase which results in faster ingestion. |\n```\n\n----------------------------------------\n\nTITLE: Configuring Partial Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Partial Extraction Function in Apache Druid. It returns the dimension value unchanged if the regex matches, otherwise returns null.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"partial\", \"expr\" : <regular_expression> }\n```\n\n----------------------------------------\n\nTITLE: Using Bloom Filter in SQL WHERE Clause\nDESCRIPTION: SQL query example demonstrating how to use a bloom filter in a WHERE clause using the bloom_filter_test operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*) FROM druid.foo WHERE bloom_filter_test(<expr>, '<serialized_bytes_for_BloomKFilter>')\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Extraction Function in Druid\nDESCRIPTION: Demonstrates how to chain multiple extraction functions together using the cascade type. This example chains regex, JavaScript, and substring extraction functions to sequentially transform a dimension value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"cascade\", \n  \"extractionFns\": [\n    { \n      \"type\" : \"regex\", \n      \"expr\" : \"/([^/]+)/\", \n      \"replaceMissingValue\": false,\n      \"replaceMissingValueWith\": null\n    },\n    { \n      \"type\" : \"javascript\", \n      \"function\" : \"function(str) { return \\\"the \\\".concat(str) }\" \n    },\n    { \n      \"type\" : \"substring\", \n      \"index\" : 0, \"length\" : 7 \n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Scan Query in Apache Druid SQL\nDESCRIPTION: This SQL query demonstrates a scan operation, retrieving user and page information for Wikipedia edits between 2015-09-12 02:00:00 and 2015-09-12 03:00:00, limited to 5 results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT user, page FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Example InfluxDB Line Protocol Format\nDESCRIPTION: Example of an InfluxDB Line Protocol data point showing measurement, tags, metrics, and timestamp. The line represents CPU metrics with tags for application, host, and region, along with usage measurements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Double Min Aggregator in Druid\nDESCRIPTION: Computes minimum of metric values and Double.POSITIVE_INFINITY. Takes output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Metadata Query in Druid\nDESCRIPTION: Configuration properties for Segment Metadata queries in Druid. These settings control default history interval and analysis types for metadata queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_56\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.segmentMetadata.defaultHistory=P1W\ndruid.query.segmentMetadata.defaultAnalysisTypes=[\"cardinality\", \"interval\", \"minmax\"]\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Task with Roll-up Configuration\nDESCRIPTION: JSON configuration for a Druid index task that specifies how to ingest the sample data with roll-up enabled. The configuration defines dimensions, metrics, and a query granularity of one minute.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"rollup-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"srcIP\",\n              \"dstIP\"\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"rollup-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Processing Settings in Markdown\nDESCRIPTION: A markdown table detailing various processing configuration options for Druid, including buffer sizes, thread counts, and caching parameters for query processing and intermediate result storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.processing.buffer.sizeBytes`|This specifies a buffer size for the storage of intermediate results. The computation engine in both the Historical and Realtime nodes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.|1073741824 (1GB)|\n|`druid.processing.buffer.poolCacheMaxCount`|processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.|Integer.MAX_VALUE|\n|`druid.processing.formatString`|Realtime and historical nodes use this format string to name their processing threads.|processing-%s|\n|`druid.processing.numMergeBuffers`|The number of direct memory buffers available for merging query results. The buffers are sized by `druid.processing.buffer.sizeBytes`. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.|`max(2, druid.processing.numThreads / 4)`|\n|`druid.processing.numThreads`|The number of processing threads to have available for parallel processing of segments. Our rule of thumb is `num_cores - 1`, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value `1`.|Number of cores - 1 (or 1)|\n|`druid.processing.columnCache.sizeBytes`|Maximum size in bytes for the dimension value lookup cache. Any value greater than `0` enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.|`0` (disabled)|\n|`druid.processing.fifo`|If the processing queue should treat tasks of equal priority in a FIFO manner|`false`|\n|`druid.processing.tmpDir`|Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default `java.io.tmpdir` path.|path represented by `java.io.tmpdir`|\n```\n\n----------------------------------------\n\nTITLE: Defining Time Intervals for Batch Ingestion in Druid\nDESCRIPTION: Demonstrates how to specify the time interval for batch ingestion tasks within the granularitySpec, which limits which data rows will be ingested based on their timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"queryGranularity\" : \"MINUTE\",\n    \"intervals\" : [\"2018-01-01/2018-01-02\"],\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Community Extensions Using Pull-deps Tool\nDESCRIPTION: Command for downloading and installing community/third-party Druid extensions using the pull-deps tool, specifying Maven coordinates for the desired extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/including-extensions.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava \\\n  -cp \"lib/*\" \\\n  -Ddruid.extensions.directory=\"extensions\" \\\n  -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n  org.apache.druid.cli.Main tools pull-deps \\\n  --no-default-hadoop \\\n  -c \"com.example:druid-example-extension:1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Consistent Hash Balancer Configuration\nDESCRIPTION: Configuration property for enabling the Consistent Hash Balancer for Avatica JDBC request routing, which is an experimental implementation provided for testing purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_7\n\nLANGUAGE: java\nCODE:\n```\ndruid.router.avatica.balancer.type=consistentHash\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid Console URL Format\nDESCRIPTION: The format for accessing the Druid Console through the Router process. Requires Router's management proxy to be enabled and Druid SQL to be enabled on Broker processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/druid-console.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Defining Protobuf Message Structure\nDESCRIPTION: Proto file defining the structure of the Protobuf message for metrics data. It includes fields for various metric attributes like unit, http_method, value, timestamp, etc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\nmessage Metrics {\n  string unit = 1;\n  string http_method = 2;\n  int32 value = 3;\n  string timestamp = 4;\n  string http_code = 5;\n  string page = 6;\n  string metricType = 7;\n  string server = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Metrics Table in Markdown\nDESCRIPTION: Table defining SQL metrics emitted by Broker including metric names, descriptions, dimensions and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_20\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`sqlQuery/time`|Milliseconds taken to complete a SQL.|id, nativeQueryIds, dataSource, remoteAddress, success.|< 1s|\n|`sqlQuery/bytes`|number of bytes returned in SQL response.|id, nativeQueryIds, dataSource, remoteAddress, success.| |\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Medium Single Server Deployment\nDESCRIPTION: Command to start Druid in medium configuration, designed for machines with 16 CPU and 128GB RAM (equivalent to an i3.4xlarge instance).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/single-server.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-medium\n```\n\n----------------------------------------\n\nTITLE: Installing Tranquility for Apache Druid\nDESCRIPTION: This snippet shows how to download, extract, and set up Tranquility for use with Apache Druid. It downloads the Tranquility distribution, extracts it, and renames the folder for easier access.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz\ntar -xzf tranquility-distribution-0.8.3.tgz\nmv tranquility-distribution-0.8.3 tranquility\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Numeric TopNMetricSpec in Druid\nDESCRIPTION: Simple string-based metric specification for sorting topN results by a specific metric name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnmetricspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": \"<metric_name>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookups with JSON\nDESCRIPTION: Example JSON configuration for multiple lookups across different tiers, including map-based and JDBC-based lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__default\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupTable\",\n          \"keyColumn\": \"country_id\",\n          \"valueColumn\": \"country_name\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  },\n  \"realtime_customer1\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer1\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"847632\": \"Internal Use Only\"\n        }\n      }\n    }\n  },\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"77483\": \"United States\"\n        }\n      }\n    },\n    \"site_id_customer2\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"map\",\n        \"map\": {\n          \"AHF77\": \"Home\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Config\nDESCRIPTION: Configuration snippet showing how to include the DataSketches extension in Druid's configuration file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Hour Granularity\nDESCRIPTION: Example of a GroupBy query using hour granularity for time bucketing\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":\"hour\",\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Scan Query\nDESCRIPTION: A simple SQL scan query that retrieves the user and page columns from the Wikipedia dataset for a specific hour. It uses LIMIT to restrict the result to 5 rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT user, page FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Scaling in Druid\nDESCRIPTION: This snippet illustrates how to configure the shardSpec for scaling in Druid realtime processes. It shows setting up an additional process with a different partitionNum to distribute data storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"linear\",\n    \"partitionNum\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Using JavaScript Filter in Druid Queries\nDESCRIPTION: The JavaScript filter evaluates a JavaScript function for each dimension value. Rows where the function returns true are included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : <dimension_string>,\n  \"function\" : \"function(value) { <...> }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Numbered Sharding in Druid TuningConfig\nDESCRIPTION: Example of setting up numbered sharding in Druid's TuningConfig. This strategy requires sequential partition numbering and explicit specification of the total number of partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"numbered\",\n        \"partitionNum\": 0,\n        \"partitions\": 2\n    }\n```\n\n----------------------------------------\n\nTITLE: Querying Overwritten Data in Druid\nDESCRIPTION: SQL query showing the results after overwriting the initial data, where the tiger row is now lion, aardvark has a different number, and giraffe is replaced with bear.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          1     100 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n\nRetrieved 3 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Maven Shade Plugin Configuration\nDESCRIPTION: Maven shade plugin configuration for creating a fat jar with relocated Jackson packages to avoid conflicts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-shade-plugin</artifactId>\n     <executions>\n         <execution>\n             <phase>package</phase>\n             <goals>\n                 <goal>shade</goal>\n             </goals>\n             <configuration>\n                 <outputFile>\n                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                 </outputFile>\n                 <relocations>\n                     <relocation>\n                         <pattern>com.fasterxml.jackson</pattern>\n                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                     </relocation>\n                 </relocations>\n                 <artifactSet>\n                     <includes>\n                         <include>*:*</include>\n                     </includes>\n                 </artifactSet>\n                 <filters>\n                     <filter>\n                         <artifact>*:*</artifact>\n                         <excludes>\n                             <exclude>META-INF/*.SF</exclude>\n                             <exclude>META-INF/*.DSA</exclude>\n                             <exclude>META-INF/*.RSA</exclude>\n                         </excludes>\n                     </filter>\n                 </filters>\n                 <transformers>\n                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                 </transformers>\n             </configuration>\n         </execution>\n     </executions>\n </plugin>\n```\n\n----------------------------------------\n\nTITLE: Running Apache Druid Broker Node in Java\nDESCRIPTION: Command to start the Broker node in Apache Druid. This command initializes the Broker server, which is responsible for routing queries and merging results in a distributed Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/broker.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Flags for Apache Druid\nDESCRIPTION: A set of recommended JVM flags for optimizing Apache Druid performance, including memory management, garbage collection logging, and error handling. These flags cover timezone settings, file encoding, temporary directory configuration, logging, and memory allocation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=<something other than /tmp which might be mounted to volatile tmpfs file system>\n-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\n-Dorg.jboss.logging.provider=slf4j\n-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger\n-Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown\n-Dlog4j.shutdownHookEnabled=true\n-XX:+PrintGCDetails\n-XX:+PrintGCDateStamps\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCApplicationStoppedTime\n-XX:+PrintGCApplicationConcurrentTime\n-Xloggc:/var/logs/druid/historical.gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=50\n-XX:GCLogFileSize=10m\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/var/logs/druid/historical.hprof\n-XX:MaxDirectMemorySize=10240g\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Kafka for Druid\nDESCRIPTION: Command to start Tranquility Kafka integration using a configuration file. Note that this method is deprecated in favor of the Kafka Indexing Service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/stream-push.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka -configFile <path_to_config_file>/kafka.json\n```\n\n----------------------------------------\n\nTITLE: Querying Rolled-up Data Using Druid SQL\nDESCRIPTION: Example of using Druid's SQL interface to query the ingested data and observe the effects of roll-up.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"rollup-tutorial\";\n\n __time                    bytes   count  dstIP    packets  srcIP   \n\n 2018-01-01T01:01:00.000Z   35937      3  2.2.2.2      286  1.1.1.1 \n 2018-01-01T01:02:00.000Z  366260      2  2.2.2.2      415  1.1.1.1 \n 2018-01-01T01:03:00.000Z   10204      1  2.2.2.2       49  1.1.1.1 \n 2018-01-02T21:33:00.000Z  100288      2  8.8.8.8      161  7.7.7.7 \n 2018-01-02T21:35:00.000Z    2818      1  8.8.8.8       12  7.7.7.7 \n\nRetrieved 5 rows in 1.18s.\n\ndsql>\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Server\nDESCRIPTION: Commands to download, extract and start Tranquility Server for stream ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz\ntar -xzf tranquility-distribution-0.8.0.tgz\ncd tranquility-distribution-0.8.0\nbin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json\n```\n\n----------------------------------------\n\nTITLE: Running DumpSegment Tool in Java\nDESCRIPTION: Command to execute the DumpSegment tool, specifying the segment directory and output file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/dump-segment.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\njava org.apache.druid.cli.Main tools dump-segment \\\n  --directory /home/druid/path/to/segment/ \\\n  --out /home/druid/output.txt\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL User for Druid\nDESCRIPTION: Command to create a new PostgreSQL user named 'druid' with password authentication.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncreateuser druid -P\n```\n\n----------------------------------------\n\nTITLE: Using the 'like' Function in Apache Druid Expressions\nDESCRIPTION: Demonstrates the usage of the 'like' function in Druid expressions, which is equivalent to the SQL LIKE operator for pattern matching on strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/misc/math-expr.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nlike(expr, pattern[, escape])\n```\n\n----------------------------------------\n\nTITLE: Configuring Blacklisting Parameters in Apache Druid\nDESCRIPTION: These properties configure the threshold and timeout settings for blacklisting MiddleManagers in Apache Druid. They control the maximum retries before blacklisting, blacklist backoff time, cleanup period, and maximum percentage of blacklisted workers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/overlord.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.runner.maxRetriesBeforeBlacklist\ndruid.indexer.runner.workerBlackListBackoffTime\ndruid.indexer.runner.workerBlackListCleanupPeriod\ndruid.indexer.runner.maxPercentageBlacklistWorkers\n```\n\n----------------------------------------\n\nTITLE: Configuring String Format Extraction Function in Druid\nDESCRIPTION: Configuration for formatting dimension values using sprintf-style expressions with optional null handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"stringFormat\", \"format\" : <sprintf_expression>, \"nullHandling\" : <optional attribute for handling null value> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Prioritization in Druid Broker (Properties)\nDESCRIPTION: Properties for configuring how the Broker balances connections and selects segments across tiers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.balancer.type=random\ndruid.broker.select.tier=highestPriority\ndruid.broker.select.tier.custom.priorities=None\n```\n\n----------------------------------------\n\nTITLE: Adding Druid Extensions to Maven POM\nDESCRIPTION: This XML snippet demonstrates how to add Druid extensions to the Maven POM file (services/pom.xml) for including necessary dependencies in the build process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>druid-avro-extensions</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>druid-parquet-extensions</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>druid-hdfs-storage</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>mysql-metadata-storage</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Running Apache Druid Broker Process\nDESCRIPTION: Command to start the Apache Druid Broker process. This command initializes the Broker server, which is responsible for routing queries in a distributed Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/broker.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Configuring Derivative DataSource Supervisor in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure a derivativeDataSource supervisor in Druid. It specifies the base dataSource, dimensions, metrics, and tuning configuration for creating a derived dataSource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"derivativeDataSource\",\n    \"baseDataSource\": \"wikiticker\",\n    \"dimensionsSpec\": {\n        \"dimensions\": [\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\"\n        ]\n    },\n    \"metricsSpec\": [\n        {\n            \"name\": \"count\",\n            \"type\": \"count\"\n        },\n        {\n            \"name\": \"added\",\n            \"type\": \"longSum\",\n            \"fieldName\": \"added\"\n        }\n    ],\n    \"tuningConfig\": {\n        \"type\": \"hadoop\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Search Query Extraction Function in Druid\nDESCRIPTION: Configuration for extracting values based on search query specification matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"searchQuery\", \"query\" : <search_query_spec> }\n```\n\n----------------------------------------\n\nTITLE: Example Password Request JSON\nDESCRIPTION: JSON structure for password change requests in the authentication API\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"password\": \"helloworld\"\n}\n```\n\n----------------------------------------\n\nTITLE: Timestamp Specification\nDESCRIPTION: Configuration for parsing ISO 8601 timestamps from the 'ts' column in the input data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Append Task for Segment Merging in Apache Druid (Deprecated)\nDESCRIPTION: JSON configuration for an Append task in Apache Druid. This deprecated task appends a list of segments into a single segment, specifying task ID, data source, segments to append, optional aggregators, and task context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"append\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"segments\": <JSON list of DataSegment objects to append>,\n    \"aggregations\": <optional list of aggregators>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Thrift Ingestion with Tranquility in Apache Druid\nDESCRIPTION: JSON configuration example for setting up realtime ingestion of Thrift data using Tranquility. This snippet shows the parser configuration with Thrift class specification, protocol settings, and related dataSchema settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSources\": [{\n    \"spec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"book\",\n        \"granularitySpec\": {          },\n        \"parser\": {\n          \"type\": \"thrift\",\n          \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n          \"protocol\": \"compact\",\n          \"parseSpec\": {\n            \"format\": \"json\",\n            ...\n          }\n        },\n        \"metricsSpec\": [...]\n      },\n      \"tuningConfig\": {...}\n    },\n    \"properties\": {...}\n  }],\n  \"properties\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining FragmentSearchQuerySpec in JSON for Apache Druid\nDESCRIPTION: This JSON snippet defines a FragmentSearchQuerySpec, which matches if a dimension value contains all specified values, case-insensitive by default. It includes 'type', 'case_sensitive', and 'values' fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"fragment\",\n  \"case_sensitive\" : false,\n  \"values\" : [\"fragment1\", \"fragment2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Active Tasks on MiddleManager Response in JSON\nDESCRIPTION: Example JSON response from the /druid/worker/v1/tasks endpoint showing active tasks being run on a MiddleManager as a list of task ID strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\"index_wikiticker_2019-02-11T02:20:15.316Z\"]\n```\n\n----------------------------------------\n\nTITLE: Listing Active Tasks on MiddleManager Response in JSON\nDESCRIPTION: Example JSON response from the /druid/worker/v1/tasks endpoint showing active tasks being run on a MiddleManager as a list of task ID strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\"index_wikiticker_2019-02-11T02:20:15.316Z\"]\n```\n\n----------------------------------------\n\nTITLE: Defining GroupBy v1 Runtime Properties in Markdown\nDESCRIPTION: This snippet defines a table of supported runtime properties for GroupBy v1 queries in Apache Druid. It includes property names, descriptions, and default values for configurations such as maximum intermediate rows and maximum results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_61\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxIntermediateRows`|Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail.|50000|\n|`druid.query.groupBy.maxResults`|Maximum number of results. Queries that exceed this limit will fail.|500000|\n```\n\n----------------------------------------\n\nTITLE: Submit Hourly Compaction Task\nDESCRIPTION: Bash command to submit the hourly compaction task specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json\n```\n\n----------------------------------------\n\nTITLE: Using NOT Logical Expression Filter in Druid Queries\nDESCRIPTION: The NOT filter negates another filter. Rows that do not match the specified filter will be included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"not\", \"field\": <filter> }\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Extraction Function for String Manipulation\nDESCRIPTION: A JavaScript extraction function example that extracts the first three characters from a dimension value. This function takes the dimension value as a string and returns a transformed string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str.substr(0, 3); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Kill Task in Druid\nDESCRIPTION: cURL command to submit a Kill Task that permanently removes disabled segments from metadata and deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Broadcast Rule in Apache Druid\nDESCRIPTION: The Forever Broadcast Rule co-locates segments of different data sources in historical processes. It ensures that segments of a data source are broadcasted to servers holding any segments of the specified co-located data sources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastForever\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Lookup ExtractorFactory in JSON\nDESCRIPTION: JSON configuration for setting up a Kafka lookup extractor that reads name/key pairs from a Kafka topic. This allows renaming dimension values from a stream, with the old value as the key and the new value as the message.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"kafka\",\n  \"kafkaTopic\":\"testTopic\",\n  \"kafkaProperties\":{\"zookeeper.connect\":\"somehost:2181/kafka\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Hadoop Configuration to Druid Classpath\nDESCRIPTION: Commands to copy Hadoop configuration files to the Druid classpath for integration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml\n```\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p {PATH_TO_DRUID}/quickstart/tutorial/conf/druid/_common/hadoop-xml\ncp /tmp/shared/hadoop_xml/*.xml {PATH_TO_DRUID}/quickstart/tutorial/conf/druid/_common/hadoop-xml/\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Updated Data in Apache Druid\nDESCRIPTION: This SQL query groups the data by time and animal, summing the count and number columns to show how Druid handles aggregation across multiple segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect __time, animal, SUM(\"count\"), SUM(\"number\") from \"updates-tutorial\" group by __time, animal;\n```\n\n----------------------------------------\n\nTITLE: Enabling Process Termination on Out-of-Memory Errors in Java for Apache Druid\nDESCRIPTION: This Java VM argument configures the JVM to terminate the process when an out-of-memory error occurs. This is useful for Apache Druid as it allows for automatic restart of the process in case of memory issues.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n-XX:+ExitOnOutOfMemoryError\n```\n\n----------------------------------------\n\nTITLE: Basic Druid DataSchema Definition\nDESCRIPTION: Initial empty dataSchema structure that forms the foundation of the ingestion specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {}\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for object-assign\nDESCRIPTION: Copyright notice and MIT license information for the object-assign library by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring FloatSum Aggregator in Druid JSON\nDESCRIPTION: Defines a floatSum aggregator to compute the sum of values as a 32-bit floating point value. Similar to longSum and doubleSum, it requires an output name and field name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling Off-heap Lookup in Apache Druid\nDESCRIPTION: This JSON configuration shows an off-heap lookup that will be cached once and never swapped. It specifies the lookup type, data fetcher, and off-heap polling cache factory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"offHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Example Protobuf Message JSON Format\nDESCRIPTION: Sample JSON structure representing the metrics data format that will be converted to Protobuf.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"unit\": \"milliseconds\",\n  \"http_method\": \"GET\",\n  \"value\": 44,\n  \"timestamp\": \"2017-04-06T02:36:22Z\",\n  \"http_code\": \"200\",\n  \"page\": \"/\",\n  \"metricType\": \"request/latency\",\n  \"server\": \"www1.example.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Datasource in Druid\nDESCRIPTION: Defines the basic table datasource structure in Druid, which is equivalent to a database table. Can be represented as a simple string or a JSON structure with type and name properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/datasource.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"table\",\n\t\"name\": \"<string_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Druid DataSchema Definition\nDESCRIPTION: Initial empty dataSchema structure that forms the foundation of the ingestion specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Prioritization in Druid Broker (Properties)\nDESCRIPTION: Properties for configuring how the Broker balances connections and selects segments across tiers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.balancer.type=random\ndruid.broker.select.tier=highestPriority\ndruid.broker.select.tier.custom.priorities=None\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Prioritization in Druid Broker (Properties)\nDESCRIPTION: Properties for configuring how the Broker balances connections and selects segments across tiers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_29\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.balancer.type=random\ndruid.broker.select.tier=highestPriority\ndruid.broker.select.tier.custom.priorities=None\n```\n\n----------------------------------------\n\nTITLE: Implementing Strict Bounds Age Filter in Druid\nDESCRIPTION: Example of a bound filter with strict comparison for age between 21 and 31 exclusively.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"lowerStrict\": true,\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Strict Bounds Age Filter in Druid\nDESCRIPTION: Example of a bound filter with strict comparison for age between 21 and 31 exclusively.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"lowerStrict\": true,\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Feature Comparison Table in Markdown\nDESCRIPTION: A markdown table comparing features across three batch ingestion methods in Apache Druid: Hadoop-based, native parallel, and native local ingestion. The table covers aspects like parallelization, supported modes, dependencies, rollup capabilities, partitioning methods, input locations, and file format support.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop-vs-native-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|   |Hadoop-based ingestion|Native parallel ingestion|Native local ingestion|\n|---|----------------------|-------------------------|----------------------|\n| Parallel indexing | Always parallel | Parallel if firehose is splittable | Always sequential |\n| Supported indexing modes | Replacing mode | Both appending and replacing modes | Both appending and replacing modes |\n| External dependency | Hadoop (it internally submits Hadoop jobs) | No dependency | No dependency |\n| Supported [rollup modes](/docs/latest/ingestion/index.html#roll-up-modes) | Perfect rollup | Best-effort rollup | Both perfect and best-effort rollup |\n| Supported partitioning methods | [Both Hash-based and range partitioning](/docs/latest/ingestion/hadoop.html#partitioning-specification) | N/A | Hash-based partitioning (when `forceGuaranteedRollup` = true) |\n| Supported input locations | All locations accessible via HDFS client or Druid dataSource | All implemented [firehoses](./firehose.html) | All implemented [firehoses](./firehose.html) |\n| Supported file formats | All implemented Hadoop InputFormats | Currently text file formats (CSV, TSV, JSON) by default. Additional formats can be added though a [custom extension](../development/modules.html) implementing [`FiniteFirehoseFactory`](https://github.com/apache/incubator-druid/blob/master/core/src/main/java/org/apache/druid/data/input/FiniteFirehoseFactory.java) | Currently text file formats (CSV, TSV, JSON) by default. Additional formats can be added though a [custom extension](../development/modules.html) implementing [`FiniteFirehoseFactory`](https://github.com/apache/incubator-druid/blob/master/core/src/main/java/org/apache/druid/data/input/FiniteFirehoseFactory.java) |\n| Saving parse exceptions in ingestion report | Currently not supported | Currently not supported | Supported |\n| Custom segment version | Supported, but this is NOT recommended | N/A | N/A |\n```\n\n----------------------------------------\n\nTITLE: Running the DumpSegment Tool in Apache Druid\nDESCRIPTION: Example command for executing the DumpSegment tool to analyze a Druid segment. The command requires specifying the classpath with Druid libraries, the segment directory path, and an output file location.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/dump-segment.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools dump-segment \\\n  --directory /home/druid/path/to/segment/ \\\n  --out /home/druid/output.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependencies in Druid Hadoop Index Task\nDESCRIPTION: JSON configuration showing how to specify which version of Hadoop client libraries Druid should load when processing a Hadoop indexing task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Lookup in Druid\nDESCRIPTION: Example configuration for a JDBC lookup in Druid. It specifies database connection details, table information, and polling period for fetching lookup data from a MySQL database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"jdbc\",\n  \"namespace\":\"some_lookup\",\n  \"connectorConfig\":{\n    \"createTables\":true,\n    \"connectURI\":\"jdbc:mysql://localhost:3306/druid\",\n    \"user\":\"druid\",\n    \"password\":\"diurd\"\n  },\n  \"table\":\"some_lookup_table\",\n  \"keyColumn\":\"the_old_dim_value\",\n  \"valueColumn\":\"the_new_dim_value\",\n  \"tsColumn\":\"timestamp_column\",\n  \"pollPeriod\":600000\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Kafka with Configuration File in Bash\nDESCRIPTION: This command starts Tranquility Kafka using a specified configuration file. It enables loading data from Kafka into Druid without writing any code. Note that this method is deprecated in favor of the Kafka Indexing Service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/stream-push.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka -configFile <path_to_config_file>/kafka.json\n```\n\n----------------------------------------\n\nTITLE: Setting Hadoop Job Properties for Classloader Isolation in Druid Ingestion Tasks\nDESCRIPTION: Example JSON configuration showing how to set Hadoop job properties in Druid's tuning configuration to exclude specific validation classes from the system classpath while including other system classes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\": {\n  \"mapreduce.job.classloader\": \"true\",\n  \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Druid Multi-value Column Data Structures\nDESCRIPTION: This code snippet shows how the three basic data structures are modified to represent a multi-value dimension column in Druid. It demonstrates changes in the column data and bitmap structures to accommodate multiple values for a single row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/segments.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   [0,1],  <--Row value of multi-value column can have array of values\n   1,\n   1]\n\n3: Bitmaps - one for each unique value\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,1,1,1]\n                            ^\n                            |\n                            |\n    Multi-value column has multiple non-zero entries\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function without Length Configuration in Druid\nDESCRIPTION: Configuration for extracting substring of dimension values with only start index specified.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 3 }\n```\n\n----------------------------------------\n\nTITLE: Parsing Timestamps in Druid Expressions\nDESCRIPTION: Illustrates how to parse timestamps in Druid expressions using the 'timestamp_parse' function. It can use a custom Joda DateTimeFormat pattern and specify a timezone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/misc/math-expr.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\ntimestamp_parse(string expr, [pattern, [timezone]])\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling Off-heap Lookup in Apache Druid\nDESCRIPTION: Example configuration for an off-heap lookup that is cached once and never swapped, using JDBC data fetcher.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"offHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Filter in Druid\nDESCRIPTION: Filter using Java regular expressions to match dimension values against patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"regex\", \"dimension\": <dimension_string>, \"pattern\": <pattern_string> }\n```\n\n----------------------------------------\n\nTITLE: Binding Custom Storage Handlers in Druid Module\nDESCRIPTION: Example code showing how to bind custom DataSegmentPuller and DataSegmentPusher implementations for HDFS storage in a Druid module. This demonstrates the use of Guice bindings to register new storage handlers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nBinders.dataSegmentPullerBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);\n\nBinders.dataSegmentPusherBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Response with Custom Origin Time\nDESCRIPTION: Example response showing how custom origin time affects the bucketing of results. Demonstrates how events are grouped into different buckets based on the specified origin time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-29T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Running Apache Druid Historical Process\nDESCRIPTION: Command to start the Historical process in Apache Druid. This command uses the Main class from the org.apache.druid.cli package to initiate the Historical server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/historical.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server historical\n```\n\n----------------------------------------\n\nTITLE: Querying Kafka Supervisor Stats in Apache Druid\nDESCRIPTION: HTTP GET request to retrieve combined live row statistics from all tasks managed by a Kafka supervisor. This endpoint aggregates data from multiple Kafka ingestion tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/supervisor/<supervisor-id>/stats\n```\n\n----------------------------------------\n\nTITLE: Configuring Jetty Server TLS in Apache Druid\nDESCRIPTION: These properties set up TLS for the embedded Jetty web server in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.https.keyStorePath=none\ndruid.server.https.keyStoreType=none\ndruid.server.https.certAlias=none\ndruid.server.https.keyStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Hourly Compaction Task Execution\nDESCRIPTION: Command to submit the hourly granularity compaction task to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json\n```\n\n----------------------------------------\n\nTITLE: JSON Parser Configuration\nDESCRIPTION: Parser configuration for handling JSON formatted input data in string format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Ingested Data in Druid SQL\nDESCRIPTION: Example of using the Druid SQL command-line interface to query the ingested data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"ingestion-tutorial\";\n\n\n __time                    bytes  cost  count  dstIP    dstPort  packets  protocol  srcIP    srcPort \n\n 2018-01-01T01:01:00.000Z   6000   4.9      3  2.2.2.2     3000       60  6         1.1.1.1     2000 \n 2018-01-01T01:02:00.000Z   9000  18.1      2  2.2.2.2     7000       90  6         1.1.1.1     5000 \n 2018-01-01T01:03:00.000Z   6000   4.3      1  2.2.2.2     7000       60  6         1.1.1.1     5000 \n 2018-01-01T02:33:00.000Z  30000  56.9      2  8.8.8.8     5000      300  17        7.7.7.7     4000 \n 2018-01-01T02:35:00.000Z  30000  46.3      1  8.8.8.8     5000      300  17        7.7.7.7     4000 \n\nRetrieved 5 rows in 0.12s.\n\ndsql>\n```\n\n----------------------------------------\n\nTITLE: Implementing Numeric Range Filter in Druid\nDESCRIPTION: Example of filtering on a range of numeric values using bound filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"bound\",\n  \"dimension\": \"myFloatColumn\",\n  \"ordering\": \"numeric\",\n  \"lower\": \"10\",\n  \"lowerStrict\": false,\n  \"upper\": \"20\",\n  \"upperStrict\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Metadata into Derby Database\nDESCRIPTION: SQL commands to import exported metadata CSV files into Derby database tables. It uses the SYSCS_UTIL.SYSCS_IMPORT_TABLE procedure for each Druid metadata table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/export-metadata.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SEGMENTS','/tmp/csv/druid_segments.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_RULES','/tmp/csv/druid_rules.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_CONFIG','/tmp/csv/druid_config.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_DATASOURCE','/tmp/csv/druid_dataSource.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SUPERVISORS','/tmp/csv/druid_supervisors.csv',',','\"',null,0);\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Announcement in Druid\nDESCRIPTION: Configuration for how to announce and unannounce segments in ZooKeeper using Curator. Controls segment information storage and optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_21\n\nLANGUAGE: properties\nCODE:\n```\ndruid.announcer.segmentsPerNode=50\ndruid.announcer.maxBytesPerNode=524288\ndruid.announcer.skipDimensionsAndMetrics=false\ndruid.announcer.skipLoadSpec=false\n```\n\n----------------------------------------\n\nTITLE: Verifying Compaction Results - Druid SQL\nDESCRIPTION: SQL query to verify the row count remains unchanged after compaction.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select count(*) from \"compaction-tutorial\";\n\n EXPR$0 \n\n  39244 \n\nRetrieved 1 row in 1.30s.\n```\n\n----------------------------------------\n\nTITLE: Configuring Authentication Chain in Apache Druid\nDESCRIPTION: This JSON snippet shows how to configure the authentication chain in Apache Druid, enabling Kerberos and HTTP Basic authenticators from the druid-kerberos and druid-basic-security core extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/auth.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authenticatorChain=[\\\"kerberos\\\", \\\"basic\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Batch Ingestion with dataSource InputSpec in Apache Druid\nDESCRIPTION: JSON configuration example for reindexing data in Apache Druid using the dataSource inputSpec type. This configuration reads data from an existing Druid dataSource named 'wikipedia' for a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"dataSource\",\n    \"ingestionSpec\" : {\n      \"dataSource\": \"wikipedia\",\n      \"intervals\": [\"2014-10-20T00:00:00Z/P2W\"]\n    }\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Empty Druid DataSchema in JSON\nDESCRIPTION: Starting point for the Druid ingestion spec with an empty dataSchema object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {}\n```\n\n----------------------------------------\n\nTITLE: Submitting Supervisor Spec via CURL\nDESCRIPTION: Example CURL command for submitting a Kinesis supervisor specification to the Druid Overlord API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License Header\nDESCRIPTION: License header for Prism, a lightweight syntax highlighting library created by Lea Verou and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Java String Format Pattern\nDESCRIPTION: Shows the format string pattern usage following Java's String.format convention for string formatting in Druid expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/misc/math-expr.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nString.format(\"pattern\", args...)\n```\n\n----------------------------------------\n\nTITLE: Querying Ingested Data in Druid with dsql\nDESCRIPTION: Bash session showing how to use the dsql command-line client to query data that has been ingested into Druid. The example demonstrates a simple SELECT query and shows the formatted results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"ingestion-tutorial\";\n\n\n __time                    bytes  cost  count  dstIP    dstPort  packets  protocol  srcIP    srcPort \n\n 2018-01-01T01:01:00.000Z   6000   4.9      3  2.2.2.2     3000       60  6         1.1.1.1     2000 \n 2018-01-01T01:02:00.000Z   9000  18.1      2  2.2.2.2     7000       90  6         1.1.1.1     5000 \n 2018-01-01T01:03:00.000Z   6000   4.3      1  2.2.2.2     7000       60  6         1.1.1.1     5000 \n 2018-01-01T02:33:00.000Z  30000  56.9      2  8.8.8.8     5000      300  17        7.7.7.7     4000 \n 2018-01-01T02:35:00.000Z  30000  46.3      1  8.8.8.8     5000      300  17        7.7.7.7     4000 \n\nRetrieved 5 rows in 0.12s.\n\ndsql> \n```\n\n----------------------------------------\n\nTITLE: Manual Task Submission via cURL\nDESCRIPTION: Direct HTTP POST request to submit the ingestion task to Druid's indexing service without using the helper script. Demonstrates the raw API interaction method.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Certificate Authentication in Apache Druid\nDESCRIPTION: Configuration options for setting up client certificate authentication in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/tls-support.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.server.https.requireClientCertificate`|If set to true, clients must identify themselves by providing a TLS certificate.  If `requireClientCertificate` is false, the rest of the options in this table are ignored.|false|no|\n|`druid.server.https.trustStoreType`|The type of the trust store containing certificates used to validate client certificates. Not needed if `requireClientCertificate` is false.|`java.security.KeyStore.getDefaultType()`|no|\n|`druid.server.https.trustStorePath`|The file path or URL of the trust store containing certificates used to validate client certificates. Not needed if `requireClientCertificate` is false.|none|yes, only if `requireClientCertificate` is true|\n|`druid.server.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate client certificate chains. Not needed if `requireClientCertificate` is false.|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.server.https.trustStorePassword`|The [Password Provider](../operations/password-provider.html) or String password for the Trust Store.  Not needed if `requireClientCertificate` is false.|none|no|\n|`druid.server.https.validateHostnames`|If set to true, check that the client's hostname matches the CN/subjectAltNames in the client certificate.  Not used if `requireClientCertificate` is false.|true|no|\n|`druid.server.https.crlPath`|Specifies a path to a file containing static [Certificate Revocation Lists](https://en.wikipedia.org/wiki/Certificate_revocation_list), used to check if a client certificate has been revoked. Not used if `requireClientCertificate` is false.|null|no|\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Configuration File Structure\nDESCRIPTION: Shows the recommended directory structure for organizing Druid configuration files, including common settings and service-specific configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: DoubleSum Aggregator Configuration in Druid\nDESCRIPTION: Computes sum of values as 64-bit floating point. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple JSON Lookup in Apache Druid\nDESCRIPTION: Example of a namespaceParseSpec configuration for simple JSON lookup in Druid. It only specifies the format as 'simpleJson' for parsing line-delimited JSON data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\":{\n  \"format\": \"simpleJson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Float Last Aggregator in Druid\nDESCRIPTION: Computes metric value with maximum timestamp or 0 if no rows exist. For query time use only.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Hourly Compaction Task Execution\nDESCRIPTION: Command to submit the hourly granularity compaction task to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchMerge Aggregator in Druid\nDESCRIPTION: This JSON configuration defines the HLLSketchMerge aggregator. It merges HLL sketches at query time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchMerge\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Prefix Filtered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering dimension values that start with a specific prefix.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"prefixFiltered\", \"delegate\" : <dimensionSpec>, \"prefix\": <prefix string> }\n```\n\n----------------------------------------\n\nTITLE: Installing Community Extensions using pull-deps\nDESCRIPTION: Command to download and install community or third-party extensions using Druid's pull-deps tool. This example shows installing a hypothetical extension with Maven coordinates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/including-extensions.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava \\\n  -cp \"lib/*\" \\\n  -Ddruid.extensions.directory=\"extensions\" \\\n  -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n  org.apache.druid.cli.Main tools pull-deps \\\n  --no-default-hadoop \\\n  -c \"com.example:druid-example-extension:1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ZooKeeper Path for Process Announcements in Druid\nDESCRIPTION: The ZooKeeper path where Historical and Realtime processes announce their existence by creating ephemeral znodes. This allows other components to discover active processes in the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/zookeeper.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n${druid.zk.paths.announcementsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Joining Servers and Segments Tables in Druid\nDESCRIPTION: SQL query demonstrating how to join the servers and segments tables to count segments for a specific datasource, grouped by server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(segments.segment_id) as num_segments from sys.segments as segments \nINNER JOIN sys.server_segments as server_segments \nON segments.segment_id  = server_segments.segment_id \nINNER JOIN sys.servers as servers \nON servers.server = server_segments.server\nWHERE segments.datasource = 'wikipedia' \nGROUP BY servers.server;\n```\n\n----------------------------------------\n\nTITLE: Listing Segments in Druid Deep Storage Directory\nDESCRIPTION: Command to list all segments stored in Druid's deep storage for the 'deletion-tutorial' datasource, showing the hourly segment files before any deletion has occurred.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -l1 var/druid/segments/deletion-tutorial/\n2015-09-12T00:00:00.000Z_2015-09-12T01:00:00.000Z\n2015-09-12T01:00:00.000Z_2015-09-12T02:00:00.000Z\n2015-09-12T02:00:00.000Z_2015-09-12T03:00:00.000Z\n2015-09-12T03:00:00.000Z_2015-09-12T04:00:00.000Z\n2015-09-12T04:00:00.000Z_2015-09-12T05:00:00.000Z\n2015-09-12T05:00:00.000Z_2015-09-12T06:00:00.000Z\n2015-09-12T06:00:00.000Z_2015-09-12T07:00:00.000Z\n2015-09-12T07:00:00.000Z_2015-09-12T08:00:00.000Z\n2015-09-12T08:00:00.000Z_2015-09-12T09:00:00.000Z\n2015-09-12T09:00:00.000Z_2015-09-12T10:00:00.000Z\n2015-09-12T10:00:00.000Z_2015-09-12T11:00:00.000Z\n2015-09-12T11:00:00.000Z_2015-09-12T12:00:00.000Z\n2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z\n2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z\n2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z\n2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z\n2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z\n2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z\n2015-09-12T18:00:00.000Z_2015-09-12T19:00:00.000Z\n2015-09-12T19:00:00.000Z_2015-09-12T20:00:00.000Z\n2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z\n2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z\n2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z\n2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Jekyll Frontmatter for Druid Documentation\nDESCRIPTION: YAML frontmatter used by Jekyll static site generator to define page metadata for the Druid documentation page, including layout and title attributes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/index.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) Design\"\n---\n```\n\n----------------------------------------\n\nTITLE: Segment Naming Pattern Example for Version 2\nDESCRIPTION: Example showing how reindexed data with a new schema creates segments with a higher version id (v2). These new segments will eventually replace the v1 segments for the same time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-changes.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v2_0\nfoo_2015-01-01/2015-01-02_v2_1\nfoo_2015-01-01/2015-01-02_v2_2\n```\n\n----------------------------------------\n\nTITLE: Bloom Filter Creation in Apache Druid SQL\nDESCRIPTION: Illustrates how to compute a bloom filter from values produced by an expression, specifying the maximum number of distinct values before the false positive rate increases.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nBLOOM_FILTER(expr, numEntries)\n```\n\n----------------------------------------\n\nTITLE: React DOM License Header\nDESCRIPTION: Copyright notice and MIT license declaration for React's react-dom.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query: Retention Analysis with Theta Sketch\nDESCRIPTION: Advanced Druid query using thetaSketch for retention analysis, counting unique users who visited Product A in two different time periods.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_1\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-08T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" : {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_2\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_1\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_2\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\"2014-10-01T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading On-heap Guava Cache in Druid\nDESCRIPTION: Example configuration for a loading lookup using Guava cache implementation with custom eviction settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"guava\"},\n   \"reverseLoadingCacheSpec\":{\"type\":\"guava\", \"maximumSize\":500000, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Extraction Function for String Manipulation\nDESCRIPTION: Uses a JavaScript function to transform dimension values. This example extracts the first three characters of a string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str.substr(0, 3); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: HLLSketchEstimateWithBounds Post-Aggregator in Druid\nDESCRIPTION: This snippet illustrates the configuration for the HLLSketchEstimateWithBounds post-aggregator, which estimates cardinality with error bounds.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchEstimateWithBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>,\n  \"numStdDev\" : <number of standard deviations: 1 (default), 2 or 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents Structure with HTML and Markdown for Apache Druid Documentation\nDESCRIPTION: This code provides a comprehensive navigation structure for Apache Druid's documentation website. It organizes the documentation into logical sections including Getting Started, Data Ingestion, Querying, Design, Operations, Configuration, and Development, with nested links to specific documentation pages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/toc.md#2025-04-09_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n---\nlayout: toc\n---\n\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n\n## Getting Started\n  * [Design](/docs/VERSION/design/index.html)\n    * [What is Druid?](/docs/VERSION/design/index.html#what-is-druid)\n    * [When should I use Druid](/docs/VERSION/design/index.html#when-to-use-druid)\n    * [Architecture](/docs/VERSION/design/index.html#architecture)\n    * [Datasources & Segments](/docs/VERSION/design/index.html#datasources-and-segments)\n    * [Query processing](/docs/VERSION/design/index.html#query-processing)\n    * [External dependencies](/docs/VERSION/design/index.html#external-dependencies)\n    * [Ingestion overview](/docs/VERSION/ingestion/index.html)\n  * [Quickstart](/docs/VERSION/tutorials/index.html)\n    * [Tutorial: Loading a file](/docs/VERSION/tutorials/tutorial-batch.html)\n    * [Tutorial: Loading stream data from Apache Kafka](/docs/VERSION/tutorials/tutorial-kafka.html)\n    * [Tutorial: Loading a file using Apache Hadoop](/docs/VERSION/tutorials/tutorial-batch-hadoop.html)\n    * [Tutorial: Loading stream data using HTTP push](/docs/VERSION/tutorials/tutorial-tranquility.html)\n    * [Tutorial: Querying data](/docs/VERSION/tutorials/tutorial-query.html)\n  * Further tutorials\n    * [Tutorial: Rollup](/docs/VERSION/tutorials/tutorial-rollup.html)\n    * [Tutorial: Configuring retention](/docs/VERSION/tutorials/tutorial-retention.html)\n    * [Tutorial: Updating existing data](/docs/VERSION/tutorials/tutorial-update-data.html)\n    * [Tutorial: Compacting segments](/docs/VERSION/tutorials/tutorial-compaction.html)\n    * [Tutorial: Deleting data](/docs/VERSION/tutorials/tutorial-delete-data.html)\n    * [Tutorial: Writing your own ingestion specs](/docs/VERSION/tutorials/tutorial-ingestion-spec.html)\n    * [Tutorial: Transforming input data](/docs/VERSION/tutorials/tutorial-transform-spec.html)\n  * [Clustering](/docs/VERSION/tutorials/cluster.html)\n\n## Data Ingestion\n  * [Ingestion overview](/docs/VERSION/ingestion/index.html)\n  * [Schema Design](/docs/VERSION/ingestion/schema-design.html)\n  * [Data Formats](/docs/VERSION/ingestion/data-formats.html)\n  * [Tasks Overview](/docs/VERSION/ingestion/tasks.html)\n  * [Ingestion Spec](/docs/VERSION/ingestion/ingestion-spec.html)\n    * [Transform Specs](/docs/VERSION/ingestion/transform-spec.html)\n    * [Firehoses](/docs/VERSION/ingestion/firehose.html)\n  * [Schema Changes](/docs/VERSION/ingestion/schema-changes.html)\n  * [Batch File Ingestion](/docs/VERSION/ingestion/batch-ingestion.html)\n    * [Native Batch Ingestion](/docs/VERSION/ingestion/native_tasks.html)\n    * [Hadoop Batch Ingestion](/docs/VERSION/ingestion/hadoop.html)\n  * [Stream Ingestion](/docs/VERSION/ingestion/stream-ingestion.html)\n    * [Kafka Indexing Service (Stream Pull)](/docs/VERSION/development/extensions-core/kafka-ingestion.html)\n    * [Stream Push](/docs/VERSION/ingestion/stream-push.html)\n  * [Compaction](/docs/VERSION/ingestion/compaction.html)\n  * [Updating Existing Data](/docs/VERSION/ingestion/update-existing-data.html)\n  * [Deleting Data](/docs/VERSION/ingestion/delete-data.html)\n  * [Task Locking & Priority](/docs/VERSION/ingestion/locking-and-priority.html)\n  * [Task Reports](/docs/VERSION/ingestion/reports.html)\n  * [FAQ](/docs/VERSION/ingestion/faq.html)\n  * [Misc. Tasks](/docs/VERSION/ingestion/misc-tasks.html)\n\n## Querying\n  * [Overview](/docs/VERSION/querying/querying.html)\n  * [Timeseries](/docs/VERSION/querying/timeseriesquery.html)\n  * [TopN](/docs/VERSION/querying/topnquery.html)\n  * [GroupBy](/docs/VERSION/querying/groupbyquery.html)\n  * [Time Boundary](/docs/VERSION/querying/timeboundaryquery.html)\n  * [Segment Metadata](/docs/VERSION/querying/segmentmetadataquery.html)\n  * [DataSource Metadata](/docs/VERSION/querying/datasourcemetadataquery.html)\n  * [Search](/docs/VERSION/querying/searchquery.html)\n  * [Select](/docs/VERSION/querying/select-query.html)\n  * [Scan](/docs/VERSION/querying/scan-query.html)\n  * Components\n    * [Datasources](/docs/VERSION/querying/datasource.html)\n    * [Filters](/docs/VERSION/querying/filters.html)\n    * [Aggregations](/docs/VERSION/querying/aggregations.html)\n    * [Post Aggregations](/docs/VERSION/querying/post-aggregations.html)\n    * [Granularities](/docs/VERSION/querying/granularities.html)\n    * [DimensionSpecs](/docs/VERSION/querying/dimensionspecs.html)\n    * [Context](/docs/VERSION/querying/query-context.html)\n  * [Multi-value dimensions](/docs/VERSION/querying/multi-value-dimensions.html)\n  * [SQL](/docs/VERSION/querying/sql.html)\n  * [Lookups](/docs/VERSION/querying/lookups.html)\n  * [Joins](/docs/VERSION/querying/joins.html)\n  * [Multitenancy](/docs/VERSION/querying/multitenancy.html)\n  * [Caching](/docs/VERSION/querying/caching.html)\n  * [Sorting Orders](/docs/VERSION/querying/sorting-orders.html)\n  * [Virtual Columns](/docs/VERSION/querying/virtual-columns.html)\n\n## Design\n  * [Overview](/docs/VERSION/design/index.html)\n  * Storage\n    * [Segments](/docs/VERSION/design/segments.html)\n  * [Processes and Servers](/docs/VERSION/design/processes.html)\n    * [Coordinator](/docs/VERSION/design/coordinator.html)\n    * [Overlord](/docs/VERSION/design/overlord.html)\n    * [Broker](/docs/VERSION/design/broker.html)\n    * [Historical](/docs/VERSION/design/historical.html)\n    * [MiddleManager](/docs/VERSION/design/middlemanager.html)\n      * [Peons](/docs/VERSION/design/peons.html)\n    * [Realtime (Deprecated)](/docs/VERSION/design/realtime.html)\n  * Dependencies\n    * [Deep Storage](/docs/VERSION/dependencies/deep-storage.html)\n    * [Metadata Storage](/docs/VERSION/dependencies/metadata-storage.html)\n    * [ZooKeeper](/docs/VERSION/dependencies/zookeeper.html)\n\n## Operations\n  * [API Reference](/docs/VERSION/operations/api-reference.html)\n    * [Coordinator](/docs/VERSION/operations/api-reference.html#coordinator)\n    * [Overlord](/docs/VERSION/operations/api-reference.html#overlord)\n    * [MiddleManager](/docs/VERSION/operations/api-reference.html#middlemanager)\n    * [Peon](/docs/VERSION/operations/api-reference.html#peon)\n    * [Broker](/docs/VERSION/operations/api-reference.html#broker)\n    * [Historical](/docs/VERSION/operations/api-reference.html#historical)\n  * [Including Extensions](/docs/VERSION/operations/including-extensions.html)\n  * [Data Retention](/docs/VERSION/operations/rule-configuration.html)\n  * [Metrics and Monitoring](/docs/VERSION/operations/metrics.html)\n  * [Alerts](/docs/VERSION/operations/alerts.html)\n  * [Updating the Cluster](/docs/VERSION/operations/rolling-updates.html)\n  * [Different Hadoop Versions](/docs/VERSION/operations/other-hadoop.html)\n  * [Performance FAQ](/docs/VERSION/operations/performance-faq.html)\n  * [Management UIs](/docs/VERSION/operations/management-uis.html)\n  * [Dump Segment Tool](/docs/VERSION/operations/dump-segment.html)\n  * [Insert Segment Tool](/docs/VERSION/operations/insert-segment-to-db.html)\n  * [Pull Dependencies Tool](/docs/VERSION/operations/pull-deps.html)\n  * [Recommendations](/docs/VERSION/operations/recommendations.html)\n  * [TLS Support](/docs/VERSION/operations/tls-support.html)\n  * [Password Provider](/docs/VERSION/operations/password-provider.html)\n\n## Configuration\n  * [Configuration Reference](/docs/VERSION/configuration/index.html)\n  * [Recommended Configuration File Organization](/docs/VERSION/configuration/index.html#recommended-configuration-file-organization)\n  * [JVM Configuration Best Practices](/docs/VERSION/configuration/index.html#jvm-configuration-best-practices)\n  * [Common Configuration](/docs/VERSION/configuration/index.html#common-configurations)\n  * [Coordinator](/docs/VERSION/configuration/index.html#coordinator)\n  * [Overlord](/docs/VERSION/configuration/index.html#overlord)\n  * [MiddleManager & Peons](/docs/VERSION/configuration/index.html#middle-manager-and-peons)\n  * [Broker](/docs/VERSION/configuration/index.html#broker)\n  * [Historical](/docs/VERSION/configuration/index.html#historical)\n  * [Caching](/docs/VERSION/configuration/index.html#cache-configuration)\n  * [General Query Configuration](/docs/VERSION/configuration/index.html#general-query-configuration)\n  * [Configuring Logging](/docs/VERSION/configuration/logging.html)\n  \n## Development\n  * [Overview](/docs/VERSION/development/overview.html)\n  * [Libraries](/docs/VERSION/development/libraries.html)\n  * [Extensions](/docs/VERSION/development/extensions.html)\n  * [JavaScript](/docs/VERSION/development/javascript.html)\n  * [Build From Source](/docs/VERSION/development/build.html)\n  * [Versioning](/docs/VERSION/development/versioning.html)\n  * [Integration](/docs/VERSION/development/integrating-druid-with-other-technologies.html)\n  * Experimental Features\n    * [Overview](/docs/VERSION/development/experimental.html)\n    * [Approximate Histograms and Quantiles](/docs/VERSION/development/extensions-core/approximate-histograms.html)\n    * [Datasketches](/docs/VERSION/development/extensions-core/datasketches-extension.html)\n    * [Geographic Queries](/docs/VERSION/development/geo.html)\n    * [Router](/docs/VERSION/development/router.html)\n    * [Kafka Indexing Service](/docs/VERSION/development/extensions-core/kafka-ingestion.html)\n```\n\n----------------------------------------\n\nTITLE: Segment Naming Pattern Example for Version 2\nDESCRIPTION: Example showing how reindexed data with a new schema creates segments with a higher version id (v2). These new segments will eventually replace the v1 segments for the same time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-changes.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v2_0\nfoo_2015-01-01/2015-01-02_v2_1\nfoo_2015-01-01/2015-01-02_v2_2\n```\n\n----------------------------------------\n\nTITLE: Configuring CustomJSON Lookup Parse Specification in Druid\nDESCRIPTION: Defines a namespaceParseSpec for custom JSON format lookups, specifying key field name and value field name for transforming JSON data into key-value lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"key\": \"foo\", \"value\": \"bar\", \"somethingElse\" : \"something\"}\n{\"key\": \"baz\", \"value\": \"bat\", \"somethingElse\" : \"something\"}\n{\"key\": \"buck\", \"somethingElse\": \"something\", \"value\": \"truck\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"customJson\",\n  \"keyFieldName\": \"key\",\n  \"valueFieldName\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Lookup Extraction Function with Map in Druid\nDESCRIPTION: An example of an inline lookup extraction function that specifies a mapping directly in the query rather than referencing a registered lookup. This example retains missing values and marks the lookup as injective.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":true,\n  \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose in Druid for Remote File Ingestion\nDESCRIPTION: Configuration for the HttpFirehose which fetches data from remote URLs. This firehose is splittable and allows each worker task in a parallel index task to read a separate file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/firehose.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"http\",\n    \"uris\"  : [\"http://example.com/uri1\", \"http://example2.com/uri2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Retained Entries Count Post-Aggregator\nDESCRIPTION: Post-aggregator that returns the number of entries retained in an ArrayOfDoublesSketch, which can be useful for understanding sketch size and memory usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToNumEntries\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Live Row Stats JSON in Apache Druid\nDESCRIPTION: Example JSON output of live row statistics, showing moving averages and totals for processed rows, unparseable events, and other metrics during task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"movingAverages\": {\n    \"buildSegments\": {\n      \"5m\": {\n        \"processed\": 3.392158326408501,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"15m\": {\n        \"processed\": 1.736165476881023,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"1m\": {\n        \"processed\": 4.206417693750045,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      }\n    }\n  },\n  \"totals\": {\n    \"buildSegments\": {\n      \"processed\": 1994,\n      \"processedWithError\": 0,\n      \"thrownAway\": 0,\n      \"unparseable\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Student's t-test Post-Aggregator\nDESCRIPTION: Post-aggregator that performs Student's t-test between two ArrayOfDoublesSketch instances, returning a list of p-values. Useful for statistical comparison of sketch distributions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchTTest\",\n  \"name\": <output name>,\n  \"fields\"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>,\n}\n```\n\n----------------------------------------\n\nTITLE: Regex Format Parse Specification\nDESCRIPTION: Configuration template for parsing data using regular expressions, including pattern matching and column specifications\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"regex\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [<your_list_of_dimensions>]\n    },\n    \"columns\" : [<your_columns_here>],\n    \"pattern\" : <regex pattern for partitioning data>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Suspending a Supervisor in Druid\nDESCRIPTION: Use this POST endpoint to suspend a running Kafka supervisor. The supervisor will continue operating but will ensure no indexing tasks are running until resumed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/suspend\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Stream Parser with Schema Repo\nDESCRIPTION: Example configuration for setting up an Avro stream parser using schema repository for data ingestion. Includes schema repo decoder configuration with subject and ID converter settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parser\" : {\n    \"type\" : \"avro_stream\",\n    \"avroBytesDecoder\" : {\n      \"type\" : \"schema_repo\",\n      \"subjectAndIdConverter\" : {\n        \"type\" : \"avro_1124\",\n        \"topic\" : \"${YOUR_TOPIC}\"\n      },\n      \"schemaRepository\" : {\n        \"type\" : \"avro_1124_rest_client\",\n        \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\"\n      }\n    },\n    \"parseSpec\" : {\n      \"format\": \"avro\",\n      \"timestampSpec\": <standard timestampSpec>,\n      \"dimensionsSpec\": <standard dimensionsSpec>,\n      \"flattenSpec\": <optional>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Overlord Dynamic Configuration in JSON\nDESCRIPTION: This JSON object can be submitted to the Overlord via a POST request to configure worker behavior dynamically. It includes settings for worker selection strategy and autoscaling configuration for EC2.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"selectStrategy\": {\n    \"type\": \"fillCapacity\",\n    \"affinityConfig\": {\n      \"affinity\": {\n        \"datasource1\": [\"host1:port\", \"host2:port\"],\n        \"datasource2\": [\"host3:port\"]\n      }\n    }\n  },\n  \"autoScaler\": {\n    \"type\": \"ec2\",\n    \"minNumWorkers\": 2,\n    \"maxNumWorkers\": 12,\n    \"envConfig\": {\n      \"availabilityZone\": \"us-east-1a\",\n      \"nodeData\": {\n        \"amiId\": \"${AMI}\",\n        \"instanceType\": \"c3.8xlarge\",\n        \"minInstances\": 1,\n        \"maxInstances\": 1,\n        \"securityGroupIds\": [\"${IDs}\"],\n        \"keyName\": \"${KEY_NAME}\"\n      },\n      \"userData\": {\n        \"impl\": \"string\",\n        \"data\": \"${SCRIPT_COMMAND}\",\n        \"versionReplacementString\": \":VERSION:\",\n        \"version\": null\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bound Filter on Numeric Column in Druid (JSON)\nDESCRIPTION: This example demonstrates how to configure a bound filter on a numeric column in a Druid query. It filters for float values between 10 and 20.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"bound\",\n  \"dimension\": \"myFloatColumn\",\n  \"ordering\": \"numeric\",\n  \"lower\": \"10\",\n  \"lowerStrict\": false,\n  \"upper\": \"20\",\n  \"upperStrict\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Audit History API Endpoint with Count Limit\nDESCRIPTION: HTTP GET endpoint for retrieving a specific number of most recent Coordinator configuration audit entries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_28\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia edits data for deletion tutorial in Druid\nDESCRIPTION: Command to post a pre-configured index task from a JSON specification file that creates hourly segments in a datasource named 'deletion-tutorial'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/deletion-index.json \n```\n\n----------------------------------------\n\nTITLE: User Retention Analysis Query\nDESCRIPTION: Complex query example showing how to analyze user retention across different time periods using Theta sketches and set operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" : {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_1\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-08T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" : {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_2\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\": {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_1\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_2\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\"2014-10-01T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Retained Entries Count Post-Aggregator\nDESCRIPTION: Post-aggregator that returns the number of entries retained in an ArrayOfDoublesSketch, which can be useful for understanding sketch size and memory usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToNumEntries\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Timestamp Specification in Druid Parser\nDESCRIPTION: Adding a timestampSpec to the parser to extract the main timestamp field 'ts' in ISO 8601 format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Combining and Overwriting Data in Apache Druid\nDESCRIPTION: This command submits a task that combines existing data with new data and overwrites the original datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalFirehose in Apache Druid\nDESCRIPTION: This snippet shows how to configure a LocalFirehose for reading data from files on local disk. It specifies the type, file filter, and base directory for ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"local\",\n    \"filter\"   : \"*.csv\",\n    \"baseDir\"  : \"/data/directory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Shade Plugin for Jackson Package Relocation\nDESCRIPTION: This XML configuration for the Maven Shade plugin demonstrates how to relocate Jackson packages and create a fat jar. This approach helps resolve conflicts by using a renamed version of Jackson within Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-shade-plugin</artifactId>\n     <executions>\n         <execution>\n             <phase>package</phase>\n             <goals>\n                 <goal>shade</goal>\n             </goals>\n             <configuration>\n                 <outputFile>\n                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                 </outputFile>\n                 <relocations>\n                     <relocation>\n                         <pattern>com.fasterxml.jackson</pattern>\n                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                     </relocation>\n                 </relocations>\n                 <artifactSet>\n                     <includes>\n                         <include>*:*</include>\n                     </includes>\n                 </artifactSet>\n                 <filters>\n                     <filter>\n                         <artifact>*:*</artifact>\n                         <excludes>\n                             <exclude>META-INF/*.SF</exclude>\n                             <exclude>META-INF/*.DSA</exclude>\n                             <exclude>META-INF/*.RSA</exclude>\n                         </excludes>\n                     </filter>\n                 </filters>\n                 <transformers>\n                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                 </transformers>\n             </configuration>\n         </execution>\n     </executions>\n </plugin>\n```\n\n----------------------------------------\n\nTITLE: Fetching Druid Worker Configuration History via HTTP GET\nDESCRIPTION: HTTP endpoint for retrieving the audit history of worker configuration changes for a specified time interval. This allows administrators to view historical configuration changes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_22\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker/history?interval=<interval>\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data\nDESCRIPTION: Commands to extract the sample Wikipedia data file for ingestion into Kafka.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial\ngunzip -k wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Shade Plugin for Jackson Package Relocation\nDESCRIPTION: This XML configuration for the Maven Shade plugin demonstrates how to relocate Jackson packages and create a fat jar. This approach helps resolve conflicts by using a renamed version of Jackson within Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-shade-plugin</artifactId>\n     <executions>\n         <execution>\n             <phase>package</phase>\n             <goals>\n                 <goal>shade</goal>\n             </goals>\n             <configuration>\n                 <outputFile>\n                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                 </outputFile>\n                 <relocations>\n                     <relocation>\n                         <pattern>com.fasterxml.jackson</pattern>\n                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                     </relocation>\n                 </relocations>\n                 <artifactSet>\n                     <includes>\n                         <include>*:*</include>\n                     </includes>\n                 </artifactSet>\n                 <filters>\n                     <filter>\n                         <artifact>*:*</artifact>\n                         <excludes>\n                             <exclude>META-INF/*.SF</exclude>\n                             <exclude>META-INF/*.DSA</exclude>\n                             <exclude>META-INF/*.RSA</exclude>\n                         </excludes>\n                     </filter>\n                 </filters>\n                 <transformers>\n                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                 </transformers>\n             </configuration>\n         </execution>\n     </executions>\n </plugin>\n```\n\n----------------------------------------\n\nTITLE: React DOM MIT License Declaration\nDESCRIPTION: License declaration for react-dom.production.min.js by Facebook, version 17.0.2, under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query Using Bloom Filter Aggregator\nDESCRIPTION: Complete JSON query example demonstrating the use of a bloom filter aggregator in a Druid timeseries query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"wikiticker\",\n  \"intervals\": [ \"2015-09-12T00:00:00.000/2015-09-13T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"bloom\",\n      \"name\": \"userBloom\",\n      \"maxNumEntries\": 100000,\n      \"field\": {\n        \"type\":\"default\",\n        \"dimension\":\"user\",\n        \"outputType\": \"STRING\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Stream Parser with Schema Repo\nDESCRIPTION: Example configuration for setting up an Avro stream parser using schema repository for data ingestion. Includes schema repo decoder configuration with subject and ID converter settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parser\" : {\n    \"type\" : \"avro_stream\",\n    \"avroBytesDecoder\" : {\n      \"type\" : \"schema_repo\",\n      \"subjectAndIdConverter\" : {\n        \"type\" : \"avro_1124\",\n        \"topic\" : \"${YOUR_TOPIC}\"\n      },\n      \"schemaRepository\" : {\n        \"type\" : \"avro_1124_rest_client\",\n        \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\"\n      }\n    },\n    \"parseSpec\" : {\n      \"format\": \"avro\",\n      \"timestampSpec\": <standard timestampSpec>,\n      \"dimensionsSpec\": <standard dimensionsSpec>,\n      \"flattenSpec\": <optional>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing doubleMin Aggregator in Druid\nDESCRIPTION: The doubleMin aggregator computes the minimum of all metric values and Double.POSITIVE_INFINITY. It requires name for the output and fieldName to specify which metric column to evaluate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Extraction Function with Injective Property\nDESCRIPTION: Uses a JavaScript function to append '!!!' to the input string, with the injective property set to true to preserve uniqueness.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str + '!!!'; }\",\n  \"injective\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Expression Virtual Column in Apache Druid\nDESCRIPTION: This snippet outlines the syntax for defining an expression virtual column in Apache Druid. It specifies the required and optional properties for creating a virtual column based on an expression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/virtual-columns.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"expression\",\n  \"name\": <name of the virtual column>,\n  \"expression\": <row expression>,\n  \"outputType\": <output value type of expression>\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Segments by Interval in Druid\nDESCRIPTION: cURL command to mark segments as unused for a specific time interval using the Coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d '{ \"interval\" : \"2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z\" }' http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchEstimateWithBounds Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the HLLSketchEstimateWithBounds post-aggregator. This post-aggregator estimates the distinct count with error bounds.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchEstimateWithBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>,\n  \"numStdDev\" : <number of standard deviations: 1 (default), 2 or 3>\n}\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Tool Help Output in Apache Druid\nDESCRIPTION: Sample output of the help command for the ResetCluster tool, showing the tool's name, synopsis, and available options with descriptions for each flag.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nNAME\n        druid tools reset-cluster - Cleanup all persisted state from metadata\n        and deep storage.\n\nSYNOPSIS\n        druid tools reset-cluster [--all] [--hadoopWorkingPath]\n                [--metadataStore] [--segmentFiles] [--taskLogs]\n\nOPTIONS\n        --all\n            delete all state stored in metadata and deep storage\n\n        --hadoopWorkingPath\n            delete hadoopWorkingPath\n\n        --metadataStore\n            delete all records in metadata storage\n\n        --segmentFiles\n            delete all segment files from deep storage\n\n        --taskLogs\n            delete all tasklogs\n```\n\n----------------------------------------\n\nTITLE: Sample Wikipedia Page Edit Event in JSON\nDESCRIPTION: Example of a JSON object representing a Wikipedia page edit event, used in the tutorial dataset for data loading examples.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/index.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bucket Extraction Function in Druid\nDESCRIPTION: Configuration for bucketing numerical values into fixed-size ranges with custom offset settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bucket\",\n  \"size\" : 5,\n  \"offset\" : 2\n}\n```\n\n----------------------------------------\n\nTITLE: Running DumpSegment Tool in Java\nDESCRIPTION: Command to execute the DumpSegment tool using Java to analyze a Druid segment and output the results to a file. Requires Druid libraries in the classpath.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/dump-segment.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools dump-segment \\\n  --directory /home/druid/path/to/segment/ \\\n  --out /home/druid/output.txt\n```\n\n----------------------------------------\n\nTITLE: Native JSON TopN Query for Wikipedia Page Edits\nDESCRIPTION: A native Druid JSON query that retrieves the top 10 Wikipedia pages with the most edits on a specific date. Uses TopN query type with count aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Native JSON TopN Query for Wikipedia Page Edits\nDESCRIPTION: A native Druid JSON query that retrieves the top 10 Wikipedia pages with the most edits on a specific date. Uses TopN query type with count aggregation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\" : \"topN\",\n  \"dataSource\" : \"wikipedia\",\n  \"intervals\" : [\"2015-09-12/2015-09-13\"],\n  \"granularity\" : \"all\",\n  \"dimension\" : \"page\",\n  \"metric\" : \"count\",\n  \"threshold\" : 10,\n  \"aggregations\" : [\n    {\n      \"type\" : \"count\",\n      \"name\" : \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metrics for OpenTSDB Emitter in JSON\nDESCRIPTION: A JSON configuration example showing how to map Druid metrics to OpenTSDB with specified dimensions. The format follows the schema '<druid metric name>: [<dimension list>]', which defines which dimensions should be included with each metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/opentsdb-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"query/time\": [\n    \"dataSource\",\n    \"type\"\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Tables for Druid Deep Storage\nDESCRIPTION: SQL statements to create the 'index_storage' and 'descriptor_storage' tables in Cassandra. These tables are used to store segment data and metadata for Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for Apache Druid Documentation\nDESCRIPTION: This YAML snippet defines the frontmatter for the Druid documentation page, specifying the layout and title of the document.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/overlord.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Overlord Process\"\n---\n```\n\n----------------------------------------\n\nTITLE: Using Comparison Operators in Druid SQL\nDESCRIPTION: Shows various comparison operators available in Druid SQL, including equality, inequality, and range checks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nx = y\nx <> y\nx > y\nx >= y\nx < y\nx <= y\nx BETWEEN y AND z\nx NOT BETWEEN y AND z\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Blacklisting Properties in Apache Druid\nDESCRIPTION: These configuration properties control the blacklisting behavior for Druid MiddleManagers. They define thresholds for when workers are blacklisted, how long they remain blacklisted, cleanup periods, and the maximum percentage of workers that can be blacklisted at once.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/overlord.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.runner.maxRetriesBeforeBlacklist\ndruid.indexer.runner.workerBlackListBackoffTime\ndruid.indexer.runner.workerBlackListCleanupPeriod\ndruid.indexer.runner.maxPercentageBlacklistWorkers\n```\n\n----------------------------------------\n\nTITLE: Maven Shade Plugin Configuration\nDESCRIPTION: Maven shade plugin configuration for creating a fat jar with relocated Jackson packages\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-shade-plugin</artifactId>\n     <executions>\n         <execution>\n             <phase>package</phase>\n             <goals>\n                 <goal>shade</goal>\n             </goals>\n             <configuration>\n                 <outputFile>\n                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                 </outputFile>\n                 <relocations>\n                     <relocation>\n                         <pattern>com.fasterxml.jackson</pattern>\n                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                     </relocation>\n                 </relocations>\n                 <artifactSet>\n                     <includes>\n                         <include>*:*</include>\n                     </includes>\n                 </artifactSet>\n                 <filters>\n                     <filter>\n                         <artifact>*:*</artifact>\n                         <excludes>\n                             <exclude>META-INF/*.SF</exclude>\n                             <exclude>META-INF/*.DSA</exclude>\n                             <exclude>META-INF/*.RSA</exclude>\n                         </excludes>\n                     </filter>\n                 </filters>\n                 <transformers>\n                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                 </transformers>\n             </configuration>\n         </execution>\n     </executions>\n </plugin>\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authorizer in Druid\nDESCRIPTION: Configuration example showing how to enable the basic authorizer implementation from the druid-basic-security extension. This determines which authorizer will make authorization decisions for authenticated requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/auth.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authorizers=[\"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Using Extraction Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A deprecated filter that matches dimensions using extraction functions. This example uses a lookup extraction to match specific product values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Lookup with Missing Value Replacement\nDESCRIPTION: This example configures an inline lookup extraction function that replaces missing values with a default string 'MISSING'. It doesn't retain missing values and specifies that the lookup is not injective.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":false,\n  \"injective\":false,\n  \"replaceMissingValueWith\":\"MISSING\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON response from Tranquility Server in Apache Druid\nDESCRIPTION: This JSON snippet shows the expected response from the Tranquility Server after successfully sending data. It indicates the number of events received and sent to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":39244}}\n```\n\n----------------------------------------\n\nTITLE: Running DumpSegment Tool in Java\nDESCRIPTION: Command to execute the DumpSegment tool using Java classpath to analyze a Druid segment and output the results to a file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/dump-segment.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools dump-segment \\\n  --directory /home/druid/path/to/segment/ \\\n  --out /home/druid/output.txt\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Directory Prefix and File Pattern in Druid\nDESCRIPTION: Configuration for a URI-based lookup that uses a directory prefix and file pattern. This approach allows for selecting files from a directory based on a regex pattern, useful for file rotation scenarios.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uriPrefix\": \"s3://bucket/some/key/prefix/\",\n  \"fileRegex\":\"renames-[0-9]*\\\\.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring String Format Extraction Function in Druid\nDESCRIPTION: Shows how to format dimension values according to a given sprintf expression. Includes options for handling null values through the nullHandling attribute.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"stringFormat\", \"format\" : <sprintf_expression>, \"nullHandling\" : <optional attribute for handling null value> }\n```\n\n----------------------------------------\n\nTITLE: Example Error Message for Validator Class Incompatibility with Hadoop\nDESCRIPTION: An example error message that occurs when there's a version incompatibility between Druid and Hadoop related to the Validator class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nError: java.lang.ClassNotFoundException: javax.validation.Validator\n```\n\n----------------------------------------\n\nTITLE: Submitting Hourly Compaction Task\nDESCRIPTION: Command to submit the compaction task that maintains hourly granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Setting up Double Max Aggregator in Druid\nDESCRIPTION: Computes maximum of metric values and Double.NEGATIVE_INFINITY. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring uniform granularity specifications in Apache Druid\nDESCRIPTION: The uniform granularity spec is used to generate segments with uniform time intervals. It includes configuration for segment granularity, query granularity, rollup behavior, and data intervals. This determines how Druid partitions data into time chunks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"segmentGranularity\": \"DAY\",\n  \"queryGranularity\": \"NONE\",\n  \"rollup\": true,\n  \"intervals\": [\"interval1\", \"interval2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: SegmentWriteOutMediumFactory Configuration\nDESCRIPTION: JSON configuration object for specifying the segment write-out medium factory type used when creating segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"string_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticCloudFilesFirehose in Apache Druid\nDESCRIPTION: JSON configuration for setting up a StaticCloudFilesFirehose in Apache Druid to ingest data from Rackspace Cloud Files. This firehose supports reading from multiple blobs across different regions and containers, with caching and prefetching capabilities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/cloudfiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-cloudfiles\",\n    \"blobs\": [\n        {\n          \"region\": \"DFW\"\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"region\": \"ORD\"\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Extraction Filter Example in Druid\nDESCRIPTION: Example of extraction filter using lookup function to match specific product values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/filters.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with AND Logical Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Demonstrates how to use an 'AND' logical filter in a HavingSpec for a groupBy query. This filter combines multiple conditions that all must be true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"and\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"lessThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Caching Options for Apache Druid Realtime Node\nDESCRIPTION: YAML configuration for enabling and configuring caching options in a Druid realtime node, including cache usage, population, and size limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/realtime.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.realtime.cache.useCache: false\ndruid.realtime.cache.populateCache: false\ndruid.realtime.cache.unCacheable: [\"select\"]\ndruid.realtime.cache.maxEntrySize: 1000000\n```\n\n----------------------------------------\n\nTITLE: Listing Druid Segments Directory Contents\nDESCRIPTION: Command to display the contents of the Druid segments directory, showing all segment files for the deletion-tutorial datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nls -l1 var/druid/segments/deletion-tutorial/\n```\n\n----------------------------------------\n\nTITLE: Verifying segment deletion from deep storage\nDESCRIPTION: Command to list segments after running the Kill Task, showing that disabled segments have been permanently removed from deep storage. The remaining segments are those that haven't been marked as unused.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -l1 var/druid/segments/deletion-tutorial/\n2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z\n2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z\n2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z\n2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z\n2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z\n2015-09-12T18:00:00.000Z_2015-09-12T19:00:00.000Z\n2015-09-12T19:00:00.000Z_2015-09-12T20:00:00.000Z\n2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z\n2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z\n2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z\n2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Operation Settings for Apache Druid\nDESCRIPTION: YAML configuration for specifying realtime operation parameters, including where to publish segments and the location of the realtime spec file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/realtime.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.publish.type: metadata\ndruid.realtime.specFile: none\n```\n\n----------------------------------------\n\nTITLE: Representing Multi-Value Columns in Druid Segments\nDESCRIPTION: Illustrates how Druid modifies the segment data structures to handle multi-value columns, showing the dictionary, column data with array values, and modified bitmaps that have multiple non-zero entries for rows with multiple values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/segments.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   [0,1],  <--Row value of multi-value column can have array of values\n   1,\n   1]\n\n3: Bitmaps - one for each unique value\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,1,1,1]\n                            ^\n                            |\n                            |\n    Multi-value column has multiple non-zero entries\n```\n```\n\n----------------------------------------\n\nTITLE: Testing Kafka Rename Functionality Using Console Producer\nDESCRIPTION: Bash command for testing the Kafka rename functionality by sending key/value pairs to a Kafka stream. This console producer allows publishing renames in the format OLD_VAL->NEW_VAL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-console-producer.sh --property parse.key=true --property key.separator=\"->\" --broker-list localhost:9092 --topic testTopic\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Distinct Count Estimate Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for extracting a distinct count estimate from an ArrayOfDoublesSketch, returning a single value representing the estimated number of distinct keys.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting ZooKeeper\nDESCRIPTION: Commands to download, extract, configure and start ZooKeeper for Druid coordination\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\ncd zookeeper-3.4.11\ncp conf/zoo_sample.cfg conf/zoo.cfg\n./bin/zkServer.sh start\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Kafka\nDESCRIPTION: Commands to download Kafka 0.10.2.2 and extract it from the archive.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz\ntar -xzf kafka_2.12-0.10.2.2.tgz\ncd kafka_2.12-0.10.2.2\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Multiple Hadoop Client Versions\nDESCRIPTION: Example directory structure showing how to organize multiple versions of Hadoop client libraries under the hadoop-dependencies directory for use with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-dependencies/\n hadoop-client\n     2.3.0\n        activation-1.1.jar\n        avro-1.7.4.jar\n        commons-beanutils-1.7.0.jar\n        commons-beanutils-core-1.8.0.jar\n        commons-cli-1.2.jar\n        commons-codec-1.4.jar\n    ..... lots of jars\n     2.4.0\n         activation-1.1.jar\n         avro-1.7.4.jar\n         commons-beanutils-1.7.0.jar\n         commons-beanutils-core-1.8.0.jar\n         commons-cli-1.2.jar\n         commons-codec-1.4.jar\n    ..... lots of jars\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: License notice for the React core production build, version 17.0.2, released under the MIT license by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading On-heap Guava Cache in Druid\nDESCRIPTION: JSON configuration example for a loading-based cache using Guava implementation with customizable eviction policies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"guava\"},\n   \"reverseLoadingCacheSpec\":{\"type\":\"guava\", \"maximumSize\":500000, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Line Protocol Parser in Apache Druid\nDESCRIPTION: This JSON snippet shows how to configure the InfluxDB Line Protocol parser in a Druid ingestion spec. It includes settings for the parser type, timestamp specification, dimension exclusions, and measurement whitelist.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parser\": {\n      \"type\": \"string\",\n      \"parseSpec\": {\n        \"format\": \"influx\",\n        \"timestampSpec\": {\n          \"column\": \"__ts\",\n          \"format\": \"millis\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensionExclusions\": [\n            \"__ts\"\n          ]\n        },\n        \"whitelistMeasurements\": [\n          \"cpu\"\n        ]\n      }\n```\n\n----------------------------------------\n\nTITLE: Defining IndexSpec Configuration in Apache Druid\nDESCRIPTION: This snippet shows the structure of the IndexSpec configuration, which defines segment storage format options for indexing in Apache Druid. It includes fields for bitmap, dimension compression, metric compression, and long encoding.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|bitmap|Object|Compression format for bitmap indexes. Should be a JSON object; see below for options.|no (defaults to Concise)|\n|dimensionCompression|String|Compression format for dimension columns. Choose from `LZ4`, `LZF`, or `uncompressed`.|no (default == `LZ4`)|\n|metricCompression|String|Compression format for metric columns. Choose from `LZ4`, `LZF`, `uncompressed`, or `none`.|no (default == `LZ4`)|\n|longEncoding|String|Encoding format for metric and dimension columns with type long. Choose from `auto` or `longs`. `auto` encodes the values using offset or lookup table depending on column cardinality, and store them with variable size. `longs` stores the value as is with 8 bytes each.|no (default == `longs`)|\n```\n\n----------------------------------------\n\nTITLE: TSV ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing TSV (tab-separated values) data in Druid, including delimiter and column specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\": {\n    \"format\" : \"tsv\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"],\n    \"delimiter\":\"|\",\n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Druid Ingestion Task via Command Line\nDESCRIPTION: Shell command to submit the roll-up ingestion task to Druid's indexing service. The command uses the post-index-task script to submit the task specification file to the Druid Overlord at the specified URL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/rollup-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Running Hadoop Indexer with Custom Fat Jar in Bash\nDESCRIPTION: This Bash command demonstrates how to run the Hadoop indexer using the custom fat jar created to resolve Jackson conflicts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n```\n\n----------------------------------------\n\nTITLE: Example JSON Message Format\nDESCRIPTION: Sample JSON message structure showing the metrics data format before Protobuf conversion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"unit\": \"milliseconds\",\n  \"http_method\": \"GET\",\n  \"value\": 44,\n  \"timestamp\": \"2017-04-06T02:36:22Z\",\n  \"http_code\": \"200\",\n  \"page\": \"/\",\n  \"metricType\": \"request/latency\",\n  \"server\": \"www1.example.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Double First Aggregator in Druid\nDESCRIPTION: Computes metric value with minimum timestamp or 0 if no rows exist. For query time use only.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Granularity Data Example - Input Data\nDESCRIPTION: Example input data showing timestamps with millisecond granularity stored in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"AAA\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-01T01:02:33Z\", \"page\": \"BBB\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"}\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for the documentation page, specifying the layout and title of the document.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/getting-started.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Getting Started with Apache Druid (incubating)\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticCloudFilesFirehose in Apache Druid\nDESCRIPTION: Sample configuration for setting up a static Cloud Files firehose in Druid. This configuration allows ingesting data from multiple Cloud Files blobs across different regions and containers. The firehose supports caching and prefetching features for optimized performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/cloudfiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-cloudfiles\",\n    \"blobs\": [\n        {\n          \"region\": \"DFW\"\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"region\": \"ORD\"\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Druid Distribution to Master Server using rsync\nDESCRIPTION: Uses rsync to copy the Druid distribution and edited configurations from a local machine to the Master server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrsync -az apache-druid-0.14.1-incubating/ COORDINATION_SERVER:apache-druid-0.14.1-incubating/\n```\n\n----------------------------------------\n\nTITLE: Submitting Overlord Worker Configuration via HTTP POST\nDESCRIPTION: HTTP endpoint for submitting worker configuration to the Druid Overlord. This endpoint accepts POST requests with JSON worker configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_20\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker\n```\n\n----------------------------------------\n\nTITLE: Sending Sample Wikipedia Data to Tranquility Server\nDESCRIPTION: Commands to extract the compressed sample data file and then send it to the Tranquility Server via an HTTP POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz \ncurl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia\n```\n\n----------------------------------------\n\nTITLE: Segment Serving Path Configuration\nDESCRIPTION: Permanent ZooKeeper path where processes create znodes to indicate which segments they are serving.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: HyperUnique Cardinality Usage Example in Druid\nDESCRIPTION: Example showing how to use hyperUniqueCardinality in a calculation. This example computes the average unique users per row by dividing the hyperUnique cardinality of unique_users by the count of rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n  \"aggregations\" : [{\n    {\"type\" : \"count\", \"name\" : \"rows\"},\n    {\"type\" : \"hyperUnique\", \"name\" : \"unique_users\", \"fieldName\" : \"uniques\"}\n  }],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average_users_per_row\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n      { \"type\" : \"hyperUniqueCardinality\", \"fieldName\" : \"unique_users\" },\n      { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n    ]\n  }]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Period Broadcast Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period broadcast rule that specifies how segments of different data sources should be co-located in Historical processes for a rolling time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByPeriod\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to Multiple Quantiles Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for computing multiple quantiles from a DoublesSketch using an array of fractions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantiles\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fractions\" : <array of fractional positions in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Query Example: Unique Users for Product A\nDESCRIPTION: GroupBy query example showing how to count unique users who visited product A. This demonstrates the basic use of the Theta sketch aggregator in a query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"aggregations\": [\n    { \"type\": \"thetaSketch\", \"name\": \"unique_users\", \"fieldName\": \"user_id_sketch\" }\n  ],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\" },\n  \"intervals\": [ \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authorizer in Apache Druid\nDESCRIPTION: Configuration for setting up a Basic authorizer in Druid, which implements role-based access control for authorization decisions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authorizers=[\"MyBasicAuthorizer\"]\n\ndruid.auth.authorizer.MyBasicAuthorizer.type=basic\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Extraction Function in Apache Druid\nDESCRIPTION: Demonstrates the configuration of a Cascade extraction function, which chains multiple extraction functions together. This example combines regular expression, JavaScript, and substring extraction functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"cascade\", \n  \"extractionFns\": [\n    { \n      \"type\" : \"regex\", \n      \"expr\" : \"/([^/]+)/\", \n      \"replaceMissingValue\": false,\n      \"replaceMissingValueWith\": null\n    },\n    { \n      \"type\" : \"javascript\", \n      \"function\" : \"function(str) { return \\\"the \\\".concat(str) }\" \n    },\n    { \n      \"type\" : \"substring\", \n      \"index\" : 0, \"length\" : 7 \n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to Multiple Quantiles Post-Aggregator\nDESCRIPTION: Post-aggregator configuration for computing multiple quantiles from a DoublesSketch using an array of fractions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantiles\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fractions\" : <array of fractional positions in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Declaration\nDESCRIPTION: MIT license declaration for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Extracting Druid Distribution Files\nDESCRIPTION: Commands to extract the Druid distribution archive and navigate to the installation directory. This is typically done on a single machine before distributing to other servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.14.0-incubating-bin.tar.gz\ncd apache-druid-0.14.0-incubating\n```\n\n----------------------------------------\n\nTITLE: Defining Time Functions Table in Markdown\nDESCRIPTION: This markdown table lists and describes the time-related functions available in Druid's expression language, including timestamp parsing, formatting, and manipulation functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/misc/math-expr.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|name|description|\n|----|-----------|\n|timestamp|timestamp(expr[,format-string]) parses string expr into date then returns milli-seconds from java epoch. without 'format-string' it's regarded as ISO datetime format |\n|unix_timestamp|same with 'timestamp' function but returns seconds instead |\n|timestamp_ceil|timestamp_ceil(expr, period, \\[origin, \\[timezone\\]\\]) rounds up a timestamp, returning it as a new timestamp. Period can be any ISO8601 period, like P3M (quarters) or PT12H (half-days). The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\".|\n|timestamp_floor|timestamp_floor(expr, period, \\[origin, [timezone\\]\\]) rounds down a timestamp, returning it as a new timestamp. Period can be any ISO8601 period, like P3M (quarters) or PT12H (half-days). The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\".|\n|timestamp_shift|timestamp_shift(expr, period, step, \\[timezone\\]) shifts a timestamp by a period (step times), returning it as a new timestamp. Period can be any ISO8601 period. Step may be negative. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\".|\n|timestamp_extract|timestamp_extract(expr, unit, \\[timezone\\]) extracts a time part from expr, returning it as a number. Unit can be EPOCH (number of seconds since 1970-01-01 00:00:00 UTC), SECOND, MINUTE, HOUR, DAY (day of month), DOW (day of week), DOY (day of year), WEEK (week of [week year](https://en.wikipedia.org/wiki/ISO_week_date)), MONTH (1 through 12), QUARTER (1 through 4), or YEAR. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\"|\n|timestamp_parse|timestamp_parse(string expr, \\[pattern, [timezone\\]\\]) parses a string into a timestamp using a given [Joda DateTimeFormat pattern](http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html). If the pattern is not provided, this parses time strings in either ISO8601 or SQL format. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\", and will be used as the time zone for strings that do not include a time zone offset. Pattern and time zone must be literals. Strings that cannot be parsed as timestamps will be returned as nulls.|\n|timestamp_format|timestamp_format(expr, \\[pattern, \\[timezone\\]\\]) formats a timestamp as a string with a given [Joda DateTimeFormat pattern](http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html), or ISO8601 if the pattern is not provided. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". Pattern and time zone must be literals.|\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Simple Consumer Firehose in Apache Druid\nDESCRIPTION: JSON configuration for the KafkaSimpleConsumerFirehose in Apache Druid. This configuration specifies how Druid connects to Kafka brokers and consumes data from a specific topic with defined partitions and client settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/kafka-simple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehoseV2\": {\n  \"type\" : \"kafka-0.8-v2\",\n  \"brokerList\" :  [\"localhost:4443\"],\n  \"queueBufferLength\":10001,\n  \"resetOffsetToEarliest\":\"true\",\n  \"partitionIdList\" : [\"0\"],\n  \"clientId\" : \"localclient\",\n  \"feed\": \"wikipedia\"\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Day of Week using Time Format Function in Druid\nDESCRIPTION: Example of using the time format extraction function to return the day of the week for Montral in French.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : \"__time\",\n  \"outputName\" :  \"dayOfWeek\",\n  \"extractionFn\" : {\n    \"type\" : \"timeFormat\",\n    \"format\" : \"EEEE\",\n    \"timeZone\" : \"America/Montreal\",\n    \"locale\" : \"fr\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties for Classloader Isolation in JSON\nDESCRIPTION: This JSON snippet shows how to set Hadoop job properties in the tuningConfig of an indexing task to enable classloader isolation and avoid library conflicts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\": {\n  \"mapreduce.job.classloader\": \"true\",\n  \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting a Kafka broker server in Bash\nDESCRIPTION: Command to start a Kafka server using the default configuration properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-server-start.sh config/server.properties\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Converter for Ambari Metrics Emitter in Druid\nDESCRIPTION: JSON configuration for the 'all' event converter, which sends all Druid service metrics events to Ambari Metrics. This converter formats the metrics path as namespacePrefix.[service name].[hostname].[dimensions].[metric].\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"appName\":\"druid\"}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output for Row Dump in DumpSegment Tool\nDESCRIPTION: Example of a single row output from the DumpSegment tool when dumping rows. It shows various fields and their values in JSON format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/dump-segment.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__time\": 1442018818771,\n  \"added\": 36,\n  \"channel\": \"#en.wikipedia\",\n  \"cityName\": null,\n  \"comment\": \"added project\",\n  \"count\": 1,\n  \"countryIsoCode\": null,\n  \"countryName\": null,\n  \"deleted\": 0,\n  \"delta\": 36,\n  \"isAnonymous\": \"false\",\n  \"isMinor\": \"false\",\n  \"isNew\": \"false\",\n  \"isRobot\": \"false\",\n  \"isUnpatrolled\": \"false\",\n  \"iuser\": \"00001553\",\n  \"metroCode\": null,\n  \"namespace\": \"Talk\",\n  \"page\": \"Talk:Oswald Tilghman\",\n  \"regionIsoCode\": null,\n  \"regionName\": null,\n  \"user\": \"GELongstreet\"\n}\n```\n\n----------------------------------------\n\nTITLE: Mixed Version Segments Example\nDESCRIPTION: Shows how segments with different versions can coexist during a rolling update process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v2_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Setting Up Period Broadcast Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period broadcast rule that specifies how segments of different data sources should be co-located in Historical processes for a rolling time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByPeriod\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a JDBC-based Cached Namespace Lookup in Druid\nDESCRIPTION: Example configuration for a globally cached lookup using the JDBC type. This configuration connects to a MySQL database and specifies table, key column, value column, and filtering options for lookup data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"jdbc\",\n       \"connectorConfig\": {\n         \"createTables\": true,\n         \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n         \"user\": \"druid\",\n         \"password\": \"diurd\"\n       },\n       \"table\": \"lookupTable\",\n       \"keyColumn\": \"mykeyColumn\",\n       \"valueColumn\": \"myValueColumn\",\n       \"filter\" : \"myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)\",\n       \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Hour Granularity in Apache Druid\nDESCRIPTION: Example of a GroupBy query in Apache Druid using 'hour' granularity. This query groups data by language and counts occurrences, with results bucketed by hour.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":\"hour\",\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Query Results Format for Timestamp Min/Max Aggregation in Druid\nDESCRIPTION: Example of the query results showing how Timestamp Min/Max aggregators provide more precise timestamp information than the query granularity, with one row per dimension value per day.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T00:00:00.000Z A 4 2015-07-28T01:00:00.000Z 2015-07-28T05:00:00.000Z\n2015-07-28T00:00:00.000Z B 2 2015-07-28T04:00:00.000Z 2015-07-28T06:00:00.000Z\n2015-07-29T00:00:00.000Z A 2 2015-07-29T03:00:00.000Z 2015-07-29T04:00:00.000Z\n2015-07-29T00:00:00.000Z C 2 2015-07-29T01:00:00.000Z 2015-07-29T02:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Processing in YAML for Apache Druid Realtime Process\nDESCRIPTION: This YAML snippet configures query processing parameters for the Apache Druid Realtime Process, including buffer size, thread naming, merge buffers, and processing threads. It affects how queries are processed and optimized.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/realtime.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.processing.buffer.sizeBytes: auto\ndruid.processing.formatString: processing-%s\ndruid.processing.numMergeBuffers: max(2, druid.processing.numThreads / 4)\ndruid.processing.numThreads: Number of cores - 1\ndruid.processing.columnCache.sizeBytes: 0\ndruid.processing.tmpDir: path represented by java.io.tmpdir\n```\n\n----------------------------------------\n\nTITLE: Querying Appended Data in Druid with Select\nDESCRIPTION: SQL query showing the results after appending data to the 'updates-tutorial' datasource, with duplicate rows preserved in separate segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          2     400 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n 2018-01-01T05:01:00.000Z  mongoose      1     737 \n 2018-01-01T06:01:00.000Z  snake         1    1234 \n 2018-01-01T07:01:00.000Z  octopus       1     115 \n 2018-01-01T04:01:00.000Z  bear          1     222 \n 2018-01-01T09:01:00.000Z  falcon        1    1241 \n\nRetrieved 8 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Running COUNT Query on Druid Datasource\nDESCRIPTION: SQL query to count the total number of rows in the 'compaction-tutorial' datasource using Druid SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select count(*) from \"compaction-tutorial\";\n\n EXPR$0 \n\n  39244 \n\nRetrieved 1 row in 1.38s.\n```\n\n----------------------------------------\n\nTITLE: Listing Druid Configuration Directory Structure\nDESCRIPTION: Shows the recommended directory structure for organizing Druid configuration files, including service-specific configs and common properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: Listing Druid Configuration Directory Structure\nDESCRIPTION: Shows the recommended directory structure for organizing Druid configuration files, including service-specific configs and common properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Stream Parser with Schema Repo in Apache Druid\nDESCRIPTION: This snippet demonstrates how to configure the Avro stream parser with a schema repo Avro bytes decoder. It specifies the parser type, bytes decoder configuration, and parse specification including timestamp and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"avro_stream\",\n  \"avroBytesDecoder\" : {\n    \"type\" : \"schema_repo\",\n    \"subjectAndIdConverter\" : {\n      \"type\" : \"avro_1124\",\n      \"topic\" : \"${YOUR_TOPIC}\"\n    },\n    \"schemaRepository\" : {\n      \"type\" : \"avro_1124_rest_client\",\n      \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\",\n    }\n  },\n  \"parseSpec\" : {\n    \"format\": \"avro\",\n    \"timestampSpec\": <standard timestampSpec>,\n    \"dimensionsSpec\": <standard dimensionsSpec>,\n    \"flattenSpec\": <optional>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query Example with Variance Aggregator in Apache Druid\nDESCRIPTION: Example of a timeseries query using the variance aggregator in Apache Druid. Demonstrates how to include the variance aggregator in the query configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"testing\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS General Properties in Apache Druid\nDESCRIPTION: Enables or disables HTTP and HTTPS connectors for Druid processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Intermediate Segment Storage for Apache Druid Realtime Node\nDESCRIPTION: YAML configuration for specifying where intermediate segments are stored in a Druid realtime node.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/realtime.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.segmentCache.locations: none\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka 0.8.x Firehose in JSON for Apache Druid\nDESCRIPTION: Sample JSON configuration for setting up a Kafka 0.8.x firehose in Apache Druid. This configuration establishes connection to Zookeeper, sets consumer group properties, and specifies the Kafka topic to ingest data from.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kafka-eight-firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\": {\n  \"type\": \"kafka-0.8\",\n  \"consumerProps\": {\n    \"zookeeper.connect\": \"localhost:2181\",\n    \"zookeeper.connection.timeout.ms\" : \"15000\",\n    \"zookeeper.session.timeout.ms\" : \"15000\",\n    \"zookeeper.sync.time.ms\" : \"5000\",\n    \"group.id\": \"druid-example\",\n    \"fetch.message.max.bytes\" : \"1048586\",\n    \"auto.offset.reset\": \"largest\",\n    \"auto.commit.enable\": \"false\"\n  },\n  \"feed\": \"wikipedia\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up HDFS Directories for Druid\nDESCRIPTION: Commands to create necessary HDFS directories for Druid and copy input data to HDFS.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/hadoop/bin\n./hadoop fs -mkdir /druid\n./hadoop fs -mkdir /druid/segments\n./hadoop fs -mkdir /quickstart\n./hadoop fs -chmod 777 /druid\n./hadoop fs -chmod 777 /druid/segments\n./hadoop fs -chmod 777 /quickstart\n./hadoop fs -chmod -R 777 /tmp\n./hadoop fs -chmod -R 777 /user\n./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Rule in Druid\nDESCRIPTION: A Period Drop Rule configuration that instructs Druid to drop segments based on a rolling time period. This rule matches if the specified period contains the interval of a segment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByPeriod\",\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Properties in Apache Druid\nDESCRIPTION: This snippet shows a table of configuration properties for the Historical process in Apache Druid. It includes settings for host, port, and service name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_25\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.host`|The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that `http://${druid.host}/` could actually talk to this process|InetAddress.getLocalHost().getCanonicalHostName()|\n|`druid.bindOnHost`|Indicating whether the process's internal jetty server bind on `druid.host`. Default is false, which means binding to all interfaces.|false|\n|`druid.plaintextPort`|This is the port to actually listen on; unless port mapping is used, this will be the same port as is on `druid.host`|8083|\n|`druid.tlsPort`|TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.html) is set then this config will be used. If `druid.host` contains port then that port will be ignored. This should be a non-negative Integer.|8283|\n|`druid.service`|The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services|druid/historical|\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling Off-heap Lookup Cache in Druid\nDESCRIPTION: JSON configuration example for an off-heap lookup cache that loads once without periodic updates using JDBC data fetcher.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"offHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring White-list Based Converter for Ambari Metrics Emitter in Druid\nDESCRIPTION: JSON configuration for the 'whiteList' event converter, which only sends white-listed metrics and dimensions to Ambari Metrics. This example shows how to specify a custom map file path for defining which metrics should be included.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"appName\":\"druid\", \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring a JDBC-based Cached Namespace Lookup in Druid\nDESCRIPTION: Example configuration for a globally cached lookup using the JDBC type. This configuration connects to a MySQL database and specifies table, key column, value column, and filtering options for lookup data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"jdbc\",\n       \"connectorConfig\": {\n         \"createTables\": true,\n         \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n         \"user\": \"druid\",\n         \"password\": \"diurd\"\n       },\n       \"table\": \"lookupTable\",\n       \"keyColumn\": \"mykeyColumn\",\n       \"valueColumn\": \"myValueColumn\",\n       \"filter\" : \"myFilterSQL (Where clause statement  e.g LOOKUPTYPE=1)\",\n       \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring duplicate columns for schema-less dimensions in Druid\nDESCRIPTION: Example showing how to include the same data twice in the input record - once as a dimension field and once as a metric field. This approach allows the same ID to be used for both filtering and unique counting operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"device_id_dim\":123, \"device_id_met\":123}\n```\n\n----------------------------------------\n\nTITLE: Configuring Upper Case Extraction Function with Locale in Druid\nDESCRIPTION: Configuration for converting dimension values to upper case with specific locale settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"upper\",\n  \"locale\":\"fr\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Noop Task in Apache Druid\nDESCRIPTION: JSON configuration for a Noop task in Apache Druid. This task is used for testing purposes, allowing specification of task ID, segment interval, runtime, and optional firehose testing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"noop\",\n    \"id\": <optional_task_id>,\n    \"interval\" : <optional_segment_interval>,\n    \"runTime\" : <optional_millis_to_sleep>,\n    \"firehose\": <optional_firehose_to_test_connect>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Segment Identifier Without Partition\nDESCRIPTION: Example showing the format of a Druid segment identifier for partition number 0, which omits the partition number from the identifier string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output Format\nDESCRIPTION: Example of a single row output from the DumpSegment tool showing various fields and their values in JSON format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/dump-segment.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__time\": 1442018818771,\n  \"added\": 36,\n  \"channel\": \"#en.wikipedia\",\n  \"cityName\": null,\n  \"comment\": \"added project\",\n  \"count\": 1,\n  \"countryIsoCode\": null,\n  \"countryName\": null,\n  \"deleted\": 0,\n  \"delta\": 36,\n  \"isAnonymous\": \"false\",\n  \"isMinor\": \"false\",\n  \"isNew\": \"false\",\n  \"isRobot\": \"false\",\n  \"isUnpatrolled\": \"false\",\n  \"iuser\": \"00001553\",\n  \"metroCode\": null,\n  \"namespace\": \"Talk\",\n  \"page\": \"Talk:Oswald Tilghman\",\n  \"regionIsoCode\": null,\n  \"regionName\": null,\n  \"user\": \"GELongstreet\"\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying ZooKeeper Path for Individual Served Segments in Druid\nDESCRIPTION: This snippet defines the ZooKeeper path where Historical and Realtime processes in Druid create ephemeral znodes for each segment they are serving.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_\n```\n\n----------------------------------------\n\nTITLE: Declaring License for React in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the React production minified file, version 17.0.2, created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining Druid SQL Metrics in Markdown\nDESCRIPTION: This snippet presents a table of SQL metrics emitted by the Broker. It includes metric names, descriptions, dimensions, and expected normal values for SQL query performance monitoring.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_21\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`sqlQuery/time`|Milliseconds taken to complete a SQL.|id, nativeQueryIds, dataSource, remoteAddress, success.|< 1s|\n|`sqlQuery/bytes`|number of bytes returned in SQL response.|id, nativeQueryIds, dataSource, remoteAddress, success.| |\n```\n\n----------------------------------------\n\nTITLE: Using True Filter in Apache Druid\nDESCRIPTION: This example shows a true filter which matches all values. It can be used to temporarily disable other filters without removing them from the query configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"true\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring Aggregators for Flattened JSON Fields in Apache Druid\nDESCRIPTION: This JSON configuration shows how to set up aggregators to use the metric column names defined in the flattenSpec. It includes examples for longSum and doubleSum aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metricsSpec\" : [ \n{\n  \"type\" : \"longSum\",\n  \"name\" : \"path-metric-sum\",\n  \"fieldName\" : \"path-metric\"\n}, \n{\n  \"type\" : \"doubleSum\",\n  \"name\" : \"hello-0-sum\",\n  \"fieldName\" : \"hello-0\"\n},\n{\n  \"type\" : \"longSum\",\n  \"name\" : \"metrica-sum\",\n  \"fieldName\" : \"metrica\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Password Provider in Jackson Module\nDESCRIPTION: Example of how to register a custom PasswordProvider implementation with Jackson. This allows the custom provider to be specified in Druid's configuration files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/modules.md#2025-04-09_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n    return ImmutableList.of(\n        new SimpleModule(\"SomePasswordProviderModule\")\n            .registerSubtypes(\n                new NamedType(SomePasswordProvider.class, \"some\")\n            )\n    );\n```\n\n----------------------------------------\n\nTITLE: Classnames License Comment\nDESCRIPTION: License comment for the classnames library, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: React-Is License Declaration\nDESCRIPTION: License declaration for React's react-is.production.min.js under MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring toInclude List Option in Druid Query\nDESCRIPTION: Configuration to specify a list of columns to include in the segment metadata query result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"toInclude\": { \"type\": \"list\", \"columns\": [<string list of column names>]}\n```\n\n----------------------------------------\n\nTITLE: JVM Settings for Druid Router in Production\nDESCRIPTION: Example JVM configuration for running the Druid Router process in a production environment on a c3.2xlarge EC2 instance with memory and GC settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/mnt/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/mnt/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n```\n\n----------------------------------------\n\nTITLE: Registering Jackson Module for S3 Firehose\nDESCRIPTION: Implementation of getJacksonModules() to register a StaticS3FirehoseFactory as a subtype for Jackson serialization/deserialization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Header\nDESCRIPTION: License header for React's react-dom.production.min.js (version 17.0.2) module, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Executing insert-segment-to-db Tool with MySQL and HDFS\nDESCRIPTION: Example command to run the insert-segment-to-db tool using MySQL as metadata storage and HDFS as deep storage. It demonstrates how to specify necessary JVM arguments and command-line options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava \n-Ddruid.metadata.storage.type=mysql \n-Ddruid.metadata.storage.connector.connectURI=jdbc\\:mysql\\://localhost\\:3306/druid \n-Ddruid.metadata.storage.connector.user=druid \n-Ddruid.metadata.storage.connector.password=diurd \n-Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\",\\\"druid-hdfs-storage\\\"] \n-Ddruid.storage.type=hdfs\n-cp $DRUID_CLASSPATH \norg.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true\n```\n\n----------------------------------------\n\nTITLE: Overwriting Data in Apache Druid\nDESCRIPTION: This command submits a task to overwrite the existing data in the 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json\n```\n\n----------------------------------------\n\nTITLE: Updating Rules for a Datasource in Druid Coordinator API\nDESCRIPTION: POST endpoint to update rules for a specific datasource. Accepts a list of rules in JSON format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_9\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/rules/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Selector Filter on Numeric Column in Druid (JSON)\nDESCRIPTION: This snippet shows how to set up a selector filter on a numeric column in a Druid query. It filters for a specific float value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"myFloatColumn\",\n  \"value\": \"10.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Executing insert-segment-to-db Tool with MySQL and HDFS\nDESCRIPTION: Example command to run the insert-segment-to-db tool using MySQL as metadata storage and HDFS as deep storage. It demonstrates how to specify necessary JVM arguments and command-line options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava \n-Ddruid.metadata.storage.type=mysql \n-Ddruid.metadata.storage.connector.connectURI=jdbc\\:mysql\\://localhost\\:3306/druid \n-Ddruid.metadata.storage.connector.user=druid \n-Ddruid.metadata.storage.connector.password=diurd \n-Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\",\\\"druid-hdfs-storage\\\"] \n-Ddruid.storage.type=hdfs\n-cp $DRUID_CLASSPATH \norg.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true\n```\n\n----------------------------------------\n\nTITLE: Extracting Apache Druid Binary Package\nDESCRIPTION: Commands to extract the downloaded Apache Druid tarball and navigate to its root directory. This is the first step after downloading the Druid package.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/index.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.15.1-incubating-bin.tar.gz\ncd apache-druid-0.15.1-incubating\n```\n\n----------------------------------------\n\nTITLE: Distinct Countries Cardinality Example\nDESCRIPTION: Example showing how to calculate distinct countries from origin and residence fields using the cardinality aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/hll-old.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_countries\",\n  \"fields\": [ \"country_of_origin\", \"country_of_residence\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Scaling in Druid (JSON)\nDESCRIPTION: JSON configuration for shardSpec to achieve scale by adding processes with different partition numbers. This allows storage of segments with the same datasource, time interval, and version, but different partition numbers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"linear\",\n    \"partitionNum\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HttpFirehose with Basic Authentication\nDESCRIPTION: Example of HttpFirehose configuration with Basic Authentication using the DefaultPassword provider, which requires the password to be included directly in the ingestion spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"http\",\n    \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"],\n    \"httpAuthenticationUsername\": \"username\",\n    \"httpAuthenticationPassword\": \"password123\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CombiningFirehose in Druid for Merged Data Sources\nDESCRIPTION: Configuration for the CombiningFirehose which merges data from multiple firehoses. This allows combining data from different source types into a single ingestion process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/firehose.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"combining\",\n    \"delegates\" : [ { firehose1 }, { firehose2 }, ..... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Network Flow Event Data in JSON\nDESCRIPTION: Example JSON data representing network flow events with timestamp, source IP, destination IP, packet count, and byte count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":20,\"bytes\":9024\n}\n{\n\"timestamp\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":255,\"bytes\":21133\n}\n{\n\"timestamp\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":11,\"bytes\":5780\n}\n{\n\"timestamp\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":38,\"bytes\":6289\n}\n{\n\"timestamp\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":377,\"bytes\":359971\n}\n{\n\"timestamp\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":49,\"bytes\":10204\n}\n{\n\"timestamp\":\"2018-01-02T21:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":38,\"bytes\":6289\n}\n{\n\"timestamp\":\"2018-01-02T21:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":123,\"bytes\":93999\n}\n{\n\"timestamp\":\"2018-01-02T21:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":12,\"bytes\":2818\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Load Rule in Apache Druid\nDESCRIPTION: The Interval Load Rule specifies how many replicas of a segment should exist in different server tiers for a specific time interval. It only retains data within the specified ISO-8601 interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByInterval\",\n  \"interval\": \"2012-01-01/2013-01-01\",\n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Load Rule in Apache Druid\nDESCRIPTION: The Interval Load Rule specifies how many replicas of a segment should exist in different server tiers for a specific time interval. It only retains data within the specified ISO-8601 interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByInterval\",\n  \"interval\": \"2012-01-01/2013-01-01\",\n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom TLS Certificate Checker in Apache Druid\nDESCRIPTION: Configuration property for specifying a custom TLS certificate checker in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/tls-support.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.tls.certificateChecker`|Type name of custom TLS certificate checker, provided by extensions. Please refer to extension documentation for the type name that should be specified.|\"default\"|no|\n```\n\n----------------------------------------\n\nTITLE: Configuring Tuning Parameters for Druid Ingestion Task in JSON\nDESCRIPTION: This snippet demonstrates how to add a tuningConfig section to a Druid ingestion task, setting a target segment size for the native batch ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n```\n\n----------------------------------------\n\nTITLE: Metrics Specification for Flattened Fields\nDESCRIPTION: Example of metrics specification that uses the flattened field names for aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/flatten-json.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metricsSpec\" : [ \n{\n  \"type\" : \"longSum\",\n  \"name\" : \"path-metric-sum\",\n  \"fieldName\" : \"path-metric\"\n}, \n{\n  \"type\" : \"doubleSum\",\n  \"name\" : \"hello-0-sum\",\n  \"fieldName\" : \"hello-0\"\n},\n{\n  \"type\" : \"longSum\",\n  \"name\" : \"metrica-sum\",\n  \"fieldName\" : \"metrica\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Resetting Kafka State in Bash\nDESCRIPTION: Command to remove Kafka log directory, used when resetting the cluster state after completing the Kafka streaming tutorial.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/index.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /tmp/kafka-logs\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Layout\nDESCRIPTION: Front matter and layout configuration for the documentation page comparing Druid and Redshift.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/comparisons/druid-vs-redshift.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) vs Redshift\"\n---\n```\n\n----------------------------------------\n\nTITLE: Running the Command Line Hadoop Indexer in Bash\nDESCRIPTION: Command to execute the Hadoop indexer from the command line with memory settings, timezone, and encoding configurations. This command runs the indexer with a specified spec file containing the ingestion configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>\n```\n\n----------------------------------------\n\nTITLE: PrefixFiltered DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for filtering multi-value dimensions based on prefix matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"prefixFiltered\", \"delegate\" : <dimensionSpec>, \"prefix\": <prefix string> }\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container\nDESCRIPTION: Command to start the Hadoop container with necessary port mappings and volume mounts\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Submitting Kinesis Supervisor Spec via cURL\nDESCRIPTION: This cURL command demonstrates how to submit a Kinesis supervisor specification to the Druid Overlord via HTTP POST.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQFirehose in Apache Druid\nDESCRIPTION: A sample JSON configuration for setting up a RabbitMQFirehose in Druid. The configuration specifies connection parameters to RabbitMQ and queue settings for data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/rabbitmq.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n   \"type\" : \"rabbitmq\",\n   \"connection\" : {\n     \"host\": \"localhost\",\n     \"port\": \"5672\",\n     \"username\": \"test-dude\",\n     \"password\": \"test-word\",\n     \"virtualHost\": \"test-vhost\",\n     \"uri\": \"amqp://mqserver:1234/vhost\"\n   },\n   \"config\" : {\n     \"exchange\": \"test-exchange\",\n     \"queue\" : \"druidtest\",\n     \"routingKey\": \"#\",\n     \"durable\": \"true\",\n     \"exclusive\": \"false\",\n     \"autoDelete\": \"false\",\n     \"maxRetries\": \"10\",\n     \"retryIntervalSeconds\": \"1\",\n     \"maxDurationSeconds\": \"300\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Re-enabling a Middle Manager in Apache Druid\nDESCRIPTION: Send a POST request to manually re-enable a Middle Manager after an update.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rolling-updates.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/enable\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Configuration Directory Structure\nDESCRIPTION: Shell command output showing the recommended organization of Druid configuration files and directories. The structure includes separate directories for each service type with individual configuration files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Approximate Histogram Aggregator in Druid\nDESCRIPTION: JSON configuration for the approximate histogram aggregator in Apache Druid. This includes parameters for controlling resolution, bucket count, and value limits. The aggregator provides histogram approximation with configurable accuracy tradeoffs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"approxHistogram or approxHistogramFold (at ingestion time), approxHistogramFold (at query time)\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"resolution\" : <integer>,\n  \"numBuckets\" : <integer>,\n  \"lowerLimit\" : <float>,\n  \"upperLimit\" : <float>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchToQuantilesSketch Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchToQuantilesSketch post-aggregator. This returns a quantiles DoublesSketch constructed from a given column of values from a given ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToQuantilesSketch\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"column\" : <number>,\n  \"k\" : <parameter that determines the accuracy and size of the quantiles sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bloom Filter JSON in Apache Druid\nDESCRIPTION: JSON configuration structure for implementing a Bloom Filter in Druid. Specifies required fields including type, dimension, bloomKFilter for serialized filter data, and optional extractionFn for dimension value processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bloom\",\n  \"dimension\" : <dimension_name>,\n  \"bloomKFilter\" : <serialized_bytes_for_BloomKFilter>,\n  \"extractionFn\" : <extraction_fn>\n}\n```\n\n----------------------------------------\n\nTITLE: Metrics Specification for Flattened Fields\nDESCRIPTION: Example of metrics specification that uses the flattened field names for aggregations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/flatten-json.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metricsSpec\" : [ \n{\n  \"type\" : \"longSum\",\n  \"name\" : \"path-metric-sum\",\n  \"fieldName\" : \"path-metric\"\n}, \n{\n  \"type\" : \"doubleSum\",\n  \"name\" : \"hello-0-sum\",\n  \"fieldName\" : \"hello-0\"\n},\n{\n  \"type\" : \"longSum\",\n  \"name\" : \"metrica-sum\",\n  \"fieldName\" : \"metrica\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Search Query Response Format in Apache Druid\nDESCRIPTION: Example response format from a search query showing matched dimension values across different time intervals. Each result includes the dimension name, matching value, and count of occurrences.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/searchquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"Ke$ha\",\n        \"count\": 3\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"Ke$haForPresident\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"SomethingThatContainsKe\",\n        \"count\": 1\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"SomethingElseThatContainsKe\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV Lookup ParseSpec in Apache Druid\nDESCRIPTION: Example JSON configuration for a TSV (tab-separated values) lookup parseSpec. Defines how to parse a TSV file with custom delimiter specification and column mappings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"tsv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\",\n  \"delimiter\": \"|\"\n}\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Help Output in Apache Druid\nDESCRIPTION: Sample output of the help command for the ResetCluster tool, showing the tool's name, synopsis, and detailed descriptions of all available command-line options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/reset-cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nNAME\n        druid tools reset-cluster - Cleanup all persisted state from metadata\n        and deep storage.\n\nSYNOPSIS\n        druid tools reset-cluster [--all] [--hadoopWorkingPath]\n                [--metadataStore] [--segmentFiles] [--taskLogs]\n\nOPTIONS\n        --all\n            delete all state stored in metadata and deep storage\n\n        --hadoopWorkingPath\n            delete hadoopWorkingPath\n\n        --metadataStore\n            delete all records in metadata storage\n\n        --segmentFiles\n            delete all segment files from deep storage\n\n        --taskLogs\n            delete all tasklogs\n```\n\n----------------------------------------\n\nTITLE: License Comment for React use-sync-external-store-shim\nDESCRIPTION: Copyright and license information for the React use-sync-external-store-shim production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Scan Query in Apache Druid\nDESCRIPTION: This SQL query performs a scan operation, selecting the user and page columns for Wikipedia edits within a specific hour on 2015-09-12.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT user, page FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00' LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Running Druid Coordinator Server Command\nDESCRIPTION: Command to start the Druid Coordinator server process using the Main class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/coordinator.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server coordinator\n```\n\n----------------------------------------\n\nTITLE: Realtime Operation Configuration in YAML\nDESCRIPTION: Configuration properties for Realtime operations in Druid, specifying how segments are published and where the specification file is located. These settings control the core behavior of the Realtime Process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/realtime.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.publish.type: metadata\ndruid.realtime.specFile: none\n```\n\n----------------------------------------\n\nTITLE: Using OR Logical Expression Filter in Druid groupBy Query\nDESCRIPTION: Demonstrates how to use an OR logical expression filter in a having clause to combine multiple conditions where at least one must be satisfied.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Bezier Curve Functions in JavaScript\nDESCRIPTION: This code generates functions for calculating Bezier curves. Bezier curves are commonly used in computer graphics and animation to create smooth transitions and paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/7724.4bbc210b.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! Bezier curve function generator. Copyright Gaetan Renaudeau. MIT License: http://en.wikipedia.org/wiki/MIT_License */\n```\n\n----------------------------------------\n\nTITLE: Submitting Ingestion Task in Apache Druid\nDESCRIPTION: This bash command submits the ingestion task to Apache Druid using the post-index-task script. It references the JSON file containing the ingestion spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/ingestion-tutorial-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Count Metric in Druid Ingestion Spec\nDESCRIPTION: This JSON snippet shows how to configure a count metric in the metricsSpec section of a Druid ingestion specification. It defines a metric named 'count' of type 'count'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n\"metricsSpec\" : [\n      {\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      },\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Extensions in Druid's common.runtime.properties\nDESCRIPTION: Example configuration for loading bundled core extensions in Druid by adding their names to the druid.extensions.loadList property in common.runtime.properties. This example loads the postgresql-metadata-storage and druid-hdfs-storage extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/including-extensions.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\", \"druid-hdfs-storage\"]\n```\n\n----------------------------------------\n\nTITLE: Applying Dimension Selector Filter in groupBy Query Having Clause (JSON)\nDESCRIPTION: This snippet demonstrates how to use a dimension selector filter in the having clause of a groupBy query to match rows with specific dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/having.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n       {\n            \"type\": \"dimSelector\",\n            \"dimension\": \"<dimension>\",\n            \"value\": <dimension_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Using OR Logical Expression Filter in Druid groupBy Query\nDESCRIPTION: Demonstrates how to use an OR logical expression filter in a having clause to combine multiple conditions where at least one must be satisfied.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Input Data Structure - Druid JSON Format\nDESCRIPTION: Example of raw data stored in Druid with millisecond ingestion granularity\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"AAA\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-01T01:02:33Z\", \"page\": \"BBB\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"}\n{\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"}\n```\n\n----------------------------------------\n\nTITLE: License Comment for Prism\nDESCRIPTION: Copyright and license information for the Prism syntax highlighting library by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: AND Filter for Multi-value Dimension in Druid\nDESCRIPTION: Shows an 'and' filter that matches rows where the 'tags' dimension contains both 't1' and 't3'. This filter would only match the first row of the sample dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"and\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Druid Servers Information\nDESCRIPTION: SQL query to retrieve information about all servers in the Druid cluster from the sys.servers table. This includes server types, tiers, and capacity information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Shutdown Task Response - JSON\nDESCRIPTION: Example response when shutting down a specific task via the /druid/worker/v1/task/{taskid}/shutdown endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing True Filter in Druid\nDESCRIPTION: Example of a true filter that matches all values, useful for temporarily disabling other filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"true\" }\n```\n\n----------------------------------------\n\nTITLE: Running Export Metadata Tool in Apache Druid\nDESCRIPTION: Command to execute the export-metadata tool from the Druid root directory. It specifies the classpath, log configuration, extensions directory, and connection URI for the Derby database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/export-metadata.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ${DRUID_ROOT}\nmkdir -p /tmp/csv\njava -classpath \"lib/*\" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory=\"extensions\" -Ddruid.extensions.loadList=[] org.apache.druid.cli.Main tools export-metadata --connectURI \"jdbc:derby://localhost:1527/var/druid/metadata.db;\" -o /tmp/csv\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on Master Server\nDESCRIPTION: Bash commands for downloading and installing Zookeeper on a Druid master server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Segment Identifier Example\nDESCRIPTION: Shows the format of a Druid segment identifier with partition number 1, which includes datasource name, time interval, version number, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Manual Task Submission Using cURL in Druid\nDESCRIPTION: Shows how to manually submit an ingestion task to Druid using cURL, posting the task specification directly to the indexer endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8081/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Example Mixed Version Segments\nDESCRIPTION: Shows a mixed state during segment updates where both v1 and v2 segments exist simultaneously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v2_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Served Segments in Druid\nDESCRIPTION: Specifies the ZooKeeper path where Historical and Realtime processes in Druid create permanent znodes to indicate the segments they are serving.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Shutdown Task Response - JSON\nDESCRIPTION: Example JSON response when shutting down a specific task, showing the task ID.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"}\n```\n\n----------------------------------------\n\nTITLE: Segment Identifier Example\nDESCRIPTION: Shows the format of a Druid segment identifier with partition number 1, which includes datasource name, time interval, version number, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Permission Assignment Request\nDESCRIPTION: JSON request format for assigning permissions to a role, showing datasource access examples\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[\n{\n  \"resource\": {\n    \"name\": \"wiki.*\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"READ\"\n},\n{\n  \"resource\": {\n    \"name\": \"wikiticker\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"WRITE\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Compression Properties in Apache Druid\nDESCRIPTION: This table outlines the configuration properties for HTTP compression in Apache Druid. It includes settings for compression level and buffer size for request decompression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/http-compression.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|\n|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|\n```\n\n----------------------------------------\n\nTITLE: Configuring tuningConfig for Apache Druid Ingestion Task\nDESCRIPTION: A JSON snippet that defines tuning parameters for a Druid ingestion task. This example sets the maximum number of rows per segment to control segment size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Page Layout and Title\nDESCRIPTION: This snippet sets the layout and title for the Markdown page using YAML front matter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/indexing-service.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Indexing Service\"\n---\n```\n\n----------------------------------------\n\nTITLE: Router Priority Strategy Configuration\nDESCRIPTION: JSON configuration for the priority-based routing strategy that routes queries based on priority levels.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"priority\",\n  \"minPriority\":0,\n  \"maxPriority\":1\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Event for Flattening in Apache Druid\nDESCRIPTION: This JSON object represents a sample event with nested structures that can be flattened using the JSON Flatten Spec in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"timestamp\": \"2015-09-12T12:10:53.155Z\",\n \"dim1\": \"qwerty\",\n \"dim2\": \"asdf\",\n \"dim3\": \"zxcv\",\n \"ignore_me\": \"ignore this\",\n \"metrica\": 9999,\n \"foo\": {\"bar\": \"abc\"},\n \"foo.bar\": \"def\",\n \"nestmet\": {\"val\": 42},\n \"hello\": [1.0, 2.0, 3.0, 4.0, 5.0],\n \"mixarray\": [1.0, 2.0, 3.0, 4.0, {\"last\": 5}],\n \"world\": [{\"hey\": \"there\"}, {\"tree\": \"apple\"}],\n \"thing\": {\"food\": [\"sandwich\", \"pizza\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Druid TopN Query Results for Wikipedia Page Edits\nDESCRIPTION: The JSON response from the Druid TopN query showing the top 10 most edited Wikipedia pages on September 12, 2015, with their edit counts. This is the expected output from the previous HTTP query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2015-09-12T00:46:58.771Z\",\n  \"result\" : [ {\n    \"count\" : 33,\n    \"page\" : \"Wikipedia:Vandalismusmeldung\"\n  }, {\n    \"count\" : 28,\n    \"page\" : \"User:Cyde/List of candidates for speedy deletion/Subpage\"\n  }, {\n    \"count\" : 27,\n    \"page\" : \"Jeremy Corbyn\"\n  }, {\n    \"count\" : 21,\n    \"page\" : \"Wikipedia:Administrators' noticeboard/Incidents\"\n  }, {\n    \"count\" : 20,\n    \"page\" : \"Flavia Pennetta\"\n  }, {\n    \"count\" : 18,\n    \"page\" : \"Total Drama Presents: The Ridonculous Race\"\n  }, {\n    \"count\" : 18,\n    \"page\" : \"User talk:Dudeperson176123\"\n  }, {\n    \"count\" : 18,\n    \"page\" : \"Wikipdia:Le Bistro/12 septembre 2015\"\n  }, {\n    \"count\" : 17,\n    \"page\" : \"Wikipedia:In the news/Candidates\"\n  }, {\n    \"count\" : 17,\n    \"page\" : \"Wikipedia:Requests for page protection\"\n  } ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchTTest Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchTTest post-aggregator. This performs Student's t-test and returns a list of p-values given two instances of ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchTTest\",\n  \"name\": <output name>,\n  \"fields\"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>,\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Metrics Data Format\nDESCRIPTION: Sample JSON format showing the structure of metrics data that will be converted to Protobuf\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"unit\": \"milliseconds\",\n  \"http_method\": \"GET\",\n  \"value\": 44,\n  \"timestamp\": \"2017-04-06T02:36:22Z\",\n  \"http_code\": \"200\",\n  \"page\": \"/\",\n  \"metricType\": \"request/latency\",\n  \"server\": \"www1.example.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring IngestSegmentFirehose in Apache Druid\nDESCRIPTION: This snippet shows the configuration for an IngestSegmentFirehose, which is used to read data from existing Druid segments. It specifies the data source and time interval for ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/firehose.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"ingestSegment\",\n    \"dataSource\"   : \"wikipedia\",\n    \"interval\" : \"2013-01-01/2013-01-02\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Password Provider in Apache Druid\nDESCRIPTION: JSON configuration pattern for using a custom password provider implementation. The type property should match the registered name of your custom password provider implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/password-provider.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"<registered_password_provider_name>\", \"<jackson_property>\": \"<value>\", ... }\n```\n\n----------------------------------------\n\nTITLE: Shutdown Task Response - JSON\nDESCRIPTION: Example JSON response when shutting down a specific task, showing the task ID.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"}\n```\n\n----------------------------------------\n\nTITLE: Buckets Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the buckets post-aggregator, which computes a visual representation with custom bucket size and offset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"buckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"bucketSize\": <bucket_size>,\n  \"offset\": <offset>\n}\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query: Unique Users for Both Product A and B\nDESCRIPTION: Complex Druid query using thetaSketch aggregators and post-aggregators to count unique users who visited both product A and B.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"},\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"B\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"A\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"B\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"B_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"B_unique_users\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\n    \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Input JSON Structure\nDESCRIPTION: Example of a nested JSON document that demonstrates various data types and structures that can be flattened.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/flatten-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"timestamp\": \"2015-09-12T12:10:53.155Z\",\n \"dim1\": \"qwerty\",\n \"dim2\": \"asdf\",\n \"dim3\": \"zxcv\",\n \"ignore_me\": \"ignore this\",\n \"metrica\": 9999,\n \"foo\": {\"bar\": \"abc\"},\n \"foo.bar\": \"def\",\n \"nestmet\": {\"val\": 42},\n \"hello\": [1.0, 2.0, 3.0, 4.0, 5.0],\n \"mixarray\": [1.0, 2.0, 3.0, 4.0, {\"last\": 5}],\n \"world\": [{\"hey\": \"there\"}, {\"tree\": \"apple\"}],\n \"thing\": {\"food\": [\"sandwich\", \"pizza\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring toInclude None Option in Druid Query\nDESCRIPTION: Configuration to exclude all columns from the segment metadata query result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"toInclude\": { \"type\": \"none\"}\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch Merge Aggregator Configuration\nDESCRIPTION: JSON configuration for HLLSketchMerge aggregator that merges HLL sketches at query time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchMerge\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Real-time Server\nDESCRIPTION: Command to start the Druid real-time server process using the Main class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec for Redundancy\nDESCRIPTION: Example configuration showing how to set up redundant realtime nodes using linear shardSpec with identical partitionNum values. This allows multiple nodes to store the same segment data for redundancy.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: React Use Sync External Store Shim License\nDESCRIPTION: License notice for the React use-sync-external-store-shim production build, released under the MIT license by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Sending a Query with Jackson Smile Format\nDESCRIPTION: Example of how to send a query to Druid using the Jackson Smile binary format for more efficient data transfer. The query content is still JSON, but the response will be in Smile format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/querying.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Servers Information in Druid SQL\nDESCRIPTION: Simple SQL query to retrieve information about all servers in the Druid cluster from the sys.servers table. This provides details about server configurations and capacities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Segment Load Queue Path\nDESCRIPTION: ZooKeeper path where Coordinator writes instructions for Historical processes to load or drop segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier\n```\n\n----------------------------------------\n\nTITLE: Get Lookups Response Example\nDESCRIPTION: JSON response format for getting all active lookups on a process, showing the lookup name, version, and extractor factory configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/lookups.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"site_id_customer2\": {\n    \"version\": \"v1\",\n    \"lookupExtractorFactory\": {\n      \"type\": \"map\",\n      \"map\": {\n        \"AHF77\": \"Home\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Comment\nDESCRIPTION: License comment for the NProgress library by Rico Sta. Cruz, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Running the DumpSegment Tool in Apache Druid\nDESCRIPTION: Command to execute the DumpSegment tool by specifying the segment directory and output file. This tool is used for debugging purposes to extract segment data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/dump-segment.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools dump-segment \\\n  --directory /home/druid/path/to/segment/ \\\n  --out /home/druid/output.txt\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: MIT license declaration for React Is production bundle\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Native JSON Request Log Example - TSV Format\nDESCRIPTION: Example of a TSV-formatted request log entry for a native JSON query showing timestamp, remote address, query details, and context information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_5\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1   {\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikiticker\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"LegacyDimensionSpec\",\"dimension\":\"page\",\"outputName\":\"page\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"LegacyTopNMetricSpec\",\"metric\":\"count\"},\"threshold\":10,\"intervals\":{\"type\":\"LegacySegmentSpec\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"count\"}],\"postAggregations\":[],\"context\":{\"queryId\":\"74c2d540-d700-4ebd-b4a9-3d02397976aa\"},\"descending\":false}    {\"query/time\":100,\"query/bytes\":800,\"success\":true,\"identity\":\"user1\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authenticator in Druid\nDESCRIPTION: Basic configuration to enable Kerberos authenticator in Druid's authentication chain.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyKerberosAuthenticator\"]\n\ndruid.auth.authenticator.MyKerberosAuthenticator.type=kerberos\n```\n\n----------------------------------------\n\nTITLE: Using Dimension Selector Filter in Druid groupBy Query\nDESCRIPTION: Shows how to use a dimSelector filter in a having clause to match rows with dimension values equal to a specified value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n       {\n            \"type\": \"dimSelector\",\n            \"dimension\": \"<dimension>\",\n            \"value\": <dimension_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool with Selective Options in Apache Druid\nDESCRIPTION: Command to run the ResetCluster tool with specific options to selectively reset parts of the Druid cluster. The command requires the Druid classpath with configuration files and allows targeting specific components with flags.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React use-sync-external-store-shim in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the React use-sync-external-store-shim production build, attributing it to Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: S3 Deep Storage Configuration\nDESCRIPTION: Example configuration settings for using Amazon S3 as Druid's deep storage system. Shows required property settings for storage and indexing service logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-s3-extensions\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=s3\ndruid.storage.bucket=your-bucket\ndruid.storage.baseKey=druid/segments\ndruid.s3.accessKey=...\ndruid.s3.secretKey=...\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=s3\ndruid.indexer.logs.s3Bucket=your-bucket\ndruid.indexer.logs.s3Prefix=druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Local Deep Storage Configuration Example\nDESCRIPTION: Example configuration showing local storage type and directory settings for Druid deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/export-metadata.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=local\ndruid.storage.storageDirectory=/druid/segments\n```\n\n----------------------------------------\n\nTITLE: Managing Real-time Ingestion in Druid using Java\nDESCRIPTION: OverlordResource.java is the entry point for coordination logic in Druid's indexing service, particularly for real-time ingestion. FirehoseFactory.java classes handle data loading, while RealtimeManager.java and RealtimePlumber.java manage core ingestion logic and data hand-off.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/overview.md#2025-04-09_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nOverlordResource.java\nFirehoseFactory.java\nRealtimeManager.java\nRealtimePlumber.java\n```\n\n----------------------------------------\n\nTITLE: Inserting Indexing Service Architecture Diagram\nDESCRIPTION: Embeds an image showing the architecture of the Druid indexing service using Markdown syntax.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/indexing-service.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n![Indexing Service](../../img/indexing_service.png \"Indexing Service\")\n```\n\n----------------------------------------\n\nTITLE: Preparing sample data for Kafka ingestion in Bash\nDESCRIPTION: Commands to extract the compressed sample Wikipedia data file that will be used for streaming into Kafka.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial\ngunzip -k wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Example Druid Segment Identifier with Partition Number\nDESCRIPTION: Shows the format of a Druid segment identifier that includes a partition number. The identifier consists of datasource name, time interval, version number, and partition number.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/index.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Broadcast Rule in Apache Druid\nDESCRIPTION: The Interval Broadcast Rule co-locates segments within a specific time interval. Only segments within the specified ISO-8601 interval will be broadcasted to servers holding segments of the co-located data sources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByInterval\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Druid Column Data Structures\nDESCRIPTION: Example showing the three core data structures used to represent a dimension column in Druid: dictionary mapping, column data array, and bitmaps for each unique value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/segments.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   0,\n   1,\n   1]\n\n3: Bitmaps - one for each unique value of the column\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,0,1,1]\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool with Selective Options in Apache Druid\nDESCRIPTION: Command to run the ResetCluster tool with specific options to selectively reset parts of the Druid cluster. The command requires the Druid classpath with configuration files and allows targeting specific components with flags.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Specifying Segment Identifier Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path where processes create ephemeral znodes for individual segments they are serving.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_\n```\n\n----------------------------------------\n\nTITLE: Filter Configuration in Druid Transform Spec\nDESCRIPTION: Demonstrates how to configure a selector filter to only ingest rows where the country column equals 'United States'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"country\",\n  \"value\": \"United States\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Front Matter for Druid Documentation Page\nDESCRIPTION: YAML front matter block defining the layout and title for a Druid documentation page about basic cluster tuning.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Basic Cluster Tuning\"\n---\n```\n\n----------------------------------------\n\nTITLE: SegmentWriteOutMediumFactory Configuration\nDESCRIPTION: Configuration object for specifying the segment write-out medium factory type in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Node Information in Apache Druid\nDESCRIPTION: These endpoints retrieve information about a Druid node, including version, loaded extensions, memory usage, health status, and configuration properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /status\nGET /status/health\nGET /status/properties\n```\n\n----------------------------------------\n\nTITLE: HDFS Directory Setup Commands\nDESCRIPTION: Commands to create and configure required HDFS directories for Druid data ingestion\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/hadoop/bin\n./hadoop fs -mkdir /druid\n./hadoop fs -mkdir /druid/segments\n./hadoop fs -mkdir /quickstart\n./hadoop fs -chmod 777 /druid\n./hadoop fs -chmod 777 /druid/segments\n./hadoop fs -chmod 777 /quickstart\n./hadoop fs -chmod -R 777 /tmp\n./hadoop fs -chmod -R 777 /user\n./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Retrieving Metadata Store Information in Apache Druid\nDESCRIPTION: These endpoints fetch metadata information about datasources and segments from the Druid metadata store, including listing datasources, retrieving full metadata, and querying segment information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_3\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/metadata/datasources\nGET /druid/coordinator/v1/metadata/datasources?includeDisabled\nGET /druid/coordinator/v1/metadata/datasources?full\nGET /druid/coordinator/v1/metadata/datasources/{dataSourceName}\nGET /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments\nGET /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments?full\nGET /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments/{segmentId}\n```\n\n----------------------------------------\n\nTITLE: Configuring TimedShutoffFirehose in Druid\nDESCRIPTION: JSON configuration for TimedShutoffFirehose which creates a firehose that automatically shuts down at a specified time. It wraps another firehose (the delegate) and adds scheduled shutdown functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/firehose.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"timed\",\n    \"shutoffTime\": \"2015-08-25T01:26:05.119Z\",\n    \"delegate\": {\n          \"type\": \"receiver\",\n          \"serviceName\": \"eventReceiverServiceName\",\n          \"bufferSize\": 100000\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Process Storage Capacity in Druid\nDESCRIPTION: This code snippet shows how to configure the storage capacity for Druid Historical processes. It sets the segment cache location and maximum size, as well as the server's maximum size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/faq.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}]\n-Ddruid.server.maxSize=500000000000\n```\n\n----------------------------------------\n\nTITLE: Supervisor Service Launch Output in Bash\nDESCRIPTION: Example output showing the startup of all Druid services via the supervise script. Each service is launched with its configuration and a dedicated log file in the var/sv directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/index.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n[Wed Feb 27 12:46:13 2019] Running command[zk], logging to[/apache-druid-0.14.0-incubating/var/sv/zk.log]: bin/run-zk quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[coordinator], logging to[/apache-druid-0.14.0-incubating/var/sv/coordinator.log]: bin/run-druid coordinator quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[broker], logging to[/apache-druid-0.14.0-incubating/var/sv/broker.log]: bin/run-druid broker quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[router], logging to[/apache-druid-0.14.0-incubating/var/sv/router.log]: bin/run-druid router quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[historical], logging to[/apache-druid-0.14.0-incubating/var/sv/historical.log]: bin/run-druid historical quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[overlord], logging to[/apache-druid-0.14.0-incubating/var/sv/overlord.log]: bin/run-druid overlord quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[middleManager], logging to[/apache-druid-0.14.0-incubating/var/sv/middleManager.log]: bin/run-druid middleManager quickstart/tutorial/conf\n```\n\n----------------------------------------\n\nTITLE: Defining JavaScript Filter in Apache Druid JSON\nDESCRIPTION: Shows the JSON structure for a JavaScript filter in Apache Druid. This filter uses a JavaScript function to match dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : <dimension_string>,\n  \"function\" : \"function(value) { <...> }\"\n}\n```\n\n----------------------------------------\n\nTITLE: HLL Sketch Build Aggregator Configuration\nDESCRIPTION: JSON configuration for the HLLSketchBuild aggregator used during ingestion to create HLL sketch objects.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"HLLSketchBuild\",\n  \"name\" : <output name>,\n  \"fieldName\" : <metric name>,\n  \"lgK\" : <size and accuracy parameter>,\n  \"tgtHllType\" : <target HLL type>\n }\n```\n\n----------------------------------------\n\nTITLE: Running the Druid Real-time Server Process in Java\nDESCRIPTION: Command to start a Druid real-time server process. This initializes a real-time node that will handle immediate data indexing with data becoming instantly available for querying.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: Configuring Numbered Sharding in Druid\nDESCRIPTION: Configuration for numbered sharding strategy that requires sequential partition numbering and explicit partition count specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"numbered\",\n        \"partitionNum\": 0,\n        \"partitions\": 2\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining OrderByColumnSpec in Apache Druid groupBy Queries\nDESCRIPTION: This JSON snippet illustrates the structure of an OrderByColumnSpec used within a DefaultLimitSpec. It specifies a dimension or metric name, sort direction, and optional dimensionOrder for custom sorting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/limitspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dimension\" : \"<Any dimension or metric name>\",\n    \"direction\" : <\"ascending\"|\"descending\">,\n    \"dimensionOrder\" : <\"lexicographic\"(default)|\"alphanumeric\"|\"strlen\"|\"numeric\">\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Apache Druid Historical Process\nDESCRIPTION: Command to start the Druid Historical process using the Main CLI. This command initiates a Historical server that will load and serve segments as part of a Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/historical.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server historical\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Cache Locations in Apache Druid\nDESCRIPTION: This snippet demonstrates how to configure segment cache locations for a Historical process in Apache Druid. It specifies the path, maximum size, and optional free space percentage for storing segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n\"druid.segmentCache.locations=[{\\\"path\\\": \\\"/mnt/druidSegments\\\", \\\"maxSize\\\": 10000, \\\"freeSpacePercent\\\": 1.0}]\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Page Front Matter\nDESCRIPTION: YAML front matter defining the page layout and title for Jekyll processing\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/cassandra.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Cassandra\"\n---\n```\n\n----------------------------------------\n\nTITLE: Adding Jersey Resources to Druid\nDESCRIPTION: Shows how to bind a new Jersey resource class in a module using the Jerseys utility class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nJerseys.addResource(binder, NewResource.class);\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Historical/Realtime Process Announcement Path\nDESCRIPTION: ZooKeeper path where Historical and Realtime processes create ephemeral znodes to announce their presence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.announcementsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: SQL Like Pattern Example\nDESCRIPTION: Example of using the LIKE operator in Druid expressions for pattern matching\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/misc/math-expr.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nexpr LIKE pattern\n```\n\n----------------------------------------\n\nTITLE: Configuring Noop Task in Apache Druid\nDESCRIPTION: JSON configuration for a Noop task that is used for testing purposes. The task can be configured with an optional ID, segment interval, sleep duration, and test firehose connection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"noop\",\n    \"id\": <optional_task_id>,\n    \"interval\" : <optional_segment_interval>,\n    \"runTime\" : <optional_millis_to_sleep>,\n    \"firehose\": <optional_firehose_to_test_connect>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeMax Aggregator for Druid Ingestion\nDESCRIPTION: JSON configuration for including a timeMax aggregator during data ingestion in Druid. The fieldName typically refers to the column specified in the timestamp spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMax\",\n    \"name\": \"tmax\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Comment\nDESCRIPTION: License comment for the React-DOM production module, version 17.0.2, released under the MIT license by Facebook, Inc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Store Task Logs in Druid\nDESCRIPTION: Configuration for storing task logs in Azure Blob Store. Requires the druid-azure-extensions extension and uses the same storage account as deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_16\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.container=none\ndruid.indexer.logs.prefix=none\n```\n\n----------------------------------------\n\nTITLE: Binding Query Components in Java\nDESCRIPTION: Example of how to bind custom QueryToolChest and QueryRunnerFactory implementations for a new query type (SegmentMetadataQuery) using Guice bindings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nDruidBinders.queryToolChestBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryQueryToolChest.class);\n    \nDruidBinders.queryRunnerFactoryBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryRunnerFactory.class);\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Segment Load/Drop Instructions in Druid\nDESCRIPTION: Specifies the ZooKeeper path where the Coordinator writes instructions for Historical processes to load or drop segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier\n```\n\n----------------------------------------\n\nTITLE: Numeric Greater Than Filter in Druid\nDESCRIPTION: Example of using a numeric greater than filter in Having clause to filter results based on aggregate metric values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"greaterThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Logging Properties\nDESCRIPTION: Properties for configuring where Druid task logs are stored and how they are retained. Supports various storage backends including S3, Azure, Google Cloud Storage, HDFS and local filesystem.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.type=file\ndruid.indexer.logs.kill.enabled=false\ndruid.indexer.logs.kill.durationToRetain=None\ndruid.indexer.logs.kill.initialDelay=300000\ndruid.indexer.logs.kill.delay=21600000\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license declaration for React's scheduler.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Aggregator in Apache Druid\nDESCRIPTION: The JavaScript aggregator computes an arbitrary JavaScript function over a set of columns (both metrics and dimensions). It requires three JavaScript functions: fnAggregate (updates partial aggregates), fnCombine (combines partial results), and fnReset (provides initial value).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"javascript\",\n  \"name\": \"<output_name>\",\n  \"fieldNames\"  : [ <column1>, <column2>, ... ],\n  \"fnAggregate\" : \"function(current, column1, column2, ...) {\n                     <updates partial aggregate (current) based on the current row values>\n                     return <updated partial aggregate>\n                   }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return <combined partial results>; }\",\n  \"fnReset\"     : \"function()                   { return <initial value>; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Master Server Without Zookeeper\nDESCRIPTION: Starts the Druid Master server processes without a local Zookeeper instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/cluster.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-cluster-master-no-zk-server\n```\n\n----------------------------------------\n\nTITLE: Example Druid Supervise Script Output\nDESCRIPTION: Example console output when running the supervise script to start Druid services. It shows the various Druid components being launched with their corresponding log file locations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/index.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n[Wed Feb 27 12:46:13 2019] Running command[zk], logging to[/apache-druid-0.14.2-incubating/var/sv/zk.log]: bin/run-zk quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[coordinator], logging to[/apache-druid-0.14.2-incubating/var/sv/coordinator.log]: bin/run-druid coordinator quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[broker], logging to[/apache-druid-0.14.2-incubating/var/sv/broker.log]: bin/run-druid broker quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[router], logging to[/apache-druid-0.14.2-incubating/var/sv/router.log]: bin/run-druid router quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[historical], logging to[/apache-druid-0.14.2-incubating/var/sv/historical.log]: bin/run-druid historical quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[overlord], logging to[/apache-druid-0.14.2-incubating/var/sv/overlord.log]: bin/run-druid overlord quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[middleManager], logging to[/apache-druid-0.14.2-incubating/var/sv/middleManager.log]: bin/run-druid middleManager quickstart/tutorial/conf\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kinesis Tasks\nDESCRIPTION: Formula for calculating the minimum worker capacity needed to support concurrent reading and publishing tasks in Kinesis indexing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Configuring Noop Task in Apache Druid\nDESCRIPTION: Defines a test task that sleeps for a specified duration. Used primarily for testing purposes with optional parameters for task ID, segment interval, runtime duration, and firehose testing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"noop\",\n    \"id\": <optional_task_id>,\n    \"interval\" : <optional_segment_interval>,\n    \"runTime\" : <optional_millis_to_sleep>,\n    \"firehose\": <optional_firehose_to_test_connect>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Properties for Cassandra Deep Storage\nDESCRIPTION: Properties to be added to Druid's Historical and realtime runtime properties files to enable Cassandra as the deep storage backend. Specifies the required extension, storage type, host, and keyspace.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-cassandra-storage\"]\ndruid.storage.type=c*\ndruid.storage.host=localhost:9160\ndruid.storage.keyspace=druid\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query with Grand Totals in Apache Druid\nDESCRIPTION: Example showing how to enable grand totals in a timeseries query by adding the grandTotal context parameter. This will include an additional summary row in the results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/timeseriesquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Live Row Statistics JSON in Apache Druid\nDESCRIPTION: Example of live row statistics JSON, showing moving averages and totals for processed, unparseable, thrown away, and processed with error rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/reports.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"movingAverages\": {\n    \"buildSegments\": {\n      \"5m\": {\n        \"processed\": 3.392158326408501,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"15m\": {\n        \"processed\": 1.736165476881023,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"1m\": {\n        \"processed\": 4.206417693750045,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      }\n    }\n  },\n  \"totals\": {\n    \"buildSegments\": {\n      \"processed\": 1994,\n      \"processedWithError\": 0,\n      \"thrownAway\": 0,\n      \"unparseable\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Extensions in Druid Properties\nDESCRIPTION: Configuration example showing how to load bundled core extensions like postgresql-metadata-storage and druid-hdfs-storage using the druid.extensions.loadList property.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/including-extensions.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\", \"druid-hdfs-storage\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Node SpecFile in Apache Druid\nDESCRIPTION: Complete JSON configuration example for setting up a Realtime node specFile in Druid. Includes dataSchema configuration for Wikipedia data source, Kafka firehose setup, and tuning parameters for real-time ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [{\n        \"type\" : \"count\",\n        \"name\" : \"count\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"added\",\n        \"fieldName\" : \"added\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"deleted\",\n        \"fieldName\" : \"deleted\"\n      }, {\n        \"type\" : \"doubleSum\",\n        \"name\" : \"delta\",\n        \"fieldName\" : \"delta\"\n      }],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\"\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"realtime\",\n      \"firehose\": {\n        \"type\": \"kafka-0.8\",\n        \"consumerProps\": {\n          \"zookeeper.connect\": \"localhost:2181\",\n          \"zookeeper.connection.timeout.ms\" : \"15000\",\n          \"zookeeper.session.timeout.ms\" : \"15000\",\n          \"zookeeper.sync.time.ms\" : \"5000\",\n          \"group.id\": \"druid-example\",\n          \"fetch.message.max.bytes\" : \"1048586\",\n          \"auto.offset.reset\": \"largest\",\n          \"auto.commit.enable\": \"false\"\n        },\n        \"feed\": \"wikipedia\"\n      },\n      \"plumber\": {\n        \"type\": \"realtime\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\" : \"realtime\",\n      \"maxRowsInMemory\": 1000000,\n      \"intermediatePersistPeriod\": \"PT10M\",\n      \"windowPeriod\": \"PT10M\",\n      \"basePersistDirectory\": \"\\/tmp\\/realtime\\/basePersist\",\n      \"rejectionPolicy\": {\n        \"type\": \"serverTime\"\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data in Apache Druid\nDESCRIPTION: This command loads an initial dataset into a datasource called 'updates-tutorial' using a specified index task JSON file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Asynchronous Logging in Druid with Log4j2\nDESCRIPTION: XML configuration for Log4j2 that implements asynchronous logging for chatty Druid classes. This configuration helps reduce logging overhead by using async loggers for specific Druid components while maintaining synchronous logging for others.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/performance-faq.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <AsyncLogger name=\"org.apache.druid.curator.inventory.CuratorInventoryManager\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name=\"org.apache.druid.client.BatchServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->\n    <AsyncLogger name=\"org.apache.druid.client.ServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name =\"org.apache.druid.java.util.http.client.pool.ChannelResourceFactory\" level=\"info\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Binding Query Components in Java\nDESCRIPTION: Example of how to bind custom QueryToolChest and QueryRunnerFactory implementations for a new query type (SegmentMetadataQuery) using Guice bindings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nDruidBinders.queryToolChestBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryQueryToolChest.class);\n    \nDruidBinders.queryRunnerFactoryBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryRunnerFactory.class);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Interval Information using GET HTTP Requests\nDESCRIPTION: A collection of GET endpoints for retrieving interval information for datasources, including total size and count metrics with different levels of detail through optional parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_8\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/intervals\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/intervals/{interval}\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/intervals/{interval}?simple\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/intervals/{interval}?full\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Server for Druid Stream Ingestion\nDESCRIPTION: Command to start Tranquility server with a configuration file. This allows sending data to Druid without developing a JVM app.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-push.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility server -configFile <path_to_config_file>/server.json\n```\n\n----------------------------------------\n\nTITLE: Accessing Live Row Stats Endpoint\nDESCRIPTION: HTTP endpoint for retrieving real-time row statistics from a running task\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/reports.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/rowStats\n```\n\n----------------------------------------\n\nTITLE: Submitting Hadoop Batch Ingestion Task in Druid\nDESCRIPTION: Command to submit a Hadoop-based batch ingestion task to Druid for loading Wikipedia edit data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Parsing Extraction Function in Druid\nDESCRIPTION: The Time Parsing extraction function parses dimension values as timestamps using a specified input format and returns them formatted using a given output format. It supports both Joda and SimpleDateFormat patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"time\",\n  \"timeFormat\" : <input_format>,\n  \"resultFormat\" : <output_format>,\n  \"joda\" : <true, false> }\n```\n\n----------------------------------------\n\nTITLE: Estimate with Bounds Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for post-aggregator to get distinct count estimate with error bounds\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimateAndBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,\n  \"numStdDevs\", <number from 1 to 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing MySQL Metadata Tables for Druid\nDESCRIPTION: Command for initializing Druid metadata tables in MySQL. It uses the mysql-metadata-storage extension and sets necessary connection parameters like URI, username, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metadata-migration.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ${DRUID_ROOT}\njava -classpath \"lib/*\" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory=\"extensions\" -Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\"] -Ddruid.metadata.storage.type=mysql org.apache.druid.cli.Main tools metadata-init --connectURI=\"<mysql-uri>\" --user <user> --password <pass> --base druid\n```\n\n----------------------------------------\n\nTITLE: Complete Ingestion Spec for Apache Druid Native Batch Task\nDESCRIPTION: This JSON configuration represents a complete ingestion spec for a native batch task in Apache Druid. It includes dataSchema, ioConfig, and parser configurations for ingesting netflow data with specific dimensions and metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]\n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/\",\n        \"filter\" : \"ingestion-tutorial-data.json\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Compaction Task JSON in Apache Druid\nDESCRIPTION: This example demonstrates a simple Compaction Task configuration for Apache Druid. It specifies the task type as 'compact', sets the data source to 'wikipedia', and defines the interval for segment merging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"compact\",\n  \"dataSource\" : \"wikipedia\",\n  \"interval\" : \"2017-01-01/2018-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Compaction Task with Day Granularity\nDESCRIPTION: JSON specification for a compaction task that changes segment granularity from hourly to daily, resulting in a single segment for the entire day of data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"segmentGranularity\": \"DAY\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Network Flow Event Data in JSON Format\nDESCRIPTION: Example dataset containing network flow events with source and destination IP addresses, packet counts, and byte counts. This data will be used to demonstrate Druid's roll-up functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":20,\"bytes\":9024}\n{\"timestamp\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":255,\"bytes\":21133}\n{\"timestamp\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":11,\"bytes\":5780}\n{\"timestamp\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":38,\"bytes\":6289}\n{\"timestamp\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":377,\"bytes\":359971}\n{\"timestamp\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":49,\"bytes\":10204}\n{\"timestamp\":\"2018-01-02T21:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":38,\"bytes\":6289}\n{\"timestamp\":\"2018-01-02T21:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":123,\"bytes\":93999}\n{\"timestamp\":\"2018-01-02T21:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":12,\"bytes\":2818}\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Extraction Function for String Manipulation\nDESCRIPTION: This JavaScript extraction function example shows how to extract the first three characters from a dimension value using a custom JavaScript function. It demonstrates basic string manipulation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str.substr(0, 3); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Druid SQL Query Plan Execution Example\nDESCRIPTION: Shows the output of executing an EXPLAIN PLAN FOR command in the Druid SQL command line interface (dsql). The output displays the native JSON TopN query that will be executed for the given SQL query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n\n PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\n DruidQueryRel(query=[{\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"page\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":10,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}],\"postAggregations\":[],\"context\":{},\"descending\":false}], signature=[{d0:STRING, a0:LONG}]) \n\nRetrieved 1 row in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: Running the Real-time Process in Apache Druid\nDESCRIPTION: Command to start a Druid real-time server process. This command initializes a real-time node that will ingest streaming data and make it immediately available for querying.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: License Declaration for NProgress Library\nDESCRIPTION: MIT license declaration for the NProgress library created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Downloading and Setting up Tranquility Server\nDESCRIPTION: Commands to download, extract and set up Tranquility distribution package for Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz\ntar -xzf tranquility-distribution-0.8.3.tgz\nmv tranquility-distribution-0.8.3 tranquility\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for Prism Library\nDESCRIPTION: This snippet contains the copyright notice and license information for the Prism syntax highlighting library by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Same Interval Merge Task in Apache Druid\nDESCRIPTION: Deprecated task that serves as a shortcut for merging all segments within a specified interval. Simplifies the merge task configuration by using an interval instead of explicit segment list.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"same_interval_merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"interval\": <DataSegment objects in this interval are going to be merged>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: SBT Assembly Plugin Configuration\nDESCRIPTION: Configuration for the SBT assembly plugin used to build a custom Druid fat jar that excludes conflicting Jackson dependencies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n```\n\n----------------------------------------\n\nTITLE: React-DOM License\nDESCRIPTION: Copyright notice and MIT license declaration for the react-dom.production.min.js file from React.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Druid IndexSpec Table Schema\nDESCRIPTION: Markdown table defining the IndexSpec configuration options that control segment storage format, including bitmap types and compression formats.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|bitmap|Object|Compression format for bitmap indexes. Should be a JSON object; see below for options.|no (defaults to Concise)|\n|dimensionCompression|String|Compression format for dimension columns. Choose from `LZ4`, `LZF`, or `uncompressed`.|no (default == `LZ4`)|\n|metricCompression|String|Compression format for metric columns.|no (default == `LZ4`)|\n|longEncoding|String|Encoding format for metric and dimension columns with type long.|no (default == `longs`)|\n```\n\n----------------------------------------\n\nTITLE: Configuring Common GroupBy Query Settings in Druid\nDESCRIPTION: Common configuration properties for all GroupBy query strategies in Druid. These settings control default strategy selection and thread utilization during result merging.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_58\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.defaultStrategy=v2\ndruid.query.groupBy.singleThreaded=false\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting ZooKeeper on the Coordination Server\nDESCRIPTION: Commands to download, install, and start ZooKeeper on the coordination server. ZooKeeper is required for Druid service coordination and cluster management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\ncd zookeeper-3.4.11\ncp conf/zoo_sample.cfg conf/zoo.cfg\n./bin/zkServer.sh start\n```\n\n----------------------------------------\n\nTITLE: Task Shutdown Response in JSON\nDESCRIPTION: Example JSON response after shutting down a task via the /druid/worker/v1/task/{taskid}/shutdown endpoint, showing the task ID that was shut down.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"}\n```\n\n----------------------------------------\n\nTITLE: Starting the Apache Druid Historical Process via Command Line\nDESCRIPTION: Command to start a Druid Historical server process using the main CLI entry point. This launches the Historical process which is responsible for loading and serving data segments in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/historical.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server historical\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Header\nDESCRIPTION: Copyright notice and MIT license declaration for the object-assign library created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Implementing Regex Search in Druid\nDESCRIPTION: Specifies a regex-based search query that matches if any part of a dimension value matches the specified pattern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"regex\",\n  \"pattern\" : \"some_pattern\"\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Coordination Metrics\nDESCRIPTION: Table describing coordination-related metrics for segment management and balancing\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/assigned/count`|Number of segments assigned to be loaded in the cluster.|tier.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Adding Host Entry for Hadoop Docker Container\nDESCRIPTION: Command showing the entry that needs to be added to the host machine's /etc/hosts file to enable proper communication with the Hadoop container.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n127.0.0.1 druid-hadoop-demo\n```\n\n----------------------------------------\n\nTITLE: Resetting Kafka State\nDESCRIPTION: Command to clear Kafka logs when resetting the cluster state.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/index.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /tmp/kafka-logs\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Query in Druid\nDESCRIPTION: This snippet demonstrates how to cancel a running query in Druid using its unique identifier. It uses a DELETE HTTP request to the appropriate endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/querying.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://host:port/druid/v2/abc123\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Load Rule in Druid\nDESCRIPTION: A Period Load Rule configuration that specifies how segments within a rolling time period should be retained across different tiers in a Druid cluster. This rule applies only to segments matching the defined ISO-8601 period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByPeriod\",\n  \"period\" : \"P1M\",\n  \"tieredReplicants\": {\n      \"hot\": 1,\n      \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Shade Plugin for Fat Jar Creation in XML\nDESCRIPTION: This XML configuration for the Maven Shade plugin creates a fat jar by relocating Jackson packages and assembling all dependencies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-shade-plugin</artifactId>\n     <executions>\n         <execution>\n             <phase>package</phase>\n             <goals>\n                 <goal>shade</goal>\n             </goals>\n             <configuration>\n                 <outputFile>\n                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                 </outputFile>\n                 <relocations>\n                     <relocation>\n                         <pattern>com.fasterxml.jackson</pattern>\n                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                     </relocation>\n                 </relocations>\n                 <artifactSet>\n                     <includes>\n                         <include>*:*</include>\n                     </includes>\n                 </artifactSet>\n                 <filters>\n                     <filter>\n                         <artifact>*:*</artifact>\n                         <excludes>\n                             <exclude>META-INF/*.SF</exclude>\n                             <exclude>META-INF/*.DSA</exclude>\n                             <exclude>META-INF/*.RSA</exclude>\n                         </excludes>\n                     </filter>\n                 </filters>\n                 <transformers>\n                     <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                 </transformers>\n             </configuration>\n         </execution>\n     </executions>\n </plugin>\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Task for Segment Merging in Apache Druid (Deprecated)\nDESCRIPTION: JSON configuration for a Merge task in Apache Druid. This deprecated task merges a list of segments, with options to control rollup behavior, specify aggregations, and define the segments to be merged.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"segments\": <JSON list of DataSegment objects to merge>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Including Cassandra Storage Extension in Druid\nDESCRIPTION: Instructions for including the druid-cassandra-storage extension to enable Apache Cassandra as a deep storage option for Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/cassandra.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo use this Apache Druid (incubating) extension, make sure to [include](../../operations/including-extensions.html) `druid-cassandra-storage` extension.\n```\n\n----------------------------------------\n\nTITLE: Implementing DoubleMin Aggregator in Apache Druid\nDESCRIPTION: Shows how to set up a doubleMin aggregator in Druid. This aggregator computes the minimum of all metric values and Double.POSITIVE_INFINITY.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Druid Segment Metadata JSON Structure\nDESCRIPTION: Example JSON structure showing the payload format for segment metadata storage. Contains segment properties like dataSource, interval, version, and loading specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"dataSource\":\"wikipedia\",\n \"interval\":\"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z\",\n \"version\":\"2012-05-24T00:10:00.046Z\",\n \"loadSpec\":{\n    \"type\":\"s3_zip\",\n    \"bucket\":\"bucket_for_segment\",\n    \"key\":\"path/to/segment/on/s3\"\n },\n \"dimensions\":\"comma-delimited-list-of-dimension-names\",\n \"metrics\":\"comma-delimited-list-of-metric-names\",\n \"shardSpec\":{\"type\":\"none\"},\n \"binaryVersion\":9,\n \"size\":size_of_segment,\n \"identifier\":\"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Query Error Response Structure in Apache Druid\nDESCRIPTION: This JSON structure represents the format of an error response when a query fails in Druid. It includes fields for error code, error message, error class, and the host where the error occurred.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/querying.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\" : \"Query timeout\",\n  \"errorMessage\" : \"Timeout waiting for task.\",\n  \"errorClass\" : \"java.util.concurrent.TimeoutException\",\n  \"host\" : \"druid1.example.com:8083\"\n}\n```\n\n----------------------------------------\n\nTITLE: Pull-deps Command with Default Version\nDESCRIPTION: Example showing how to use the --defaultVersion parameter to specify a common version for multiple extensions without explicitly including the version in each coordinate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/pull-deps.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.13.0-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Schema-less Dimension Example with HyperUnique Metric\nDESCRIPTION: Example showing data structure and metrics specification for handling unique ID counting with schema-less dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"device_id_dim\":123, \"device_id_met\":123}\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"hyperUnique\", \"name\" : \"devices\", \"fieldName\" : \"device_id_met\" }\n```\n\n----------------------------------------\n\nTITLE: Follow-up Paginated Select Query with Manual Offset Increment in Druid\nDESCRIPTION: Example of a follow-up paginated Select query using the pagingIdentifiers from a previous query result with the offset manually incremented by 1, as required when fromNext is set to false.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/select-query.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n {\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {\"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 5}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Load Rule in Druid\nDESCRIPTION: A Period Load Rule configuration that specifies how segments within a rolling time period should be retained across different tiers in a Druid cluster. This rule applies only to segments matching the defined ISO-8601 period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByPeriod\",\n  \"period\" : \"P1M\",\n  \"tieredReplicants\": {\n      \"hot\": 1,\n      \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Segment Information for Specific Intervals in Apache Druid\nDESCRIPTION: These POST endpoints allow querying segment information for specific time intervals in a datasource, returning either a list of segments or full segment metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_4\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments\nPOST /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments?full\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Attribution\nDESCRIPTION: MIT License attribution for React's scheduler production build developed by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JSDoc\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Using DistinctCount Aggregator in GroupBy Query with Apache Druid\nDESCRIPTION: Example of a GroupBy query that uses the distinctCount aggregator to count unique visitor_id values grouped by sample_dim. The query operates over a single day with all granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimensions\": \"[sample_dim]\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Same Interval Merge Task in Apache Druid\nDESCRIPTION: Deprecated task that serves as a shortcut for merging all segments within a specified interval. Simplifies the merge task configuration by using an interval instead of explicit segment list.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"same_interval_merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"interval\": <DataSegment objects in this interval are going to be merged>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Worker Task Spec State in Apache Druid (JSON)\nDESCRIPTION: Example JSON response from the subtaskspec state endpoint, containing the worker task specification, current task status, and task attempt history.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"spec\": {\n    \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\",\n    \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"context\": null,\n    \"inputSplit\": {\n      \"split\": \"/path/to/data/lineitem.tbl.5\"\n    },\n    \"ingestionSpec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"lineitem\",\n        \"parser\": {\n          \"type\": \"hadoopyString\",\n          \"parseSpec\": {\n            \"format\": \"tsv\",\n            \"delimiter\": \"|\",\n            \"timestampSpec\": {\n              \"column\": \"l_shipdate\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"l_orderkey\",\n                \"l_partkey\",\n                \"l_suppkey\",\n                \"l_linenumber\",\n                \"l_returnflag\",\n                \"l_linestatus\",\n                \"l_shipdate\",\n                \"l_commitdate\",\n                \"l_receiptdate\",\n                \"l_shipinstruct\",\n                \"l_shipmode\",\n                \"l_comment\"\n              ]\n            },\n            \"columns\": [\n              \"l_orderkey\",\n              \"l_partkey\",\n              \"l_suppkey\",\n              \"l_linenumber\",\n              \"l_quantity\",\n              \"l_extendedprice\",\n              \"l_discount\",\n              \"l_tax\",\n              \"l_returnflag\",\n              \"l_linestatus\",\n              \"l_shipdate\",\n              \"l_commitdate\",\n              \"l_receiptdate\",\n              \"l_shipinstruct\",\n              \"l_shipmode\",\n              \"l_comment\"\n            ]\n          }\n        },\n        \"metricsSpec\": [\n          {\n            \"type\": \"count\",\n            \"name\": \"count\"\n          },\n          {\n            \"type\": \"longSum\",\n            \"name\": \"l_quantity\",\n            \"fieldName\": \"l_quantity\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_extendedprice\",\n            \"fieldName\": \"l_extendedprice\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_discount\",\n            \"fieldName\": \"l_discount\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_tax\",\n            \"fieldName\": \"l_tax\",\n            \"expression\": null\n          }\n        ],\n        \"granularitySpec\": {\n          \"type\": \"uniform\",\n          \"segmentGranularity\": \"YEAR\",\n          \"queryGranularity\": {\n            \"type\": \"none\"\n          },\n          \"rollup\": true,\n          \"intervals\": [\n            \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n          ]\n        },\n        \"transformSpec\": {\n          \"filter\": null,\n          \"transforms\": []\n        }\n      },\n      \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"/path/to/data/\",\n          \"filter\": \"lineitem.tbl.5\",\n          \"parser\": null\n        },\n        \"appendToExisting\": false\n      },\n      \"tuningConfig\": {\n        \"type\": \"index_parallel\",\n        \"targetPartitionSize\": 5000000,\n        \"maxRowsInMemory\": 1000000,\n        \"maxTotalRows\": 20000000,\n        \"numShards\": null,\n        \"indexSpec\": {\n          \"bitmap\": {\n            \"type\": \"concise\"\n          },\n          \"dimensionCompression\": \"lz4\",\n          \"metricCompression\": \"lz4\",\n          \"longEncoding\": \"longs\"\n        },\n        \"maxPendingPersists\": 0,\n        \"forceExtendableShardSpecs\": false,\n        \"reportParseExceptions\": false,\n        \"pushTimeout\": 0,\n        \"segmentWriteOutMediumFactory\": null,\n        \"maxNumSubTasks\": 2147483647,\n        \"maxRetry\": 3,\n        \"taskStatusCheckPeriodMs\": 1000,\n        \"chatHandlerTimeout\": \"PT10S\",\n        \"chatHandlerNumRetries\": 5,\n        \"logParseExceptions\": false,\n        \"maxParseExceptions\": 2147483647,\n        \"maxSavedParseExceptions\": 0,\n        \"forceGuaranteedRollup\": false,\n        \"buildV9Directly\": true\n      }\n    }\n  },\n  \"currentStatus\": {\n    \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\",\n    \"type\": \"index_sub\",\n    \"createdTime\": \"2018-04-20T22:16:29.925Z\",\n    \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\",\n    \"statusCode\": \"RUNNING\",\n    \"duration\": -1,\n    \"location\": {\n      \"host\": null,\n      \"port\": -1,\n      \"tlsPort\": -1\n    },\n    \"dataSource\": \"lineitem\",\n    \"errorMsg\": null\n  },\n  \"taskHistory\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Default DimensionSpec Configuration in Druid\nDESCRIPTION: Basic configuration for dimension value transformation that returns values as-is with optional renaming and type conversion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"default\",\n  \"dimension\" : <dimension>,\n  \"outputName\": <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Z-test Query Example in Druid\nDESCRIPTION: Example JSON query demonstrating how to use both zscore2sample and pvalue2tailedZtest post aggregators together in a Druid query to calculate the statistical significance between two sample groups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  ...\n    \"postAggregations\" : {\n    \"type\"   : \"pvalue2tailedZtest\",\n    \"name\"   : \"pvalue\",\n    \"zScore\" : \n    {\n     \"type\"   : \"zscore2sample\",\n     \"name\"   : \"zscore\",\n     \"successCount1\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation1Sample\",\n         \"value\"  : 300\n       },\n     \"sample1Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation1\",\n         \"value\"  : 500\n       },\n     \"successCount2\":\n       { \"type\"   : \"constant\",\n         \"name\"   : \"successCountFromPopulation2Sample\",\n         \"value\"  : 450\n       },\n     \"sample2Size\" :\n       { \"type\"   : \"constant\",\n         \"name\"   : \"sampleSizeOfPopulation2\",\n         \"value\"  : 600\n       }\n     }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Server Properties in Druid\nDESCRIPTION: HTTP server configuration for Druid Broker including thread management, queue settings, timeouts and request limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_43\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.http.numThreads=max(10, (Number of cores * 17) / 16 + 2) + 30\ndruid.server.http.queueSize=Unbounded\ndruid.server.http.maxIdleTime=PT5M\ndruid.server.http.enableRequestLimit=false\ndruid.server.http.defaultQueryTimeout=300000\n```\n\n----------------------------------------\n\nTITLE: No Rollup Configuration in Druid\nDESCRIPTION: Shows how to configure dimensions when rollup is not being used, where all columns are specified in the dimensionsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Druid StatsD Metric Mapping Example\nDESCRIPTION: Example JSON configuration showing how to map Druid metrics to StatsD format. Demonstrates metric mapping structure with dimensions, type specification, and range conversion options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/statsd.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query/time\": { \"dimensions\": [\"dataSource\", \"type\"], \"type\": \"timer\"},\n  \"coordinator-segment/count\": { \"dimensions\": [\"dataSource\"], \"type\": \"gauge\" },\n  \"historical-segment/count\": { \"dimensions\": [\"dataSource\", \"tier\", \"priority\"], \"type\": \"gauge\" }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring log4j2 XML for Druid Logging\nDESCRIPTION: Example log4j2.xml configuration file for Druid that sets up console logging with timestamp patterns and configurable log levels. Includes optional HTTP request logging configuration that can be uncommented.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/logging.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n\n    <!-- Uncomment to enable logging of all HTTP requests\n    <Logger name=\"org.apache.druid.jetty.RequestLog\" additivity=\"false\" level=\"DEBUG\">\n        <AppenderRef ref=\"Console\"/>\n    </Logger>\n    -->\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Extensions in Druid Properties\nDESCRIPTION: Configuration example for loading built-in Druid extensions like postgresql-metadata-storage and hdfs-storage through the common.runtime.properties file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/including-extensions.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\", \"druid-hdfs-storage\"]\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Header in JavaScript\nDESCRIPTION: License header for React's use-sync-external-store-shim.production.min.js file. This component is part of the React library, developed by Facebook and licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Sample Response from Manual Task Submission\nDESCRIPTION: The expected JSON response from the Druid overlord when manually submitting an ingestion task, containing the generated task ID.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n{\"task\":\"index_wikipedia_2018-06-09T21:30:32.802Z\"}\n```\n\n----------------------------------------\n\nTITLE: Displaying Segment Identifier without Partition Number in Apache Druid\nDESCRIPTION: This code snippet demonstrates a segment identifier for partition number 0, which omits the partition number in its representation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/index.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z\n```\n\n----------------------------------------\n\nTITLE: Querying Row Count in Druid SQL\nDESCRIPTION: This SQL query counts the total number of rows in the 'compaction-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) from \"compaction-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Forever Broadcast Rule Configuration in Druid\nDESCRIPTION: Specifies indefinite co-location of segments from different data sources across Historical processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastForever\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring React Core License in JavaScript\nDESCRIPTION: This snippet declares the license information for the React core production module version 17.0.2, which is under the MIT license and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Downloading Druid Source Code\nDESCRIPTION: Git commands to clone the Apache Druid repository and navigate to the project directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/incubator-druid.git\ncd druid\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authenticator in Apache Druid\nDESCRIPTION: Basic configuration for creating a Kerberos authenticator in the authenticator chain. This establishes the foundation for SPNEGO authentication in Druid services.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyKerberosAuthenticator\"]\n\ndruid.auth.authenticator.MyKerberosAuthenticator.type=kerberos\n```\n\n----------------------------------------\n\nTITLE: Time Format Extraction Function in Druid JSON\nDESCRIPTION: Formats time dimension values according to specified format, time zone, and locale. Used for both __time dimension values and regular dimensions with ISO-8601 formatted strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"timeFormat\",\n  \"format\" : <output_format> (optional),\n  \"timeZone\" : <time_zone> (optional, default UTC),\n  \"locale\" : <locale> (optional, default current locale),\n  \"granularity\" : <granularity> (optional, default none) },\n  \"asMillis\" : <true or false> (optional) }\n```\n\n----------------------------------------\n\nTITLE: Creating Union Datasource in Apache Druid JSON\nDESCRIPTION: Shows how to create a union datasource in Apache Druid using JSON. Union datasources combine two or more table datasources, which should have the same schema. These queries must be sent to the broker/router node.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/datasource.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n       \"type\": \"union\",\n       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Variance Aggregator for Druid Ingestion\nDESCRIPTION: JSON configuration for setting up a variance aggregator during data ingestion in Apache Druid. This allows pre-aggregating variance of numeric values with options to specify the input type and estimator method.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"variance\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"inputType\" : <input_type>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy v2 Advanced Runtime Properties\nDESCRIPTION: Advanced runtime properties for GroupBy v2 including buffer configuration, hash aggregation, and parallel processing settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.bufferGrouperInitialBuckets=0\ndruid.query.groupBy.bufferGrouperMaxLoadFactor=0\ndruid.query.groupBy.forceHashAggregation=false\ndruid.query.groupBy.intermediateCombineDegree=8\ndruid.query.groupBy.numParallelCombineThreads=1\n```\n\n----------------------------------------\n\nTITLE: Classnames License Comment\nDESCRIPTION: License comment for the classnames library, which is licensed under the MIT License and created by Jed Watson.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6f6dba15.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Segment Load/Drop Queue Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path where the Coordinator writes instructions for Historical processes regarding segment loading or dropping.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Task in Apache Druid (Deprecated)\nDESCRIPTION: JSON configuration for a Merge task in Apache Druid. This deprecated task merges a list of segments, with options to specify aggregations, enable/disable rollup, and define the segments to merge along with task context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"segments\": <JSON list of DataSegment objects to merge>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Example Compaction Task Configuration in Apache Druid (JSON)\nDESCRIPTION: A simple example of a compaction task configuration in Apache Druid. This task will compact all segments of the 'wikipedia' data source for the year 2017, using default settings for segment granularity and other parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"compact\",\n  \"dataSource\" : \"wikipedia\",\n  \"interval\" : \"2017-01-01/2018-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalFirehose in Druid for File-based Ingestion\nDESCRIPTION: Configuration for the LocalFirehose which reads data from files on local disk. This firehose is splittable and can be used by native parallel index tasks where each worker task reads a file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"local\",\n    \"filter\"   : \"*.csv\",\n    \"baseDir\"  : \"/data/directory\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL User for Druid\nDESCRIPTION: Command to create a PostgreSQL user named 'druid' with password prompt for Druid metadata storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncreateuser druid -P\n```\n\n----------------------------------------\n\nTITLE: Sample Network Flow Data in JSON Format\nDESCRIPTION: Example network flow data containing IP addresses, ports, protocol numbers, and traffic metrics like packets, bytes and cost.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4}\n```\n\n----------------------------------------\n\nTITLE: Response from groupBy Query with Custom Origin and Pacific Timezone\nDESCRIPTION: This JSON response shows the result when using both a custom origin and timezone in the granularity. The timestamps in the results reflect the custom bucketing, where each bucket starts at 20:30 Pacific time. This demonstrates how the origin parameter affects the boundaries of time buckets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-29T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Demonstrates how to include the DataSketches extension in the Druid configuration file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Accessing the Druid Console URL\nDESCRIPTION: Shows the URL pattern for accessing the Druid Console through the Router process. The console is accessed by navigating to the Router's IP address and port in a web browser.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/management-uis.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Example Completion Report JSON Structure in Apache Druid\nDESCRIPTION: This JSON snippet illustrates the structure of a completion report in Apache Druid. It includes task ID, ingestion state, row statistics, and error information for different phases of ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making Authenticated Curl Requests to Druid with Kerberos\nDESCRIPTION: This command shows how to make an authenticated request to Druid HTTP endpoints using curl with SPNEGO negotiation. It stores authentication cookies for subsequent requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json' <HTTP_END_POINT>\n```\n\n----------------------------------------\n\nTITLE: Default DimensionSpec Configuration in Druid\nDESCRIPTION: Basic configuration for dimension transformation that returns values as-is with optional renaming. Supports output type specification for numeric columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"default\",\n  \"dimension\" : <dimension>,\n  \"outputName\": <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Druid Ingestion Task Configuration\nDESCRIPTION: Full configuration for a native batch ingestion task, including dataSchema, parser, metrics, and granularity specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"ingestion-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"format\" : \"iso\",\n            \"column\" : \"ts\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\n              \"srcIP\",\n              { \"name\" : \"srcPort\", \"type\" : \"long\" },\n              { \"name\" : \"dstIP\", \"type\" : \"string\" },\n              { \"name\" : \"dstPort\", \"type\" : \"long\" },\n              { \"name\" : \"protocol\", \"type\" : \"string\" }\n            ]              \n          }      \n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n        { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n        { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"HOUR\",\n        \"queryGranularity\" : \"MINUTE\",\n        \"intervals\" : [\"2018-01-01/2018-01-02\"],\n        \"rollup\" : true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: ArrayOfDoublesSketch Distinct Count with Error Bounds Post-Aggregator\nDESCRIPTION: Post-aggregator that returns a distinct count estimate with error bounds from an ArrayOfDoublesSketch. The result contains three values: the estimate, lower bound, and upper bound at the specified confidence level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimateAndBounds\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an  ArrayOfDoublesSketch (fieldAccess or another post aggregator)>,\n  \"numStdDevs\", <number from 1 to 3>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Properties for Cassandra Storage\nDESCRIPTION: Configuration properties to enable Cassandra backend in Druid historical and realtime runtime properties files. Specifies the storage type, host connection, and keyspace.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-cassandra-storage\"]\ndruid.storage.type=c*\ndruid.storage.host=localhost:9160\ndruid.storage.keyspace=druid\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor API Endpoint - Update\nDESCRIPTION: API endpoint to update an existing supervisor's configuration, triggering a graceful handoff to new tasks with updated settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for classnames\nDESCRIPTION: Copyright notice and MIT license information for the classnames library by Jed Watson.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Job Properties in Druid TuningConfig\nDESCRIPTION: This JSON snippet demonstrates how to set custom Hadoop job properties within the TuningConfig of a Druid ingestion task. It allows specifying Hadoop-specific configuration parameters to customize the behavior of the MapReduce job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n   \"tuningConfig\" : {\n     \"type\": \"hadoop\",\n     \"jobProperties\": {\n       \"<hadoop-property-a>\": \"<value-a>\",\n       \"<hadoop-property-b>\": \"<value-b>\"\n     }\n   }\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a03dfc13.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Min Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the min post-aggregator, which returns the minimum value of the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"min\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry Policy in Apache Druid Broker\nDESCRIPTION: This property defines the retry policy for queries in case of transient errors encountered by the Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_45\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.retryPolicy.numTries=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Granularity in Druid\nDESCRIPTION: JSON configuration for segment granularity settings in a Druid ingestion specification. This example sets uniform type with HOUR segment granularity for determining the time interval covered by each segment.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Uniform Granularity Spec Configuration Table\nDESCRIPTION: Defines the configuration parameters for uniform granularity specification in Druid, including segment and query granularity settings, rollup options, and intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Field | Type | Description | Required |\n|-------|------|-------------|----------|\n| segmentGranularity | string | The granularity to create segments at. | no (default == 'DAY') |\n| queryGranularity | string | The minimum granularity to be able to query results at and the granularity of the data inside the segment. E.g. a value of \"minute\" will mean that data is aggregated at minutely granularity. That is, if there are collisions in the tuple (minute(timestamp), dimensions), then it will aggregate values together using the aggregators instead of storing individual rows. A granularity of 'NONE' means millisecond granularity.| no (default == 'NONE') |\n| rollup | boolean | rollup or not | no (default == true) |\n| intervals | string | A list of intervals for the raw data being ingested. Ignored for real-time ingestion. | no. If specified, batch ingestion tasks may skip determining partitions phase which results in faster ingestion. |\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for React Scheduler\nDESCRIPTION: This comment block provides license information for the React Scheduler production build, which is released under the MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example\nDESCRIPTION: Example of an InfluxDB Line Protocol data point containing measurement, tags, metrics and timestamp. The line represents CPU usage metrics with application and region tags.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000\n```\n\n----------------------------------------\n\nTITLE: Declaring License for object-assign in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the object-assign library, created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for React use-sync-external-store-shim\nDESCRIPTION: This snippet contains the copyright notice and license information for the React use-sync-external-store-shim production file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license declaration for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for React Is\nDESCRIPTION: This comment block provides license information for the React Is production build, which is released under the MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Extracting Druid Package in Bash\nDESCRIPTION: Commands to extract the downloaded Druid package and navigate to its directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/index.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.13.0-incubating-bin.tar.gz\ncd apache-druid-0.13.0-incubating\n```\n\n----------------------------------------\n\nTITLE: React Core Copyright Notice\nDESCRIPTION: Copyright notice for the React core library, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka 0.8.x Firehose in Druid\nDESCRIPTION: Sample configuration for setting up a Kafka 0.8.x firehose in Druid. The specification includes consumer properties for Zookeeper connection, group ID configuration, message settings, and offset management, with 'wikipedia' as the sample topic name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-eight-firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\": {\n  \"type\": \"kafka-0.8\",\n  \"consumerProps\": {\n    \"zookeeper.connect\": \"localhost:2181\",\n    \"zookeeper.connection.timeout.ms\" : \"15000\",\n    \"zookeeper.session.timeout.ms\" : \"15000\",\n    \"zookeeper.sync.time.ms\" : \"5000\",\n    \"group.id\": \"druid-example\",\n    \"fetch.message.max.bytes\" : \"1048586\",\n    \"auto.offset.reset\": \"largest\",\n    \"auto.commit.enable\": \"false\"\n  },\n  \"feed\": \"wikipedia\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Substring Extraction Function without Length in Apache Druid JSON\nDESCRIPTION: This snippet demonstrates how to set up a Substring Extraction Function without specifying length in Apache Druid. It returns the remainder of the dimension value starting from the specified index.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_11\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 3 }\n```\n\n----------------------------------------\n\nTITLE: Inline Lookup with Missing Value Replacement in Druid\nDESCRIPTION: An inline lookup extraction function example that replaces missing values with a custom string 'MISSING' rather than retaining the original values. The lookup is defined inline with the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"lookup\":{\n    \"type\":\"map\",\n    \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"}\n  },\n  \"retainMissingValue\":false,\n  \"injective\":false,\n  \"replaceMissingValueWith\":\"MISSING\"\n}\n```\n\n----------------------------------------\n\nTITLE: Segment Naming Convention Example - Version 1\nDESCRIPTION: Demonstrates the naming pattern for multiple segments of version 1 covering the same time interval with different partition numbers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-01/2015-01-02_v1_1\nfoo_2015-01-01/2015-01-02_v1_2\n```\n\n----------------------------------------\n\nTITLE: Running Router Process in Java\nDESCRIPTION: Command to start the Router process in Apache Druid using the Main class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/router.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server router\n```\n\n----------------------------------------\n\nTITLE: React-Is License Declaration\nDESCRIPTION: MIT license declaration for React's react-is.production.min.js v16.13.1 library created by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Extensions in Druid Properties File\nDESCRIPTION: Example configuration in common.runtime.properties file to load PostgreSQL metadata storage and HDFS storage extensions in Apache Druid. This property needs to be added to the configuration file to load bundled extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/including-extensions.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\", \"druid-hdfs-storage\"]\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function for Regular Dimensions in Druid JSON\nDESCRIPTION: Transforms dimension values using a JavaScript function. For regular dimensions, input is passed as a string.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str.substr(0, 3); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Mount Storage in Apache Druid\nDESCRIPTION: Configuration properties required to set up local file system as the deep storage implementation in Apache Druid. This includes setting the storage type to 'local' and specifying the storage directory path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.storage.type`|local||Must be set.|\n|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Task in Apache Druid\nDESCRIPTION: Deprecated task that merges multiple segments with handling for common timestamps. Includes rollup configuration to control timestamp merging behavior and data reordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"merge\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"aggregations\": <list of aggregators>,\n    \"rollup\": <whether or not to rollup data during a merge>,\n    \"segments\": <JSON list of DataSegment objects to merge>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Coordinator Console URL\nDESCRIPTION: Shows the URL pattern for accessing the Coordinator console. This legacy interface can be accessed by navigating to the Coordinator's IP address and port in a web browser.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/management-uis.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Extensions in Druid Properties File\nDESCRIPTION: Example configuration in common.runtime.properties file to load PostgreSQL metadata storage and HDFS storage extensions in Apache Druid. This property needs to be added to the configuration file to load bundled extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/including-extensions.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\", \"druid-hdfs-storage\"]\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kinesis Indexing Tasks in Druid\nDESCRIPTION: Formula for calculating the minimum worker capacity needed to support concurrent reading and publishing tasks in a Kinesis indexing service configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Expression Transform Example in Druid\nDESCRIPTION: Shows how to create a new column 'fooPage' by concatenating 'foo' with existing page values using an expression transform.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring White-list Based Event Converter in JSON for Graphite Emitter\nDESCRIPTION: JSON configuration for the 'whiteList' event converter that sends only white-listed metrics and dimensions to Graphite. This example specifies a custom file path for the white list map.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true, \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Equal To Having Filter in Druid\nDESCRIPTION: Demonstrates using an equalTo filter in Having clause to match rows with specific aggregate values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"equalTo\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Rules from Coordinator using GET HTTP Requests\nDESCRIPTION: A collection of GET endpoints for retrieving rules information from the Druid Coordinator, including all rules, specific datasource rules, and audit history of rules with optional parameters for time intervals and entry counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_6\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/{dataSourceName}\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/{dataSourceName}?full\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/history?interval=<interval>\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/history?count=<n>\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?interval=<interval>\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantiles Post-Aggregator in Druid\nDESCRIPTION: Shows the JSON configuration for the quantilesDoublesSketchToQuantiles post-aggregator, which returns an array of quantiles for given fractions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantiles\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fractions\" : <array of fractional positions in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Large Configuration in Bash\nDESCRIPTION: Command to start Druid in the large configuration, which is designed for machines with 32 CPU cores and 256GB RAM, roughly equivalent to an Amazon i3.8xlarge instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/single-server.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-large\n```\n\n----------------------------------------\n\nTITLE: Displaying Indexing Service Metrics Table in Markdown\nDESCRIPTION: A markdown table showing various indexing service metrics in Apache Druid, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`task/run/time`|Milliseconds taken to run a task.|dataSource, taskId, taskType, taskStatus.|Varies.|\n|`task/action/log/time`|Milliseconds taken to log a task action to the audit log.|dataSource, taskId, taskType|< 1000 (subsecond)|\n|`task/action/run/time`|Milliseconds taken to execute a task action.|dataSource, taskId, taskType|Varies from subsecond to a few seconds, based on action type.|\n|`segment/added/bytes`|Size in bytes of new segments created.|dataSource, taskId, taskType, interval.|Varies.|\n|`segment/moved/bytes`|Size in bytes of segments moved/archived via the Move Task.|dataSource, taskId, taskType, interval.|Varies.|\n|`segment/nuked/bytes`|Size in bytes of segments deleted via the Kill Task.|dataSource, taskId, taskType, interval.|Varies.|\n|`task/success/count`|Number of successful tasks per emission period. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/failed/count`|Number of failed tasks per emission period. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/running/count`|Number of current running tasks. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/pending/count`|Number of current pending tasks. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/waiting/count`|Number of current waiting tasks. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Configuring InsensitiveContainsSearchQuerySpec in Druid\nDESCRIPTION: Defines a case-insensitive search that matches when any part of a dimension value contains the specified search value. This specification is useful for simple substring searches without case sensitivity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"insensitive_contains\",\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function with Length Configuration in Druid\nDESCRIPTION: Configuration for extracting substring of dimension values with specified index and length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 1, \"length\" : 4 }\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store Shim License\nDESCRIPTION: MIT License attribution for React's use-sync-external-store-shim production build, developed by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JSDoc\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Filtered Aggregator in Druid\nDESCRIPTION: Demonstrates how to set up a filtered aggregator that wraps another aggregator and only processes values matching a specified dimension filter. Useful for computing filtered and unfiltered aggregations simultaneously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"filtered\",\n  \"filter\" : {\n    \"type\" : \"selector\",\n    \"dimension\" : <dimension>,\n    \"value\" : <dimension value>\n  }\n  \"aggregator\" : <aggregation>\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Storage Schema for Druid\nDESCRIPTION: SQL create statements for the required Cassandra tables. Creates index_storage table for storing compressed segments and descriptor_storage table for segment metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Access and Compression for Hadoop Indexing in Druid\nDESCRIPTION: This JSON configuration sets properties for accessing S3 and specifying compression codecs in a Hadoop indexing task. It includes AWS access keys, S3 file system implementations, and supported compression codecs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\" : {\n   \"fs.s3.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"fs.s3n.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3n.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3n.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Interval Filter for Time Ranges in Apache Druid\nDESCRIPTION: This example demonstrates an interval filter that selects data from two specific time ranges: October 1-7, 2014 and November 15-16, 2014, using ISO 8601 interval strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Expression Transform Example in Druid\nDESCRIPTION: Shows how to create a new column 'fooPage' by concatenating 'foo' with existing page values using an expression transform.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining MomentSketch Post-Aggregator for Min/Max\nDESCRIPTION: JSON configuration for the momentSketchMin or momentSketchMax post-aggregators that retrieve minimum or maximum values from a moment sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"momentSketchMin\" | \"momentSketchMax\",\n  \"name\" : <output_name>,\n  \"field\" : <reference to moment sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File Request Logging in Apache Druid\nDESCRIPTION: Example of a TSV-formatted request log entry for a native JSON query in Druid. It shows the timestamp, remote address, query details, and context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1   {\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikiticker\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"LegacyDimensionSpec\",\"dimension\":\"page\",\"outputName\":\"page\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"LegacyTopNMetricSpec\",\"metric\":\"count\"},\"threshold\":10,\"intervals\":{\"type\":\"LegacySegmentSpec\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"count\"}],\"postAggregations\":[],\"context\":{\"queryId\":\"74c2d540-d700-4ebd-b4a9-3d02397976aa\"},\"descending\":false}    {\"query/time\":100,\"query/bytes\":800,\"success\":true,\"identity\":\"user1\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Standalone JVM Parquet Ingestion in Druid\nDESCRIPTION: Extended JSON configuration for Parquet ingestion using standalone JVM, including additional required fields like metadataUpdateSpec and segmentOutputPath.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/parquet.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"no_metrics\"\n      },\n      \"metadataUpdateSpec\": {\n        \"type\": \"postgresql\",\n        \"connectURI\": \"jdbc:postgresql://localhost/druid\",\n        \"user\" : \"druid\",\n        \"password\" : \"asdf\",\n        \"segmentTable\": \"druid_segments\"\n      },\n      \"segmentOutputPath\": \"tmp/segments\"\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"no_metrics\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"name\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      \"metricsSpec\": [{\n        \"type\": \"count\",\n        \"name\": \"count\"\n      }],\n      \"granularitySpec\": {\n        \"type\": \"uniform\",\n        \"segmentGranularity\": \"DAY\",\n        \"queryGranularity\": \"ALL\",\n        \"intervals\": [\"2015-12-31/2016-01-02\"]\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"workingPath\": \"tmp/working_path\",\n      \"partitionsSpec\": {\n        \"targetPartitionSize\": 5000000\n      },\n      \"jobProperties\" : {},\n      \"leaveIntermediate\": true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Configuration Table in Markdown\nDESCRIPTION: Markdown table defining SQL-related configuration properties for the Druid Broker, including Avatica settings and query planning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_47\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.sql.enable`|Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely.|false|\n|`druid.sql.avatica.enable`|Whether to enable JDBC querying at `/druid/v2/sql/avatica/`.|true|\n|`druid.sql.avatica.maxConnections`|Maximum number of open connections for the Avatica server. These are not HTTP connections, but are logical client connections that may span multiple HTTP connections.|50|\n|`druid.sql.avatica.maxRowsPerFrame`|Maximum number of rows to return in a single JDBC frame. Setting this property to -1 indicates that no row limit should be applied. Clients can optionally specify a row limit in their requests; if a client specifies a row limit, the lesser value of the client-provided limit and `maxRowsPerFrame` will be used.|5,000|\n|`druid.sql.avatica.maxStatementsPerConnection`|Maximum number of simultaneous open statements per Avatica client connection.|1|\n|`druid.sql.avatica.connectionIdleTimeout`|Avatica client connection idle timeout.|PT5M|\n```\n\n----------------------------------------\n\nTITLE: Configuring Lower Case Extraction Function in Druid\nDESCRIPTION: Simple configuration for converting dimension values to lower case using default locale.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"lower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Object-Assign Copyright and License Information\nDESCRIPTION: Copyright notice for the object-assign library created by Sindre Sorhus and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop InputSpec for Dataource Reindexing in Druid\nDESCRIPTION: JSON configuration for the 'dataSource' inputSpec type in Druid's Hadoop batch ingestion. This spec allows reading data already stored in Druid for reindexing purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/update-existing-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"ioConfig\" : {\n  \"type\" : \"hadoop\",\n  \"inputSpec\" : {\n    \"type\" : \"dataSource\",\n    \"ingestionSpec\" : {\n      \"dataSource\": \"wikipedia\",\n      \"intervals\": [\"2014-10-20T00:00:00Z/P2W\"]\n    }\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Lunr.js Library Copyright Notice\nDESCRIPTION: The main copyright notice for the Lunr.js library, describing it as a lightweight search engine similar to Solr but smaller. It includes the version number, copyright holder, and license type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/4611.9dd84f2d.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9\n * Copyright (C) 2020 Oliver Nightingale\n * @license MIT\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous Authenticator\nDESCRIPTION: Example configuration for setting up Anonymous Authenticator with basic security extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/auth.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"basic\", \"anonymous\"]\n\ndruid.auth.authenticator.anonymous.type=anonymous\ndruid.auth.authenticator.anonymous.identity=defaultUser\ndruid.auth.authenticator.anonymous.authorizerName=myBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for Prism Library in JavaScript\nDESCRIPTION: Copyright notice for the Prism syntax highlighting library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for React Scheduler\nDESCRIPTION: This snippet contains the copyright notice and license information for the React Scheduler production file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for React Core\nDESCRIPTION: License declaration for React's react.production.min.js v17.0.2 created by Facebook under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Registering Druid Module in META-INF services file\nDESCRIPTION: Example of registering a custom Druid module in the META-INF/services file. This file should contain the fully qualified class name of the DruidModule implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\norg.apache.druid.storage.cassandra.CassandraDruidModule\n```\n\n----------------------------------------\n\nTITLE: Example Query Results with Timestamp Min/Max in Apache Druid\nDESCRIPTION: Sample results from a query using timeMin and timeMax aggregators. The results show the granularity (day), dimension value, count, minimum timestamp, and maximum timestamp for each group.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T00:00:00.000Z A 4 2015-07-28T01:00:00.000Z 2015-07-28T05:00:00.000Z\n2015-07-28T00:00:00.000Z B 2 2015-07-28T04:00:00.000Z 2015-07-28T06:00:00.000Z\n2015-07-29T00:00:00.000Z A 2 2015-07-29T03:00:00.000Z 2015-07-29T04:00:00.000Z\n2015-07-29T00:00:00.000Z C 2 2015-07-29T01:00:00.000Z 2015-07-29T02:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Parsing Extraction Function in Druid\nDESCRIPTION: The Time Parsing extraction function parses dimension values as timestamps using a specified input format and returns them formatted according to a specified output format. It supports both Joda and SimpleDateFormat patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"time\",\n  \"timeFormat\" : <input_format>,\n  \"resultFormat\" : <output_format>,\n  \"joda\" : <true, false> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Selector Filter with Extraction Function in Druid (JSON)\nDESCRIPTION: This example demonstrates how to configure a selector filter with an extraction function in a Druid query. It uses a lookup map to transform input values before applying the filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleLast Aggregator in Apache Druid\nDESCRIPTION: Illustrates the setup for a doubleLast aggregator in Druid. This aggregator computes the metric value with the maximum timestamp or 0 if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Site Properties for EMR\nDESCRIPTION: YARN site configuration for Amazon EMR to optimize memory settings and timeout values for MapReduce jobs running with Druid. This configuration sets memory allocation, Java options, and task timeout parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nclassification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000]\n```\n\n----------------------------------------\n\nTITLE: Data Source Metadata Query Result Format in Apache Druid (JSON)\nDESCRIPTION: This snippet illustrates the format of the result returned by a Data Source Metadata query. It includes the timestamp and the maxIngestedEventTime.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"maxIngestedEventTime\" : \"2013-05-09T18:24:09.007Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Equivalent OR Bound Filter in Druid\nDESCRIPTION: Example showing equivalent OR of bound filters for time intervals using millisecond timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Local Mount Configuration Properties for Druid Deep Storage\nDESCRIPTION: Configuration properties required for setting up local mount deep storage in Druid. Defines the storage type and directory location for storing segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type: local\ndruid.storage.storageDirectory: <directory_path>\n```\n\n----------------------------------------\n\nTITLE: Configuring Append Task in Apache Druid (Deprecated)\nDESCRIPTION: JSON configuration for an Append task in Apache Druid. This deprecated task appends a list of segments into a single segment, specifying the task ID, data source, segments to append, optional aggregators, and task context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"append\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"segments\": <JSON list of DataSegment objects to append>,\n    \"aggregations\": <optional list of aggregators>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying ResetCluster Help Documentation\nDESCRIPTION: Command to display the help documentation for the ResetCluster tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main help tools reset-cluster\n```\n\n----------------------------------------\n\nTITLE: Query for Unique Users for a Single Product\nDESCRIPTION: GroupBy query that uses the Theta sketch aggregator to count unique users who visited a specific product during a time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"aggregations\": [\n    { \"type\": \"thetaSketch\", \"name\": \"unique_users\", \"fieldName\": \"user_id_sketch\" }\n  ],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\" },\n  \"intervals\": [ \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Historical Segment Loading Status in JSON\nDESCRIPTION: GET request to /druid/historical/v1/loadstatus returns a JSON object indicating if all segments in the local cache have been loaded. This can be used to determine if a Historical process is ready for queries after a restart.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"cacheInitialized\":<value>}\n```\n\n----------------------------------------\n\nTITLE: Sample Query Results for Timestamp Min/Max Aggregations in Druid\nDESCRIPTION: Example of query results showing min and max timestamps for events, grouped by day and dimension. The timestamps are more precise than the query granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T00:00:00.000Z A 4 2015-07-28T01:00:00.000Z 2015-07-28T05:00:00.000Z\n2015-07-28T00:00:00.000Z B 2 2015-07-28T04:00:00.000Z 2015-07-28T06:00:00.000Z\n2015-07-29T00:00:00.000Z A 2 2015-07-29T03:00:00.000Z 2015-07-29T04:00:00.000Z\n2015-07-29T00:00:00.000Z C 2 2015-07-29T01:00:00.000Z 2015-07-29T02:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleMin Aggregator in Druid JSON\nDESCRIPTION: Defines a doubleMin aggregator to compute the minimum of all metric values and Double.POSITIVE_INFINITY. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container for Druid Integration\nDESCRIPTION: Command to start the Hadoop Docker container with necessary port mappings and volume mounts for integration with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Getting Supervisors Details GET Endpoint\nDESCRIPTION: REST endpoint to retrieve detailed information about all active supervisors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_14\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/supervisor?full\n```\n\n----------------------------------------\n\nTITLE: Configuring Sketch Summary Post-Aggregator in Druid\nDESCRIPTION: Shows the JSON configuration for the quantilesDoublesSketchToString post-aggregator, which returns a debug-friendly summary of the sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Micro-Quickstart Server\nDESCRIPTION: Command to start the Druid micro-quickstart configuration which launches all required services.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/start-micro-quickstart\n```\n\n----------------------------------------\n\nTITLE: Realtime Ingestion Metrics Table in Markdown\nDESCRIPTION: Table documenting metrics for real-time data ingestion process including event processing, persistence, and handoff metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/thrownAway`|Number of events rejected because they are outside the windowPeriod.|dataSource, taskId, taskType.|0|\n|`ingest/events/unparseable`|Number of events rejected because the events are unparseable.|dataSource, taskId, taskType.|0|\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Extraction Function in Druid\nDESCRIPTION: Example showing how to chain multiple extraction functions including regex, javascript, and substring extractions. Transforms values sequentially through the specified functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"cascade\", \n  \"extractionFns\": [\n    { \n      \"type\" : \"regex\", \n      \"expr\" : \"/([^/]+)/\", \n      \"replaceMissingValue\": false,\n      \"replaceMissingValueWith\": null\n    },\n    { \n      \"type\" : \"javascript\", \n      \"function\" : \"function(str) { return \\\"the \\\".concat(str) }\" \n    },\n    { \n      \"type\" : \"substring\", \n      \"index\" : 0, \"length\" : 7 \n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Lookup DimensionSpec with Map Implementation in Druid\nDESCRIPTION: Configuration for lookup-based dimension transformation using inline map implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"replaceMissingValueWith\":\"missing_value\",\n  \"retainMissingValue\":false,\n  \"lookup\":{\"type\": \"map\", \"map\":{\"key\":\"value\"}, \"isOneToOne\":false}\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for object-assign\nDESCRIPTION: This comment block provides license information for the object-assign library, which is released under the MIT license and created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Response with Pacific Timezone\nDESCRIPTION: Example response showing query results with timestamps converted to Pacific time. Shows how events are grouped into daily buckets based on Pacific timezone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 2,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Submitting Hadoop-based Ingestion Task to Druid\nDESCRIPTION: Command to submit a Hadoop-based indexing task to Druid for ingesting the Wikipedia sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json\n```\n\n----------------------------------------\n\nTITLE: Retrieving Combined Live Row Stats for Kafka Indexing Service in Apache Druid\nDESCRIPTION: Demonstrates the API endpoint for retrieving combined live row statistics from all tasks managed by a Kafka Indexing Service supervisor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/supervisor/<supervisor-id>/stats\n```\n\n----------------------------------------\n\nTITLE: Executing Hadoop Indexer Command\nDESCRIPTION: Command to run the Hadoop indexer with specified memory, timezone, and encoding settings. Requires Java runtime and Hadoop configuration directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>\n```\n\n----------------------------------------\n\nTITLE: Parsing Completion Report JSON in Apache Druid\nDESCRIPTION: Example JSON output of a completion report, showing ingestion statistics including row counts for different processing stages and any error messages encountered during the task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Equivalent OR Bound Filter in Druid\nDESCRIPTION: Example showing equivalent OR of bound filters for time intervals using millisecond timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Completion Report JSON in Apache Druid\nDESCRIPTION: Example JSON output of a completion report, showing ingestion statistics including row counts for different processing stages and any error messages encountered during the task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Count Aggregator in Apache Druid\nDESCRIPTION: Demonstrates how to set up a count aggregator in Druid. This aggregator computes the count of Druid rows that match specified filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"count\", \"name\" : <output_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Query Request Logging in Apache Druid\nDESCRIPTION: Example of a TSV-formatted request log entry for an SQL query in Druid. It shows the timestamp, remote address, query statistics, and the SQL query details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1       {\"sqlQuery/time\":100,\"sqlQuery/bytes\":600,\"success\":true,\"identity\":\"user1\"}  {\"query\":\"SELECT page, COUNT(*) AS Edits FROM wikiticker WHERE __time BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\",\"context\":{\"sqlQueryId\":\"c9d035a0-5ffd-4a79-a865-3ffdadbb5fdd\",\"nativeQueryIds\":\"[490978e4-f5c7-4cf6-b174-346e63cf8863]\"}}\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container for Druid Integration\nDESCRIPTION: Command to start the Hadoop Docker container with necessary port mappings and volume mounts for integration with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Retrieving Compaction Configuration in Druid Coordinator API\nDESCRIPTION: GET request to retrieve all compaction configs or a specific dataSource's compaction config from the Druid Coordinator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/config/compaction\nGET /druid/coordinator/v1/config/compaction/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: Displaying ResetCluster Tool Help in Apache Druid\nDESCRIPTION: Command to display the help documentation for the ResetCluster tool, showing its purpose and available options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main help tools reset-cluster\n```\n\n----------------------------------------\n\nTITLE: Druid Alert JSON Structure\nDESCRIPTION: Defines the standard JSON structure for Druid alerts including timestamp, service name, host, severity level, description and optional exception data. These alerts are generated when Druid encounters unexpected situations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/alerts.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"<alert creation time>\",\n  \"service\": \"<service name>\",\n  \"host\": \"<host name>\",\n  \"severity\": \"<alert severity>\",\n  \"description\": \"<alert description>\",\n  \"data\": {\n    \"exceptionType\": \"<type of exception>\",\n    \"exceptionMessage\": \"<exception message>\",\n    \"exceptionStackTrace\": \"<stack trace>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Classnames License Declaration\nDESCRIPTION: MIT license declaration for the classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Properties for Cassandra Deep Storage\nDESCRIPTION: Configuration properties to add to Druid's Historical and realtime runtime properties files to enable Cassandra as a backend storage. These settings specify the storage type, host connection string, and keyspace.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-cassandra-storage\"]\ndruid.storage.type=c*\ndruid.storage.host=localhost:9160\ndruid.storage.keyspace=druid\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Parser in Druid Ingestion Spec\nDESCRIPTION: Specifies a string parser with JSON format for interpreting the input data in the Druid ingestion spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Parser in Druid Ingestion Spec\nDESCRIPTION: Specifies a string parser with JSON format for interpreting the input data in the Druid ingestion spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Support in Druid\nDESCRIPTION: Configuration for enabling JavaScript functionality in Druid, which affects parsers, filters, extraction functions, aggregators, and other components.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_22\n\nLANGUAGE: properties\nCODE:\n```\ndruid.javascript.enabled=false\n```\n\n----------------------------------------\n\nTITLE: NProgress MIT License Header\nDESCRIPTION: License header for the NProgress library created by Rico Sta. Cruz, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Document Loading Check Comment in JavaScript\nDESCRIPTION: A comment block indicating code that waits for the document to be fully loaded before executing additional functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.e18c3dea.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n   * Wait for document loaded before starting the execution\n   */\n```\n\n----------------------------------------\n\nTITLE: Downloading Third-Party Extensions Using pull-deps Command\nDESCRIPTION: Example command using Druid's pull-deps tool to download a third-party extension from Maven. This command specifies the full Maven coordinate of the extension, sets the extensions directory, and disables default Hadoop dependencies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/including-extensions.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava \\\n  -cp \"lib/*\" \\\n  -Ddruid.extensions.directory=\"extensions\" \\\n  -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n  org.apache.druid.cli.Main tools pull-deps \\\n  --no-default-hadoop \\\n  -c \"com.example:druid-example-extension:1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: React DOM License Attribution\nDESCRIPTION: MIT License attribution for React DOM production build v17.0.2, used for rendering React components in the browser.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JSDoc\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Max Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for extracting maximum value from histogram aggregator. Works with both approximate and fixed buckets histograms.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"max\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Schema Avro Bytes Decoder in Apache Druid\nDESCRIPTION: This snippet shows how to configure an inline schema-based Avro bytes decoder. It includes the decoder type and a sample Avro schema definition within the configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"schema_inline\",\n  \"schema\": {\n    \"namespace\": \"org.apache.druid.data\",\n    \"name\": \"User\",\n    \"type\": \"record\",\n    \"fields\": [\n      { \"name\": \"FullName\", \"type\": \"string\" },\n      { \"name\": \"Country\", \"type\": \"string\" }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Supervisor Tuning Configuration Fields\nDESCRIPTION: Table documenting the configuration fields for KafkaSupervisorTuningConfig, including data types, descriptions, and requirement status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|The indexing task type, this should always be `kafka`.|yes|\n|`maxRowsInMemory`|Integer|The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in. This is used to manage the required JVM heap size. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set.|no (default == 1000000)|\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Supervisor Tuning Configuration Fields\nDESCRIPTION: Table documenting the configuration fields for KafkaSupervisorTuningConfig, including data types, descriptions, and requirement status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|The indexing task type, this should always be `kafka`.|yes|\n|`maxRowsInMemory`|Integer|The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in. This is used to manage the required JVM heap size. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set.|no (default == 1000000)|\n```\n\n----------------------------------------\n\nTITLE: Illustrating Multi-value Column Data Structures in Druid\nDESCRIPTION: This snippet shows how the data structures for a dimension column change when dealing with multi-value columns. It demonstrates modifications to the column data and bitmaps to accommodate multiple values for a single row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/segments.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n1: Dictionary that encodes column values\n  {\n    \"Justin Bieber\": 0,\n    \"Ke$ha\":         1\n  }\n\n2: Column data\n  [0,\n   [0,1],  <--Row value of multi-value column can have array of values\n   1,\n   1]\n\n3: Bitmaps - one for each unique value\n  value=\"Justin Bieber\": [1,1,0,0]\n  value=\"Ke$ha\":         [0,1,1,1]\n                            ^\n                            |\n                            |\n    Multi-value column has multiple non-zero entries\n```\n\n----------------------------------------\n\nTITLE: Submitting an Ingestion Task to Druid\nDESCRIPTION: Bash command for submitting an ingestion task to Apache Druid using the post-index-task script. This command references a JSON file containing the complete ingestion specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/ingestion-tutorial-index.json \n```\n\n----------------------------------------\n\nTITLE: Executing MiddleManager Server in Apache Druid (Java)\nDESCRIPTION: This command starts the MiddleManager server process in Apache Druid. It uses the Main class from the org.apache.druid.cli package to initialize and run the MiddleManager server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/middlemanager.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Registering a Firehose Implementation in Jackson Module for Druid\nDESCRIPTION: Example of registering a StaticS3FirehoseFactory as a subtype in the Jackson module. This allows the system to load the FirehoseFactory when specified in a realtime configuration with type 'static-s3'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Task on MiddleManager Response in JSON\nDESCRIPTION: POST request to /druid/worker/v1/task/{taskid}/shutdown shuts down a running task by taskid. Returns a JSON object containing the task ID. The Overlord API is preferred for normal usage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"}\n```\n\n----------------------------------------\n\nTITLE: JSON ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing JSON data in Druid, including timestamp and dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"json\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"dimensionSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced GroupBy Query Configuration in Druid\nDESCRIPTION: Common configuration properties for all GroupBy query strategies in Druid, controlling threading behavior and default strategy.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_31\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.defaultStrategy=v2\ndruid.query.groupBy.singleThreaded=false\n```\n\n----------------------------------------\n\nTITLE: JSON ParseSpec Configuration for Druid\nDESCRIPTION: Configuration specification for parsing JSON data in Druid, including timestamp and dimension specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parseSpec\":{\n    \"format\" : \"json\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },\n    \"dimensionSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Sharding in Druid TuningConfig\nDESCRIPTION: Example of setting up linear sharding in Druid's TuningConfig. This strategy allows for easy addition of new nodes without updating existing configurations and supports querying of non-sequential partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Segment Load Status in HTTP\nDESCRIPTION: This HTTP GET request returns the percentage of segments loaded in the cluster versus segments that should be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/loadstatus\n```\n\n----------------------------------------\n\nTITLE: Submitting a Kafka Supervisor Spec using cURL in Bash\nDESCRIPTION: Command to submit a Kafka supervisor specification to the Druid Overlord via HTTP POST. This initiates the Kafka ingestion process by creating a supervisor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Coordinator Server Information\nDESCRIPTION: GET requests to retrieve information about Druid servers, including URLs and detailed server data objects.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_11\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/servers\nGET /druid/coordinator/v1/servers?simple\n```\n\n----------------------------------------\n\nTITLE: Applying Query Filter in groupBy Query Having Clause (JSON)\nDESCRIPTION: This snippet demonstrates how to use a query filter in the having clause of a groupBy query. It shows the general structure and provides an example using a selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/having.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : <any Druid query filter>\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : {\n              \"type\": \"selector\",\n              \"dimension\" : \"<dimension>\",\n              \"value\" : \"<dimension_value>\"\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Notice\nDESCRIPTION: License notice for the React scheduler production module, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Starting Deprecated Tranquility Kafka for Apache Druid\nDESCRIPTION: Command to start the deprecated Tranquility Kafka with a specified configuration file. This method allows loading data from Kafka into Druid without writing code.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-push.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka -configFile <path_to_config_file>/kafka.json\n```\n\n----------------------------------------\n\nTITLE: Segment Naming Convention Example - Version 2\nDESCRIPTION: Shows the naming pattern for reindexed segments with a new schema (version 2) covering the same time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v2_0\nfoo_2015-01-01/2015-01-02_v2_1\nfoo_2015-01-01/2015-01-02_v2_2\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: License notice for Prism, a lightweight syntax highlighting library by Lea Verou, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Numbered Sharding in Apache Druid TuningConfig\nDESCRIPTION: JSON configuration for setting up numbered sharding in the tuningConfig. This strategy requires sequential partition numbering and explicit specification of total partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"numbered\",\n        \"partitionNum\": 0,\n        \"partitions\": 2\n    }\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Custom Origin Time\nDESCRIPTION: Example of setting a custom origin time in the granularity specification. Shows how to offset the bucket start times using the origin parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Live Row Statistics in Apache Druid\nDESCRIPTION: Example of a live row statistics report JSON structure returned by the Druid API during task execution. It includes moving averages and totals for processed, unparseable, thrown away, and processed with error rows.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"movingAverages\": {\n    \"buildSegments\": {\n      \"5m\": {\n        \"processed\": 3.392158326408501,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"15m\": {\n        \"processed\": 1.736165476881023,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"1m\": {\n        \"processed\": 4.206417693750045,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      }\n    }\n  },\n  \"totals\": {\n    \"buildSegments\": {\n      \"processed\": 1994,\n      \"processedWithError\": 0,\n      \"thrownAway\": 0,\n      \"unparseable\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DoubleMax Aggregator in Druid JSON\nDESCRIPTION: Defines a doubleMax aggregator to compute the maximum of all metric values and Double.NEGATIVE_INFINITY. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMax\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependencies in Hadoop Index Task\nDESCRIPTION: JSON configuration showing how to specify which version of Hadoop client libraries to load for a particular indexing task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Kafka\nDESCRIPTION: Commands to set UTF-8 encoding and produce messages to Kafka topic from the sample data file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport KAFKA_OPTS=\"-Dfile.encoding=UTF-8\"\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json\n```\n\n----------------------------------------\n\nTITLE: LIKE Filter Example in Druid\nDESCRIPTION: LIKE filter for wildcard matching, equivalent to SQL LIKE operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"like\",\n    \"dimension\": \"last_name\",\n    \"pattern\": \"D%\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Specification for Bloom Filter Aggregator in Druid\nDESCRIPTION: This JSON snippet shows the structure for specifying a Bloom Filter aggregator in Druid queries. It includes the aggregator type, output field name, maximum number of entries, and the dimension specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/bloom-filter.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"type\": \"bloom\",\n      \"name\": <output_field_name>,\n      \"maxNumEntries\": <maximum_number_of_elements_for_BloomKFilter>\n      \"field\": <dimension_spec>\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous Authenticator with Basic Security in Druid\nDESCRIPTION: Configuration example showing how to set up an authentication chain with basic security and anonymous access. This creates a fallback authentication method for users without credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/auth.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"basic\", \"anonymous\"]\n\ndruid.auth.authenticator.anonymous.type=anonymous\ndruid.auth.authenticator.anonymous.identity=defaultUser\ndruid.auth.authenticator.anonymous.authorizerName=myBasicAuthorizer\n\n# ... usual configs for basic authentication would go here ...\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTSDB Metric Mapping in JSON\nDESCRIPTION: Example JSON configuration showing how to map Druid metrics to OpenTSDB dimensions. This snippet demonstrates the structure for defining which dimensions should be included with specific metrics when sending to OpenTSDB.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/opentsdb-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"query/time\": [\n    \"dataSource\",\n    \"type\"\n]\n```\n\n----------------------------------------\n\nTITLE: Estimating Distinct Keys with ArrayOfDoublesSketch Post-Aggregator\nDESCRIPTION: Post-aggregator configuration to estimate the number of distinct keys from an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring License for mark.js in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the mark.js library, version 8.11.1, created by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for NProgress\nDESCRIPTION: License declaration for the NProgress library created by Rico Sta. Cruz under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Running the Router Process in Java\nDESCRIPTION: Command to start the Router process using the Druid CLI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/router.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server router\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Server for Apache Druid\nDESCRIPTION: Command to start Tranquility server with a configuration file. This allows sending data to Druid without developing a JVM app.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-push.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility server -configFile <path_to_config_file>/server.json\n```\n\n----------------------------------------\n\nTITLE: Downloading and Setting Up Tranquility for Druid\nDESCRIPTION: Commands to download the Tranquility distribution tarball, extract it, and place it in the correct location for the tutorial.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.2.tgz -o tranquility-distribution-0.8.2.tgz\ntar -xzf tranquility-distribution-0.8.2.tgz\nmv tranquility-distribution-0.8.2 tranquility\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authenticator in Apache Druid\nDESCRIPTION: Configuration properties for setting up a Basic authenticator in Druid. This example shows how to add an authenticator to the authenticator chain with initial admin and internal client passwords.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyBasicAuthenticator\"]\n\ndruid.auth.authenticator.MyBasicAuthenticator.type=basic\ndruid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1\ndruid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2\ndruid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Eight Firehose in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure the Kafka Eight Firehose in Apache Druid. It specifies consumer properties for connecting to Zookeeper, setting up the consumer group, and defining message handling parameters. The 'feed' property specifies the Kafka topic to consume from.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-eight-firehose.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\": {\n  \"type\": \"kafka-0.8\",\n  \"consumerProps\": {\n    \"zookeeper.connect\": \"localhost:2181\",\n    \"zookeeper.connection.timeout.ms\" : \"15000\",\n    \"zookeeper.session.timeout.ms\" : \"15000\",\n    \"zookeeper.sync.time.ms\" : \"5000\",\n    \"group.id\": \"druid-example\",\n    \"fetch.message.max.bytes\" : \"1048586\",\n    \"auto.offset.reset\": \"largest\",\n    \"auto.commit.enable\": \"false\"\n  },\n  \"feed\": \"wikipedia\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Long Last Aggregator in Druid\nDESCRIPTION: Computes metric value with maximum timestamp or 0 if no rows exist. For query time use only.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"longLast\",\n  \"name\" : <output_name>, \n  \"fieldName\" : <metric_name>,\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Event Structure for Flattening in Apache Druid\nDESCRIPTION: An example of a complex JSON event structure that can be flattened using the JSON Flatten Spec in Apache Druid. It includes nested objects, arrays, and various data types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/flatten-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"timestamp\": \"2015-09-12T12:10:53.155Z\",\n \"dim1\": \"qwerty\",\n \"dim2\": \"asdf\",\n \"dim3\": \"zxcv\",\n \"ignore_me\": \"ignore this\",\n \"metrica\": 9999,\n \"foo\": {\"bar\": \"abc\"},\n \"foo.bar\": \"def\",\n \"nestmet\": {\"val\": 42},\n \"hello\": [1.0, 2.0, 3.0, 4.0, 5.0],\n \"mixarray\": [1.0, 2.0, 3.0, 4.0, {\"last\": 5}],\n \"world\": [{\"hey\": \"there\"}, {\"tree\": \"apple\"}],\n \"thing\": {\"food\": [\"sandwich\", \"pizza\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Time Boundary Query in Apache Druid\nDESCRIPTION: This snippet demonstrates the structure of a time boundary query in Apache Druid. It includes optional parameters for specifying the bound (maxTime or minTime) and applying filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"timeBoundary\",\n    \"dataSource\": \"sample_datasource\",\n    \"bound\"     : < \"maxTime\" | \"minTime\" > # optional, defaults to returning both timestamps if not set \n    \"filter\"    : { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] } # optional\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Union Datasource in Druid\nDESCRIPTION: Specifies how to combine multiple table datasources into a union. All datasources being unioned must share the same schema. This operation must be performed through a Broker/Router process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/datasource.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n       \"type\": \"union\",\n       \"dataSources\": [\"<string_value1>\", \"<string_value2>\", \"<string_value3>\", ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Server for Apache Druid\nDESCRIPTION: Command to start Tranquility server with a configuration file. This allows sending data to Druid without developing a JVM app.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-push.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility server -configFile <path_to_config_file>/server.json\n```\n\n----------------------------------------\n\nTITLE: Enabling Druid Datasource Segments in HTTP\nDESCRIPTION: This HTTP POST request enables all segments of a specified datasource which are not overshadowed by others.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/coordinator/v1/datasources/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authorizer in Apache Druid\nDESCRIPTION: Configuration for setting up a Basic authorizer in Druid. This example shows how to add an authorizer of type 'basic' to the authorizers list.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authorizers=[\"MyBasicAuthorizer\"]\n\ndruid.auth.authorizer.MyBasicAuthorizer.type=basic\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Front Matter for Druid Documentation\nDESCRIPTION: This snippet defines the front matter for a Markdown document in the Apache Druid website. It specifies the layout and title of the page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Tasks Overview\"\n---\n```\n\n----------------------------------------\n\nTITLE: Managing Tasks using HTTP Requests in Overlord\nDESCRIPTION: Endpoints for retrieving information about tasks, submitting new tasks, and shutting down tasks, including the ability to retrieve task status, segments, and completion reports.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_12\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/indexer/v1/task/{taskId}/status\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/indexer/v1/task/{taskId}/segments\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/indexer/v1/task/{taskId}/reports\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/indexer/v1/task\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST druid/indexer/v1/task/{taskId}/shutdown\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST druid/indexer/v1/datasources/{dataSource}/shutdownAllTasks\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Management in Markdown\nDESCRIPTION: A markdown table specifying configuration properties for segment management, including discovery method and load queue implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_25\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.serverview.type`|batch or http|Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper.|batch|\n|`druid.coordinator.loadqueuepeon.type`|curator or http|Whether to use \"http\" or \"curator\" implementation to assign segment loads/drops to historical|curator|\n|`druid.coordinator.segment.awaitInitializationOnStart`|true or false|Whether the the Coordinator will wait for its view of segments to fully initialize before starting up. If set to 'true', the Coordinator's HTTP server will not start up, and the Coordinator will not announce itself as available, until the server view is initialized.|true|\n```\n\n----------------------------------------\n\nTITLE: Implementing Float First Aggregator in Druid\nDESCRIPTION: Computes metric value with minimum timestamp or 0 if no rows exist. For query time use only.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Appending Data in Apache Druid\nDESCRIPTION: This command submits a task to append new data to the existing 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index2.json\n```\n\n----------------------------------------\n\nTITLE: Submitting Druid Kafka Supervisor\nDESCRIPTION: cURL command to submit the Kafka supervisor specification to the Druid overlord, initiating the ingestion process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8081/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Get MiddleManager Active Tasks Response - JSON\nDESCRIPTION: Example response from the /druid/worker/v1/tasks endpoint showing list of active task IDs on a MiddleManager.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n[\"index_wikiticker_2019-02-11T02:20:15.316Z\"]\n```\n\n----------------------------------------\n\nTITLE: Displaying Indexing Service Metrics Table in Markdown\nDESCRIPTION: This code snippet presents a markdown table listing various metrics for the Indexing Service in Apache Druid. It includes metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`task/run/time`|Milliseconds taken to run a task.|dataSource, taskId, taskType, taskStatus.|Varies.|\n|`task/action/log/time`|Milliseconds taken to log a task action to the audit log.|dataSource, taskId, taskType|< 1000 (subsecond)|\n|`task/action/run/time`|Milliseconds taken to execute a task action.|dataSource, taskId, taskType|Varies from subsecond to a few seconds, based on action type.|\n|`segment/added/bytes`|Size in bytes of new segments created.|dataSource, taskId, taskType, interval.|Varies.|\n|`segment/moved/bytes`|Size in bytes of segments moved/archived via the Move Task.|dataSource, taskId, taskType, interval.|Varies.|\n|`segment/nuked/bytes`|Size in bytes of segments deleted via the Kill Task.|dataSource, taskId, taskType, interval.|Varies.|\n|`task/success/count`|Number of successful tasks per emission period. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/failed/count`|Number of failed tasks per emission period. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/running/count`|Number of current running tasks. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/pending/count`|Number of current pending tasks. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n|`task/waiting/count`|Number of current waiting tasks. This metric is only available if the TaskCountStatsMonitor module is included.|dataSource.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: ListFiltered GroupBy Query for Multi-value Dimensions\nDESCRIPTION: Advanced GroupBy query using listFiltered dimensionSpec to filter specific dimension values after explosion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"listFiltered\",\n      \"delegate\": {\n        \"type\": \"default\",\n        \"dimension\": \"tags\",\n        \"outputName\": \"tags\"\n      },\n      \"values\": [\"t3\"]\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Get MiddleManager Enabled Status Response - JSON\nDESCRIPTION: Example response from the /druid/worker/v1/enabled endpoint showing the enabled state of a MiddleManager keyed by host:port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":true}\n```\n\n----------------------------------------\n\nTITLE: Classnames License Declaration\nDESCRIPTION: MIT license declaration for classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Specification in Druid\nDESCRIPTION: JSON configuration for defining metrics in a Druid ingestion task using metricsSpec within the dataSchema. This example defines count, longSum and doubleSum aggregations with rollup enabled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }   \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Exhibitor Integration in Druid\nDESCRIPTION: Configuration properties for integrating Exhibitor with Druid for dynamic Zookeeper cluster management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.exhibitor.service.hosts=none\ndruid.exhibitor.service.port=8080\ndruid.exhibitor.service.restUriPath=/exhibitor/v1/cluster/list\ndruid.exhibitor.service.useSsl=false\ndruid.exhibitor.service.pollingMs=10000\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Load Rule in Druid\nDESCRIPTION: JSON configuration for an interval load rule that specifies segment replication for a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByInterval\",\n  \"interval\": \"2012-01-01/2013-01-01\",\n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec with Partition 1 for Scaling in Druid\nDESCRIPTION: Configuration example for a linear shardSpec with partitionNum 1. When processes use different partitionNum values, they store different data for the same datasource, enabling horizontal scaling of ingestion capacity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-pull.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 1\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Append Task in Apache Druid\nDESCRIPTION: Deprecated task that combines multiple segments sequentially into a single segment. Requires task ID, datasource, and list of segments to append, with optional aggregations and context parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"append\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"segments\": <JSON list of DataSegment objects to append>,\n    \"aggregations\": <optional list of aggregators>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing JavaScript Aggregator in Apache Druid\nDESCRIPTION: Demonstrates how to set up a JavaScript aggregator in Druid. This aggregator computes an arbitrary JavaScript function over a set of columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"javascript\",\n  \"name\": \"<output_name>\",\n  \"fieldNames\"  : [ <column1>, <column2>, ... ],\n  \"fnAggregate\" : \"function(current, column1, column2, ...) {\n                     <updates partial aggregate (current) based on the current row values>\n                     return <updated partial aggregate>\n                   }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return <combined partial results>; }\",\n  \"fnReset\"     : \"function()                   { return <initial value>; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Priority Strategy for Router\nDESCRIPTION: JSON configuration for the priority strategy, which routes queries based on their priority level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/router.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"priority\",\n  \"minPriority\":0,\n  \"maxPriority\":1\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Tranquility in Bash\nDESCRIPTION: Commands to download the Tranquility distribution tarball, extract it, and move it to the correct location in the Druid package directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz\ntar -xzf tranquility-distribution-0.8.3.tgz\nmv tranquility-distribution-0.8.3 tranquility\n```\n\n----------------------------------------\n\nTITLE: Creating Markdown Navigation Links for Apache Druid Documentation\nDESCRIPTION: Markdown code that defines a structured navigation menu for Apache Druid documentation. The navigation is organized into 'Development' and 'Misc' sections with links to various documentation pages using relative URLs with VERSION placeholders.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/toc.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## Development\n  * [Overview](/docs/VERSION/development/overview.html)\n  * [Libraries](/libraries.html)\n  * [Extensions](/docs/VERSION/development/extensions.html)\n  * [JavaScript](/docs/VERSION/development/javascript.html)\n  * [Build From Source](/docs/VERSION/development/build.html)\n  * [Versioning](/docs/VERSION/development/versioning.html)\n  * [Integration](/docs/VERSION/development/integrating-druid-with-other-technologies.html)\n  * [Experimental Features](/docs/VERSION/development/experimental.html)\n\n## Misc\n  * [Druid Expressions Language](/docs/VERSION/misc/math-expr.html)\n  * [Papers & Talks](/docs/VERSION/misc/papers-and-talks.html)\n  * [Thanks](/thanks.html)\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Drop Rule in Apache Druid\nDESCRIPTION: The Interval Drop Rule drops segments that fall within a specific time interval. Segments whose intervals are contained within the specified ISO-8601 interval will be removed from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring NProgress License in JavaScript\nDESCRIPTION: This snippet provides license information for the NProgress library, which is under the MIT license and created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Default DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for default dimension specification that returns dimension values as-is with optional renaming and type conversion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"default\",\n  \"dimension\" : <dimension>,\n  \"outputName\": <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Sharding in Druid\nDESCRIPTION: Configuration for linear sharding strategy that allows non-sequential partition numbering and doesn't require updating fileSpec configurations when adding new processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Declaration\nDESCRIPTION: MIT license declaration for object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Coordinating Historical Nodes in Druid using Java\nDESCRIPTION: The coordination logic for historical nodes is mainly handled by the Druid coordinator, with DruidCoordinator.java as the starting point.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/overview.md#2025-04-09_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nDruidCoordinator.java\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Query Server\nDESCRIPTION: Command to start the Broker process on Query servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/broker/jvm.config | xargs` -cp conf/druid/_common:conf/druid/broker:lib/* org.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighting MIT License Declaration\nDESCRIPTION: License declaration for Prism, a lightweight syntax highlighting library created by Lea Verou, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for mark.js Library in JavaScript\nDESCRIPTION: Copyright notice for the mark.js library, version 8.11.1, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Configuring Append Task for Segment Merging in Druid\nDESCRIPTION: JSON configuration for Append tasks that join multiple segments sequentially into a single segment. This task type is deprecated.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"append\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"segments\": <JSON list of DataSegment objects to append>,\n    \"aggregations\": <optional list of aggregators>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Cache in Apache Druid\nDESCRIPTION: This snippet outlines the configuration properties for using Memcached as a cache backend in Apache Druid. It includes settings for expiration time, timeout, hosts, and other Memcached-specific options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_41\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.cache.expiration`|Memcached [expiration time](https://code.google.com/p/memcached/wiki/NewCommands#Standard_Protocol).|2592000 (30 days)|\n|`druid.cache.timeout`|Maximum time in milliseconds to wait for a response from Memcached.|500|\n|`druid.cache.hosts`|Comma separated list of Memcached hosts `<host:port>`.|none|\n|`druid.cache.maxObjectSize`|Maximum object size in bytes for a Memcached object.|52428800 (50 MB)|\n|`druid.cache.memcachedPrefix`|Key prefix for all keys in Memcached.|druid|\n|`druid.cache.numConnections`|Number of memcached connections to use.|1|\n|`druid.cache.protocol`|Memcached communication protocol. Can be binary or text.|binary|\n|`druid.cache.locator`|Memcached locator. Can be consistent or array_mod.|consistent|\n```\n\n----------------------------------------\n\nTITLE: Period Granularity Configuration\nDESCRIPTION: Examples of period granularity specifications using ISO8601 format with timezone and origin options\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P2D\", \"timeZone\": \"America/Los_Angeles\"}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P3M\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"2012-02-01T00:00:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: User Information Response with Basic Role Data\nDESCRIPTION: JSON response format for retrieving user information showing basic role assignments\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"druid2\",\n  \"roles\": [\n    \"druidRole\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: HyperUnique Cardinality Post-Aggregator in Druid\nDESCRIPTION: JSON structure for a hyperUniqueCardinality post-aggregator that wraps a hyperUnique object to enable its use in post-aggregations, typically for unique count operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"hyperUniqueCardinality\",\n  \"name\": <output name>,\n  \"fieldName\"  : <the name field value of the hyperUnique aggregator>\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data in Apache Druid\nDESCRIPTION: This command loads the initial dataset into Druid using a task specification file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Enabling MiddleManager Response in JSON\nDESCRIPTION: POST request to /druid/worker/v1/enable 'enables' a MiddleManager, allowing it to accept new tasks if previously disabled. Returns a JSON object with the combined druid.host and druid.port as the key.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"enabled\"}\n```\n\n----------------------------------------\n\nTITLE: Querying Data Source Metadata in Apache Druid using JSON\nDESCRIPTION: This JSON structure defines a Data Source Metadata query in Apache Druid. It specifies the query type and the data source to be queried.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/datasourcemetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\" : \"dataSourceMetadata\",\n    \"dataSource\": \"sample_datasource\"\n}\n```\n\n----------------------------------------\n\nTITLE: Server Information Endpoints\nDESCRIPTION: HTTP GET endpoints for retrieving server information and status\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_7\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/servers\nGET /druid/coordinator/v1/servers?simple\n```\n\n----------------------------------------\n\nTITLE: Segment Discovery Configuration Table in Markdown\nDESCRIPTION: Markdown table defining segment discovery configuration properties for the Druid Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_49\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.serverview.type`|batch or http|Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper.|batch|\n|`druid.broker.segment.watchedTiers`|List of strings|Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of tiers. By default, Broker would consider all tiers. This can be used to partition your dataSources in specific Historical tiers and configure brokers in partitions so that they are only queryable for specific dataSources.|none|\n|`druid.broker.segment.watchedDataSources`|List of strings|Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of dataSources. By default, Broker would consider all datasources. This can be used to configure brokers in partitions so that they are only queryable for specific dataSources.|none|\n|`druid.broker.segment.awaitInitializationOnStart`|Boolean|Whether the the Broker will wait for its view of segments to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also `druid.sql.planner.awaitInitializationOnStart`, a related setting.|true|\n```\n\n----------------------------------------\n\nTITLE: Configuring MomentSketch Aggregator\nDESCRIPTION: JSON schema for configuring the momentSketch and momentSketchMerge aggregators. It defines parameters like name, fieldName, k (accuracy parameter), and compress flag.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : <aggregator_type>,\n  \"name\" : <output_name>,\n  \"fieldName\" : <input_name>,\n  \"k\" : <int>,\n  \"compress\" : <boolean>\n }\n```\n\n----------------------------------------\n\nTITLE: Example of JavaScript Filter in Apache Druid JSON Query\nDESCRIPTION: This example demonstrates a JavaScript filter that matches dimension values for the 'name' dimension between 'bar' and 'foo' alphabetically.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : \"name\",\n  \"function\" : \"function(x) { return(x >= 'bar' && x <= 'foo') }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Kafka Supervisor API Endpoint - Terminate\nDESCRIPTION: API endpoint to terminate a supervisor and stop all associated indexing tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/terminate\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Druid Repository\nDESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/incubator-druid.git\ncd druid\n```\n\n----------------------------------------\n\nTITLE: License Comment for object-assign\nDESCRIPTION: Copyright and license information for the object-assign library by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Query Context Parameters in Markdown Table\nDESCRIPTION: This markdown table defines various query context parameters, their default values, and descriptions. It includes parameters for timeout, priority, caching, and result formatting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/query-context.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default                                 | description          |\n|-----------------|----------------------------------------|----------------------|\n|timeout          | `druid.server.http.defaultQueryTimeout`| Query timeout in millis, beyond which unfinished queries will be cancelled. 0 timeout means `no timeout`. To set the default timeout, see [broker configuration](../configuration/index.html#broker) |\n|priority         | `0`                                    | Query Priority. Queries with higher priority get precedence for computational resources.|\n|queryId          | auto-generated                         | Unique identifier given to this query. If a query ID is set or known, this can be used to cancel the query |\n|useCache         | `true`                                 | Flag indicating whether to leverage the query cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useCache or druid.historical.cache.useCache to determine whether or not to read from the query cache |\n|populateCache    | `true`                                 | Flag indicating whether to save the results of the query to the query cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache or druid.historical.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|useResultLevelCache         | `false`                                 | Flag indicating whether to leverage the result level cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useResultLevelCache to determine whether or not to read from the query cache |\n|populateResultLevelCache    | `false`                                 | Flag indicating whether to save the results of the query to the result level cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|bySegment        | `false`                                | Return \"by segment\" results. Primarily used for debugging, setting it to `true` returns results associated with the data segment they came from |\n|finalize         | `true`                                 | Flag indicating whether to \"finalize\" aggregation results. Primarily used for debugging. For instance, the `hyperUnique` aggregator will return the full HyperLogLog sketch instead of the estimated cardinality when this flag is set to `false` |\n|chunkPeriod      | `P0D` (off)                            | At the broker node level, long interval queries (of any type) may be broken into shorter interval queries to parallelize merging more than normal. Broken up queries will use a larger share of cluster resources, but may be able to complete faster as a result. Use ISO 8601 periods. For example, if this property is set to `P1M` (one month), then a query covering a year would be broken into 12 smaller queries. The broker uses its query processing executor service to initiate processing for query chunks, so make sure \"druid.processing.numThreads\" is configured appropriately on the broker. [groupBy queries](groupbyquery.html) do not support chunkPeriod by default, although they do if using the legacy \"v1\" engine. |\n|maxScatterGatherBytes| `druid.server.http.maxScatterGatherBytes` | Maximum number of bytes gathered from data nodes such as historicals and realtime processes to execute a query. This parameter can be used to further reduce `maxScatterGatherBytes` limit at query time. See [broker configuration](../configuration/index.html#broker) for more details.|\n|maxQueuedBytes       | `druid.broker.http.maxQueuedBytes`        | Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to `maxScatterGatherBytes`, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled.|\n|serializeDateTimeAsLong| `false`       | If true, DateTime is serialized as long in the result returned by broker and the data transportation between broker and compute node|\n|serializeDateTimeAsLongInner| `false`  | If true, DateTime is serialized as long in the data transportation between broker and compute node|\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL/TLS Parameters in Apache Druid\nDESCRIPTION: This configuration table outlines the main parameters for setting up SSL/TLS in Apache Druid using the Simple SSLContext Provider Module. It includes options for specifying the SSL protocol, trust store details, and other essential settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/simple-client-sslcontext.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.client.https.protocol`|SSL protocol to use.|`TLSv1.2`|no|\n|`druid.client.https.trustStoreType`|The type of the key store where trusted root certificates are stored.|`java.security.KeyStore.getDefaultType()`|no|\n|`druid.client.https.trustStorePath`|The file path or URL of the TLS/SSL Key store where trusted root certificates are stored.|none|yes|\n|`druid.client.https.trustStoreAlgorithm`|Algorithm to be used by TrustManager to validate certificate chains|`javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()`|no|\n|`druid.client.https.trustStorePassword`|The [Password Provider](../../operations/password-provider.html) or String password for the Trust Store.|none|yes|\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for React Is Package\nDESCRIPTION: License declaration for React's react-is.production.min.js v16.13.1 created by Facebook under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Exhibitor Properties in Apache Druid\nDESCRIPTION: Defines properties for integrating with Exhibitor, a supervisor system for Zookeeper.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.exhibitor.service.hosts=none\ndruid.exhibitor.service.port=8080\ndruid.exhibitor.service.restUriPath=/exhibitor/v1/cluster/list\ndruid.exhibitor.service.useSsl=false\ndruid.exhibitor.service.pollingMs=10000\n```\n\n----------------------------------------\n\nTITLE: Executing GroupBy Query with DistinctCount in Druid\nDESCRIPTION: Example of a GroupBy query using distinctCount aggregator to calculate unique visitors grouped by sample dimension. The query runs on all granularity for a specific day.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimensions\": \"[sample_dim]\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling a Druid Middle Manager for Updates\nDESCRIPTION: Demonstrates how to gracefully disable a Middle Manager using a POST request to prepare it for updating.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/disable\n```\n\n----------------------------------------\n\nTITLE: Configuring None Sharding in Druid\nDESCRIPTION: Basic configuration for disabling sharding in small-data scenarios where partitioning is unnecessary.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\"type\": \"none\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Layout for Apache Druid Documentation\nDESCRIPTION: This snippet sets the layout for the Markdown file to 'toc', indicating it's a table of contents page for the Apache Druid documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/toc.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: toc\n---\n```\n\n----------------------------------------\n\nTITLE: Specific Columns Include List in Druid Metadata Query\nDESCRIPTION: Configuration to include a specific list of columns in the segment metadata query result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"toInclude\": { \"type\": \"list\", \"columns\": [<string list of column names>]}\n```\n\n----------------------------------------\n\nTITLE: Router Management Proxy Configuration\nDESCRIPTION: Configuration property to enable the router's management proxy functionality for high availability.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.managementProxy.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Extracting Apache Druid Distribution\nDESCRIPTION: These bash commands download and extract the Apache Druid 0.15.1-incubating release distribution. The tar command unpacks the archive, and the cd command changes the current directory to the extracted Druid folder.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.15.1-incubating-bin.tar.gz\ncd apache-druid-0.15.1-incubating\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with URI Extraction\nDESCRIPTION: JSON configuration for a globally cached lookup using a URI extraction namespace. This configuration specifies a file-based lookup with CSV format that polls for updates every 5 minutes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"uri\",\n       \"uri\": \"file:/tmp/prefix/\",\n       \"namespaceParseSpec\": {\n         \"format\": \"csv\",\n         \"columns\": [\n           \"key\",\n           \"value\"\n         ]\n       },\n       \"pollPeriod\": \"PT5M\"\n     },\n     \"firstCacheTimeout\": 0\n }\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Process Announcements in Druid\nDESCRIPTION: Specifies the ZooKeeper path where Historical and Realtime processes in Druid create ephemeral znodes to announce their existence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.announcementsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool with All Options\nDESCRIPTION: Command to run the ResetCluster tool to reset all components at once using the --all flag.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster --all\n```\n\n----------------------------------------\n\nTITLE: Configuring Registered Lookup Extraction Function in Druid\nDESCRIPTION: The Registered Lookup extraction function references a lookup that has been registered in the cluster-wide configuration. It includes options for handling missing values and optimization behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"registeredLookup\",\n  \"lookup\":\"some_lookup_name\",\n  \"retainMissingValue\":true\n}\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Full Reset Command\nDESCRIPTION: Command to perform a complete reset of all cluster components using the --all flag.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster --all\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0c610519.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: MIT license declaration for react-dom.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Running Druid Real-time Server Command\nDESCRIPTION: Command-line instruction for starting a Druid real-time server process. This command initializes a real-time node that can ingest and index data in real-time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: Submitting Queries to Broker using HTTP Requests\nDESCRIPTION: Endpoints for submitting queries to the broker and retrieving segment information including server locations for a given query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_16\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/v2/\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/v2/candidates/\n```\n\n----------------------------------------\n\nTITLE: Configuring flattenSpec for Nested Data in Druid ORC Ingestion\nDESCRIPTION: This JSON snippet shows how to use flattenSpec expressions to flatten nested data structures in ORC files when ingesting into Druid. It replaces the previous mapFieldNameFormat property used in the 'contrib' extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/orc.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"flattenSpec\": {\n    \"fields\": [\n      {\n        \"type\": \"path\",\n        \"name\": \"nestedData_dim1\",\n        \"expr\": \"$.nestedData.dim1\"\n      }\n    ]\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Hadoop Parser for Druid Ingestion\nDESCRIPTION: JSON configuration for the ORC Hadoop Parser in Druid. It specifies the input format, parse spec, and type string for ORC data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/orc.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"orc\",\n  \"parseSpec\": {\n    \"format\": \"timeAndDims\",\n    \"timestampSpec\": {\n      \"column\": \"time\",\n      \"format\": \"auto\"\n    },\n    \"dimensionsSpec\": {\n      \"dimensions\": [\n        \"name\"\n      ],\n      \"dimensionExclusions\": [],\n      \"spatialDimensions\": []\n    }\n  },\n  \"typeString\": \"struct<time:string,name:string>\",\n  \"mapFieldNameFormat\": \"<PARENT>_<CHILD>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Timeseries Query with Variance in Druid\nDESCRIPTION: Sample timeseries query demonstrating the use of variance aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/stats.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"testing\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Escalator in Apache Druid\nDESCRIPTION: This snippet demonstrates the configuration of a Basic escalator in Druid. It sets the escalator type, internal client username and password, and the authorizer name to be used for escalated requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# Escalator\ndruid.escalator.type=basic\ndruid.escalator.internalClientUsername=druid_system\ndruid.escalator.internalClientPassword=password2\ndruid.escalator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Defining Constant Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Illustrates the structure of a constant post-aggregator that always returns a specified numerical value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\"  : \"constant\", \"name\"  : <output_name>, \"value\" : <numerical_value> }\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch to Histogram Post Aggregator\nDESCRIPTION: Post aggregator configuration for generating a histogram from a DoublesSketch using specified split points.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToHistogram\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch>,\n  \"splitPoints\" : <array of split points>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Event Converter in JSON for Graphite Emitter\nDESCRIPTION: JSON configuration for the 'all' event converter that sends all Druid service metrics events to Graphite. This example ignores both hostname and service name in the metric path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Server Information using GET HTTP Requests\nDESCRIPTION: GET endpoints for retrieving information about servers in the Druid cluster, with different output formats showing varying levels of detail about server configurations and resources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_10\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/servers\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/servers?simple\n```\n\n----------------------------------------\n\nTITLE: Example JSON Row Output from DumpSegment Tool\nDESCRIPTION: Sample of the row data output from the DumpSegment tool when using the default rows dump format. Each row is represented as a JSON object containing various segment columns with their values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/dump-segment.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__time\": 1442018818771,\n  \"added\": 36,\n  \"channel\": \"#en.wikipedia\",\n  \"cityName\": null,\n  \"comment\": \"added project\",\n  \"count\": 1,\n  \"countryIsoCode\": null,\n  \"countryName\": null,\n  \"deleted\": 0,\n  \"delta\": 36,\n  \"isAnonymous\": \"false\",\n  \"isMinor\": \"false\",\n  \"isNew\": \"false\",\n  \"isRobot\": \"false\",\n  \"isUnpatrolled\": \"false\",\n  \"iuser\": \"00001553\",\n  \"metroCode\": null,\n  \"namespace\": \"Talk\",\n  \"page\": \"Talk:Oswald Tilghman\",\n  \"regionIsoCode\": null,\n  \"regionName\": null,\n  \"user\": \"GELongstreet\"\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Markdown Links for Druid Miscellaneous Documentation\nDESCRIPTION: This snippet creates markdown links for various Druid documentation resources in the miscellaneous section. It includes links to the Druid Expressions Language, papers and talks, and a thanks page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/toc.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Misc\n  * [Druid Expressions Language](/docs/VERSION/misc/math-expr.html)\n  * [Papers & Talks](/docs/VERSION/misc/papers-and-talks.html)\n  * [Thanks](/thanks.html)\n```\n\n----------------------------------------\n\nTITLE: Checking Broker Load Status using GET HTTP Request\nDESCRIPTION: Endpoint for checking if a broker knows about all segments in Zookeeper, which is useful for determining if a broker node is ready to be queried after a restart.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_15\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/broker/v1/loadstatus\n```\n\n----------------------------------------\n\nTITLE: API Endpoint for Worker Configuration History in Druid\nDESCRIPTION: The HTTP endpoint for retrieving the audit history of worker configuration changes in Druid. Supports querying by time interval or limiting to a specific number of entries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_35\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker/history?interval=<interval>\n```\n\n----------------------------------------\n\nTITLE: Configuring White-List Based Graphite Event Converter in JSON\nDESCRIPTION: JSON configuration for the 'whiteList' event converter that selectively sends metrics based on a whitelist map. This configuration specifies a custom file path for the whitelist map.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true, \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Caching in Apache Druid\nDESCRIPTION: These properties control caching behavior on Historical nodes in Druid. They allow enabling the cache, populating it, and specifying which query types should not be cached.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_40\n\nLANGUAGE: properties\nCODE:\n```\ndruid.historical.cache.useCache=true\ndruid.historical.cache.populateCache=true\ndruid.historical.cache.unCacheable=[\"groupBy\", \"select\"]\n```\n\n----------------------------------------\n\nTITLE: Druid DataSource Configuration\nDESCRIPTION: DataSchema configuration specifying the datasource name for the ingestion task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n}\n```\n\n----------------------------------------\n\nTITLE: Kinesis Supervisor IndexSpec Configuration Example\nDESCRIPTION: Configuration object for specifying bitmap compression, dimension compression, metric compression and long encoding settings for Kinesis ingestion in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmap\": {\n    \"type\": \"concise\"\n  },\n  \"dimensionCompression\": \"LZ4\",\n  \"metricCompression\": \"LZ4\",\n  \"longEncoding\": \"longs\"\n}\n```\n\n----------------------------------------\n\nTITLE: Apache License Header in Markdown\nDESCRIPTION: Standard Apache 2.0 License header comment block used at the top of the documentation file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/plumber.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Configuring zscore2sample Post Aggregator in Druid\nDESCRIPTION: JSON configuration for the zscore2sample post aggregator that calculates z-score using two-sample z-test by converting binary variables to continuous variables, with parameters for success counts and sample sizes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"zscore2sample\",\n  \"name\": \"<output_name>\",\n  \"successCount1\": <post_aggregator> success count of sample 1,\n  \"sample1Size\": <post_aggregaror> sample 1 size,\n  \"successCount2\": <post_aggregator> success count of sample 2,\n  \"sample2Size\" : <post_aggregator> sample 2 size\n}\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Response Format\nDESCRIPTION: Example JSON response from Tranquility Server showing the number of events received and sent to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":39244}}\n```\n\n----------------------------------------\n\nTITLE: Example Lookup Configuration for Apache Druid (JSON)\nDESCRIPTION: This snippet shows a complete example of configuring a lookup tier with a cached namespace lookup. It includes the tier name, lookup name, version, and lookup extractor factory configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupValues\",\n          \"keyColumn\": \"value_id\",\n          \"valueColumn\": \"value_text\",\n          \"filter\": \"value_type='country'\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: License Notice for React Core\nDESCRIPTION: This snippet provides the license information for React core production build (v17.0.2), which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Running pull-deps Tool in Java for Apache Druid\nDESCRIPTION: Example command to run the pull-deps tool for downloading specific Druid extensions and Hadoop dependencies. It demonstrates using the clean option and specifying multiple coordinates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/pull-deps.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.14.2-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.14.2-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Starting a Druid Historical Node from Command Line in Java\nDESCRIPTION: Command to start a Historical Node server in Apache Druid using the Main class. This command initializes a historical node that will load and serve data segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/historical.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server historical\n```\n\n----------------------------------------\n\nTITLE: Enabling a Druid Middle Manager After Updates\nDESCRIPTION: Demonstrates how to manually enable a Middle Manager after updating using a POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/enable\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0c610519.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Escalator in Apache Druid\nDESCRIPTION: This snippet demonstrates the configuration of a Basic escalator in Druid. It sets the escalator type, internal client username and password, and the authorizer name to be used for escalated requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# Escalator\ndruid.escalator.type=basic\ndruid.escalator.internalClientUsername=druid_system\ndruid.escalator.internalClientPassword=password2\ndruid.escalator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Coordinator API Response for Lookup Configuration in Druid\nDESCRIPTION: Example response from the Coordinator API endpoint for a specific lookup. This shows what is returned when querying '/druid/coordinator/v1/lookups/realtime_customer2/country_code'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v0\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n      \"type\": \"jdbc\",\n      \"connectorConfig\": {\n        \"createTables\": true,\n        \"connectURI\": \"jdbc:mysql://localhost:3306/druid\",\n        \"user\": \"druid\",\n        \"password\": \"diurd\"\n      },\n      \"table\": \"lookupValues\",\n      \"keyColumn\": \"value_id\",\n      \"valueColumn\": \"value_text\",\n      \"filter\": \"value_type='country'\",\n      \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Data for Druid Ingestion\nDESCRIPTION: Example JSON data containing timestamp, animal, location, and number fields to be ingested into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"timestamp\":\"2018-01-01T07:01:35Z\",\"animal\":\"octopus\",  \"location\":1, \"number\":100},\n  {\"timestamp\":\"2018-01-01T05:01:35Z\",\"animal\":\"mongoose\", \"location\":2,\"number\":200},\n  {\"timestamp\":\"2018-01-01T06:01:35Z\",\"animal\":\"snake\", \"location\":3, \"number\":300},\n  {\"timestamp\":\"2018-01-01T01:01:35Z\",\"animal\":\"lion\", \"location\":4, \"number\":300}\n]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Scan Query Results in Compacted List Format\nDESCRIPTION: Example of Scan query results when resultFormat is set to 'compactedList'. It shows the segmentId, columns, and events in a more compact format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/scan-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\",\n    \"columns\" : [\n      \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\"\n    ],\n    \"events\" : [\n     [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0],\n     [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._243\", \"en\", \"1\", \"MZMcBride\", 1.0, 77.0, 77.0, 77.0, 0.0]\n    ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous Authenticator with Basic Security in Apache Druid\nDESCRIPTION: This JSON configuration enables the Anonymous Authenticator with the 'druid-basic-security' extension in Apache Druid. It sets up the authentication chain, specifies the anonymous authenticator type, identity, and authorizer name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/auth.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authenticatorChain\":[\"basic\", \"anonymous\"]\n\n\"druid.auth.authenticator.anonymous.type\":\"anonymous\"\n\"druid.auth.authenticator.anonymous.identity\":\"defaultUser\"\n\"druid.auth.authenticator.anonymous.authorizerName\":\"myBasicAuthorizer\"\n```\n\n----------------------------------------\n\nTITLE: Downloading and Setting Up Zookeeper\nDESCRIPTION: Commands to download Apache Zookeeper, extract it, and set it up for use with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/index.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Custom Buckets Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for computing histogram representation with custom break points.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"customBuckets\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"breaks\" : [ <value>, <value>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration for including the DataSketches extension in Druid. This must be added to the config file to use Theta sketch functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: JavaScript Extraction Function with Injective Property in Druid JSON\nDESCRIPTION: JavaScript extraction function that specifies the injective property to indicate whether uniqueness is preserved. Default is false.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"function\" : \"function(str) { return str + '!!!'; }\",\n  \"injective\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Whitelist-based Graphite Emitter\nDESCRIPTION: Configuration example for the 'whiteList' type event converter that selectively sends metrics based on a whitelist. Includes custom mapping file path and ignores hostname and service name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true, \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license declaration for React scheduler production bundle\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Downloading Multiple Extensions with Default Version\nDESCRIPTION: Example command showing how to download multiple Druid extensions using a default version specification, which simplifies the coordinate syntax by not requiring version information in each coordinate.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/pull-deps.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.14.0-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License Declaration\nDESCRIPTION: License declaration for Prism, a lightweight syntax highlighting library created by Lea Verou under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Enable MiddleManager Response - JSON\nDESCRIPTION: Example JSON response when enabling a MiddleManager, showing status keyed by host:port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"enabled\"}\n```\n\n----------------------------------------\n\nTITLE: Querying Rolled-up Data Using Druid SQL\nDESCRIPTION: Example of using Druid's SQL interface (dsql) to query the ingested and rolled-up data, showing how multiple original records were combined based on time bucket and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"rollup-tutorial\";\n\n __time                    bytes   count  dstIP    packets  srcIP   \n\n 2018-01-01T01:01:00.000Z   35937      3  2.2.2.2      286  1.1.1.1 \n 2018-01-01T01:02:00.000Z  366260      2  2.2.2.2      415  1.1.1.1 \n 2018-01-01T01:03:00.000Z   10204      1  2.2.2.2       49  1.1.1.1 \n 2018-01-02T21:33:00.000Z  100288      2  8.8.8.8      161  7.7.7.7 \n 2018-01-02T21:35:00.000Z    2818      1  8.8.8.8       12  7.7.7.7 \n\nRetrieved 5 rows in 1.18s.\n\ndsql>\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for React DOM\nDESCRIPTION: Copyright notice and MIT license information for React's react-dom.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Core License Information\nDESCRIPTION: License information for React's core react.production.min.js (v17.0.2) created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Drop Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a forever drop rule, which indicates that all matching segments should be dropped from the cluster permanently.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropForever\"  \n}\n```\n\n----------------------------------------\n\nTITLE: Avro Hadoop Parser Configuration\nDESCRIPTION: Configuration for batch ingestion using Avro Hadoop parser with custom schema file specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/avro.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",  \n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"\",\n      \"parser\" : {\n        \"type\" : \"avro_hadoop\",\n        \"parseSpec\" : {\n          \"format\": \"avro\",\n          \"timestampSpec\": <standard timestampSpec>,\n          \"dimensionsSpec\": <standard dimensionsSpec>,\n          \"flattenSpec\": <optional>\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\",\n        \"paths\" : \"\"\n      }\n    },\n    \"tuningConfig\" : {\n       \"jobProperties\" : {\n          \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Min Post-Aggregator Configuration in Apache Druid\nDESCRIPTION: JSON configuration for the min post-aggregator, which returns the minimum value of the underlying histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"min\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Registered Lookup Extraction Function Configuration in JSON\nDESCRIPTION: Configuration for using registered cluster-wide lookups to replace dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"registeredLookup\",\n  \"lookup\":\"some_lookup_name\",\n  \"retainMissingValue\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Storage Configuration Properties\nDESCRIPTION: Configuration properties for using Google Cloud Storage via the HDFS extension in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type: hdfs\ndruid.storage.storageDirectory: gs://bucket/example/directory\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantiles Doubles Sketch Aggregator in Druid\nDESCRIPTION: Shows the JSON configuration for the quantilesDoublesSketch aggregator, including required and optional parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"quantilesDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"k\": <parameter that controls size and accuracy>\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Emitter Properties in Apache Druid\nDESCRIPTION: Example configuration for setting up Kafka Emitter in Apache Druid. Specifies bootstrap servers, metric and alert topics, producer configuration, and an optional cluster name parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/kafka-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092\ndruid.emitter.kafka.metric.topic=druid-metric\ndruid.emitter.kafka.alert.topic=druid-alert\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Enabled Status in JSON (MiddleManager API)\nDESCRIPTION: This JSON response indicates whether a MiddleManager is in an enabled state. The key is the combined druid.host and druid.port, with a boolean value representing the state.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":true}\n```\n\n----------------------------------------\n\nTITLE: Registering a Password Provider in Druid's Jackson Module\nDESCRIPTION: Example of implementing getJacksonModules to register a custom PasswordProvider implementation with a specific name ('some') for Jackson's polymorphic serialization/deserialization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n    return ImmutableList.of(\n        new SimpleModule(\"SomePasswordProviderModule\")\n            .registerSubtypes(\n                new NamedType(SomePasswordProvider.class, \"some\")\n            )\n    );\n```\n\n----------------------------------------\n\nTITLE: Managing Compaction Configuration in Druid Coordinator API\nDESCRIPTION: GET, POST, and DELETE endpoints to manage compaction configuration for datasources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_11\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/config/compaction\nGET /druid/coordinator/v1/config/compaction/{dataSource}\nPOST /druid/coordinator/v1/config/compaction/taskslots?ratio={someRatio}&max={someMaxSlots}\nPOST /druid/coordinator/v1/config/compaction\nDELETE /druid/coordinator/v1/config/compaction/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Custom Origin Time\nDESCRIPTION: Example showing how to set a custom origin time in the granularity specification. The origin parameter defines the starting point for granularity buckets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Exploring Recommended Druid Configuration Structure Using Terminal Commands\nDESCRIPTION: This example shows the recommended organization of Druid configuration files using a directory listing. It demonstrates how configuration is split between service-specific directories and common configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -R conf\ndruid       tranquility\n\nconf/druid:\n_common       broker        coordinator   historical    middleManager overlord\n\nconf/druid/_common:\ncommon.runtime.properties log4j2.xml\n\nconf/druid/broker:\njvm.config         runtime.properties\n\nconf/druid/coordinator:\njvm.config         runtime.properties\n\nconf/druid/historical:\njvm.config         runtime.properties\n\nconf/druid/middleManager:\njvm.config         runtime.properties\n\nconf/druid/overlord:\njvm.config         runtime.properties\n\nconf/tranquility:\nkafka.json  server.json\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Custom Origin Time\nDESCRIPTION: Example showing how to set a custom origin time in the granularity specification. The origin parameter defines the starting point for granularity buckets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Producing Data to Kafka Topic\nDESCRIPTION: Commands to set Java encoding and use Kafka console producer to send sample data to the 'wikipedia' topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport KAFKA_OPTS=\"-Dfile.encoding=UTF-8\"\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json\n```\n\n----------------------------------------\n\nTITLE: HTTP Server Configuration for Druid Brokers\nDESCRIPTION: Jetty server configuration options for Druid Brokers that control thread counts, queue sizes, timeouts, and request handling. These settings affect how the Broker handles HTTP connections and manages query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_37\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.http.numThreads=max(10, (Number of cores * 17) / 16 + 2) + 30\ndruid.server.http.queueSize=Unbounded\ndruid.server.http.maxIdleTime=PT5M\ndruid.server.http.enableRequestLimit=false\ndruid.server.http.defaultQueryTimeout=300000\ndruid.server.http.maxScatterGatherBytes=Long.MAX_VALUE\ndruid.server.http.gracefulShutdownTimeout=PT0S\ndruid.server.http.unannouncePropagationDelay=PT0S\ndruid.server.http.maxQueryTimeout=Long.MAX_VALUE\ndruid.server.http.maxRequestHeaderSize=8 * 1024\n```\n\n----------------------------------------\n\nTITLE: Configuring TimedShutoffFirehose in Druid for Scheduled Termination\nDESCRIPTION: Configuration for the TimedShutoffFirehose which wraps another firehose and shuts it down at a specified time. This provides a mechanism for scheduled termination of streaming ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/firehose.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"timed\",\n    \"shutoffTime\": \"2015-08-25T01:26:05.119Z\",\n    \"delegate\": {\n          \"type\": \"receiver\",\n          \"serviceName\": \"eventReceiverServiceName\",\n          \"bufferSize\": 100000\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: HTML License Comment in Druid Documentation\nDESCRIPTION: HTML comment containing the Apache License 2.0 information for the Druid documentation. This common header appears in many Apache project files to indicate licensing terms.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/index.md#2025-04-09_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Process in Druid (Properties)\nDESCRIPTION: Essential configuration properties for setting up a Broker process in Druid, including host, port, and service name settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_28\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8082\ndruid.tlsPort=8282\ndruid.service=druid/broker\n```\n\n----------------------------------------\n\nTITLE: Configuring Base Zookeeper Properties in Druid\nDESCRIPTION: Core Zookeeper configuration properties including connection settings, authentication, and base paths. These settings control how Druid connects to and interacts with Zookeeper.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.service.host=none\ndruid.zk.service.user=none\ndruid.zk.service.pwd=none\ndruid.zk.service.authScheme=digest\n```\n\n----------------------------------------\n\nTITLE: Configuring Standard Deviation Post-Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for the standard deviation post-aggregator. It calculates the standard deviation from the variance and specifies the output name, aggregator name, and estimator type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"stddev\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"estimator\": <string>\n}\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: MIT license declaration for React's react-is.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Using Query Filter HavingSpec in Druid groupBy Query\nDESCRIPTION: Shows how to add a query filter HavingSpec to a groupBy query. This allows any Druid query filter to be used in the Having part of the query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\" : \"filter\",\n            \"filter\" : <any Druid query filter>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Intermediate Segment Storage Configuration in Apache Druid\nDESCRIPTION: Configuration for where realtime nodes store intermediate segments before they're published. The maxSize should always be zero.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/realtime.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.segmentCache.locations: none\n```\n\n----------------------------------------\n\nTITLE: Enable MiddleManager Response - JSON\nDESCRIPTION: Example JSON response when enabling a MiddleManager, showing status keyed by host:port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"enabled\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage via HDFS Extension in Apache Druid\nDESCRIPTION: Configuration properties for using the HDFS extension to connect to Google Cloud Storage as a deep storage option in Druid. Requires the GCS connector jar in the classpath.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.storage.type: hdfs\ndruid.storage.storageDirectory: gs://bucket/example/directory\n```\n\n----------------------------------------\n\nTITLE: Parsing InfluxDB Line Protocol in Apache Druid\nDESCRIPTION: This snippet demonstrates the InfluxDB Line Protocol format. It shows a typical line of data containing a measurement, tags, measurements, and a timestamp.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/influx.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncpu,application=dbhost=prdb123,region=us-east-1 usage_idle=99.24,usage_user=0.55 1520722030000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring CustomJSON Lookup Parse Specification in Druid\nDESCRIPTION: JSON configuration for custom JSON lookup parsing in Druid, specifying key and value field names for extracting lookup data from JSON objects.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"customJson\",\n  \"keyFieldName\": \"key\",\n  \"valueFieldName\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Column Comparison Filter in Apache Druid JSON\nDESCRIPTION: Shows the JSON structure for a column comparison filter in Apache Druid. This filter compares two dimensions to each other, equivalent to WHERE dimension_a = dimension_b in SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n\"filter\": { \"type\": \"columnComparison\", \"dimensions\": [<dimension_a>, <dimension_b>] }\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous Authenticator with Basic Security in Druid\nDESCRIPTION: This JSON configuration enables the Anonymous Authenticator with the druid-basic-security extension, setting up a default level of access at the end of the authentication chain.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/auth.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authenticatorChain\":[\"basic\", \"anonymous\"]\n\n\"druid.auth.authenticator.anonymous.type\":\"anonymous\"\n\"druid.auth.authenticator.anonymous.identity\":\"defaultUser\"\n\"druid.auth.authenticator.anonymous.authorizerName\":\"myBasicAuthorizer\"\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Network Flow Data in JSON Format\nDESCRIPTION: Example JSON data representing network flow events with timestamp, source IP, destination IP, packet count, and byte count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2018-01-01T01:01:35Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":20,\n  \"bytes\":9024\n}\n```\n\n----------------------------------------\n\nTITLE: Sparse Histogram Binary Serialization Format\nDESCRIPTION: Specification for the sparse histogram serialization format that uses (bucketNum, count) pairs. Used when less than half of the buckets have values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x02 for sparse\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\nint: number of following (bucketNum, count) pairs\nsequence of (int, long) pairs:\n  int: bucket number\n  count: bucket count\n```\n\n----------------------------------------\n\nTITLE: Configuring IngestSegmentFirehose in Apache Druid\nDESCRIPTION: IngestSegmentFirehose configuration for reading data from existing Druid segments. It can ingest existing segments using a new schema and change various properties like name, dimensions, metrics, and rollup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"ingestSegment\",\n    \"dataSource\"   : \"wikipedia\",\n    \"interval\" : \"2013-01-01/2013-01-02\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring License for React Scheduler in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the React Scheduler production minified file, version 0.20.2, created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Full Histogram Serialization Format\nDESCRIPTION: Binary format specification for full histogram serialization that includes complete bucket count array. Uses version 0x01 and encoding mode 0x01.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x01 for full\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\narray of longs: bucket counts for the histogram\n```\n\n----------------------------------------\n\nTITLE: Period Drop Rule Configuration in Druid\nDESCRIPTION: Defines segment dropping based on a rolling time period using ISO-8601 period format. Includes option for future data consideration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Expression Function Example\nDESCRIPTION: Example showing function syntax for general purpose operations like casting, conditional logic, and null handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/misc/math-expr.md#2025-04-09_snippet_1\n\nLANGUAGE: expression\nCODE:\n```\ncast(expr,'LONG' or 'DOUBLE' or 'STRING')\nif(predicate,then,else)\nnvl(expr,expr-for-null)\nlike(expr, pattern[, escape])\ncase_searched(expr1, result1, [[expr2, result2, ...], else-result])\ncase_simple(expr, value1, result1, [[value2, result2, ...], else-result])\nbloom_filter_test(expr, filter)\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration\nDESCRIPTION: YAML frontmatter defining the page layout and title for the Druid console documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/druid-console.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) console\"\n---\n```\n\n----------------------------------------\n\nTITLE: Prism License Notice\nDESCRIPTION: License notice for the Prism syntax highlighting library, created by Lea Verou and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Input Data for Druid Transformation\nDESCRIPTION: Example JSON data containing timestamp, animal, location and number fields that will be transformed during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\":\"2018-01-01T07:01:35Z\",\"animal\":\"octopus\",  \"location\":1, \"number\":100}\n{\"timestamp\":\"2018-01-01T05:01:35Z\",\"animal\":\"mongoose\", \"location\":2,\"number\":200}\n{\"timestamp\":\"2018-01-01T06:01:35Z\",\"animal\":\"snake\", \"location\":3, \"number\":300}\n{\"timestamp\":\"2018-01-01T01:01:35Z\",\"animal\":\"lion\", \"location\":4, \"number\":300}\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL User for Druid\nDESCRIPTION: Command to create a PostgreSQL user named 'druid' with password prompt. This user will own the Druid database and be used for connections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncreateuser druid -P\n```\n\n----------------------------------------\n\nTITLE: Using Custom Password Provider in Apache Druid (JSON)\nDESCRIPTION: This code snippet shows the format for configuring a custom password provider implementation in Apache Druid. This allows users to implement their own password retrieval mechanisms through Druid extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/password-provider.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"<registered_password_provider_name>\", \"<jackson_property>\": \"<value>\", ... }\n```\n\n----------------------------------------\n\nTITLE: Configuring Upper Case Extraction Function with Locale in Apache Druid\nDESCRIPTION: Illustrates the configuration of an Upper Case extraction function with a specified locale. This function converts dimension values to uppercase using the specified language rules.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"upper\",\n  \"locale\":\"fr\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Input Data for Druid Transformation\nDESCRIPTION: Example JSON data containing timestamp, animal, location and number fields that will be transformed during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\":\"2018-01-01T07:01:35Z\",\"animal\":\"octopus\",  \"location\":1, \"number\":100}\n{\"timestamp\":\"2018-01-01T05:01:35Z\",\"animal\":\"mongoose\", \"location\":2,\"number\":200}\n{\"timestamp\":\"2018-01-01T06:01:35Z\",\"animal\":\"snake\", \"location\":3, \"number\":300}\n{\"timestamp\":\"2018-01-01T01:01:35Z\",\"animal\":\"lion\", \"location\":4, \"number\":300}\n```\n\n----------------------------------------\n\nTITLE: Distinct Countries Cardinality Example\nDESCRIPTION: Example configuration for counting distinct countries across two dimensions: country of origin and residence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/hll-old.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_countries\",\n  \"fields\": [ \"country_of_origin\", \"country_of_residence\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bound Filter in Druid Query (JSON)\nDESCRIPTION: This snippet demonstrates how to configure a bound filter in a Druid query. It filters for age values between 21 and 31 using numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"upper\": \"31\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in Druid's config file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring stringLast Aggregator in Apache Druid for Queries\nDESCRIPTION: The stringLast aggregator computes the string metric value with the maximum timestamp or null if no row exists. It supports optional parameters for maximum string bytes and filtering null values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Configuration\nDESCRIPTION: Configuration snippet to include the DataSketches extension in Druid's config file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList=[\\\"druid-datasketches\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring White-list Based Converter for Ambari Metrics in Druid\nDESCRIPTION: JSON configuration for the 'whiteList' event converter that selectively sends metrics to Ambari metrics collector based on a whitelist. Includes options for namespace prefix, application name, hostname handling, and a custom whitelist map path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"appName\":\"druid\", \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Curl Command for Accessing Druid Endpoints with Kerberos\nDESCRIPTION: Example curl commands to access Druid HTTP endpoints with Kerberos authentication using cookie-based persistence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json' <HTTP_END_POINT>\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json\n```\n\n----------------------------------------\n\nTITLE: Enable MiddleManager Response - JSON\nDESCRIPTION: Example response when enabling a MiddleManager via the /druid/worker/v1/enable endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"enabled\"}\n```\n\n----------------------------------------\n\nTITLE: Middle Manager Enable API Request\nDESCRIPTION: HTTP POST endpoint to manually re-enable a Middle Manager node after updating.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_3\n\nLANGUAGE: http\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/enable\n```\n\n----------------------------------------\n\nTITLE: Native JSON Query Log Example - TSV Format\nDESCRIPTION: Example of a native JSON query log entry showing timestamp, remote address, query details and context in TSV format. The log demonstrates a topN query with aggregations and filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_5\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1   {\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikiticker\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"LegacyDimensionSpec\",\"dimension\":\"page\",\"outputName\":\"page\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"LegacyTopNMetricSpec\",\"metric\":\"count\"},\"threshold\":10,\"intervals\":{\"type\":\"LegacySegmentSpec\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"count\"}],\"postAggregations\":[],\"context\":{\"queryId\":\"74c2d540-d700-4ebd-b4a9-3d02397976aa\"},\"descending\":false}    {\"query/time\":100,\"query/bytes\":800,\"success\":true,\"identity\":\"user1\"}\n```\n\n----------------------------------------\n\nTITLE: Query Example: Intersection of Unique Users for Products A and B\nDESCRIPTION: Complex query example that computes the intersection of unique users who visited both product A and product B. This demonstrates filtered aggregators and sketch set operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"},\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"B\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"A\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"B\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"B_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"B_unique_users\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\n    \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Druid Coordinator Leadership in HTTP\nDESCRIPTION: This HTTP GET request returns a JSON object indicating if the server is the current leader Coordinator, along with appropriate HTTP status codes for load balancer checks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/isLeader\n```\n\n----------------------------------------\n\nTITLE: Metadata Retrieval Configuration Properties Table\nDESCRIPTION: Configuration settings for metadata management including polling durations and rule management parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_28\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.manager.config.pollDuration`|How often the manager polls the config table for updates.|PT1M|\n|`druid.manager.segments.pollDuration`|The duration between polls the Coordinator does for updates to the set of active segments. Generally defines the amount of lag time it can take for the Coordinator to notice new segments.|PT1M|\n|`druid.manager.rules.pollDuration`|The duration between polls the Coordinator does for updates to the set of active rules. Generally defines the amount of lag time it can take for the Coordinator to notice rules.|PT1M|\n|`druid.manager.rules.defaultTier`|The default tier from which default rules will be loaded from.|_default|\n|`druid.manager.rules.alertThreshold`|The duration after a failed poll upon which an alert should be emitted.|PT10M|\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Sharding in Druid\nDESCRIPTION: Configuration for linear sharding strategy that supports non-sequential partition numbering and easier process scaling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.14f7867a.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Query Example: Intersection of Unique Users for Products A and B\nDESCRIPTION: Complex query example that computes the intersection of unique users who visited both product A and product B. This demonstrates filtered aggregators and sketch set operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"},\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"B\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"A\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"selector\",\n        \"dimension\" : \"product\",\n        \"value\" : \"B\"\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"B_unique_users\", \"fieldName\": \"user_id_sketch\"\n      }\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"B_unique_users\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\n    \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling Off-heap Lookup Cache in Apache Druid\nDESCRIPTION: JSON configuration for a polling off-heap lookup cache that is cached once and never swapped. It uses a JDBC data fetcher to retrieve data from a MySQL database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"offHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: NProgress MIT License Declaration\nDESCRIPTION: License declaration for NProgress, a JavaScript library created by Rico Sta. Cruz under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Processing Check Comment\nDESCRIPTION: Comment indicating a check for previous processing state.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.0e5d9014.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*! Check if previously processed */\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL User for Druid\nDESCRIPTION: Command to create a PostgreSQL user named 'druid' with password prompt. This user will own the Druid database and be used for connections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncreateuser druid -P\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator in Apache Druid\nDESCRIPTION: This snippet shows how to configure a Cardinality aggregator in Apache Druid. It computes the cardinality of a set of dimensions using HyperLogLog estimation. The aggregator can be configured to compute cardinality by value or by row.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/hll-old.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"<output_name>\",\n  \"fields\": [ <dimension1>, <dimension2>, ... ],\n  \"byRow\": <false | true> # (optional, defaults to false),\n  \"round\": <false | true> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration History API Endpoint\nDESCRIPTION: HTTP endpoint for retrieving audit history of Coordinator configuration changes with an interval parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?interval=<interval>\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Cache Properties in YAML\nDESCRIPTION: YAML configuration block for setting up Redis cache in Druid's common.runtime.properties file. Includes host, port, expiration time, timeout, and connection pool settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/redis-cache.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.cache.host: None\ndruid.cache.port: None\ndruid.cache.expiration: 24 * 3600 * 1000\ndruid.cache.timeout: 2000\ndruid.cache.maxTotalConnections: 8\ndruid.cache.maxIdleConnections: 8\ndruid.cache.minIdleConnections: 0\n```\n\n----------------------------------------\n\nTITLE: Query Results from Transformed Data\nDESCRIPTION: Example query output showing the transformed data with the modified animal column and new triple-number column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"transform-tutorial\";\n\n __time                    animal          count  location  number  triple-number \n\n 2018-01-01T05:01:00.000Z  super-mongoose      1         2     200            600 \n 2018-01-01T06:01:00.000Z  super-snake         1         3     300            900 \n 2018-01-01T07:01:00.000Z  super-octopus       1         1     100            300 \n\nRetrieved 3 rows in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom JSON Lookup in Druid\nDESCRIPTION: Example configuration for a custom JSON lookup in Druid using namespaceParseSpec. It specifies key and value field names for parsing JSON data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"customJson\",\n  \"keyFieldName\": \"key\",\n  \"valueFieldName\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Search Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A filter for partial string matches using different search types. This example uses an insensitive contains search to find 'foo' substring in the product dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"search\",\n        \"dimension\": \"product\",\n        \"query\": {\n          \"type\": \"insensitive_contains\",\n          \"value\": \"foo\" \n        }        \n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variable Password Provider in Apache Druid\nDESCRIPTION: JSON configuration for the environment variable password provider which retrieves passwords from specified environment variables. This approach avoids storing passwords directly in runtime.properties files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/password-provider.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" }\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Pending Response\nDESCRIPTION: Example JSON response showing received events but pending processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":0}}\n```\n\n----------------------------------------\n\nTITLE: Max Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Max post-aggregator, which returns the maximum value of the underlying approximate histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"max\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Process in Apache Druid\nDESCRIPTION: These properties define the core settings for Broker processes in Druid, including host information, port configuration, and service naming.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_41\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8082\ndruid.tlsPort=8282\ndruid.service=druid/broker\n```\n\n----------------------------------------\n\nTITLE: Performing Student's t-test on ArrayOfDoublesSketch Instances\nDESCRIPTION: Post-aggregator configuration to perform Student's t-test on two ArrayOfDoublesSketch instances and return p-values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchTTest\",\n  \"name\": <output name>,\n  \"fields\"  : <array with two post aggregators to access sketch aggregators or post aggregators referring to an ArrayOfDoublesSketch>,\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Segment Serving Path in ZooKeeper\nDESCRIPTION: The permanent ZooKeeper path where processes create znodes to indicate which segments they are serving.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Regex Extraction Function Configuration in Druid\nDESCRIPTION: Configuration for extracting values using regular expressions with support for missing value handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"regex\",\n  \"expr\" : <regular_expression>,\n  \"index\" : <group to extract, default 1>,\n  \"replaceMissingValue\" : true,\n  \"replaceMissingValueWith\" : \"foobar\"\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Legacy Coordinator Console URL\nDESCRIPTION: Shows the URL pattern for accessing the version 1 (legacy) Coordinator console which is maintained for backwards compatibility. It's accessed through the old-console path on the Coordinator process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/management-uis.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console\n```\n\n----------------------------------------\n\nTITLE: Node Announcement Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path where Historical and Realtime nodes announce their presence using ephemeral znodes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.announcementsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Druid HDFS Storage Configuration\nDESCRIPTION: Configuration properties for enabling HDFS deep storage and log storage in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\n#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS:\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#\n# Indexing service logs\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Extracting Apache Druid Package in Bash\nDESCRIPTION: These commands extract the downloaded Druid tarball and navigate to the extracted directory. The extraction creates various directories containing scripts, configurations, extensions, and dependencies needed for Druid operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/index.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.14.0-incubating-bin.tar.gz\ncd apache-druid-0.14.0-incubating\n```\n\n----------------------------------------\n\nTITLE: Implementing GroupBy Query with DistinctCount in Druid\nDESCRIPTION: Example of a GroupBy query using DistinctCount aggregator to count unique visitors grouped by a sample dimension. The query uses 'all' granularity for a specific day interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimensions\": \"[sample_dim]\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: Copyright notice and MIT license declaration for mark.js v8.11.1, a text marking library created by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: License Declaration for Prism Syntax Highlighting Library\nDESCRIPTION: MIT license declaration for the Prism syntax highlighting library created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Retention Analysis Query Using ThetaSketch in Druid\nDESCRIPTION: Illustrates a complex retention analysis query using thetaSketch aggregators and set operations to find users who performed specific actions in different time periods.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_1\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-08T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" : {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_2\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_1\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_2\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\"2014-10-01T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Standard Deviation Post-Aggregator for Apache Druid\nDESCRIPTION: JSON configuration for the stddev post-aggregator that calculates standard deviation from a variance aggregator. It allows choosing between population and sample estimation methods.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"stddev\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"estimator\": <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid Console URL\nDESCRIPTION: The URL pattern to access the Druid Console after enabling necessary cluster settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/druid-console.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Get Single Lookup Response Example\nDESCRIPTION: JSON response format for retrieving a specific lookup's LookupExtractorFactory configuration, including version and mapping details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/lookups.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DataSketches Extension in Druid Config\nDESCRIPTION: Configuration required to include the DataSketches extension in Druid's config file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Stream Parser with Schema Repo\nDESCRIPTION: Example configuration for Avro stream parser using schema repository integration. Demonstrates setup of avroBytesDecoder with schema repo and parse specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"parser\" : {\n    \"type\" : \"avro_stream\",\n    \"avroBytesDecoder\" : {\n      \"type\" : \"schema_repo\",\n      \"subjectAndIdConverter\" : {\n        \"type\" : \"avro_1124\",\n        \"topic\" : \"${YOUR_TOPIC}\"\n      },\n      \"schemaRepository\" : {\n        \"type\" : \"avro_1124_rest_client\",\n        \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\"\n      }\n    },\n    \"parseSpec\" : {\n      \"format\": \"avro\",\n      \"timestampSpec\": <standard timestampSpec>,\n      \"dimensionsSpec\": <standard dimensionsSpec>,\n      \"flattenSpec\": <optional>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Period Drop Before Rule Configuration in Druid\nDESCRIPTION: Configures dropping of segments older than a specified period using ISO-8601 period format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropBeforeByPeriod\",\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid SQL Server Properties in Markdown\nDESCRIPTION: A markdown table listing configuration properties for the Druid SQL server on the Broker node. It includes properties for enabling SQL, JDBC querying, HTTP querying, and various planner settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.sql.enable`|Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely.|false|\n|`druid.sql.avatica.enable`|Whether to enable JDBC querying at `/druid/v2/sql/avatica/`.|true|\n|`druid.sql.avatica.maxConnections`|Maximum number of open connections for the Avatica server. These are not HTTP connections, but are logical client connections that may span multiple HTTP connections.|25|\n|`druid.sql.avatica.maxRowsPerFrame`|Maximum number of rows to return in a single JDBC frame. Setting this property to -1 indicates that no row limit should be applied. Clients can optionally specify a row limit in their requests; if a client specifies a row limit, the lesser value of the client-provided limit and `maxRowsPerFrame` will be used.|5,000|\n|`druid.sql.avatica.maxStatementsPerConnection`|Maximum number of simultaneous open statements per Avatica client connection.|4|\n|`druid.sql.avatica.connectionIdleTimeout`|Avatica client connection idle timeout.|PT5M|\n|`druid.sql.http.enable`|Whether to enable JSON over HTTP querying at `/druid/v2/sql/`.|true|\n|`druid.sql.planner.maxQueryCount`|Maximum number of queries to issue, including nested queries. Set to 1 to disable sub-queries, or set to 0 for unlimited.|8|\n|`druid.sql.planner.maxSemiJoinRowsInMemory`|Maximum number of rows to keep in memory for executing two-stage semi-join queries like `SELECT * FROM Employee WHERE DeptName IN (SELECT DeptName FROM Dept)`.|100000|\n|`druid.sql.planner.maxTopNLimit`|Maximum threshold for a [TopN query](../querying/topnquery.html). Higher limits will be planned as [GroupBy queries](../querying/groupbyquery.html) instead.|100000|\n|`druid.sql.planner.metadataRefreshPeriod`|Throttle for metadata refreshes.|PT1M|\n|`druid.sql.planner.useApproximateCountDistinct`|Whether to use an approximate cardinalty algorithm for `COUNT(DISTINCT foo)`.|true|\n|`druid.sql.planner.useApproximateTopN`|Whether to use approximate [TopN queries](../querying/topnquery.html) when a SQL query could be expressed as such. If false, exact [GroupBy queries](../querying/groupbyquery.html) will be used instead.|true|\n|`druid.sql.planner.requireTimeCondition`|Whether to require SQL to have filter conditions on __time column so that all generated native queries will have user specified intervals. If true, all queries wihout filter condition on __time column will fail|false|\n|`druid.sql.planner.sqlTimeZone`|Sets the default time zone for the server, which will affect how time functions and timestamp literals behave. Should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\".|UTC|\n|`druid.sql.planner.metadataSegmentCacheEnable`|Whether to keep a cache of published segments in broker. If true, broker polls coordinator in background to get segments from metadata store and maintains a local cache. If false, coordinator's REST api will be invoked when broker needs published segments info.|false|\n|`druid.sql.planner.metadataSegmentPollPeriod`|How often to poll coordinator for published segments list if `druid.sql.planner.metadataSegmentCacheEnable` is set to true. Poll period is in milliseconds. |60000|\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Help Command\nDESCRIPTION: Command to display the help documentation for the ResetCluster tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main help tools reset-cluster\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Settings in Druid\nDESCRIPTION: TLS/SSL configuration properties for Druid's Jetty server including keystore settings and cipher suites.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\ndruid.server.https.keyStorePath=none\ndruid.server.https.keyStoreType=none\ndruid.server.https.certAlias=none\ndruid.server.https.keyStorePassword=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Converter for Ambari Metrics in Apache Druid\nDESCRIPTION: Configuration for the 'all' event converter implementation which sends all Druid service metrics events to Ambari Metrics. The path will follow the format <namespacePrefix>.[<druid service name>].[<druid hostname>].<dimensions values ordered by dimension's name>.<metric>.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"appName\":\"druid\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Storage in Druid\nDESCRIPTION: Configuration settings for enabling HDFS storage in Druid, including deep storage and indexing service logs configuration. Requires druid-hdfs-storage extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-hdfs-storage\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Defining Miscellaneous Documentation Links in Markdown\nDESCRIPTION: A markdown section containing links to miscellaneous Druid documentation resources. The links use relative paths with a VERSION placeholder that gets replaced with the actual version number during site generation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/toc.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## Misc\n  * [Druid Expressions Language](/docs/VERSION/misc/math-expr.html)\n  * [Papers & Talks](/docs/VERSION/misc/papers-and-talks.html)\n  * [Thanks](/thanks.html)\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Supervisor for Protobuf Ingestion\nDESCRIPTION: Complete Supervisor spec JSON for submitting to the Overlord, configuring Kafka ingestion of Protobuf data in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"kafka\",\n  \"dataSchema\": {\n    \"dataSource\": \"metrics-kafka2\",\n    \"parser\": {\n      \"type\": \"protobuf\",\n      \"descriptor\": \"file:///tmp/metrics.desc\",\n      \"protoMessageType\": \"Metrics\",\n      \"parseSpec\": {\n        \"format\": \"json\",\n        \"timestampSpec\": {\n          \"column\": \"timestamp\",\n          \"format\": \"auto\"\n        },\n        \"dimensionsSpec\": {\n          \"dimensions\": [\n            \"unit\",\n            \"http_method\",\n            \"http_code\",\n            \"page\",\n            \"metricType\",\n            \"server\"\n          ],\n          \"dimensionExclusions\": [\n            \"timestamp\",\n            \"value\"\n          ]\n        }\n      }\n    },\n    \"metricsSpec\": [\n      {\n        \"name\": \"count\",\n        \"type\": \"count\"\n      },\n      {\n        \"name\": \"value_sum\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleSum\"\n      },\n      {\n        \"name\": \"value_min\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMin\"\n      },\n      {\n        \"name\": \"value_max\",\n        \"fieldName\": \"value\",\n        \"type\": \"doubleMax\"\n      }\n    ],\n    \"granularitySpec\": {\n      \"type\": \"uniform\",\n      \"segmentGranularity\": \"HOUR\",\n      \"queryGranularity\": \"NONE\"\n    }\n  },\n  \"tuningConfig\": {\n    \"type\": \"kafka\",\n    \"maxRowsPerSegment\": 5000000\n  },\n  \"ioConfig\": {\n    \"topic\": \"metrics_pb\",\n    \"consumerProperties\": {\n      \"bootstrap.servers\": \"localhost:9092\"\n    },\n    \"taskCount\": 1,\n    \"replicas\": 1,\n    \"taskDuration\": \"PT1H\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Object-Assign MIT License Attribution\nDESCRIPTION: Copyright notice for the object-assign JavaScript utility, created by Sindre Sorhus and licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Apache Druid TopN Query with Variance and Standard Deviation\nDESCRIPTION: Example of a TopN query that calculates variance of the index field and derives standard deviation using a post-aggregator, grouped by the alias dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"threshold\": 5,\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring None Sharding in Druid\nDESCRIPTION: A simple JSON configuration snippet for specifying 'none' as the sharding strategy in Druid, which is suitable for small-data scenarios where sharding is unnecessary.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/stream-pull.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\"type\": \"none\"}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Defining Column Comparison Filter in Apache Druid JSON Query\nDESCRIPTION: The column comparison filter compares two dimensions to each other. It is equivalent to comparing two columns in a WHERE clause in SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"columnComparison\", \"dimensions\": [<dimension_a>, <dimension_b>] }\n```\n\n----------------------------------------\n\nTITLE: Search Query Result Format in Apache Druid\nDESCRIPTION: This JSON snippet illustrates the format of results returned by a search query in Apache Druid. It shows the structure with timestamps and matching dimension values, including their counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/searchquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2012-01-01T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"Ke$ha\",\n        \"count\": 3\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"Ke$haForPresident\",\n        \"count\": 1\n      }\n    ]\n  },\n  {\n    \"timestamp\": \"2012-01-02T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dimension\": \"dim1\",\n        \"value\": \"SomethingThatContainsKe\",\n        \"count\": 1\n      },\n      {\n        \"dimension\": \"dim2\",\n        \"value\": \"SomethingElseThatContainsKe\",\n        \"count\": 2\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Emitter Module in Druid\nDESCRIPTION: Properties for configuring the HTTP emitter module in Druid. This module is used for emitting metrics and alerts via HTTP POST requests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.http.flushMillis=60000\ndruid.emitter.http.flushCount=500\ndruid.emitter.http.recipientBaseUrl=none\n```\n\n----------------------------------------\n\nTITLE: Implementing Kafka Producer for Protobuf Messages\nDESCRIPTION: Python script that reads JSON data, converts it to Protobuf format, and publishes it to a Kafka topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\nimport json\n\nfrom kafka import KafkaProducer\nfrom metrics_pb2 import Metrics\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ntopic = 'metrics_pb'\nmetrics = Metrics()\n\nfor row in iter(sys.stdin):\n    d = json.loads(row)\n    for k, v in d.items():\n        setattr(metrics, k, v)\n    pb = metrics.SerializeToString()\n    producer.send(topic, pb)\n```\n\n----------------------------------------\n\nTITLE: Extracting Apache Druid Package in Bash\nDESCRIPTION: These commands extract the downloaded Druid tarball and navigate to the extracted directory. The extraction creates various directories containing scripts, configurations, extensions, and dependencies needed for Druid operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/index.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.14.0-incubating-bin.tar.gz\ncd apache-druid-0.14.0-incubating\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Lookup in Druid\nDESCRIPTION: Example configuration for a CSV lookup in Druid using namespaceParseSpec. It defines columns, key column, and value column for parsing CSV data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"csv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Lower Bound Age Filter in Druid\nDESCRIPTION: Example of a one-sided bound filter that checks if age is greater than or equal to 18.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"18\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced TLS Settings in Apache Druid\nDESCRIPTION: These are advanced configuration options for TLS in Apache Druid. They include settings for key manager algorithms, cipher suites, and protocol selection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/tls-support.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.https.keyManagerFactoryAlgorithm=javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()\ndruid.server.https.keyManagerPassword=none\ndruid.server.https.includeCipherSuites=Jetty's default include cipher list\ndruid.server.https.excludeCipherSuites=Jetty's default exclude cipher list\ndruid.server.https.includeProtocols=Jetty's default include protocol list\ndruid.server.https.excludeProtocols=Jetty's default exclude protocol list\n```\n\n----------------------------------------\n\nTITLE: SQL TopN Query for Wikipedia Page Edits\nDESCRIPTION: Equivalent SQL query to get top 10 Wikipedia pages with most edits. Shows how to use time filtering, grouping, and ordering in Druid SQL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Min/Max Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for computing minimum and maximum values from histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"min\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"max\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Defining a Kill Task in Apache Druid\nDESCRIPTION: JSON specification for a Kill Task that deletes all information about a segment and removes it from deep storage. Kill Tasks target disabled segments (used==0) within a specified interval in the Druid segment table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"kill\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\" : <all_segments_in_this_interval_will_die!>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Using Kinit for Kerberos Authentication\nDESCRIPTION: Command to login using kinit with a keytab file for accessing Druid HTTP endpoints.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkinit -k -t <path_to_keytab_file> user@REALM.COM\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Path Properties in Apache Druid\nDESCRIPTION: Defines various Zookeeper paths used by Druid for announcements, segments, and coordination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.paths.propertiesPath=${druid.zk.paths.base}/properties\ndruid.zk.paths.announcementsPath=${druid.zk.paths.base}/announcements\ndruid.zk.paths.liveSegmentsPath=${druid.zk.paths.base}/segments\ndruid.zk.paths.loadQueuePath=${druid.zk.paths.base}/loadQueue\ndruid.zk.paths.coordinatorPath=${druid.zk.paths.base}/coordinator\ndruid.zk.paths.servedSegmentsPath=${druid.zk.paths.base}/servedSegments\n```\n\n----------------------------------------\n\nTITLE: Regex Extraction Function Configuration\nDESCRIPTION: Configuration for extracting dimension values using regular expressions with support for missing value handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"regex\",\n  \"expr\" : <regular_expression>,\n  \"index\" : <group to extract, default 1>\n  \"replaceMissingValue\" : true,\n  \"replaceMissingValueWith\" : \"foobar\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Broker Process Settings\nDESCRIPTION: Core configuration properties for the Druid Broker process including host settings, ports, and service name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_31\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8082\ndruid.tlsPort=8282\ndruid.service=druid/broker\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - System Metrics\nDESCRIPTION: Table describing system-level metrics including swap, disk, network, and CPU usage\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`sys/swap/free`|Free swap.||Varies.|\n```\n\n----------------------------------------\n\nTITLE: Distinct People Cardinality Example\nDESCRIPTION: Example configuration for counting distinct combinations of first and last names using byRow=true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/hll-old.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_people\",\n  \"fields\": [ \"first_name\", \"last_name\" ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing zscore2sample Post Aggregator in Druid\nDESCRIPTION: JSON configuration for the zscore2sample post aggregator which calculates the z-score using a two-sample z-test. It converts binary variables (success/failure) to continuous variables (proportions) and computes the statistical difference between two samples.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/test-stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"zscore2sample\",\n  \"name\": \"<output_name>\",\n  \"successCount1\": <post_aggregator> success count of sample 1,\n  \"sample1Size\": <post_aggregaror> sample 1 size,\n  \"successCount2\": <post_aggregator> success count of sample 2,\n  \"sample2Size\" : <post_aggregator> sample 2 size\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Features in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to enable experimental features in Druid by adding them to the loadList in runtime.properties. This example shows enabling the druid-histogram extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/experimental.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-histogram\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Community Extensions using Pull-deps Tool\nDESCRIPTION: Command to download and install a community extension using Druid's pull-deps tool. The example shows installing a hypothetical extension with Maven coordinates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/including-extensions.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava \\\n  -cp \"lib/*\" \\\n  -Ddruid.extensions.directory=\"extensions\" \\\n  -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n  org.apache.druid.cli.Main tools pull-deps \\\n  --no-default-hadoop \\\n  -c \"com.example:druid-example-extension:1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Enabling DataSketches HLL Sketch Extension in Druid Configuration\nDESCRIPTION: This snippet shows how to include the DataSketches extension in the Druid configuration file. It's required to use the HLL sketch aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Real-time Thrift Ingestion in Tranquility\nDESCRIPTION: JSON configuration for real-time ingestion of Thrift data using Tranquility. Specifies the data schema, parser configuration, and required Thrift class information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSources\": [{\n    \"spec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"book\",\n        \"granularitySpec\": {          },\n        \"parser\": {\n          \"type\": \"thrift\",\n          \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n          \"protocol\": \"compact\",\n          \"parseSpec\": {\n            \"format\": \"json\",\n            \"...\": \"...\"\n          }\n        },\n        \"metricsSpec\": [...]\n      },\n      \"tuningConfig\": {...}\n    },\n    \"properties\": {...}\n  }],\n  \"properties\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Escalator\nDESCRIPTION: Properties for setting up the Escalator component which handles internal system requests. Includes type, internal client credentials, and authorizer reference.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# Escalator\ndruid.escalator.type=basic\ndruid.escalator.internalClientUsername=druid_system\ndruid.escalator.internalClientPassword=password2\ndruid.escalator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Query Prioritization\nDESCRIPTION: Settings for controlling how the broker balances and prioritizes queries across Historical processes and tiers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_32\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.balancer.type=random\ndruid.broker.select.tier=highestPriority\ndruid.broker.select.tier.custom.priorities=None\n```\n\n----------------------------------------\n\nTITLE: Running a Kill Task to Permanently Delete Segments in Apache Druid\nDESCRIPTION: Command to submit a Kill Task to the Druid Overlord using a predefined task specification, which will permanently delete disabled segments from both metadata and deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Sample Druid Worker Configuration JSON\nDESCRIPTION: Example JSON configuration for Druid worker behavior, including selection strategy with affinity configuration and EC2 auto-scaling settings. This defines how tasks are distributed and how worker nodes are managed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"selectStrategy\": {\n    \"type\": \"fillCapacity\",\n    \"affinityConfig\": {\n      \"affinity\": {\n        \"datasource1\": [\"host1:port\", \"host2:port\"],\n        \"datasource2\": [\"host3:port\"]\n      }\n    }\n  },\n  \"autoScaler\": {\n    \"type\": \"ec2\",\n    \"minNumWorkers\": 2,\n    \"maxNumWorkers\": 12,\n    \"envConfig\": {\n      \"availabilityZone\": \"us-east-1a\",\n      \"nodeData\": {\n        \"amiId\": \"${AMI}\",\n        \"instanceType\": \"c3.8xlarge\",\n        \"minInstances\": 1,\n        \"maxInstances\": 1,\n        \"securityGroupIds\": [\"${IDs}\"],\n        \"keyName\": \"${KEY_NAME}\"\n      },\n      \"userData\": {\n        \"impl\": \"string\",\n        \"data\": \"${SCRIPT_COMMAND}\",\n        \"versionReplacementString\": \":VERSION:\",\n        \"version\": null\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Experimental Features in Druid Configuration\nDESCRIPTION: Configuration snippet showing how to enable experimental features in Druid by adding them to the loadList in runtime.properties. This example shows enabling the druid-histogram extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/experimental.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-histogram\"]\n```\n\n----------------------------------------\n\nTITLE: Common GroupBy Strategy Configurations in Apache Druid\nDESCRIPTION: Configuration settings applicable to all GroupBy query strategies, including threading and default strategy selection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/groupbyquery.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.defaultStrategy`|Default groupBy query strategy.|v2|\n|`druid.query.groupBy.singleThreaded`|Merge results using a single thread.|false|\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop-based Parquet Ingestion in Druid\nDESCRIPTION: JSON configuration for ingesting Parquet data using Druid's Hadoop indexer. Specifies data schema, input format, and tuning parameters for Hadoop-based ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/parquet.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"no_metrics\"\n      }\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"no_metrics\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"name\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      \"metricsSpec\": [{\n        \"type\": \"count\",\n        \"name\": \"count\"\n      }],\n      \"granularitySpec\": {\n        \"type\": \"uniform\",\n        \"segmentGranularity\": \"DAY\",\n        \"queryGranularity\": \"ALL\",\n        \"intervals\": [\"2015-12-31/2016-01-02\"]\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"partitionsSpec\": {\n        \"targetPartitionSize\": 5000000\n      },\n      \"jobProperties\" : {},\n      \"leaveIntermediate\": true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Prism.js License Header in JavaScript\nDESCRIPTION: License header for Prism.js, a lightweight syntax highlighting library created by Lea Verou. It is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Variance Aggregator for Ingestion in Apache Druid\nDESCRIPTION: JSON configuration for the variance aggregator used during data ingestion. It specifies the output name, metric name, input type, and estimator type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"variance\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"inputType\" : <input_type>,\n  \"estimator\" : <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Search Filter Example in Druid\nDESCRIPTION: Search filter for case-insensitive partial string matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"search\",\n        \"dimension\": \"product\",\n        \"query\": {\n          \"type\": \"insensitive_contains\",\n          \"value\": \"foo\" \n        }        \n    }\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Post-Aggregator Example for Percentage Calculation\nDESCRIPTION: Example of a JavaScript post-aggregator that calculates the absolute percentage by taking two fields (delta and total) and applying a custom function to compute 100 times the absolute value of delta divided by total.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"absPercent\",\n  \"fieldNames\": [\"delta\", \"total\"],\n  \"function\": \"function(delta, total) { return 100 * Math.abs(delta) / total; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Basic Authentication for Druid HTTP Emitter\nDESCRIPTION: Configuration example for setting up basic authentication for the HTTP emitter, requiring a username and password in the format 'login:password'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.http.basicAuthentication=admin:adminpassword\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Broker Retry Policy in Markdown\nDESCRIPTION: A markdown table showing the configuration option for the Druid broker's retry policy for transient errors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.broker.retryPolicy.numTries`|Number of tries.|1|\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Cache Properties in Druid\nDESCRIPTION: Configuration properties for using Memcached as a cache backend in Druid, including expiration time, timeout, hosts, object size, and connection settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_38\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.expiration=2592000\ndruid.cache.timeout=500\ndruid.cache.hosts=host1:port1,host2:port2\ndruid.cache.maxObjectSize=52428800\ndruid.cache.memcachedPrefix=druid\ndruid.cache.numConnections=1\ndruid.cache.protocol=binary\ndruid.cache.locator=consistent\n```\n\n----------------------------------------\n\nTITLE: Configuring Protobuf Parser in Apache Druid\nDESCRIPTION: JSON configuration for the Protobuf parser in Apache Druid. Specifies required fields like descriptor file, message type, and parse specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"protobuf\",\n  \"descriptor\": \"String\",\n  \"protoMessageType\": \"String\",\n  \"parseSpec\": \"JSON Object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Data Format for Timestamp Min/Max\nDESCRIPTION: Example dataset showing timestamp, dimension, and metric value format that can be used with timeMin/timeMax aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T01:00:00.000Z  A  1\n2015-07-28T02:00:00.000Z  A  1\n2015-07-28T03:00:00.000Z  A  1\n2015-07-28T04:00:00.000Z  B  1\n2015-07-28T05:00:00.000Z  A  1\n2015-07-28T06:00:00.000Z  B  1\n2015-07-29T01:00:00.000Z  C  1\n2015-07-29T02:00:00.000Z  C  1\n2015-07-29T03:00:00.000Z  A  1\n2015-07-29T04:00:00.000Z  A  1\n```\n\n----------------------------------------\n\nTITLE: True Filter in Druid\nDESCRIPTION: Example of a true filter that matches all values, useful for temporarily disabling other filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"true\" }\n```\n\n----------------------------------------\n\nTITLE: Implementing Day of Week Filter in Druid\nDESCRIPTION: Example of filtering on day of week using timeFormat extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"Friday\",\n  \"extractionFn\": {\n    \"type\": \"timeFormat\",\n    \"format\": \"EEEE\",\n    \"timeZone\": \"America/New_York\",\n    \"locale\": \"en\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Binding DataSegmentPuller and DataSegmentPusher in Java\nDESCRIPTION: Example of how to bind custom DataSegmentPuller and DataSegmentPusher implementations in a Druid module. This is typically done in the configure method of a DruidModule.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/modules.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nBinders.dataSegmentPullerBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);\n\nBinders.dataSegmentPusherBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n```\n\n----------------------------------------\n\nTITLE: Disabling Segments by ID in Druid\nDESCRIPTION: cURL command to disable segments using their segment IDs via the Coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-disable-segments.json http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused\n```\n\n----------------------------------------\n\nTITLE: Configuring Registered Lookup Extraction Function in Druid\nDESCRIPTION: The Registered Lookup extraction function references a lookup that has been pre-registered in the cluster-wide configuration. It includes options for handling missing values and optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"registeredLookup\",\n  \"lookup\":\"some_lookup_name\",\n  \"retainMissingValue\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Maven Build Command\nDESCRIPTION: Basic Maven command to build Druid from source, which runs static analysis, unit tests, compiles classes, and packages projects into JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/build.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Properties for Cassandra Storage\nDESCRIPTION: Runtime property configurations for enabling Cassandra backend in Druid Historical and realtime nodes. Specifies extension loading, storage type, host connection, and keyspace settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-cassandra-storage\"]\ndruid.storage.type=c*\ndruid.storage.host=localhost:9160\ndruid.storage.keyspace=druid\n```\n\n----------------------------------------\n\nTITLE: Referencing Column Class for Druid Storage Format in Java\nDESCRIPTION: The Column.java class and its subclasses are crucial for understanding Druid's custom column storage format used in segments. This class serves as a starting point for exploring the internal structure of Druid's data storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/overview.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nColumn.java\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticGoogleBlobStoreFirehose in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticGoogleBlobStoreFirehose in Apache Druid. It specifies the firehose type and an array of Google Cloud Storage blobs to ingest data from. This firehose is splittable and can be used with native parallel index tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/google.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-google-blobstore\",\n    \"blobs\": [\n        {\n          \"bucket\": \"foo\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"bucket\": \"bar\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticGoogleBlobStoreFirehose in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticGoogleBlobStoreFirehose in Apache Druid. It specifies the firehose type and an array of Google Cloud Storage blobs to ingest data from. This firehose is splittable and can be used with native parallel index tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/google.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-google-blobstore\",\n    \"blobs\": [\n        {\n          \"bucket\": \"foo\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"bucket\": \"bar\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: TimeMin Aggregator Configuration\nDESCRIPTION: JSON configuration for timeMin aggregator during ingestion. Specifies the output name and source field for minimum timestamp calculation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMin\",\n    \"name\": \"tmin\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0ba9c98a.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple Numeric TopNMetricSpec in JSON for Apache Druid\nDESCRIPTION: Demonstrates the simplest metric specification as a String value indicating the metric to sort topN results by in a Druid topN query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": \"<metric_name>\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Unparseable Events in Apache Druid\nDESCRIPTION: API endpoint to retrieve current lists of unparseable events from a running ingestion task in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/reports.md#2025-04-09_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/unparseableEvents\n```\n\n----------------------------------------\n\nTITLE: Displaying EventReceiverFirehose Metrics Table in Markdown\nDESCRIPTION: A markdown table showing metrics for EventReceiverFirehose in Apache Druid when the EventReceiverFirehoseMonitor module is included, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/buffered`|Number of events queued in the EventReceiverFirehose's buffer|serviceName, dataSource, taskId, taskType, bufferCapacity.|Equal to current # of events in the buffer queue.|\n|`ingest/bytes/received`|Number of bytes received by the EventReceiverFirehose.|serviceName, dataSource, taskId, taskType.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Settings for Router Process\nDESCRIPTION: Example JVM settings for running the Router process on a c3.2xlarge EC2 instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/router.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/mnt/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/mnt/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Ingestion with TimeAndDims ParseSpec\nDESCRIPTION: This JSON configuration shows how to set up Hadoop-based indexing to ingest Parquet files using the 'parquet' parser and 'timeAndDims' parseSpec. It specifies the input format, timestamp spec, and dimensions explicitly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Single Lookup Configuration JSON in Druid\nDESCRIPTION: Example JSON response from a GET request to /druid/listen/v1/lookups/some_lookup_name, showing the configuration for a specific lookup. It includes the lookup version and extractor factory details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining IndexSpec Configuration for Kinesis Ingestion\nDESCRIPTION: Configuration object for specifying bitmap compression, dimension and metric compression formats, and long value encoding settings for Kinesis data ingestion in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmap\": {\n    \"type\": \"concise\"\n  },\n  \"dimensionCompression\": \"LZ4\",\n  \"metricCompression\": \"LZ4\",\n  \"longEncoding\": \"longs\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Ingestion with TimeAndDims ParseSpec\nDESCRIPTION: This JSON configuration shows how to set up Hadoop-based indexing to ingest Parquet files using the 'parquet' parser and 'timeAndDims' parseSpec. It specifies the input format, timestamp spec, and dimensions explicitly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading On-heap Guava Lookup in Apache Druid\nDESCRIPTION: Example configuration for a loading lookup using on-heap Guava cache with custom settings for reverse lookup cache.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"guava\"},\n   \"reverseLoadingCacheSpec\":{\"type\":\"guava\", \"maximumSize\":500000, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage Task Logs in Druid\nDESCRIPTION: Configuration for storing task logs in Google Cloud Storage. Requires the druid-google-extensions extension and uses the same storage settings as deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_17\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.bucket=none\ndruid.indexer.logs.prefix=none\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Query Server\nDESCRIPTION: This command starts the Query server in a Druid cluster. It should be run from the distribution root directory after copying the Druid distribution and edited configurations to the Query servers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-cluster-query-server\n```\n\n----------------------------------------\n\nTITLE: Configuring Authorizers in Apache Druid\nDESCRIPTION: This JSON snippet shows how to enable the 'basic' authorizer implementation from the druid-basic-security extension in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/auth.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authorizers=[\\\"basic\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring doubleFirst Aggregator in Apache Druid for Queries\nDESCRIPTION: The doubleFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. It can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage with HDFS Extension in Druid\nDESCRIPTION: Configuration properties for using Google Cloud Storage (GCS) as deep storage via the HDFS extension in Druid. Requires setting storage type and using GCS path format for the storage directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/hdfs.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=gs://bucket/example/directory\n```\n\n----------------------------------------\n\nTITLE: GroupBy v1 Configuration Settings in Apache Druid\nDESCRIPTION: Legacy GroupBy v1 configuration options including row limits and result constraints.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/groupbyquery.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.groupBy.maxIntermediateRows`|Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail.|50000|\n|`druid.query.groupBy.maxResults`|Maximum number of results. Queries that exceed this limit will fail.|500000|\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet-Avro Parser with Avro ParseSpec in Druid\nDESCRIPTION: Example configuration for ingesting Parquet files using the 'parquet-avro' parser type with an avro parseSpec. This approach converts Parquet to Avro records first, then uses the Avro extension to parse the data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet-avro\",\n        \"parseSpec\": {\n          \"format\": \"avro\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: React-Is License Comment\nDESCRIPTION: License comment for the React-Is production build, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Batch Ingestion Task for Wikipedia Data\nDESCRIPTION: JSON configuration for a Druid ingestion task that defines the schema, input source, and processing parameters for loading Wikipedia page edit data. Specifies dimensions, timestamp format, granularity settings, and IO configuration for local file ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Batch Ingestion Task for Wikipedia Data\nDESCRIPTION: JSON configuration for a Druid ingestion task that defines the schema, input source, and processing parameters for loading Wikipedia page edit data. Specifies dimensions, timestamp format, granularity settings, and IO configuration for local file ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Bitmap Index Output\nDESCRIPTION: Example of bitmap index output showing the bitmap serde factory type and encoded bitmap data for column values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/dump-segment.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmapSerdeFactory\": {\n    \"type\": \"concise\"\n  },\n  \"bitmaps\": {\n    \"isRobot\": {\n      \"false\": \"//aExfu+Nv3X...\",\n      \"true\": \"gAl7OoRByQ...\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Less Than Filter in Druid groupBy Query\nDESCRIPTION: Shows how to use a lessThan filter in a having clause to match rows with aggregate values less than the specified value, equivalent to SQL's HAVING <aggregate> < <value>.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/having.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"lessThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Parser in Druid DataSchema\nDESCRIPTION: Setting up a string parser with JSON format for interpreting input data in the Druid ingestion spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Jersey Resource\nDESCRIPTION: Code example showing how to bind a new Jersey resource in a Druid module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nJerseys.addResource(binder, NewResource.class);\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Estimate Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the thetaSketchEstimate post-aggregator, used to estimate the cardinality of a Theta sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Legacy Coordinator Console URL\nDESCRIPTION: The URL pattern for accessing the oldest version of Druid's Coordinator console, maintained for backwards compatibility.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/management-uis.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced GroupBy v2 Properties in Druid\nDESCRIPTION: Advanced configuration properties for GroupBy v2 queries, including hash table settings, aggregation methods, and parallel processing options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_45\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.groupBy.bufferGrouperInitialBuckets=0\ndruid.query.groupBy.bufferGrouperMaxLoadFactor=0\ndruid.query.groupBy.forceHashAggregation=false\ndruid.query.groupBy.intermediateCombineDegree=8\ndruid.query.groupBy.numParallelCombineThreads=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Hadoop Parser for Batch Ingestion in Druid\nDESCRIPTION: This snippet shows the configuration for using the Avro Hadoop parser in a batch ingestion task. It includes the parser type, parse specification, input format, and custom schema file path in job properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index_hadoop\",  \n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"\",\n      \"parser\" : {\n        \"type\" : \"avro_hadoop\",\n        \"parseSpec\" : {\n          \"format\": \"avro\",\n          \"timestampSpec\": <standard timestampSpec>,\n          \"dimensionsSpec\": <standard dimensionsSpec>,\n          \"flattenSpec\": <optional>\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"hadoop\",\n      \"inputSpec\" : {\n        \"type\" : \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\",\n        \"paths\" : \"\"\n      }\n    },\n    \"tuningConfig\" : {\n       \"jobProperties\" : {\n          \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Theta Sketch Estimate Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the thetaSketchEstimate post-aggregator, used to estimate the cardinality of a Theta sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>\n}\n```\n\n----------------------------------------\n\nTITLE: Strict Bounds Age Filter in Druid\nDESCRIPTION: Implements a strict bound filter for ages exclusively between 21 and 31 using strict comparison operators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"21\",\n    \"lowerStrict\": true,\n    \"upper\": \"31\" ,\n    \"upperStrict\": true,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Query Result for GroupBy with Custom Origin Granularity\nDESCRIPTION: This snippet shows the result of the groupBy query with a custom origin set for the granularity. It demonstrates how the custom origin affects the bucketing of data, resulting in different timestamp boundaries for each event group.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-29T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Batch Ingestion Task for Wikipedia Data\nDESCRIPTION: JSON configuration for a Druid ingestion task that defines the schema, input source, and processing parameters for loading Wikipedia page edit data. Specifies dimensions, timestamp format, granularity settings, and IO configuration for local file ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"channel\",\n              \"cityName\",\n              \"comment\",\n              \"countryIsoCode\",\n              \"countryName\",\n              \"isAnonymous\",\n              \"isMinor\",\n              \"isNew\",\n              \"isRobot\",\n              \"isUnpatrolled\",\n              \"metroCode\",\n              \"namespace\",\n              \"page\",\n              \"regionIsoCode\",\n              \"regionName\",\n              \"user\",\n              { \"name\": \"added\", \"type\": \"long\" },\n              { \"name\": \"deleted\", \"type\": \"long\" },\n              { \"name\": \"delta\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"day\",\n        \"queryGranularity\" : \"none\",\n        \"intervals\" : [\"2015-09-12/2015-09-13\"],\n        \"rollup\" : false\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial/\",\n        \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Configuration Properties\nDESCRIPTION: Configuration parameters for connecting Druid to MySQL metadata storage, including extension loading and connection details\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"mysql-metadata-storage\"]\ndruid.metadata.storage.type=mysql\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=druid\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Extraction Function in Druid\nDESCRIPTION: Configuration for extracting values using regular expressions with optional group selection and missing value handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"regex\",\n  \"expr\" : <regular_expression>,\n  \"index\" : <group to extract, default 1>\n  \"replaceMissingValue\" : true,\n  \"replaceMissingValueWith\" : \"foobar\"\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Filtered DimensionSpec for Multi-value Dimensions\nDESCRIPTION: Example of a GroupBy query with both a selector filter and a filtered dimension specification. This approach filters the rows first, then limits the dimension values to only 't3', providing more targeted results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"listFiltered\",\n      \"delegate\": {\n        \"type\": \"default\",\n        \"dimension\": \"tags\",\n        \"outputName\": \"tags\"\n      },\n      \"values\": [\"t3\"]\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Indexing Service Metrics Table in Markdown\nDESCRIPTION: Table showing metrics related to the Indexing Service, including task execution times and segment operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`task/run/time`|Milliseconds taken to run a task.|dataSource, taskId, taskType, taskStatus.|Varies.|\n|`task/action/log/time`|Milliseconds taken to log a task action to the audit log.|dataSource, taskId, taskType|< 1000 (subsecond)|\n```\n\n----------------------------------------\n\nTITLE: JVM Configuration Settings for Router Node\nDESCRIPTION: Recommended JVM settings for running a Router node in production on a c3.2xlarge EC2 instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/mnt/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/mnt/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n```\n\n----------------------------------------\n\nTITLE: Checking Middle Manager Enabled Status in Apache Druid\nDESCRIPTION: Send a GET request to check if a Middle Manager is currently enabled or disabled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rolling-updates.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/enabled\n```\n\n----------------------------------------\n\nTITLE: Implementing String Range Filter in Druid\nDESCRIPTION: Demonstrates a bound filter for filtering string values between 'foo' and 'hoo' using lexicographic ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing String Range Filter in Druid\nDESCRIPTION: Demonstrates a bound filter for filtering string values between 'foo' and 'hoo' using lexicographic ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Expression Transform in JSON\nDESCRIPTION: An example of an expression transform that creates a new 'fooPage' column by prepending 'foo' to the values in the 'page' column using the concat function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"type\": \"expression\",\n      \"name\": \"fooPage\",\n      \"expression\": \"concat('foo' + page)\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Sample Multi-value Dimension Data Structure\nDESCRIPTION: Example showing how multi-value dimension data is structured in Druid with timestamp and tags arrays.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2011-01-12T00:00:00.000Z\", \"tags\": [\"t1\",\"t2\",\"t3\"]}\n{\"timestamp\": \"2011-01-13T00:00:00.000Z\", \"tags\": [\"t3\",\"t4\",\"t5\"]}\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": [\"t5\",\"t6\",\"t7\"]}\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": []}\n```\n\n----------------------------------------\n\nTITLE: Resetting Kafka Logs\nDESCRIPTION: Command to clean up Kafka logs when resetting the cluster state after completing the Kafka tutorial.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/index.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /tmp/kafka-logs\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Emitter Properties in Druid\nDESCRIPTION: Configuration properties for the HTTP Emitter module that sends metrics data to an HTTP endpoint. Includes settings for batch size, authentication, timeouts, and TLS configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=http\ndruid.emitter.http.flushMillis=60000\ndruid.emitter.http.flushCount=500\ndruid.emitter.http.recipientBaseUrl=http://metrics-collector:8080\n```\n\n----------------------------------------\n\nTITLE: Sample Data Structure for Multi-value Dimension in Druid\nDESCRIPTION: Illustrates the structure of data with a multi-value dimension called 'tags' in Apache Druid. Each row contains a timestamp and an array of tag values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2011-01-12T00:00:00.000Z\", \"tags\": [\"t1\",\"t2\",\"t3\"]\n}\n{\n  \"timestamp\": \"2011-01-13T00:00:00.000Z\", \"tags\": [\"t3\",\"t4\",\"t5\"]\n}\n{\n  \"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": [\"t5\",\"t6\",\"t7\"]\n}\n{\n  \"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Emitter for Apache Druid\nDESCRIPTION: Example configuration for setting up the Kafka Emitter in Apache Druid. This configuration specifies Kafka bootstrap servers, topic names for metrics and alerts, additional producer configuration, and defines the maximum block time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/kafka-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092\ndruid.emitter.kafka.metric.topic=druid-metric\ndruid.emitter.kafka.alert.topic=druid-alert\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bucket Extraction Function in Druid\nDESCRIPTION: Setup for bucketing numerical values into ranges of specified size with custom offset. Converts non-numeric values to null.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bucket\",\n  \"size\" : 5,\n  \"offset\" : 2\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bucket Extraction Function in Apache Druid\nDESCRIPTION: Demonstrates the configuration of a Bucket extraction function, which is used to group numerical values into buckets of a specified size. This example creates buckets of size 5 with an offset of 2.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bucket\",\n  \"size\" : 5,\n  \"offset\" : 2\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kinesis Indexing Tasks in Druid\nDESCRIPTION: Formula for determining the minimum worker capacity needed to support concurrent reading and publishing tasks in a Kinesis indexing configuration. This ensures sufficient resources for task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Ingestion with Parquet Parser and TimeAndDims ParseSpec in Druid\nDESCRIPTION: Configuration example for ingesting Parquet files using the 'parquet' parser type with a 'timeAndDims' parseSpec format. This approach explicitly specifies dimensions to be extracted from the Parquet data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Coordinator Endpoint Response for Lookup Configuration in Apache Druid\nDESCRIPTION: Expected JSON response from the Coordinator API endpoint for a specific lookup configuration. This shows what would be returned when querying the lookup details via the API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v0\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n      \"type\": \"jdbc\",\n      \"connectorConfig\": {\n        \"createTables\": true,\n        \"connectURI\": \"jdbc:mysql://localhost:3306/druid\",\n        \"user\": \"druid\",\n        \"password\": \"diurd\"\n      },\n      \"table\": \"lookupValues\",\n      \"keyColumn\": \"value_id\",\n      \"valueColumn\": \"value_text\",\n      \"filter\": \"value_type='country'\",\n      \"tsColumn\": \"timeColumn\"\n    },\n    \"firstCacheTimeout\": 120000,\n    \"injective\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query Request Log Example in TSV format\nDESCRIPTION: Example of a TSV log entry for an SQL query request to Druid. The format includes timestamp, remote address, an empty native query field, query context details, and the SQL query information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_6\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1       {\"sqlQuery/time\":100,\"sqlQuery/bytes\":600,\"success\":true,\"identity\":\"user1\"}  {\"query\":\"SELECT page, COUNT(*) AS Edits FROM wikiticker WHERE __time BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\",\"context\":{\"sqlQueryId\":\"c9d035a0-5ffd-4a79-a865-3ffdadbb5fdd\",\"nativeQueryIds\":\"[490978e4-f5c7-4cf6-b174-346e63cf8863]\"}}\n```\n\n----------------------------------------\n\nTITLE: Opening Additional Shell to Hadoop Container\nDESCRIPTION: Command to open another bash shell to the running Hadoop Docker container for executing additional commands.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it druid-hadoop-demo bash\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous Authenticator in Apache Druid\nDESCRIPTION: This JSON snippet demonstrates how to configure the Anonymous Authenticator with the druid-basic-security extension in Apache Druid. It sets up the authentication chain and specifies the anonymous authenticator properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/auth.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"druid.auth.authenticatorChain=[\\\"basic\\\", \\\"anonymous\\\"]\n\ndruid.auth.authenticator.anonymous.type=anonymous\ndruid.auth.authenticator.anonymous.identity=defaultUser\ndruid.auth.authenticator.anonymous.authorizerName=myBasicAuthorizer\n\n# ... usual configs for basic authentication would go here ...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Properties for SQLServer Metadata Storage\nDESCRIPTION: These properties configure Druid to use SQLServer for metadata storage. They specify the storage type, connection URI, username, and password for the SQLServer database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/sqlserver.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=sqlserver\ndruid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Password Provider in Apache Druid\nDESCRIPTION: This snippet shows the JSON structure for configuring a custom password provider in Apache Druid. It includes a placeholder for the registered provider name and additional Jackson properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/password-provider.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"<registered_password_provider_name>\", \"<jackson_property>\": \"<value>\", ... }\n```\n\n----------------------------------------\n\nTITLE: Running a Peon in Apache Druid\nDESCRIPTION: Command to run a Peon independently for development purposes. It requires a task file containing the task JSON object and a status file to output the task status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/peons.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main internal peon <task_file> <status_file>\n```\n\n----------------------------------------\n\nTITLE: License Notice for React Is\nDESCRIPTION: This snippet provides the license information for React Is production build (v16.13.1), which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Select Query Behavior in Apache Druid\nDESCRIPTION: Table showing the server configuration property for select queries. The property 'druid.query.select.enableFromNextDefault' controls the default value of the 'fromNext' property in a query's pagingSpec, which affects pagination behavior.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/select-query.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.select.enableFromNextDefault`|If the `fromNext` property in a query's `pagingSpec` is left unspecified, the system will use the value of this property as the default value for `fromNext`. This option is true by default: the option of setting `fromNext` to false by default is intended to support backwards compatibility for deployments where some users may still expect behavior from older versions of Druid.|true|\n```\n\n----------------------------------------\n\nTITLE: Setting up Long Min Aggregator in Druid\nDESCRIPTION: Computes minimum of metric values and Long.MAX_VALUE. Takes output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Append Task in Druid\nDESCRIPTION: Defines a deprecated task that combines multiple segments sequentially into a single segment. Requires task ID, datasource, list of segments to append, with optional aggregations and context.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"append\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"segments\": <JSON list of DataSegment objects to append>,\n    \"aggregations\": <optional list of aggregators>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Load Rule in Druid\nDESCRIPTION: A Forever Load Rule configuration that specifies how segments should be retained indefinitely across different tiers in a Druid cluster. This rule defines the number of replicas per tier.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining IndexSpec Configuration Fields\nDESCRIPTION: Table documenting the configuration fields for IndexSpec settings, including bitmap, dimension, and metric compression options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|bitmap|Object|Compression format for bitmap indexes. Should be a JSON object; see below for options.|no (defaults to Concise)|\n|dimensionCompression|String|Compression format for dimension columns. Choose from `LZ4`, `LZF`, or `uncompressed`.|no (default == `LZ4`)|\n|metricCompression|String|Compression format for metric columns. Choose from `LZ4`, `LZF`, `uncompressed`, or `none`.|no (default == `LZ4`)|\n|longEncoding|String|Encoding format for metric and dimension columns with type long. Choose from `auto` or `longs`. `auto` encodes the values using offset or lookup table depending on column cardinality, and store them with variable size. `longs` stores the value as is with 8 bytes each.|no (default == `longs`)|\n```\n\n----------------------------------------\n\nTITLE: Druid Metrics Table Structure - Query Metrics for Broker\nDESCRIPTION: Markdown table showing the structure and details of query metrics collected by the Druid Broker component, including metric names, descriptions, dimensions, and expected normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`query/time`|Milliseconds taken to complete a query.|Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension.|< 1s|\n```\n\n----------------------------------------\n\nTITLE: Multiple Inline Schemas Avro Decoder Setup\nDESCRIPTION: Configuration for multiple schemas-based Avro bytes decoder supporting different schemas for different input events. Shows schema ID mapping setup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"avroBytesDecoder\": {\n    \"type\": \"multiple_schemas_inline\",\n    \"schemas\": {\n      \"1\": {\n        \"namespace\": \"org.apache.druid.data\",\n        \"name\": \"User\",\n        \"type\": \"record\",\n        \"fields\": [\n          { \"name\": \"FullName\", \"type\": \"string\" },\n          { \"name\": \"Country\", \"type\": \"string\" }\n        ]\n      },\n      \"2\": {\n        \"namespace\": \"org.apache.druid.otherdata\",\n        \"name\": \"UserIdentity\",\n        \"type\": \"record\",\n        \"fields\": [\n          { \"name\": \"Name\", \"type\": \"string\" },\n          { \"name\": \"Location\", \"type\": \"string\" }\n        ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Results with Custom Origin and Pacific Timezone in Apache Druid\nDESCRIPTION: This JSON output shows the results of the GroupBy query with custom origin and Pacific timezone. The result demonstrates how setting an origin time affects bucket boundaries, leading to different grouping of events compared to the default origin.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-29T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Filter in Apache Druid JSON Query (Deprecated)\nDESCRIPTION: The extraction filter, now deprecated, matches a dimension using a specific extraction function. It should be replaced with a selector filter using an extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FragmentSearchQuerySpec in Druid\nDESCRIPTION: Defines a search where a match occurs if a dimension value contains all of the specified fragments. The case sensitivity can be configured but is case-insensitive by default.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/searchqueryspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"fragment\",\n  \"case_sensitive\" : false,\n  \"values\" : [\"fragment1\", \"fragment2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining IndexSpec Configuration Fields\nDESCRIPTION: Table documenting the configuration fields for IndexSpec settings, including bitmap, dimension, and metric compression options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|bitmap|Object|Compression format for bitmap indexes. Should be a JSON object; see below for options.|no (defaults to Concise)|\n|dimensionCompression|String|Compression format for dimension columns. Choose from `LZ4`, `LZF`, or `uncompressed`.|no (default == `LZ4`)|\n|metricCompression|String|Compression format for metric columns. Choose from `LZ4`, `LZF`, `uncompressed`, or `none`.|no (default == `LZ4`)|\n|longEncoding|String|Encoding format for metric and dimension columns with type long. Choose from `auto` or `longs`. `auto` encodes the values using offset or lookup table depending on column cardinality, and store them with variable size. `longs` stores the value as is with 8 bytes each.|no (default == `longs`)|\n```\n\n----------------------------------------\n\nTITLE: JVM Settings for Production Router Configuration\nDESCRIPTION: Example JVM settings for running the Router process in a production environment on a c3.2xlarge EC2 instance, including memory allocation, garbage collection, and JMX configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/mnt/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/mnt/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n```\n\n----------------------------------------\n\nTITLE: Progress Response JSON Structure\nDESCRIPTION: Example JSON response from the progress endpoint showing task execution statistics including running, succeeded, failed, and total task counts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"running\":10,\n  \"succeeded\":0,\n  \"failed\":0,\n  \"complete\":0,\n  \"total\":10,\n  \"expectedSucceeded\":10\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hybrid Cache in Druid\nDESCRIPTION: Configuration for a two-level L1/L2 cache system in Druid, combining two cache implementations such as a local in-memory cache with a remote Memcached cache.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_26\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.l1.type=caffeine\ndruid.cache.l2.type=caffeine\ndruid.cache.l1.*=defaults for given cache type\ndruid.cache.l2.*=defaults for given cache type\ndruid.cache.useL2=true\ndruid.cache.populateL2=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Emitter in Apache Druid\nDESCRIPTION: Example configuration for setting up the Kafka Emitter in Apache Druid. This snippet shows how to specify Kafka bootstrap servers, topic names for metrics and alerts, additional producer configurations, and an optional cluster name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/kafka-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092\ndruid.emitter.kafka.metric.topic=druid-metric\ndruid.emitter.kafka.alert.topic=druid-alert\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom DBCP Properties for Metadata Storage (Properties)\nDESCRIPTION: Example of adding custom DBCP (Database Connection Pooling) properties for metadata storage configuration in Druid. Sets maximum connection lifetime and default query timeout.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000\ndruid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Historical and JVM Metrics\nDESCRIPTION: Table describing historical node metrics and JVM-related monitoring metrics\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/max`|Maximum byte limit available for segments.||Varies.|\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Cache in Druid\nDESCRIPTION: Configuration properties for the deprecated local cache implementation in Druid. These settings control cache size, initial hashtable size, and logging of eviction events.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_51\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.sizeInBytes=0\ndruid.cache.initialSize=500000\ndruid.cache.logEvictionCount=0\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Cache in Druid\nDESCRIPTION: Configuration properties for the deprecated local cache implementation in Druid. These settings control cache size, initial hashtable size, and logging of eviction events.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_51\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.sizeInBytes=0\ndruid.cache.initialSize=500000\ndruid.cache.logEvictionCount=0\n```\n\n----------------------------------------\n\nTITLE: Configuring PrefixFiltered DimensionSpec in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a PrefixFiltered DimensionSpec in Apache Druid. It retains only the values starting with the specified prefix in multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{ \"type\" : \"prefixFiltered\", \"delegate\" : <dimensionSpec>, \"prefix\": <prefix string> }\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: MIT license declaration for React DOM production bundle\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: JSON Data Flattening Example\nDESCRIPTION: Example showing how nested JSON data should be flattened before ingestion into Druid, as nested dimensions are not supported.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\":{\"bar\": 3}}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"foo_bar\": 3}\n```\n\n----------------------------------------\n\nTITLE: Implementing Timeseries Query with DistinctCount in Druid\nDESCRIPTION: Example of a Timeseries query using the DistinctCount aggregator to count unique visitor IDs over a specified time interval. The query is configured with daily granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-01T00:00:00.000/2013-03-20T00:00:00.000\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Quantiles Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for computing multiple quantiles from histogram aggregator. Accepts array of probability values for desired quantiles.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantiles\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probabilities\" : [ <quantile>, <quantile>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: HTTP LoadQueuePeon Configuration Properties Table\nDESCRIPTION: Additional configuration properties specific to HTTP implementation of load queue management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_27\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.coordinator.loadqueuepeon.http.batchSize`|Number of segment load/drop requests to batch in one HTTP request. Note that it must be smaller than `druid.segmentCache.numLoadingThreads` config on Historical process.|1|\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container for Druid\nDESCRIPTION: Command to start the Hadoop container with necessary port mappings and volume mounts for integration with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kinesis Indexing Tasks in Apache Druid\nDESCRIPTION: Formula to calculate the minimum worker capacity required for Kinesis indexing tasks. This ensures sufficient capacity for both reading and publishing tasks to run concurrently.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Parser in Druid\nDESCRIPTION: Example configuration for JavaScript-based data parsing in Druid, showing timestamp specification, dimension configuration, and custom parsing function. The parser splits strings on hyphens and returns an object with 'one' and 'two' properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/data-formats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"javascript\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    },\n    \"function\" : \"function(str) { var parts = str.split(\\\"-\\\"); return { one: parts[0], two: parts[1] } }\"\n  }\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Directory Prefix and Regex in Apache Druid\nDESCRIPTION: JSON configuration for a URI-based lookup using a directory prefix and file regex pattern. This enables selecting files from an S3 directory that match a specific naming pattern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uriPrefix\": \"s3://bucket/some/key/prefix/\",\n  \"fileRegex\":\"renames-[0-9]*\\\\.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Priority Router Strategy in JSON\nDESCRIPTION: JSON configuration for the priority router strategy, which routes queries based on their priority level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/router.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"priority\",\n  \"minPriority\":0,\n  \"maxPriority\":1\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown License Header\nDESCRIPTION: Apache 2.0 License header in markdown format defining the terms of use and distribution for the documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/cassandra.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container with Port Mappings\nDESCRIPTION: Docker run command to start the Hadoop container with various ports exposed and volume mounted for shared file access between host and container.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Console Producer for Testing (Bash)\nDESCRIPTION: Bash command to start a Kafka console producer for testing the Kafka rename functionality. It sets up a producer that accepts key-value pairs separated by '->' and publishes them to the specified Kafka topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-console-producer.sh --property parse.key=true --property key.separator=\"->\" --broker-list localhost:9092 --topic testTopic\n```\n\n----------------------------------------\n\nTITLE: Configuring Real-time Thrift Ingestion in Tranquility for Apache Druid\nDESCRIPTION: A JSON configuration example for real-time ingestion of Thrift data using Tranquility. It demonstrates how to set up the Thrift parser with the required thriftClass and protocol specifications within a dataSchema configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSources\": [{\n    \"spec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"book\",\n        \"granularitySpec\": {          },\n        \"parser\": {\n          \"type\": \"thrift\",\n          \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n          \"protocol\": \"compact\",\n          \"parseSpec\": {\n            \"format\": \"json\",\n            ...\n          }\n        },\n        \"metricsSpec\": [...]\n      },\n      \"tuningConfig\": {...}\n    },\n    \"properties\": {...}\n  }],\n  \"properties\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: JSON response indicating unsuccessful data ingestion in Apache Druid\nDESCRIPTION: This JSON snippet shows a response where events were received but not sent to Druid, indicating a potential issue with the ingestion process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":0}}\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Indexing Service Metrics\nDESCRIPTION: Table describing indexing service metrics including task execution and segment operations\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`task/run/time`|Milliseconds taken to run a task.|dataSource, taskId, taskType, taskStatus.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: MiddleManager Disable Response in JSON\nDESCRIPTION: Example JSON response after disabling a MiddleManager via the /druid/worker/v1/disable endpoint, showing the host:port and disabled status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":\"disabled\"}\n```\n\n----------------------------------------\n\nTITLE: Markdown License Header\nDESCRIPTION: Apache 2.0 License header in markdown format defining the terms of use and distribution for the documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/cassandra.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Prism Copyright Notice\nDESCRIPTION: Copyright notice for the Prism syntax highlighting library by Lea Verou, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Joining Tables to Count Segments per Server for a Datasource in Druid SQL\nDESCRIPTION: Complex SQL query that joins the sys.segments, sys.server_segments, and sys.servers tables to count segments for a specific datasource grouped by server. This shows segment distribution across the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nSELECT count(segments.segment_id) as num_segments from sys.segments as segments\nINNER JOIN sys.server_segments as server_segments\nON segments.segment_id  = server_segments.segment_id\nINNER JOIN sys.servers as servers\nON servers.server = server_segments.server\nWHERE segments.datasource = 'wikipedia'\nGROUP BY servers.server;\n```\n\n----------------------------------------\n\nTITLE: Starting Hadoop Docker Container with Port Mappings\nDESCRIPTION: Docker run command to start the Hadoop container with various ports exposed and volume mounted for shared file access between host and container.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 50010:50010 -p 50020:50020 -p 50075:50075 -p 50090:50090 -p 8020:8020 -p 10020:10020 -p 19888:19888 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 2049:2049 -p 9000:9000 -p 49707:49707 -p 2122:2122  -p 34455:34455 -v /tmp/shared:/shared druid-hadoop-demo:2.8.3 /etc/bootstrap.sh -bash\n```\n\n----------------------------------------\n\nTITLE: Enabling JavaScript in Druid Configuration\nDESCRIPTION: Configuration property to enable JavaScript functionality in Druid. By default JavaScript is disabled for security reasons and must be explicitly enabled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/javascript.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.javascript.enabled = true\n```\n\n----------------------------------------\n\nTITLE: Druid Bitmap Types Configuration\nDESCRIPTION: Markdown tables defining configuration options for Concise and Roaring bitmap types used in segment storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|type|String|Must be `concise`.|yes|\n\nFor Roaring bitmaps:\n\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|type|String|Must be `roaring`.|yes|\n|compressRunOnSerialization|Boolean|Use a run-length encoding where it is estimated as more space efficient.|no (default == `true`)|\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Compression Properties in Apache Druid\nDESCRIPTION: Configuration properties for controlling HTTP compression in Apache Druid. These properties allow setting the compression level (between -1 and 9) and the buffer size used by the gzip decoder for request decompression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/http-compression.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.server.http.compressionLevel`|The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression.|-1 (default compression level)|\n|`druid.server.http.inflateBufferSize`|The buffer size used by gzip decoder. Set to 0 to disable request decompression.|4096|\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Coordinator Process Properties\nDESCRIPTION: Core configuration properties for the Coordinator process including host, ports and service name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_24\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=<hostname>\ndruid.bindOnHost=false\ndruid.plaintextPort=8081\ndruid.tlsPort=8281\ndruid.service=druid/coordinator\n```\n\n----------------------------------------\n\nTITLE: Submitting an Index Task with post-index-task Script in Apache Druid\nDESCRIPTION: A bash command that submits the defined ingestion specification to the Druid overlord using the post-index-task script. This command references the JSON specification file and specifies the overlord endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/ingestion-tutorial-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Lookup DimensionSpec with Map Implementation\nDESCRIPTION: Configuration for lookup dimension specification using an inline map implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"lookup\",\n  \"dimension\":\"dimensionName\",\n  \"outputName\":\"dimensionOutputName\",\n  \"replaceMissingValueWith\":\"missing_value\",\n  \"retainMissingValue\":false,\n  \"lookup\":{\"type\": \"map\", \"map\":{\"key\":\"value\"}, \"isOneToOne\":false}\n}\n```\n\n----------------------------------------\n\nTITLE: Terminating All Supervisors POST Endpoint\nDESCRIPTION: REST endpoint to terminate all supervisors simultaneously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_11\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/terminateAll\n```\n\n----------------------------------------\n\nTITLE: Configuring Priority Router Strategy in JSON\nDESCRIPTION: JSON configuration for the priority router strategy, which routes queries based on their priority level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/router.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"priority\",\n  \"minPriority\":0,\n  \"maxPriority\":1\n}\n```\n\n----------------------------------------\n\nTITLE: Buckets Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Buckets post-aggregator, which computes a visual representation with custom bucket size and offset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"buckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"bucketSize\": <bucket_size>,\n  \"offset\": <offset>\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Data Structure for Metrics\nDESCRIPTION: Sample JSON structure representing metrics data that will be converted to Protobuf format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"unit\": \"milliseconds\",\n  \"http_method\": \"GET\",\n  \"value\": 44,\n  \"timestamp\": \"2017-04-06T02:36:22Z\",\n  \"http_code\": \"200\",\n  \"page\": \"/\",\n  \"metricType\": \"request/latency\",\n  \"server\": \"www1.example.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Connection Threads in Druid\nDESCRIPTION: Sets the number of HTTP connection handling threads for a Druid process. This configuration limits the number of concurrent HTTP API requests a process can handle.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.server.http.numThreads: <number_of_threads>\n```\n\n----------------------------------------\n\nTITLE: Configuring Non-Injective Lookup in JSON\nDESCRIPTION: Example of a non-injective lookup configuration where multiple keys map to the same value. This type of lookup requires additional processing during query execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n1 -> Foo\n2 -> Bar\n3 -> Bar\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Druid Distribution using rsync\nDESCRIPTION: Command to copy Druid distribution and configurations to the Master server using rsync.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrsync -az apache-druid-0.14.0-incubating/ COORDINATION_SERVER:apache-druid-0.14.0-incubating/\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid to use SQLServer as Metadata Storage\nDESCRIPTION: Configuration parameters needed in Druid properties files to establish connection with a Microsoft SQLServer database for metadata storage. Replace <host> with the actual server hostname and port.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/sqlserver.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=sqlserver\ndruid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Emitter Properties in Druid\nDESCRIPTION: Example configuration for setting up Kafka Emitter in Druid. Demonstrates setting bootstrap servers, metric and alert topics, producer config, and other essential parameters needed for Kafka integration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/kafka-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092\ndruid.emitter.kafka.metric.topic=druid-metric\ndruid.emitter.kafka.alert.topic=druid-alert\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\n```\n\n----------------------------------------\n\nTITLE: Specifying Process Announcement Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path where Historical and Realtime processes create ephemeral znodes to announce their existence.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.announcementsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Quantile Post-Aggregator Configurations\nDESCRIPTION: JSON configurations for computing single quantile and multiple quantiles from histogram aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantile\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probability\" : <quantile> }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantiles\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probabilities\" : [ <quantile>, <quantile>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: OR Filter for Multi-value Dimension in Druid\nDESCRIPTION: Demonstrates an 'or' filter that matches rows where the 'tags' dimension contains either 't1' or 't3'. This filter would match the first two rows of the sample dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"or\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Docker Image for Druid\nDESCRIPTION: Commands to build a Docker image for a Hadoop 2.8.3 cluster, which will be used to run the batch indexing task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:2.8.3 .\n```\n\n----------------------------------------\n\nTITLE: Querying All Druid Servers\nDESCRIPTION: SQL query to retrieve information about all servers from the sys.servers table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Password Request JSON for Druid Authentication API\nDESCRIPTION: Example JSON payload for the password assignment API endpoint, used when setting a user's password for HTTP basic authentication.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"password\": \"helloworld\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with DimSelector Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Illustrates how to use a 'dimSelector' filter in a HavingSpec for a groupBy query. This filter matches rows with dimension values equal to the specified value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n       {\n            \"type\": \"dimSelector\",\n            \"dimension\": \"<dimension>\",\n            \"value\": <dimension_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Lookup Tier Configuration Example in Druid\nDESCRIPTION: A complete example configuration for a lookup tier named 'realtime_customer2' with a 'country_code' lookup. This shows the structure required when configuring lookups through the Coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupValues\",\n          \"keyColumn\": \"value_id\",\n          \"valueColumn\": \"value_text\",\n          \"filter\": \"value_type='country'\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Kafka\nDESCRIPTION: Commands to download Kafka 0.10.2.2 and extract it to the local filesystem.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz\ntar -xzf kafka_2.12-0.10.2.2.tgz\ncd kafka_2.12-0.10.2.2\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Coordinator Server\nDESCRIPTION: Command to start the Druid Coordinator process using the Main class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/coordinator.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server coordinator\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Broker with Kerberos Authentication\nDESCRIPTION: Example of sending a JSON query to the Druid Broker with Kerberos authentication. This demonstrates how to submit queries while maintaining Kerberos security.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json\n```\n\n----------------------------------------\n\nTITLE: Setting Task Priority in Druid Context Configuration\nDESCRIPTION: JSON configuration snippet showing how to override the default task priority in the task context. The priority value determines the task's ability to preempt locks from other tasks, with higher numbers indicating higher priority.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/locking-and-priority.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"context\" : {\n  \"priority\" : 100\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Overlord Discovery in Druid\nDESCRIPTION: Configuration for finding the Overlord using Curator service discovery. Only required when running an Overlord process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_19\n\nLANGUAGE: properties\nCODE:\n```\ndruid.selectors.indexing.serviceName=druid/overlord\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data\nDESCRIPTION: Command to extract the compressed sample Wikipedia data file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial\ngunzip -k wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring Derivative DataSource Supervisor in Druid\nDESCRIPTION: JSON configuration for creating a derivative dataSource supervisor that maintains materialized views. Specifies the base dataSource, dimensions, metrics and Hadoop tuning configs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"derivativeDataSource\",\n    \"baseDataSource\": \"wikiticker\",\n    \"dimensionsSpec\": {\n        \"dimensions\": [\n            \"isUnpatrolled\",\n            \"metroCode\",\n            \"namespace\",\n            \"page\",\n            \"regionIsoCode\",\n            \"regionName\",\n            \"user\"\n        ]\n    },\n    \"metricsSpec\": [\n        {\n            \"name\": \"count\",\n            \"type\": \"count\"\n        },\n        {\n            \"name\": \"added\",\n            \"type\": \"longSum\",\n            \"fieldName\": \"added\"\n        }\n    ],\n    \"tuningConfig\": {\n        \"type\": \"hadoop\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for the NProgress library created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Sample Results of Timestamp Min/Max Query in Druid\nDESCRIPTION: Example output from a Druid query using timestamp min/max aggregators, showing grouped results with count, minimum timestamp, and maximum timestamp for each group.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T00:00:00.000Z A 4 2015-07-28T01:00:00.000Z 2015-07-28T05:00:00.000Z\n2015-07-28T00:00:00.000Z B 2 2015-07-28T04:00:00.000Z 2015-07-28T06:00:00.000Z\n2015-07-29T00:00:00.000Z A 2 2015-07-29T03:00:00.000Z 2015-07-29T04:00:00.000Z\n2015-07-29T00:00:00.000Z C 2 2015-07-29T01:00:00.000Z 2015-07-29T02:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Wikipedia Data in Druid\nDESCRIPTION: Command to load the initial Wikipedia edits data using a pre-configured indexing specification that creates hourly segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/deletion-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Base Settings in Druid\nDESCRIPTION: Core Zookeeper configuration properties for Druid including base path, host connection, and authentication settings. These properties establish the basic connectivity between Druid and Zookeeper.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.service.host=none\ndruid.zk.service.user=none\ndruid.zk.service.pwd=none\ndruid.zk.service.authScheme=digest\n```\n\n----------------------------------------\n\nTITLE: Basic Druid SQL Query Structure\nDESCRIPTION: Shows the complete syntax structure for writing SQL queries in Druid, including optional clauses for WITH, WHERE, GROUP BY, HAVING, ORDER BY, LIMIT, and UNION ALL operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Basic Druid SQL Query Structure\nDESCRIPTION: Shows the complete syntax structure for writing SQL queries in Druid, including optional clauses for WITH, WHERE, GROUP BY, HAVING, ORDER BY, LIMIT, and UNION ALL operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[ EXPLAIN PLAN FOR ]\n[ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ]\nSELECT [ ALL | DISTINCT ] { * | exprs }\nFROM table\n[ WHERE expr ]\n[ GROUP BY exprs ]\n[ HAVING expr ]\n[ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ]\n[ LIMIT limit ]\n[ UNION ALL <another query> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Base Settings in Druid\nDESCRIPTION: Core Zookeeper configuration properties for Druid including base path, host connection, and authentication settings. These properties establish the basic connectivity between Druid and Zookeeper.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.service.host=none\ndruid.zk.service.user=none\ndruid.zk.service.pwd=none\ndruid.zk.service.authScheme=digest\n```\n\n----------------------------------------\n\nTITLE: Configuring Security Escalator in Apache Druid\nDESCRIPTION: Configuration for the Druid escalator which handles internal system communications. This setup defines credentials for the internal client and specifies which authorizer to use.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# Escalator\ndruid.escalator.type=basic\ndruid.escalator.internalClientUsername=druid_system\ndruid.escalator.internalClientPassword=password2\ndruid.escalator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Get Lookups Response Example\nDESCRIPTION: Example JSON response from the GET /druid/listen/v1/lookups endpoint showing active lookups on a process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"site_id_customer2\": {\n    \"version\": \"v1\",\n    \"lookupExtractorFactory\": {\n      \"type\": \"map\",\n      \"map\": {\n        \"AHF77\": \"Home\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Data Format for Timestamp Min/Max Aggregation in Druid\nDESCRIPTION: Example of a data set consisting of timestamp, dimension, and metric value, which can be used with timestamp min/max aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T01:00:00.000Z  A  1\n2015-07-28T02:00:00.000Z  A  1\n2015-07-28T03:00:00.000Z  A  1\n2015-07-28T04:00:00.000Z  B  1\n2015-07-28T05:00:00.000Z  A  1\n2015-07-28T06:00:00.000Z  B  1\n2015-07-29T01:00:00.000Z  C  1\n2015-07-29T02:00:00.000Z  C  1\n2015-07-29T03:00:00.000Z  A  1\n2015-07-29T04:00:00.000Z  A  1\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Load Queue Peon in Markdown\nDESCRIPTION: A markdown table showing additional configuration options when using the 'http' loadqueuepeon implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_26\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.coordinator.loadqueuepeon.http.batchSize`|Number of segment load/drop requests to batch in one HTTP request. Note that it must be smaller than `druid.segmentCache.numLoadingThreads` config on Historical process.|1|\n```\n\n----------------------------------------\n\nTITLE: Regex Filtered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering dimension values using regular expression patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"regexFiltered\", \"delegate\" : <dimensionSpec>, \"pattern\": <java regex pattern> }\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kafka Indexing Tasks\nDESCRIPTION: Formula for calculating the minimum worker capacity needed to support concurrent reading and publishing tasks in Druid's Kafka indexing system. The calculation is based on replicas and task count parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Configuring floatSum Aggregator in Apache Druid\nDESCRIPTION: The floatSum aggregator computes and stores the sum of values as 32-bit floating point values. Similar to longSum and doubleSum, it requires an output name and the metric field name to sum over.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Load Rule in Druid\nDESCRIPTION: An Interval Load Rule configuration that specifies how segments within a specific time interval should be retained across different tiers in a Druid cluster. This rule applies only to segments within the defined ISO-8601 interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadByInterval\",\n  \"interval\": \"2012-01-01/2013-01-01\",\n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Pagination Specification Example\nDESCRIPTION: Example showing how to specify pagination parameters in a Druid Select query using PagingSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/select-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"pagingSpec\":{\"pagingIdentifiers\": {}, \"threshold\":5}\n```\n\n----------------------------------------\n\nTITLE: Minimal ORC Parser Configuration with Field Discovery in JSON\nDESCRIPTION: A simple configuration for ORC parser with automatic field discovery enabled by default. This example doesn't specify a flattenSpec or dimensionSpec, relying on Druid to discover fields automatically.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/orc.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"orc\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"millis\"\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Tranquility Server for Stream Ingestion\nDESCRIPTION: Commands to download, install, and start Tranquility Server for push-based stream ingestion with Kafka or over HTTP. This component facilitates real-time data ingestion into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz\ntar -xzf tranquility-distribution-0.8.0.tgz\ncd tranquility-distribution-0.8.0\nbin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json\n```\n\n----------------------------------------\n\nTITLE: Displaying JVM Metrics Table in Markdown\nDESCRIPTION: A markdown table showing various JVM metrics available in Apache Druid when the JVMMonitor module is included, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`jvm/pool/committed`|Committed pool.|poolKind, poolName.|close to max pool|\n|`jvm/pool/init`|Initial pool.|poolKind, poolName.|Varies.|\n|`jvm/pool/max`|Max pool.|poolKind, poolName.|Varies.|\n|`jvm/pool/used`|Pool used.|poolKind, poolName.|< max pool|\n|`jvm/bufferpool/count`|Bufferpool count.|bufferPoolName.|Varies.|\n|`jvm/bufferpool/used`|Bufferpool used.|bufferPoolName.|close to capacity|\n|`jvm/bufferpool/capacity`|Bufferpool capacity.|bufferPoolName.|Varies.|\n|`jvm/mem/init`|Initial memory.|memKind.|Varies.|\n|`jvm/mem/max`|Max memory.|memKind.|Varies.|\n|`jvm/mem/used`|Used memory.|memKind.|< max memory|\n|`jvm/mem/committed`|Committed memory.|memKind.|close to max memory|\n|`jvm/gc/count`|Garbage collection count.|gcName (cms/g1/parallel/etc.), gcGen (old/young)|Varies.|\n|`jvm/gc/cpu`|Cpu time in Nanoseconds spent on garbage collection.|gcName, gcGen|Sum of `jvm/gc/cpu` should be within 10-30% of sum of `jvm/cpu/total`, depending on the GC algorithm used (reported by [`JvmCpuMonitor`](../configuration/index.html#enabling-metrics)) |\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Paths in Druid\nDESCRIPTION: Zookeeper path configurations for different Druid components including segments, announcements, and coordinator paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.base=/druid\ndruid.zk.paths.propertiesPath=${druid.zk.paths.base}/properties\ndruid.zk.paths.announcementsPath=${druid.zk.paths.base}/announcements\ndruid.zk.paths.liveSegmentsPath=${druid.zk.paths.base}/segments\ndruid.zk.paths.loadQueuePath=${druid.zk.paths.base}/loadQueue\ndruid.zk.paths.coordinatorPath=${druid.zk.paths.base}/coordinator\ndruid.zk.paths.servedSegmentsPath=${druid.zk.paths.base}/servedSegments\n```\n\n----------------------------------------\n\nTITLE: OR Logical Filter in Druid\nDESCRIPTION: Logical OR filter that combines multiple filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Follow-up Paginated Select Query\nDESCRIPTION: Shows how to construct a follow-up paginated query using the pagingIdentifiers from a previous response, with manually incremented offset for backwards compatibility mode.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/select-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {\"wikipedia_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\" : 5}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Flattened JSON Structure Example\nDESCRIPTION: Example showing the correct flattened JSON structure for Druid ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-design.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"foo_bar\": 3}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Storage for SQLServer\nDESCRIPTION: This code snippet shows the configuration properties needed to set up Microsoft SQLServer as the metadata storage for Apache Druid. It specifies the storage type, connection URI, username, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/sqlserver.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=sqlserver\ndruid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Example of Multi-interval Druid Segments (Version 1)\nDESCRIPTION: Example showing Druid segments spanning multiple intervals with version 1. Each segment represents a different time interval for the same datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v1_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data in Bash\nDESCRIPTION: Commands to navigate to the tutorial directory and unzip the sample data file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial\ngunzip -k wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: JSON Password Request Example for Druid Security API\nDESCRIPTION: Sample JSON request body for the password assignment API endpoint in Druid's basic security extension. This is used when setting a password for a user via the API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"password\": \"helloworld\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dimension Selector Filter in Druid\nDESCRIPTION: Demonstrates how to implement a dimension selector filter to match rows with specific dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/having.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n       {\n            \"type\": \"dimSelector\",\n            \"dimension\": \"<dimension>\",\n            \"value\": <dimension_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Specific Lookup Configuration in Apache Druid\nDESCRIPTION: Example JSON response when retrieving a specific lookup configuration from Apache Druid. This shows the configuration for the 'site_id_customer2' lookup in the 'realtime_customer2' tier.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TwitterSpritzerFirehose in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates how to set up a TwitterSpritzerFirehose in Druid to connect directly to Twitter's spritzer data stream. It includes parameters for controlling the maximum event count and run duration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/examples.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"twitzer\",\n    \"maxEventCount\": -1,\n    \"maxRunMinutes\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid Console URL\nDESCRIPTION: The URL pattern for accessing the Druid Console, which is hosted by the Router process. This assumes the Router's management proxy is enabled and Druid SQL is enabled on Broker processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/management-uis.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Retrieval Properties\nDESCRIPTION: Settings that control how the Coordinator polls and manages metadata including config updates, active segments, and rules.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.manager.config.pollDuration=PT1M\ndruid.manager.segments.pollDuration=PT1M\ndruid.manager.rules.pollDuration=PT1M\ndruid.manager.rules.defaultTier=_default\ndruid.manager.rules.alertThreshold=PT10M\n```\n\n----------------------------------------\n\nTITLE: Query Prioritization Configuration for Druid Brokers\nDESCRIPTION: Configuration options that control how Brokers balance connections to Historical processes and select segments across tiers. These settings affect query routing and data source selection strategies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_36\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.balancer.type=random\ndruid.broker.select.tier=highestPriority\ndruid.broker.select.tier.custom.priorities=None\n```\n\n----------------------------------------\n\nTITLE: Example TSV Input Data for Druid Lookup\nDESCRIPTION: Sample TSV input data with pipe delimiter that would be parsed by the corresponding TSV namespaceParseSpec configuration. Contains values that will be mapped to key-value pairs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nbar|something,1|foo\nbat|something,2|baz\ntruck|something,3|buck\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Lookup Parse Specification in Druid\nDESCRIPTION: Defines a namespaceParseSpec for CSV format lookups, specifying columns, key column, and value column mappings for transforming CSV data into key-value lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\nbar,something,foo\nbat,something2,baz\ntruck,something3,buck\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"csv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: React-DOM MIT License Declaration\nDESCRIPTION: License declaration for react-dom.production.min.js (v17.0.2) created by Facebook, Inc. and its affiliates under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring String Summary Post Aggregator for DoublesSketch\nDESCRIPTION: Post aggregator configuration to generate a string summary of a DoublesSketch for debugging purposes. This invokes the toString() method on the underlying sketch object.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Task Submission Command\nDESCRIPTION: Bash command to submit the transformation ingestion task to Druid using the post-index-task script.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/transform-index.json\n```\n\n----------------------------------------\n\nTITLE: Distinct Countries Cardinality Example\nDESCRIPTION: Example showing how to calculate distinct countries across country of origin and residence fields using the cardinality aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/hll-old.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_countries\",\n  \"fields\": [ \"country_of_origin\", \"country_of_residence\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Dimensions in Druid ParseSpec\nDESCRIPTION: Defining dimensions for network flow data including IP addresses, ports, and protocol in the dimensionsSpec of the parseSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Realtime Thrift Ingestion in Tranquility (JSON)\nDESCRIPTION: This snippet demonstrates how to configure realtime Thrift data ingestion using Tranquility. It specifies the Thrift parser, class, and protocol within the dataSchema.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSources\": [{\n    \"spec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"book\",\n        \"granularitySpec\": {          },\n        \"parser\": {\n          \"type\": \"thrift\",\n          \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n          \"protocol\": \"compact\",\n          \"parseSpec\": {\n            \"format\": \"json\",\n            ...\n          }\n        },\n        \"metricsSpec\": [...]\n      },\n      \"tuningConfig\": {...}\n    },\n    \"properties\": {...}\n  }],\n  \"properties\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Direct File Reference in Druid\nDESCRIPTION: Configuration for a URI-based lookup that directly references a single file. This example points to an S3 location with a CSV file containing key-value pairs for remapping values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uri\": \"s3://bucket/some/key/prefix/renames-0003.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting a Supervisor Spec to Druid via cURL\nDESCRIPTION: This command demonstrates how to submit a Kafka supervisor specification to the Druid Overlord using cURL. The specification is sent as a JSON payload to the supervisor endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: React Core License Comment\nDESCRIPTION: License comment for the React core production module, version 17.0.2, released under the MIT license by Facebook, Inc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Displaying JVM Metrics Table in Markdown\nDESCRIPTION: A markdown table showing various JVM metrics available in Apache Druid when the JVMMonitor module is included, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`jvm/pool/committed`|Committed pool.|poolKind, poolName.|close to max pool|\n|`jvm/pool/init`|Initial pool.|poolKind, poolName.|Varies.|\n|`jvm/pool/max`|Max pool.|poolKind, poolName.|Varies.|\n|`jvm/pool/used`|Pool used.|poolKind, poolName.|< max pool|\n|`jvm/bufferpool/count`|Bufferpool count.|bufferPoolName.|Varies.|\n|`jvm/bufferpool/used`|Bufferpool used.|bufferPoolName.|close to capacity|\n|`jvm/bufferpool/capacity`|Bufferpool capacity.|bufferPoolName.|Varies.|\n|`jvm/mem/init`|Initial memory.|memKind.|Varies.|\n|`jvm/mem/max`|Max memory.|memKind.|Varies.|\n|`jvm/mem/used`|Used memory.|memKind.|< max memory|\n|`jvm/mem/committed`|Committed memory.|memKind.|close to max memory|\n|`jvm/gc/count`|Garbage collection count.|gcName (cms/g1/parallel/etc.), gcGen (old/young)|Varies.|\n|`jvm/gc/cpu`|Cpu time in Nanoseconds spent on garbage collection.|gcName, gcGen|Sum of `jvm/gc/cpu` should be within 10-30% of sum of `jvm/cpu/total`, depending on the GC algorithm used (reported by [`JvmCpuMonitor`](../configuration/index.html#enabling-metrics)) |\n```\n\n----------------------------------------\n\nTITLE: Registered Lookup Extraction Function in Druid JSON\nDESCRIPTION: References a lookup that has been registered in the cluster-wide configuration to replace dimension values. Includes options for handling missing values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"registeredLookup\",\n  \"lookup\":\"some_lookup_name\",\n  \"retainMissingValue\":true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Simple Consumer Firehose in Druid\nDESCRIPTION: JSON configuration for setting up a Kafka Simple Consumer firehose in Druid. Specifies broker connection details, queue settings, partition configuration, and topic information. This experimental firehose works with standalone realtime processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/kafka-simple.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"firehoseV2\": {\n    \"type\" : \"kafka-0.8-v2\",\n    \"brokerList\" :  [\"localhost:4443\"],\n    \"queueBufferLength\":10001,\n    \"resetOffsetToEarliest\":\"true\",\n    \"partitionIdList\" : [\"0\"],\n    \"clientId\" : \"localclient\",\n    \"feed\": \"wikipedia\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Resuming Supervisor POST Endpoint\nDESCRIPTION: REST endpoint to resume indexing tasks for a specific supervisor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_7\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/resume\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Broadcast Rule in Druid\nDESCRIPTION: A Period Broadcast Rule configuration that instructs Druid to co-locate segments from different data sources based on a rolling time period. This rule ensures segments matching the period are replicated appropriately.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByPeriod\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Tranquility Server on Data Server\nDESCRIPTION: Downloads, extracts, and starts Tranquility Server on the Data server for push-based stream ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz\ntar -xzf tranquility-distribution-0.8.0.tgz\ncd tranquility-distribution-0.8.0\nbin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Docker Image for Druid Ingestion\nDESCRIPTION: Commands to build a Docker image for a Hadoop 2.8.3 cluster, which will be used to run the batch indexing task for Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:2.8.3 .\n```\n\n----------------------------------------\n\nTITLE: Theta Sketch Estimate Post Aggregator\nDESCRIPTION: Configuration for the Theta sketch estimate post aggregator, which extracts the cardinality estimate from a sketch. This is used to get the final count of unique items from a sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>\n}\n```\n\n----------------------------------------\n\nTITLE: Sample TopN Query Response Format\nDESCRIPTION: Example of the JSON response format for a TopN query, showing timestamp and results array containing dimension values and their associated metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/topnquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2013-08-31T00:00:00.000Z\",\n    \"result\": [\n      {\n        \"dim1\": \"dim1_val\",\n        \"count\": 111,\n        \"some_metrics\": 10669,\n        \"average\": 96.11711711711712\n      },\n      {\n        \"dim1\": \"another_dim1_val\",\n        \"count\": 88,\n        \"some_metrics\": 28344,\n        \"average\": 322.09090909090907\n      },\n      {\n        \"dim1\": \"dim1_val3\",\n        \"count\": 70,\n        \"some_metrics\": 871,\n        \"average\": 12.442857142857143\n      },\n      {\n        \"dim1\": \"dim1_val4\",\n        \"count\": 62,\n        \"some_metrics\": 815,\n        \"average\": 13.14516129032258\n      },\n      {\n        \"dim1\": \"dim1_val5\",\n        \"count\": 60,\n        \"some_metrics\": 2787,\n        \"average\": 46.45\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example Live Row Stats JSON Response in Apache Druid\nDESCRIPTION: This JSON snippet shows the structure of a live row statistics report in Apache Druid. It includes moving averages for different time windows and current totals for various row counters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/reports.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"movingAverages\": {\n    \"buildSegments\": {\n      \"5m\": {\n        \"processed\": 3.392158326408501,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"15m\": {\n        \"processed\": 1.736165476881023,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"1m\": {\n        \"processed\": 4.206417693750045,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      }\n    }\n  },\n  \"totals\": {\n    \"buildSegments\": {\n      \"processed\": 1994,\n      \"processedWithError\": 0,\n      \"thrownAway\": 0,\n      \"unparseable\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Last Name First Character Cardinality Example\nDESCRIPTION: Advanced example showing how to compute cardinality of first characters of last names using extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/hll-old.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_last_name_first_char\",\n  \"fields\": [\n    {\n     \"type\" : \"extraction\",\n     \"dimension\" : \"last_name\",\n     \"outputName\" :  \"last_name_first_char\",\n     \"extractionFn\" : { \"type\" : \"substring\", \"index\" : 0, \"length\" : 1 }\n    }\n  ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Equivalent OR of Bound Filters in Apache Druid\nDESCRIPTION: This example shows the equivalent representation of an interval filter as an OR of two bound filters with numeric comparisons on millisecond timestamps, demonstrating left-closed and right-open matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Roll-up Using Druid's Post-Index-Task Script\nDESCRIPTION: Bash command to load the example data into Druid using the post-index-task script with the roll-up-enabled ingestion specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/rollup-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Thrift Ingestion with Hadoop in Apache Druid\nDESCRIPTION: JSON configuration for batch ingestion of Thrift data using HadoopDruidIndexer. The configuration specifies the thrift class, protocol, and input format. It supports both SequenceFileInputFormat and LzoThriftBlockInputFormat.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"book\",\n      \"parser\": {\n        \"type\": \"thrift\",\n        \"jarPath\": \"book.jar\",\n        \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n        \"protocol\": \"compact\",\n        \"parseSpec\": {\n          \"format\": \"json\",\n          ...\n        }\n      },\n      \"metricsSpec\": [],\n      \"granularitySpec\": {}\n    },\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\",\n        // \"inputFormat\": \"com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat\",\n        \"paths\": \"/user/to/some/book.seq\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"jobProperties\": {\n        \"tmpjars\":\"/user/h_user_profile/du00/druid/test/book.jar\",\n        // \"elephantbird.class.for.MultiInputFormat\" : \"${YOUR_THRIFT_CLASS_NAME}\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OR Filter Query for Multi-value Dimensions\nDESCRIPTION: Example of an OR filter that matches multiple values within the tags dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"or\",\n  \"fields\": [\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t1\"\n    },\n    {\n      \"type\": \"selector\",\n      \"dimension\": \"tags\",\n      \"value\": \"t3\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Updating a Specific Lookup in Apache Druid\nDESCRIPTION: Example JSON payload for updating a specific lookup extractor factory in Apache Druid. This demonstrates how to update the 'site_id_customer1' lookup in the 'realtime_customer1' tier.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"847632\": \"Internal Use Only\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Active Lookups JSON in Druid\nDESCRIPTION: Example JSON response from a GET request to /druid/listen/v1/lookups, showing the structure of active lookups on a Druid node. It includes the lookup name, version, and extractor factory configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/lookups.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"site_id_customer2\": {\n    \"version\": \"v1\",\n    \"lookupExtractorFactory\": {\n      \"type\": \"map\",\n      \"map\": {\n        \"AHF77\": \"Home\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Druid Source Repository\nDESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/incubator-druid.git\ncd druid\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating Compaction Config in Druid Coordinator API\nDESCRIPTION: POST request to create or update the compaction config for a dataSource in the Druid Coordinator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_2\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/config/compaction\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Properties in Hadoop TuningConfig for Apache Druid\nDESCRIPTION: This snippet demonstrates how to set custom Hadoop job properties within the tuningConfig of a Druid ingestion task. It allows for fine-tuning Hadoop-specific parameters for optimal performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n   \"tuningConfig\" : {\n     \"type\": \"hadoop\",\n     \"jobProperties\": {\n       \"<hadoop-property-a>\": \"<value-a>\",\n       \"<hadoop-property-b>\": \"<value-b>\"\n     }\n   }\n```\n\n----------------------------------------\n\nTITLE: Executing HTTP POST Query in Druid\nDESCRIPTION: This snippet demonstrates how to send a query to Druid using curl. It shows the basic structure of a POST request, including headers for content type and accept format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/querying.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST '<queryable_host>:<port>/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @<query_json_file>\n```\n\n----------------------------------------\n\nTITLE: Example CustomJSON Input Data for Druid Lookup\nDESCRIPTION: Sample JSON input data that would be parsed by the customJson namespaceParseSpec configuration. Contains JSON objects with key and value fields along with additional fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"key\": \"foo\", \"value\": \"bar\", \"somethingElse\" : \"something\"}\n{\"key\": \"baz\", \"value\": \"bat\", \"somethingElse\" : \"something\"}\n{\"key\": \"buck\", \"somethingElse\": \"something\", \"value\": \"truck\"}\n```\n\n----------------------------------------\n\nTITLE: Bulk Lookup Configuration JSON Structure\nDESCRIPTION: Example JSON structure showing how to configure multiple lookups across different tiers using the bulk update API endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"<tierName>\": {\n        \"<lookupName>\": {\n          \"version\": \"<version>\",\n          \"lookupExtractorFactory\": {\n            \"type\": \"<someExtractorFactoryType>\",\n            \"<someExtractorField>\": \"<someExtractorValue>\"\n          }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: React Is Production License\nDESCRIPTION: MIT license declaration for React's react-is.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Properties for Cassandra Storage\nDESCRIPTION: Properties configuration for enabling Cassandra backend in Druid. These settings should be added to Historical and realtime runtime properties files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-cassandra-storage\"]\ndruid.storage.type=c*\ndruid.storage.host=localhost:9160\ndruid.storage.keyspace=druid\n```\n\n----------------------------------------\n\nTITLE: ListFiltered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering multi-value dimensions using whitelist or blacklist functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"listFiltered\", \"delegate\" : <dimensionSpec>, \"values\": <array of strings>, \"isWhitelist\": <optional attribute for true/false, default is true> }\n```\n\n----------------------------------------\n\nTITLE: Viewing Active Tasks on a Middle Manager in Apache Druid\nDESCRIPTION: Send a GET request to retrieve a list of all existing tasks on a Middle Manager.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rolling-updates.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/tasks\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Lookup in Apache Druid\nDESCRIPTION: Example of CSV data format and its corresponding namespaceParseSpec configuration for Druid lookups. Shows how to map columns to keys and values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_6\n\nLANGUAGE: csv\nCODE:\n```\nbar,something,foo\nbat,something2,baz\ntruck,something3,buck\n```\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"csv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Suspending All Supervisors POST Endpoint\nDESCRIPTION: REST endpoint to suspend all supervisors simultaneously.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_6\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/suspendAll\n```\n\n----------------------------------------\n\nTITLE: Coordination Metrics Table in Markdown\nDESCRIPTION: Table documenting metrics for the Druid Coordinator including segment management and load balancing metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/assigned/count`|Number of segments assigned to be loaded in the cluster.|tier.|Varies.|\n|`segment/moved/count`|Number of segments moved in the cluster.|tier.|Varies.|\n```\n\n----------------------------------------\n\nTITLE: Optimized Lookup-based Extraction Filter in Druid JSON\nDESCRIPTION: Example of a selector filter using a registered lookup with optimization enabled, which allows Druid to rewrite the filter as a clause of selector filters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"selector\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"registeredLookup\",\n            \"optimize\": true,\n            \"lookup\": \"some_lookup_name\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Thrift Ingestion with Hadoop (JSON)\nDESCRIPTION: This configuration demonstrates batch ingestion of Thrift data using HadoopDruidIndexer. It specifies the Thrift parser, input format, and necessary job properties for processing Thrift files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"book\",\n      \"parser\": {\n        \"type\": \"thrift\",\n        \"jarPath\": \"book.jar\",\n        \"thriftClass\": \"org.apache.druid.data.input.thrift.Book\",\n        \"protocol\": \"compact\",\n        \"parseSpec\": {\n          \"format\": \"json\",\n          ...\n        }\n      },\n      \"metricsSpec\": [],\n      \"granularitySpec\": {}\n    },\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\",\n        // \"inputFormat\": \"com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat\",\n        \"paths\": \"/user/to/some/book.seq\"\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"jobProperties\": {\n        \"tmpjars\":\"/user/h_user_profile/du00/druid/test/book.jar\",\n        // \"elephantbird.class.for.MultiInputFormat\" : \"${YOUR_THRIFT_CLASS_NAME}\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Bitmap Compression for Roaring Bitmap Type in Druid\nDESCRIPTION: JSON configuration for using Roaring bitmaps in Druid's IndexSpec. The compressRunOnSerialization parameter can be used to enable run-length encoding for more efficient storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"roaring\",\n  \"compressRunOnSerialization\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Count Query in Druid SQL\nDESCRIPTION: SQL query to count total rows in the compaction-tutorial datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) from \"compaction-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Theta Sketch Estimate Post Aggregator\nDESCRIPTION: Configuration for the Theta sketch estimate post aggregator, which extracts the cardinality estimate from a sketch. This is used to get the final count of unique items from a sketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"thetaSketchEstimate\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator of type fieldAccess that refers to a thetaSketch aggregator or that of type thetaSketchSetOp>\n}\n```\n\n----------------------------------------\n\nTITLE: Running Post-Index-Task Script in Druid\nDESCRIPTION: Using Druid's built-in batch ingestion helper script to load Wikipedia data from a spec file. The script posts the ingestion task and monitors its progress until completion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/wikipedia-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Using Custom Password Provider in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates the structure for implementing a custom password provider in Apache Druid. The configuration requires specifying the registered provider name and any additional properties needed by the implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/password-provider.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"<registered_password_provider_name>\", \"<jackson_property>\": \"<value>\", ... }\n```\n\n----------------------------------------\n\nTITLE: Combining Existing and New Data in Druid\nDESCRIPTION: Submitting a task that reads from both the existing datasource and a new file, combines them, and overwrites the original data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json\n```\n\n----------------------------------------\n\nTITLE: Defining SegmentWriteOutMediumFactory Fields in Markdown\nDESCRIPTION: This snippet defines the fields, types, descriptions, and requirements for SegmentWriteOutMediumFactory using a Markdown table format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|`type`|String|See [Additional Peon Configuration: SegmentWriteOutMediumFactory](../../configuration/index.html#segmentwriteoutmediumfactory) for explanation and available options.|yes|\n```\n\n----------------------------------------\n\nTITLE: Example Ingestion Aggregator Configuration\nDESCRIPTION: Example configuration for setting up a momentSketch aggregator during data ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"momentSketch\", \n  \"name\": \"sketch\", \n  \"fieldName\": \"value\", \n  \"k\": 10, \n  \"compress\": true,\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Existing and New Data in Druid\nDESCRIPTION: Submitting a task that reads from both the existing datasource and a new file, combines them, and overwrites the original data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index.json\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data with Post-Index Task in Apache Druid\nDESCRIPTION: Command to load Wikipedia edits data into Druid using a pre-configured indexing specification that creates hourly segments in a datasource called 'deletion-tutorial'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/deletion-index.json \n```\n\n----------------------------------------\n\nTITLE: Setting up DoubleSum Aggregator in Apache Druid\nDESCRIPTION: Illustrates the setup for a doubleSum aggregator in Druid. This aggregator computes and stores the sum of values as a 64-bit floating point value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Emitter Module in Druid\nDESCRIPTION: Properties for configuring the logging emitter module in Druid. This module is used for emitting metrics and alerts via log4j.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.logging.loggerClass=LoggingEmitter\ndruid.emitter.logging.logLevel=info\n```\n\n----------------------------------------\n\nTITLE: Quantiles Doubles Sketch Aggregator Configuration\nDESCRIPTION: JSON configuration for setting up the quantilesDoublesSketch aggregator which performs union operations on sketches or builds them from raw data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"quantilesDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"k\": <parameter that controls size and accuracy>\n }\n```\n\n----------------------------------------\n\nTITLE: Setting up DoubleSum Aggregator in Apache Druid\nDESCRIPTION: Illustrates the setup for a doubleSum aggregator in Druid. This aggregator computes and stores the sum of values as a 64-bit floating point value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Implementing longSum Aggregator in Druid\nDESCRIPTION: The longSum aggregator computes the sum of values as a 64-bit signed integer. It requires the name parameter for the output and the fieldName parameter to specify which metric column to sum over.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension TopNMetricSpec in JSON for Apache Druid\nDESCRIPTION: Illustrates the JSON configuration for sorting TopN results by dimension value in Druid. It includes options for specifying the sorting order and pagination.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"dimension\",\n    \"ordering\": \"lexicographic\",\n    \"previousStop\": \"<previousStop_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TSV Lookup Parse Specification in Druid\nDESCRIPTION: JSON configuration for TSV-based lookup parsing in Druid, defining columns, key column, value column, and a custom delimiter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"tsv\",\n  \"columns\": [\"value\",\"somethingElse\",\"key\"],\n  \"keyColumn\": \"key\",\n  \"valueColumn\": \"value\",\n  \"delimiter\": \"|\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Live Row Stats JSON Structure in Apache Druid\nDESCRIPTION: Illustrates the JSON structure of a live row statistics report, including moving averages and totals for different time windows and ingestion phases.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"movingAverages\": {\n    \"buildSegments\": {\n      \"5m\": {\n        \"processed\": 3.392158326408501,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"15m\": {\n        \"processed\": 1.736165476881023,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"1m\": {\n        \"processed\": 4.206417693750045,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      }\n    }\n  },\n  \"totals\": {\n    \"buildSegments\": {\n      \"processed\": 1994,\n      \"processedWithError\": 0,\n      \"thrownAway\": 0,\n      \"unparseable\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Tranquility with Thrift Extensions\nDESCRIPTION: Command line instructions for running Tranquility with Thrift extensions, including configuration file and extension directory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka \\\n  -configFile $jsonConfig \\\n  -Ddruid.extensions.directory=/path/to/extensions \\\n  -Ddruid.extensions.loadList='[\"druid-thrift-extensions\"]'\n```\n\n----------------------------------------\n\nTITLE: Updating Druid Coordinator Rules\nDESCRIPTION: POST request to update rules for a specific datasource. Includes optional header parameters for auditing the configuration change.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_8\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/rules/{dataSourceName}\nOptional Headers:\nX-Druid-Author: author making the config change\nX-Druid-Comment: comment describing the change being done\n```\n\n----------------------------------------\n\nTITLE: Displaying GroupBy v2 Query Contexts Table in Markdown\nDESCRIPTION: This snippet presents a markdown table showing the supported query contexts for GroupBy v2 queries in Apache Druid. It includes context keys, descriptions, and default values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_57\n\nLANGUAGE: markdown\nCODE:\n```\n|Key|Description|Default|\n|---|-----------|-------|\n|`bufferGrouperInitialBuckets`|Overrides the value of `druid.query.groupBy.bufferGrouperInitialBuckets` for this query.|None|\n|`bufferGrouperMaxLoadFactor`|Overrides the value of `druid.query.groupBy.bufferGrouperMaxLoadFactor` for this query.|None|\n|`forceHashAggregation`|Overrides the value of `druid.query.groupBy.forceHashAggregation`|None|\n|`intermediateCombineDegree`|Overrides the value of `druid.query.groupBy.intermediateCombineDegree`|None|\n|`numParallelCombineThreads`|Overrides the value of `druid.query.groupBy.numParallelCombineThreads`|None|\n|`sortByDimsFirst`|Sort the results first by dimension values and then by timestamp.|false|\n|`forceLimitPushDown`|When all fields in the orderby are part of the grouping key, the broker will push limit application down to the Historical processes. When the sorting order uses fields that are not in the grouping key, applying this optimization can result in approximate results with unknown accuracy, so this optimization is disabled by default in that case. Enabling this context flag turns on limit push down for limit/orderbys that contain non-grouping key columns.|false|\n```\n\n----------------------------------------\n\nTITLE: Launching Druid X-Large Configuration in Bash\nDESCRIPTION: Command to start Druid in the x-large configuration, which is designed for machines with 64 CPU cores and 512GB RAM, roughly equivalent to an Amazon i3.16xlarge instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/single-server.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-xlarge\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear ShardSpec with partitionNum 1 for Scaling in Druid\nDESCRIPTION: JSON configuration example showing how to set up a linear shard specification with partition number 1 for a Druid real-time process. This configuration allows for horizontally scaling by having different processes handle different data partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 1\n    }\n```\n\n----------------------------------------\n\nTITLE: Non-Injective Lookup Example for Apache Druid\nDESCRIPTION: Example of a non-injective lookup where multiple keys map to the same value. This requires Druid to apply lookups during scanning and aggregation rather than after aggregation is complete.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n1 -> Foo\n2 -> Bar\n3 -> Bar\n```\n\n----------------------------------------\n\nTITLE: Querying All Servers Information\nDESCRIPTION: SQL query to retrieve information about all Druid servers from sys.servers table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Querying All Servers Information\nDESCRIPTION: SQL query to retrieve information about all Druid servers from sys.servers table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Quantiles Doubles Sketch Aggregator Configuration\nDESCRIPTION: JSON configuration for setting up the quantilesDoublesSketch aggregator which performs union operations on sketches or builds them from raw data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"quantilesDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"k\": <parameter that controls size and accuracy>\n }\n```\n\n----------------------------------------\n\nTITLE: JavaScript Worker Selection Strategy Configuration in Druid\nDESCRIPTION: Example configuration showing a JavaScript function that routes batch index tasks to specific workers (10.0.0.1 and 10.0.0.2) while sending all other tasks to available workers. The function considers worker capacity and version compatibility.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\":\"javascript\",\n\"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"10.0.0.1\\\");\\nbatch_workers.add(\\\"10.0.0.2\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i < workers.length; i++){\\n sortedWorkers[i] = workers[i];\\n}\\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\\nvar minWorkerVer = config.getMinWorkerVersion();\\nfor (var i = 0; i < sortedWorkers.length; i++) {\\n var worker = sortedWorkers[i];\\n  var zkWorker = zkWorkers.get(worker);\\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\\n      return worker;\\n    } else {\\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\\n        return worker;\\n      }\\n    }\\n  }\\n}\\nreturn null;\\n}\"\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Worker Selection Strategy Configuration in Druid\nDESCRIPTION: Example configuration showing a JavaScript function that routes batch index tasks to specific workers (10.0.0.1 and 10.0.0.2) while sending all other tasks to available workers. The function considers worker capacity and version compatibility.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\":\"javascript\",\n\"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"10.0.0.1\\\");\\nbatch_workers.add(\\\"10.0.0.2\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i < workers.length; i++){\\n sortedWorkers[i] = workers[i];\\n}\\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\\nvar minWorkerVer = config.getMinWorkerVersion();\\nfor (var i = 0; i < sortedWorkers.length; i++) {\\n var worker = sortedWorkers[i];\\n  var zkWorker = zkWorkers.get(worker);\\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\\n      return worker;\\n    } else {\\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\\n        return worker;\\n      }\\n    }\\n  }\\n}\\nreturn null;\\n}\"\n}\n```\n\n----------------------------------------\n\nTITLE: React Sync External Store License Header\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fea5e92a.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: File Structure and License Header\nDESCRIPTION: YAML frontmatter defining document layout and title, followed by Apache 2.0 license header in HTML comments\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/caching.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Query Caching\"\n---\n\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Implementing Environment Variable Password Provider in Apache Druid\nDESCRIPTION: Configuration for environment variable password provider which retrieves passwords from system environment variables instead of hardcoding them in configuration files. This provider requires specifying the variable name from which to fetch the password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/password-provider.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" }\n```\n\n----------------------------------------\n\nTITLE: Resetting Supervisor POST Endpoint\nDESCRIPTION: REST endpoint to clear stored Kafka offsets and restart tasks from either earliest or latest offsets.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_9\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/reset\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Druid Database\nDESCRIPTION: Command to create a new PostgreSQL database named 'druid' owned by the druid user\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncreatedb druid -O druid\n```\n\n----------------------------------------\n\nTITLE: Displaying Coordination Metrics Table in Markdown\nDESCRIPTION: This code snippet shows a markdown table listing various coordination metrics for the Druid Coordinator. It includes metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/assigned/count`|Number of segments assigned to be loaded in the cluster.|tier.|Varies.|\n|`segment/moved/count`|Number of segments moved in the cluster.|tier.|Varies.|\n|`segment/dropped/count`|Number of segments dropped due to being overshadowed.|tier.|Varies.|\n|`segment/deleted/count`|Number of segments dropped due to rules.|tier.|Varies.|\n|`segment/unneeded/count`|Number of segments dropped due to being marked as unused.|tier.|Varies.|\n|`segment/cost/raw`|Used in cost balancing. The raw cost of hosting segments.|tier.|Varies.|\n|`segment/cost/normalization`|Used in cost balancing. The normalization of hosting segments.|tier.|Varies.|\n|`segment/cost/normalized`|Used in cost balancing. The normalized cost of hosting segments.|tier.|Varies.|\n|`segment/loadQueue/size`|Size in bytes of segments to load.|server.|Varies.|\n|`segment/loadQueue/failed`|Number of segments that failed to load.|server.|0|\n|`segment/loadQueue/count`|Number of segments to load.|server.|Varies.|\n|`segment/dropQueue/count`|Number of segments to drop.|server.|Varies.|\n|`segment/size`|Size in bytes of available segments.|dataSource.|Varies.|\n|`segment/count`|Number of available segments.|dataSource.|< max|\n|`segment/overShadowed/count`|Number of overShadowed segments.||Varies.|\n|`segment/unavailable/count`|Number of segments (not including replicas) left to load until segments that should be loaded in the cluster are available for queries.|datasource.|0|\n|`segment/underReplicated/count`|Number of segments (including replicas) left to load until segments that should be loaded in the cluster are available for queries.|tier, datasource.|0|\n```\n\n----------------------------------------\n\nTITLE: Configuring Upper Case Extraction Function with Locale in Druid\nDESCRIPTION: Demonstrates how to convert dimension values to uppercase with a specified locale (French in this example). The locale parameter determines language-specific case conversion rules.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"upper\",\n  \"locale\":\"fr\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring metadataUpdateSpec and segmentOutputPath in Druid Hadoop Ingestion JSON\nDESCRIPTION: Example JSON configuration for the ioConfig section in a Hadoop indexing spec file. This snippet shows how to configure the metadataUpdateSpec for MySQL and define a segmentOutputPath for the output data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n      \"ioConfig\" : {\n        ...\n        \"metadataUpdateSpec\" : {\n          \"type\":\"mysql\",\n          \"connectURI\" : \"jdbc:mysql://localhost:3306/druid\",\n          \"password\" : \"diurd\",\n          \"segmentTable\" : \"druid_segments\",\n          \"user\" : \"druid\"\n        },\n        \"segmentOutputPath\" : \"/MyDirectory/data/index/output\"\n      },\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Rule in Apache Druid\nDESCRIPTION: The Period Drop Rule drops segments based on a rolling time period. Segments whose intervals are contained within the specified ISO-8601 period will be removed, typically used for dropping recent data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Standalone JVM Parquet Ingestion\nDESCRIPTION: Bash command to run Parquet ingestion using a standalone JVM with Hadoop classpath configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/parquet.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nHADOOP_CLASS_PATH=`hadoop classpath | sed s/*.jar/*/g`\n\njava -Xmx32m -Duser.timezone=UTC -Dfile.encoding=UTF-8 \\\n  -classpath config/overlord:config/_common:lib/*:$HADOOP_CLASS_PATH:extensions/druid-avro-extensions/*  \\\n  org.apache.druid.cli.Main index hadoop \\\n  wikipedia_hadoop_parquet_job.json\n```\n\n----------------------------------------\n\nTITLE: Submitting SQL Query to Druid via curl\nDESCRIPTION: A curl command that submits an SQL query in JSON format to the Druid SQL endpoint. The SQL query is contained in the referenced JSON file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8082/druid/v2/sql\n```\n\n----------------------------------------\n\nTITLE: Retrieving Completion Report in Apache Druid\nDESCRIPTION: Shows the API endpoint for retrieving a completion report after a task finishes. The report contains information about ingested rows and parse exceptions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports\n```\n\n----------------------------------------\n\nTITLE: Retrieving MiddleManager Enabled State in JSON\nDESCRIPTION: GET request to /druid/worker/v1/enabled returns a JSON object indicating whether a MiddleManager is in an enabled or disabled state. The key is the combined druid.host and druid.port, with a boolean value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"localhost:8091\":true}\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Load Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a forever load rule, which specifies how many replicas of a segment should exist in different server tiers indefinitely.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TopN Query Properties in Druid\nDESCRIPTION: Configuration property for TopN queries that sets the minimum threshold for TopN results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_40\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.topN.minTopNThreshold=1000\n```\n\n----------------------------------------\n\nTITLE: Configuring TopN Query Properties in Druid\nDESCRIPTION: Configuration property for TopN queries that sets the minimum threshold for TopN results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_40\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.topN.minTopNThreshold=1000\n```\n\n----------------------------------------\n\nTITLE: Checking Druid Coordinator Leadership in HTTP\nDESCRIPTION: This HTTP GET request returns a JSON object indicating if the server is the current leader Coordinator, along with appropriate HTTP status codes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/isLeader\n```\n\n----------------------------------------\n\nTITLE: Example Segment Naming - Version 1\nDESCRIPTION: Demonstrates the naming convention for multiple segments with version 1, showing how partition numbers are used when data exceeds single segment capacity for a time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-01/2015-01-02_v1_1\nfoo_2015-01-01/2015-01-02_v1_2\n```\n\n----------------------------------------\n\nTITLE: Running the MiddleManager Server in Apache Druid (Java)\nDESCRIPTION: Command to start the MiddleManager process in Apache Druid. The MiddleManager acts as a worker that executes submitted tasks by forwarding them to Peons running in separate JVMs for resource and log isolation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/middlemanager.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Count Aggregator Configuration in Druid\nDESCRIPTION: Basic count aggregator that computes the number of Druid rows matching specified filters. Requires output name parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"count\", \"name\" : <output_name> }\n```\n\n----------------------------------------\n\nTITLE: Implementing Strlen Extraction Function in Druid\nDESCRIPTION: Returns the length of dimension values, measured in Unicode code units. Null strings are considered as having zero length.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"strlen\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticCloudFilesFirehose for Druid Data Ingestion\nDESCRIPTION: JSON configuration for setting up a static Cloud Files firehose to ingest data from multiple blobs. Supports caching and prefetching features with configurable parameters for capacity and timeout settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/cloudfiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"static-cloudfiles\",\n    \"blobs\": [\n        {\n          \"region\": \"DFW\"\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"region\": \"ORD\"\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Completion Report in Apache Druid\nDESCRIPTION: This snippet shows the API endpoint for retrieving a completion report after a task finishes in Apache Druid. The report contains information about ingested rows and any parse exceptions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/reports.md#2025-04-09_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/<task-id>/reports\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Deep Storage for Druid\nDESCRIPTION: Configuration for using HDFS as Druid's deep storage. Requires the 'druid-hdfs-storage' extension to be loaded and specifies the HDFS directory to use.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_14\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Druid Basic Authenticator\nDESCRIPTION: Configuration properties for setting up a Basic authenticator in Apache Druid, including admin password, internal client password, and connection to an authorizer.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyBasicAuthenticator\"]\n\ndruid.auth.authenticator.MyBasicAuthenticator.type=basic\ndruid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1\ndruid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2\ndruid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Displaying ResetCluster Tool Help Information in Apache Druid\nDESCRIPTION: Command to display detailed usage documentation for the ResetCluster tool, showing all available options and their descriptions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main help tools reset-cluster\n```\n\n----------------------------------------\n\nTITLE: Native Query Request Log Example in TSV format\nDESCRIPTION: Example of a TSV log entry for a native JSON query request to Druid. The format includes timestamp, remote address, native query details, query context, and an empty SQL query field since this is a native query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_5\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1   {\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikiticker\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"LegacyDimensionSpec\",\"dimension\":\"page\",\"outputName\":\"page\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"LegacyTopNMetricSpec\",\"metric\":\"count\"},\"threshold\":10,\"intervals\":{\"type\":\"LegacySegmentSpec\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"count\"}],\"postAggregations\":[],\"context\":{\"queryId\":\"74c2d540-d700-4ebd-b4a9-3d02397976aa\"},\"descending\":false}    {\"query/time\":100,\"query/bytes\":800,\"success\":true,\"identity\":\"user1\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Noop Task in Apache Druid\nDESCRIPTION: JSON configuration for a Noop task in Apache Druid. This task is used for testing purposes, allowing specification of an optional task ID, segment interval, runtime, and firehose connection test.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/misc-tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"noop\",\n    \"id\": <optional_task_id>,\n    \"interval\" : <optional_segment_interval>,\n    \"runTime\" : <optional_millis_to_sleep>,\n    \"firehose\": <optional_firehose_to_test_connect>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring White-List Based Graphite Event Converter in Druid\nDESCRIPTION: JSON configuration for the 'whiteList' event converter that selectively sends metrics based on a whitelist map. This example specifies a custom file path for the whitelist map configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"whiteList\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true, \"mapPath\":\"/pathPrefix/fileName.json\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring OR of Bound Filters in Druid Query (JSON)\nDESCRIPTION: This snippet demonstrates how to configure an OR of bound filters in a Druid query, equivalent to the interval filter example.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"or\",\n    \"fields\": [\n      {\n        \"type\": \"bound\",\n        \"dimension\": \"__time\",\n        \"lower\": \"1412121600000\",\n        \"lowerStrict\": false,\n        \"upper\": \"1412640000000\" ,\n        \"upperStrict\": true,\n        \"ordering\": \"numeric\"\n      },\n      {\n         \"type\": \"bound\",\n         \"dimension\": \"__time\",\n         \"lower\": \"1416009600000\",\n         \"lowerStrict\": false,\n         \"upper\": \"1416096000000\" ,\n         \"upperStrict\": true,\n         \"ordering\": \"numeric\"\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling On-heap Lookup Cache in Druid\nDESCRIPTION: Example configuration for a polling cache that updates its on-heap cache every 10 minutes using JDBC data fetcher.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"pollPeriod\":\"PT10M\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"onHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Tables for Druid Deep Storage\nDESCRIPTION: SQL statements to create the required 'index_storage' and 'descriptor_storage' tables in Cassandra for storing Druid segments and metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Tables for Druid Deep Storage\nDESCRIPTION: SQL statements to create the required 'index_storage' and 'descriptor_storage' tables in Cassandra for storing Druid segments and metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: Configuring Standard Deviation Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the standard deviation post-aggregator in Druid queries. This post-aggregator computes the standard deviation from a variance aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/stats.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"stddev\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"estimator\": <string>\n}\n```\n\n----------------------------------------\n\nTITLE: Running Druid MiddleManager Server\nDESCRIPTION: Command to start the Druid MiddleManager server process. The MiddleManager is responsible for executing tasks by forwarding them to Peon workers that run in separate JVMs for resource and log isolation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/middlemanager.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Enabling JavaScript Configuration in Druid\nDESCRIPTION: Configuration property to enable JavaScript execution in Druid. By default JavaScript is disabled for security reasons.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/javascript.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.javascript.enabled = true\n```\n\n----------------------------------------\n\nTITLE: SimpleJSON Example Input for Druid Lookup\nDESCRIPTION: Example line-delimited JSON input data for simpleJSON parsing in Druid, where each line contains a single key-value pair.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"foo\": \"bar\"}\n{\"baz\": \"bat\"}\n{\"buck\": \"truck\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Partial Extraction Function in Apache Druid JSON\nDESCRIPTION: This snippet shows how to configure a Partial Extraction Function in Apache Druid. It returns the dimension value unchanged if the regex matches, otherwise returns null.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"partial\", \"expr\" : <regular_expression> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Caching Properties\nDESCRIPTION: Configuration properties for enabling and controlling caching on the Broker, including options for result level caching and cache limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_42\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.cache.useCache=false\ndruid.broker.cache.populateCache=false\ndruid.broker.cache.useResultLevelCache=false\ndruid.broker.cache.populateResultLevelCache=false\ndruid.broker.cache.resultLevelCacheLimit=Integer.MAX_VALUE\ndruid.broker.cache.unCacheable=[\"groupBy\", \"select\"]\ndruid.broker.cache.cacheBulkMergeLimit=Integer.MAX_VALUE\n```\n\n----------------------------------------\n\nTITLE: Example of Time Format Extraction for Day of Week\nDESCRIPTION: An example of a dimension specification that applies a time format extraction function to the __time dimension, returning the day of the week in French for the Montreal timezone.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : \"__time\",\n  \"outputName\" :  \"dayOfWeek\",\n  \"extractionFn\" : {\n    \"type\" : \"timeFormat\",\n    \"format\" : \"EEEE\",\n    \"timeZone\" : \"America/Montreal\",\n    \"locale\" : \"fr\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running pull-deps Tool with Default Version in Java for Apache Druid\nDESCRIPTION: Example command to run the pull-deps tool using the --defaultVersion option. This allows specifying coordinates without version information for multiple extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/pull-deps.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.15.1-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Drop Rule in Druid\nDESCRIPTION: JSON configuration for an interval drop rule that removes segments within a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Parser Configuration\nDESCRIPTION: Parser configuration for handling JSON-formatted input data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Aggregator in Druid\nDESCRIPTION: Configuration for hyperUnique aggregator that processes pre-aggregated HyperLogLog values. Includes options for handling pre-computed HLL inputs and result rounding.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/hll-old.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"hyperUnique\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"isInputHyperUnique\" : false,\n  \"round\" : false\n}\n```\n\n----------------------------------------\n\nTITLE: Printing ResetCluster Tool Help Documentation in Druid\nDESCRIPTION: Command to display the usage documentation for the ResetCluster tool, which explains all available options and their purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main help tools reset-cluster\n```\n\n----------------------------------------\n\nTITLE: Setting Up Period Drop Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period drop rule that indicates segments within a rolling time period should be dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByPeriod\",\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Moving Average Query in Druid\nDESCRIPTION: Example query calculating a 7-bucket moving average for Wikipedia edit deltas using 30-minute granularity intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"movingAverage\",\n  \"dataSource\": \"wikipedia\",\n  \"granularity\": {\n    \"type\": \"period\",\n    \"period\": \"PT30M\"\n  },\n  \"intervals\": [\n    \"2015-09-12T00:00:00Z/2015-09-13T00:00:00Z\"\n  ],\n  \"aggregations\": [\n    {\n      \"name\": \"delta30Min\",\n      \"fieldName\": \"delta\",\n      \"type\": \"longSum\"\n    }\n  ],\n  \"averagers\": [\n    {\n      \"name\": \"trailing30MinChanges\",\n      \"fieldName\": \"delta30Min\",\n      \"type\": \"longMean\",\n      \"buckets\": 7\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Kafka Topic in Bash\nDESCRIPTION: Commands to set Kafka options and use the console producer to load sample data into the 'wikipedia' topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport KAFKA_OPTS=\"-Dfile.encoding=UTF-8\"\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeMin Aggregator for Druid Ingestion\nDESCRIPTION: JSON configuration for including a timeMin aggregator during data ingestion in Druid. The fieldName typically refers to the column specified in the timestamp spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMin\",\n    \"name\": \"tmin\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Subtask Spec State Response JSON Structure\nDESCRIPTION: Detailed JSON response structure for the subtask spec state endpoint including task specification, current status, and execution history for a specific worker task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"spec\": {\n    \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\",\n    \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"context\": null,\n    \"inputSplit\": {\n      \"split\": \"/path/to/data/lineitem.tbl.5\"\n    },\n    \"ingestionSpec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"lineitem\",\n        \"parser\": {\n          \"type\": \"hadoopyString\",\n          \"parseSpec\": {\n            \"format\": \"tsv\",\n            \"delimiter\": \"|\",\n            \"timestampSpec\": {\n              \"column\": \"l_shipdate\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"l_orderkey\",\n                \"l_partkey\",\n                \"l_suppkey\",\n                \"l_linenumber\",\n                \"l_returnflag\",\n                \"l_linestatus\",\n                \"l_shipdate\",\n                \"l_commitdate\",\n                \"l_receiptdate\",\n                \"l_shipinstruct\",\n                \"l_shipmode\",\n                \"l_comment\"\n              ]\n            },\n            \"columns\": [\n              \"l_orderkey\",\n              \"l_partkey\",\n              \"l_suppkey\",\n              \"l_linenumber\",\n              \"l_quantity\",\n              \"l_extendedprice\",\n              \"l_discount\",\n              \"l_tax\",\n              \"l_returnflag\",\n              \"l_linestatus\",\n              \"l_shipdate\",\n              \"l_commitdate\",\n              \"l_receiptdate\",\n              \"l_shipinstruct\",\n              \"l_shipmode\",\n              \"l_comment\"\n            ]\n          }\n        },\n        \"metricsSpec\": [\n          {\n            \"type\": \"count\",\n            \"name\": \"count\"\n          },\n          {\n            \"type\": \"longSum\",\n            \"name\": \"l_quantity\",\n            \"fieldName\": \"l_quantity\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_extendedprice\",\n            \"fieldName\": \"l_extendedprice\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_discount\",\n            \"fieldName\": \"l_discount\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_tax\",\n            \"fieldName\": \"l_tax\",\n            \"expression\": null\n          }\n        ],\n        \"granularitySpec\": {\n          \"type\": \"uniform\",\n          \"segmentGranularity\": \"YEAR\",\n          \"queryGranularity\": {\n            \"type\": \"none\"\n          },\n          \"rollup\": true,\n          \"intervals\": [\n            \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n          ]\n        },\n        \"transformSpec\": {\n          \"filter\": null,\n          \"transforms\": []\n        }\n      },\n      \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"/path/to/data/\",\n          \"filter\": \"lineitem.tbl.5\",\n          \"parser\": null\n        },\n        \"appendToExisting\": false\n      },\n      \"tuningConfig\": {\n        \"type\": \"index_parallel\",\n        \"maxRowsPerSegment\": 5000000,\n        \"maxRowsInMemory\": 1000000,\n        \"maxTotalRows\": 20000000,\n        \"numShards\": null,\n        \"indexSpec\": {\n          \"bitmap\": {\n            \"type\": \"concise\"\n          },\n          \"dimensionCompression\": \"lz4\",\n          \"metricCompression\": \"lz4\",\n          \"longEncoding\": \"longs\"\n        },\n        \"maxPendingPersists\": 0,\n        \"reportParseExceptions\": false,\n        \"pushTimeout\": 0,\n        \"segmentWriteOutMediumFactory\": null,\n        \"maxNumSubTasks\": 4,\n        \"maxRetry\": 3,\n        \"taskStatusCheckPeriodMs\": 1000,\n        \"chatHandlerTimeout\": \"PT10S\",\n        \"chatHandlerNumRetries\": 5,\n        \"logParseExceptions\": false,\n        \"maxParseExceptions\": 2147483647,\n        \"maxSavedParseExceptions\": 0,\n        \"forceGuaranteedRollup\": false,\n        \"buildV9Directly\": true\n      }\n    }\n  },\n  \"currentStatus\": {\n    \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\",\n    \"type\": \"index_sub\",\n    \"createdTime\": \"2018-04-20T22:16:29.925Z\",\n    \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\",\n    \"statusCode\": \"RUNNING\",\n    \"duration\": -1,\n    \"location\": {\n      \"host\": null,\n      \"port\": -1,\n      \"tlsPort\": -1\n    },\n    \"dataSource\": \"lineitem\",\n    \"errorMsg\": null\n  },\n  \"taskHistory\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering for Null Values in Multi-value Dimensions in Druid\nDESCRIPTION: Example of a selector filter that matches rows where the multi-value dimension 'tags' is null or empty. This query would match row4 from the sample dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"tags\",\n  \"value\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring floatMin Aggregator in Apache Druid\nDESCRIPTION: The floatMin aggregator computes the minimum of all metric values and Float.POSITIVE_INFINITY. It requires an output name and the field name of the metric to find the minimum value of.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"floatMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: One-Sided Lower Bound Age Filter in Druid\nDESCRIPTION: Demonstrates a one-sided bound filter for ages greater than or equal to 18.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"age\",\n    \"lower\": \"18\" ,\n    \"ordering\": \"numeric\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Druid Basic Escalator\nDESCRIPTION: Configuration properties for setting up the escalator, which handles internal system authentication between Druid services.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# Escalator\ndruid.escalator.type=basic\ndruid.escalator.internalClientUsername=druid_system\ndruid.escalator.internalClientPassword=password2\ndruid.escalator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Concise Bitmap Configuration Table in Markdown\nDESCRIPTION: Configuration table showing the required fields for Concise bitmap type specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|type|String|Must be `concise`.|yes|\n```\n\n----------------------------------------\n\nTITLE: Specifying ZooKeeper Path for Process Announcements in Druid\nDESCRIPTION: This snippet defines the ZooKeeper path where Historical and Realtime processes in Druid announce their existence by creating an ephemeral znode.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.announcementsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Ingestion Metrics\nDESCRIPTION: Table describing ingestion-related metrics with their descriptions, dimensions, and normal values\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/thrownAway`|Number of events rejected because they are outside the windowPeriod.|dataSource, taskId, taskType.|0|\n```\n\n----------------------------------------\n\nTITLE: Retrieving Worker Task Spec State in Apache Druid\nDESCRIPTION: This JSON response shows the detailed state of a worker task specification, including the complete spec configuration, current task status, and task history. It contains the data schema, IO configuration, and tuning parameters used for the indexing task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/native_tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"spec\": {\n    \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\",\n    \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"context\": null,\n    \"inputSplit\": {\n      \"split\": \"/path/to/data/lineitem.tbl.5\"\n    },\n    \"ingestionSpec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"lineitem\",\n        \"parser\": {\n          \"type\": \"hadoopyString\",\n          \"parseSpec\": {\n            \"format\": \"tsv\",\n            \"delimiter\": \"|\",\n            \"timestampSpec\": {\n              \"column\": \"l_shipdate\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"l_orderkey\",\n                \"l_partkey\",\n                \"l_suppkey\",\n                \"l_linenumber\",\n                \"l_returnflag\",\n                \"l_linestatus\",\n                \"l_shipdate\",\n                \"l_commitdate\",\n                \"l_receiptdate\",\n                \"l_shipinstruct\",\n                \"l_shipmode\",\n                \"l_comment\"\n              ]\n            },\n            \"columns\": [\n              \"l_orderkey\",\n              \"l_partkey\",\n              \"l_suppkey\",\n              \"l_linenumber\",\n              \"l_quantity\",\n              \"l_extendedprice\",\n              \"l_discount\",\n              \"l_tax\",\n              \"l_returnflag\",\n              \"l_linestatus\",\n              \"l_shipdate\",\n              \"l_commitdate\",\n              \"l_receiptdate\",\n              \"l_shipinstruct\",\n              \"l_shipmode\",\n              \"l_comment\"\n            ]\n          }\n        },\n        \"metricsSpec\": [\n          {\n            \"type\": \"count\",\n            \"name\": \"count\"\n          },\n          {\n            \"type\": \"longSum\",\n            \"name\": \"l_quantity\",\n            \"fieldName\": \"l_quantity\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_extendedprice\",\n            \"fieldName\": \"l_extendedprice\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_discount\",\n            \"fieldName\": \"l_discount\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_tax\",\n            \"fieldName\": \"l_tax\",\n            \"expression\": null\n          }\n        ],\n        \"granularitySpec\": {\n          \"type\": \"uniform\",\n          \"segmentGranularity\": \"YEAR\",\n          \"queryGranularity\": {\n            \"type\": \"none\"\n          },\n          \"rollup\": true,\n          \"intervals\": [\n            \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n          ]\n        },\n        \"transformSpec\": {\n          \"filter\": null,\n          \"transforms\": []\n        }\n      },\n      \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"/path/to/data/\",\n          \"filter\": \"lineitem.tbl.5\",\n          \"parser\": null\n        },\n        \"appendToExisting\": false\n      },\n      \"tuningConfig\": {\n        \"type\": \"index_parallel\",\n        \"maxRowsPerSegment\": 5000000,\n        \"maxRowsInMemory\": 1000000,\n        \"maxTotalRows\": 20000000,\n        \"numShards\": null,\n        \"indexSpec\": {\n          \"bitmap\": {\n            \"type\": \"concise\"\n          },\n          \"dimensionCompression\": \"lz4\",\n          \"metricCompression\": \"lz4\",\n          \"longEncoding\": \"longs\"\n        },\n        \"maxPendingPersists\": 0,\n        \"forceExtendableShardSpecs\": false,\n        \"reportParseExceptions\": false,\n        \"pushTimeout\": 0,\n        \"segmentWriteOutMediumFactory\": null,\n        \"maxNumSubTasks\": 4,\n        \"maxRetry\": 3,\n        \"taskStatusCheckPeriodMs\": 1000,\n        \"chatHandlerTimeout\": \"PT10S\",\n        \"chatHandlerNumRetries\": 5,\n        \"logParseExceptions\": false,\n        \"maxParseExceptions\": 2147483647,\n        \"maxSavedParseExceptions\": 0,\n        \"forceGuaranteedRollup\": false,\n        \"buildV9Directly\": true\n      }\n    }\n  },\n  \"currentStatus\": {\n    \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\",\n    \"type\": \"index_sub\",\n    \"createdTime\": \"2018-04-20T22:16:29.925Z\",\n    \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\",\n    \"statusCode\": \"RUNNING\",\n    \"duration\": -1,\n    \"location\": {\n      \"host\": null,\n      \"port\": -1,\n      \"tlsPort\": -1\n    },\n    \"dataSource\": \"lineitem\",\n    \"errorMsg\": null\n  },\n  \"taskHistory\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Completion Report in Apache Druid\nDESCRIPTION: Example of a completion report JSON structure returned by the Druid API after a task completes. It includes task ID, ingestion state, row statistics, and error information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/reports.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for NProgress in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the NProgress library, attributing it to Rico Sta. Cruz and providing a link to the author's website.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring CustomJSON Lookup ParseSpec in Apache Druid\nDESCRIPTION: Example JSON configuration for a customJson lookup parseSpec. Specifies how to extract key-value pairs from JSON objects by defining the field names for keys and values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"namespaceParseSpec\": {\n  \"format\": \"customJson\",\n  \"keyFieldName\": \"key\",\n  \"valueFieldName\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Parser with timeAndDims ParseSpec\nDESCRIPTION: JSON configuration example for the ORC parser using the 'timeAndDims' parseSpec format which requires explicitly specified dimensions in the dimensionsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/orc.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Parser with timeAndDims ParseSpec\nDESCRIPTION: JSON configuration example for the ORC parser using the 'timeAndDims' parseSpec format which requires explicitly specified dimensions in the dimensionsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/orc.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"dim1\",\n              \"dim2\",\n              \"dim3\",\n              \"listDim\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Parser with Auto Field Discovery and Flatten Expressions in JSON\nDESCRIPTION: Example configuration for Hadoop-based indexing with ORC parser using auto field discovery and flatten expressions. This demonstrates how to extract nested fields and list elements from ORC data structures.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/orc.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\",\n        \"paths\": \"path/to/file.orc\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"orc\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"millis\"\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Header\nDESCRIPTION: YAML frontmatter defining the document layout and title for the Druid indexing service documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/indexing-service.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Indexing Service\"\n---\n```\n\n----------------------------------------\n\nTITLE: Chrome Kerberos Configuration Commands\nDESCRIPTION: Command line instructions for configuring Google Chrome browser to work with Kerberos authentication for Druid UI access.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngoogle-chrome --auth-server-whitelist=\"druid-coordinator-hostname\" --auth-negotiate-delegate-whitelist=\"druid-coordinator-hostname\"\ngoogle-chrome --auth-server-whitelist=\"druid-overlord-hostname\" --auth-negotiate-delegate-whitelist=\"druid-overlord-hostname\"\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration with Directory Prefix and File Pattern\nDESCRIPTION: JSON configuration for a URI-based lookup that searches for files in a directory using a regular expression pattern. This approach allows polling a directory for files that match a specific naming pattern.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uriPrefix\": \"s3://bucket/some/key/prefix/\",\n  \"fileRegex\":\"renames-[0-9]*\\\\.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LongMin Aggregator in Druid JSON\nDESCRIPTION: Defines a longMin aggregator to compute the minimum of all metric values and Long.MAX_VALUE. It requires an output name and the field name to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Query in Apache Druid\nDESCRIPTION: This snippet outlines the configuration properties for Search queries in Apache Druid, including the maximum search limit and default search strategy.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_44\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.query.search.maxSearchLimit`|Maximum number of search results to return.|1000|\n|`druid.query.search.searchStrategy`|Default search query strategy.|useIndexes|\n```\n\n----------------------------------------\n\nTITLE: Partial Extraction Function Configuration\nDESCRIPTION: Configuration for conditional extraction based on regex matching that returns null for non-matches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"partial\", \"expr\" : <regular_expression> }\n```\n\n----------------------------------------\n\nTITLE: Advanced Maven Build with Profiles\nDESCRIPTION: Comprehensive Maven build command that generates source and binary distributions with signatures and checksums, runs license audits, and skips unit tests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/build.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Papache-release,dist,rat -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Load Rule in Druid\nDESCRIPTION: JSON configuration for a forever load rule that specifies how many replicas of a segment should exist in different server tiers permanently.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Granularity in Apache Druid\nDESCRIPTION: This snippet demonstrates how to configure the segment granularity in the granularitySpec, which determines the time interval size for each segment. This example creates hourly segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }      \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"type\" : \"uniform\",\n    \"segmentGranularity\" : \"HOUR\",\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring metadataUpdateSpec and segmentOutputPath in JSON\nDESCRIPTION: JSON configuration for the ioConfig section of a Hadoop indexing spec. This shows how to configure the metadataUpdateSpec for MySQL database connection and specifies the segmentOutputPath where output data will be stored.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n      \"ioConfig\" : {\n        ...\n        \"metadataUpdateSpec\" : {\n          \"type\":\"mysql\",\n          \"connectURI\" : \"jdbc:mysql://localhost:3306/druid\",\n          \"password\" : \"diurd\",\n          \"segmentTable\" : \"druid_segments\",\n          \"user\" : \"druid\"\n        },\n        \"segmentOutputPath\" : \"/MyDirectory/data/index/output\"\n      },\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Broadcast Rule in Druid\nDESCRIPTION: JSON configuration for an interval broadcast rule that defines segment co-location for a specific time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/rule-configuration.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByInterval\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Context Parameters in Apache Druid\nDESCRIPTION: A markdown table listing global query context parameters for Apache Druid, including their default values and descriptions. This table covers essential settings such as timeout, priority, caching, and result formatting options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/query-context.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default                                 | description          |\n|-----------------|----------------------------------------|----------------------|\n|timeout          | `druid.server.http.defaultQueryTimeout`| Query timeout in millis, beyond which unfinished queries will be cancelled. 0 timeout means `no timeout`. To set the default timeout, see [Broker configuration](../configuration/index.html#broker) |\n|priority         | `0`                                    | Query Priority. Queries with higher priority get precedence for computational resources.|\n|queryId          | auto-generated                         | Unique identifier given to this query. If a query ID is set or known, this can be used to cancel the query |\n|useCache         | `true`                                 | Flag indicating whether to leverage the query cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Apache Druid (incubating) uses druid.broker.cache.useCache or druid.historical.cache.useCache to determine whether or not to read from the query cache |\n|populateCache    | `true`                                 | Flag indicating whether to save the results of the query to the query cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache or druid.historical.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|useResultLevelCache         | `false`                                 | Flag indicating whether to leverage the result level cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useResultLevelCache to determine whether or not to read from the query cache |\n|populateResultLevelCache    | `false`                                 | Flag indicating whether to save the results of the query to the result level cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|bySegment        | `false`                                | Return \"by segment\" results. Primarily used for debugging, setting it to `true` returns results associated with the data segment they came from |\n|finalize         | `true`                                 | Flag indicating whether to \"finalize\" aggregation results. Primarily used for debugging. For instance, the `hyperUnique` aggregator will return the full HyperLogLog sketch instead of the estimated cardinality when this flag is set to `false` |\n|chunkPeriod      | `P0D` (off)                            | At the Broker process level, long interval queries (of any type) may be broken into shorter interval queries to parallelize merging more than normal. Broken up queries will use a larger share of cluster resources, but, if you use groupBy \"v1, it may be able to complete faster as a result. Use ISO 8601 periods. For example, if this property is set to `P1M` (one month), then a query covering a year would be broken into 12 smaller queries. The broker uses its query processing executor service to initiate processing for query chunks, so make sure \"druid.processing.numThreads\" is configured appropriately on the broker. [groupBy queries](groupbyquery.html) do not support chunkPeriod by default, although they do if using the legacy \"v1\" engine. This context is deprecated since it's only useful for groupBy \"v1\", and will be removed in the future releases.|\n|maxScatterGatherBytes| `druid.server.http.maxScatterGatherBytes` | Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. This parameter can be used to further reduce `maxScatterGatherBytes` limit at query time. See [Broker configuration](../configuration/index.html#broker) for more details.|\n|maxQueuedBytes       | `druid.broker.http.maxQueuedBytes`        | Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to `maxScatterGatherBytes`, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled.|\n|serializeDateTimeAsLong| `false`       | If true, DateTime is serialized as long in the result returned by Broker and the data transportation between Broker and compute process|\n|serializeDateTimeAsLongInner| `false`  | If true, DateTime is serialized as long in the data transportation between Broker and compute process|\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Context Parameters in Apache Druid\nDESCRIPTION: A markdown table listing global query context parameters for Apache Druid, including their default values and descriptions. This table covers essential settings such as timeout, priority, caching, and result formatting options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/query-context.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default                                 | description          |\n|-----------------|----------------------------------------|----------------------|\n|timeout          | `druid.server.http.defaultQueryTimeout`| Query timeout in millis, beyond which unfinished queries will be cancelled. 0 timeout means `no timeout`. To set the default timeout, see [Broker configuration](../configuration/index.html#broker) |\n|priority         | `0`                                    | Query Priority. Queries with higher priority get precedence for computational resources.|\n|queryId          | auto-generated                         | Unique identifier given to this query. If a query ID is set or known, this can be used to cancel the query |\n|useCache         | `true`                                 | Flag indicating whether to leverage the query cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Apache Druid (incubating) uses druid.broker.cache.useCache or druid.historical.cache.useCache to determine whether or not to read from the query cache |\n|populateCache    | `true`                                 | Flag indicating whether to save the results of the query to the query cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache or druid.historical.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|useResultLevelCache         | `false`                                 | Flag indicating whether to leverage the result level cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useResultLevelCache to determine whether or not to read from the query cache |\n|populateResultLevelCache    | `false`                                 | Flag indicating whether to save the results of the query to the result level cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache to determine whether or not to save the results of this query to the query cache |\n|bySegment        | `false`                                | Return \"by segment\" results. Primarily used for debugging, setting it to `true` returns results associated with the data segment they came from |\n|finalize         | `true`                                 | Flag indicating whether to \"finalize\" aggregation results. Primarily used for debugging. For instance, the `hyperUnique` aggregator will return the full HyperLogLog sketch instead of the estimated cardinality when this flag is set to `false` |\n|chunkPeriod      | `P0D` (off)                            | At the Broker process level, long interval queries (of any type) may be broken into shorter interval queries to parallelize merging more than normal. Broken up queries will use a larger share of cluster resources, but, if you use groupBy \"v1, it may be able to complete faster as a result. Use ISO 8601 periods. For example, if this property is set to `P1M` (one month), then a query covering a year would be broken into 12 smaller queries. The broker uses its query processing executor service to initiate processing for query chunks, so make sure \"druid.processing.numThreads\" is configured appropriately on the broker. [groupBy queries](groupbyquery.html) do not support chunkPeriod by default, although they do if using the legacy \"v1\" engine. This context is deprecated since it's only useful for groupBy \"v1\", and will be removed in the future releases.|\n|maxScatterGatherBytes| `druid.server.http.maxScatterGatherBytes` | Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. This parameter can be used to further reduce `maxScatterGatherBytes` limit at query time. See [Broker configuration](../configuration/index.html#broker) for more details.|\n|maxQueuedBytes       | `druid.broker.http.maxQueuedBytes`        | Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to `maxScatterGatherBytes`, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled.|\n|serializeDateTimeAsLong| `false`       | If true, DateTime is serialized as long in the result returned by Broker and the data transportation between Broker and compute process|\n|serializeDateTimeAsLongInner| `false`  | If true, DateTime is serialized as long in the data transportation between Broker and compute process|\n```\n\n----------------------------------------\n\nTITLE: Submitting Druid Kafka Supervisor\nDESCRIPTION: cURL command to submit a Kafka supervisor specification to Druid's overlord service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Including MomentSketch Extension in Druid Configuration\nDESCRIPTION: Configuration snippet for loading the MomentSketch module in Apache Druid. This should be added to the Druid configuration file to enable the extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-momentsketch\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Library Dependencies and Assembly Merge Strategy for Druid in Scala\nDESCRIPTION: This snippet defines the library dependencies for a Scala project using Apache Druid, including AWS SDK, Joda Time, various Druid modules, Jackson libraries, and testing frameworks. It also specifies an assembly merge strategy to handle conflicts during the build process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/use_sbt_to_build_fat_jar.md#2025-04-09_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.9.23\" exclude(\"common-logging\", \"common-logging\"),\n  \"org.joda\" % \"joda-convert\" % \"1.7\",\n  \"joda-time\" % \"joda-time\" % \"2.7\",\n  \"org.apache.druid\" % \"druid\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-services\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-service\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-hadoop\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"mysql-metadata-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-s3-extensions\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-histogram\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-hdfs-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-guava\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-joda\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-base\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-json-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-smile-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-jaxb-annotations\" % \"2.3.0\",\n  \"com.sun.jersey\" % \"jersey-servlet\" % \"1.17.1\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.34\",\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.3\" % \"test\",\n  \"org.mockito\" % \"mockito-core\" % \"1.10.19\" % \"test\"\n)\n\nassemblyMergeStrategy in assembly := {\n  case path if path contains \"pom.\" => MergeStrategy.first\n  case path if path contains \"javax.inject.Named\" => MergeStrategy.first\n  case path if path contains \"mime.types\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog$1.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/NoOpLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogFactory.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogConfigurationException.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/Log.class\" => MergeStrategy.first\n  case path if path contains \"META-INF/jersey-module-version\" => MergeStrategy.first\n  case path if path contains \".properties\" => MergeStrategy.first\n  case path if path contains \".class\" => MergeStrategy.first\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Library Dependencies and Assembly Merge Strategy for Druid in Scala\nDESCRIPTION: This snippet defines the library dependencies for a Scala project using Apache Druid, including AWS SDK, Joda Time, various Druid modules, Jackson libraries, and testing frameworks. It also specifies an assembly merge strategy to handle conflicts during the build process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/use_sbt_to_build_fat_jar.md#2025-04-09_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.9.23\" exclude(\"common-logging\", \"common-logging\"),\n  \"org.joda\" % \"joda-convert\" % \"1.7\",\n  \"joda-time\" % \"joda-time\" % \"2.7\",\n  \"org.apache.druid\" % \"druid\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-services\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-service\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid\" % \"druid-indexing-hadoop\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"mysql-metadata-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-s3-extensions\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-histogram\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"org.apache.druid.extensions\" % \"druid-hdfs-storage\" % \"0.8.1\" excludeAll (\n    ExclusionRule(\"org.ow2.asm\"),\n    ExclusionRule(\"com.fasterxml.jackson.core\"),\n    ExclusionRule(\"com.fasterxml.jackson.datatype\"),\n    ExclusionRule(\"com.fasterxml.jackson.dataformat\"),\n    ExclusionRule(\"com.fasterxml.jackson.jaxrs\"),\n    ExclusionRule(\"com.fasterxml.jackson.module\")\n  ),\n  \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.3.0\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-guava\" % \"2.3.0\",\n  \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-joda\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-base\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-json-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-smile-provider\" % \"2.3.0\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-jaxb-annotations\" % \"2.3.0\",\n  \"com.sun.jersey\" % \"jersey-servlet\" % \"1.17.1\",\n  \"mysql\" % \"mysql-connector-java\" % \"5.1.34\",\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.3\" % \"test\",\n  \"org.mockito\" % \"mockito-core\" % \"1.10.19\" % \"test\"\n)\n\nassemblyMergeStrategy in assembly := {\n  case path if path contains \"pom.\" => MergeStrategy.first\n  case path if path contains \"javax.inject.Named\" => MergeStrategy.first\n  case path if path contains \"mime.types\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/SimpleLog$1.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/impl/NoOpLog.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogFactory.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/LogConfigurationException.class\" => MergeStrategy.first\n  case path if path contains \"org/apache/commons/logging/Log.class\" => MergeStrategy.first\n  case path if path contains \"META-INF/jersey-module-version\" => MergeStrategy.first\n  case path if path contains \".properties\" => MergeStrategy.first\n  case path if path contains \".class\" => MergeStrategy.first\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n```\n\n----------------------------------------\n\nTITLE: Distinct People Cardinality Example\nDESCRIPTION: Example demonstrating how to count distinct combinations of first and last names using byRow cardinality calculation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/hll-old.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_people\",\n  \"fields\": [ \"first_name\", \"last_name\" ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Interval Filter on Timestamp Column in Druid (JSON)\nDESCRIPTION: This snippet shows how to set up an interval filter on the timestamp column in a Druid query. It filters for specific time ranges using ISO 8601 intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"interval\",\n    \"dimension\" : \"__time\",\n    \"intervals\" : [\n      \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\",\n      \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid for PostgreSQL Metadata Storage\nDESCRIPTION: Configuration properties required for connecting Druid to a PostgreSQL metadata store. These parameters specify the extension to load, storage type, connection URI, username, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/postgresql.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"postgresql-metadata-storage\"]\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://<host>/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: JSON Password Request Object for Apache Druid Basic Security\nDESCRIPTION: Example JSON body for a password assignment request to the Basic Security API, used when setting a user's password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"password\": \"helloworld\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Deep Storage\nDESCRIPTION: Configuration properties for setting up HDFS as the deep storage layer for Druid segments and indexing logs\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-hdfs-storage\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Log Retention in Druid\nDESCRIPTION: Configuration properties for automated task log retention in Druid, controlling how long logs are kept and how frequently cleanup tasks run.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_13\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.logs.kill.enabled=false\ndruid.indexer.logs.kill.durationToRetain=None\ndruid.indexer.logs.kill.initialDelay=random value less than 300000 (5 mins)\ndruid.indexer.logs.kill.delay=21600000 (6 hours)\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Site Properties for Elastic MapReduce in Druid\nDESCRIPTION: This YAML configuration sets various MapReduce properties for memory, Java options, and task timeout in the YARN site. It's used when creating an EMR cluster for Druid data indexing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nclassification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000]\n```\n\n----------------------------------------\n\nTITLE: Equal Buckets Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for computing histogram representation with equal-sized bins. Requires number of buckets parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"equalBuckets\",\n  \"name\": \"<output_name>\",\n  \"fieldName\": \"<aggregator_name>\",\n  \"numBuckets\": <count>\n}\n```\n\n----------------------------------------\n\nTITLE: Query Error Response Structure in Druid\nDESCRIPTION: This JSON snippet illustrates the structure of an error response returned by Druid when a query fails. It includes fields for error code, message, exception class, and host information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/querying.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\" : \"Query timeout\",\n  \"errorMessage\" : \"Timeout waiting for task.\",\n  \"errorClass\" : \"java.util.concurrent.TimeoutException\",\n  \"host\" : \"druid1.example.com:8083\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Histogram Post Aggregator for DoublesSketch\nDESCRIPTION: Post aggregator configuration to generate histogram data from a DoublesSketch. It divides data into bins defined by the splitPoints array and returns the approximate count in each bin.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToHistogram\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"splitPoints\" : <array of split points>\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Event Structure in Apache Druid\nDESCRIPTION: This snippet shows an example of a JSON event structure that could be ingested into Apache Druid. It includes various data types and nested structures.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/flatten-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"timestamp\": \"2015-09-12T12:10:53.155Z\",\n \"dim1\": \"qwerty\",\n \"dim2\": \"asdf\",\n \"dim3\": \"zxcv\",\n \"ignore_me\": \"ignore this\",\n \"metrica\": 9999,\n \"foo\": {\"bar\": \"abc\"},\n \"foo.bar\": \"def\",\n \"nestmet\": {\"val\": 42},\n \"hello\": [1.0, 2.0, 3.0, 4.0, 5.0],\n \"mixarray\": [1.0, 2.0, 3.0, 4.0, {\"last\": 5}],\n \"world\": [{\"hey\": \"there\"}, {\"tree\": \"apple\"}],\n \"thing\": {\"food\": [\"sandwich\", \"pizza\"]}\n}\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query for Unique User Count by Product\nDESCRIPTION: Demonstrates a complete Druid query using thetaSketch aggregator to count unique users for a specific product.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"aggregations\": [\n    { \"type\": \"thetaSketch\", \"name\": \"unique_users\", \"fieldName\": \"user_id_sketch\" }\n  ],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\" },\n  \"intervals\": [ \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Druid Query for Unique User Count by Product\nDESCRIPTION: Demonstrates a complete Druid query using thetaSketch aggregator to count unique users for a specific product.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"aggregations\": [\n    { \"type\": \"thetaSketch\", \"name\": \"unique_users\", \"fieldName\": \"user_id_sketch\" }\n  ],\n  \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\" },\n  \"intervals\": [ \"2014-10-19T00:00:00.000Z/2014-10-22T00:00:00.000Z\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Druid Database and User in MySQL\nDESCRIPTION: SQL commands to create a Druid database with UTF-8 encoding, create a Druid user, and grant necessary permissions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;\n\nCREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';\n\nGRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Defining a Kill Task in Apache Druid\nDESCRIPTION: JSON structure for creating a Kill Task in Apache Druid that permanently deletes disabled segments. The task requires specifying the task ID, data source, and interval for which segments should be deleted.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"kill\",\n    \"id\": <task_id>,\n    \"dataSource\": <task_datasource>,\n    \"interval\" : <all_segments_in_this_interval_will_die!>,\n    \"context\": <task context>\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Password Request Example for Basic Authentication\nDESCRIPTION: Example JSON body for a password assignment request to the Druid authentication API. Used when setting user credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"password\": \"helloworld\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cached Namespace Lookup with URI\nDESCRIPTION: Example configuration for a globally cached lookup using a URI-based namespace with CSV format and polling period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"cachedNamespace\",\n    \"extractionNamespace\": {\n       \"type\": \"uri\",\n       \"uri\": \"file:/tmp/prefix/\",\n       \"namespaceParseSpec\": {\n         \"format\": \"csv\",\n         \"columns\": [\n           \"key\",\n           \"value\"\n         ]\n       },\n       \"pollPeriod\": \"PT5M\"\n     },\n     \"firstCacheTimeout\": 0\n }\n```\n\n----------------------------------------\n\nTITLE: HTTP Endpoint for Viewing Coordinator Configuration History in Druid\nDESCRIPTION: HTTP endpoint to retrieve the audit history of Coordinator dynamic configuration changes for a specified time interval. The default interval can be configured in Coordinator runtime properties.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_30\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?interval=<interval>\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction Function without Length\nDESCRIPTION: Configuration for extracting substring of dimension values with only start index specified.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"substring\", \"index\" : 3 }\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Multiple Hadoop Client Versions\nDESCRIPTION: Example directory structure showing how to organize multiple versions of Hadoop client libraries under the hadoop-dependencies directory for use with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhadoop-dependencies/\n hadoop-client\n     2.3.0\n        activation-1.1.jar\n        avro-1.7.4.jar\n        commons-beanutils-1.7.0.jar\n        commons-beanutils-core-1.8.0.jar\n        commons-cli-1.2.jar\n        commons-codec-1.4.jar\n    ..... lots of jars\n     2.4.0\n         activation-1.1.jar\n         avro-1.7.4.jar\n         commons-beanutils-1.7.0.jar\n         commons-beanutils-core-1.8.0.jar\n         commons-cli-1.2.jar\n         commons-codec-1.4.jar\n    ..... lots of jars\n```\n\n----------------------------------------\n\nTITLE: Accessing the Overlord Console URL\nDESCRIPTION: Shows the URL pattern for accessing the Overlord console which is used to view tasks, workers, and worker status. It's accessed through the console.html path on the Overlord process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/management-uis.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<OVERLORD_IP>:<OVERLORD_PORT>/console.html\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kafka Indexing Tasks in Druid\nDESCRIPTION: Formula for calculating the minimum required worker capacity to support both reading and publishing tasks in Druid's Kafka indexing service. This ensures sufficient capacity for tasks to run concurrently without queueing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Building Druid with Maven\nDESCRIPTION: Basic Maven command to build Druid from source. This command runs static analysis, unit tests, compiles classes, and packages projects into JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/build.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: Submitting Daily Compaction Task in Druid\nDESCRIPTION: This command submits the compaction task to reduce the number of segments and change the granularity to daily.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json\n```\n\n----------------------------------------\n\nTITLE: Implementing Insensitive Contains Search in Druid\nDESCRIPTION: Defines a case-insensitive search query that matches if any part of a dimension value contains the specified search value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"insensitive_contains\",\n  \"value\" : \"some_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Time Functions in Apache Druid SQL\nDESCRIPTION: Examples of time-related functions in Druid SQL, including current timestamp, date truncation, time shifting, and parsing. These functions can be used with the __time column or any column storing timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/sql.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCURRENT_TIMESTAMP\nCURRENT_DATE\nDATE_TRUNC('day', __time)\nTIME_FLOOR(__time, 'P3M')\nTIME_SHIFT(__time, 'P1D', 1, 'America/Los_Angeles')\nTIME_EXTRACT(__time, 'HOUR', 'America/Los_Angeles')\nTIME_PARSE('2000-01-02T03:04:05Z')\nTIME_FORMAT(__time, 'yyyy-MM-dd')\nMILLIS_TO_TIMESTAMP(millis_expr)\nTIMESTAMP_TO_MILLIS(__time)\nEXTRACT(HOUR FROM __time)\nFLOOR(__time TO DAY)\nCEIL(__time TO MONTH)\nTIMESTAMPADD(HOUR, 2, __time)\n__time + INTERVAL '2' HOUR\n```\n\n----------------------------------------\n\nTITLE: Individual Segment Identifier Path\nDESCRIPTION: Ephemeral ZooKeeper path structure for individual segment identifiers served by processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_\n```\n\n----------------------------------------\n\nTITLE: Implementing stringFirst Aggregator in Druid Queries\nDESCRIPTION: The stringFirst aggregator computes the string metric value with the minimum timestamp or null if no row exists. Includes optional maxStringBytes parameter to limit string size and filterNullValues to exclude null values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Polling Off-heap Lookup Cache in JSON\nDESCRIPTION: Example configuration for an off-heap lookup that will be cached once and never swapped (pollPeriod is null). This setup uses a JDBC data fetcher to retrieve key-value pairs from a MySQL database table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\":\"pollingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"cacheFactory\":{\"type\":\"offHeapPolling\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with OR Logical Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Shows how to use an 'OR' logical filter in a HavingSpec for a groupBy query. This filter combines multiple conditions where at least one must be true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: NULL Selector Filter for Multi-value Dimensions in Druid\nDESCRIPTION: Example of a selector filter that matches rows where the 'tags' dimension contains null values. This would match row4 from the dataset which has an empty array.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"tags\",\n  \"value\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Dimension Selector Filter Having Specification\nDESCRIPTION: Example of using dimension selector filter to match rows with specific dimension values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/having.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n       {\n            \"type\": \"dimSelector\",\n            \"dimension\": \"<dimension>\",\n            \"value\": <dimension_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Numeric Column Filter in Druid\nDESCRIPTION: Shows how to filter on numeric columns using string representation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"myFloatColumn\",\n  \"value\": \"10.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Storage in Druid\nDESCRIPTION: Properties for configuring metadata storage in Druid. These settings specify the JDBC connection and other configurations for the metadata storage used by Coordinator, Overlord, and Realtime processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=none\ndruid.metadata.storage.connector.user=none\ndruid.metadata.storage.connector.password=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Consistent Hash Balancer for Avatica Queries\nDESCRIPTION: Property configuration to set Consistent Hashing as the balancing algorithm for Avatica JDBC requests in Druid Router.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.avatica.balancer.type=consistentHash\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Prioritization in Apache Druid Broker\nDESCRIPTION: These properties control how the Broker balances connections to Historical processes and selects segments across tiers in a cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_42\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.balancer.type=random\ndruid.broker.select.tier=highestPriority\ndruid.broker.select.tier.custom.priorities=None\n```\n\n----------------------------------------\n\nTITLE: MiddleManager Configuration Example\nDESCRIPTION: Example configuration for Druid MiddleManager nodes showing worker capacity and processing settings for task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.worker.capacity=4\ndruid.indexer.fork.property.druid.processing.numMergeBuffers=2\ndruid.indexer.fork.property.druid.processing.buffer.sizeBytes=100000000\ndruid.indexer.fork.property.druid.processing.numThreads=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Password Provider in Apache Druid\nDESCRIPTION: Template for configuring a custom password provider implementation in Apache Druid. This configuration points to a registered password provider with custom properties specific to that implementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/password-provider.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"<registered_password_provider_name>\", \"<jackson_property>\": \"<value>\", ... }\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Process Properties in Druid\nDESCRIPTION: Core configuration properties for Druid Broker processes including host settings, port configuration, and service naming.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_42\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8082\ndruid.tlsPort=8282\ndruid.service=druid/broker\n```\n\n----------------------------------------\n\nTITLE: Cardinality Aggregator for Counting Distinct People\nDESCRIPTION: Example JSON configuration for using the cardinality aggregator with byRow=true to count distinct people based on combinations of first and last names.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/hll-old.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_people\",\n  \"fields\": [ \"first_name\", \"last_name\" ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Daily Compaction Task in Druid\nDESCRIPTION: This command submits the compaction task to reduce the number of segments and change the granularity to daily.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json\n```\n\n----------------------------------------\n\nTITLE: PrefixFiltered DimensionSpec in Druid\nDESCRIPTION: Configuration for filtering multi-value dimensions based on prefix matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"prefixFiltered\", \"delegate\" : <dimensionSpec>, \"prefix\": <prefix string> }\n```\n\n----------------------------------------\n\nTITLE: Setting up Long First Aggregator in Druid\nDESCRIPTION: Computes metric value with minimum timestamp or 0 if no rows exist. For query time use only.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"longFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighting Library License Information\nDESCRIPTION: License information for the Prism syntax highlighting library created by Lea Verou and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighting Library License Information\nDESCRIPTION: License information for the Prism syntax highlighting library created by Lea Verou and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Querying with OR Logical Filter HavingSpec in Apache Druid JSON\nDESCRIPTION: Shows how to use an 'OR' logical filter in a HavingSpec for a groupBy query. This filter combines multiple conditions where at least one must be true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/having.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Response for Successful Data Ingestion\nDESCRIPTION: Example JSON response from Tranquility Server after successful data ingestion, showing the count of events received and sent to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":39244}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Asynchronous Logging in Apache Druid using Log4j2\nDESCRIPTION: This XML configuration for Log4j2 sets up asynchronous logging for specific Druid classes to improve performance. It defines console appenders and configures async loggers for chatty classes while setting the root logger to 'info' level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/performance-faq.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <AsyncLogger name=\"org.apache.druid.curator.inventory.CuratorInventoryManager\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name=\"org.apache.druid.client.BatchServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <!-- Make extra sure nobody adds logs in a bad way that can hurt performance -->\n    <AsyncLogger name=\"org.apache.druid.client.ServerInventoryView\" level=\"debug\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <AsyncLogger name =\"org.apache.druid.java.util.http.client.pool.ChannelResourceFactory\" level=\"info\" additivity=\"false\">\n      <AppenderRef ref=\"Console\"/>\n    </AsyncLogger>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Console Producer for Testing\nDESCRIPTION: Bash command to start a Kafka console producer for testing the rename functionality. Allows sending key-value pairs with the format OLD_VAL->NEW_VAL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-console-producer.sh --property parse.key=true --property key.separator=\"->\" --broker-list localhost:9092 --topic testTopic\n```\n\n----------------------------------------\n\nTITLE: React Scheduler MIT License Declaration\nDESCRIPTION: License declaration for React scheduler.production.min.js by Facebook, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Audit History API Endpoint with Interval\nDESCRIPTION: HTTP GET endpoint for retrieving Coordinator configuration audit history within a specified time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_27\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?interval=<interval>\n```\n\n----------------------------------------\n\nTITLE: Configuring Upper Case Extraction Function with Locale in Druid\nDESCRIPTION: Example of upper case extraction function configuration with specific locale setting for string transformation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"upper\",\n  \"locale\":\"fr\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Druid Extensions Dependencies in Maven POM\nDESCRIPTION: This XML snippet demonstrates how to add various Druid extension dependencies to the Maven POM file. This is part of the process of creating a custom build of Druid with all necessary extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-avro-extensions</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-parquet-extensions</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-hdfs-storage</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>mysql-metadata-storage</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n```\n\n----------------------------------------\n\nTITLE: Accessing Coordinator Console URL\nDESCRIPTION: Demonstrates the URL format for accessing the Coordinator's web console. The Coordinator's IP address and port should be used.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/management-uis.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>\n```\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kafka Indexing Tasks in Druid\nDESCRIPTION: Formula for calculating the minimum worker capacity needed to handle both reading and publishing tasks in a Kafka indexing service deployment. This ensures sufficient capacity for tasks to run concurrently.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Processing Configuration Table in Markdown\nDESCRIPTION: Markdown table defining processing-related configuration properties for the Druid Broker, including buffer sizes, thread counts, and memory settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_46\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.processing.buffer.sizeBytes`|This specifies a buffer size for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.|auto (max 1GB)|\n|`druid.processing.buffer.poolCacheMaxCount`|processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.|Integer.MAX_VALUE|\n|`druid.processing.formatString`|Realtime and Historical processes use this format string to name their processing threads.|processing-%s|\n|`druid.processing.numMergeBuffers`|The number of direct memory buffers available for merging query results. The buffers are sized by `druid.processing.buffer.sizeBytes`. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.|`max(2, druid.processing.numThreads / 4)`|\n|`druid.processing.numThreads`|The number of processing threads to have available for parallel processing of segments. Our rule of thumb is `num_cores - 1`, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value `1`.|Number of cores - 1 (or 1)|\n|`druid.processing.columnCache.sizeBytes`|Maximum size in bytes for the dimension value lookup cache. Any value greater than `0` enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.|`0` (disabled)|\n|`druid.processing.fifo`|If the processing queue should treat tasks of equal priority in a FIFO manner|`false`|\n|`druid.processing.tmpDir`|Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default `java.io.tmpdir` path.|path represented by `java.io.tmpdir`|\n```\n\n----------------------------------------\n\nTITLE: Paginated Select Query with FromNext Parameter\nDESCRIPTION: Demonstrates how to construct a paginated Select query with the fromNext parameter set to false for backwards compatibility mode, requiring manual offset incrementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/select-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for React Scheduler\nDESCRIPTION: License declaration for React's scheduler.production.min.js v0.20.2 created by Facebook under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Get Lookup Response Example in Druid\nDESCRIPTION: The expected JSON response when retrieving a specific lookup configuration via the Coordinator API GET endpoint, showing version and lookup extractor details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/lookups.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Query Example with Aggregation and Post-Aggregation\nDESCRIPTION: Comprehensive example showing momentSketchMerge aggregator combined with quantile and min post-aggregators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\": [{\n    \"type\": \"momentSketchMerge\",\n    \"name\": \"sketch\",\n    \"fieldName\": \"sketch\",\n    \"k\": 10,\n    \"compress\": true\n  }],\n  \"postAggregations\": [\n  {\n    \"type\": \"momentSketchSolveQuantiles\",\n    \"name\": \"quantiles\",\n    \"fractions\": [0.1, 0.5, 0.9],\n    \"field\": {\n      \"type\": \"fieldAccess\",\n      \"fieldName\": \"sketch\"\n    }\n  },\n  {\n    \"type\": \"momentSketchMin\",\n    \"name\": \"min\",\n    \"field\": {\n      \"type\": \"fieldAccess\",\n      \"fieldName\": \"sketch\"\n    }\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Kinesis Supervisor IndexSpec Configuration Object\nDESCRIPTION: JSON configuration object for specifying bitmap, dimension and metric compression settings for Kinesis ingestion in Druid. Defines compression formats and encoding options for different column types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmap\": {\n    \"type\": \"concise\"\n  },\n  \"dimensionCompression\": \"LZ4\",\n  \"metricCompression\": \"LZ4\",\n  \"longEncoding\": \"longs\"\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Context Parameters\nDESCRIPTION: Example JSON payload showing how to include context parameters with a Druid SQL query. Demonstrates setting timezone context for timestamp handling.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/sql.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"context\" : {\n    \"sqlTimeZone\" : \"America/Los_Angeles\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logical Expression Filter Having Specifications\nDESCRIPTION: Examples of logical operators (AND, OR, NOT) for combining multiple having conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/having.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\": \"and\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"lessThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n            \"type\": \"or\",\n            \"havingSpecs\": [        \n                {\n                    \"type\": \"greaterThan\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                },\n                {\n                    \"type\": \"equalTo\",\n                    \"aggregation\": \"<aggregate_metric>\",\n                    \"value\": <numeric_value>\n                }\n            ]\n        }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...,\n    \"having\": \n        {\n        \"type\": \"not\",\n        \"havingSpec\":         \n            {\n                \"type\": \"equalTo\",\n                \"aggregation\": \"<aggregate_metric>\",\n                \"value\": <numeric_value>\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: True Filter in Druid\nDESCRIPTION: Shows the implementation of a true filter that matches all values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"true\" }\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Coordinator Leader Election in Druid\nDESCRIPTION: Specifies the ZooKeeper path used for Coordinator leader election in Druid using the Curator LeadershipLatch recipe.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.coordinatorPath}/_COORDINATOR\n```\n\n----------------------------------------\n\nTITLE: Defining NOT Logical Expression Filter in Apache Druid JSON\nDESCRIPTION: Illustrates the JSON structure for a NOT logical expression filter in Apache Druid. This filter negates another filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n\"filter\": { \"type\": \"not\", \"field\": <filter> }\n```\n\n----------------------------------------\n\nTITLE: Coordinator Process Configuration\nDESCRIPTION: Core configuration properties for the Coordinator process including networking and service identification settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_23\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=InetAddress.getLocalHost().getCanonicalHostName()\ndruid.bindOnHost=false\ndruid.plaintextPort=8081\ndruid.tlsPort=8281\ndruid.service=druid/coordinator\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Zookeeper in Bash\nDESCRIPTION: Commands to download Apache Zookeeper, extract it, and rename the directory for use with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/index.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\nmv zookeeper-3.4.11 zk\n```\n\n----------------------------------------\n\nTITLE: Tranquility Server Response for Pending Data Ingestion\nDESCRIPTION: Example JSON response from Tranquility Server when events have been received but not yet sent to Druid, indicating that Druid resources are still being allocated.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":0}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Graphite Event Converter in Druid\nDESCRIPTION: JSON configuration for the 'all' event converter that sends all Druid service metrics events to Graphite. This configuration ignores hostname and service name in the metric path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.graphite.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true}\n```\n\n----------------------------------------\n\nTITLE: HTTP Endpoint for Coordinator Configuration\nDESCRIPTION: The endpoint URL pattern for submitting Coordinator configuration via HTTP POST request.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config\n```\n\n----------------------------------------\n\nTITLE: Running Count Query in Druid SQL\nDESCRIPTION: SQL query to count total rows in the compaction-tutorial datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) from \"compaction-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Drop Rule in Druid\nDESCRIPTION: An Interval Drop Rule configuration that instructs Druid to drop segments that fall within the specified time interval. This rule is used for time-based data retention management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Supervisor IDs GET Endpoint\nDESCRIPTION: REST endpoint to retrieve a list of active supervisor IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_13\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Sample Wikipedia Edit Event\nDESCRIPTION: Example JSON structure of a Wikipedia page edit event from the tutorial dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/index.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0\n}\n```\n\n----------------------------------------\n\nTITLE: Filtered GroupBy Query for Multi-value Dimensions\nDESCRIPTION: Example of a GroupBy query with a selector filter on a multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Filtered GroupBy Query for Multi-value Dimensions\nDESCRIPTION: Example of a GroupBy query with a selector filter on a multi-value dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"default\",\n      \"dimension\": \"tags\",\n      \"outputName\": \"tags\"\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Deep Storage Properties\nDESCRIPTION: Configuration properties for setting up HDFS and Cassandra deep storage. Requires respective storage extensions to be loaded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_20\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.storageDirectory=<hdfs-dir>\ndruid.storage.host=<cassandra-host>\ndruid.storage.keyspace=<cassandra-keyspace>\n```\n\n----------------------------------------\n\nTITLE: Testing Kafka Lookups Using Console Producer\nDESCRIPTION: Bash command for testing Kafka rename functionality using the Kafka console producer. Allows sending key-value pairs to a Kafka stream with a specified separator for testing lookup transformations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kafka-extraction-namespace.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-console-producer.sh --property parse.key=true --property key.separator=\"->\" --broker-list localhost:9092 --topic testTopic\n```\n\n----------------------------------------\n\nTITLE: Accessing the Druid Console URL Format\nDESCRIPTION: The URL format used to access the Druid Console after enabling Druid SQL on Brokers and deploying a Router with management proxy enabled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/druid-console.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Quantiles Sketch Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for post-aggregator to get quantiles sketch from a column in ArrayOfDoublesSketch\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToQuantilesSketch\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"column\" : <number>,\n  \"k\" : <parameter that determines the accuracy and size of the quantiles sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Document Layout Configuration\nDESCRIPTION: YAML front matter configuration for the Druid FAQ documentation page, specifying the layout type and title.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/faq.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) FAQ\"\n---\n```\n\n----------------------------------------\n\nTITLE: Worker History Count API Endpoint\nDESCRIPTION: HTTP endpoint for retrieving last N entries of worker configuration audit history.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_33\n\nLANGUAGE: text\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Drop Rule in Druid\nDESCRIPTION: An Interval Drop Rule configuration that instructs Druid to drop segments that fall within the specified time interval. This rule is used for time-based data retention management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Jackson Dependency Conflict Error in Java\nDESCRIPTION: This code snippet shows the Java VerifyError that occurs due to Jackson library version conflicts between Druid and CDH during a MapReduce job execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/other-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\njava.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid Console URL\nDESCRIPTION: Shows the URL format for accessing the Druid Console via the Router process. The Router's IP address and port should be used.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/management-uis.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Indexer Paths in Apache Druid\nDESCRIPTION: These properties configure Zookeeper paths specific to the Druid indexing service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\ndruid.zk.paths.indexer.base=${druid.zk.paths.base}/indexer\ndruid.zk.paths.indexer.announcementsPath=${druid.zk.paths.indexer.base}/announcements\ndruid.zk.paths.indexer.tasksPath=${druid.zk.paths.indexer.base}/tasks\ndruid.zk.paths.indexer.statusPath=${druid.zk.paths.indexer.base}/status\n```\n\n----------------------------------------\n\nTITLE: License Notice for React use-sync-external-store-shim\nDESCRIPTION: This snippet contains the license information for React's use-sync-external-store-shim production build, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Duration Granularity Configuration\nDESCRIPTION: Examples of duration granularity specifications in milliseconds, with optional origin time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 7200000}\n\n{\"type\": \"duration\", \"duration\": 3600000, \"origin\": \"2012-01-01T00:30:00Z\"}\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool to Clean All Druid Cluster State\nDESCRIPTION: Command to run the ResetCluster tool with the --all flag to clean up all persisted state from metadata and deep storage in a Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster --all\n```\n\n----------------------------------------\n\nTITLE: Means Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for post-aggregator to get mean values for each column from ArrayOfDoublesSketch\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToMeans\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Pull-deps Command Example\nDESCRIPTION: Example command for downloading Druid extensions (mysql-metadata-storage and druid-rabbitmq) and multiple Hadoop client versions using the pull-deps tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/pull-deps.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.13.0-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.13.0-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Field Accessor Post-Aggregator with finalizingFieldAccess Type\nDESCRIPTION: JSON structure for a field accessor post-aggregator that returns a finalized value from a specified aggregator, useful for complex aggregators like 'cardinality' and 'hyperUnique'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"finalizingFieldAccess\", \"name\": <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Greater Than Filter in Druid\nDESCRIPTION: Shows implementation of a greaterThan filter in the Having clause, equivalent to SQL HAVING <aggregate> > <value>.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/having.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n            \"type\": \"greaterThan\",\n            \"aggregation\": \"<aggregate_metric>\",\n            \"value\": <numeric_value>\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cassandra Deep Storage Properties\nDESCRIPTION: Configuration properties for connecting Druid to Cassandra deep storage. Requires the druid-cassandra-storage extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.host=none\ndruid.storage.keyspace=none\n```\n\n----------------------------------------\n\nTITLE: Configuration Example for Hadoop Job Properties\nDESCRIPTION: JSON configuration example showing how to exclude specific validation classes from system classpath while including core Hadoop classes when using classloader isolation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\": {\n  \"mapreduce.job.classloader\": \"true\",\n  \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Discovery Path in Apache Druid\nDESCRIPTION: Sets the Zookeeper path for service discovery, which is not affected by the base path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\ndruid.discovery.curator.path=/druid/discovery\n```\n\n----------------------------------------\n\nTITLE: Setting Up HDFS Directories for Druid Data Ingestion\nDESCRIPTION: Commands to create necessary HDFS directories and copy input data to HDFS for Druid ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/hadoop/bin\n./hadoop fs -mkdir /druid\n./hadoop fs -mkdir /druid/segments\n./hadoop fs -mkdir /quickstart\n./hadoop fs -chmod 777 /druid\n./hadoop fs -chmod 777 /druid/segments\n./hadoop fs -chmod 777 /quickstart\n./hadoop fs -chmod -R 777 /tmp\n./hadoop fs -chmod -R 777 /user\n./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Submitting Ingestion Task for Wikipedia Edits Data in Druid\nDESCRIPTION: This command submits an ingestion task specification to create a datasource named 'retention-tutorial' with hourly segments from Wikipedia edits sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-retention.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/retention-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authorizer in Druid\nDESCRIPTION: Configuration properties for setting up a basic authorizer\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authorizers=[\"MyBasicAuthorizer\"]\n\ndruid.auth.authorizer.MyBasicAuthorizer.type=basic\n```\n\n----------------------------------------\n\nTITLE: Configuring QuantilesDoublesSketchToHistogram Post Aggregator\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToHistogram post aggregator, which returns an approximation to the histogram given an array of split points that define the histogram bins.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToHistogram\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"splitPoints\" : <array of split points>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LongMin Aggregator in Apache Druid\nDESCRIPTION: Illustrates how to set up a longMin aggregator in Druid. This aggregator computes the minimum of all metric values and Long.MAX_VALUE.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"longMin\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Implementing Escalator Methods for Internal System User in Apache Druid\nDESCRIPTION: Java interface methods that must be implemented by an Escalator to handle internal system user authentication for communication between Druid processes. The methods create HTTP clients with escalated credentials and authentication results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/auth.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic HttpClient createEscalatedClient(HttpClient baseClient);\n\npublic org.eclipse.jetty.client.HttpClient createEscalatedJettyClient(org.eclipse.jetty.client.HttpClient baseClient);\n\npublic AuthenticationResult createEscalatedAuthenticationResult();\n```\n\n----------------------------------------\n\nTITLE: SegmentWriteOutMediumFactory Configuration\nDESCRIPTION: Configuration for specifying the segment write-out medium type used when creating segments during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"\" \n}\n```\n\n----------------------------------------\n\nTITLE: SegmentWriteOutMediumFactory Configuration\nDESCRIPTION: Configuration for specifying the segment write-out medium type used when creating segments during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"\" \n}\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: MIT license declaration for react-is.production.min.js\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Extracting Druid Distribution Files\nDESCRIPTION: Commands to extract the downloaded Druid distribution archive and navigate to its directory. This is typically done on a single machine before distributing to cluster nodes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzf apache-druid-0.14.1-incubating-bin.tar.gz\ncd apache-druid-0.14.1-incubating\n```\n\n----------------------------------------\n\nTITLE: Running pull-deps Tool in Java for Apache Druid\nDESCRIPTION: Example command to run the pull-deps tool for downloading specific Druid extensions and Hadoop dependencies. It demonstrates using the --clean option and specifying multiple coordinates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/pull-deps.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:0.15.1-incubating -c org.apache.druid.extensions.contrib:druid-rabbitmq:0.15.1-incubating -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Displaying Raw Data Example in Markdown\nDESCRIPTION: Shows a markdown table representing raw data with timestamp, source IP, destination IP, packet count, and byte count. This example is used to demonstrate Druid's rollup functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Peon Configuration Property Prefix in Druid\nDESCRIPTION: The property prefix pattern used to explicitly set child peon configurations in the MiddleManager component. Properties with this prefix will be passed down to peons.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_35\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.fork.property\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Drop Rule in Druid\nDESCRIPTION: A Forever Drop Rule configuration that instructs Druid to permanently drop segments matching this rule from the cluster. This rule is used for data retention management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropForever\"  \n}\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Network Flow Event Data in JSON Format\nDESCRIPTION: Example JSON records representing network flow events with timestamp, source IP, destination IP, packet count, and byte count fields. This sample data will be used to demonstrate Druid's roll-up functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2018-01-01T01:01:35Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":20,\n  \"bytes\":9024\n}\n{\n  \"timestamp\":\"2018-01-01T01:01:51Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":255,\n  \"bytes\":21133\n}\n{\n  \"timestamp\":\"2018-01-01T01:01:59Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":11,\n  \"bytes\":5780\n}\n{\n  \"timestamp\":\"2018-01-01T01:02:14Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":38,\n  \"bytes\":6289\n}\n{\n  \"timestamp\":\"2018-01-01T01:02:29Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":377,\n  \"bytes\":359971\n}\n{\n  \"timestamp\":\"2018-01-01T01:03:29Z\",\n  \"srcIP\":\"1.1.1.1\", \n  \"dstIP\":\"2.2.2.2\",\n  \"packets\":49,\n  \"bytes\":10204\n}\n{\n  \"timestamp\":\"2018-01-02T21:33:14Z\",\n  \"srcIP\":\"7.7.7.7\", \n  \"dstIP\":\"8.8.8.8\",\n  \"packets\":38,\n  \"bytes\":6289\n}\n{\n  \"timestamp\":\"2018-01-02T21:33:45Z\",\n  \"srcIP\":\"7.7.7.7\", \n  \"dstIP\":\"8.8.8.8\",\n  \"packets\":123,\n  \"bytes\":93999\n}\n{\n  \"timestamp\":\"2018-01-02T21:35:45Z\",\n  \"srcIP\":\"7.7.7.7\", \n  \"dstIP\":\"8.8.8.8\",\n  \"packets\":12,\n  \"bytes\":2818\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Druid Console via Router Process\nDESCRIPTION: URL pattern for accessing the Druid Console through the Router process. The Router process must have the management proxy enabled, and Brokers must have Druid SQL enabled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/management-uis.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: Submitting Tasks and Supervisor Specs in Druid Overlord API\nDESCRIPTION: POST request to submit tasks and supervisor specs to the Druid Overlord, returning the taskId of the submitted task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_3\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Greatest Post-Aggregator in Druid\nDESCRIPTION: JSON structure for a 'doubleGreatest' post-aggregator that computes the maximum value of multiple columns in one row, returning the highest value among the specified fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"doubleGreatest\",\n  \"name\"  : <output_name>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...]\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Duration Granularity in Apache Druid\nDESCRIPTION: Examples of specifying duration granularity in Apache Druid queries. Shows how to set a custom duration in milliseconds and optionally specify an origin timestamp.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 7200000}\n\n{\"type\": \"duration\", \"duration\": 3600000, \"origin\": \"2012-01-01T00:30:00Z\"}\n```\n\n----------------------------------------\n\nTITLE: Using Kinit for Kerberos Authentication in Druid\nDESCRIPTION: Command for obtaining Kerberos credentials using kinit before accessing Druid HTTP endpoints. This authenticates the user with the Kerberos KDC using a keytab file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkinit -k -t <path_to_keytab_file> user@REALM.COM\n```\n\n----------------------------------------\n\nTITLE: Example Node Identification Configuration in Apache Druid\nDESCRIPTION: Configuration properties for node identification in Apache Druid, including host address, ports, and service name. These settings are used for node discovery and metrics reporting.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.host: InetAddress.getLocalHost().getCanonicalHostName()\ndruid.plaintextPort: 8084\ndruid.tlsPort: 8284\ndruid.service: druid/realtime\n```\n\n----------------------------------------\n\nTITLE: Creating Quantiles Sketch from ArrayOfDoublesSketch Column\nDESCRIPTION: Post-aggregator configuration to create a quantiles DoublesSketch from a column of an ArrayOfDoublesSketch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToQuantilesSketch\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"column\" : <number>,\n  \"k\" : <parameter that determines the accuracy and size of the quantiles sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Python Kafka Producer for Protobuf Messages\nDESCRIPTION: Python script that reads JSON data from stdin, converts it to Protobuf format using the generated Python bindings, and publishes the messages to a Kafka topic. Requires 'protobuf' and 'kafka-python' modules.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\nimport json\n\nfrom kafka import KafkaProducer\nfrom metrics_pb2 import Metrics\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ntopic = 'metrics_pb'\nmetrics = Metrics()\n\nfor row in iter(sys.stdin):\n    d = json.loads(row)\n    for k, v in d.items():\n        setattr(metrics, k, v)\n    pb = metrics.SerializeToString()\n    producer.send(topic, pb)\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Medium Configuration in Bash\nDESCRIPTION: Command to start Druid in the medium configuration, which is designed for machines with 16 CPU cores and 128GB RAM, roughly equivalent to an Amazon i3.4xlarge instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/single-server.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-medium\n```\n\n----------------------------------------\n\nTITLE: Defining Operator Precedence Table in Markdown\nDESCRIPTION: This code snippet creates a markdown table that lists the supported operators in Druid's expression language, ordered by decreasing precedence. It includes unary, binary, and logical operators.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/misc/math-expr.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Operators|Description|\n|---------|-----------|\n|!, -|Unary NOT and Minus|\n|^|Binary power op|\n|*, /, %|Binary multiplicative|\n|+, -|Binary additive|\n|<, <=, >, >=, ==, !=|Binary Comparison|\n|&&,\\|\\||Binary Logical AND, OR|\n```\n\n----------------------------------------\n\nTITLE: NOT Logical Expression Having Filter in Druid\nDESCRIPTION: Example of using a NOT logical expression to negate a Having filter condition.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/having.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"groupBy\",\n    \"dataSource\": \"sample_datasource\",\n    ...\n    \"having\": \n        {\n        \"type\": \"not\",\n        \"havingSpec\":         \n            {\n                \"type\": \"equalTo\",\n                \"aggregation\": \"<aggregate_metric>\",\n                \"value\": <numeric_value>\n            }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Task Status in Druid Overlord API\nDESCRIPTION: POST request to retrieve a list of task status objects for a given list of task id strings in the request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_4\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/indexer/v1/taskStatus\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Discovery Properties\nDESCRIPTION: Configuration properties for segment discovery, including the discovery method, watched tiers, and datasources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_43\n\nLANGUAGE: properties\nCODE:\n```\ndruid.serverview.type=batch\ndruid.broker.segment.watchedTiers=none\ndruid.broker.segment.watchedDataSources=none\ndruid.broker.segment.awaitInitializationOnStart=true\n```\n\n----------------------------------------\n\nTITLE: Memory Mapping Druid Segments for Querying in Java\nDESCRIPTION: IndexIO.java is responsible for memory mapping Druid segments, making them available for querying. This class is key to understanding how Druid efficiently accesses stored data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/overview.md#2025-04-09_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nIndexIO.java\n```\n\n----------------------------------------\n\nTITLE: Setting Up Graphite as Druid Emitter\nDESCRIPTION: Basic configuration to enable Graphite as the metrics emitter in Druid. Additional configuration details would be required from the Graphite extension documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=graphite\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid for HDFS Storage\nDESCRIPTION: Updates to Druid's configuration file to enable HDFS for deep storage and indexing service logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Server for Apache Druid\nDESCRIPTION: Command to start Tranquility Server with a specified configuration file. This allows sending data to Druid without developing a JVM app.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-push.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility server -configFile <path_to_config_file>/server.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Retrieval in Markdown\nDESCRIPTION: A markdown table listing configuration properties for metadata retrieval, including polling durations and rule management settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_27\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.manager.config.pollDuration`|How often the manager polls the config table for updates.|PT1M|\n|`druid.manager.segments.pollDuration`|The duration between polls the Coordinator does for updates to the set of active segments. Generally defines the amount of lag time it can take for the Coordinator to notice new segments.|PT1M|\n|`druid.manager.rules.pollDuration`|The duration between polls the Coordinator does for updates to the set of active rules. Generally defines the amount of lag time it can take for the Coordinator to notice rules.|PT1M|\n|`druid.manager.rules.defaultTier`|The default tier from which default rules will be loaded from.|_default|\n|`druid.manager.rules.alertThreshold`|The duration after a failed poll upon which an alert should be emitted.|PT10M|\n```\n\n----------------------------------------\n\nTITLE: ToString Post-Aggregator Configuration\nDESCRIPTION: JSON configuration for post-aggregator to get human-readable summary of ArrayOfDoublesSketch\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to an ArrayOfDoublesSketch (fieldAccess or another post aggregator)>\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data with Batch Index Task in Druid\nDESCRIPTION: Example of submitting a batch index task using the post-index-task script to load initial data into a datasource named 'updates-tutorial'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Period Granularity Configuration\nDESCRIPTION: Examples of period granularity specifications using ISO8601 format with timezone and origin options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"period\", \"period\": \"P2D\", \"timeZone\": \"America/Los_Angeles\"}\n\n{\"type\": \"period\", \"period\": \"P3M\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"2012-02-01T00:00:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: Accessing Legacy Coordinator Console URL\nDESCRIPTION: Shows the URL format for accessing the older version of the Coordinator's console, which is maintained for backwards compatibility.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/management-uis.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Rendezvous Hash Balancer for Avatica Queries\nDESCRIPTION: Property configuration to set Rendezvous Hashing as the balancing algorithm for Avatica JDBC requests in Druid Router.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.avatica.balancer.type=rendezvousHash\n```\n\n----------------------------------------\n\nTITLE: Configuring Rendezvous Hash Balancer for Avatica Queries\nDESCRIPTION: Property configuration to set Rendezvous Hashing as the balancing algorithm for Avatica JDBC requests in Druid Router.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.avatica.balancer.type=rendezvousHash\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data in Druid\nDESCRIPTION: Command to load Wikipedia edits data into Druid using a predefined indexing specification file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/deletion-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DBCP Properties for Metadata Storage in Druid\nDESCRIPTION: Example of setting custom DBCP properties for metadata storage connection pooling in Druid. Shows how to set maximum connection lifetime and default query timeout.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000\ndruid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting ZooKeeper on Master Server\nDESCRIPTION: Downloads, extracts, configures, and starts ZooKeeper on the Master server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\ncd zookeeper-3.4.11\ncp conf/zoo_sample.cfg conf/zoo.cfg\n./bin/zkServer.sh start\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Tranquility for Apache Druid\nDESCRIPTION: Commands to download, extract, and rename the Tranquility distribution package for use with Apache Druid. This is necessary for enabling HTTP push functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz\ntar -xzf tranquility-distribution-0.8.3.tgz\nmv tranquility-distribution-0.8.3 tranquility\n```\n\n----------------------------------------\n\nTITLE: Implementing FloatLast Aggregator in Apache Druid\nDESCRIPTION: Shows the configuration for a floatLast aggregator in Druid. This aggregator computes the metric value with the maximum timestamp or 0 if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"floatLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Config History Count Endpoint\nDESCRIPTION: HTTP GET endpoint for retrieving a specific number of most recent Coordinator configuration change entries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: GET Request for Rules Management\nDESCRIPTION: HTTP GET endpoints for retrieving rules configurations and history for datasources\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/api-reference.md#2025-04-09_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/rules\nGET /druid/coordinator/v1/rules/{dataSourceName}\nGET /druid/coordinator/v1/rules/{dataSourceName}?full\nGET /druid/coordinator/v1/rules/history?interval=<interval>\nGET /druid/coordinator/v1/rules/history?count=<n>\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?interval=<interval>\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Installing Tranquility for Druid Stream Ingestion\nDESCRIPTION: This bash script downloads and installs Tranquility, a tool for push-based stream ingestion in Druid. It retrieves the Tranquility distribution, extracts it, and renames the folder.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.3.tgz -o tranquility-distribution-0.8.3.tgz\ntar -xzf tranquility-distribution-0.8.3.tgz\nmv tranquility-distribution-0.8.3 tranquility\n```\n\n----------------------------------------\n\nTITLE: Launching Druid Small Single Server Deployment\nDESCRIPTION: Command to start Druid in small configuration, designed for machines with 8 CPU and 64GB RAM (equivalent to an i3.2xlarge instance).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/single-server.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/start-small\n```\n\n----------------------------------------\n\nTITLE: Runtime Properties for Druid Router in Production\nDESCRIPTION: Example runtime.properties configuration for the Druid Router process in a production environment, including service definition, tier mapping, and connection settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/router.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=#{IP_ADDR}:8080\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.defaultBrokerServiceName=druid:broker-cold\ndruid.router.coordinatorServiceName=druid:coordinator\ndruid.router.tierToBrokerMap={\"hot\":\"druid:broker-hot\",\"_default_tier\":\"druid:broker-cold\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the Router proxy http client\ndruid.router.http.numMaxThreads=100\n\ndruid.server.http.numThreads=100\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Sharding in Druid TuningConfig\nDESCRIPTION: Example of configuring linear sharding strategy in Druid's TuningConfig. Linear sharding allows flexible partition numbering and querying of all unique shards.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-pull.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"linear\",\n    \"partitionNum\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Using Extraction Filter in Druid Queries (Deprecated)\nDESCRIPTION: The extraction filter matches dimension values after applying an extraction function. This filter is deprecated in favor of the selector filter with an extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Non-Injective Lookup Configuration in Druid\nDESCRIPTION: An example of a non-injective lookup where multiple keys map to the same value, which impacts how Druid executes aggregation queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/lookups.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n1 -> Foo\n2 -> Bar\n3 -> Bar\n```\n\n----------------------------------------\n\nTITLE: Coordinating Historical Processes in Druid using Java\nDESCRIPTION: DruidCoordinator.java is the starting point for understanding the coordination logic for Historical processes in Druid. This class manages the distribution and balancing of data across the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/overview.md#2025-04-09_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nDruidCoordinator.java\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema-less Dimension Data Structure in Druid\nDESCRIPTION: Example of data structure showing how to include the same ID twice - once as a dimension with '_dim' suffix and once as a metric with '_met' suffix for schema-less dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"device_id_dim\":123, \"device_id_met\":123}\n```\n\n----------------------------------------\n\nTITLE: Running Tranquility with Thrift Extensions for Apache Druid\nDESCRIPTION: Command line example showing how to run Tranquility with the Druid Thrift extensions. It demonstrates the required parameters for specifying the extensions directory and loading the Thrift extension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka \\\n  -configFile $jsonConfig \\\n  -Ddruid.extensions.directory=/path/to/extensions \\\n  -Ddruid.extensions.loadList='[\"druid-thrift-extensions\"]'\n```\n\n----------------------------------------\n\nTITLE: Setting up String Last Aggregator in Druid\nDESCRIPTION: Computes string metric value with maximum timestamp or null if no rows exist. Includes optional maxStringBytes and filterNullValues parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringLast\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Hour Granularity\nDESCRIPTION: Example of a Druid GroupBy query using hour granularity for time-based aggregation\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/granularities.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":\"hour\",\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"2000-01-01T00:00Z/3000-01-01T00:00Z\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Segment Intervals in HTTP\nDESCRIPTION: This HTTP POST request returns a list of all segments, overlapping with given intervals, for a datasource as stored in the metadata store.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics and Dimensions in Druid Ingestion\nDESCRIPTION: This snippet demonstrates how to specify metrics and dimensions in the dataSchema of a Druid ingestion task. It includes configuration for the parser, dimensionsSpec, and metricsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }   \n    }\n  },\n  \"metricsSpec\" : [\n    { \"type\" : \"count\", \"name\" : \"count\" },\n    { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" },\n    { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" }\n  ],\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Druid Repository in Bash\nDESCRIPTION: Commands to clone the Apache Druid repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:apache/incubator-druid.git\ncd druid\n```\n\n----------------------------------------\n\nTITLE: Configuring Composing Emitter in Druid\nDESCRIPTION: Setup for the composing emitter which allows loading multiple emitter modules simultaneously, specified as a list of emitter types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.composing.emitters=[\"logging\",\"http\"]\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter for Druid Documentation\nDESCRIPTION: YAML front matter defining the page layout and title for the Druid processes documentation page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/processes.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) Processes and Servers\"\n---\n```\n\n----------------------------------------\n\nTITLE: Executing Tranquility with Thrift Extension (Bash)\nDESCRIPTION: This command shows how to run Tranquility with the Thrift extension enabled. It specifies the configuration file, extensions directory, and the Thrift extension to load.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/thrift.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka \\\n  -configFile $jsonConfig \\\n  -Ddruid.extensions.directory=/path/to/extensions \\\n  -Ddruid.extensions.loadList='[\"druid-thrift-extensions\"]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Server HTTP Settings in Markdown\nDESCRIPTION: A markdown table detailing various HTTP server configuration properties for Druid, including thread counts, queue sizes, timeout settings, and request limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.server.http.numThreads`|Number of threads for HTTP requests.|max(10, (Number of cores * 17) / 16 + 2) + 30|\n|`druid.server.http.queueSize`|Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server.|Unbounded|\n|`druid.server.http.maxIdleTime`|The Jetty max idle time for a connection.|PT5M|\n|`druid.server.http.enableRequestLimit`|If enabled, no requests would be queued in jetty queue and \"HTTP 429 Too Many Requests\" error response would be sent. |false|\n|`druid.server.http.defaultQueryTimeout`|Query timeout in millis, beyond which unfinished queries will be cancelled|300000|\n|`druid.server.http.maxScatterGatherBytes`|Maximum number of bytes gathered from data nodes such as historicals and realtime processes to execute a query. Queries that exceed this limit will fail. This is an advance configuration that allows to protect in case broker is under heavy load and not utilizing the data gathered in memory fast enough and leading to OOMs. This limit can be further reduced at query time using `maxScatterGatherBytes` in the context. Note that having large limit is not necessarily bad if broker is never under heavy concurrent load in which case data gathered is processed quickly and freeing up the memory used.|Long.MAX_VALUE|\n|`druid.server.http.gracefulShutdownTimeout`|The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete.|`PT0S` (do not wait)|\n|`druid.server.http.unannouncePropagationDelay`|How long to wait for zookeeper unannouncements to propagate before shutting down Jetty. This is a minimum and `druid.server.http.gracefulShutdownTimeout` does not start counting down until after this period elapses.|`PT0S` (do not wait)|\n|`druid.server.http.maxQueryTimeout`|Maximum allowed value (in milliseconds) for `timeout` parameter. See [query-context](../querying/query-context.html) to know more about `timeout`. Query is rejected if the query context `timeout` is greater than this value. |Long.MAX_VALUE|\n|`druid.server.http.maxRequestHeaderSize`|Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks. |8 * 1024|\n```\n\n----------------------------------------\n\nTITLE: Configuring Concise Bitmap Type in Apache Druid\nDESCRIPTION: This snippet shows the configuration options for Concise bitmaps in Apache Druid's IndexSpec. It only requires specifying the type as 'concise'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|type|String|Must be `concise`.|yes|\n```\n\n----------------------------------------\n\nTITLE: Querying Initial Data from Druid Datasource\nDESCRIPTION: SQL query to view the initial data loaded into the 'updates-tutorial' datasource, showing three rows with animal dimension and number metric.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\"; \n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  tiger         1     100 \n 2018-01-01T03:01:00.000Z  aardvark      1      42 \n 2018-01-01T03:01:00.000Z  giraffe       1   14124 \n\nRetrieved 3 rows in 1.42s.\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: MIT license declaration for react-is.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Micro-Quickstart Configuration\nDESCRIPTION: Command to start all Druid services using the micro-quickstart single-machine configuration. This script launches ZooKeeper and all required Druid processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/start-micro-quickstart\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Micro-Quickstart Configuration\nDESCRIPTION: Command to start all Druid services using the micro-quickstart single-machine configuration. This script launches ZooKeeper and all required Druid processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/start-micro-quickstart\n```\n\n----------------------------------------\n\nTITLE: Implementing Cascade Extraction Function in Druid\nDESCRIPTION: Example of chaining multiple extraction functions including regex, javascript, and substring operations. This function executes extraction functions in array index order to transform dimension values sequentially.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/dimensionspecs.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"cascade\", \n  \"extractionFns\": [\n    { \n      \"type\" : \"regex\", \n      \"expr\" : \"/([^/]+)/\", \n      \"replaceMissingValue\": false,\n      \"replaceMissingValueWith\": null\n    },\n    { \n      \"type\" : \"javascript\", \n      \"function\" : \"function(str) { return \\\"the \\\".concat(str) }\" \n    },\n    { \n      \"type\" : \"substring\", \n      \"index\" : 0, \"length\" : 7 \n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Completion Report JSON Structure in Apache Druid\nDESCRIPTION: Demonstrates the JSON structure of a completion report, including ingestion state, row statistics, and error information for different phases of ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Completion Report JSON Structure in Apache Druid\nDESCRIPTION: Demonstrates the JSON structure of a completion report, including ingestion state, row statistics, and error information for different phases of ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ingestionStatsAndErrors\": {\n    \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\",\n    \"payload\": {\n      \"ingestionState\": \"COMPLETED\",\n      \"unparseableEvents\": {},\n      \"rowStats\": {\n        \"determinePartitions\": {\n          \"processed\": 0,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        },\n        \"buildSegments\": {\n          \"processed\": 5390324,\n          \"processedWithError\": 0,\n          \"thrownAway\": 0,\n          \"unparseable\": 0\n        }\n      },\n      \"errorMsg\": null\n    },\n    \"type\": \"ingestionStatsAndErrors\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Forever Load Rule in Apache Druid\nDESCRIPTION: The Forever Load Rule specifies how many replicas of a segment should exist in different server tiers without any time restrictions. This rule retains data indefinitely in the specified tiers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/rule-configuration.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"loadForever\",  \n  \"tieredReplicants\": {\n    \"hot\": 1,\n    \"_default_tier\" : 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Coordination Services\nDESCRIPTION: Commands to start the Druid coordinator and overlord services on the coordination server\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator\njava `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord\n```\n\n----------------------------------------\n\nTITLE: Segment Disable Request Payload\nDESCRIPTION: JSON payload for disabling specific segments by their segment IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"segmentIds\":\n  [\n    \"deletion-tutorial_2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z_2019-05-01T17:38:46.961Z\",\n    \"deletion-tutorial_2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z_2019-05-01T17:38:46.961Z\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketch Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketch aggregator, which allows creating sketches that associate arrays of double values with unique keys. Parameters include output name, input field, accuracy settings, and metric columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"arrayOfDoublesSketch\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"nominalEntries\": <number>,\n  \"numberOfValues\" : <number>,\n  \"metricColumns\" : <array of strings>\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Emitter in Druid\nDESCRIPTION: Properties for the logging emitter module which controls the class used for logging and the log level at which messages are emitted.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter.logging.loggerClass=LoggingEmitter\ndruid.emitter.logging.logLevel=info\n```\n\n----------------------------------------\n\nTITLE: Updating Compaction Task Capacity in Druid Coordinator API\nDESCRIPTION: POST request to update the capacity for compaction tasks, specifying ratio and max slots for limiting the number of compaction tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/api-reference.md#2025-04-09_snippet_1\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/config/compaction/taskslots?ratio={someRatio}&max={someMaxSlots}\n```\n\n----------------------------------------\n\nTITLE: Syncing Druid Configuration Files\nDESCRIPTION: Command to sync Druid distribution and configuration files to a master server using rsync.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/cluster.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nrsync -az apache-druid-0.15.1-incubating/ MASTER_SERVER:apache-druid-0.15.1-incubating/\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster with All Option in Apache Druid\nDESCRIPTION: Command to run the ResetCluster tool with the --all flag to completely wipe out all persisted state from both metadata and deep storage in one operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/reset-cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster --all\n```\n\n----------------------------------------\n\nTITLE: Router Runtime Properties Configuration\nDESCRIPTION: Runtime configuration properties for the Router including service settings, connection parameters, and thread configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/router.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.host=#{IP_ADDR}:8080\ndruid.plaintextPort=8080\ndruid.service=druid/router\n\ndruid.router.defaultBrokerServiceName=druid:broker-cold\ndruid.router.coordinatorServiceName=druid:coordinator\ndruid.router.tierToBrokerMap={\"hot\":\"druid:broker-hot\",\"_default_tier\":\"druid:broker-cold\"}\ndruid.router.http.numConnections=50\ndruid.router.http.readTimeout=PT5M\n\n# Number of threads used by the Router proxy http client\ndruid.router.http.numMaxThreads=100\n\ndruid.server.http.numThreads=100\n```\n\n----------------------------------------\n\nTITLE: Configuring CombiningFirehose in Apache Druid\nDESCRIPTION: This snippet shows the configuration for a CombiningFirehose, which is used to combine and merge data from multiple firehoses. It specifies a list of delegate firehoses.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/firehose.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"combining\",\n    \"delegates\" : [ { firehose1 }, { firehose2 }, ..... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Search Query Extraction Function Configuration in Druid\nDESCRIPTION: Configuration for extracting values based on search query specification matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"searchQuery\", \"query\" : <search_query_spec> }\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Results with Hour Granularity in Apache Druid\nDESCRIPTION: Example results from a GroupBy query in Apache Druid using 'hour' granularity. Shows how data is aggregated into hourly buckets, with counts for each language.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T01:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T01:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T23:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-03T03:00:00.000Z\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Raw Traffic Data Example\nDESCRIPTION: Example showing raw network traffic data before rollup, with timestamp, source IP, destination IP, packets, and bytes columns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Configuring HyperUnique Metric Specification in Druid\nDESCRIPTION: Configuration for metricsSpec showing how to set up a hyperUnique metric that references the metric version of the ID field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/schema-design.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"hyperUnique\", \"name\" : \"devices\", \"fieldName\" : \"device_id_met\" }\n```\n\n----------------------------------------\n\nTITLE: Multiple Inline Schemas Avro Decoder Configuration\nDESCRIPTION: Configuration for multiple inline schemas-based Avro bytes decoder. Allows different input events to use different schemas identified by schema IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/avro.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"avroBytesDecoder\": {\n    \"type\": \"multiple_schemas_inline\",\n    \"schemas\": {\n      \"1\": {\n        \"namespace\": \"org.apache.druid.data\",\n        \"name\": \"User\",\n        \"type\": \"record\",\n        \"fields\": [\n          { \"name\": \"FullName\", \"type\": \"string\" },\n          { \"name\": \"Country\", \"type\": \"string\" }\n        ]\n      },\n      \"2\": {\n        \"namespace\": \"org.apache.druid.otherdata\",\n        \"name\": \"UserIdentity\",\n        \"type\": \"record\",\n        \"fields\": [\n          { \"name\": \"Name\", \"type\": \"string\" },\n          { \"name\": \"Location\", \"type\": \"string\" }\n        ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Segment Load/Drop Instructions in Druid\nDESCRIPTION: The ZooKeeper path where the Coordinator writes instructions for Historical processes to load or drop segments. The Historical processes monitor this path and take action based on the payload, deleting the znode when complete.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/zookeeper.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier\n```\n\n----------------------------------------\n\nTITLE: Implementing doubleFirst Aggregator in Druid Queries\nDESCRIPTION: The doubleFirst aggregator computes the metric value with the minimum timestamp or 0 if no row exists. This can only be used in queries, not in ingestion specs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"doubleFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>\n}\n```\n\n----------------------------------------\n\nTITLE: Executing GroupBy Query with Pacific Timezone\nDESCRIPTION: A Druid GroupBy query example that uses P1D (1 day) period granularity in America/Los_Angeles timezone. The query counts events grouped by language dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/granularities.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Druid Datasource in HTTP\nDESCRIPTION: This HTTP DELETE request disables a specified datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_6\n\nLANGUAGE: http\nCODE:\n```\nDELETE /druid/coordinator/v1/datasources/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Configuring CombiningFirehose in Apache Druid\nDESCRIPTION: CombiningFirehose combines and merges data from multiple firehoses. This can be used when data needs to be merged from more than one source.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/firehose.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"  :   \"combining\",\n    \"delegates\" : [ { firehose1 }, { firehose2 }, ..... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using pull-deps Tool to Download Community Extensions\nDESCRIPTION: Command to download a community or third-party extension using Druid's pull-deps tool. This example shows how to download a hypothetical extension with Maven coordinates, specifying the extensions and Hadoop dependencies directories.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/including-extensions.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava \\\n  -cp \"lib/*\" \\\n  -Ddruid.extensions.directory=\"extensions\" \\\n  -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\\n  org.apache.druid.cli.Main tools pull-deps \\\n  --no-default-hadoop \\\n  -c \"com.example:druid-example-extension:1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Deep Storage Properties in Druid\nDESCRIPTION: Configuration properties for Druid's deep storage system that handles segment storage. Includes settings for different storage types including local, S3, and HDFS implementations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\ndruid.storage.type=s3\ndruid.storage.bucket=druid-data\ndruid.storage.baseKey=segments\ndruid.s3.accessKey=ACCESS_KEY\ndruid.s3.secretKey=SECRET_KEY\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting ZooKeeper\nDESCRIPTION: Commands to download, extract, configure and start ZooKeeper on the coordination server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/cluster.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz\ntar -xzf zookeeper-3.4.11.tar.gz\ncd zookeeper-3.4.11\ncp conf/zoo_sample.cfg conf/zoo.cfg\n./bin/zkServer.sh start\n```\n\n----------------------------------------\n\nTITLE: JavaScript Worker Selection Strategy in Druid\nDESCRIPTION: Example JavaScript function for custom worker selection logic that routes batch index tasks to specific workers while sending other tasks to available workers based on capacity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_32\n\nLANGUAGE: javascript\nCODE:\n```\n{\n\"type\":\"javascript\",\n\"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"middleManager1_hostname:8091\\\");\\nbatch_workers.add(\\\"middleManager2_hostname:8091\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i < workers.length; i++){\\n sortedWorkers[i] = workers[i];\\n}\\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\\nvar minWorkerVer = config.getMinWorkerVersion();\\nfor (var i = 0; i < sortedWorkers.length; i++) {\\n var worker = sortedWorkers[i];\\n  var zkWorker = zkWorkers.get(worker);\\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\\n      return worker;\\n    } else {\\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\\n        return worker;\\n      }\\n    }\\n  }\\n}\\nreturn null;\\n}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Bitmap Type Configuration - Concise Format\nDESCRIPTION: JSON configuration object for specifying Concise bitmap compression format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"concise\"\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query for Rolled-up Data in Druid\nDESCRIPTION: Running a GroupBy SQL query that aggregates data at query time, showing how duplicate dimensions (bear) are combined when querying even if they exist in separate segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select __time, animal, SUM(\"count\"), SUM(\"number\") from \"updates-tutorial\" group by __time, animal;\n\n __time                    animal    EXPR$2  EXPR$3 \n\n 2018-01-01T01:01:00.000Z  lion           2     400 \n 2018-01-01T03:01:00.000Z  aardvark       1    9999 \n 2018-01-01T04:01:00.000Z  bear           2     333 \n 2018-01-01T05:01:00.000Z  mongoose       1     737 \n 2018-01-01T06:01:00.000Z  snake          1    1234 \n 2018-01-01T07:01:00.000Z  octopus        1     115 \n 2018-01-01T09:01:00.000Z  falcon         1    1241 \n\nRetrieved 7 rows in 0.23s.\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Declaration\nDESCRIPTION: MIT license declaration for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d8507f6b.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Complete Tier-Based Lookup Configuration in Apache Druid\nDESCRIPTION: Example of a complete lookup configuration for a tier named 'realtime_customer2' with a country_code lookup using JDBC source. This demonstrates the full JSON structure required for the lookup API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"realtime_customer2\": {\n    \"country_code\": {\n      \"version\": \"v0\",\n      \"lookupExtractorFactory\": {\n        \"type\": \"cachedNamespace\",\n        \"extractionNamespace\": {\n          \"type\": \"jdbc\",\n          \"connectorConfig\": {\n            \"createTables\": true,\n            \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\",\n            \"user\": \"druid\",\n            \"password\": \"diurd\"\n          },\n          \"table\": \"lookupValues\",\n          \"keyColumn\": \"value_id\",\n          \"valueColumn\": \"value_text\",\n          \"filter\": \"value_type='country'\",\n          \"tsColumn\": \"timeColumn\"\n        },\n        \"firstCacheTimeout\": 120000,\n        \"injective\": true\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing groupBy Query with Daily Period Granularity in Pacific Timezone\nDESCRIPTION: This query demonstrates how to use period granularity with a Pacific timezone in a Druid groupBy query. It groups data by language in daily buckets, counting the occurrences in each bucket. The query specifies a wide interval range and a P1D (1 day) period granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\":\"groupBy\",\n   \"dataSource\":\"my_dataSource\",\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\"},\n   \"dimensions\":[\n      \"language\"\n   ],\n   \"aggregations\":[\n      {\n         \"type\":\"count\",\n         \"name\":\"count\"\n      }\n   ],\n   \"intervals\":[\n      \"1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00\"\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Filter in Apache Druid JSON\nDESCRIPTION: Demonstrates the JSON structure for an extraction filter in Apache Druid. This filter uses an extraction function to transform dimension values before matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Filter in Apache Druid JSON\nDESCRIPTION: Demonstrates the JSON structure for an extraction filter in Apache Druid. This filter uses an extraction function to transform dimension values before matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"filter\": {\n        \"type\": \"extraction\",\n        \"dimension\": \"product\",\n        \"value\": \"bar_1\",\n        \"extractionFn\": {\n            \"type\": \"lookup\",\n            \"lookup\": {\n                \"type\": \"map\",\n                \"map\": {\n                    \"product_1\": \"bar_1\",\n                    \"product_5\": \"bar_1\",\n                    \"product_3\": \"bar_1\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FragmentSearchQuerySpec in Druid\nDESCRIPTION: Defines a search that matches when a dimension value contains all specified fragments. The case_sensitive parameter allows toggling case sensitivity, defaulting to false (case-insensitive).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"fragment\",\n  \"case_sensitive\" : false,\n  \"values\" : [\"fragment1\", \"fragment2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Data with Post Index Task\nDESCRIPTION: Command to submit the initial index task that creates 1-3 segments per hour from Wikipedia edits sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-init-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Broker Service\nDESCRIPTION: Command to start the Druid Broker service on designated servers. Brokers route queries to appropriate data nodes and merge results before returning them to the client.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/broker/jvm.config | xargs` -cp conf/druid/_common:conf/druid/broker:lib/* org.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Displaying Druid Production Architecture Image in Markdown\nDESCRIPTION: This snippet embeds an image showing the Druid production architecture, illustrating how it integrates with streaming technologies. The image is resized to a width of 800 pixels.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/integrating-druid-with-other-technologies.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<img src=\"../../img/druid-production.png\" width=\"800\"/>\n```\n\n----------------------------------------\n\nTITLE: Enabling TLS in Apache Druid\nDESCRIPTION: These properties control the enabling of HTTP and HTTPS connectors in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\ndruid.enablePlaintextPort=true\ndruid.enableTlsPort=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Emitter Properties\nDESCRIPTION: Basic configuration to specify which type of metrics emitter Druid should use. Options include 'noop', 'logging', 'http', 'parametrized', or 'composing' for multiple emitters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=logging\n```\n\n----------------------------------------\n\nTITLE: Executing Kill Task for Permanent Deletion\nDESCRIPTION: cURL command to submit a Kill Task that permanently removes disabled segments from both metadata and deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Hourly Compaction Task Specification\nDESCRIPTION: JSON configuration for compacting segments while maintaining hourly granularity. Specifies datasource, time interval, and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"compact\",\n  \"dataSource\": \"compaction-tutorial\",\n  \"interval\": \"2015-09-12/2015-09-13\",\n  \"tuningConfig\" : {\n    \"type\" : \"index\",\n    \"maxRowsPerSegment\" : 5000000,\n    \"maxRowsInMemory\" : 25000,\n    \"forceExtendableShardSpecs\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Authentication Chain for Kerberos and HTTP Basic in Druid\nDESCRIPTION: Configuration example showing how to enable Kerberos and HTTP Basic authenticators in Druid's authentication chain. This setting determines which authenticators will process incoming requests and in what order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/auth.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"kerberos\", \"basic\"]\n```\n\n----------------------------------------\n\nTITLE: Router JVM Configuration Settings\nDESCRIPTION: JVM settings for production deployment of Druid Router on c3.2xlarge EC2 instance, including memory, GC, and JMX configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/router.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n-server\n-Xmx13g\n-Xms13g\n-XX:NewSize=256m\n-XX:MaxNewSize=256m\n-XX:+UseConcMarkSweepGC\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+UseLargePages\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/mnt/galaxy/deploy/current/\n-Duser.timezone=UTC\n-Dfile.encoding=UTF-8\n-Djava.io.tmpdir=/mnt/tmp\n\n-Dcom.sun.management.jmxremote.port=17071\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n```\n\n----------------------------------------\n\nTITLE: Role Information Response\nDESCRIPTION: JSON response format for retrieving role information and associated permissions\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"druidRole2\",\n  \"permissions\": [\n    {\n      \"resourceAction\": {\n        \"resource\": {\n          \"name\": \"E\",\n          \"type\": \"DATASOURCE\"\n        },\n        \"action\": \"WRITE\"\n      },\n      \"resourceNamePattern\": \"E\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Data Server Processes\nDESCRIPTION: Commands to start the Historical and MiddleManager processes on Data servers\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical\njava `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Defining General Functions Table in Markdown\nDESCRIPTION: This markdown table lists and describes the general-purpose functions available in Druid's expression language, including cast, if, nvl, like, and case functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/misc/math-expr.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|name|description|\n|----|-----------|\n|cast|cast(expr,'LONG' or 'DOUBLE' or 'STRING') returns expr with specified type. exception can be thrown |\n|if|if(predicate,then,else) returns 'then' if 'predicate' evaluates to a positive number, otherwise it returns 'else' |\n|nvl|nvl(expr,expr-for-null) returns 'expr-for-null' if 'expr' is null (or empty string for string type) |\n|like|like(expr, pattern[, escape]) is equivalent to SQL `expr LIKE pattern`|\n|case_searched|case_searched(expr1, result1, \\[\\[expr2, result2, ...\\], else-result\\])|\n|case_simple|case_simple(expr, value1, result1, \\[\\[value2, result2, ...\\], else-result\\])|\n```\n\n----------------------------------------\n\nTITLE: Implementing StringFirst Aggregator in Apache Druid\nDESCRIPTION: Shows how to configure a stringFirst aggregator in Druid. This aggregator computes the metric value with the minimum timestamp or null if no row exists.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/aggregations.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"stringFirst\",\n  \"name\" : <output_name>,\n  \"fieldName\" : <metric_name>,\n  \"maxStringBytes\" : <integer> # (optional, defaults to 1024),\n  \"filterNullValues\" : <boolean> # (optional, defaults to false)\n}\n```\n\n----------------------------------------\n\nTITLE: Ingesting Data with Millisecond Granularity in Apache Druid\nDESCRIPTION: Example of data ingested into Apache Druid with millisecond granularity. This shows the format of individual data points with timestamp, page, and language fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"AAA\", \"language\" : \"en\"\n}\n{\n\"timestamp\": \"2013-09-01T01:02:33Z\", \"page\": \"BBB\", \"language\" : \"en\"\n}\n{\n\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"\n}\n{\n\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Sharding in Apache Druid TuningConfig\nDESCRIPTION: JSON configuration for setting up linear sharding in the tuningConfig. This strategy allows for easy scaling and querying of non-sequential partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/stream-pull.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    \"shardSpec\": {\n        \"type\": \"linear\",\n        \"partitionNum\": 0\n    }\n```\n\n----------------------------------------\n\nTITLE: Router TimeBoundary Strategy Configuration\nDESCRIPTION: JSON configuration for the timeBoundary routing strategy that routes timeBoundary queries to high priority brokers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"timeBoundary\"\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Recent Druid Worker Configuration Changes via HTTP GET\nDESCRIPTION: HTTP endpoint for retrieving the most recent N entries from the worker configuration audit history. This provides a way to see the most recent configuration changes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_23\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD_IP>:<port>/druid/indexer/v1/worker/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Coordinator Leader Election in Druid\nDESCRIPTION: This snippet shows the ZooKeeper path used for Coordinator leader election in Druid. It uses the Curator LeadershipLatch recipe at this specific path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.coordinatorPath}/_COORDINATOR\n```\n\n----------------------------------------\n\nTITLE: Listing Active Tasks in JSON (MiddleManager API)\nDESCRIPTION: This JSON response lists the active tasks being run on a MiddleManager. It returns an array of taskid strings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n[\"index_wikiticker_2019-02-11T02:20:15.316Z\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring SqlFirehose in Apache Druid\nDESCRIPTION: SqlFirehose ingests events residing in RDBMS. Database connection information is provided as part of the ingestion spec. Results from each query are fetched locally and indexed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/firehose.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\" : \"sql\",\n    \"database\": {\n        \"type\": \"mysql\",\n        \"connectorConfig\" : {\n        \"connectURI\" : \"jdbc:mysql://host:port/schema\",\n        \"user\" : \"user\",\n        \"password\" : \"password\"\n        }\n     },\n    \"sqls\" : [\"SELECT * FROM table1\", \"SELECT * FROM table2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Response from groupBy Query with Pacific Timezone Grouping\nDESCRIPTION: This JSON response shows the result of a groupBy query with Pacific timezone granularity. Each result item includes a timestamp (converted to Pacific time), version information, and an event object containing the count and dimension values. Note how records from the same Pacific day are grouped together.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 2,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Cache in Druid\nDESCRIPTION: Configuration properties for using Memcached as a cache backend in Druid, allowing all nodes to share the same cache.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_25\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.expiration=2592000\ndruid.cache.timeout=500\ndruid.cache.hosts=none\ndruid.cache.maxObjectSize=52428800\ndruid.cache.memcachedPrefix=druid\ndruid.cache.numConnections=1\ndruid.cache.protocol=binary\ndruid.cache.locator=consistent\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool with All Options in Druid\nDESCRIPTION: Command to run the ResetCluster tool with the --all flag to reset all components of a Druid cluster at once, including metadata store, segment files, task logs, and Hadoop working paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster --all\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry Policy for Druid Broker (Properties)\nDESCRIPTION: Property for configuring the number of retry attempts for queries in case of transient errors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_32\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.retryPolicy.numTries=1\n```\n\n----------------------------------------\n\nTITLE: Custom Buckets Post-Aggregator Configuration in Druid\nDESCRIPTION: JSON configuration for creating histogram representation with custom break points. Allows manual specification of bin boundaries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"customBuckets\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"breaks\" : [ <value>, <value>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: Sample Worker Configuration JSON Specification for Druid Overlord\nDESCRIPTION: A comprehensive JSON configuration for Druid workers, including selection strategy with affinity settings and EC2 autoscaling configuration. This spec controls how tasks are assigned to MiddleManagers and how worker instances are scaled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"selectStrategy\": {\n    \"type\": \"fillCapacity\",\n    \"affinityConfig\": {\n      \"affinity\": {\n        \"datasource1\": [\"host1:port\", \"host2:port\"],\n        \"datasource2\": [\"host3:port\"]\n      }\n    }\n  },\n  \"autoScaler\": {\n    \"type\": \"ec2\",\n    \"minNumWorkers\": 2,\n    \"maxNumWorkers\": 12,\n    \"envConfig\": {\n      \"availabilityZone\": \"us-east-1a\",\n      \"nodeData\": {\n        \"amiId\": \"${AMI}\",\n        \"instanceType\": \"c3.8xlarge\",\n        \"minInstances\": 1,\n        \"maxInstances\": 1,\n        \"securityGroupIds\": [\"${IDs}\"],\n        \"keyName\": \"${KEY_NAME}\"\n      },\n      \"userData\": {\n        \"impl\": \"string\",\n        \"data\": \"${SCRIPT_COMMAND}\",\n        \"versionReplacementString\": \":VERSION:\",\n        \"version\": null\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Druid segments in deep storage\nDESCRIPTION: Command to list all segment directories in the local deep storage location for the 'deletion-tutorial' datasource, showing the 24 hourly segments created during initial data loading.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -l1 var/druid/segments/deletion-tutorial/\n2015-09-12T00:00:00.000Z_2015-09-12T01:00:00.000Z\n2015-09-12T01:00:00.000Z_2015-09-12T02:00:00.000Z\n2015-09-12T02:00:00.000Z_2015-09-12T03:00:00.000Z\n2015-09-12T03:00:00.000Z_2015-09-12T04:00:00.000Z\n2015-09-12T04:00:00.000Z_2015-09-12T05:00:00.000Z\n2015-09-12T05:00:00.000Z_2015-09-12T06:00:00.000Z\n2015-09-12T06:00:00.000Z_2015-09-12T07:00:00.000Z\n2015-09-12T07:00:00.000Z_2015-09-12T08:00:00.000Z\n2015-09-12T08:00:00.000Z_2015-09-12T09:00:00.000Z\n2015-09-12T09:00:00.000Z_2015-09-12T10:00:00.000Z\n2015-09-12T10:00:00.000Z_2015-09-12T11:00:00.000Z\n2015-09-12T11:00:00.000Z_2015-09-12T12:00:00.000Z\n2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z\n2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z\n2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z\n2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z\n2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z\n2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z\n2015-09-12T18:00:00.000Z_2015-09-12T19:00:00.000Z\n2015-09-12T19:00:00.000Z_2015-09-12T20:00:00.000Z\n2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z\n2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z\n2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z\n2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: License Comment for NProgress\nDESCRIPTION: Copyright and license information for the NProgress library by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Sample Wikipedia Page Edit Event in JSON\nDESCRIPTION: Example of a Wikipedia page edit event in JSON format from the sample dataset used in the tutorials. This shows the structure of the data that will be loaded into Druid through various ingestion methods.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/index.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Storage in Apache Druid\nDESCRIPTION: Properties for configuring the metadata storage database connection and table names used by the Coordinator, Overlord, and Realtime processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_17\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=postgresql\ndruid.metadata.storage.connector.connectURI=jdbc:postgresql://localhost:5432/druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=druid_pass\ndruid.metadata.storage.connector.createTables=true\ndruid.metadata.storage.tables.base=druid\n```\n\n----------------------------------------\n\nTITLE: Configuring DimensionsSpec with Custom Column Types in Druid\nDESCRIPTION: Example of a dimensionsSpec configuration that specifies different data types for columns, including String, Long, and Float types, along with disabling bitmap indexing for specific string columns. This demonstrates how to customize dimension handling during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Access for Hadoop Indexing Task\nDESCRIPTION: Job properties configuration for accessing S3 in Hadoop indexing tasks, including AWS credentials, filesystem implementations, and compression codecs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\" : {\n   \"fs.s3.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"fs.s3n.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3n.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3n.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Access for Hadoop Indexing Task\nDESCRIPTION: Job properties configuration for accessing S3 in Hadoop indexing tasks, including AWS credentials, filesystem implementations, and compression codecs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"jobProperties\" : {\n   \"fs.s3.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"fs.s3n.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\",\n   \"fs.s3n.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\",\n   \"fs.s3n.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n   \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DimensionsSpec with Custom Column Types in Druid\nDESCRIPTION: Example of a dimensionsSpec configuration that specifies different data types for columns, including String, Long, and Float types, along with disabling bitmap indexing for specific string columns. This demonstrates how to customize dimension handling during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DimensionsSpec with Custom Column Types in Druid\nDESCRIPTION: Example of a dimensionsSpec configuration that specifies different data types for columns, including String, Long, and Float types, along with disabling bitmap indexing for specific string columns. This demonstrates how to customize dimension handling during ingestion.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"page\",\n    \"language\",\n    \"user\",\n    \"unpatrolled\",\n    \"newPage\",\n    \"robot\",\n    \"anonymous\",\n    \"namespace\",\n    \"continent\",\n    \"country\",\n    \"region\",\n    \"city\",\n    {\n      \"type\": \"string\",\n      \"name\": \"comment\",\n      \"createBitmapIndex\": false\n    },\n    {\n      \"type\": \"long\",\n      \"name\": \"countryNum\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLatitude\"\n    },\n    {\n      \"type\": \"float\",\n      \"name\": \"userLongitude\"\n    }\n  ],\n  \"dimensionExclusions\" : [],\n  \"spatialDimensions\" : []\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Compaction Task - Bash Command\nDESCRIPTION: Command to submit the compaction task that will combine the segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-final-index.json\n```\n\n----------------------------------------\n\nTITLE: Starting the Druid Broker Process in Java\nDESCRIPTION: Command to start the Apache Druid Broker process using the Main class. This is the entry point for running a Broker node in a Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/broker.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Defining Druid SQL Metrics in Markdown\nDESCRIPTION: A markdown table listing SQL metrics emitted by the Broker. It includes metrics for query time and response size, along with their dimensions and expected normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`sqlQuery/time`|Milliseconds taken to complete a SQL.|id, nativeQueryIds, dataSource, remoteAddress, success.|< 1s|\n|`sqlQuery/bytes`|number of bytes returned in SQL response.|id, nativeQueryIds, dataSource, remoteAddress, success.| |\n```\n\n----------------------------------------\n\nTITLE: Sample Worker Configuration JSON Specification for Druid Overlord\nDESCRIPTION: A comprehensive JSON configuration for Druid workers, including selection strategy with affinity settings and EC2 autoscaling configuration. This spec controls how tasks are assigned to MiddleManagers and how worker instances are scaled.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/index.md#2025-04-09_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"selectStrategy\": {\n    \"type\": \"fillCapacity\",\n    \"affinityConfig\": {\n      \"affinity\": {\n        \"datasource1\": [\"host1:port\", \"host2:port\"],\n        \"datasource2\": [\"host3:port\"]\n      }\n    }\n  },\n  \"autoScaler\": {\n    \"type\": \"ec2\",\n    \"minNumWorkers\": 2,\n    \"maxNumWorkers\": 12,\n    \"envConfig\": {\n      \"availabilityZone\": \"us-east-1a\",\n      \"nodeData\": {\n        \"amiId\": \"${AMI}\",\n        \"instanceType\": \"c3.8xlarge\",\n        \"minInstances\": 1,\n        \"maxInstances\": 1,\n        \"securityGroupIds\": [\"${IDs}\"],\n        \"keyName\": \"${KEY_NAME}\"\n      },\n      \"userData\": {\n        \"impl\": \"string\",\n        \"data\": \"${SCRIPT_COMMAND}\",\n        \"versionReplacementString\": \":VERSION:\",\n        \"version\": null\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeBoundary Router Strategy in JSON\nDESCRIPTION: JSON configuration for the timeBoundary router strategy, which routes all timeBoundary queries to the highest priority Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/router.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"timeBoundary\"\n}\n```\n\n----------------------------------------\n\nTITLE: Listing segments after disabling to confirm segment status\nDESCRIPTION: Command to list segments in deep storage after some have been disabled, showing that disabled segments remain in deep storage until a Kill Task is executed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -l1 var/druid/segments/deletion-tutorial/\n2015-09-12T00:00:00.000Z_2015-09-12T01:00:00.000Z\n2015-09-12T01:00:00.000Z_2015-09-12T02:00:00.000Z\n2015-09-12T02:00:00.000Z_2015-09-12T03:00:00.000Z\n2015-09-12T03:00:00.000Z_2015-09-12T04:00:00.000Z\n2015-09-12T04:00:00.000Z_2015-09-12T05:00:00.000Z\n2015-09-12T05:00:00.000Z_2015-09-12T06:00:00.000Z\n2015-09-12T06:00:00.000Z_2015-09-12T07:00:00.000Z\n2015-09-12T07:00:00.000Z_2015-09-12T08:00:00.000Z\n2015-09-12T08:00:00.000Z_2015-09-12T09:00:00.000Z\n2015-09-12T09:00:00.000Z_2015-09-12T10:00:00.000Z\n2015-09-12T10:00:00.000Z_2015-09-12T11:00:00.000Z\n2015-09-12T11:00:00.000Z_2015-09-12T12:00:00.000Z\n2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z\n2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z\n2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z\n2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z\n2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z\n2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z\n2015-09-12T18:00:00.000Z_2015-09-12T19:00:00.000Z\n2015-09-12T19:00:00.000Z_2015-09-12T20:00:00.000Z\n2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z\n2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z\n2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z\n2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z\n```\n\n----------------------------------------\n\nTITLE: Running insert-segment-to-db Tool with MySQL and HDFS for Wikipedia Segments\nDESCRIPTION: Command to run the insert-segment-to-db tool with MySQL as metadata storage and HDFS as deep storage. The example loads segments from a Wikipedia dataset stored in HDFS, configuring necessary connection parameters and loading the required MySQL and HDFS storage extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava \n-Ddruid.metadata.storage.type=mysql \n-Ddruid.metadata.storage.connector.connectURI=jdbc\\:mysql\\://localhost\\:3306/druid \n-Ddruid.metadata.storage.connector.user=druid \n-Ddruid.metadata.storage.connector.password=diurd \n-Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\",\\\"druid-hdfs-storage\\\"] \n-Ddruid.storage.type=hdfs\n-cp $DRUID_CLASSPATH \norg.apache.druid.cli.Main tools insert-segment-to-db --workingDir hdfs://host:port//druid/storage/wikipedia --updateDescriptor true\n```\n\n----------------------------------------\n\nTITLE: Running a Real-time Node in Apache Druid\nDESCRIPTION: Command to start a real-time node server in Apache Druid. This initializes a node that provides real-time indexing capabilities for immediate data availability.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Data Server Processes\nDESCRIPTION: Commands to start the Historical and MiddleManager processes on Data servers\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* org.apache.druid.cli.Main server historical\njava `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* org.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Druid Join Limitations Header\nDESCRIPTION: Markdown header introducing the documentation for Druid's join functionality and limitations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/joins.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Joins\n```\n\n----------------------------------------\n\nTITLE: Executing TopN Query with DistinctCount in Druid\nDESCRIPTION: Example of a TopN query using distinctCount aggregator to find the top 5 values of sample_dim based on unique visitor counts. The query operates over a specific day with 'all' granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"topN\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimension\": \"sample_dim\",\n  \"threshold\": 5,\n  \"metric\": \"uv\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Druid Database and User in MySQL\nDESCRIPTION: SQL commands to create a Druid database with UTF-8 encoding, create a Druid user with password, and grant the necessary permissions for Druid to use the database.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- create a druid database, make sure to use utf8mb4 as encoding\nCREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;\n\n-- create a druid user\nCREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';\n\n-- grant the user all the permissions on the database we just created\nGRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Submitting Supervisor Spec via cURL\nDESCRIPTION: Command to submit a Kinesis supervisor specification using cURL to the Druid Overlord API endpoint.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Extension for DataSketches Theta Sketch\nDESCRIPTION: Shows how to include the DataSketches Theta Sketch extension in the Druid configuration file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"druid.extensions.loadList\":[\"druid-datasketches\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Historical Node Storage Capacity\nDESCRIPTION: Command line arguments to increase the storage capacity of Druid historical nodes by setting segment cache locations and maximum server size. This configuration helps when historical nodes are unable to download segments due to size limitations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/faq.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n-Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}]\n-Ddruid.server.maxSize=500000000000\n```\n\n----------------------------------------\n\nTITLE: Retrieving Rule History in Druid Coordinator API\nDESCRIPTION: GET endpoints to retrieve audit history of rules for all datasources or a specific datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_8\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules/history?interval=<interval>\nGET /druid/coordinator/v1/rules/history?count=<n>\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?interval=<interval>\nGET /druid/coordinator/v1/rules/{dataSourceName}/history?count=<n>\n```\n\n----------------------------------------\n\nTITLE: Configuring EventReceiverFirehose in Druid for HTTP Push Ingestion\nDESCRIPTION: Configuration for the EventReceiverFirehose which ingests events via an HTTP endpoint. This is used with Tranquility stream push and the deprecated stream-pull ingestion model.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/firehose.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"receiver\",\n  \"serviceName\": \"eventReceiverServiceName\",\n  \"bufferSize\": 10000\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Runge-Kutta Spring Physics in JavaScript\nDESCRIPTION: This snippet generates functions for Runge-Kutta spring physics calculations. It's adapted from Framer.js and is useful for creating realistic spring-based animations and interactions in web applications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/7724.4bbc210b.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! Runge-Kutta spring physics function generator. Adapted from Framer.js, copyright Koen Bok. MIT License: http://en.wikipedia.org/wiki/MIT_License */\n```\n\n----------------------------------------\n\nTITLE: Configuring Broker Caching in Druid\nDESCRIPTION: Cache configuration properties for the Druid Broker, including enabling/disabling caching, result level caching, and cache limits.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_35\n\nLANGUAGE: properties\nCODE:\n```\ndruid.broker.cache.useCache=false\ndruid.broker.cache.populateCache=false\ndruid.broker.cache.useResultLevelCache=false\ndruid.broker.cache.populateResultLevelCache=false\ndruid.broker.cache.resultLevelCacheLimit=Integer.MAX_VALUE\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Processing Settings for Apache Druid Realtime Node\nDESCRIPTION: YAML configuration for setting up query processing parameters, including buffer sizes, thread counts, and caching options for the Druid realtime node.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/realtime.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.processing.buffer.sizeBytes: auto\ndruid.processing.formatString: processing-%s\ndruid.processing.numMergeBuffers: max(2, druid.processing.numThreads / 4)\ndruid.processing.numThreads: Number of cores - 1\ndruid.processing.columnCache.sizeBytes: 0\ndruid.processing.tmpDir: path represented by java.io.tmpdir\n```\n\n----------------------------------------\n\nTITLE: Help Output for ResetCluster Tool in Apache Druid\nDESCRIPTION: The help output for the ResetCluster tool showing its name, synopsis, and available options for cleaning different parts of the Druid cluster state.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nNAME\n        druid tools reset-cluster - Cleanup all persisted state from metadata\n        and deep storage.\n\nSYNOPSIS\n        druid tools reset-cluster [--all] [--hadoopWorkingPath]\n                [--metadataStore] [--segmentFiles] [--taskLogs]\n\nOPTIONS\n        --all\n            delete all state stored in metadata and deep storage\n\n        --hadoopWorkingPath\n            delete hadoopWorkingPath\n\n        --metadataStore\n            delete all records in metadata storage\n\n        --segmentFiles\n            delete all segment files from deep storage\n\n        --taskLogs\n            delete all tasklogs\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Spec with Transforms\nDESCRIPTION: Complete ingestion specification that includes transform specs to modify the animal column, create a triple-number column, and filter the input data based on multiple conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"transform-tutorial\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"dimensionsSpec\" : {\n            \"dimensions\" : [\n              \"animal\",\n              { \"name\": \"location\", \"type\": \"long\" }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"iso\"\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        { \"type\" : \"count\", \"name\" : \"count\" },\n        { \"type\" : \"longSum\", \"name\" : \"number\", \"fieldName\" : \"number\" },\n        { \"type\" : \"longSum\", \"name\" : \"triple-number\", \"fieldName\" : \"triple-number\" }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"week\",\n        \"queryGranularity\" : \"minute\",\n        \"intervals\" : [\"2018-01-01/2018-01-03\"],\n        \"rollup\" : true\n      },\n      \"transformSpec\": {\n        \"transforms\": [\n          {\n            \"type\": \"expression\",\n            \"name\": \"animal\",\n            \"expression\": \"concat('super-', animal)\"\n          },\n          {\n            \"type\": \"expression\",\n            \"name\": \"triple-number\",\n            \"expression\": \"number * 3\"\n          }\n        ],\n        \"filter\": {\n          \"type\":\"or\",\n          \"fields\": [\n            { \"type\": \"selector\", \"dimension\": \"animal\", \"value\": \"super-mongoose\" },\n            { \"type\": \"selector\", \"dimension\": \"triple-number\", \"value\": \"300\" },\n            { \"type\": \"selector\", \"dimension\": \"location\", \"value\": \"3\" }\n          ]\n        }\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"quickstart/tutorial\",\n        \"filter\" : \"transform-data.json\"\n      },\n      \"appendToExisting\" : false\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000,\n      \"maxRowsInMemory\" : 25000,\n      \"forceExtendableShardSpecs\" : true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license declaration for React's use-sync-external-store-shim production build\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Using JavaScript Filter in Apache Druid Queries (JSON)\nDESCRIPTION: A filter that uses a JavaScript function to determine matches, allowing for complex custom filtering logic. The function takes a dimension value and returns true or false.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/filters.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"javascript\",\n  \"dimension\" : <dimension_string>,\n  \"function\" : \"function(value) { <...> }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Post Averager Query Results\nDESCRIPTION: Sample results from the post averager query showing the original values, moving averages, and calculated ratios across multiple time intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T22:00:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 144269,\n    \"trailing30MinChanges\" : 204088.14285714287,\n    \"ratioTrailing30MinChanges\" : 0.7068955500319539\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T22:30:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 242860,\n    \"trailing30MinChanges\" : 214031.57142857142,\n    \"ratioTrailing30MinChanges\" : 1.134692411867141\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:00:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 119100,\n    \"trailing30MinChanges\" : 198697.2857142857,\n    \"ratioTrailing30MinChanges\" : 0.5994042624782422\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:30:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 177882,\n    \"trailing30MinChanges\" : 193890.0,\n    \"ratioTrailing30MinChanges\" : 0.9174377224199288\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0ba9c98a.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0ba9c98a.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Engine Crawler Access for Apache Druid Website in robots.txt\nDESCRIPTION: This robots.txt file sets rules for all search engine crawlers, preventing them from indexing older documentation versions (0.x through 9.x) and the blog endpoint. This helps direct users to the most current documentation and prevents outdated content from appearing in search results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/robots.txt#2025-04-09_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\nDisallow: /docs/0*/\nDisallow: /docs/1*/\nDisallow: /docs/2*/\nDisallow: /docs/3*/\nDisallow: /docs/4*/\nDisallow: /docs/5*/\nDisallow: /docs/6*/\nDisallow: /docs/7*/\nDisallow: /docs/8*/\nDisallow: /docs/9*/\nDisallow: /blog*\n```\n\n----------------------------------------\n\nTITLE: Executing EXPLAIN PLAN Query in Apache Druid\nDESCRIPTION: This snippet shows the execution of the EXPLAIN PLAN FOR command in the Druid SQL interface (dsql). It displays the resulting query plan, which includes details about the TopN query type, data source, dimensions, metrics, and other query parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;\n\n PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\n DruidQueryRel(query=[{\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"default\",\"dimension\":\"page\",\"outputName\":\"d0\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":10,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}],\"postAggregations\":[],\"context\":{},\"descending\":false}], signature=[{d0:STRING, a0:LONG}]) \n\nRetrieved 1 row in 0.03s.\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Coordinator Server\nDESCRIPTION: Command to start the Druid coordinator server process using the Main class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/coordinator.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server coordinator\n```\n\n----------------------------------------\n\nTITLE: Disabling Zero-filling in Druid Timeseries Queries\nDESCRIPTION: Example of how to disable zero-filling for empty time buckets in Druid timeseries queries by setting the 'skipEmptyBuckets' property to true in the query context. This removes time buckets with no data from the results.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-04T00:00:00.000\" ],\n  \"context\" : {\n    \"skipEmptyBuckets\": \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using AND Logical Expression Filter in Druid Queries\nDESCRIPTION: The AND filter combines multiple filters with a logical AND operation. Only rows matching all specified filters will be included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"and\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Middle Manager Tasks List Endpoint\nDESCRIPTION: HTTP GET endpoint to retrieve all currently running tasks on a Middle Manager node.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_2\n\nLANGUAGE: http\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/tasks\n```\n\n----------------------------------------\n\nTITLE: Running Compaction Task with Preserved Granularity\nDESCRIPTION: Command to submit the compaction task that maintains the original hourly granularity of segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Task Processing Threads and Buffers in Druid\nDESCRIPTION: Sets the number of processing threads and buffer sizes for Tasks launched by MiddleManager. These settings affect query processing and ingestion performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/basic-cluster-tuning.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.indexer.fork.property.druid.processing.numThreads: 1\ndruid.indexer.fork.property.druid.processing.numMergeBuffers: 2\ndruid.indexer.fork.property.druid.processing.buffer.sizeBytes: 100MB\n```\n\n----------------------------------------\n\nTITLE: Coordinator Leader Election Path in ZooKeeper\nDESCRIPTION: The ZooKeeper path used for Coordinator leader election using Curator LeadershipLatch recipe.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/dependencies/zookeeper.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.coordinatorPath}/_COORDINATOR\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Servers Information\nDESCRIPTION: SQL query to retrieve information about all Druid servers from the sys.servers table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.servers;\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for NProgress\nDESCRIPTION: This snippet contains the copyright and license information for NProgress, a progress bar library for Ajax'y applications, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring TwitterSpritzerFirehose in Druid\nDESCRIPTION: Configuration specification for the TwitterSpritzerFirehose, which connects to Twitter's spritzer data stream. Includes parameters for controlling event count limits and runtime duration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/examples.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"twitzer\",\n    \"maxEventCount\": -1,\n    \"maxRunMinutes\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Druid License Header Comment\nDESCRIPTION: HTML comment block containing Apache License 2.0 information for the documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-dimension Partitioning in Druid\nDESCRIPTION: Example of configuring single-dimension partitioning for Druid indexing. This strategy partitions data based on ranges of a single dimension, creating segments containing rows with dimension values in specific ranges.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"dimension\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Creating Druid Database and User in MySQL\nDESCRIPTION: SQL commands to create a Druid database with UTF-8 encoding, create a Druid user with password, and grant appropriate privileges to that user.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- create a druid database, make sure to use utf8mb4 as encoding\nCREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;\n\n-- create a druid user\nCREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';\n\n-- grant the user all the permissions on the database we just created\nGRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop-based ORC Ingestion in Druid\nDESCRIPTION: Example configuration for batch ingestion of ORC data using HadoopDruidIndexer. Demonstrates setup of input format, parser specifications, and data schema definition with ORC type handling. Includes essential configurations for HDFS integration and segment management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/orc.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat\",\n        \"paths\": \"/data/path/in/HDFS/\"\n      },\n      \"metadataUpdateSpec\": {\n        \"type\": \"postgresql\",\n        \"connectURI\": \"jdbc:postgresql://localhost/druid\",\n        \"user\" : \"druid\",\n        \"password\" : \"asdf\",\n        \"segmentTable\": \"druid_segments\"\n      },\n      \"segmentOutputPath\": \"tmp/segments\"\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"no_metrics\",\n      \"parser\": {\n        \"type\": \"orc\",\n        \"parseSpec\": {\n          \"format\": \"timeAndDims\",\n          \"timestampSpec\": {\n            \"column\": \"time\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [\n              \"name\"\n            ],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        },\n        \"typeString\": \"struct<time:string,name:string>\",\n        \"mapFieldNameFormat\": \"<PARENT>_<CHILD>\"\n      },\n      \"metricsSpec\": [{\n        \"type\": \"count\",\n        \"name\": \"count\"\n      }],\n      \"granularitySpec\": {\n        \"type\": \"uniform\",\n        \"segmentGranularity\": \"DAY\",\n        \"queryGranularity\": \"ALL\",\n        \"intervals\": [\"2015-12-31/2016-01-02\"]\n      }\n    },\n    \"tuningConfig\": {\n      \"type\": \"hadoop\",\n      \"workingPath\": \"tmp/working_path\",\n      \"partitionsSpec\": {\n        \"targetPartitionSize\": 5000000\n      },\n      \"jobProperties\" : {},\n      \"leaveIntermediate\": true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TwitterSpritzerFirehose in Druid\nDESCRIPTION: Configuration specification for the TwitterSpritzerFirehose, which connects to Twitter's spritzer data stream. Includes parameters for controlling event count limits and runtime duration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/examples.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"twitzer\",\n    \"maxEventCount\": -1,\n    \"maxRunMinutes\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Realtime Operation Configuration in Apache Druid\nDESCRIPTION: Configuration properties for realtime node operations in Apache Druid, specifying where to publish segments and the location of the realtime specification file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/realtime.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.publish.type: metadata\ndruid.realtime.specFile: none\n```\n\n----------------------------------------\n\nTITLE: Enabling Segments for a Datasource in Druid Coordinator API\nDESCRIPTION: POST endpoint to enable all non-overshadowed segments for a specified datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/datasources/{dataSourceName}\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Hadoop Dependencies in Druid\nDESCRIPTION: Shows the expected directory structure when using multiple versions of Hadoop client libraries with Druid. Each version is organized in its own subdirectory with all required JAR files.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-dependencies/\n hadoop-client\n     2.3.0\n        activation-1.1.jar\n        avro-1.7.4.jar\n        commons-beanutils-1.7.0.jar\n        commons-beanutils-core-1.8.0.jar\n        commons-cli-1.2.jar\n        commons-codec-1.4.jar\n    ..... lots of jars\n     2.4.0\n         activation-1.1.jar\n         avro-1.7.4.jar\n         commons-beanutils-1.7.0.jar\n         commons-beanutils-core-1.8.0.jar\n         commons-cli-1.2.jar\n         commons-codec-1.4.jar\n    ..... lots of jars\n```\n\n----------------------------------------\n\nTITLE: License Comment for React Scheduler\nDESCRIPTION: Copyright and license information for the React Scheduler production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Caching Configuration in Apache Druid Realtime Nodes\nDESCRIPTION: Optional configuration properties for enabling caching on realtime nodes, including settings to enable/disable the cache, populate the cache, specify uncacheable query types, and set maximum cache entry size.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/realtime.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.realtime.cache.useCache: false\ndruid.realtime.cache.populateCache: false\ndruid.realtime.cache.unCacheable: [\"select\"]\ndruid.realtime.cache.maxEntrySize: 1000000\n```\n\n----------------------------------------\n\nTITLE: Running the Historical Process in Apache Druid\nDESCRIPTION: Command to start a Druid Historical node using the Main class. This command initializes a Historical server process which will load and serve immutable data segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/historical.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server historical\n```\n\n----------------------------------------\n\nTITLE: Getting Supervisor Status GET Endpoint\nDESCRIPTION: REST endpoint to retrieve current state and metrics of tasks managed by a specific supervisor.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_16\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/indexer/v1/supervisor/<supervisorId>/status\n```\n\n----------------------------------------\n\nTITLE: Retrieving Unparseable Events in Apache Druid\nDESCRIPTION: Shows the API endpoint for retrieving current lists of unparseable events from a running task. This is supported by non-parallel Native Batch Tasks and Kafka Indexing Service tasks.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/reports.md#2025-04-09_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nhttp://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/<task-id>/unparseableEvents\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Directories for Hadoop-Druid Integration\nDESCRIPTION: Commands to create temporary shared directories that facilitate file exchange between the host machine and the Hadoop Docker container.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p /tmp/shared\nmkdir -p /tmp/shared/hadoop_xml\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license declaration for React's scheduler.production.min.js\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: License Declaration for React DOM Library\nDESCRIPTION: MIT license declaration for the react-dom.production.min.js library, version 17.0.2, developed by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Inline Schemas Avro Bytes Decoder in Druid\nDESCRIPTION: This snippet demonstrates the configuration for a multiple inline schemas-based Avro bytes decoder. It includes the decoder type and a map of schema IDs to Avro schema definitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"avroBytesDecoder\": {\n  \"type\": \"multiple_schemas_inline\",\n  \"schemas\": {\n    \"1\": {\n      \"namespace\": \"org.apache.druid.data\",\n      \"name\": \"User\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"FullName\", \"type\": \"string\" },\n        { \"name\": \"Country\", \"type\": \"string\" }\n      ]\n    },\n    \"2\": {\n      \"namespace\": \"org.apache.druid.otherdata\",\n      \"name\": \"UserIdentity\",\n      \"type\": \"record\",\n      \"fields\": [\n        { \"name\": \"Name\", \"type\": \"string\" },\n        { \"name\": \"Location\", \"type\": \"string\" }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Grand Totals in Druid Timeseries Queries (JSON)\nDESCRIPTION: A timeseries query with the 'grandTotal' context parameter set to true. This adds an extra row with aggregated totals across all time buckets as the last element in the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Grand Totals in Druid Timeseries Queries (JSON)\nDESCRIPTION: A timeseries query with the 'grandTotal' context parameter set to true. This adds an extra row with aggregated totals across all time buckets as the last element in the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Raw Data Format for Packet/Byte Counts Example in Druid\nDESCRIPTION: Sample raw data representing network traffic with timestamp, source IP, destination IP, packet count, and byte count, used to demonstrate Druid's rollup functionality.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:35Z      1.1.1.1       2.2.2.2            100      1000\n2018-01-01T01:01:51Z      1.1.1.1       2.2.2.2            200      2000\n2018-01-01T01:01:59Z      1.1.1.1       2.2.2.2            300      3000\n2018-01-01T01:02:14Z      1.1.1.1       2.2.2.2            400      4000\n2018-01-01T01:02:29Z      1.1.1.1       2.2.2.2            500      5000\n2018-01-01T01:03:29Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:14Z      7.7.7.7       8.8.8.8            100      1000\n2018-01-02T21:33:45Z      7.7.7.7       8.8.8.8            200      2000\n2018-01-02T21:35:45Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Math Functions in Apache Druid\nDESCRIPTION: A markdown table listing various mathematical functions available in Apache Druid. Each row contains the function name and a brief description of its operation. The table covers a wide range of mathematical operations including basic arithmetic, trigonometry, logarithms, and special functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/misc/math-expr.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|name|description|\n|----|-----------|\n|abs|abs(x) would return the absolute value of x|\n|acos|acos(x) would return the arc cosine of x|\n|asin|asin(x) would return the arc sine of x|\n|atan|atan(x) would return the arc tangent of x|\n|atan2|atan2(y, x) would return the angle theta from the conversion of rectangular coordinates (x, y) to polar * coordinates (r, theta)|\n|cbrt|cbrt(x) would return the cube root of x|\n|ceil|ceil(x) would return the smallest (closest to negative infinity) double value that is greater than or equal to x and is equal to a mathematical integer|\n|copysign|copysign(x) would return the first floating-point argument with the sign of the second floating-point argument|\n|cos|cos(x) would return the trigonometric cosine of x|\n|cosh|cosh(x) would return the hyperbolic cosine of x|\n|cot|cot(x) would return the trigonometric cotangent of an angle x|\n|div|div(x,y) is integer division of x by y|\n|exp|exp(x) would return Euler's number raised to the power of x|\n|expm1|expm1(x) would return e^x-1|\n|floor|floor(x) would return the largest (closest to positive infinity) double value that is less than or equal to x and is equal to a mathematical integer|\n|getExponent|getExponent(x) would return the unbiased exponent used in the representation of x|\n|hypot|hypot(x, y) would return sqrt(x^2+y^2) without intermediate overflow or underflow|\n|log|log(x) would return the natural logarithm of x|\n|log10|log10(x) would return the base 10 logarithm of x|\n|log1p|log1p(x) would the natural logarithm of x + 1|\n|max|max(x, y) would return the greater of two values|\n|min|min(x, y) would return the smaller of two values|\n|nextafter|nextafter(x, y) would return the floating-point number adjacent to the x in the direction of the y|\n|nextUp|nextUp(x) would return the floating-point value adjacent to x in the direction of positive infinity|\n|pi|pi would return the constant value of the  |\n|pow|pow(x, y) would return the value of the x raised to the power of y|\n|remainder|remainder(x, y) would return the remainder operation on two arguments as prescribed by the IEEE 754 standard|\n|rint|rint(x) would return value that is closest in value to x and is equal to a mathematical integer|\n|round|round(x, y) would return the value of the x rounded to the y decimal places. While x can be an integer or floating-point number, y must be an integer. The type of the return value is specified by that of x. y defaults to 0 if omitted. When y is negative, x is rounded on the left side of the y decimal points.|\n|scalb|scalb(d, sf) would return d * 2^sf rounded as if performed by a single correctly rounded floating-point multiply to a member of the double value set|\n|signum|signum(x) would return the signum function of the argument x|\n|sin|sin(x) would return the trigonometric sine of an angle x|\n|sinh|sinh(x) would return the hyperbolic sine of x|\n|sqrt|sqrt(x) would return the correctly rounded positive square root of x|\n|tan|tan(x) would return the trigonometric tangent of an angle x|\n|tanh|tanh(x) would return the hyperbolic tangent of x|\n|todegrees|todegrees(x) converts an angle measured in radians to an approximately equivalent angle measured in degrees|\n|toradians|toradians(x) converts an angle measured in degrees to an approximately equivalent angle measured in radians|\n|ulp|ulp(x) would return the size of an ulp of the argument x|\n```\n\n----------------------------------------\n\nTITLE: Running the Druid MiddleManager Server\nDESCRIPTION: Command to start the Apache Druid MiddleManager process from the command line. This launches the MiddleManager service that manages Peons and handles task execution.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/middlemanager.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Basic Timeseries Query in Apache Druid\nDESCRIPTION: A comprehensive example of a timeseries query object in Druid that includes filtering, aggregations, and post-aggregations. This query retrieves daily data for a specified interval with complex filtering conditions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"granularity\": \"day\",\n  \"descending\": \"true\",\n  \"filter\": {\n    \"type\": \"and\",\n    \"fields\": [\n      { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" },\n      { \"type\": \"or\",\n        \"fields\": [\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" },\n          { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" }\n        ]\n      }\n    ]\n  },\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"postAggregations\": [\n    { \"type\": \"arithmetic\",\n      \"name\": \"sample_divide\",\n      \"fn\": \"/\",\n      \"fields\": [\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" },\n        { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" }\n      ]\n    }\n  ],\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: React DOM License Information\nDESCRIPTION: License information for React's react-dom.production.min.js (v17.0.2) created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Task Response in JSON (MiddleManager API)\nDESCRIPTION: This JSON response confirms that a running task has been shut down. It returns an object with the 'task' key and the taskid as the value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"}\n```\n\n----------------------------------------\n\nTITLE: Example of Mixed Version Druid Segments During Update\nDESCRIPTION: Example showing a mixture of v1 and v2 segments that may exist during a partial update. This demonstrates how segments are updated on an interval-by-interval basis rather than atomically across multiple intervals.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/schema-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v2_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: Displaying JVM Metrics Table in Markdown\nDESCRIPTION: This code snippet shows a markdown table listing various JVM metrics available in Apache Druid when the JVMMonitor module is included. It includes metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`jvm/pool/committed`|Committed pool.|poolKind, poolName.|close to max pool|\n|`jvm/pool/init`|Initial pool.|poolKind, poolName.|Varies.|\n|`jvm/pool/max`|Max pool.|poolKind, poolName.|Varies.|\n|`jvm/pool/used`|Pool used.|poolKind, poolName.|< max pool|\n|`jvm/bufferpool/count`|Bufferpool count.|bufferPoolName.|Varies.|\n|`jvm/bufferpool/used`|Bufferpool used.|bufferPoolName.|close to capacity|\n|`jvm/bufferpool/capacity`|Bufferpool capacity.|bufferPoolName.|Varies.|\n|`jvm/mem/init`|Initial memory.|memKind.|Varies.|\n|`jvm/mem/max`|Max memory.|memKind.|Varies.|\n|`jvm/mem/used`|Used memory.|memKind.|< max memory|\n|`jvm/mem/committed`|Committed memory.|memKind.|close to max memory|\n|`jvm/gc/count`|Garbage collection count.|gcName (cms/g1/parallel/etc.), gcGen (old/young)|Varies.|\n|`jvm/gc/cpu`|Cpu time in Nanoseconds spent on garbage collection.|gcName, gcGen|Sum of `jvm/gc/cpu` should be within 10-30% of sum of `jvm/cpu/total`, depending on the GC algorithm used (reported by [`JvmCpuMonitor`](../configuration/index.html#enabling-metrics)) |\n```\n\n----------------------------------------\n\nTITLE: Adding DataSegmentPuller and DataSegmentPusher bindings in Java\nDESCRIPTION: Example of how to add bindings for custom DataSegmentPuller and DataSegmentPusher implementations in a Druid module. This code demonstrates binding HDFS-specific implementations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/modules.md#2025-04-09_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nBinders.dataSegmentPullerBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);\n\nBinders.dataSegmentPusherBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n```\n\n----------------------------------------\n\nTITLE: Time Function Example in Druid SQL\nDESCRIPTION: Example of time-related function syntax in Druid SQL, including timestamp manipulation and extraction functions. These functions work with Druid's __time column and can handle both millisecond and string timestamps.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nTIME_EXTRACT(__time, 'HOUR')\nTIME_EXTRACT(__time, 'HOUR', 'America/Los_Angeles')\nTIMESTAMPADD(HOUR, 2, timestamp)\ntimestamp + INTERVAL '2' HOUR\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data for Kafka in Bash\nDESCRIPTION: Commands to extract the sample Wikipedia data file for loading into Kafka.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial\ngunzip -k wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Header\nDESCRIPTION: Front matter and license header for Druid multitenancy documentation page\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/multitenancy.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Multitenancy Considerations\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Metadata Query Properties in Druid\nDESCRIPTION: Configuration properties for Segment Metadata queries, setting default history interval and analysis types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_42\n\nLANGUAGE: properties\nCODE:\n```\ndruid.query.segmentMetadata.defaultHistory=P1W\ndruid.query.segmentMetadata.defaultAnalysisTypes=[\"cardinality\", \"interval\", \"minmax\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Kafka\nDESCRIPTION: Commands to set UTF-8 encoding and send Wikipedia sample data to Kafka topic using console producer.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport KAFKA_OPTS=\"-Dfile.encoding=UTF-8\"\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Quantiles Post Aggregator for DoublesSketch\nDESCRIPTION: Post aggregator configuration to extract multiple quantile values from a DoublesSketch. It returns an array of approximate values at specified fractional positions in the theoretical sorted stream.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantiles\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fractions\" : <array of fractional positions in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Numeric TopNMetricSpec in Druid JSON\nDESCRIPTION: Demonstrates how to specify a simple numeric metric for sorting topN results in a Druid query. It shows both the string format and the more verbose JSON object format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/topnmetricspec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": \"<metric_name>\"\n```\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"numeric\",\n    \"metric\": \"<metric_name>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Overlord Worker Blacklist Properties\nDESCRIPTION: Configuration properties for controlling worker blacklisting behavior in Druid Overlord. These settings determine thresholds for blacklisting workers, cleanup periods, and maximum percentage of blacklisted workers allowed.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/overlord.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.indexer.runner.maxRetriesBeforeBlacklist\ndruid.indexer.runner.workerBlackListBackoffTime\ndruid.indexer.runner.workerBlackListCleanupPeriod\ndruid.indexer.runner.maxPercentageBlacklistWorkers\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Jackson Dependency Conflict Error in Java\nDESCRIPTION: This snippet shows the Java VerifyError that occurs due to Jackson dependency conflicts between Druid and CDH in a MapReduce job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\njava.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n```\n\n----------------------------------------\n\nTITLE: React DOM Copyright Notice\nDESCRIPTION: Copyright notice for the React DOM library, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Cardinality Aggregator for Distinct People in Apache Druid\nDESCRIPTION: This example shows how to use the Cardinality aggregator to determine the number of distinct people based on combinations of first and last names. It uses the byRow option set to true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/hll-old.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_people\",\n  \"fields\": [ \"first_name\", \"last_name\" ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Service Output\nDESCRIPTION: Example output showing the startup of various Druid services and their log locations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/index.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n[Wed Feb 27 12:46:13 2019] Running command[zk], logging to[/apache-druid-0.14.1-incubating/var/sv/zk.log]: bin/run-zk quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[coordinator], logging to[/apache-druid-0.14.1-incubating/var/sv/coordinator.log]: bin/run-druid coordinator quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[broker], logging to[/apache-druid-0.14.1-incubating/var/sv/broker.log]: bin/run-druid broker quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[router], logging to[/apache-druid-0.14.1-incubating/var/sv/router.log]: bin/run-druid router quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[historical], logging to[/apache-druid-0.14.1-incubating/var/sv/historical.log]: bin/run-druid historical quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[overlord], logging to[/apache-druid-0.14.1-incubating/var/sv/overlord.log]: bin/run-druid overlord quickstart/tutorial/conf\n[Wed Feb 27 12:46:13 2019] Running command[middleManager], logging to[/apache-druid-0.14.1-incubating/var/sv/middleManager.log]: bin/run-druid middleManager quickstart/tutorial/conf\n```\n\n----------------------------------------\n\nTITLE: Defining Datasource Name in Druid Ingestion Spec\nDESCRIPTION: Adding the dataSource parameter to the dataSchema to specify the name of the datasource as 'ingestion-tutorial'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n}\n```\n\n----------------------------------------\n\nTITLE: Jackson Dependency Error Example\nDESCRIPTION: Example of the Jackson version conflict error that occurs when running Druid with CDH\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\njava.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n```\n\n----------------------------------------\n\nTITLE: Druid Timestamp Specification\nDESCRIPTION: Configuration for timestamp handling including format and source column specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Declaration in JavaScript\nDESCRIPTION: License declaration for React's scheduler.production.min.js version 0.20.2, created by Facebook. The library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Basic Segment Metadata Query Structure in Druid\nDESCRIPTION: Basic query structure for retrieving segment metadata from Druid. Specifies the query type, data source, and time interval to analyze.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/segmentmetadataquery.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\":\"segmentMetadata\",\n  \"dataSource\":\"sample_datasource\",\n  \"intervals\":[\"2013-01-01/2014-01-01\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Last Name First Character Cardinality Example\nDESCRIPTION: Advanced example showing cardinality calculation with dimension extraction to count distinct first characters of last names.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/hll-old.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_last_name_first_char\",\n  \"fields\": [\n    {\n     \"type\" : \"extraction\",\n     \"dimension\" : \"last_name\",\n     \"outputName\" :  \"last_name_first_char\",\n     \"extractionFn\" : { \"type\" : \"substring\", \"index\" : 0, \"length\" : 1 }\n    }\n  ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: React-Is License Notice\nDESCRIPTION: License notice for the React-Is production module, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React-Is License Notice\nDESCRIPTION: License notice for the React-Is production module, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Apache Druid License Header\nDESCRIPTION: Standard Apache 2.0 license header included as an HTML comment at the top of the documentation page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/integrating-druid-with-other-technologies.md#2025-04-09_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Starting Router Process with Java Command\nDESCRIPTION: Command to start the Druid Router process using the Main class.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server router\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for mark.js\nDESCRIPTION: Copyright and license declaration for the mark.js library, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Local Mount Storage Configuration Table\nDESCRIPTION: Configuration properties table for setting up local mount storage in Apache Druid. Details the required properties druid.storage.type and druid.storage.storageDirectory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.storage.type`|local||Must be set.|\n|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|\n```\n\n----------------------------------------\n\nTITLE: Accessing Overlord Console URL\nDESCRIPTION: Illustrates the URL format for accessing the Overlord's console, which provides task and worker management features.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/management-uis.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n```\nhttp://<OVERLORD_IP>:<OVERLORD_PORT>/console.html\n```\n```\n\n----------------------------------------\n\nTITLE: Local Mount Storage Configuration Table\nDESCRIPTION: Configuration properties table for setting up local mount storage in Apache Druid. Details the required properties druid.storage.type and druid.storage.storageDirectory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Possible Values|Description|Default|\n|--------|---------------|-----------|-------|\n|`druid.storage.type`|local||Must be set.|\n|`druid.storage.storageDirectory`||Directory for storing segments.|Must be set.|\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Rollup SQL-like Operation in Druid\nDESCRIPTION: Pseudocode showing how Druid's rollup operation works by grouping and aggregating data based on truncated timestamps and dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_3\n\nLANGUAGE: pseudocode\nCODE:\n```\nGROUP BY TRUNCATE(timestamp, MINUTE), srcIP, dstIP :: SUM(packets), SUM(bytes)\n```\n\n----------------------------------------\n\nTITLE: General Health Metrics Table in Markdown\nDESCRIPTION: General health metrics including Historical node statistics and JVM monitoring data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/metrics.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`segment/max`|Maximum byte limit available for segments.||Varies.|\n|`segment/used`|Bytes used for served segments.|dataSource, tier, priority.|< max|\n```\n\n----------------------------------------\n\nTITLE: Registering a Password Provider Implementation with Jackson\nDESCRIPTION: Demonstrates how to register a custom PasswordProvider implementation with Jackson's subtype registration system. This allows the custom provider to be used when specified by name in Druid's configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/modules.md#2025-04-09_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nreturn ImmutableList.of(\n    new SimpleModule(\"SomePasswordProviderModule\")\n        .registerSubtypes(\n            new NamedType(SomePasswordProvider.class, \"some\")\n        )\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Context for Apache Druid Scan Query in JSON\nDESCRIPTION: This JSON object demonstrates how to set custom values for maxRowsQueuedForOrdering and maxSegmentPartitionsOrderedInMemory in the query context. These settings override the default configuration properties for the specific query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/scan-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"maxRowsQueuedForOrdering\": 100001,\n  \"maxSegmentPartitionsOrderedInMemory\": 100\t\n}\n```\n\n----------------------------------------\n\nTITLE: Task ID Response from Druid API\nDESCRIPTION: Example JSON response from Druid after successfully submitting an ingestion task. The response contains the unique task ID that can be used to monitor the task's progress.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n{\"task\":\"index_wikipedia_2018-06-09T21:30:32.802Z\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Properties for Druid Broker\nDESCRIPTION: This snippet defines SQL-related configuration properties for the Druid Broker, including enablement flags, connection limits, and query planning options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_46\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.sql.enable`|Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely.|false|\n|`druid.sql.avatica.enable`|Whether to enable JDBC querying at `/druid/v2/sql/avatica/`.|true|\n|`druid.sql.avatica.maxConnections`|Maximum number of open connections for the Avatica server. These are not HTTP connections, but are logical client connections that may span multiple HTTP connections.|50|\n|`druid.sql.avatica.maxRowsPerFrame`|Maximum number of rows to return in a single JDBC frame. Setting this property to -1 indicates that no row limit should be applied. Clients can optionally specify a row limit in their requests; if a client specifies a row limit, the lesser value of the client-provided limit and `maxRowsPerFrame` will be used.|5,000|\n|`druid.sql.avatica.maxStatementsPerConnection`|Maximum number of simultaneous open statements per Avatica client connection.|1|\n|`druid.sql.avatica.connectionIdleTimeout`|Avatica client connection idle timeout.|PT5M|\n|`druid.sql.http.enable`|Whether to enable JSON over HTTP querying at `/druid/v2/sql/`.|true|\n|`druid.sql.planner.awaitInitializationOnStart`|Boolean|Whether the the Broker will wait for its SQL metadata view to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also `druid.broker.segment.awaitInitializationOnStart`, a related setting.|true|\n|`druid.sql.planner.maxQueryCount`|Maximum number of queries to issue, including nested queries. Set to 1 to disable sub-queries, or set to 0 for unlimited.|8|\n|`druid.sql.planner.maxSemiJoinRowsInMemory`|Maximum number of rows to keep in memory for executing two-stage semi-join queries like `SELECT * FROM Employee WHERE DeptName IN (SELECT DeptName FROM Dept)`.|100000|\n|`druid.sql.planner.maxTopNLimit`|Maximum threshold for a [TopN query](../querying/topnquery.html). Higher limits will be planned as [GroupBy queries](../querying/groupbyquery.html) instead.|100000|\n|`druid.sql.planner.metadataRefreshPeriod`|Throttle for metadata refreshes.|PT1M|\n|`druid.sql.planner.selectThreshold`|Page size threshold for [Select queries](../querying/select-query.html). Select queries for larger resultsets will be issued back-to-back using pagination.|1000|\n|`druid.sql.planner.useApproximateCountDistinct`|Whether to use an approximate cardinalty algorithm for `COUNT(DISTINCT foo)`.|true|\n|`druid.sql.planner.useApproximateTopN`|Whether to use approximate [TopN queries](../querying/topnquery.html) when a SQL query could be expressed as such. If false, exact [GroupBy queries](../querying/groupbyquery.html) will be used instead.|true|\n|`druid.sql.planner.requireTimeCondition`|Whether to require SQL to have filter conditions on __time column so that all generated native queries will have user specified intervals. If true, all queries wihout filter condition on __time column will fail|false|\n|`druid.sql.planner.sqlTimeZone`|Sets the default time zone for the server, which will affect how time functions and timestamp literals behave. Should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\".|UTC|\n|`druid.sql.planner.serializeComplexValues`|Whether to serialize \"complex\" output values, false will return the class name instead of the serialized value.|true|\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kafka Supervisor Stats in Apache Druid\nDESCRIPTION: This HTTP GET request retrieves combined live row statistics from all tasks managed by a Kafka supervisor in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/reports.md#2025-04-09_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nhttp://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/supervisor/<supervisor-id>/stats\n```\n\n----------------------------------------\n\nTITLE: Implementing GroupBy Query with DistinctCount in Druid\nDESCRIPTION: Example of a GroupBy query using distinctCount aggregator to count unique visitors grouped by a sample dimension. The query operates over a single day with all granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimensions\": \"[sample_dim]\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Authentication Configuration Properties JSON\nDESCRIPTION: Configuration properties for Druid authentication including authenticator chain, escalator type, authorizers and security paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"druid.auth.authenticatorChain\": [\"allowAll\"],\n  \"druid.escalator.type\": \"noop\",\n  \"druid.auth.authorizers\": [\"allowAll\"],\n  \"druid.auth.unsecuredPaths\": [],\n  \"druid.auth.allowUnauthenticatedHttpOptions\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Emitter in Apache Druid YAML\nDESCRIPTION: Example YAML configuration for setting up the Kafka Emitter in Apache Druid. It specifies the bootstrap servers, metric and alert topics, and additional producer configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/kafka-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.emitter.kafka.bootstrap.servers=hostname1:9092,hostname2:9092\ndruid.emitter.kafka.metric.topic=druid-metric\ndruid.emitter.kafka.alert.topic=druid-alert\ndruid.emitter.kafka.producer.config={\"max.block.ms\":10000}\n```\n\n----------------------------------------\n\nTITLE: Connecting to MySQL from Command Line\nDESCRIPTION: Command to connect to the MySQL server as the root user from a terminal.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n> mysql -u root\n```\n\n----------------------------------------\n\nTITLE: Dimension Specification Without Rollup in Druid\nDESCRIPTION: Example of dimensionsSpec configuration when rollup is disabled. In this case, all columns including metric fields are specified as dimensions with their respective data types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n},\n```\n\n----------------------------------------\n\nTITLE: CSV Example Input for Druid Lookup\nDESCRIPTION: Example CSV input data with three columns that can be used for lookup configuration in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nbar,something,foo\nbat,something2,baz\ntruck,something3,buck\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Notice\nDESCRIPTION: License notice for the mark.js library, version 8.11.1, created by Julian Khnel and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Cardinality Aggregator with Extraction Function\nDESCRIPTION: Advanced example showing how to use the cardinality aggregator with a substring extraction function to count distinct starting characters of last names.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/hll-old.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_last_name_first_char\",\n  \"fields\": [\n    {\n     \"type\" : \"extraction\",\n     \"dimension\" : \"last_name\",\n     \"outputName\" :  \"last_name_first_char\",\n     \"extractionFn\" : { \"type\" : \"substring\", \"index\" : 0, \"length\" : 1 }\n    }\n  ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Data Structure for Wikipedia Page Edits\nDESCRIPTION: Example of a JSON object representing a Wikipedia page edit event from the sample dataset used in the tutorials. The object contains various fields like timestamp, user, page edited, and location data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/index.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\":\"2015-09-12T20:03:45.018Z\",\n  \"channel\":\"#en.wikipedia\",\n  \"namespace\":\"Main\",\n  \"page\":\"Spider-Man's powers and equipment\",\n  \"user\":\"foobar\",\n  \"comment\":\"/* Artificial web-shooters */\",\n  \"cityName\":\"New York\",\n  \"regionName\":\"New York\",\n  \"regionIsoCode\":\"NY\",\n  \"countryName\":\"United States\",\n  \"countryIsoCode\":\"US\",\n  \"isAnonymous\":false,\n  \"isNew\":false,\n  \"isMinor\":false,\n  \"isRobot\":false,\n  \"isUnpatrolled\":false,\n  \"added\":99,\n  \"delta\":99,\n  \"deleted\":0,\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Worker Capacity for Kafka Indexing Tasks in Druid\nDESCRIPTION: Formula for calculating the minimum worker capacity needed to support concurrent reading and publishing tasks in a Kafka indexing service deployment. This ensures sufficient resources are available for tasks to function properly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\nworkerCapacity = 2 * replicas * taskCount\n```\n\n----------------------------------------\n\nTITLE: Configuring Hybrid Cache in Apache Druid\nDESCRIPTION: This snippet describes the configuration for a hybrid cache in Apache Druid, which combines two caches as a two-level L1/L2 cache. It includes properties for specifying cache types and their respective configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_42\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.cache.l1.type`|type of cache to use for L1 cache. See `druid.cache.type` configuration for valid types.|`caffeine`|\n|`druid.cache.l2.type`|type of cache to use for L2 cache. See `druid.cache.type` configuration for valid types.|`caffeine`|\n|`druid.cache.l1.*`|Any property valid for the given type of L1 cache can be set using this prefix. For instance, if you are using a `caffeine` L1 cache, specify `druid.cache.l1.sizeInBytes` to set its size.|defaults are the same as for the given cache type.|\n|`druid.cache.l2.*`|Prefix for L2 cache settings, see description for L1.|defaults are the same as for the given cache type.|\n|`druid.cache.useL2`|A boolean indicating whether to query L2 cache, if it's a miss in L1. It makes sense to configure this to `false` on Historical processes, if L2 is a remote cache like `memcached`, and this cache also used on brokers, because in this case if a query reached Historical it means that a broker didn't find corresponding results in the same remote cache, so a query to the remote cache from Historical is guaranteed to be a miss.|`true`|\n|`druid.cache.populateL2`|A boolean indicating whether to put results into L2 cache.|`true`|\n```\n\n----------------------------------------\n\nTITLE: Dimension Specification Without Rollup in Druid\nDESCRIPTION: Example of dimensionsSpec configuration when rollup is disabled. In this case, all columns including metric fields are specified as dimensions with their respective data types.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n},\n```\n\n----------------------------------------\n\nTITLE: React Core License Notice\nDESCRIPTION: License notice for the React core production module, version 17.0.2, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: License Comment for React Is\nDESCRIPTION: Copyright and license information for the React Is production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: License Comment for mark.js\nDESCRIPTION: Copyright and license information for the mark.js library by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Emitter in Apache Druid\nDESCRIPTION: Configuration properties for the Logging Emitter module in Druid, which logs metrics using the specified logger class and log level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=logging\ndruid.emitter.logging.loggerClass=LoggingEmitter\ndruid.emitter.logging.logLevel=info\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Emitter in Apache Druid\nDESCRIPTION: Configuration properties for the Logging Emitter module in Druid, which logs metrics using the specified logger class and log level.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/configuration/index.md#2025-04-09_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\ndruid.emitter=logging\ndruid.emitter.logging.loggerClass=LoggingEmitter\ndruid.emitter.logging.logLevel=info\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Comment\nDESCRIPTION: License comment for the mark.js library, indicating its version, website, author, and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Example Druid Segment Identifier without Partition Number\nDESCRIPTION: Shows the format of a Druid segment identifier for the first partition (partition number 0) in a time chunk. The partition number is omitted in this case.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/index.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nclarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool with Specific Options\nDESCRIPTION: Command to run the ResetCluster tool with selective cleanup options. Users can specify which components to reset using flags for metadata store, segment files, task logs, and Hadoop working path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Mark.js License\nDESCRIPTION: MIT license declaration for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Properties for EMR\nDESCRIPTION: YARN site configuration properties for EMR cluster setup, including memory settings, Java options, and task timeout configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nclassification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000]\n```\n\n----------------------------------------\n\nTITLE: Defining ZooKeeper Path for Served Segments in Druid\nDESCRIPTION: This snippet shows the ZooKeeper path where Historical and Realtime processes in Druid create a permanent znode to indicate the segments they are serving.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Network Flow Data in JSON\nDESCRIPTION: Example JSON data representing network flow information, including timestamps, IP addresses, ports, protocol, packets, bytes, and cost.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Metrics Table Structure - Cache Metrics\nDESCRIPTION: Markdown table detailing cache-related metrics including hit rates, entry counts, and error statistics for Druid's caching system.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`*/numEntries`|Number of cache entries.||Varies.|\n|`*/sizeBytes`|Size in bytes of cache entries.||Varies.|\n```\n\n----------------------------------------\n\nTITLE: CSV Example Input for Druid Lookup\nDESCRIPTION: Example CSV input data with three columns that can be used for lookup configuration in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nbar,something,foo\nbat,something2,baz\ntruck,something3,buck\n```\n\n----------------------------------------\n\nTITLE: Retrieving Retention Rules in Druid Coordinator API\nDESCRIPTION: GET endpoints to retrieve retention rules for all datasources or a specific datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_7\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/rules\nGET /druid/coordinator/v1/rules/{dataSourceName}\nGET /druid/coordinator/v1/rules/{dataSourceName}?full\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c344594c.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Converter in JSON\nDESCRIPTION: Configuration for the 'all' event converter implementation that sends all Druid service metrics events to Ambari Metrics. Specifies namespace prefix and application name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"appName\":\"druid\"}\n```\n\n----------------------------------------\n\nTITLE: Mark.js MIT License Declaration\nDESCRIPTION: License declaration for mark.js v8.11.1 created by Julian Khnel, released under the MIT license with a link to the full license text.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Declaring React Scheduler License in JavaScript\nDESCRIPTION: This snippet provides license information for the React scheduler production module version 0.20.2, which is under the MIT license and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Starting Router Node in Java\nDESCRIPTION: Command to start a Druid Router node server process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/router.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server router\n```\n\n----------------------------------------\n\nTITLE: Implementing GroupBy Query with DistinctCount in Druid\nDESCRIPTION: Example of a GroupBy query using DistinctCount aggregator to count unique visitors grouped by a sample dimension. The query covers a single day with 'all' granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/distinctcount.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"sample_datasource\",\n  \"dimensions\": \"[sample_dim]\",\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"distinctCount\",\n      \"name\": \"uv\",\n      \"fieldName\": \"visitor_id\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Send-All Converter in JSON\nDESCRIPTION: Configuration for the 'all' event converter implementation that sends all Druid service metrics events to Ambari Metrics. Specifies namespace prefix and application name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/ambari-metrics-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.emitter.ambari-metrics.eventConverter={\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"appName\":\"druid\"}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Druid Metadata Datasources in HTTP\nDESCRIPTION: This HTTP GET request returns a list of the names of enabled datasources in the Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nGET /druid/coordinator/v1/metadata/datasources\n```\n\n----------------------------------------\n\nTITLE: Sending Wikipedia Sample Data to Tranquility Server\nDESCRIPTION: Commands to decompress the sample Wikipedia edits data and send it to the Tranquility Server via HTTP POST. This demonstrates how to push streaming data into Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz \ncurl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia\n```\n\n----------------------------------------\n\nTITLE: MySQL Connection Setup - Command Line\nDESCRIPTION: Command to connect to MySQL server as root user\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n> mysql -u root\n```\n\n----------------------------------------\n\nTITLE: Configuring YARN Properties for EMR Cluster\nDESCRIPTION: YARN site configuration properties for setting up MapReduce memory, Java options, and task timeout settings in EMR cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nclassification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000]\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for mark.js\nDESCRIPTION: License declaration for mark.js v8.11.1 created by Julian Khnel under MIT license with reference to the license URL.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Last Name First Character Cardinality Example\nDESCRIPTION: Complex example showing cardinality calculation with substring extraction on last names.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/hll-old.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"cardinality\",\n  \"name\": \"distinct_last_name_first_char\",\n  \"fields\": [\n    {\n     \"type\" : \"extraction\",\n     \"dimension\" : \"last_name\",\n     \"outputName\" :  \"last_name_first_char\",\n     \"extractionFn\" : { \"type\" : \"substring\", \"index\" : 0, \"length\" : 1 }\n    }\n  ],\n  \"byRow\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Management Proxy in Druid Router Configuration\nDESCRIPTION: This configuration snippet shows how to enable the management proxy functionality in the Druid Router's runtime.properties file. When set to true, it allows the Router to proxy management requests to the Coordinator and Overlord.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/router.md#2025-04-09_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\ndruid.router.managementProxy.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Simplified Role Information Response\nDESCRIPTION: JSON response format when using ?simplifyPermissions flag, showing streamlined permission structure\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"druidRole2\",\n  \"users\": null,\n  \"permissions\": [\n    {\n      \"resource\": {\n        \"name\": \"E\",\n        \"type\": \"DATASOURCE\"\n      },\n      \"action\": \"WRITE\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Worker Selection Strategy for Druid\nDESCRIPTION: Example of a JavaScript-based worker selection strategy that routes batch index tasks to specific workers and other tasks to different available workers based on capacity usage. This demonstrates custom task routing logic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\":\"javascript\",\n\"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"10.0.0.1\\\");\\nbatch_workers.add(\\\"10.0.0.2\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i < workers.length; i++){\\n sortedWorkers[i] = workers[i];\\n}\\nArray.prototype.sort.call(sortedWorkers,function(a, b){return zkWorkers.get(b).getCurrCapacityUsed() - zkWorkers.get(a).getCurrCapacityUsed();});\\nvar minWorkerVer = config.getMinWorkerVersion();\\nfor (var i = 0; i < sortedWorkers.length; i++) {\\n var worker = sortedWorkers[i];\\n  var zkWorker = zkWorkers.get(worker);\\n  if(zkWorker.canRunTask(task) && zkWorker.isValidVersion(minWorkerVer)){\\n    if(task.getType() == 'index_hadoop' && batch_workers.contains(worker)){\\n      return worker;\\n    } else {\\n      if(task.getType() != 'index_hadoop' && !batch_workers.contains(worker)){\\n        return worker;\\n      }\\n    }\\n  }\\n}\\nreturn null;\\n}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Ingested Data in Apache Druid using dsql\nDESCRIPTION: This bash snippet demonstrates how to use the dsql command-line tool to query the ingested data in Apache Druid. It shows a SELECT * query on the 'ingestion-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/dsql\nWelcome to dsql, the command-line client for Druid SQL.\nType \"\\h\" for help.\ndsql> select * from \"ingestion-tutorial\";\n\n\n __time                    bytes  cost  count  dstIP    dstPort  packets  protocol  srcIP    srcPort \n\n 2018-01-01T01:01:00.000Z   6000   4.9      3  2.2.2.2     3000       60  6         1.1.1.1     2000 \n 2018-01-01T01:02:00.000Z   9000  18.1      2  2.2.2.2     7000       90  6         1.1.1.1     5000 \n 2018-01-01T01:03:00.000Z   6000   4.3      1  2.2.2.2     7000       60  6         1.1.1.1     5000 \n 2018-01-01T02:33:00.000Z  30000  56.9      2  8.8.8.8     5000      300  17        7.7.7.7     4000 \n 2018-01-01T02:35:00.000Z  30000  46.3      1  8.8.8.8     5000      300  17        7.7.7.7     4000 \n\nRetrieved 5 rows in 0.12s.\n\ndsql>\n```\n\n----------------------------------------\n\nTITLE: Processing Check Comment in JavaScript\nDESCRIPTION: This comment suggests a check for previous processing. It's likely used to prevent duplicate execution of certain code blocks or functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.37a11f44.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! Check if previously processed */\n```\n\n----------------------------------------\n\nTITLE: Duration Granularity Configuration in Druid\nDESCRIPTION: Examples of duration granularity specifications with and without origin time.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/granularities.md#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 7200000}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 3600000, \"origin\": \"2012-01-01T00:30:00Z\"}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Comment\nDESCRIPTION: License comment for the React scheduler production file, indicating its version, copyright, and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Running Druid Real-time Server\nDESCRIPTION: Command to start the Druid real-time server process. This process handles real-time data indexing and makes data immediately available for querying.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: Running Druid Real-time Server\nDESCRIPTION: Command to start the Druid real-time server process. This process handles real-time data indexing and makes data immediately available for querying.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/design/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server realtime\n```\n\n----------------------------------------\n\nTITLE: Loading Initial Wikipedia Data - Bash Command\nDESCRIPTION: Command to submit the initial ingestion task that creates 24 hourly segments from Wikipedia sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-compaction.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/compaction-init-index.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Exhibitor Service Settings in Druid\nDESCRIPTION: Configuration properties for connecting Druid to Exhibitor-managed Zookeeper clusters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/configuration/index.md#2025-04-09_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\ndruid.exhibitor.service.hosts=none\ndruid.exhibitor.service.port=8080\ndruid.exhibitor.service.restUriPath=/exhibitor/v1/cluster/list\ndruid.exhibitor.service.useSsl=false\ndruid.exhibitor.service.pollingMs=10000\n```\n\n----------------------------------------\n\nTITLE: Registering a Firehose Factory with Jackson in Druid\nDESCRIPTION: Shows how to implement the getJacksonModules method to register a custom FirehoseFactory with Jackson's polymorphic serialization/deserialization system. This enables the system to load the custom implementation when specified in configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@Override\npublic List<? extends Module> getJacksonModules()\n{\n  return ImmutableList.of(\n          new SimpleModule().registerSubtypes(new NamedType(StaticS3FirehoseFactory.class, \"static-s3\"))\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React Scheduler in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the React Scheduler production build, attributing it to Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Simplified Permission Format Response\nDESCRIPTION: JSON response format when using both ?full and ?simplifyPermissions flags, showing streamlined permission structure\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"druid2\",\n  \"roles\": [\n    {\n      \"name\": \"druidRole\",\n      \"users\": null,\n      \"permissions\": [\n        {\n          \"resource\": {\n            \"name\": \"A\",\n            \"type\": \"DATASOURCE\"\n          },\n          \"action\": \"READ\"\n        },\n        {\n          \"resource\": {\n            \"name\": \"C\",\n            \"type\": \"CONFIG\"\n          },\n          \"action\": \"WRITE\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ZooKeeper Path for Segment Announcements in Druid\nDESCRIPTION: The ZooKeeper path where Historical and Realtime processes create ephemeral znodes for each segment they are serving. Coordinator and Broker processes watch these paths to track segment availability.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/dependencies/zookeeper.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_\n```\n\n----------------------------------------\n\nTITLE: Complete Dimensions Configuration\nDESCRIPTION: Full configuration of dimensions including string and numeric types for network flow data fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      },\n      \"dimensionsSpec\" : {\n        \"dimensions\": [\n          \"srcIP\",\n          { \"name\" : \"srcPort\", \"type\" : \"long\" },\n          { \"name\" : \"dstIP\", \"type\" : \"string\" },\n          { \"name\" : \"dstPort\", \"type\" : \"long\" },\n          { \"name\" : \"protocol\", \"type\" : \"string\" }\n        ]\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Druid Alert JSON Structure\nDESCRIPTION: Example structure of a Druid alert showing the standard fields including timestamp, service name, host, severity level, description, and optional exception data. Alerts can be emitted to log files or HTTP endpoints.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/alerts.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"<alert creation time>\",\n  \"service\": \"<service name>\",\n  \"host\": \"<host name>\",\n  \"severity\": \"<severity level>\",\n  \"description\": \"<alert description>\",\n  \"data\": {\n    \"exceptionType\": \"<type of exception>\",\n    \"exceptionMessage\": \"<exception message>\",\n    \"exceptionStackTrace\": \"<stack trace>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: URI Lookup Configuration Examples\nDESCRIPTION: Example configurations for URI-based lookups using both direct URI and URI prefix with file regex patterns.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/lookups-cached-global.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uri\": \"s3://bucket/some/key/prefix/renames-0003.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"uri\",\n  \"uriPrefix\": \"s3://bucket/some/key/prefix/\",\n  \"fileRegex\":\"renames-[0-9]*\\\\.gz\",\n  \"namespaceParseSpec\":{\n    \"format\":\"csv\",\n    \"columns\":[\"key\",\"value\"]\n  },\n  \"pollPeriod\":\"PT5M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Extensions for Hadoop Integration\nDESCRIPTION: Configuration for enabling necessary Druid extensions including hdfs-storage and kerberos. These extensions are required for Druid to interact with Kerberized Hadoop.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-kerberos-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n#\n# Extensions\n#\n\ndruid.extensions.directory=dist/druid/extensions\ndruid.extensions.hadoopDependenciesDir=dist/druid/hadoop-dependencies\ndruid.extensions.loadList=[\"druid-parser-route\", \"mysql-metadata-storage\", \"druid-hdfs-storage\", \"druid-kerberos\"]\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Information\nDESCRIPTION: License information for React's scheduler.production.min.js (v0.20.2) created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for Prism Syntax Highlighter\nDESCRIPTION: Copyright notice and MIT license information for the Prism syntax highlighting library by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Lunr.js Core Component Declarations\nDESCRIPTION: Copyright and component declarations for the Lunr.js search library including Builder, Index, Pipeline, Set, TokenSet, Vector, stemmer, stopWordFilter, tokenizer, trimmer, and utils modules. Licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8591.9ee7f2b0.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9\n * Copyright (C) 2020 Oliver Nightingale\n * @license MIT\n */\n```\n\n----------------------------------------\n\nTITLE: Running a Peon in Apache Druid\nDESCRIPTION: Command to run a Peon independently for development purposes. It requires a task file containing the task JSON object and a status file to output the task status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/peons.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main internal peon <task_file> <status_file>\n```\n\n----------------------------------------\n\nTITLE: Submitting Ingestion Task for Retention Tutorial in Druid\nDESCRIPTION: Command to post an index task specification that creates hourly segments for Wikipedia edit data. This creates a datasource named 'retention-tutorial' with 24 hourly segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-retention.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/retention-index.json\n```\n\n----------------------------------------\n\nTITLE: Example Query with Basic Post-Aggregation\nDESCRIPTION: Demonstrates a complete query using post-aggregators to calculate averages from aggregated values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\" : [\n    { \"type\" : \"count\", \"name\" : \"rows\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n         ]\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Query with Basic Post-Aggregation\nDESCRIPTION: Demonstrates a complete query using post-aggregators to calculate averages from aggregated values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"aggregations\" : [\n    { \"type\" : \"count\", \"name\" : \"rows\" },\n    { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }\n  ],\n  \"postAggregations\" : [{\n    \"type\"   : \"arithmetic\",\n    \"name\"   : \"average\",\n    \"fn\"     : \"/\",\n    \"fields\" : [\n           { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" },\n           { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" }\n         ]\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Docker Image\nDESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster from the provided Dockerfile.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:2.8.3 .\n```\n\n----------------------------------------\n\nTITLE: Removing Kafka Logs for Reset\nDESCRIPTION: Command to delete the Kafka log directory when resetting the cluster state after completing the Kafka streaming tutorial. This ensures a clean state for future tests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/index.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /tmp/kafka-logs\n```\n\n----------------------------------------\n\nTITLE: Building Hadoop Docker Image\nDESCRIPTION: Commands to build a Docker image for Hadoop 2.8.3 cluster for use with Druid\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:2.8.3 .\n```\n\n----------------------------------------\n\nTITLE: License Notice for React Scheduler\nDESCRIPTION: This snippet contains the license information for the React Scheduler production build, which is part of the React library and is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Data Loading Command for Druid\nDESCRIPTION: Command to load the example data using the post-index-task script.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/rollup-index.json\n```\n\n----------------------------------------\n\nTITLE: Sample Bitmap Output Format from DumpSegment Tool\nDESCRIPTION: Example of bitmap index output from the DumpSegment tool when using the --dump bitmaps option. Shows the bitmap serialization format and encoded bitmap values for column 'isRobot'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/dump-segment.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmapSerdeFactory\": {\n    \"type\": \"concise\"\n  },\n  \"bitmaps\": {\n    \"isRobot\": {\n      \"false\": \"//aExfu+Nv3X...\",\n      \"true\": \"gAl7OoRByQ...\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Query Request Log Example - TSV Format\nDESCRIPTION: Example of a TSV-formatted request log entry for an SQL query showing timestamp, remote address, performance metrics, and SQL query details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/configuration/index.md#2025-04-09_snippet_6\n\nLANGUAGE: tsv\nCODE:\n```\n2019-01-14T10:00:00.000Z        127.0.0.1       {\"sqlQuery/time\":100,\"sqlQuery/bytes\":600,\"success\":true,\"identity\":\"user1\"}  {\"query\":\"SELECT page, COUNT(*) AS Edits FROM wikiticker WHERE __time BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\",\"context\":{\"sqlQueryId\":\"c9d035a0-5ffd-4a79-a865-3ffdadbb5fdd\",\"nativeQueryIds\":\"[490978e4-f5c7-4cf6-b174-346e63cf8863]\"}}\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Druid Distribution to Coordination Server using rsync\nDESCRIPTION: Command to copy the Druid distribution and edited configurations from local machine to the coordination server using rsync. This establishes the initial deployment of Druid on the server infrastructure.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrsync -az apache-druid-0.13.0-incubating/ COORDINATION_SERVER:apache-druid-0.13.0-incubating/\n```\n\n----------------------------------------\n\nTITLE: Document Load Handler Comment\nDESCRIPTION: Comment block indicating functionality to wait for document loading before execution begins.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.0e5d9014.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n   * Wait for document loaded before starting the execution\n   */\n```\n\n----------------------------------------\n\nTITLE: Configuring General TLS Settings in Apache Druid\nDESCRIPTION: This configuration table shows the general TLS settings for Apache Druid, including options to enable/disable HTTP and HTTPS connectors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/tls-support.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.enablePlaintextPort`|Enable/Disable HTTP connector.|`true`|\n|`druid.enableTlsPort`|Enable/Disable HTTPS connector.|`false`|\n```\n\n----------------------------------------\n\nTITLE: Configuring General TLS Settings in Apache Druid\nDESCRIPTION: This configuration table shows the general TLS settings for Apache Druid, including options to enable/disable HTTP and HTTPS connectors.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/tls-support.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|\n|--------|-----------|-------|\n|`druid.enablePlaintextPort`|Enable/Disable HTTP connector.|`true`|\n|`druid.enableTlsPort`|Enable/Disable HTTPS connector.|`false`|\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Notice\nDESCRIPTION: License notice for the React use-sync-external-store-shim production module, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Querying Lookups with Druid SQL\nDESCRIPTION: Example SQL query showing how to use the LOOKUP function to access lookup values in Druid SQL queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1\n```\n\n----------------------------------------\n\nTITLE: Querying Lookups with Druid SQL\nDESCRIPTION: Example SQL query showing how to use the LOOKUP function to access lookup values in Druid SQL queries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1\n```\n\n----------------------------------------\n\nTITLE: Initial Paginated Select Query with fromNext Option in Druid\nDESCRIPTION: Example of an initial paginated Select query with the fromNext option set to false, which requires manual increment of pagingIdentifiers offset in subsequent queries for backward compatibility.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/select-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n {\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: License Declaration for React-is Library\nDESCRIPTION: MIT license declaration for the react-is.production.min.js library, version 16.13.1, developed by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Registering Query Components in Druid\nDESCRIPTION: Demonstrates how to register custom query components using Guice bindings. This example shows binding a QueryToolChest and QueryRunnerFactory for a SegmentMetadataQuery, allowing Druid to use these implementations when executing this query type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/modules.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nDruidBinders.queryToolChestBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryQueryToolChest.class);\n    \nDruidBinders.queryRunnerFactoryBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryRunnerFactory.class);\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Notice\nDESCRIPTION: License notice for the React-DOM production module, version 17.0.2, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Implementing Numeric Column Filter in Druid\nDESCRIPTION: Example of filtering on numeric columns using selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"myFloatColumn\",\n  \"value\": \"10.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Numeric Column Filter in Druid\nDESCRIPTION: Example of filtering on numeric columns using selector filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"selector\",\n  \"dimension\": \"myFloatColumn\",\n  \"value\": \"10.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Protobuf Messages in Kafka\nDESCRIPTION: Command to verify the Protobuf messages were successfully published to the Kafka topic using the Kafka console consumer.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkafka-console-consumer --zookeeper localhost --topic metrics_pb\n```\n\n----------------------------------------\n\nTITLE: Druid Ingestion Task Tuning Configuration\nDESCRIPTION: Tuning configuration that sets maximum rows per segment for optimizing ingestion performance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"maxRowsPerSegment\" : 5000000\n    }\n```\n\n----------------------------------------\n\nTITLE: Accessing Overlord Console URL\nDESCRIPTION: URL pattern for accessing the Overlord Console which shows task and worker information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/management-uis.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<OVERLORD_IP>:<OVERLORD_PORT>/console.html\n```\n\n----------------------------------------\n\nTITLE: Executing GroupBy Query with Variance Aggregator and Standard Deviation Post-Aggregator in Apache Druid\nDESCRIPTION: JSON configuration for a GroupBy query using the variance aggregator and standard deviation post-aggregator. It specifies the query type, data source, dimensions, granularity, aggregations, post-aggregations, and time interval.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Network Flow Data in JSON Format\nDESCRIPTION: Sample network flow data in JSON format that contains fields such as srcIP, dstIP, srcPort, dstPort, protocol, packets, bytes, and cost. The data represents IP traffic information between different hosts.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4}\n{\"ts\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":20, \"bytes\":2000, \"cost\": 3.1}\n{\"ts\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":30, \"bytes\":3000, \"cost\": 0.4}\n{\"ts\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":40, \"bytes\":4000, \"cost\": 7.9}\n{\"ts\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":50, \"bytes\":5000, \"cost\": 10.2}\n{\"ts\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":60, \"bytes\":6000, \"cost\": 4.3}\n{\"ts\":\"2018-01-01T02:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\", \"srcPort\":4000, \"dstPort\":5000, \"protocol\": 17, \"packets\":100, \"bytes\":10000, \"cost\": 22.4}\n{\"ts\":\"2018-01-01T02:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\", \"srcPort\":4000, \"dstPort\":5000, \"protocol\": 17, \"packets\":200, \"bytes\":20000, \"cost\": 34.5}\n{\"ts\":\"2018-01-01T02:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\", \"srcPort\":4000, \"dstPort\":5000, \"protocol\": 17, \"packets\":300, \"bytes\":30000, \"cost\": 46.3}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for NProgress\nDESCRIPTION: Copyright and license declaration for the NProgress library, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring QuantilesDoublesSketchToQuantiles Post Aggregator\nDESCRIPTION: JSON configuration for the quantilesDoublesSketchToQuantiles post aggregator, which returns an array of quantiles corresponding to a given array of fractional positions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/datasketches-quantiles.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"quantilesDoublesSketchToQuantiles\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that refers to a DoublesSketch (fieldAccess or another post aggregator)>,\n  \"fractions\" : <array of fractional positions in the hypothetical sorted stream, number from 0 to 1 inclusive>\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka topic for Wikipedia data in Bash\nDESCRIPTION: Command to create a new Kafka topic named 'wikipedia' with a single partition and replication factor of 1, using the local Zookeeper instance.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wikipedia\n```\n\n----------------------------------------\n\nTITLE: Disabling Segments by ID in Druid\nDESCRIPTION: cURL command to disable segments using their segment IDs via the Coordinator API.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-disable-segments.json http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Deep Storage\nDESCRIPTION: Configuration properties for setting up S3 as the deep storage layer for Druid segments and indexing logs\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-s3-extensions\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=s3\ndruid.storage.bucket=your-bucket\ndruid.storage.baseKey=druid/segments\ndruid.s3.accessKey=...\ndruid.s3.secretKey=...\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=s3\ndruid.indexer.logs.s3Bucket=your-bucket\ndruid.indexer.logs.s3Prefix=druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Example JavaScript Aggregator in Druid JSON\nDESCRIPTION: Provides an example of a JavaScript aggregator that computes the sum of log(x)*y + 10. It demonstrates how to define the aggregate, combine, and reset functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/aggregations.md#2025-04-09_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"sum(log(x)*y) + 10\",\n  \"fieldNames\": [\"x\", \"y\"],\n  \"fnAggregate\" : \"function(current, a, b)      { return current + (Math.log(a) * b); }\",\n  \"fnCombine\"   : \"function(partialA, partialB) { return partialA + partialB; }\",\n  \"fnReset\"     : \"function()                   { return 10; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Logic in Druid using Java\nDESCRIPTION: QueryResource.java serves as the entry point for tracing Druid's query logic. The query engine utilizes a system of nested query runners, each adding a layer of functionality to the query processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/overview.md#2025-04-09_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nQueryResource.java\n```\n\n----------------------------------------\n\nTITLE: Rolled Up Network Traffic Data\nDESCRIPTION: Example data after rollup aggregation, showing how records are consolidated at minute-level granularity with summed metrics.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/index.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000\n2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000\n2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Adding workingPath to tuningConfig in JSON\nDESCRIPTION: JSON configuration for the tuningConfig section of a Hadoop indexing spec. This shows how to set the workingPath parameter which determines where intermediate results between Hadoop jobs will be stored.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n  \"tuningConfig\" : {\n   ...\n    \"workingPath\": \"/tmp\",\n    ...\n  }\n```\n\n----------------------------------------\n\nTITLE: Declaring License for React DOM in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the React DOM production minified file, version 17.0.2, created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query with Filtered DimensionSpec for Multi-value Dimension in Druid\nDESCRIPTION: Illustrates a GroupBy query using a filtered dimensionSpec to filter the 'tags' dimension after explosion. This approach allows for more precise filtering of multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test\",\n  \"intervals\": [\n    \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"\n  ],\n  \"filter\": {\n    \"type\": \"selector\",\n    \"dimension\": \"tags\",\n    \"value\": \"t3\"\n  },\n  \"granularity\": {\n    \"type\": \"all\"\n  },\n  \"dimensions\": [\n    {\n      \"type\": \"listFiltered\",\n      \"delegate\": {\n        \"type\": \"default\",\n        \"dimension\": \"tags\",\n        \"outputName\": \"tags\"\n      },\n      \"values\": [\"t3\"]\n    }\n  ],\n  \"aggregations\": [\n    {\n      \"type\": \"count\",\n      \"name\": \"count\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Druid Module Class Example\nDESCRIPTION: Example of how to register a custom Druid module in the META-INF/services file. This configuration allows Druid to discover and load the custom module at runtime.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\norg.apache.druid.storage.cassandra.CassandraDruidModule\n```\n\n----------------------------------------\n\nTITLE: Running the Command Line Hadoop Indexer in Bash\nDESCRIPTION: Command to execute the Hadoop indexer from the command line. It requires setting the Java heap size, timezone, file encoding, classpath with Hadoop configuration directory, and specifying the indexer class and spec file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/command-line-hadoop-indexer.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*:<hadoop_config_dir> org.apache.druid.cli.Main index hadoop <spec_file>\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Broadcast Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period broadcast rule, which specifies how segments of different data sources should be co-located in Historical processes for a rolling time period.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByPeriod\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"period\" : \"P1M\",\n  \"includeFuture\" : true\n}\n```\n\n----------------------------------------\n\nTITLE: Syncing Druid Distribution Files\nDESCRIPTION: Command to copy Druid distribution and configuration files to the coordination server using rsync\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrsync -az apache-druid-0.14.2-incubating/ COORDINATION_SERVER:apache-druid-0.14.2-incubating/\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Deep Storage in Druid common.runtime.properties\nDESCRIPTION: This snippet shows the configuration for setting up HDFS as deep storage for Druid segments and indexing logs. It specifies the storage type and directory paths for segments and indexing logs in HDFS.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-kerberos-hadoop.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n# Deep storage\n#\n# For HDFS:\n druid.storage.type=hdfs\n druid.storage.storageDirectory=/druid/segments\n# OR\n# druid.storage.storageDirectory=/apps/druid/segments\n\n#\n# Indexing service logs\n#\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n# OR\n# druid.storage.storageDirectory=/apps/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Notice\nDESCRIPTION: License notice for the React-DOM production module, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Transform Spec Basic Syntax in JSON\nDESCRIPTION: The basic syntax for the transformSpec object which includes optional transforms list and filter properties. Transforms are applied to input rows, and the filter determines which rows will be ingested.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"transformSpec\": {\n  \"transforms: <List of transforms>,\n  \"filter\": <filter>\n}\n```\n\n----------------------------------------\n\nTITLE: Transform Spec Basic Syntax in JSON\nDESCRIPTION: The basic syntax for the transformSpec object which includes optional transforms list and filter properties. Transforms are applied to input rows, and the filter determines which rows will be ingested.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/transform-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"transformSpec\": {\n  \"transforms: <List of transforms>,\n  \"filter\": <filter>\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Ingestion Metrics Table in Markdown\nDESCRIPTION: A markdown table showing various ingestion metrics for realtime processes in Apache Druid, including metric names, descriptions, dimensions, and normal values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Metric|Description|Dimensions|Normal Value|\n|------|-----------|----------|------------|\n|`ingest/events/thrownAway`|Number of events rejected because they are outside the windowPeriod.|dataSource, taskId, taskType.|0|\n|`ingest/events/unparseable`|Number of events rejected because the events are unparseable.|dataSource, taskId, taskType.|0|\n|`ingest/events/duplicate`|Number of events rejected because the events are duplicated.|dataSource, taskId, taskType.|0|\n|`ingest/events/processed`|Number of events successfully processed per emission period.|dataSource, taskId, taskType.|Equal to your # of events per emission period|\n|`ingest/rows/output`|Number of Druid rows persisted.|dataSource, taskId, taskType.|Your # of events with rollup|\n|`ingest/persists/count`|Number of times persist occurred.|dataSource, taskId, taskType.|Depends on configuration|\n|`ingest/persists/time`|Milliseconds spent doing intermediate persist.|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/persists/cpu`|Cpu time in Nanoseconds spent on doing intermediate persist.|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/persists/backPressure`|Milliseconds spent creating persist tasks and blocking waiting for them to finish.|dataSource, taskId, taskType.|0 or very low|\n|`ingest/persists/failed`|Number of persists that failed.|dataSource, taskId, taskType.|0|\n|`ingest/handoff/failed`|Number of handoffs that failed.|dataSource, taskId, taskType.|0|\n|`ingest/merge/time`|Milliseconds spent merging intermediate segments|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/merge/cpu`|Cpu time in Nanoseconds spent on merging intermediate segments.|dataSource, taskId, taskType.|Depends on configuration. Generally a few minutes at most|\n|`ingest/handoff/count`|Number of handoffs that happened.|dataSource, taskId, taskType.|Varies. Generally greater than 0 once every segment granular period if cluster operating normally|\n|`ingest/sink/count`|Number of sinks not handoffed.|dataSource, taskId, taskType.|1~3|\n|`ingest/events/messageGap`|Time gap between the data time in event and current system time.|dataSource, taskId, taskType.|Greater than 0, depends on the time carried in event|\n```\n\n----------------------------------------\n\nTITLE: Object-assign License Notice\nDESCRIPTION: License notice for the object-assign library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Displaying Integration Diagram in Markdown\nDESCRIPTION: This snippet uses Markdown syntax to display an image illustrating the integration of Druid with other technologies in a production environment. The image shows the flow from event streams through message bus and stream processor to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/integrating-druid-with-other-technologies.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<img src=\"../../img/druid-production.png\" width=\"800\"/>\n```\n\n----------------------------------------\n\nTITLE: Declaring License for React Is in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the React Is production minified file, version 16.13.1, created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Middle Manager Status Check Endpoint\nDESCRIPTION: HTTP GET endpoint to check the enabled/disabled status of a Middle Manager node.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/rolling-updates.md#2025-04-09_snippet_1\n\nLANGUAGE: http\nCODE:\n```\n<MiddleManager_IP:PORT>/druid/worker/v1/enabled\n```\n\n----------------------------------------\n\nTITLE: Generating Protobuf Descriptor File\nDESCRIPTION: Command to generate the Protobuf descriptor file from the proto definition. The descriptor file is required by Druid's Protobuf parser for interpreting the Protobuf messages.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nprotoc -o /tmp/metrics.desc metrics.proto\n```\n\n----------------------------------------\n\nTITLE: Sample Network Flow Event Data in JSON\nDESCRIPTION: Example dataset containing network flow events with timestamp, source IP, destination IP, packet count, and byte count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"timestamp\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":20,\"bytes\":9024\n}\n{\n\"timestamp\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":255,\"bytes\":21133\n}\n{\n\"timestamp\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":11,\"bytes\":5780\n}\n{\n\"timestamp\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":38,\"bytes\":6289\n}\n{\n\"timestamp\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":377,\"bytes\":359971\n}\n{\n\"timestamp\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":49,\"bytes\":10204\n}\n{\n\"timestamp\":\"2018-01-02T21:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":38,\"bytes\":6289\n}\n{\n\"timestamp\":\"2018-01-02T21:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":123,\"bytes\":93999\n}\n{\n\"timestamp\":\"2018-01-02T21:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":12,\"bytes\":2818\n}\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Example with Variance Aggregator and Standard Deviation Post-Aggregator in Apache Druid\nDESCRIPTION: Example of a GroupBy query using both the variance aggregator and standard deviation post-aggregator in Apache Druid. Illustrates how to use these aggregators in a GroupBy query configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/stats.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"testing\",\n  \"dimensions\": [\"alias\"],\n  \"granularity\": \"all\",\n  \"aggregations\": [\n    {\n      \"type\": \"variance\",\n      \"name\": \"index_var\",\n      \"fieldName\": \"index\"\n    }\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"stddev\",\n      \"name\": \"index_stddev\",\n      \"fieldName\": \"index_var\"\n    }\n  ],\n  \"intervals\": [\n    \"2016-03-06T00:00:00/2016-03-06T23:59:59\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Timeseries Query Context Parameters in Markdown Table\nDESCRIPTION: This markdown table defines the query context parameter specific to Timeseries queries, including the skipEmptyBuckets property.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/query-context.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default              | description          |\n|-----------------|---------------------|----------------------|\n|skipEmptyBuckets | `false`             | Disable timeseries zero-filling behavior, so only buckets with results will be returned. |\n```\n\n----------------------------------------\n\nTITLE: JavaScript Post-Aggregator in Druid\nDESCRIPTION: JSON structure for a JavaScript post-aggregator that applies a custom JavaScript function to the given fields. Fields are passed as arguments to the function in the specified order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": <output_name>,\n  \"fieldNames\" : [<aggregator_name>, <aggregator_name>, ...],\n  \"function\": <javascript function>\n}\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for React DOM\nDESCRIPTION: License declaration for React's react-dom.production.min.js v17.0.2 created by Facebook under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Loading On-heap Guava Cache in Apache Druid\nDESCRIPTION: This JSON configuration demonstrates a loading lookup using an on-heap Guava cache. It specifies the lookup type, data fetcher, and separate cache specifications for forward and reverse lookups.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-lookups.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"loadingLookup\",\n   \"dataFetcher\":{ \"type\":\"jdbcDataFetcher\", \"connectorConfig\":\"jdbc://mysql://localhost:3306/my_data_base\", \"table\":\"lookup_table_name\", \"keyColumn\":\"key_column_name\", \"valueColumn\": \"value_column_name\"},\n   \"loadingCacheSpec\":{\"type\":\"guava\"},\n   \"reverseLoadingCacheSpec\":{\"type\":\"guava\", \"maximumSize\":500000, \"expireAfterAccess\":100000, \"expireAfterAccess\":10000}\n}\n```\n\n----------------------------------------\n\nTITLE: Failed Tranquility Server Response Format\nDESCRIPTION: Example JSON response showing that events were received but not sent to Druid, indicating a potential issue that requires retrying the send command.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"result\":{\"received\":39244,\"sent\":0}}\n```\n\n----------------------------------------\n\nTITLE: Defining Jekyll Front Matter for Apache Druid Papers Page\nDESCRIPTION: This YAML front matter block sets the layout and title for a Jekyll-based documentation page about Apache Druid papers and presentations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/misc/papers-and-talks.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) Papers\"\n---\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Frontmatter for Batch Data Ingestion Documentation\nDESCRIPTION: This code snippet defines the frontmatter for a Markdown document about batch data ingestion in Apache Druid. It specifies the layout and title of the page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/batch-ingestion.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Batch Data Ingestion\"\n---\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Services\nDESCRIPTION: Command to start all Druid services using the supervisor script with tutorial configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/index.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf\n```\n\n----------------------------------------\n\nTITLE: Java Memory Buffer Size Command Example\nDESCRIPTION: Sample Java option command to set maximum direct memory size for Druid processing buffers.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/configuration/index.md#2025-04-09_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n[\"-XX:OnOutOfMemoryError=kill -9 %p\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Front Matter for Apache Druid Papers Page\nDESCRIPTION: This snippet defines the front matter for the Markdown page, specifying the layout and title for the Apache Druid papers documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/misc/papers-and-talks.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) Papers\"\n---\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for NProgress Library\nDESCRIPTION: This snippet contains the copyright notice and license information for the NProgress library by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Result from GroupBy Query with Selector Filter\nDESCRIPTION: The response from a GroupBy query with a selector filter for 't3' includes all tag values from rows that have 't3' as one of their values. This shows how filtering happens before multi-value dimensions are exploded.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t1\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t2\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t3\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t4\"\n    }\n  },\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 1,\n      \"tags\": \"t5\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for React DOM\nDESCRIPTION: This comment block provides license information for the React DOM production build, which is released under the MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Registering a DruidModule in META-INF services file\nDESCRIPTION: A plain text example of how to register a Druid module in the META-INF/services directory. The file contains a list of fully-qualified class names that implement DruidModule, one per line.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\norg.apache.druid.storage.cassandra.CassandraDruidModule\n```\n\n----------------------------------------\n\nTITLE: Using Kinit for Kerberos Authentication Before Making Curl Requests\nDESCRIPTION: Command to authenticate with Kerberos using kinit before accessing Druid HTTP endpoints. This uses a keytab file to obtain Kerberos credentials.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkinit -k -t <path_to_keytab_file> user@REALM.COM\n```\n\n----------------------------------------\n\nTITLE: Implementing Fragment Search in Druid\nDESCRIPTION: Specifies a search query that matches if a dimension value contains all specified fragments, with optional case sensitivity control.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/searchqueryspec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"type\" : \"fragment\",\n  \"case_sensitive\" : false,\n  \"values\" : [\"fragment1\", \"fragment2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Maven Dependencies for Druid Extensions\nDESCRIPTION: Maven POM configuration showing required Druid extension dependencies\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-avro-extensions</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-parquet-extensions</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>druid-hdfs-storage</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.druid.extensions</groupId>\n      <artifactId>mysql-metadata-storage</artifactId>\n      <version>${project.parent.version}</version>\n  </dependency>\n```\n\n----------------------------------------\n\nTITLE: Time Parsing Extraction Function Configuration in JSON\nDESCRIPTION: Configuration for parsing dimension values as timestamps using specified input and output formats.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"time\",\n  \"timeFormat\" : <input_format>,\n  \"resultFormat\" : <output_format>,\n  \"joda\" : <true, false> }\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Declaration\nDESCRIPTION: MIT license declaration for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Querying Rolled-up Data with Druid SQL\nDESCRIPTION: SQL query to retrieve all data from the rolled-up dataset, demonstrating the effects of summarization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"rollup-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Querying Rolled-up Data with Druid SQL\nDESCRIPTION: SQL query to retrieve all data from the rolled-up dataset, demonstrating the effects of summarization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"rollup-tutorial\";\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running insert-segment-to-db Tool with MySQL and S3\nDESCRIPTION: This example shows how to run the insert-segment-to-db tool with MySQL as metadata storage and S3 as deep storage. It includes the necessary Java VM arguments, S3 configuration, and command-line options.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\njava\n-Ddruid.metadata.storage.type=mysql \n-Ddruid.metadata.storage.connector.connectURI=jdbc\\:mysql\\://localhost\\:3306/druid \n-Ddruid.metadata.storage.connector.user=druid \n-Ddruid.metadata.storage.connector.password=diurd\n-Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\",\\\"druid-s3-extensions\\\"]\n-Ddruid.storage.type=s3\n-Ddruid.s3.accessKey=... \n-Ddruid.s3.secretKey=...\n-Ddruid.storage.bucket=your-bucket\n-Ddruid.storage.baseKey=druid/storage/wikipedia\n-Ddruid.storage.maxListingLength=1000\n-cp $DRUID_CLASSPATH\norg.apache.druid.cli.Main tools insert-segment-to-db --workingDir \"druid/storage/wikipedia\" --updateDescriptor true\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Comment\nDESCRIPTION: License comment for the React-DOM production file, indicating its version, copyright, and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Using Case Statements in Druid Expressions\nDESCRIPTION: Demonstrates the use of case statements in Druid expressions. Two variants are shown: case_searched for conditional logic and case_simple for value matching.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/misc/math-expr.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ncase_searched(expr1, result1, [[expr2, result2, ...], else-result])\n```\n\nLANGUAGE: SQL\nCODE:\n```\ncase_simple(expr, value1, result1, [[value2, result2, ...], else-result])\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c344594c.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Index Task in Apache Druid (JSON)\nDESCRIPTION: Example JSON configuration for a Local Index Task in Apache Druid, specifying the data schema, IO config, and tuning config for ingesting Wikipedia data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/native_tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"index\",\n  \"spec\" : {\n    \"dataSchema\" : {\n      \"dataSource\" : \"wikipedia\",\n      \"parser\" : {\n        \"type\" : \"string\",\n        \"parseSpec\" : {\n          \"format\" : \"json\",\n          \"timestampSpec\" : {\n            \"column\" : \"timestamp\",\n            \"format\" : \"auto\"\n          },\n          \"dimensionsSpec\" : {\n            \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"],\n            \"dimensionExclusions\" : [],\n            \"spatialDimensions\" : []\n          }\n        }\n      },\n      \"metricsSpec\" : [\n        {\n          \"type\" : \"count\",\n          \"name\" : \"count\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"added\",\n          \"fieldName\" : \"added\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"deleted\",\n          \"fieldName\" : \"deleted\"\n        },\n        {\n          \"type\" : \"doubleSum\",\n          \"name\" : \"delta\",\n          \"fieldName\" : \"delta\"\n        }\n      ],\n      \"granularitySpec\" : {\n        \"type\" : \"uniform\",\n        \"segmentGranularity\" : \"DAY\",\n        \"queryGranularity\" : \"NONE\",\n        \"intervals\" : [ \"2013-08-31/2013-09-01\" ]\n      }\n    },\n    \"ioConfig\" : {\n      \"type\" : \"index\",\n      \"firehose\" : {\n        \"type\" : \"local\",\n        \"baseDir\" : \"examples/indexing/\",\n        \"filter\" : \"wikipedia_data.json\"\n       }\n    },\n    \"tuningConfig\" : {\n      \"type\" : \"index\",\n      \"targetPartitionSize\" : 5000000,\n      \"maxRowsInMemory\" : 1000000\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Cache Properties in Druid\nDESCRIPTION: This snippet shows the configuration options for the Redis cache in Druid's common.runtime.properties file. It includes settings for Redis server connection, cache entry expiration, timeouts, and connection pool management.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-contrib/redis-cache.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.cache.host=None\ndruid.cache.port=None\ndruid.cache.expiration=24 * 3600 * 1000\ndruid.cache.timeout=2000\ndruid.cache.maxTotalConnections=8\ndruid.cache.maxIdleConnections=8\ndruid.cache.minIdleConnections=0\n```\n\n----------------------------------------\n\nTITLE: Sending Wikipedia data to Kafka topic in Bash\nDESCRIPTION: Commands to configure Kafka encoding options and use the console producer to send the sample Wikipedia data to the Kafka topic, which will then be picked up by Druid's Kafka indexing service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-kafka.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport KAFKA_OPTS=\"-Dfile.encoding=UTF-8\"\n./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia < {PATH_TO_DRUID}/quickstart/tutorial/wikiticker-2015-09-12-sampled.json\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Result with Pacific Timezone\nDESCRIPTION: Example result from the GroupBy query showing data grouped by day in Pacific timezone. Demonstrates how records from the same Pacific timezone day are grouped together.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/granularities.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-31T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T00:00:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 2,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Custom Buckets Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Custom Buckets post-aggregator, which computes a visual representation with user-defined break points.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"customBuckets\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"breaks\" : [ <value>, <value>, ... ] }\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: MIT license declaration for React's react.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Classnames License Declaration\nDESCRIPTION: MIT license header for the classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.df5a69b6.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Lunr.js Library Copyright Notice\nDESCRIPTION: This snippet provides the main copyright notice for the Lunr.js library. It includes the version number, a brief description of the library, and licensing information.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/4611.46940362.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9\n * Copyright (C) 2020 Oliver Nightingale\n * @license MIT\n */\n```\n\n----------------------------------------\n\nTITLE: Loading MomentSketch Extension in Druid Config\nDESCRIPTION: Configuration to load the druid-momentsketch extension in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/momentsketch-quantiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.extensions.loadList=[\"druid-momentsketch\"]\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Supervisor POST Endpoint (Deprecated)\nDESCRIPTION: Deprecated REST endpoint for shutting down a supervisor. Use terminate endpoint instead.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_12\n\nLANGUAGE: http\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/shutdown\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Documentation Page\nDESCRIPTION: YAML front matter block defining the documentation page layout and title for a comparison between Apache Druid and Elasticsearch.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/comparisons/druid-vs-elasticsearch.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Apache Druid (incubating) vs Elasticsearch\"\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Deep Storage in Druid\nDESCRIPTION: Configuration changes in common.runtime.properties file to set up S3 as deep storage for Druid segments and indexing service logs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/cluster.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.extensions.loadList=[\"druid-s3-extensions\"]\n\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\ndruid.storage.type=s3\ndruid.storage.bucket=your-bucket\ndruid.storage.baseKey=druid/segments\ndruid.s3.accessKey=...\ndruid.s3.secretKey=...\n\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\ndruid.indexer.logs.type=s3\ndruid.indexer.logs.s3Bucket=your-bucket\ndruid.indexer.logs.s3Prefix=druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Broadcast Rule in Druid\nDESCRIPTION: An Interval Broadcast Rule configuration that instructs Druid to co-locate segments from different data sources within a specific time interval. This rule ensures segments within the interval are replicated appropriately.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByInterval\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Min Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Min post-aggregator, which returns the minimum value of the underlying approximate histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"min\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name> }\n```\n\n----------------------------------------\n\nTITLE: Executing Pull-deps with Default Version\nDESCRIPTION: Example command demonstrating how to use the --defaultVersion flag to specify a common version for multiple extensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/pull-deps.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njava -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --defaultVersion 0.14.1-incubating --clean -c org.apache.druid.extensions:mysql-metadata-storage -c org.apache.druid.extensions.contrib:druid-rabbitmq -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Hash-based Partitioning\nDESCRIPTION: Configuration for hash-based partitioning strategy that distributes data across segments based on dimension hashes. Supports either targetPartitionSize or numShards for controlling partition count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"hashed\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Including Apache License 2.0 Comment in HTML\nDESCRIPTION: This HTML comment includes the full text of the Apache License 2.0, which is applied to the content of this documentation page.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/misc/papers-and-talks.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for React Core\nDESCRIPTION: Copyright notice and MIT license information for React's react.production.min.js core module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Result from GroupBy Query with Filtered DimensionSpec\nDESCRIPTION: The response from a GroupBy query with both a selector filter and filtered dimension specification shows only the specific tag value 't3' with its count. This demonstrates how to get precisely targeted results from multi-value dimensions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"1970-01-01T00:00:00.000Z\",\n    \"event\": {\n      \"count\": 2,\n      \"tags\": \"t3\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: React Core License Attribution\nDESCRIPTION: MIT License attribution for React core production build v17.0.2, the main library for building user interfaces.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JSDoc\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Apache Druid License Header\nDESCRIPTION: Standard Apache 2.0 License header comment block used in the documentation file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/versioning.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Running Apache Druid MiddleManager Server\nDESCRIPTION: Command to start the MiddleManager server process in Apache Druid. The MiddleManager is responsible for executing submitted tasks by forwarding them to Peons that run in separate JVMs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/middlemanager.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Broadcast Rule in Druid\nDESCRIPTION: An Interval Broadcast Rule configuration that instructs Druid to co-locate segments from different data sources within a specific time interval. This rule ensures segments within the interval are replicated appropriately.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"broadcastByInterval\",\n  \"colocatedDataSources\" : [ \"target_source1\", \"target_source2\" ],\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: Role Users List Response\nDESCRIPTION: JSON response showing users assigned to a role when using the ?full flag\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"users\":[\"druid\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Inline Schema Avro Bytes Decoder in JSON\nDESCRIPTION: This JSON snippet shows the configuration for an inline schema-based Avro bytes decoder. It includes the decoder type and a sample Avro schema definition.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/avro.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"schema_inline\",\n  \"schema\": {\n    \"namespace\": \"org.apache.druid.data\",\n    \"name\": \"User\",\n    \"type\": \"record\",\n    \"fields\": [\n      { \"name\": \"FullName\", \"type\": \"string\" },\n      { \"name\": \"Country\", \"type\": \"string\" }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React DOM in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the React DOM production build, attributing it to Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Executing Druid SQL Query using JDBC (Java)\nDESCRIPTION: Demonstrates how to execute a Druid SQL query using the Avatica JDBC driver in Java, including connection setup and result processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Druid SQL Query using JDBC (Java)\nDESCRIPTION: Demonstrates how to execute a Druid SQL query using the Avatica JDBC driver in Java, including connection setup and result processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n// Connect to /druid/v2/sql/avatica/ on your broker.\nString url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\";\n\n// Set any connection context parameters you need here (see \"Connection context\" below).\n// Or leave empty for default behavior.\nProperties connectionProperties = new Properties();\n\ntry (Connection connection = DriverManager.getConnection(url, connectionProperties)) {\n  try (\n      final Statement statement = connection.createStatement();\n      final ResultSet resultSet = statement.executeQuery(query)\n  ) {\n    while (resultSet.next()) {\n      // Do something\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Authentication for Kinesis in Druid\nDESCRIPTION: Demonstrates how to provide AWS access and secret keys via runtime.properties for authenticating with AWS Kinesis. If not provided, Druid will use credentials from environment variables, default profile configuration file, or EC2 instance profile provider in that order.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\n-Ddruid.kinesis.accessKey=123 -Ddruid.kinesis.secretKey=456\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighting License Comment\nDESCRIPTION: License comment for the Prism syntax highlighting library, which is licensed under the MIT License and created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6f6dba15.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Standard Averager Definition Example in Druid\nDESCRIPTION: Example of a standard averager definition in a Druid moving average query. This shows the basic structure for defining an averager of type doubleMean.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleMean\", \"name\" : <output_name>, \"fieldName\": <input_name> }\n```\n\n----------------------------------------\n\nTITLE: React Scheduler Copyright Notice\nDESCRIPTION: Copyright notice for the React scheduler component, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Copying Sample Data to Shared Directory\nDESCRIPTION: Command to copy the WikiTicker sample data file to the shared directory for later use in Hadoop.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncp quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz /tmp/shared/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTSDB Emitter Metrics in JSON\nDESCRIPTION: This JSON snippet demonstrates how to define desired metrics and dimensions for the OpenTSDB Emitter. It shows an example configuration for the 'query/time' metric, specifying 'dataSource' and 'type' as dimensions to be included.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/opentsdb-emitter.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"query/time\": [\n    \"dataSource\",\n    \"type\"\n]\n```\n\n----------------------------------------\n\nTITLE: Python Kafka Producer for Protobuf Messages\nDESCRIPTION: Python script that reads JSON input and publishes Protobuf-encoded messages to Kafka topic.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\n\nimport sys\nimport json\n\nfrom kafka import KafkaProducer\nfrom metrics_pb2 import Metrics\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ntopic = 'metrics_pb'\nmetrics = Metrics()\n\nfor row in iter(sys.stdin):\n    d = json.loads(row)\n    for k, v in d.items():\n        setattr(metrics, k, v)\n    pb = metrics.SerializeToString()\n    producer.send(topic, pb)\n```\n\n----------------------------------------\n\nTITLE: GroupBy Query Result with Custom Origin Granularity\nDESCRIPTION: This snippet shows the result of a groupBy query with a custom origin set for the granularity. It illustrates how the custom origin affects the bucketing of data, resulting in different timestamp boundaries for each event.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/granularities.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-29T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-30T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-01T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-09-02T20:30:00.000-07:00\",\n  \"event\" : {\n    \"count\" : 1,\n    \"language\" : \"en\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeMax Aggregator at Ingestion in Druid\nDESCRIPTION: JSON configuration for including a timeMax aggregator during data ingestion in Druid. The fieldName typically refers to the column specified in the timestamp spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMax\",\n    \"name\": \"tmax\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license declaration for React's scheduler.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Declaration\nDESCRIPTION: Copyright notice and MIT license declaration for the object-assign library by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for object-assign Library in JavaScript\nDESCRIPTION: Copyright notice for the object-assign library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Initializing Document Load Wait in JavaScript\nDESCRIPTION: This comment indicates that the script should wait for the document to be fully loaded before starting execution. It's a common practice to ensure all DOM elements are available before manipulating them.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.37a11f44.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\\n   * Wait for document loaded before starting the execution\\n   */\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Declaration\nDESCRIPTION: MIT license declaration for React's scheduler.production.min.js v0.20.2 library created by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Core License Declaration in JavaScript\nDESCRIPTION: License declaration for React's react.production.min.js version 17.0.2, created by Facebook. The library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Sample Error Response from Apache Druid Query\nDESCRIPTION: This JSON snippet shows the structure of an error response from Apache Druid when a query fails. It includes fields for error code, error message, error class, and the host where the error occurred.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/querying.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\" : \"Query timeout\",\n  \"errorMessage\" : \"Timeout waiting for task.\",\n  \"errorClass\" : \"java.util.concurrent.TimeoutException\",\n  \"host\" : \"druid1.example.com:8083\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Lookups with Druid SQL\nDESCRIPTION: Example of using the LOOKUP function in a Druid SQL query to apply a lookup named 'lookup-name' to a column.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/lookups.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT LOOKUP(column_name, 'lookup-name'), COUNT(*) FROM datasource GROUP BY 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-dimension Partitioning\nDESCRIPTION: Configuration for dimension-based partitioning that segments data based on ranges of a single dimension. Allows specifying target partition size and optional dimension selection.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"dimension\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: mark.js License Declaration in JavaScript\nDESCRIPTION: License declaration for mark.js version 8.11.1, created by Julian Khnel. The library is used for marking/highlighting and is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for mark.js Library\nDESCRIPTION: This snippet provides the copyright and license information for the mark.js library, which is used for highlighting and marking text on web pages. It is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Duration Granularity Configuration\nDESCRIPTION: Examples of duration granularity specifications in Druid using millisecond durations\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/granularities.md#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 7200000}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\"type\": \"duration\", \"duration\": 3600000, \"origin\": \"2012-01-01T00:30:00Z\"}\n```\n\n----------------------------------------\n\nTITLE: Classnames MIT License Declaration\nDESCRIPTION: License declaration for classnames library created by Jed Watson under MIT license with link to repository.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Running a Peon in Apache Druid (Java)\nDESCRIPTION: Command to run a Peon independently for development purposes. It requires a task file containing the task JSON object and a status file to output the task status.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/design/peons.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main internal peon <task_file> <status_file>\n```\n\n----------------------------------------\n\nTITLE: Example groupBy Query with subtotalsSpec in Apache Druid\nDESCRIPTION: Demonstrates how to use the subtotalsSpec feature in groupBy queries to compute multiple sub-groupings in a single query, with custom dimension specifications and extraction functions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/groupbyquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"type\": \"groupBy\",\n ...\n ...\n\"dimensions\": [\n  {\n  \"type\" : \"default\",\n  \"dimension\" : \"d1col\",\n  \"outputName\": \"D1\"\n  },\n  {\n  \"type\" : \"extraction\",\n  \"dimension\" : \"d2col\",\n  \"outputName\" :  \"D2\",\n  \"extractionFn\" : extraction_func\n  },\n  {\n  \"type\":\"lookup\",\n  \"dimension\":\"d3col\",\n  \"outputName\":\"D3\",\n  \"name\":\"my_lookup\"\n  }\n],\n...\n...\n\"subtotalsSpec\":[ [\"D1\", \"D2\", D3\"], [\"D1\", \"D3\"], [\"D3\"]],\n..\n\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Math Functions Table in Markdown\nDESCRIPTION: This snippet shows a markdown table listing various mathematical functions available in Apache Druid. Each row contains the function name and a brief description of its operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/misc/math-expr.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|name|description|\n|----|-----------|\n|abs|abs(x) would return the absolute value of x|\n|acos|acos(x) would return the arc cosine of x|\n|asin|asin(x) would return the arc sine of x|\n|atan|atan(x) would return the arc tangent of x|\n|atan2|atan2(y, x) would return the angle theta from the conversion of rectangular coordinates (x, y) to polar * coordinates (r, theta)|\n|cbrt|cbrt(x) would return the cube root of x|\n|ceil|ceil(x) would return the smallest (closest to negative infinity) double value that is greater than or equal to x and is equal to a mathematical integer|\n|copysign|copysign(x) would return the first floating-point argument with the sign of the second floating-point argument|\n|cos|cos(x) would return the trigonometric cosine of x|\n|cosh|cosh(x) would return the hyperbolic cosine of x|\n|div|div(x,y) is integer division of x by y|\n|exp|exp(x) would return Euler's number raised to the power of x|\n|expm1|expm1(x) would return e^x-1|\n|floor|floor(x) would return the largest (closest to positive infinity) double value that is less than or equal to x and is equal to a mathematical integer|\n|getExponent|getExponent(x) would return the unbiased exponent used in the representation of x|\n|hypot|hypot(x, y) would return sqrt(x^2+y^2) without intermediate overflow or underflow|\n|log|log(x) would return the natural logarithm of x|\n|log10|log10(x) would return the base 10 logarithm of x|\n|log1p|log1p(x) would the natural logarithm of x + 1|\n|max|max(x, y) would return the greater of two values|\n|min|min(x, y) would return the smaller of two values|\n|nextafter|nextafter(x, y) would return the floating-point number adjacent to the x in the direction of the y|\n|nextUp|nextUp(x) would return the floating-point value adjacent to x in the direction of positive infinity|\n|pow|pow(x, y) would return the value of the x raised to the power of y|\n|remainder|remainder(x, y) would return the remainder operation on two arguments as prescribed by the IEEE 754 standard|\n|rint|rint(x) would return value that is closest in value to x and is equal to a mathematical integer|\n|round|round(x) would return the closest long value to x, with ties rounding up|\n|scalb|scalb(d, sf) would return d * 2^sf rounded as if performed by a single correctly rounded floating-point multiply to a member of the double value set|\n|signum|signum(x) would return the signum function of the argument x|\n|sin|sin(x) would return the trigonometric sine of an angle x|\n|sinh|sinh(x) would return the hyperbolic sine of x|\n|sqrt|sqrt(x) would return the correctly rounded positive square root of x|\n|tan|tan(x) would return the trigonometric tangent of an angle x|\n|tanh|tanh(x) would return the hyperbolic tangent of x|\n|todegrees|todegrees(x) converts an angle measured in radians to an approximately equivalent angle measured in degrees|\n|toradians|toradians(x) converts an angle measured in degrees to an approximately equivalent angle measured in radians|\n|ulp|ulp(x) would return the size of an ulp of the argument x|\n```\n\n----------------------------------------\n\nTITLE: Defining Protobuf Message Structure with Proto3\nDESCRIPTION: Protocol Buffer definition file that defines the Metrics message structure. This proto file is used to generate the descriptor file required by Druid's Protobuf parser.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\nmessage Metrics {\n  string unit = 1;\n  string http_method = 2;\n  int32 value = 3;\n  string timestamp = 4;\n  string http_code = 5;\n  string page = 6;\n  string metricType = 7;\n  string server = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Tranquility Kafka for Druid Stream Ingestion (Deprecated)\nDESCRIPTION: Command to start Tranquility Kafka with a configuration file. This method is deprecated in favor of the Kafka Indexing Service.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/stream-push.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/tranquility kafka -configFile <path_to_config_file>/kafka.json\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for Prism\nDESCRIPTION: Copyright and license declaration for the Prism syntax highlighting library, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Querying INFORMATION_SCHEMA for Column Metadata (SQL)\nDESCRIPTION: Illustrates how to retrieve column metadata for a specific Druid datasource using the INFORMATION_SCHEMA.COLUMNS table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'\n```\n\n----------------------------------------\n\nTITLE: Querying INFORMATION_SCHEMA for Column Metadata (SQL)\nDESCRIPTION: Illustrates how to retrieve column metadata for a specific Druid datasource using the INFORMATION_SCHEMA.COLUMNS table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo'\n```\n\n----------------------------------------\n\nTITLE: Classnames License Header\nDESCRIPTION: MIT license header for the classnames package by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.832012d1.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: License Comment for React\nDESCRIPTION: Copyright and license information for the React production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Prism License Comment\nDESCRIPTION: License comment for the Prism syntax highlighting library, indicating its purpose, MIT license, author, and namespace.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for React DOM in JavaScript\nDESCRIPTION: Copyright notice for the React DOM production build, version 17.0.2, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React-Is License Comment\nDESCRIPTION: License comment for the React-Is production module, version 16.13.1, released under the MIT license by Facebook, Inc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Example Timestamp Dataset in Apache Druid\nDESCRIPTION: Sample dataset showing timestamp, dimension, and metric values that would be indexed in Druid. This illustrates the kind of data where timestamp min/max aggregators would be useful.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2015-07-28T01:00:00.000Z  A  1\n2015-07-28T02:00:00.000Z  A  1\n2015-07-28T03:00:00.000Z  A  1\n2015-07-28T04:00:00.000Z  B  1\n2015-07-28T05:00:00.000Z  A  1\n2015-07-28T06:00:00.000Z  B  1\n2015-07-29T01:00:00.000Z  C  1\n2015-07-29T02:00:00.000Z  C  1\n2015-07-29T03:00:00.000Z  A  1\n2015-07-29T04:00:00.000Z  A  1\n```\n\n----------------------------------------\n\nTITLE: Adding Jersey Resources in Druid\nDESCRIPTION: Shows how to add a new Jersey REST resource to a Druid module. The Jerseys utility class provides a helper method to bind the resource in Guice.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nJerseys.addResource(binder, NewResource.class);\n```\n\n----------------------------------------\n\nTITLE: Time Format Extraction Function Configuration in JSON\nDESCRIPTION: Configuration for formatting time dimension values according to specified format, timezone, and locale settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/dimensionspecs.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"timeFormat\",\n  \"format\" : <output_format>,\n  \"timeZone\" : <time_zone>,\n  \"locale\" : <locale>,\n  \"granularity\" : <granularity>,\n  \"asMillis\" : <true or false> }\n```\n\n----------------------------------------\n\nTITLE: Defining Protobuf Message Structure with Proto3\nDESCRIPTION: Protocol Buffer definition file that defines the Metrics message structure. This proto file is used to generate the descriptor file required by Druid's Protobuf parser.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nsyntax = \"proto3\";\nmessage Metrics {\n  string unit = 1;\n  string http_method = 2;\n  int32 value = 3;\n  string timestamp = 4;\n  string http_code = 5;\n  string page = 6;\n  string metricType = 7;\n  string server = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: MIT license header for react-is.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Numbered Sharding in Druid TuningConfig\nDESCRIPTION: Example of configuring numbered sharding strategy in Druid's TuningConfig. Numbered sharding requires sequential partition numbering and explicit specification of total partitions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/stream-pull.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"shardSpec\": {\n    \"type\": \"numbered\",\n    \"partitionNum\": 0,\n    \"partitions\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Declaration\nDESCRIPTION: Copyright notice and MIT license declaration for the mark.js library by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Querying Appended Data in Druid with GroupBy\nDESCRIPTION: SQL GroupBy query demonstrating how duplicate rows are combined at query time, even when stored in separate segments.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select __time, animal, SUM(\"count\"), SUM(\"number\") from \"updates-tutorial\" group by __time, animal;\n\n __time                    animal    EXPR$2  EXPR$3 \n\n 2018-01-01T01:01:00.000Z  lion           2     400 \n 2018-01-01T03:01:00.000Z  aardvark       1    9999 \n 2018-01-01T04:01:00.000Z  bear           2     333 \n 2018-01-01T05:01:00.000Z  mongoose       1     737 \n 2018-01-01T06:01:00.000Z  snake          1    1234 \n 2018-01-01T07:01:00.000Z  octopus        1     115 \n 2018-01-01T09:01:00.000Z  falcon         1    1241 \n\nRetrieved 7 rows in 0.23s.\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for React Scheduler\nDESCRIPTION: Copyright notice and MIT license information for React's scheduler.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: js-yaml License Header\nDESCRIPTION: License and attribution header for js-yaml library version 4.1.0, released under MIT license\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.19bec321.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Query Server\nDESCRIPTION: Command to start the Broker process on Query servers\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/cluster.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/broker/jvm.config | xargs` -cp conf/druid/_common:conf/druid/broker:lib/* org.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Defining Dimensions Without Rollup in Apache Druid\nDESCRIPTION: This snippet shows how to specify all columns as dimensions in the dimensionsSpec when not using rollup functionality. It includes specifying data types for each dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeMax Aggregator for Druid Ingestion\nDESCRIPTION: JSON configuration for adding a timeMax aggregator during data ingestion in Druid. The aggregator calculates the maximum timestamp for the specified field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMax\",\n    \"name\": \"tmax\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: React use-sync-external-store License Header\nDESCRIPTION: Copyright notice and MIT license declaration for the React use-sync-external-store-shim module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Time Boundary Query Response Format in Apache Druid\nDESCRIPTION: This snippet shows the expected response format for a time boundary query in Apache Druid. It includes the timestamp and result object containing the minTime and maxTime values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/timeboundaryquery.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"timestamp\" : \"2013-05-09T18:24:00.000Z\",\n  \"result\" : {\n    \"minTime\" : \"2013-05-09T18:24:00.000Z\",\n    \"maxTime\" : \"2013-05-09T18:37:00.000Z\"\n  }\n} ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Promise/A+ Specification in JavaScript\nDESCRIPTION: A minimal strictly-compliant implementation of the Promises/A+ 1.1.1 Thenable specification. Created by Ralf S. Engelschall and licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8055.09efec4d.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n  Embeddable Minimum Strictly-Compliant Promises/A+ 1.1.1 Thenable\n  Copyright (c) 2013-2014 Ralf S. Engelschall (http://engelschall.com)\n  Licensed under The MIT License (http://opensource.org/licenses/MIT)\n  */\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Coordinator and Overlord on Master Server\nDESCRIPTION: Launches the Druid Coordinator and Overlord services on the Master server using Java commands.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/cluster.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njava `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* org.apache.druid.cli.Main server coordinator\njava `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* org.apache.druid.cli.Main server overlord\n```\n\n----------------------------------------\n\nTITLE: Using In Filter in Druid Queries\nDESCRIPTION: The In filter selects rows where a dimension value matches any value in a specified set. It's equivalent to SQL's IN operator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"in\",\n    \"dimension\": \"outlaw\",\n    \"values\": [\"Good\", \"Bad\", \"Ugly\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Object-assign MIT License Declaration\nDESCRIPTION: License declaration for object-assign library created by Sindre Sorhus under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Get All Lookups Response Example in JSON\nDESCRIPTION: Example JSON response for a GET request to retrieve all active lookups on a Druid process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/lookups.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"site_id_customer2\": {\n    \"version\": \"v1\",\n    \"lookupExtractorFactory\": {\n      \"type\": \"map\",\n      \"map\": {\n        \"AHF77\": \"Home\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: License Notice for React Scheduler\nDESCRIPTION: This snippet contains the license information for React's scheduler production build (v0.20.2), which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: JSON Request Payload for Marking Segments\nDESCRIPTION: Example JSON payload structure for marking segments as unused in Druid. The payload can contain either an interval or a list of segment IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"interval\": \"2015-09-12T03:00:00.000Z/2015-09-12T05:00:00.000Z\",\n  \"segmentIds\": [\"segmentId1\", \"segmentId2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Request Payload for Marking Segments\nDESCRIPTION: Example JSON payload structure for marking segments as unused in Druid. The payload can contain either an interval or a list of segment IDs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/api-reference.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"interval\": \"2015-09-12T03:00:00.000Z/2015-09-12T05:00:00.000Z\",\n  \"segmentIds\": [\"segmentId1\", \"segmentId2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: String Concatenation in Druid SQL\nDESCRIPTION: Example of string concatenation using the double pipe operator in Druid SQL. This operator joins two strings together.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nx || y\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Information\nDESCRIPTION: License information for React's use-sync-external-store-shim.production.min.js module created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Comment\nDESCRIPTION: License comment for the React use-sync-external-store-shim production file, indicating its copyright and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0ba9c98a.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Sending Data to Tranquility Server\nDESCRIPTION: Commands to decompress and send sample Wikipedia data to Tranquility Server via HTTP POST.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-tranquility.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngunzip -k quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz \ncurl -XPOST -H'Content-Type: application/json' --data-binary @quickstart/tutorial/wikiticker-2015-09-12-sampled.json http://localhost:8200/v1/post/wikipedia\n```\n\n----------------------------------------\n\nTITLE: Including RocketMQ Extension in Apache Druid\nDESCRIPTION: Instructions for including the RocketMQ extension in an Apache Druid installation. Users need to ensure they include the 'druid-rocketmq' extension to use RocketMQ functionality with Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/rocketmq.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo use this Apache Druid (incubating) extension, make sure to [include](../../operations/including-extensions.html) `druid-rocketmq` extension.\n```\n\n----------------------------------------\n\nTITLE: Binding Query Components in Druid Dependency Injection\nDESCRIPTION: Shows how to bind QueryToolChest and QueryRunnerFactory implementations for a new query type (SegmentMetadataQuery) using DruidBinders in the configure method of a module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/modules.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nDruidBinders.queryToolChestBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryQueryToolChest.class);\n    \nDruidBinders.queryRunnerFactoryBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryRunnerFactory.class);\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Format Extraction Function in Druid\nDESCRIPTION: The Time Format extraction function formats dimension values according to specified format string, time zone, and locale. For __time dimension values, it formats the time value bucketed by the aggregation granularity.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"timeFormat\",\n  \"format\" : <output_format> (optional),\n  \"timeZone\" : <time_zone> (optional, default UTC),\n  \"locale\" : <locale> (optional, default current locale),\n  \"granularity\" : <granularity> (optional, default none) },\n  \"asMillis\" : <true or false> (optional) }\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license header for React's scheduler.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Timeseries Query with Grand Totals in Apache Druid\nDESCRIPTION: Example of a timeseries query that includes the 'grandTotal' context parameter to request an additional row with totals across all time intervals in the result set.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/timeseriesquery.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"timeseries\",\n  \"dataSource\": \"sample_datasource\",\n  \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ],\n  \"granularity\": \"day\",\n  \"aggregations\": [\n    { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" },\n    { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" }\n  ],\n  \"context\": {\n    \"grandTotal\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: React Is Copyright Notice\nDESCRIPTION: Copyright notice for the React Is library, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Using Custom Origin with Period Granularity in groupBy Query\nDESCRIPTION: This JavaScript snippet shows how to configure period granularity with both a timezone and a custom origin. The origin parameter sets the starting point for the granularity buckets, which affects how records are grouped when they fall near bucket boundaries.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/granularities.md#2025-04-09_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n   \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"}\n```\n\n----------------------------------------\n\nTITLE: React-Is License Notice\nDESCRIPTION: License notice for the React-Is production module, version 16.13.1, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress Copyright Notice\nDESCRIPTION: Copyright notice for the NProgress library by Rico Sta. Cruz, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Metadata Storage for SQLServer\nDESCRIPTION: This code snippet shows the configuration properties needed to set up Microsoft SQLServer as the metadata storage for Apache Druid. It specifies the storage type, connection URI, user, and password.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/sqlserver.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=sqlserver\ndruid.metadata.storage.connector.connectURI=jdbc:sqlserver://<host>;databaseName=druid\ndruid.metadata.storage.connector.user=druid\ndruid.metadata.storage.connector.password=diurd\n```\n\n----------------------------------------\n\nTITLE: Configuring timeMax Aggregator at Ingestion Time in Druid\nDESCRIPTION: JSON configuration for the timeMax aggregator that should be included during data ingestion to enable calculating the maximum timestamp of events.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMax\",\n    \"name\": \"tmax\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: License Comment for React DOM\nDESCRIPTION: Copyright and license information for the React DOM production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Shade Plugin for Jackson Package Relocation\nDESCRIPTION: This XML configuration for the Maven Shade plugin demonstrates how to relocate Jackson packages and create a fat jar to resolve dependency conflicts. It includes settings for shading, relocation, and artifact filtering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<plugin>\n    <groupId>org.apache.maven.plugins</groupId>\n    <artifactId>maven-shade-plugin</artifactId>\n    <executions>\n        <execution>\n            <phase>package</phase>\n            <goals>\n                <goal>shade</goal>\n            </goals>\n            <configuration>\n                <outputFile>\n                    ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                </outputFile>\n                <relocations>\n                    <relocation>\n                        <pattern>com.fasterxml.jackson</pattern>\n                        <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                    </relocation>\n                </relocations>\n                <artifactSet>\n                    <includes>\n                        <include>*:*</include>\n                    </includes>\n                </artifactSet>\n                <filters>\n                    <filter>\n                        <artifact>*:*</artifact>\n                        <excludes>\n                            <exclude>META-INF/*.SF</exclude>\n                            <exclude>META-INF/*.DSA</exclude>\n                            <exclude>META-INF/*.RSA</exclude>\n                        </excludes>\n                    </filter>\n                </filters>\n                <transformers>\n                    <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                </transformers>\n            </configuration>\n        </execution>\n    </executions>\n</plugin>\n```\n\n----------------------------------------\n\nTITLE: License Comment for React Is\nDESCRIPTION: Copyright and license information for the React Is production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Existing Druid Datasource\nDESCRIPTION: Command to submit a task that appends new data to the existing 'updates-tutorial' datasource without overwriting, using appendToExisting=true.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index2.json\n```\n\n----------------------------------------\n\nTITLE: Parsing and Formatting Timestamps in Druid SQL\nDESCRIPTION: Demonstrates the use of TIME_PARSE and TIME_FORMAT functions to convert between string representations and timestamp values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nTIME_PARSE(<string_expr>, [<pattern>, [<timezone>]])\nTIME_FORMAT(<timestamp_expr>, [<pattern>, [<timezone>]])\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Declaration\nDESCRIPTION: MIT license declaration for mark.js v8.11.1, a library for highlighting text created by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Basic Authenticator\nDESCRIPTION: Configuration properties for setting up a Basic HTTP Authenticator in Druid. Defines the authenticator chain, authentication type, initial passwords, and authorizer reference.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyBasicAuthenticator\"]\n\ndruid.auth.authenticator.MyBasicAuthenticator.type=basic\ndruid.auth.authenticator.MyBasicAuthenticator.initialAdminPassword=password1\ndruid.auth.authenticator.MyBasicAuthenticator.initialInternalClientPassword=password2\ndruid.auth.authenticator.MyBasicAuthenticator.authorizerName=MyBasicAuthorizer\n```\n\n----------------------------------------\n\nTITLE: Adding Jersey Resource\nDESCRIPTION: Code snippet showing how to bind a new Jersey resource in a Druid module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nJerseys.addResource(binder, NewResource.class);\n```\n\n----------------------------------------\n\nTITLE: Adding Jersey Resource\nDESCRIPTION: Code snippet showing how to bind a new Jersey resource in a Druid module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nJerseys.addResource(binder, NewResource.class);\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authenticator in Druid\nDESCRIPTION: Basic configuration for setting up a Kerberos authenticator in Druid's authentication chain.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ndruid.auth.authenticatorChain=[\"MyKerberosAuthenticator\"]\n\ndruid.auth.authenticator.MyKerberosAuthenticator.type=kerberos\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a03dfc13.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React External Store License Header\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0ba9c98a.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Using TIME_EXTRACT Function in Druid SQL\nDESCRIPTION: Examples of using the TIME_EXTRACT function to extract time parts from a timestamp expression. The function accepts a timestamp expression, an optional unit parameter, and an optional timezone parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/sql.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nTIME_EXTRACT(__time, 'HOUR')\n```\n\nLANGUAGE: SQL\nCODE:\n```\nTIME_EXTRACT(__time, 'HOUR', 'America/Los_Angeles')\n```\n\n----------------------------------------\n\nTITLE: Adding Jersey Resources to Druid\nDESCRIPTION: Shows how to bind a new Jersey resource to make it available in Druid's REST API. This simple binding makes the resource available through Druid's HTTP endpoints.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/modules.md#2025-04-09_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nJerseys.addResource(binder, NewResource.class);\n```\n\n----------------------------------------\n\nTITLE: Adding SBT Assembly Plugin in Scala\nDESCRIPTION: This code adds the SBT Assembly plugin, which is used to create a fat jar that excludes conflicting Jackson dependencies.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Lower Case Extraction Function in Apache Druid\nDESCRIPTION: Shows the configuration of a Lower Case extraction function without a specified locale. This function converts dimension values to lowercase using the default locale of the Java Virtual Machine.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"lower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for React Core in JavaScript\nDESCRIPTION: Copyright notice for the React core production build, version 17.0.2, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for React DOM\nDESCRIPTION: This snippet contains the copyright notice and license information for the React DOM production file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e831e4fb.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for use-sync-external-store-shim\nDESCRIPTION: This comment block provides license information for the use-sync-external-store-shim production build, which is part of React and released under the MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Timeseries Query Context in Apache Druid\nDESCRIPTION: A markdown table displaying the specific query context parameter for Timeseries queries in Apache Druid. It includes the parameter name, default value, and description.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/query-context.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|property         |default              | description          |\n|-----------------|---------------------|----------------------|\n|skipEmptyBuckets | `false`             | Disable timeseries zero-filling behavior, so only buckets with results will be returned. |\n```\n\n----------------------------------------\n\nTITLE: Prism License Header\nDESCRIPTION: MIT license header for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fea5e92a.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: License Declaration for React use-sync-external-store-shim Library\nDESCRIPTION: MIT license declaration for the use-sync-external-store-shim.production.min.js React library, developed by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Viewing Overwritten Data in Druid\nDESCRIPTION: SQL query showing the result after an overwrite operation where the data has been completely replaced with new values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndsql> select * from \"updates-tutorial\";\n\n __time                    animal    count  number \n\n 2018-01-01T01:01:00.000Z  lion          1     100 \n 2018-01-01T03:01:00.000Z  aardvark      1    9999 \n 2018-01-01T04:01:00.000Z  bear          1     111 \n\nRetrieved 3 rows in 0.02s.\n```\n\n----------------------------------------\n\nTITLE: Connecting to MySQL from the terminal\nDESCRIPTION: Command to connect to MySQL as the root user from the local machine.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n> mysql -u root\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeMin Aggregator for Druid Ingestion\nDESCRIPTION: JSON configuration for adding a timeMin aggregator during data ingestion in Druid. The aggregator calculates the minimum timestamp for the specified field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/time-min-max.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"timeMin\",\n    \"name\": \"tmin\",\n    \"fieldName\": \"<field_name, typically column specified in timestamp spec>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for Prism in JavaScript\nDESCRIPTION: This snippet declares the MIT license for Prism, a syntax highlighting library, attributing it to Lea Verou and providing a link to the author's website and the license text.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Hadoop Dependencies Directory Structure\nDESCRIPTION: Example directory structure showing how to organize different versions of Hadoop client dependencies for Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/other-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhadoop-dependencies/\n hadoop-client\n     2.3.0\n        activation-1.1.jar\n        avro-1.7.4.jar\n        commons-beanutils-1.7.0.jar\n        commons-beanutils-core-1.8.0.jar\n        commons-cli-1.2.jar\n        commons-codec-1.4.jar\n    ..... lots of jars\n     2.4.0\n         activation-1.1.jar\n         avro-1.7.4.jar\n         commons-beanutils-1.7.0.jar\n         commons-beanutils-core-1.8.0.jar\n         commons-cli-1.2.jar\n         commons-codec-1.4.jar\n    ..... lots of jars\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Accessing the Legacy Coordinator Console (Version 1)\nDESCRIPTION: URL pattern for accessing the oldest version of Druid's Coordinator console, maintained for backwards compatibility.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/management-uis.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>/old-console\n```\n\n----------------------------------------\n\nTITLE: Specifying Result Format in Druid SQL Query (JSON)\nDESCRIPTION: Demonstrates how to specify a result format for a Druid SQL query using the 'resultFormat' parameter in a JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/sql.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.14f7867a.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Bucket Extraction Function in Druid\nDESCRIPTION: Configuration for bucketing numerical values into ranges of specified size with custom offset. Converts numbers within each range to the same base value.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"bucket\",\n  \"size\" : 5,\n  \"offset\" : 2\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet-Avro Ingestion with Avro ParseSpec\nDESCRIPTION: This JSON configuration illustrates how to set up Hadoop-based indexing to ingest Parquet files using the 'parquet-avro' parser and 'avro' parseSpec. It includes settings for input format, flatten spec, timestamp spec, and dimensions spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet-avro\",\n        \"parseSpec\": {\n          \"format\": \"avro\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet-Avro Ingestion with Avro ParseSpec\nDESCRIPTION: This JSON configuration illustrates how to set up Hadoop-based indexing to ingest Parquet files using the 'parquet-avro' parser and 'avro' parseSpec. It includes settings for input format, flatten spec, timestamp spec, and dimensions spec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/parquet.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_hadoop\",\n  \"spec\": {\n    \"ioConfig\": {\n      \"type\": \"hadoop\",\n      \"inputSpec\": {\n        \"type\": \"static\",\n        \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat\",\n        \"paths\": \"path/to/file.parquet\"\n      },\n      ...\n    },\n    \"dataSchema\": {\n      \"dataSource\": \"example\",\n      \"parser\": {\n        \"type\": \"parquet-avro\",\n        \"parseSpec\": {\n          \"format\": \"avro\",\n          \"flattenSpec\": {\n            \"useFieldDiscovery\": true,\n            \"fields\": [\n              {\n                \"type\": \"path\",\n                \"name\": \"nestedDim\",\n                \"expr\": \"$.nestedData.dim1\"\n              },\n              {\n                \"type\": \"path\",\n                \"name\": \"listDimFirstItem\",\n                \"expr\": \"$.listDim[1]\"\n              }\n            ]\n          },\n          \"timestampSpec\": {\n            \"column\": \"timestamp\",\n            \"format\": \"auto\"\n          },\n          \"dimensionsSpec\": {\n            \"dimensions\": [],\n            \"dimensionExclusions\": [],\n            \"spatialDimensions\": []\n          }\n        }\n      },\n      ...\n    },\n    \"tuningConfig\": <hadoop-tuning-config>\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Druid database and user in MySQL\nDESCRIPTION: SQL commands to create a Druid database with UTF-8 encoding, create a Druid user with password, and grant all necessary permissions to that user.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/mysql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- create a druid database, make sure to use utf8mb4 as encoding\nCREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;\n\n-- create a druid user\nCREATE USER 'druid'@'localhost' IDENTIFIED BY 'diurd';\n\n-- grant the user all the permissions on the database we just created\nGRANT ALL PRIVILEGES ON druid.* TO 'druid'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for the mark.js library by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c344594c.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: React-Is License\nDESCRIPTION: Copyright notice and MIT license declaration for the react-is.production.min.js file from React.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.20dca3d5.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Querying Grouped Data in Apache Druid\nDESCRIPTION: This SQL query demonstrates how to group and aggregate data in the 'updates-tutorial' datasource after various updates have been applied.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect __time, animal, SUM(\"count\"), SUM(\"number\") from \"updates-tutorial\" group by __time, animal;\n```\n\n----------------------------------------\n\nTITLE: Registering Query Components with Guice\nDESCRIPTION: Example of how to register a new Query type's supporting classes with Druid's dependency injection system. Binds the QueryToolChest and QueryRunnerFactory for a custom query type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/modules.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nDruidBinders.queryToolChestBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryQueryToolChest.class);\n    \nDruidBinders.queryRunnerFactoryBinder(binder)\n            .addBinding(SegmentMetadataQuery.class)\n            .to(SegmentMetadataQueryRunnerFactory.class);\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid's Storage Format in Java\nDESCRIPTION: The storage format for data in Druid is a custom column format called segments. The Column.java class and its extensions are key to understanding this storage format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/overview.md#2025-04-09_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nColumn.java\n```\n\n----------------------------------------\n\nTITLE: Configuring Day of Week Filter on Timestamp Column in Druid (JSON)\nDESCRIPTION: This example demonstrates how to configure a filter for day of week on the timestamp column in a Druid query. It uses a time format extraction function.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/filters.md#2025-04-09_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"selector\",\n  \"dimension\": \"__time\",\n  \"value\": \"Friday\",\n  \"extractionFn\": {\n    \"type\": \"timeFormat\",\n    \"format\": \"EEEE\",\n    \"timeZone\": \"America/New_York\",\n    \"locale\": \"en\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.7d317958.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Declaring License for Prism in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the Prism syntax highlighting library, created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Implementing Arithmetic Post-Aggregator in Druid\nDESCRIPTION: Defines an arithmetic post-aggregator that applies mathematical functions to aggregated fields. Supports operations like +, -, *, /, and quotient with optional numeric ordering.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/post-aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arithmetic\",\n  \"name\"  : <output_name>,\n  \"fn\"    : <arithmetic_function>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...],\n  \"ordering\" : <null (default), or \"numericFirst\">\n}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler MIT License Declaration\nDESCRIPTION: License declaration for React's scheduler.production.min.js (v0.20.2) created by Facebook, Inc. and its affiliates under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React-Is License Attribution\nDESCRIPTION: MIT License attribution for React's react-is production build, used for type checking React elements.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JSDoc\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Extraction DimensionSpec Configuration in Druid\nDESCRIPTION: Configuration for transforming dimension values using custom extraction functions with type conversion support.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/dimensionspecs.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"extraction\",\n  \"dimension\" : <dimension>,\n  \"outputName\" :  <output_name>,\n  \"outputType\": <\"STRING\"|\"LONG\"|\"FLOAT\">,\n  \"extractionFn\" : <extraction_function>\n}\n```\n\n----------------------------------------\n\nTITLE: License Notice for React use-sync-external-store-shim\nDESCRIPTION: This snippet contains the license information for the use-sync-external-store-shim production build of React, which is part of the React library and is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for React Scheduler in JavaScript\nDESCRIPTION: Copyright notice for the React Scheduler production build, version 0.20.2, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Jackson Dependency Conflict Error in Java\nDESCRIPTION: This snippet shows the error message that occurs due to Jackson dependency conflicts between CDH and Druid in a Mapreduce job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_4\n\nLANGUAGE: java\nCODE:\n```\njava.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n```\n\n----------------------------------------\n\nTITLE: Defining IndexSpec Fields in Markdown\nDESCRIPTION: This snippet defines the fields, types, descriptions, and requirements for IndexSpec using a Markdown table format.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/kinesis-ingestion.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|Field|Type|Description|Required|\n|-----|----|-----------|--------|\n|bitmap|Object|Compression format for bitmap indexes. Should be a JSON object; see below for options.|no (defaults to Concise)|\n|dimensionCompression|String|Compression format for dimension columns. Choose from `LZ4`, `LZF`, or `uncompressed`.|no (default == `LZ4`)|\n|metricCompression|String|Compression format for metric columns. Choose from `LZ4`, `LZF`, `uncompressed`, or `none`.|no (default == `LZ4`)|\n|longEncoding|String|Encoding format for metric and dimension columns with type long. Choose from `auto` or `longs`. `auto` encodes the values using sequence number or lookup table depending on column cardinality, and store them with variable size. `longs` stores the value as is with 8 bytes each.|no (default == `longs`)|\n```\n\n----------------------------------------\n\nTITLE: React Scheduler MIT License Declaration\nDESCRIPTION: License declaration for React's scheduler.production.min.js (v0.20.2) created by Facebook, Inc. and its affiliates under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Sample Network Flow Data in JSON\nDESCRIPTION: Example network flow data containing IP addresses, ports, protocol numbers, and traffic metrics like packets, bytes and cost.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.832012d1.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Segment Serving Path in ZooKeeper\nDESCRIPTION: The permanent ZooKeeper path where nodes create their served segments directory.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/dependencies/zookeeper.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n${druid.zk.paths.servedSegmentsPath}/${druid.host}\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependency Coordinates in Task Configuration\nDESCRIPTION: JSON snippet showing how to specify a particular version of Hadoop client libraries to use in a Hadoop Index Task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster Tool with Specific Options in Druid\nDESCRIPTION: Command to run the ResetCluster tool with specific options to selectively reset different parts of a Druid cluster. This allows targeting specific components like metadata store, segment files, task logs, or Hadoop working paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0d1c82da.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n*****************************************************/\n\n```\n\n----------------------------------------\n\nTITLE: Quantile Post-Aggregator for Approximate Histogram in Druid\nDESCRIPTION: JSON configuration for the Quantile post-aggregator, which computes a single quantile based on the underlying approximate histogram aggregator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"quantile\", \"name\" : <output_name>, \"fieldName\" : <aggregator_name>,\n  \"probability\" : <quantile> }\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependency Coordinates in Task Configuration\nDESCRIPTION: JSON snippet showing how to specify a particular version of Hadoop client libraries to use in a Hadoop Index Task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Specifying Hadoop Dependency Coordinates in Task Configuration\nDESCRIPTION: JSON snippet showing how to specify a particular version of Hadoop client libraries to use in a Hadoop Index Task.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"]\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration\nDESCRIPTION: YAML front matter block defining the Jekyll page layout and title configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/versioning.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Versioning Apache Druid (incubating)\"\n---\n```\n\n----------------------------------------\n\nTITLE: NProgress License Notice\nDESCRIPTION: License notice for the NProgress library, created by Rico Sta. Cruz and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: License notice for the React Scheduler production build, version 0.20.2, released under the MIT license by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object-assign License Comment\nDESCRIPTION: License comment for the object-assign library, indicating its author and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Generating Python Protobuf Bindings\nDESCRIPTION: Command to generate Python bindings from the Protobuf definition file, which creates a metrics_pb2.py file needed for the Kafka producer script.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/protobuf.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nprotoc -o metrics.desc metrics.proto --python_out=.\n```\n\n----------------------------------------\n\nTITLE: Implementing Deep Storage Bindings in Java\nDESCRIPTION: Example showing how to bind custom DataSegmentPusher and DataSegmentPuller implementations for HDFS storage in a Druid module using Guice bindings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/modules.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nBinders.dataSegmentPullerBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPuller.class).in(LazySingleton.class);\n\nBinders.dataSegmentPusherBinder(binder)\n       .addBinding(\"hdfs\")\n       .to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for NProgress\nDESCRIPTION: This comment block provides license and copyright information for NProgress, a progress bar library created by Rico Sta. Cruz and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Initializing Druid DataSchema with DataSource\nDESCRIPTION: The beginning of a Druid ingestion spec, defining the dataSchema and specifying the dataSource name.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n}\n```\n\n----------------------------------------\n\nTITLE: License Comment for React\nDESCRIPTION: Copyright and license information for the React production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Classnames License Header\nDESCRIPTION: MIT license header for the classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e5087fc9.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Submit Ingestion Task Command\nDESCRIPTION: Bash command to submit the transform index task to Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/tutorials/tutorial-transform-spec.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/transform-index.json\n```\n\n----------------------------------------\n\nTITLE: Object-Assign MIT License Declaration\nDESCRIPTION: License declaration for the object-assign library created by Sindre Sorhus under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Constructing a Paginated Select Query with fromNext in Apache Druid\nDESCRIPTION: This JSON structure demonstrates how to construct a paginated Select query with the 'fromNext' option set to false. This option is used for backwards compatibility with older versions of Druid that required manual offset incrementation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/select-query.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"queryType\": \"select\",\n   \"dataSource\": \"wikipedia\",\n   \"descending\": \"false\",\n   \"dimensions\":[],\n   \"metrics\":[],\n   \"granularity\": \"all\",\n   \"intervals\": [\n     \"2013-01-01/2013-01-02\"\n   ],\n   \"pagingSpec\":{\"fromNext\": \"false\", \"pagingIdentifiers\": {}, \"threshold\":5}\n }\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React Is\nDESCRIPTION: Specifies the MIT license for the React Is production module (version 16.13.1).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Use Sync External Store License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0d1c82da.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Executing Druid Query via HTTP - Bash\nDESCRIPTION: cURL command to submit a native TopN query to Druid's HTTP endpoint. The command reads the query from a JSON file and formats the response.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-query.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages.json http://localhost:8888/druid/v2?pretty\n```\n\n----------------------------------------\n\nTITLE: Markdown Navigation Structure for Druid Documentation\nDESCRIPTION: Hierarchical markdown structure organizing documentation links into Development and Misc sections, with placeholders for version numbers in URLs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/toc.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Development\n  * [Overview](/docs/VERSION/development/overview.html)\n  * [Libraries](/libraries.html)\n  * [Extensions](/docs/VERSION/development/extensions.html)\n  * [JavaScript](/docs/VERSION/development/javascript.html)\n  * [Build From Source](/docs/VERSION/development/build.html)\n  * [Versioning](/docs/VERSION/development/versioning.html)\n  * [Integration](/docs/VERSION/development/integrating-druid-with-other-technologies.html)\n  * [Experimental Features](/docs/VERSION/development/experimental.html)\n\n## Misc\n  * [Druid Expressions Language](/docs/VERSION/misc/math-expr.html)\n  * [Papers & Talks](/docs/VERSION/misc/papers-and-talks.html)\n  * [Thanks](/thanks.html)\n```\n\n----------------------------------------\n\nTITLE: React Core License Comment\nDESCRIPTION: License comment for the React core production build, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: License Notice for React Core\nDESCRIPTION: This snippet provides the license information for the React core production build, which is the main React library and is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Comment\nDESCRIPTION: License comment for the React-DOM production file, which is licensed under the MIT License and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6f6dba15.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Jackson Dependency Conflict Error Example\nDESCRIPTION: Example of the Jackson version conflict error that occurs when running Druid with CDH in a MapReduce job.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_3\n\nLANGUAGE: java\nCODE:\n```\njava.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n```\n\n----------------------------------------\n\nTITLE: Configuring IngestSegmentFirehose in Druid for Re-ingesting Existing Segments\nDESCRIPTION: Configuration for the IngestSegmentFirehose which reads data from existing Druid segments. This can be used to modify existing segments with a new schema, changing dimensions, metrics, or rollup configurations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/firehose.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\"    : \"ingestSegment\",\n    \"dataSource\"   : \"wikipedia\",\n    \"interval\" : \"2013-01-01/2013-01-02\"\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for object-assign\nDESCRIPTION: Specifies the MIT license for the object-assign library created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Sample Bitmap Dump Output Format in JSON\nDESCRIPTION: Example of bitmap index dump output showing the bitmap serialization format and encoded bitmap data for column values. This format is used when specifying the --dump bitmaps option.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/dump-segment.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmapSerdeFactory\": {\n    \"type\": \"concise\"\n  },\n  \"bitmaps\": {\n    \"isRobot\": {\n      \"false\": \"//aExfu+Nv3X...\",\n      \"true\": \"gAl7OoRByQ...\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Performing TopN Query in Druid with Multiple Aggregations and Filters\nDESCRIPTION: This JSON query structure retrieves the top 2 results based on the L_QUANTITY_ metric from the tpch_year data source. It applies an OR filter for specific l_orderkey values (103136 and 1648672), performs multiple aggregations including sums and counts, and groups results by l_orderkey dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_TAX_doubleSum\",\n                 \"name\": \"L_TAX_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_DISCOUNT_doubleSum\",\n                 \"name\": \"L_DISCOUNT_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\",\n                 \"name\": \"L_EXTENDEDPRICE_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             },\n             {\n                 \"name\": \"count\",\n                 \"type\": \"count\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"filter\": {\n        \"fields\": [\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"103136\"\n            },\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"1648672\"\n            }\n        ],\n        \"type\": \"or\"\n    },\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: Performing TopN Query in Druid with Multiple Aggregations and Filters\nDESCRIPTION: This JSON query structure retrieves the top 2 results based on the L_QUANTITY_ metric from the tpch_year data source. It applies an OR filter for specific l_orderkey values (103136 and 1648672), performs multiple aggregations including sums and counts, and groups results by l_orderkey dimension.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/topnquery.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"aggregations\": [\n             {\n                 \"fieldName\": \"L_TAX_doubleSum\",\n                 \"name\": \"L_TAX_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_DISCOUNT_doubleSum\",\n                 \"name\": \"L_DISCOUNT_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\",\n                 \"name\": \"L_EXTENDEDPRICE_\",\n                 \"type\": \"doubleSum\"\n             },\n             {\n                 \"fieldName\": \"L_QUANTITY_longSum\",\n                 \"name\": \"L_QUANTITY_\",\n                 \"type\": \"longSum\"\n             },\n             {\n                 \"name\": \"count\",\n                 \"type\": \"count\"\n             }\n    ],\n    \"dataSource\": \"tpch_year\",\n    \"dimension\":\"l_orderkey\",\n    \"filter\": {\n        \"fields\": [\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"103136\"\n            },\n            {\n                \"dimension\": \"l_orderkey\",\n                \"type\": \"selector\",\n                \"value\": \"1648672\"\n            }\n        ],\n        \"type\": \"or\"\n    },\n    \"granularity\": \"all\",\n    \"intervals\": [\n        \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\"\n    ],\n    \"metric\": \"L_QUANTITY_\",\n    \"queryType\": \"topN\",\n    \"threshold\": 2\n}\n```\n\n----------------------------------------\n\nTITLE: License Comment for React Scheduler\nDESCRIPTION: Copyright and license information for the React Scheduler production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: License notice for the React DOM production build, version 17.0.2, released under the MIT license by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Help Output\nDESCRIPTION: Detailed help output showing all available options and their descriptions for the ResetCluster tool.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nNAME\n        druid tools reset-cluster - Cleanup all persisted state from metadata\n        and deep storage.\n\nSYNOPSIS\n        druid tools reset-cluster [--all] [--hadoopWorkingPath]\n                [--metadataStore] [--segmentFiles] [--taskLogs]\n\nOPTIONS\n        --all\n            delete all state stored in metadata and deep storage\n\n        --hadoopWorkingPath\n            delete hadoopWorkingPath\n\n        --metadataStore\n            delete all records in metadata storage\n\n        --segmentFiles\n            delete all segment files from deep storage\n\n        --taskLogs\n            delete all tasklogs\n```\n\n----------------------------------------\n\nTITLE: Basic log4j2.xml Configuration for Apache Druid\nDESCRIPTION: A basic log4j2.xml configuration for Apache Druid that outputs logs to the console. It includes a commented section for enabling HTTP request logging, and sets the default log level to 'info'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/logging.md#2025-04-09_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Configuration status=\"WARN\">\n  <Appenders>\n    <Console name=\"Console\" target=\"SYSTEM_OUT\">\n      <PatternLayout pattern=\"%d{ISO8601} %p [%t] %c - %m%n\"/>\n    </Console>\n  </Appenders>\n  <Loggers>\n    <Root level=\"info\">\n      <AppenderRef ref=\"Console\"/>\n    </Root>\n\n    <!-- Uncomment to enable logging of all HTTP requests\n    <Logger name=\"org.apache.druid.jetty.RequestLog\" additivity=\"false\" level=\"DEBUG\">\n        <AppenderRef ref=\"Console\"/>\n    </Logger>\n    -->\n  </Loggers>\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: License notice for the React Is production build, version 16.13.1, released under the MIT license by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Derby as Metadata Storage in Apache Druid\nDESCRIPTION: Configuration properties to set up Derby as the metadata storage for Druid. Note that Derby is not recommended for production use.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true\n```\n\n----------------------------------------\n\nTITLE: Declaring React Use-Sync-External-Store License in JavaScript\nDESCRIPTION: This snippet provides license information for the React use-sync-external-store-shim production module, which is under the MIT license and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Header in JavaScript\nDESCRIPTION: License header for the object-assign library created by Sindre Sorhus. This library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e5087fc9.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React Core License Declaration\nDESCRIPTION: License declaration for React's main library (react.production.min.js) under MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Rollup Configuration\nDESCRIPTION: Schema configuration enabling data rollup functionality through granularitySpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  },\n  \"granularitySpec\" : {\n    \"rollup\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Object-Assign MIT License Declaration\nDESCRIPTION: License declaration for the object-assign library created by Sindre Sorhus, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License Declaration\nDESCRIPTION: MIT license declaration for Prism, a syntax highlighting library created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for object-assign Library\nDESCRIPTION: This snippet provides the copyright and license information for the object-assign library, which is used in the project and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0d1c82da.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: React Core MIT License Declaration\nDESCRIPTION: License declaration for react.production.min.js by Facebook, version 17.0.2, under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: MIT license header for react-dom.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Executing a View Query in Apache Druid\nDESCRIPTION: This JSON snippet shows how to structure a view query in Druid. It includes the view queryType and a nested groupBy query, demonstrating how to leverage materialized views for query optimization.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/materialized-view.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"queryType\": \"view\",\n    \"query\": {\n        \"queryType\": \"groupBy\",\n        \"dataSource\": \"wikiticker\",\n        \"granularity\": \"all\",\n        \"dimensions\": [\n            \"user\"\n        ],\n        \"limitSpec\": {\n            \"type\": \"default\",\n            \"limit\": 1,\n            \"columns\": [\n                {\n                    \"dimension\": \"added\",\n                    \"direction\": \"descending\",\n                    \"dimensionOrder\": \"numeric\"\n                }\n            ]\n        },\n        \"aggregations\": [\n            {\n                \"type\": \"longSum\",\n                \"name\": \"added\",\n                \"fieldName\": \"added\"\n            }\n        ],\n        \"intervals\": [\n            \"2015-09-12/2015-09-13\"\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React DOM\nDESCRIPTION: Copyright and license declaration for the React DOM production minified file, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Running Apache Druid MiddleManager Server\nDESCRIPTION: Command to start a Druid MiddleManager node. The MiddleManager executes tasks by forwarding them to Peons that run in separate JVMs for resource and log isolation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/design/middlemanager.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server middleManager\n```\n\n----------------------------------------\n\nTITLE: Querying Worker Task Spec State in Druid\nDESCRIPTION: This JSON response shows the detailed state of a specific worker task specification. It includes the task spec configuration with dataSchema, ioConfig, tuningConfig parameters, current execution status, and task history for a subtask in a parallel indexing operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"spec\": {\n    \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\",\n    \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"context\": null,\n    \"inputSplit\": {\n      \"split\": \"/path/to/data/lineitem.tbl.5\"\n    },\n    \"ingestionSpec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"lineitem\",\n        \"parser\": {\n          \"type\": \"hadoopyString\",\n          \"parseSpec\": {\n            \"format\": \"tsv\",\n            \"delimiter\": \"|\",\n            \"timestampSpec\": {\n              \"column\": \"l_shipdate\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"l_orderkey\",\n                \"l_partkey\",\n                \"l_suppkey\",\n                \"l_linenumber\",\n                \"l_returnflag\",\n                \"l_linestatus\",\n                \"l_shipdate\",\n                \"l_commitdate\",\n                \"l_receiptdate\",\n                \"l_shipinstruct\",\n                \"l_shipmode\",\n                \"l_comment\"\n              ]\n            },\n            \"columns\": [\n              \"l_orderkey\",\n              \"l_partkey\",\n              \"l_suppkey\",\n              \"l_linenumber\",\n              \"l_quantity\",\n              \"l_extendedprice\",\n              \"l_discount\",\n              \"l_tax\",\n              \"l_returnflag\",\n              \"l_linestatus\",\n              \"l_shipdate\",\n              \"l_commitdate\",\n              \"l_receiptdate\",\n              \"l_shipinstruct\",\n              \"l_shipmode\",\n              \"l_comment\"\n            ]\n          }\n        },\n        \"metricsSpec\": [\n          {\n            \"type\": \"count\",\n            \"name\": \"count\"\n          },\n          {\n            \"type\": \"longSum\",\n            \"name\": \"l_quantity\",\n            \"fieldName\": \"l_quantity\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_extendedprice\",\n            \"fieldName\": \"l_extendedprice\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_discount\",\n            \"fieldName\": \"l_discount\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_tax\",\n            \"fieldName\": \"l_tax\",\n            \"expression\": null\n          }\n        ],\n        \"granularitySpec\": {\n          \"type\": \"uniform\",\n          \"segmentGranularity\": \"YEAR\",\n          \"queryGranularity\": {\n            \"type\": \"none\"\n          },\n          \"rollup\": true,\n          \"intervals\": [\n            \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n          ]\n        },\n        \"transformSpec\": {\n          \"filter\": null,\n          \"transforms\": []\n        }\n      },\n      \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"/path/to/data/\",\n          \"filter\": \"lineitem.tbl.5\",\n          \"parser\": null\n        },\n        \"appendToExisting\": false\n      },\n      \"tuningConfig\": {\n        \"type\": \"index_parallel\",\n        \"maxRowsPerSegment\": 5000000,\n        \"maxRowsInMemory\": 1000000,\n        \"maxTotalRows\": 20000000,\n        \"numShards\": null,\n        \"indexSpec\": {\n          \"bitmap\": {\n            \"type\": \"concise\"\n          },\n          \"dimensionCompression\": \"lz4\",\n          \"metricCompression\": \"lz4\",\n          \"longEncoding\": \"longs\"\n        },\n        \"maxPendingPersists\": 0,\n        \"forceExtendableShardSpecs\": false,\n        \"reportParseExceptions\": false,\n        \"pushTimeout\": 0,\n        \"segmentWriteOutMediumFactory\": null,\n        \"maxNumSubTasks\": 4,\n        \"maxRetry\": 3,\n        \"taskStatusCheckPeriodMs\": 1000,\n        \"chatHandlerTimeout\": \"PT10S\",\n        \"chatHandlerNumRetries\": 5,\n        \"logParseExceptions\": false,\n        \"maxParseExceptions\": 2147483647,\n        \"maxSavedParseExceptions\": 0,\n        \"forceGuaranteedRollup\": false,\n        \"buildV9Directly\": true\n      }\n    }\n  },\n  \"currentStatus\": {\n    \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\",\n    \"type\": \"index_sub\",\n    \"createdTime\": \"2018-04-20T22:16:29.925Z\",\n    \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\",\n    \"statusCode\": \"RUNNING\",\n    \"duration\": -1,\n    \"location\": {\n      \"host\": null,\n      \"port\": -1,\n      \"tlsPort\": -1\n    },\n    \"dataSource\": \"lineitem\",\n    \"errorMsg\": null\n  },\n  \"taskHistory\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Worker Task Spec State in Druid\nDESCRIPTION: This JSON response shows the detailed state of a specific worker task specification. It includes the task spec configuration with dataSchema, ioConfig, tuningConfig parameters, current execution status, and task history for a subtask in a parallel indexing operation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"spec\": {\n    \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\",\n    \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\",\n    \"context\": null,\n    \"inputSplit\": {\n      \"split\": \"/path/to/data/lineitem.tbl.5\"\n    },\n    \"ingestionSpec\": {\n      \"dataSchema\": {\n        \"dataSource\": \"lineitem\",\n        \"parser\": {\n          \"type\": \"hadoopyString\",\n          \"parseSpec\": {\n            \"format\": \"tsv\",\n            \"delimiter\": \"|\",\n            \"timestampSpec\": {\n              \"column\": \"l_shipdate\",\n              \"format\": \"yyyy-MM-dd\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"l_orderkey\",\n                \"l_partkey\",\n                \"l_suppkey\",\n                \"l_linenumber\",\n                \"l_returnflag\",\n                \"l_linestatus\",\n                \"l_shipdate\",\n                \"l_commitdate\",\n                \"l_receiptdate\",\n                \"l_shipinstruct\",\n                \"l_shipmode\",\n                \"l_comment\"\n              ]\n            },\n            \"columns\": [\n              \"l_orderkey\",\n              \"l_partkey\",\n              \"l_suppkey\",\n              \"l_linenumber\",\n              \"l_quantity\",\n              \"l_extendedprice\",\n              \"l_discount\",\n              \"l_tax\",\n              \"l_returnflag\",\n              \"l_linestatus\",\n              \"l_shipdate\",\n              \"l_commitdate\",\n              \"l_receiptdate\",\n              \"l_shipinstruct\",\n              \"l_shipmode\",\n              \"l_comment\"\n            ]\n          }\n        },\n        \"metricsSpec\": [\n          {\n            \"type\": \"count\",\n            \"name\": \"count\"\n          },\n          {\n            \"type\": \"longSum\",\n            \"name\": \"l_quantity\",\n            \"fieldName\": \"l_quantity\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_extendedprice\",\n            \"fieldName\": \"l_extendedprice\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_discount\",\n            \"fieldName\": \"l_discount\",\n            \"expression\": null\n          },\n          {\n            \"type\": \"doubleSum\",\n            \"name\": \"l_tax\",\n            \"fieldName\": \"l_tax\",\n            \"expression\": null\n          }\n        ],\n        \"granularitySpec\": {\n          \"type\": \"uniform\",\n          \"segmentGranularity\": \"YEAR\",\n          \"queryGranularity\": {\n            \"type\": \"none\"\n          },\n          \"rollup\": true,\n          \"intervals\": [\n            \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\"\n          ]\n        },\n        \"transformSpec\": {\n          \"filter\": null,\n          \"transforms\": []\n        }\n      },\n      \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"/path/to/data/\",\n          \"filter\": \"lineitem.tbl.5\",\n          \"parser\": null\n        },\n        \"appendToExisting\": false\n      },\n      \"tuningConfig\": {\n        \"type\": \"index_parallel\",\n        \"maxRowsPerSegment\": 5000000,\n        \"maxRowsInMemory\": 1000000,\n        \"maxTotalRows\": 20000000,\n        \"numShards\": null,\n        \"indexSpec\": {\n          \"bitmap\": {\n            \"type\": \"concise\"\n          },\n          \"dimensionCompression\": \"lz4\",\n          \"metricCompression\": \"lz4\",\n          \"longEncoding\": \"longs\"\n        },\n        \"maxPendingPersists\": 0,\n        \"forceExtendableShardSpecs\": false,\n        \"reportParseExceptions\": false,\n        \"pushTimeout\": 0,\n        \"segmentWriteOutMediumFactory\": null,\n        \"maxNumSubTasks\": 4,\n        \"maxRetry\": 3,\n        \"taskStatusCheckPeriodMs\": 1000,\n        \"chatHandlerTimeout\": \"PT10S\",\n        \"chatHandlerNumRetries\": 5,\n        \"logParseExceptions\": false,\n        \"maxParseExceptions\": 2147483647,\n        \"maxSavedParseExceptions\": 0,\n        \"forceGuaranteedRollup\": false,\n        \"buildV9Directly\": true\n      }\n    }\n  },\n  \"currentStatus\": {\n    \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\",\n    \"type\": \"index_sub\",\n    \"createdTime\": \"2018-04-20T22:16:29.925Z\",\n    \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\",\n    \"statusCode\": \"RUNNING\",\n    \"duration\": -1,\n    \"location\": {\n      \"host\": null,\n      \"port\": -1,\n      \"tlsPort\": -1\n    },\n    \"dataSource\": \"lineitem\",\n    \"errorMsg\": null\n  },\n  \"taskHistory\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Result Format in Druid SQL Query (JSON)\nDESCRIPTION: Demonstrates how to specify a result format for a Druid SQL query using the 'resultFormat' parameter in the JSON request body.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\",\n  \"resultFormat\" : \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store-Shim License\nDESCRIPTION: Copyright notice and MIT license declaration for the use-sync-external-store-shim.production.min.js file from React.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0c610519.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring HLLSketchToString Post-Aggregator in Druid\nDESCRIPTION: This JSON configuration defines the HLLSketchToString post-aggregator. It provides a human-readable summary of the HLL sketch for debugging purposes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-core/datasketches-hll.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"HLLSketchToString\",\n  \"name\": <output name>,\n  \"field\"  : <post aggregator that returns an HLL Sketch>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Process Settings in YAML for Apache Druid Realtime Process\nDESCRIPTION: This YAML configuration snippet defines key process settings for the Apache Druid Realtime Process, including host, port, and service name. It specifies how the process advertises itself and handles connections.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/configuration/realtime.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.host: InetAddress.getLocalHost().getCanonicalHostName()\ndruid.plaintextPort: 8084\ndruid.tlsPort: 8284\ndruid.service: druid/realtime\n```\n\n----------------------------------------\n\nTITLE: NProgress License Comment\nDESCRIPTION: License comment for the NProgress library, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DBCP Properties for Metadata Storage\nDESCRIPTION: Example of setting custom DBCP (Database Connection Pool) properties for Druid's metadata storage connector, including connection lifetime and query timeout values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000\ndruid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Declaration\nDESCRIPTION: MIT license declaration for object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d8507f6b.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension TopNMetricSpec in Druid\nDESCRIPTION: Dimension-based metric specification for sorting topN results using lexicographic ordering with optional pagination support.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/topnmetricspec.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"metric\": {\n    \"type\": \"dimension\",\n    \"ordering\": \"lexicographic\",\n    \"previousStop\": \"<previousStop_value>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Comment\nDESCRIPTION: License comment for the object-assign library by Sindre Sorhus, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Lunr.js Library Copyright Notice\nDESCRIPTION: The main copyright notice for the Lunr.js library, describing it as a lightweight search engine for browser use. It includes version information and license type.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8591.6723df56.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9\n * Copyright (C) 2020 Oliver Nightingale\n * @license MIT\n */\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: MIT license declaration for React's react.production.min.js core module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Core License Declaration\nDESCRIPTION: MIT license declaration for React's react.production.min.js v17.0.2 library created by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Submitting Druid Ingestion Task via Command Line\nDESCRIPTION: This bash command submits the Druid ingestion task specification file to the Druid overlord for processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/ingestion-tutorial-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: object-assign License Declaration in JavaScript\nDESCRIPTION: License declaration for the object-assign library created by Sindre Sorhus. The library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d8507f6b.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: License Declaration for object-assign Library\nDESCRIPTION: MIT license declaration for the object-assign library created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: Copyright notice and MIT license declaration for the Prism syntax highlighting library by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Adding Druid Extensions to Maven POM\nDESCRIPTION: This XML snippet shows how to add various Druid extensions as dependencies in the Maven POM file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/other-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>druid-avro-extensions</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.druid.extensions.contrib</groupId>\n    <artifactId>druid-parquet-extensions</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>druid-hdfs-storage</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.druid.extensions</groupId>\n    <artifactId>mysql-metadata-storage</artifactId>\n    <version>${project.parent.version}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticGoogleBlobStoreFirehose for Apache Druid\nDESCRIPTION: A sample configuration for the StaticGoogleBlobStoreFirehose which ingests events from Google Cloud Storage. This firehose is splittable and supports parallel ingestion. The configuration specifies multiple GCS blobs to be ingested.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/extensions-contrib/google.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-google-blobstore\",\n    \"blobs\": [\n        {\n          \"bucket\": \"foo\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"bucket\": \"bar\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for NProgress\nDESCRIPTION: Copyright notice and MIT license information for the NProgress library by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Index Task in Apache Druid\nDESCRIPTION: Example ingestion specification for an index_parallel task, which performs parallel batch indexing using Druid's own resources without dependency on external systems like Hadoop. This configuration specifies the data schema, metrics, time granularity, parser settings, IO configuration, and tuning parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/native_tasks.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"index_parallel\",\n  \"spec\": {\n    \"dataSchema\": {\n      \"dataSource\": \"wikipedia_parallel_index_test\",\n      \"metricsSpec\": [\n        {\n          \"type\": \"count\",\n              \"name\": \"count\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"added\",\n              \"fieldName\": \"added\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"deleted\",\n              \"fieldName\": \"deleted\"\n            },\n            {\n              \"type\": \"doubleSum\",\n              \"name\": \"delta\",\n              \"fieldName\": \"delta\"\n            }\n        ],\n        \"granularitySpec\": {\n          \"segmentGranularity\": \"DAY\",\n          \"queryGranularity\": \"second\",\n          \"intervals\" : [ \"2013-08-31/2013-09-02\" ]\n        },\n        \"parser\": {\n          \"parseSpec\": {\n            \"format\" : \"json\",\n            \"timestampSpec\": {\n              \"column\": \"timestamp\"\n            },\n            \"dimensionsSpec\": {\n              \"dimensions\": [\n                \"page\",\n                \"language\",\n                \"user\",\n                \"unpatrolled\",\n                \"newPage\",\n                \"robot\",\n                \"anonymous\",\n                \"namespace\",\n                \"continent\",\n                \"country\",\n                \"region\",\n                \"city\"\n              ]\n            }\n          }\n        }\n    },\n    \"ioConfig\": {\n        \"type\": \"index_parallel\",\n        \"firehose\": {\n          \"type\": \"local\",\n          \"baseDir\": \"examples/indexing/\",\n          \"filter\": \"wikipedia_index_data*\"\n        }\n    },\n    \"tuningconfig\": {\n        \"type\": \"index_parallel\",\n        \"maxNumSubTasks\": 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sparse Serialization Format for Histogram in Apache Druid\nDESCRIPTION: Defines the byte-level structure for serializing a histogram in sparse format, used when less than half of the buckets have values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x02 for sparse\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\nint: number of following (bucketNum, count) pairs\nsequence of (int, long) pairs:\n  int: bucket number\n  count: bucket count\n```\n\n----------------------------------------\n\nTITLE: Sparse Serialization Format for Histogram in Apache Druid\nDESCRIPTION: Defines the byte-level structure for serializing a histogram in sparse format, used when less than half of the buckets have values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-core/approximate-histograms.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nbyte: serialization version, must be 0x01\nbyte: encoding mode, 0x02 for sparse\ndouble: lowerLimit\ndouble: upperLimit\nint: numBuckets\nbyte: outlier handling mode (0x00 for `ignore`, 0x01 for `overflow`, and 0x02 for `clip`)\nlong: count, total number of values contained in the histogram, excluding outliers\nlong: lowerOutlierCount\nlong: upperOutlierCount\nlong: missingValueCount\ndouble: max\ndouble: min\nint: number of following (bucketNum, count) pairs\nsequence of (int, long) pairs:\n  int: bucket number\n  count: bucket count\n```\n\n----------------------------------------\n\nTITLE: Running insert-segment-to-db Tool with MySQL and S3 for Wikipedia Segments\nDESCRIPTION: Command to run the insert-segment-to-db tool with MySQL as metadata storage and S3 as deep storage. The example configures the necessary S3 connection parameters, bucket information, and loads the required MySQL and S3 extensions to process segments from a Wikipedia dataset.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njava\n-Ddruid.metadata.storage.type=mysql \n-Ddruid.metadata.storage.connector.connectURI=jdbc\\:mysql\\://localhost\\:3306/druid \n-Ddruid.metadata.storage.connector.user=druid \n-Ddruid.metadata.storage.connector.password=diurd\n-Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\",\\\"druid-s3-extensions\\\"]\n-Ddruid.storage.type=s3\n-Ddruid.s3.accessKey=... \n-Ddruid.s3.secretKey=...\n-Ddruid.storage.bucket=your-bucket\n-Ddruid.storage.baseKey=druid/storage/wikipedia\n-Ddruid.storage.maxListingLength=1000\n-cp $DRUID_CLASSPATH\norg.apache.druid.cli.Main tools insert-segment-to-db --workingDir \"druid/storage/wikipedia\" --updateDescriptor true\n```\n\n----------------------------------------\n\nTITLE: DOMPurify License Declaration in JavaScript\nDESCRIPTION: License declaration for DOMPurify version 2.4.3, indicating it's released under both Apache License 2.0 and Mozilla Public License 2.0, with a link to the license file in the repository.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.e18c3dea.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */\n```\n\n----------------------------------------\n\nTITLE: NProgress MIT License Declaration\nDESCRIPTION: License declaration for the NProgress library created by Rico Sta. Cruz, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Comparison Operators in Druid SQL\nDESCRIPTION: Example of comparison operator syntax in Druid SQL, showing various ways to compare values including equality, inequality, pattern matching, and NULL checking.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nx = y\nx <> y\nx BETWEEN y AND z\nx LIKE pattern ESCAPE esc\nx IS NULL\nx IN (values)\nx AND y\n```\n\n----------------------------------------\n\nTITLE: Mixed Version Segments During Update\nDESCRIPTION: Example illustrating a transition state during a schema update where the cluster contains a mixture of v1 and v2 segments. This shows how updates across multiple intervals are not atomic across the entire update.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/schema-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nfoo_2015-01-01/2015-01-02_v1_0\nfoo_2015-01-02/2015-01-03_v2_1\nfoo_2015-01-03/2015-01-04_v1_2\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: Copyright notice and MIT license declaration for the NProgress library by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Search Query Configuration in Apache Druid\nDESCRIPTION: Configuration property for search queries on realtime nodes, setting the maximum number of search results to return.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/realtime.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndruid.query.search.maxSearchLimit: 1000\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Declaration\nDESCRIPTION: MIT license declaration for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighting MIT License Declaration\nDESCRIPTION: License declaration for Prism syntax highlighting library created by Lea Verou under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring insert-segment-to-db Tool Properties for Metadata Storage and Deep Storage\nDESCRIPTION: Essential configuration properties required for the insert-segment-to-db tool to connect to metadata storage and identify the deep storage type. These properties must be provided either as Java JVM arguments or in a runtime.properties file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/insert-segment-to-db.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type\ndruid.metadata.storage.connector.connectURI\ndruid.metadata.storage.connector.user\ndruid.metadata.storage.connector.password\ndruid.storage.type\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fea5e92a.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Schema for Druid Deep Storage\nDESCRIPTION: SQL statements to create the necessary tables in Cassandra for Druid deep storage. The index_storage table uses a chunked object pattern to store compressed segments, while descriptor_storage stores segment metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for object-assign\nDESCRIPTION: License declaration for the object-assign library created by Sindre Sorhus under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: DOMPurify License Information in JavaScript\nDESCRIPTION: This comment block contains the license information for DOMPurify version 2.4.3. It specifies the Apache license 2.0 and Mozilla Public License 2.0, and provides a link to the full license text on GitHub.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.37a11f44.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */\n```\n\n----------------------------------------\n\nTITLE: Mark.js MIT License Declaration\nDESCRIPTION: License declaration for mark.js, a JavaScript highlighting library created by Julian Khnel under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e5087fc9.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Displaying Rolled-up Data Results in Druid\nDESCRIPTION: Example showing the result of Druid's rollup operation on the sample data, where timestamps are floored to minutes and metrics are aggregated accordingly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000\n2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000\n2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: Mark.js MIT License Attribution\nDESCRIPTION: Copyright notice for mark.js, a JavaScript text highlighting library created by Julian Khnel and licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Declaration\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.df5a69b6.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Implementing Runge-Kutta Spring Physics in JavaScript\nDESCRIPTION: A function generator for Runge-Kutta spring physics calculations. This utility is adapted from Framer.js and can be used for creating realistic spring-based animations or simulations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/7724.21e9de4d.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! Runge-Kutta spring physics function generator. Adapted from Framer.js, copyright Koen Bok. MIT License: http://en.wikipedia.org/wiki/MIT_License */\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.df5a69b6.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Filtered Aggregator Configuration\nDESCRIPTION: JSON configuration for a Filtered aggregator, which wraps another aggregator but only processes values that match a specified dimension filter, allowing filtered and unfiltered aggregations in a single query.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/aggregations.md#2025-04-09_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"filtered\",\n  \"filter\" : {\n    \"type\" : \"selector\",\n    \"dimension\" : <dimension>,\n    \"value\" : <dimension value>\n  }\n  \"aggregator\" : <aggregation>\n}\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Header in JavaScript\nDESCRIPTION: License header for React's scheduler.production.min.js file version 0.20.2. This is part of the React library developed by Facebook and licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining Arithmetic Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Demonstrates the structure of an arithmetic post-aggregator in Druid queries. It applies a specified function to given fields and can include an optional ordering parameter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arithmetic\",\n  \"name\"  : <output_name>,\n  \"fn\"    : <arithmetic_function>,\n  \"fields\": [<post_aggregator>, <post_aggregator>, ...],\n  \"ordering\" : <null (default), or \"numericFirst\">\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Timestamp Specification in Druid Ingestion\nDESCRIPTION: Adds a timestampSpec to the parseSpec, specifying the format and column name for the main timestamp field in the input data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Timestamp Specification in Druid Ingestion\nDESCRIPTION: Adds a timestampSpec to the parseSpec, specifying the format and column name for the main timestamp field in the input data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"dataSchema\" : {\n  \"dataSource\" : \"ingestion-tutorial\",\n  \"parser\" : {\n    \"type\" : \"string\",\n    \"parseSpec\" : {\n      \"format\" : \"json\",\n      \"timestampSpec\" : {\n        \"format\" : \"iso\",\n        \"column\" : \"ts\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retention Analysis Query: Unique Users Across Different Time Periods\nDESCRIPTION: Advanced query example for retention analysis, showing how to find users who visited product A in two different time periods. This combines filtered aggregators, time interval filters, and sketch intersection operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-theta.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryType\": \"groupBy\",\n  \"dataSource\": \"test_datasource\",\n  \"granularity\": \"ALL\",\n  \"dimensions\": [],\n  \"filter\": {\n    \"type\": \"or\",\n    \"fields\": [\n      {\"type\": \"selector\", \"dimension\": \"product\", \"value\": \"A\"}\n    ]\n  },\n  \"aggregations\": [\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" :     {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_1\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n    {\n      \"type\" : \"filtered\",\n      \"filter\" : {\n        \"type\" : \"and\",\n        \"fields\" : [\n          {\n            \"type\" : \"selector\",\n            \"dimension\" : \"product\",\n            \"value\" : \"A\"\n          },\n          {\n            \"type\" : \"interval\",\n            \"dimension\" : \"__time\",\n            \"intervals\" :  [\"2014-10-08T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n          }\n        ]\n      },\n      \"aggregator\" : {\n        \"type\": \"thetaSketch\", \"name\": \"A_unique_users_week_2\", \"fieldName\": \"user_id_sketch\"\n      }\n    },\n  ],\n  \"postAggregations\": [\n    {\n      \"type\": \"thetaSketchEstimate\",\n      \"name\": \"final_unique_users\",\n      \"field\":\n      {\n        \"type\": \"thetaSketchSetOp\",\n        \"name\": \"final_unique_users_sketch\",\n        \"func\": \"INTERSECT\",\n        \"fields\": [\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_1\"\n          },\n          {\n            \"type\": \"fieldAccess\",\n            \"fieldName\": \"A_unique_users_week_2\"\n          }\n        ]\n      }\n    }\n  ],\n  \"intervals\": [\"2014-10-01T00:00:00.000Z/2014-10-14T00:00:00.000Z\"]\n}\n```\n\n----------------------------------------\n\nTITLE: DOMPurify License Declaration\nDESCRIPTION: License header for DOMPurify library version 2.4.3, released under Apache 2.0 and Mozilla Public License 2.0.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.0e5d9014.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */\n```\n\n----------------------------------------\n\nTITLE: Implementing Minimal Promises/A+ 1.1.1 in JavaScript\nDESCRIPTION: This snippet provides a minimal implementation of the Promises/A+ 1.1.1 specification. It creates an embeddable, strictly-compliant Thenable object for handling asynchronous operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/7724.4bbc210b.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n  Embeddable Minimum Strictly-Compliant Promises/A+ 1.1.1 Thenable\n  Copyright (c) 2013-2014 Ralf S. Engelschall (http://engelschall.com)\n  Licensed under The MIT License (http://opensource.org/licenses/MIT)\n  */\n```\n\n----------------------------------------\n\nTITLE: Loading Data Using Druid's post-index-task Script\nDESCRIPTION: Command to load the sample data into Druid using the ingestion task specification.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-rollup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/rollup-index.json\n```\n\n----------------------------------------\n\nTITLE: Starting Druid Broker Server\nDESCRIPTION: Command to start the Druid Broker server process using the Main class. This is the entry point for running a Broker node in a Druid cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/broker.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server broker\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Displaying Rolled-up Data Results in Druid\nDESCRIPTION: Example showing the result of Druid's rollup operation on the sample data, where timestamps are floored to minutes and metrics are aggregated accordingly.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/index.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ntimestamp                 srcIP         dstIP          packets     bytes\n2018-01-01T01:01:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-01T01:02:00Z      1.1.1.1       2.2.2.2            900      9000\n2018-01-01T01:03:00Z      1.1.1.1       2.2.2.2            600      6000\n2018-01-02T21:33:00Z      7.7.7.7       8.8.8.8            300      3000\n2018-01-02T21:35:00Z      7.7.7.7       8.8.8.8            300      3000\n```\n\n----------------------------------------\n\nTITLE: License Comment for object-assign\nDESCRIPTION: Copyright and license information for the object-assign library by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for mark.js\nDESCRIPTION: This snippet provides the copyright and license information for mark.js, a JavaScript keyword highlighting library, version 8.11.1, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.df5a69b6.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React-Is License Information\nDESCRIPTION: License information for React's react-is.production.min.js (v16.13.1) created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e3bd5681.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0d1c82da.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Building Apache Druid Distribution with Additional Profiles\nDESCRIPTION: Advanced Maven command to build Druid source and binary distributions with signatures, checksums, license auditing, and skipping unit tests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/build.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Papache-release,dist,rat -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Lunr.js Main Library Copyright Declaration\nDESCRIPTION: Main copyright declaration for the Lunr.js search library, version 2.3.9. Identifies the library as a lightweight alternative to Solr.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/4611.8d581fc8.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * lunr - http://lunrjs.com - A bit like Solr, but much smaller and not as bright - 2.3.9\n * Copyright (C) 2020 Oliver Nightingale\n * @license MIT\n */\n```\n\n----------------------------------------\n\nTITLE: No Rollup Configuration Example\nDESCRIPTION: Shows how to configure dimensions when rollup is not being used, with all columns specified in the dimensionsSpec.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/tutorials/tutorial-ingestion-spec.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"dimensionsSpec\" : {\n  \"dimensions\": [\n    \"srcIP\",\n    { \"name\" : \"srcPort\", \"type\" : \"long\" },\n    { \"name\" : \"dstIP\", \"type\" : \"string\" },\n    { \"name\" : \"dstPort\", \"type\" : \"long\" },\n    { \"name\" : \"protocol\", \"type\" : \"string\" },\n    { \"name\" : \"packets\", \"type\" : \"long\" },\n    { \"name\" : \"bytes\", \"type\" : \"long\" },\n    { \"name\" : \"srcPort\", \"type\" : \"double\" }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.7d317958.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e831e4fb.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Notice\nDESCRIPTION: License notice for the object-assign library, created by Sindre Sorhus and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React use-sync-external-store-shim License Declaration in JavaScript\nDESCRIPTION: License declaration for React's use-sync-external-store-shim.production.min.js, created by Facebook. The library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: License Comment for React DOM\nDESCRIPTION: Copyright and license information for the React DOM production build.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining JavaScript Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Demonstrates how to define a JavaScript post-aggregator that applies a custom JavaScript function to specified fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"absPercent\",\n  \"fieldNames\": [\"delta\", \"total\"],\n  \"function\": \"function(delta, total) { return 100 * Math.abs(delta) / total; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining JavaScript Post-Aggregator in Apache Druid JSON Query\nDESCRIPTION: Demonstrates how to define a JavaScript post-aggregator that applies a custom JavaScript function to specified fields.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/post-aggregations.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"javascript\",\n  \"name\": \"absPercent\",\n  \"fieldNames\": [\"delta\", \"total\"],\n  \"function\": \"function(delta, total) { return 100 * Math.abs(delta) / total; }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Coordinator Console URL\nDESCRIPTION: The URL pattern for accessing the Druid Coordinator's web console, which displays cluster information and rule configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/management-uis.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<COORDINATOR_IP>:<COORDINATOR_PORT>\n```\n\n----------------------------------------\n\nTITLE: DOMPurify License Header\nDESCRIPTION: License and attribution header for DOMPurify library version 2.4.3, released under Apache 2.0 and Mozilla Public License 2.0\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.19bec321.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for object-assign in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the object-assign library, attributing it to Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Curl Command for Druid Query Execution\nDESCRIPTION: Specific example of sending a query file to Druid Broker with Kerberos authentication\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/extensions-core/druid-kerberos.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --negotiate -u:anyUser -b ~/cookies.txt -c ~/cookies.txt -X POST -H'Content-Type: application/json'  http://broker-host:port/druid/v2/?pretty -d @query.json\n```\n\n----------------------------------------\n\nTITLE: Inserting Indexing Service Architecture Diagram\nDESCRIPTION: This snippet inserts an image of the Indexing Service architecture into the Markdown document.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/design/indexing-service.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n![Indexing Service](../../img/indexing_service.png \"Indexing Service\")\n```\n\n----------------------------------------\n\nTITLE: Declaring Mark.js License in JavaScript\nDESCRIPTION: This snippet declares the license information for mark.js library version 8.11.1, which is under the MIT license and created by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid Client Communication in Markdown\nDESCRIPTION: A markdown table listing configuration options for the HTTP client used by Druid Brokers to communicate with data servers, including connection pool size, compression, and timeout settings.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_17\n\nLANGUAGE: markdown\nCODE:\n```\n|`druid.broker.http.numConnections`|Size of connection pool for the Broker to connect to historical and real-time processes. If there are more queries than this number that all need to speak to the same node, then they will queue up.|20|\n|`druid.broker.http.compressionCodec`|Compression codec the Broker uses to communicate with historical and real-time processes. May be \"gzip\" or \"identity\".|gzip|\n|`druid.broker.http.readTimeout`|The timeout for data reads from historical servers and real-time tasks.|PT15M|\n|`druid.broker.http.unusedConnectionTimeout`|The timeout for idle connections in connection pool. This timeout should be less than `druid.broker.http.readTimeout`. Set this timeout = ~90% of `druid.broker.http.readTimeout`|`PT4M`|\n|`druid.broker.http.maxQueuedBytes`|Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to `druid.server.http.maxScatterGatherBytes`, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled. Can be overridden by the [\"maxQueuedBytes\" query context parameter](../querying/query-context.html).|0 (disabled)|\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React use-sync-external-store-shim\nDESCRIPTION: Specifies the MIT license for the React use-sync-external-store-shim production module.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress MIT License Attribution\nDESCRIPTION: Copyright notice for NProgress, a progress bar library created by Rico Sta. Cruz and licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: License Notice for React DOM\nDESCRIPTION: This snippet contains the license information for React DOM production build (v17.0.2), which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Basic Druid Ingestion Specification Structure in JSON\nDESCRIPTION: The fundamental structure of a Druid ingestion specification with the three main components: dataSchema, ioConfig, and tuningConfig. This template forms the basis of all ingestion specifications in Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dataSchema\" : {...},\n  \"ioConfig\" : {...},\n  \"tuningConfig\" : {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Document Load Handler Comment\nDESCRIPTION: Comment block indicating code should wait for document to be fully loaded before execution\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.19bec321.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n   * Wait for document loaded before starting the execution\n   */\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for mark.js v8.11.1 library by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.14f7867a.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for mark.js Library\nDESCRIPTION: This snippet contains the copyright notice and license information for the mark.js library by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Notice\nDESCRIPTION: License notice for the React scheduler production module, version 0.20.2, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b5bbe9a.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Implementing Minimal Promises/A+ in JavaScript\nDESCRIPTION: A minimal implementation of the Promises/A+ 1.1.1 specification. This function provides a strictly-compliant, embeddable thenable object for asynchronous operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/7724.21e9de4d.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n  Embeddable Minimum Strictly-Compliant Promises/A+ 1.1.1 Thenable\n  Copyright (c) 2013-2014 Ralf S. Engelschall (http://engelschall.com)\n  Licensed under The MIT License (http://opensource.org/licenses/MIT)\n  */\n```\n\n----------------------------------------\n\nTITLE: License Comment for NProgress\nDESCRIPTION: Copyright and license information for the NProgress library by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring EventReceiverFirehose in Apache Druid\nDESCRIPTION: EventReceiverFirehose configuration for ingesting events using an HTTP endpoint. This is used with stream-pull ingestion model and in tasks automatically generated by Tranquility stream push.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/ingestion/firehose.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"receiver\",\n  \"serviceName\": \"eventReceiverServiceName\",\n  \"bufferSize\": 10000\n}\n```\n\n----------------------------------------\n\nTITLE: Numeric Range Filter in Druid\nDESCRIPTION: Demonstrates filtering on a range of numeric values using bound filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"bound\",\n  \"dimension\": \"myFloatColumn\",\n  \"ordering\": \"numeric\",\n  \"lower\": \"10\",\n  \"lowerStrict\": false,\n  \"upper\": \"20\",\n  \"upperStrict\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Numeric Range Filter in Druid\nDESCRIPTION: Demonstrates filtering on a range of numeric values using bound filter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/filters.md#2025-04-09_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": {\n  \"type\": \"bound\",\n  \"dimension\": \"myFloatColumn\",\n  \"ordering\": \"numeric\",\n  \"lower\": \"10\",\n  \"lowerStrict\": false,\n  \"upper\": \"20\",\n  \"upperStrict\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Managing Compaction Configuration using HTTP Requests\nDESCRIPTION: Endpoints for retrieving, creating, updating, and deleting compaction configurations for datasources, including the ability to update capacity for compaction tasks with ratio and maximum slot parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/api-reference.md#2025-04-09_snippet_9\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/config/compaction\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/coordinator/v1/config/compaction/{dataSource}\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/config/compaction/taskslots?ratio={someRatio}&max={someMaxSlots}\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/config/compaction\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nDELETE /druid/coordinator/v1/config/compaction/{dataSource}\n```\n\n----------------------------------------\n\nTITLE: Processing Check Comment in JavaScript\nDESCRIPTION: A comment that appears to be checking if some content has been previously processed, likely part of a caching or optimization mechanism.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.e18c3dea.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*! Check if previously processed */\n```\n\n----------------------------------------\n\nTITLE: Example Bitmap Index Output from DumpSegment Tool\nDESCRIPTION: Sample of the bitmap index output format from the DumpSegment tool when using the '--dump bitmaps' option. Shows the serialization format and base64-encoded bitmaps for column values.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/operations/dump-segment.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bitmapSerdeFactory\": {\n    \"type\": \"concise\"\n  },\n  \"bitmaps\": {\n    \"isRobot\": {\n      \"false\": \"//aExfu+Nv3X...\",\n      \"true\": \"gAl7OoRByQ...\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Druid Service Output\nDESCRIPTION: Example output showing the startup of various Druid services in the micro-quickstart configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/index.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ./bin/start-micro-quickstart \n[Fri May  3 11:40:50 2019] Running command[zk], logging to[/apache-druid-0.15.0-incubating/var/sv/zk.log]: bin/run-zk conf\n[Fri May  3 11:40:50 2019] Running command[coordinator-overlord], logging to[/apache-druid-0.15.0-incubating/var/sv/coordinator-overlord.log]: bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart\n[Fri May  3 11:40:50 2019] Running command[broker], logging to[/apache-druid-0.15.0-incubating/var/sv/broker.log]: bin/run-druid broker conf/druid/single-server/micro-quickstart\n[Fri May  3 11:40:50 2019] Running command[router], logging to[/apache-druid-0.15.0-incubating/var/sv/router.log]: bin/run-druid router conf/druid/single-server/micro-quickstart\n[Fri May  3 11:40:50 2019] Running command[historical], logging to[/apache-druid-0.15.0-incubating/var/sv/historical.log]: bin/run-druid historical conf/druid/single-server/micro-quickstart\n[Fri May  3 11:40:50 2019] Running command[middleManager], logging to[/apache-druid-0.15.0-incubating/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart\n```\n\n----------------------------------------\n\nTITLE: Accessing Coordinator Configuration via HTTP Endpoint\nDESCRIPTION: HTTP endpoint for submitting JSON configuration to the Druid Coordinator. This endpoint allows updating dynamic configuration parameters without restarting the coordinator.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/configuration/index.md#2025-04-09_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nhttp://<COORDINATOR_IP>:<PORT>/druid/coordinator/v1/config\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production build\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e831e4fb.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Druid Plumbers in Markdown\nDESCRIPTION: This markdown snippet defines the concept of plumbers in Apache Druid, explaining their role in handling generated segments and introducing two types of plumbers: YeOldePlumber and RealtimePlumber.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/design/plumber.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Apache Druid (incubating) Plumbers\n\nThe plumber handles generated segments both while they are being generated and when they are \"done\". This is also technically a pluggable interface and there are multiple implementations. However, plumbers handle numerous complex details, and therefore an advanced understanding of Druid is recommended before implementing your own.\n\nAvailable Plumbers\n------------------\n\n#### YeOldePlumber\n\nThis plumber creates single historical segments.\n\n#### RealtimePlumber\n\nThis plumber creates real-time/mutable segments.\n```\n\n----------------------------------------\n\nTITLE: Configuring Hash-based Partitioning in Druid\nDESCRIPTION: Example configuration for hash-based partitioning strategy that distributes data across segments based on dimension hashes. Includes options for controlling partition size and shard count.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/hadoop.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n  \"partitionsSpec\": {\n     \"type\": \"hashed\",\n     \"targetPartitionSize\": 5000000\n   }\n```\n\n----------------------------------------\n\nTITLE: Configuring EventReceiverFirehose in Apache Druid (JSON)\nDESCRIPTION: JSON configuration for EventReceiverFirehoseFactory to ingest events using an HTTP endpoint. It specifies the type, service name, and buffer size for the firehose.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/ingestion/firehose.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"receiver\",\n  \"serviceName\": \"eventReceiverServiceName\",\n  \"bufferSize\": 10000\n}\n```\n\n----------------------------------------\n\nTITLE: js-yaml License Declaration in JavaScript\nDESCRIPTION: License declaration for js-yaml version 4.1.0, indicating it's released under the MIT license, with a link to the GitHub repository.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.e18c3dea.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React Is License\nDESCRIPTION: MIT license declaration for React's react-is.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React-Is License Header\nDESCRIPTION: License header for React's react-is.production.min.js (version 16.13.1) module, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Notice\nDESCRIPTION: License notice for the NProgress library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Extraction in Apache Druid SQL\nDESCRIPTION: Shows how to use the REGEXP_EXTRACT function to apply a regular expression pattern and extract a capture group from a string expression.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/querying/sql.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nREGEXP_EXTRACT(expr, pattern, [index])\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license declaration for React's scheduler.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Implementing Double Sum Aggregator in Druid\nDESCRIPTION: Aggregator for computing sums as 64-bit floating point values. Requires output name and field name parameters.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/aggregations.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\" : \"doubleSum\", \"name\" : <output_name>, \"fieldName\" : <metric_name> }\n```\n\n----------------------------------------\n\nTITLE: Setting Up HDFS Directories for Druid\nDESCRIPTION: Hadoop commands to create and configure the necessary HDFS directories for Druid segment storage and to copy the input data to HDFS.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/hadoop/bin\n./hadoop fs -mkdir /druid\n./hadoop fs -mkdir /druid/segments\n./hadoop fs -mkdir /quickstart\n./hadoop fs -chmod 777 /druid\n./hadoop fs -chmod 777 /druid/segments\n./hadoop fs -chmod 777 /quickstart\n./hadoop fs -chmod -R 777 /tmp\n./hadoop fs -chmod -R 777 /user\n./hadoop fs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for React use-sync-external-store-shim in JavaScript\nDESCRIPTION: Copyright notice for the React use-sync-external-store-shim production build, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: JSON Flatten Configuration with Parse Spec\nDESCRIPTION: Configuration example showing how to flatten nested JSON using field specifications, including root, path, and jq type expressions.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/ingestion/flatten-json.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"parseSpec\": {\n  \"format\": \"json\",\n  \"flattenSpec\": {\n    \"useFieldDiscovery\": true,\n    \"fields\": [\n      {\n        \"type\": \"root\",\n        \"name\": \"dim1\"\n      },\n      \"dim2\",\n      {\n        \"type\": \"path\",\n        \"name\": \"foo.bar\",\n        \"expr\": \"$.foo.bar\"\n      },\n      {\n        \"type\": \"root\",\n        \"name\": \"foo.bar\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"path-metric\",\n        \"expr\": \"$.nestmet.val\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-0\",\n        \"expr\": \"$.hello[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"hello-4\",\n        \"expr\": \"$.hello[4]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"world-hey\",\n        \"expr\": \"$.world[0].hey\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"worldtree\",\n        \"expr\": \"$.world[1].tree\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"first-food\",\n        \"expr\": \"$.thing.food[0]\"\n      },\n      {\n        \"type\": \"path\",\n        \"name\": \"second-food\",\n        \"expr\": \"$.thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"first-food-by-jq\",\n        \"expr\": \".thing.food[1]\"\n      },\n      {\n        \"type\": \"jq\",\n        \"name\": \"hello-total\",\n        \"expr\": \".hello | sum\"\n      }\n    ]\n  },\n  \"dimensionsSpec\" : {\n   \"dimensions\" : [],\n   \"dimensionsExclusions\": [\"ignore_me\"]\n  },\n  \"timestampSpec\" : {\n   \"format\" : \"auto\",\n   \"column\" : \"timestamp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ArrayOfDoublesSketchSetOp Post-Aggregator in Druid\nDESCRIPTION: JSON configuration for the ArrayOfDoublesSketchSetOp post-aggregator. This performs set operations (union, intersection, set difference) on given array of sketches.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/development/extensions-core/datasketches-tuple.md#2025-04-09_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\"  : \"arrayOfDoublesSketchSetOp\",\n  \"name\": <output name>,\n  \"operation\": <\"UNION\"|\"INTERSECT\"|\"NOT\">,\n  \"fields\"  : <array of post aggregators to access sketch aggregators or post aggregators to allow arbitrary combination of set operations>,\n  \"nominalEntries\" : <parameter that determines the accuracy and size of the sketch>,\n  \"numberOfValues\" : <number of values associated with each distinct key>\n}\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster with Specific Options in Apache Druid\nDESCRIPTION: Command to run the ResetCluster tool with specific options to selectively reset different parts of the Druid cluster state. This allows targeting specific components like metadata store, segment files, task logs, or Hadoop working paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Processing Check Comment\nDESCRIPTION: A comment that appears to be checking if content has been previously processed. This is likely part of a mechanism to prevent duplicate processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8885.247eb1b2.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*! Check if previously processed */\n```\n\n----------------------------------------\n\nTITLE: Object-assign License Notice\nDESCRIPTION: License notice for the object-assign library by Sindre Sorhus, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Notice\nDESCRIPTION: License notice for the mark.js library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Moving Average Query Result Sample from Druid\nDESCRIPTION: Example of the response data returned by a moving average query in Druid. The result includes both the raw aggregated value (delta30Min) and the calculated moving average (trailing30MinChanges).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n   \"version\" : \"v1\",\n   \"timestamp\" : \"2015-09-12T00:30:00.000Z\",\n   \"event\" : {\n     \"delta30Min\" : 30490,\n     \"trailing30MinChanges\" : 4355.714285714285\n   }\n }, {\n   \"version\" : \"v1\",\n   \"timestamp\" : \"2015-09-12T01:00:00.000Z\",\n   \"event\" : {\n     \"delta30Min\" : 96526,\n     \"trailing30MinChanges\" : 18145.14285714286\n   }\n }, {\n...\n...\n... \n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:00:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 119100,\n    \"trailing30MinChanges\" : 198697.2857142857\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:30:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 177882,\n    \"trailing30MinChanges\" : 193890.0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Moving Average Query Result Sample from Druid\nDESCRIPTION: Example of the response data returned by a moving average query in Druid. The result includes both the raw aggregated value (delta30Min) and the calculated moving average (trailing30MinChanges).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-contrib/moving-average-query.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[ {\n   \"version\" : \"v1\",\n   \"timestamp\" : \"2015-09-12T00:30:00.000Z\",\n   \"event\" : {\n     \"delta30Min\" : 30490,\n     \"trailing30MinChanges\" : 4355.714285714285\n   }\n }, {\n   \"version\" : \"v1\",\n   \"timestamp\" : \"2015-09-12T01:00:00.000Z\",\n   \"event\" : {\n     \"delta30Min\" : 96526,\n     \"trailing30MinChanges\" : 18145.14285714286\n   }\n }, {\n...\n...\n... \n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:00:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 119100,\n    \"trailing30MinChanges\" : 198697.2857142857\n  }\n}, {\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2015-09-12T23:30:00.000Z\",\n  \"event\" : {\n    \"delta30Min\" : 177882,\n    \"trailing30MinChanges\" : 193890.0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Lexicographic Bound Filter in Druid\nDESCRIPTION: Example of a bound filter using lexicographic ordering to filter names between 'foo' and 'hoo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Lexicographic Bound Filter in Druid\nDESCRIPTION: Example of a bound filter using lexicographic ordering to filter names between 'foo' and 'hoo'.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/querying/filters.md#2025-04-09_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"bound\",\n    \"dimension\": \"name\",\n    \"lower\": \"foo\",\n    \"upper\": \"hoo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running ResetCluster with Specific Options in Apache Druid\nDESCRIPTION: Command to run the ResetCluster tool with specific options to selectively reset different parts of the Druid cluster state. This allows targeting specific components like metadata store, segment files, task logs, or Hadoop working paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/reset-cluster.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njava org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath]\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Shared Directories for Hadoop-Druid Integration\nDESCRIPTION: Commands to create temporary shared directories that will be used for file transfer between the host and the Hadoop container.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/tutorials/tutorial-batch-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p /tmp/shared\nmkdir -p /tmp/shared/hadoop_xml\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e831e4fb.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Filtering Tasks by Status in Druid SQL\nDESCRIPTION: SQL query to retrieve information about tasks with a specific status (FAILED) from the sys.tasks table.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/sql.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM sys.tasks WHERE status='FAILED';\n```\n\n----------------------------------------\n\nTITLE: Declaring Object-Assign License in JavaScript\nDESCRIPTION: This snippet declares the license information for the object-assign library, which is under the MIT license and created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Declaration\nDESCRIPTION: MIT license declaration for React's react-dom.production.min.js v17.0.2 library created by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Resetting a Supervisor in Druid\nDESCRIPTION: Use this POST endpoint to reset a running Kafka supervisor by clearing stored offsets. This causes the supervisor to start reading from either earliest or latest offsets in Kafka based on configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/reset\n```\n\n----------------------------------------\n\nTITLE: Resetting a Supervisor in Druid\nDESCRIPTION: Use this POST endpoint to reset a running Kafka supervisor by clearing stored offsets. This causes the supervisor to start reading from either earliest or latest offsets in Kafka based on configuration.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/development/extensions-core/kafka-ingestion.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\nPOST /druid/indexer/v1/supervisor/<supervisorId>/reset\n```\n\n----------------------------------------\n\nTITLE: Configuring Derby Metadata Storage in Druid\nDESCRIPTION: Example configuration for using Derby as the metadata storage in Druid. Specifies the storage type and connection URI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Derby Metadata Storage in Druid\nDESCRIPTION: Example configuration for using Derby as the metadata storage in Druid. Specifies the storage type and connection URI.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/dependencies/metadata-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndruid.metadata.storage.type=derby\ndruid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true\n```\n\n----------------------------------------\n\nTITLE: Building Apache Druid with Maven in Bash\nDESCRIPTION: Basic Maven command to build Druid from source, running static analysis, unit tests, compiling classes, and packaging projects into JARs.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/build.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: Interval Drop Rule Configuration in Druid\nDESCRIPTION: Specifies a time interval for dropping segments using ISO-8601 format. Segments within the specified interval will be dropped.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store-Shim License Header\nDESCRIPTION: License header for the React use-sync-external-store-shim production module, developed by Facebook and affiliates under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e831e4fb.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Classnames License Header\nDESCRIPTION: MIT license header for the classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fea5e92a.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React in JavaScript\nDESCRIPTION: This snippet declares the MIT license for the React production build, attributing it to Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Parser in Druid JSON\nDESCRIPTION: This snippet demonstrates how to configure the JavaScript parser in Druid's JSON configuration. It includes settings for timestamp specification, dimension specification, and a custom JavaScript function for parsing data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"javascript\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    },\n    \"function\" : \"function(str) { var parts = str.split(\\\"-\\\"); return { one: parts[0], two: parts[1] } }\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Parser in Druid JSON\nDESCRIPTION: This snippet demonstrates how to configure the JavaScript parser in Druid's JSON configuration. It includes settings for timestamp specification, dimension specification, and a custom JavaScript function for parsing data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n  \"parseSpec\":{\n    \"format\" : \"javascript\",\n    \"timestampSpec\" : {\n      \"column\" : \"timestamp\"\n    },        \n    \"dimensionsSpec\" : {\n      \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"]\n    },\n    \"function\" : \"function(str) { var parts = str.split(\\\"-\\\"); return { one: parts[0], two: parts[1] } }\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom TLS Certificate Checker in Apache Druid\nDESCRIPTION: This configuration table shows the setting for using a custom TLS certificate checker in Apache Druid.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/tls-support.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n|Property|Description|Default|Required|\n|--------|-----------|-------|--------|\n|`druid.tls.certificateChecker`|Type name of custom TLS certificate checker, provided by extensions. Please refer to extension documentation for the type name that should be specified.|\"default\"|no|\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Drop Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines an interval drop rule, which indicates that segments within the specified time interval should be dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropByInterval\",\n  \"interval\" : \"2012-01-01/2013-01-01\"\n}\n```\n\n----------------------------------------\n\nTITLE: License Notice for React Is\nDESCRIPTION: This snippet provides the license information for the React Is production build, which is part of the React library and is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.e0289f0e.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Example Live Row Stats Response\nDESCRIPTION: Sample JSON response showing moving averages and totals for row processing statistics\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.1-incubating/ingestion/reports.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"movingAverages\": {\n    \"buildSegments\": {\n      \"5m\": {\n        \"processed\": 3.392158326408501,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"15m\": {\n        \"processed\": 1.736165476881023,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      },\n      \"1m\": {\n        \"processed\": 4.206417693750045,\n        \"unparseable\": 0,\n        \"thrownAway\": 0,\n        \"processedWithError\": 0\n      }\n    }\n  },\n  \"totals\": {\n    \"buildSegments\": {\n      \"processed\": 1994,\n      \"processedWithError\": 0,\n      \"thrownAway\": 0,\n      \"unparseable\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Bezier Curve Functions in JavaScript\nDESCRIPTION: A utility for generating Bezier curve functions created by Gaetan Renaudeau. Used for creating smooth animation curves.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8055.09efec4d.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/*! Bezier curve function generator. Copyright Gaetan Renaudeau. MIT License: http://en.wikipedia.org/wiki/MIT_License */\n```\n\n----------------------------------------\n\nTITLE: Classnames License Header\nDESCRIPTION: MIT license header for classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a03dfc13.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Submitting an Ingestion Task in Apache Druid using Bash\nDESCRIPTION: Command to submit an ingestion task specification file to Druid's indexing service. The task creates a datasource called 'retention-tutorial' with hourly segments from Wikipedia edits sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-retention.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/retention-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Submitting an Ingestion Task in Apache Druid using Bash\nDESCRIPTION: Command to submit an ingestion task specification file to Druid's indexing service. The task creates a datasource called 'retention-tutorial' with hourly segments from Wikipedia edits sample data.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-retention.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/retention-index.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Initializing Jekyll Front Matter in Markdown\nDESCRIPTION: Basic Jekyll front matter configuration specifying the layout type as 'toc' for table of contents.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/toc.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: toc\n---\n```\n\n----------------------------------------\n\nTITLE: Manually Disabling Druid Segment via API\nDESCRIPTION: cURL command to disable a specific segment through the Coordinator API, marking it as unused but not removing it from deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -XDELETE http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/segments/{SEGMENT-ID}\n```\n\n----------------------------------------\n\nTITLE: Running Router Process Command\nDESCRIPTION: Basic command to start the Druid Router server process.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/router.md#2025-04-09_snippet_0\n\nLANGUAGE: java\nCODE:\n```\norg.apache.druid.cli.Main server router\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for object-assign\nDESCRIPTION: This snippet provides the copyright and license information for the object-assign library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f8032a27.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License Attribution\nDESCRIPTION: MIT License attribution for Prism, a lightweight syntax highlighting library created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.f9c13e33.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JSDoc\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Sample Multi-value Data Structure\nDESCRIPTION: Example showing the structure of data with multi-value dimensions using the 'tags' field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2011-01-12T00:00:00.000Z\", \"tags\": [\"t1\",\"t2\",\"t3\"]}\n{\"timestamp\": \"2011-01-13T00:00:00.000Z\", \"tags\": [\"t3\",\"t4\",\"t5\"]}\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": [\"t5\",\"t6\",\"t7\"]}\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": []}\n```\n\n----------------------------------------\n\nTITLE: Sample Multi-value Data Structure\nDESCRIPTION: Example showing the structure of data with multi-value dimensions using the 'tags' field.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/querying/multi-value-dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"timestamp\": \"2011-01-12T00:00:00.000Z\", \"tags\": [\"t1\",\"t2\",\"t3\"]}\n{\"timestamp\": \"2011-01-13T00:00:00.000Z\", \"tags\": [\"t3\",\"t4\",\"t5\"]}\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": [\"t5\",\"t6\",\"t7\"]}\n{\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": []}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.14f7867a.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Executing Kill Task in Druid\nDESCRIPTION: cURL command to submit a Kill Task that permanently removes disabled segments from metadata and deep storage.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/tutorials/tutorial-delete-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8090/druid/indexer/v1/task\n```\n\n----------------------------------------\n\nTITLE: Configuring Period Drop Before Rule in Apache Druid\nDESCRIPTION: This JSON configuration defines a period drop before rule, which indicates that segments before a specified rolling time period should be dropped from the cluster.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/rule-configuration.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"dropBeforeByPeriod\",\n  \"period\" : \"P1M\"\n}\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0d1c82da.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Declaration\nDESCRIPTION: License declaration for React's react-dom.production.min.js under MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Appending Data in Apache Druid\nDESCRIPTION: This command submits a task to append new data to the existing data in the 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index2.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Appending Data in Apache Druid\nDESCRIPTION: This command submits a task to append new data to the existing data in the 'updates-tutorial' datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/tutorials/tutorial-update-data.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbin/post-index-task --file quickstart/tutorial/updates-append-index2.json --url http://localhost:8081\n```\n\n----------------------------------------\n\nTITLE: Building Druid Distribution with Additional Profiles\nDESCRIPTION: Advanced Maven command to build Druid source and binary distributions with signatures, checksums, license auditing, and skipping unit tests.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/development/build.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Papache-release,dist,rat -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Comment in HTML\nDESCRIPTION: This HTML comment contains the Apache License 2.0 text, which is typically included at the beginning of source files in Apache Software Foundation projects.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/toc.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c344594c.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Comment\nDESCRIPTION: License comment for the mark.js library by Julian Khnel, version 8.11.1, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: DOMPurify License Declaration\nDESCRIPTION: License comment for DOMPurify library version 2.4.3, which is a DOM-based HTML sanitizer. The library is released under both Apache License 2.0 and Mozilla Public License 2.0.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8885.247eb1b2.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/*! @license DOMPurify 2.4.3 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/2.4.3/LICENSE */\n```\n\n----------------------------------------\n\nTITLE: Configuring Arbitrary Granularity Spec in Druid Ingestion\nDESCRIPTION: This JSON snippet defines the structure of an arbitrary granularity spec in Druid. It specifies fields for queryGranularity, rollup, and intervals. This configuration is used to generate segments with arbitrary intervals and is not supported for real-time processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": [\"JSON string array\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Arbitrary Granularity Spec in Druid Ingestion\nDESCRIPTION: This JSON snippet defines the structure of an arbitrary granularity spec in Druid. It specifies fields for queryGranularity, rollup, and intervals. This configuration is used to generate segments with arbitrary intervals and is not supported for real-time processing.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/ingestion/ingestion-spec.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"queryGranularity\": \"string\",\n  \"rollup\": \"boolean\",\n  \"intervals\": [\"JSON string array\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Cassandra Storage Schema for Druid\nDESCRIPTION: SQL create statements for the required Cassandra tables. Creates index_storage table for storing compressed segments and descriptor_storage table for segment metadata.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/dependencies/cassandra-deep-storage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE index_storage(key text,\n                           chunk text,\n                           value blob,\n                           PRIMARY KEY (key, chunk)) WITH COMPACT STORAGE;\n\nCREATE TABLE descriptor_storage(key varchar,\n                                lastModified timestamp,\n                                descriptor varchar,\n                                PRIMARY KEY (key)) WITH COMPACT STORAGE;\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for React Core\nDESCRIPTION: This snippet contains the copyright notice and license information for the React core production file.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c344594c.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Copyright Notice for object-assign Library\nDESCRIPTION: This snippet contains the copyright notice and license information for the object-assign library by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.3bf20a9f.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: js-yaml License Declaration\nDESCRIPTION: License header for js-yaml library version 4.1.0, released under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.0e5d9014.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Graphite Emitter\nDESCRIPTION: Configuration example for the 'all' type event converter that sends all Druid service metrics to Graphite. This configuration ignores hostname and service name in the metrics path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Graphite Emitter\nDESCRIPTION: Configuration example for the 'all' type event converter that sends all Druid service metrics to Graphite. This configuration ignores hostname and service name in the metrics path.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/graphite.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"type\":\"all\", \"namespacePrefix\": \"druid.test\", \"ignoreHostname\":true, \"ignoreServiceName\":true}\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license header for the Prism syntax highlighting library by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.7d317958.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Prism License Notice\nDESCRIPTION: License notice for the Prism syntax highlighting library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: License Comment for Prism\nDESCRIPTION: Copyright and license information for the Prism syntax highlighting library by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.55eacc87.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for Lunr.js Components\nDESCRIPTION: This snippet shows the copyright declaration format used for various Lunr.js components. It includes the component name, copyright year, and author.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/4611.d3c34935.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\\n * lunr.Builder\\n * Copyright (C) 2020 Oliver Nightingale\\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticCloudFilesFirehose in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticCloudFilesFirehose in Druid. It specifies the firehose type and an array of Cloud Files blobs to ingest, including their regions, containers, and file paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/cloudfiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-cloudfiles\",\n    \"blobs\": [\n        {\n          \"region\": \"DFW\"\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"region\": \"ORD\"\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticCloudFilesFirehose in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the StaticCloudFilesFirehose in Druid. It specifies the firehose type and an array of Cloud Files blobs to ingest, including their regions, containers, and file paths.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/development/extensions-contrib/cloudfiles.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"firehose\" : {\n    \"type\" : \"static-cloudfiles\",\n    \"blobs\": [\n        {\n          \"region\": \"DFW\"\n          \"container\": \"container\",\n          \"path\": \"/path/to/your/file.json\"\n        },\n        {\n          \"region\": \"ORD\"\n          \"container\": \"anothercontainer\",\n          \"path\": \"/another/path.json\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for mark.js library by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Get Single Lookup Response Example\nDESCRIPTION: Example JSON response from the GET /druid/listen/v1/lookups/{lookup_name} endpoint showing details for a specific lookup.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/querying/lookups.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for mark.js in JavaScript\nDESCRIPTION: This snippet declares the MIT license for mark.js, specifying the version, website, copyright holder, and providing a link to the full license text.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c260a6d4.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license declaration for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.964b4547.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Page Layout and Title\nDESCRIPTION: Sets the layout and title for the Markdown documentation page using YAML front matter.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/design/indexing-service.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: doc_page\ntitle: \"Indexing Service\"\n---\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Lookup Factory in Druid (JSON)\nDESCRIPTION: Example of a GET request response for retrieving a specific LookupExtractorFactory from a Druid process. The response includes the version and factory details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/lookups.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Lookup Factory in Druid (JSON)\nDESCRIPTION: Example of a GET request response for retrieving a specific LookupExtractorFactory from a Druid process. The response includes the version and factory details.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/querying/lookups.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"v1\",\n  \"lookupExtractorFactory\": {\n    \"type\": \"map\",\n    \"map\": {\n      \"AHF77\": \"Home\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header in JavaScript\nDESCRIPTION: License header for mark.js version 8.11.1, a library used for highlighting text. Created by Julian Khnel and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: React Use Sync External Store License\nDESCRIPTION: MIT license header for React use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a03dfc13.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Defining OR Logical Expression Filter in Apache Druid JSON Query\nDESCRIPTION: The OR filter combines multiple filters using logical OR. At least one of the specified filters must match for a row to be included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: Defining OR Logical Expression Filter in Apache Druid JSON Query\nDESCRIPTION: The OR filter combines multiple filters using logical OR. At least one of the specified filters must match for a row to be included in the result.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/querying/filters.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"filter\": { \"type\": \"or\", \"fields\": [<filter>, <filter>, ...] }\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: MIT license header for react.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration for React use-sync-external-store-shim\nDESCRIPTION: License declaration for React's use-sync-external-store-shim production module created by Facebook under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ff51740e.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Querying Druid Overlord Leadership\nDESCRIPTION: GET requests to check the current leader Overlord and leadership status of the server.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/operations/api-reference.md#2025-04-09_snippet_12\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /druid/indexer/v1/leader\nGET /druid/indexer/v1/isLeader\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Header\nDESCRIPTION: License header for React's scheduler.production.min.js (version 0.20.2) module, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Notice\nDESCRIPTION: License notice for the NProgress library by Rico Sta. Cruz, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React DOM License Declaration in JavaScript\nDESCRIPTION: License declaration for React's react-dom.production.min.js version 17.0.2, created by Facebook. The library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Comment\nDESCRIPTION: License comment for the React scheduler production build, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: MIT license declaration for React's react-dom.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Classnames Copyright Notice\nDESCRIPTION: Copyright notice for the classnames library by Jed Watson, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React Is\nDESCRIPTION: Copyright and license declaration for the React Is production minified file, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Enabling a Specific Segment in Druid Coordinator API\nDESCRIPTION: POST endpoint to enable a specific segment for a datasource.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/api-reference.md#2025-04-09_snippet_1\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId}\n```\n\n----------------------------------------\n\nTITLE: NProgress License Comment\nDESCRIPTION: License comment for the NProgress library, indicating its author, website, and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header in JavaScript\nDESCRIPTION: License header for the NProgress library created by Rico Sta. Cruz. This library is used for displaying progress indicators and is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Comment\nDESCRIPTION: License comment for the NProgress library, which is licensed under the MIT License and created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6f6dba15.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license declaration for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d8507f6b.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress MIT License Declaration\nDESCRIPTION: License declaration for NProgress library created by Rico Sta. Cruz under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Declaring License for NProgress in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the NProgress library, created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Accessing Druid Console URL Pattern\nDESCRIPTION: The URL pattern to access the Druid Console through the Router process. Requires Router management proxy to be enabled and Druid SQL to be enabled on Broker processes.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/operations/druid-console.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://<ROUTER_IP>:<ROUTER_PORT>\n```\n\n----------------------------------------\n\nTITLE: React DOM License Header in JavaScript\nDESCRIPTION: License header for React DOM library version 17.0.2. This is the DOM-specific package for React that handles rendering React components to the DOM, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.67b4a6be.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Declaration\nDESCRIPTION: MIT license declaration for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Document Load Event Handler Comment\nDESCRIPTION: A comment indicating code that waits for the document to be fully loaded before executing. This is a common pattern in JavaScript to ensure DOM elements are available for manipulation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8885.247eb1b2.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n   * Wait for document loaded before starting the execution\n   */\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for NProgress Library in JavaScript\nDESCRIPTION: Copyright notice for the NProgress library, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license declaration for React use-sync-external-store-shim production bundle\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: License Comment for mark.js\nDESCRIPTION: Copyright and license information for the mark.js library by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9dc00f46.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: React-is License Declaration in JavaScript\nDESCRIPTION: License declaration for React's react-is.production.min.js version 16.13.1, created by Facebook. The library is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.9b9fa961.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: Copyright notice and MIT license declaration for the scheduler.production.min.js file from React.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Comment\nDESCRIPTION: License comment for the object-assign library, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: MIT license declaration for react.production.min.js\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React DOM\nDESCRIPTION: Specifies the MIT license for the React DOM production module (version 17.0.2).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache License 2.0 header comment block describing the licensing terms for the documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/comparisons/druid-vs-sql-on-hadoop.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: React Core License Header\nDESCRIPTION: License header for React's core react.production.min.js (version 17.0.2) module, created by Facebook and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Header\nDESCRIPTION: MIT license header for mark.js v8.11.1 by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.7d317958.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n*****************************************************/\n\n```\n\n----------------------------------------\n\nTITLE: ResetCluster Tool Help Output\nDESCRIPTION: Complete help documentation output showing the tool's name, synopsis, and available options for cleaning up different components of the Druid cluster state.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/reset-cluster.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nNAME\n        druid tools reset-cluster - Cleanup all persisted state from metadata\n        and deep storage.\n\nSYNOPSIS\n        druid tools reset-cluster [--all] [--hadoopWorkingPath]\n                [--metadataStore] [--segmentFiles] [--taskLogs]\n\nOPTIONS\n        --all\n            delete all state stored in metadata and deep storage\n\n        --hadoopWorkingPath\n            delete hadoopWorkingPath\n\n        --metadataStore\n            delete all records in metadata storage\n\n        --segmentFiles\n            delete all segment files from deep storage\n\n        --taskLogs\n            delete all tasklogs\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Notice\nDESCRIPTION: License notice for the mark.js library by Julian Khnel, version 8.11.1, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b2b0e660.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: MySQL Import SQL Commands\nDESCRIPTION: SQL commands for importing CSV metadata into MySQL database tables with field specifications.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.1-incubating/operations/export-metadata.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nLOAD DATA INFILE '/tmp/csv/druid_segments.csv' INTO TABLE druid_segments FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,created_date,start,end,partitioned,version,used,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_rules.csv' INTO TABLE druid_rules FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,version,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_config.csv' INTO TABLE druid_config FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (name,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_dataSource.csv' INTO TABLE druid_dataSource FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (dataSource,created_date,commit_metadata_payload,commit_metadata_sha1); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_supervisors.csv' INTO TABLE druid_supervisors FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,spec_id,created_date,payload); SHOW WARNINGS;\n```\n\n----------------------------------------\n\nTITLE: Declaring License for React use-sync-external-store-shim in JavaScript\nDESCRIPTION: This code snippet declares the MIT license for the React use-sync-external-store-shim production minified file, created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.94f86a55.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store-Shim License Declaration\nDESCRIPTION: License declaration for React's use-sync-external-store-shim.production.min.js under MIT license by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.266eb8e0.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variable Password Provider in Apache Druid\nDESCRIPTION: This snippet demonstrates how to configure an environment variable password provider in Apache Druid. It specifies the type as 'environment' and the variable name to read the password from.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/operations/password-provider.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" }\n```\n\n----------------------------------------\n\nTITLE: React Core License\nDESCRIPTION: Copyright notice and MIT license declaration for the react.production.min.js file from React.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c4f9b04c.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: MIT license declaration for react-dom.production.min.js\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bfb8da85.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Certificate Authentication in Apache Druid\nDESCRIPTION: These properties control client certificate authentication settings in Apache Druid. They include options for requiring client certificates, configuring trust stores, and validating hostnames.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.13.0-incubating/operations/tls-support.md#2025-04-09_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\ndruid.server.https.requireClientCertificate=false\ndruid.server.https.trustStoreType=java.security.KeyStore.getDefaultType()\ndruid.server.https.trustStorePath=none\ndruid.server.https.trustStoreAlgorithm=javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()\ndruid.server.https.trustStorePassword=none\ndruid.server.https.validateHostnames=true\ndruid.server.https.crlPath=null\n```\n\n----------------------------------------\n\nTITLE: Classnames License Declaration\nDESCRIPTION: MIT license declaration for the classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license declaration for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign package by Sindre Sorhus\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.832012d1.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for mark.js\nDESCRIPTION: Specifies the MIT license for mark.js library (version 8.11.1) created by Julian Khnel.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring React-Is License in JavaScript\nDESCRIPTION: This snippet declares the license information for the React-Is production module version 16.13.1, which is under the MIT license and created by Facebook, Inc. and its affiliates.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Setting Role Permissions in Druid (JSON)\nDESCRIPTION: This snippet demonstrates how to set permissions for a role in Druid using a POST request. It shows the JSON structure for defining resource-action objects, which specify access levels for different datasources.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.0-incubating/development/extensions-core/druid-basic-security.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n{\n  \"resource\": {\n    \"name\": \"wiki.*\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"READ\"\n},\n{\n  \"resource\": {\n    \"name\": \"wikiticker\",\n    \"type\": \"DATASOURCE\"\n  },\n  \"action\": \"WRITE\"\n}\n]\n```\n\n----------------------------------------\n\nTITLE: NProgress License\nDESCRIPTION: MIT license declaration for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React Core License Comment\nDESCRIPTION: License comment for the React core production file, indicating its version, copyright, and MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.6d090bbf.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring Prism License in JavaScript\nDESCRIPTION: This snippet declares the license information for Prism, a syntax highlighting library, which is under the MIT license and created by Lea Verou.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c54d5655.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: Object-Assign MIT License Header\nDESCRIPTION: License header for the object-assign library created by Sindre Sorhus, released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a09c24e0.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.a03dfc13.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React-DOM License Comment\nDESCRIPTION: License comment for the React-DOM production build, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Boundary Strategy for Router\nDESCRIPTION: JSON configuration for the timeBoundary strategy, which routes all timeBoundary queries to the highest priority Broker.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/development/router.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\":\"timeBoundary\"\n}\n```\n\n----------------------------------------\n\nTITLE: React Core License Notice\nDESCRIPTION: License notice for the React core production module, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.8b320f33.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React DOM License\nDESCRIPTION: MIT license declaration for React's react-dom.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react-dom.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: js-yaml License Information in JavaScript\nDESCRIPTION: This comment provides license information for js-yaml version 4.1.0. It includes a link to the GitHub repository and specifies that it's released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/6062.37a11f44.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Prism Syntax Highlighter License\nDESCRIPTION: MIT license declaration for the Prism syntax highlighting library by Lea Verou\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.36878329.js.LICENSE.txt#2025-04-09_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * Prism: Lightweight, robust, elegant syntax highlighting\n *\n * @license MIT <https://opensource.org/licenses/MIT>\n * @author Lea Verou <https://lea.verou.me>\n * @namespace\n * @public\n */\n```\n\n----------------------------------------\n\nTITLE: SQL Like Pattern Example\nDESCRIPTION: Demonstrates usage of the LIKE operator in Druid expressions for pattern matching, equivalent to SQL LIKE syntax.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/misc/math-expr.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nexpr LIKE pattern\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React Scheduler\nDESCRIPTION: Specifies the MIT license for the React Scheduler production module (version 0.20.2).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c12efa35.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object Assign License Header\nDESCRIPTION: MIT license header for the object-assign library by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.7d317958.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Front Matter for Jekyll\nDESCRIPTION: This code snippet defines the front matter for a Jekyll-based website, specifying the layout as 'toc' (table of contents).\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.14.2-incubating/toc.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: toc\n---\n```\n\n----------------------------------------\n\nTITLE: MIT License Header for React-is\nDESCRIPTION: Copyright notice and MIT license information for React's react-is.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.1f0e5e69.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Apache License Comment Block in HTML\nDESCRIPTION: Standard Apache License 2.0 comment block indicating the licensing terms for the documentation.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/docs/0.15.0-incubating/toc.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n```\n\n----------------------------------------\n\nTITLE: Implementing Runge-Kutta Spring Physics in JavaScript\nDESCRIPTION: A spring physics function generator using the Runge-Kutta method, adapted from Framer.js by Koen Bok. Used for creating spring-based animations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8055.09efec4d.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/*! Runge-Kutta spring physics function generator. Adapted from Framer.js, copyright Koen Bok. MIT License: http://en.wikipedia.org/wiki/MIT_License */\n```\n\n----------------------------------------\n\nTITLE: Generating Bezier Curve Functions in JavaScript\nDESCRIPTION: A function generator for creating Bezier curve functions. This utility allows for the creation of custom Bezier curve calculations, which are often used in animations and graphical operations.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/7724.21e9de4d.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*! Bezier curve function generator. Copyright Gaetan Renaudeau. MIT License: http://en.wikipedia.org/wiki/MIT_License */\n```\n\n----------------------------------------\n\nTITLE: Lunr.js Component Copyright Declarations\nDESCRIPTION: Copyright declarations for individual Lunr.js components including Builder, Index, Pipeline, Set, TokenSet, Vector, stemmer, filters, and utilities.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/4611.8d581fc8.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n * lunr.Builder\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.Index\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.Pipeline\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.Set\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.TokenSet\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.Vector\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.stemmer\n * Copyright (C) 2020 Oliver Nightingale\n * Includes code from - http://tartarus.org/~martin/PorterStemmer/js.txt\n */\n\n/*!\n * lunr.stopWordFilter\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.tokenizer\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.trimmer\n * Copyright (C) 2020 Oliver Nightingale\n */\n\n/*!\n * lunr.utils\n * Copyright (C) 2020 Oliver Nightingale\n */\n```\n\n----------------------------------------\n\nTITLE: Documenting MIT License for mark.js\nDESCRIPTION: This comment block provides license and copyright information for mark.js, a highlighting library created by Julian Khnel and released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fbb4143b.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: MIT license header for the NProgress library by Rico Sta. Cruz\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.fea5e92a.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Object-Assign License Declaration\nDESCRIPTION: MIT license declaration for the object-assign library created by Sindre Sorhus.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Declaration\nDESCRIPTION: MIT license declaration for React's use-sync-external-store-shim.production.min.js library created by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.dc9c2270.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Declaration\nDESCRIPTION: MIT license declaration for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d8507f6b.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: Declaring Copyright for React-Is in JavaScript\nDESCRIPTION: Copyright notice for the React-Is production build, version 16.13.1, which is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.c745f3cf.js.LICENSE.txt#2025-04-09_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v16.13.1\n * react-is.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Object Assign Copyright Notice\nDESCRIPTION: Copyright notice for the object-assign library by Sindre Sorhus, licensed under MIT.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.38148853.js.LICENSE.txt#2025-04-09_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\nobject-assign\n(c) Sindre Sorhus\n@license MIT\n*/\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim MIT License Declaration\nDESCRIPTION: License declaration for use-sync-external-store-shim.production.min.js React library by Facebook under MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.5e106d68.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React External Store Shim License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production build\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.14f7867a.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Sync External Store License\nDESCRIPTION: MIT license header for React's use-sync-external-store-shim production module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.443e418e.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: NProgress License Header\nDESCRIPTION: Copyright notice and MIT license declaration for the NProgress library created by Rico Sta. Cruz.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress\n * @license MIT */\n```\n\n----------------------------------------\n\nTITLE: React Core License Header\nDESCRIPTION: Copyright notice and MIT license declaration for React's react.production.min.js module by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.13591f92.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Declaring MIT License for React use-sync-external-store-shim\nDESCRIPTION: Copyright and license declaration for the React use-sync-external-store-shim production minified file, which is licensed under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.d3731f75.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License\nDESCRIPTION: MIT license declaration for React's scheduler.production.min.js module\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.b91f2991.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: Mark.js License Declaration\nDESCRIPTION: MIT license declaration for mark.js v8.11.1 by Julian Khnel\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.06dcc5e2.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!***************************************************\n* mark.js v8.11.1\n* https://markjs.io/\n* Copyright (c) 20142018, Julian Khnel\n* Released under the MIT license https://git.io/vwTVl\n****************************************************/\n```\n\n----------------------------------------\n\nTITLE: License Declaration for React Core Library\nDESCRIPTION: MIT license declaration for the react.production.min.js library, version 17.0.2, developed by Facebook.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v17.0.2\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Scheduler License Comment\nDESCRIPTION: License comment for the React scheduler production module, version 0.20.2, released under the MIT license by Facebook, Inc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n/** @license React v0.20.2\n * scheduler.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: License Declaration for classnames Library\nDESCRIPTION: MIT license declaration for the classnames library created by Jed Watson.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.04388697.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store Shim MIT License Declaration\nDESCRIPTION: License declaration for the React use-sync-external-store-shim production module, created by Facebook, Inc. and its affiliates under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.ca618e05.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: js-yaml License Declaration\nDESCRIPTION: License comment for js-yaml library version 4.1.0, which is a JavaScript parser and serializer for YAML. The library is released under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/8885.247eb1b2.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n/*! js-yaml 4.1.0 https://github.com/nodeca/js-yaml @license MIT */\n```\n\n----------------------------------------\n\nTITLE: Classnames License Header\nDESCRIPTION: MIT license header for the classnames library by Jed Watson\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.0c610519.js.LICENSE.txt#2025-04-09_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*!\n\tCopyright (c) 2018 Jed Watson.\n\tLicensed under the MIT License (MIT), see\n\thttp://jedwatson.github.io/classnames\n*/\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Comment\nDESCRIPTION: License comment for the React use-sync-external-store-shim production module, released under the MIT license by Facebook, Inc.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.43e1b59a.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```\n\n----------------------------------------\n\nTITLE: React Use-Sync-External-Store License Comment\nDESCRIPTION: License comment for the React use-sync-external-store-shim production build, which is under the MIT license.\nSOURCE: https://github.com/apache/druid-website/blob/asf-site/assets/js/main.bd54ee66.js.LICENSE.txt#2025-04-09_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n/**\n * @license React\n * use-sync-external-store-shim.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n```"
  }
]
[
  {
    "owner": "delta-io",
    "repo": "delta-rs",
    "content": "TITLE: Creating a Delta Lake Table with pandas in Python\nDESCRIPTION: Creates a Delta Lake table from a pandas DataFrame containing numeric and string columns. Uses the write_deltalake function to write the DataFrame to a specified location.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/create-delta-lake-table.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import write_deltalake\nimport pandas as pd\n\ndf = pd.DataFrame({\"num\": [1, 2, 3], \"letter\": [\"a\", \"b\", \"c\"]})\nwrite_deltalake(\"tmp/some-table\", df)\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Tables into Pandas and PyArrow\nDESCRIPTION: Demonstrates how to load Delta tables into Pandas DataFrames and PyArrow tables with partition filtering and column selection.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/querying-delta-tables.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.8.0-partitioned\")\n>>> dt.schema().to_pyarrow()\nvalue: string\nyear: string\nmonth: string\nday: string\n>>> dt.to_pandas(partitions=[(\"year\", \"=\", \"2021\")], columns=[\"value\"])\n      value\n0     6\n1     7\n2     5\n3     4\n>>> dt.to_pyarrow_table(partitions=[(\"year\", \"=\", \"2021\")], columns=[\"value\"])\npyarrow.Table\nvalue: string\n```\n\n----------------------------------------\n\nTITLE: Creating and Reading Delta Tables in Python\nDESCRIPTION: Shows how to write data to a Delta table from a Pandas DataFrame and then read it back. This demonstrates the basic workflow for Delta Lake operations in Python.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import DeltaTable, write_deltalake\nimport pandas as pd\n\n# write some data into a delta table\ndf = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"]})\nwrite_deltalake(\"./data/delta\", df)\n\n# Load data from the delta table\ndt = DeltaTable(\"./data/delta\")\ndf2 = dt.to_pandas()\n\nassert df.equals(df2)\n```\n\n----------------------------------------\n\nTITLE: Time Travel to Previous Version in Python\nDESCRIPTION: This snippet demonstrates how to perform time travel to access a previous version of a Delta table in Python. It uses the DeltaTable class with a specified version number.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/appending-overwriting-delta-lake-table.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(\"tmp/some-table\", version=1)\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Lake Table with Polars in Python\nDESCRIPTION: Creates a Delta Lake table using a Polars DataFrame with numeric and string columns. Uses the write_delta method to write the DataFrame to a specified location.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/create-delta-lake-table.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\ndf = pl.DataFrame({\"num\": [1, 2, 3], \"letter\": [\"a\", \"b\", \"c\"]})\ndf.write_delta(\"tmp/some-table\")\n```\n\n----------------------------------------\n\nTITLE: Writing to S3 with DynamoDB Locking in Delta Lake using Python\nDESCRIPTION: This Python code demonstrates how to write data to an S3 location using Delta Lake with DynamoDB as the locking provider. It shows how to set up storage options, including specifying the DynamoDB table name and locking provider.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/writing-to-s3-with-locking-provider.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import write_deltalake\ndf = pd.DataFrame({'x': [1, 2, 3]})\nstorage_options = {\n    'AWS_S3_LOCKING_PROVIDER': 'dynamodb',\n    'DELTA_DYNAMO_TABLE_NAME': 'custom_table_name'\n}\nwrite_deltalake(\n    's3a://path/to/table',\n    df,\n    storage_options=storage_options\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Lake Table from pandas DataFrame\nDESCRIPTION: Example showing how to create a new Delta Lake table from a pandas DataFrame. This creates version 0 of the Delta table with three rows of data containing numeric and string columns.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import write_deltalake, DeltaTable\n\ndf = pd.DataFrame({\"num\": [1, 2, 3], \"letter\": [\"a\", \"b\", \"c\"]})\nwrite_deltalake(\"tmp/some-table\", df)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table History in Python\nDESCRIPTION: Gets the history of operations performed on a Delta table using DeltaTable.history(), showing timestamps, operations, parameters, and versions.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import DeltaTable\n\ndt = DeltaTable(\"../rust/tests/data/simple_table\")\ndt.history()\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Delta Table - Python\nDESCRIPTION: Basic example of writing a Pandas DataFrame to a Delta table using write_deltalake function. Shows both new table creation and append/overwrite modes.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import write_deltalake\n>>> df = pd.DataFrame({'x': [1, 2, 3]})\n>>> write_deltalake('path/to/table', df)\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows with Python using Delta Lake\nDESCRIPTION: Deletes rows from a Delta table where the 'num' column value is greater than 2 using Python. The DeltaTable class is used to open the table and the delete method accepts a SQL WHERE clause as a predicate.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/deleting-rows-from-delta-lake-table.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(\"tmp/my-table\")\ndt.delete(\"num > 2\")\n```\n\n----------------------------------------\n\nTITLE: Basic Writing to Delta Tables with Python\nDESCRIPTION: Demonstrates how to write a Pandas DataFrame to a Delta table using the write_deltalake function. The function accepts Pandas DataFrames but converts them to Arrow tables before writing.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/index.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import write_deltalake\n>>> df = pd.DataFrame({'x': [1, 2, 3]})\n>>> write_deltalake('path/to/table', df)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Metadata in Rust\nDESCRIPTION: Retrieves basic metadata from a Delta table including ID, name, description, partition columns, creation time, and configuration in Rust.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"../rust/tests/data/simple_table\").await?;\nlet metadata = table.metadata()?;\nprintln!(\"metadata: {:?}\", metadata);\n```\n\n----------------------------------------\n\nTITLE: Optimizing Delta Table with Z-Order in Python\nDESCRIPTION: Demonstrates how to optimize a Delta table using Z-Order, which can improve performance for queries that filter on multiple columns simultaneously.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/COVID-19_NYT\")\n>>> dt.optimize.z_order([\"date\", \"county\"])\n{'numFilesAdded': 1, 'numFilesRemoved': 8,\n 'filesAdded': {'min': 2473439, 'max': 2473439, 'avg': 2473439.0, 'totalFiles': 1, 'totalSize': 2473439},\n 'filesRemoved': {'min': 325440, 'max': 895702, 'avg': 773810.625, 'totalFiles': 8, 'totalSize': 6190485},\n 'partitionsOptimized': 0, 'numBatches': 1, 'totalConsideredFiles': 8,\n```\n\n----------------------------------------\n\nTITLE: Creating and Writing to Delta Table with Polars\nDESCRIPTION: This snippet demonstrates how to create a Polars DataFrame and write it to a Delta table. It also shows how to append data and overwrite the existing table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-polars.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\ndf = pl.DataFrame({\"x\": [1, 2, 3]})\ndf.write_delta(\"tmp/bear_delta_lake\")\n\ndf2 = pl.DataFrame({\"x\": [8, 9, 10]})\ndf2.write_delta(\"tmp/bear_delta_lake\", mode=\"append\")\n\ndf3 = pl.DataFrame({\"x\": [55, 66, 77]})\ndf3.write_delta(\"tmp/bear_delta_lake\", mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Appending Data to a Delta Lake Table\nDESCRIPTION: Example demonstrating how to append data to an existing Delta Lake table. This creates version 1 of the Delta table by adding two new rows to the original dataset.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"num\": [8, 9], \"letter\": [\"dd\", \"ee\"]})\nwrite_deltalake(\"tmp/some-table\", df, mode=\"append\")\n```\n\n----------------------------------------\n\nTITLE: Loading DeltaTable into Pandas DataFrame\nDESCRIPTION: Python code snippet demonstrating how to load a DeltaTable into a Pandas DataFrame using the to_pandas method.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnew_df = dt.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Add Actions in Python\nDESCRIPTION: Gets the current add actions from a Delta table, which show the active files in the table and their metadata such as path, size, and statistics.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.8.0\")\n>>> dt.get_add_actions(flatten=True).to_pandas()\n                                                        path  size_bytes   modification_time  data_change  num_records  null_count.value  min.value  max.value\n0  part-00000-c9b90f86-73e6-46c8-93ba-ff6bfaf892a...         440 2021-03-06 15:16:07         True            2                 0          0          2\n1  part-00000-04ec9591-0b73-459e-8d18-ba5711d6cbe...         440 2021-03-06 15:16:16         True            2                 0          2          4\n```\n\n----------------------------------------\n\nTITLE: Time Travel with Delta Tables in Python\nDESCRIPTION: Shows how to load specific versions of a Delta table and how to change versions after loading.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\", version=2)\n\n>>> dt.load_version(1)\n>>> dt.load_with_datetime(\"2021-11-04 00:05:23.283+00:00\")\n```\n\n----------------------------------------\n\nTITLE: Basic Structure of MERGE Command in Python\nDESCRIPTION: Demonstrates the basic structure of a Delta Lake MERGE command using the TableMerger API in delta-rs. It shows how to set up a merge operation with conditional updates.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n(\n    dt.merge(                                       # target data\n        source=source_data,                         # source data\n        predicate=\"target.x = source.x\",\n        source_alias=\"source\",\n        target_alias=\"target\")\n    .when_matched_update(                           # conditional statement\n        updates={\"x\": \"source.x\", \"y\":\"source.y\"})\n    .execute()\n)\n```\n\n----------------------------------------\n\nTITLE: Loading DeltaTable into Polars DataFrame\nDESCRIPTION: Python code snippet showing how to load a DeltaTable into a Polars DataFrame using pl.read_delta.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\nnew_df = pl.read_delta(\"delta_table_dir\")\n```\n\n----------------------------------------\n\nTITLE: Monitoring Delta Lake Merge Operation Metrics in Python\nDESCRIPTION: This snippet shows how to execute a merge operation and print out key performance metrics such as the number of files scanned, skipped, and the total execution time.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmetrics = dt.merge(...).execute()\n\nprint(f\"Files scanned: {metrics.get('num_target_files_scanned')}\")\nprint(f\"Files skipped: {metrics.get('num_target_files_skipped_during_scan')}\")\nprint(f\"Execution time: {metrics.get('execution_time_ms')} ms\")\n```\n\n----------------------------------------\n\nTITLE: Overwriting Delta Lake Table in Python\nDESCRIPTION: This snippet demonstrates how to overwrite an existing Delta table using Python. It uses the write_deltalake function with the 'overwrite' mode to replace the entire table content.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/appending-overwriting-delta-lake-table.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"num\": [11, 22], \"letter\": [\"aa\", \"bb\"]})\nwrite_deltalake(\"tmp/some-table\", df, mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Using DeltaTable in Python\nDESCRIPTION: Demonstrates how to create a DeltaTable object, check its version, and list files. This snippet shows basic usage of the deltalake library to interact with Delta Lake tables.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import DeltaTable\ndt = DeltaTable(\"../rust/tests/data/delta-0.2.0\")\ndt.version()\n3\ndt.files()\n['part-00000-cb6b150b-30b8-4662-ad28-ff32ddab96d2-c000.snappy.parquet',\n 'part-00000-7c2deba3-1994-4fb8-bc07-d46c948aa415-c000.snappy.parquet',\n 'part-00001-c373a5bd-85f0-4758-815e-7eb62007a15c-c000.snappy.parquet']\n```\n\n----------------------------------------\n\nTITLE: Time Travel Operations with Delta Tables\nDESCRIPTION: Examples of loading specific versions of Delta tables using version numbers and datetime strings.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/loading-table.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\", version=2)\n>>> dt.load_version(1)\n>>> dt.load_with_datetime(\"2021-11-04 00:05:23.283+00:00\")\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = open_table(\"./data/simple_table\").await?;\ntable.load_version(1).await?;\ntable.load_version(1).await?;\ntable.load_with_datetime(\"2021-11-04 00:05:23.283+00:00\".parse().unwrap()).await?;\n```\n\n----------------------------------------\n\nTITLE: Registering a Delta Lake Table with DataFusion\nDESCRIPTION: This code demonstrates how to register a Delta Lake table with DataFusion. It imports the necessary modules, creates a SessionContext, initializes a DeltaTable object, and registers it as a table provider.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-datafusion.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datafusion import SessionContext, col, functions as f\nfrom deltalake import DeltaTable\n\nctx = SessionContext()\ntable = DeltaTable(\"G1_1e9_1e2_0_0\")\nctx.register_table_provider(\"my_delta_table\", table)\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows with Rust using Delta Lake's Expression API\nDESCRIPTION: Demonstrates how to delete rows from a Delta table using Rust with the Datafusion Expression API. This example opens a table and deletes rows where the 'num' column value is greater than 2.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/deleting-rows-from-delta-lake-table.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"./data/simple_table\").await?;\nlet (table, delete_metrics) = DeltaOps(table)\n    .delete()\n    .with_predicate(col(\"num\").gt(lit(2)))\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Lake Table in Rust\nDESCRIPTION: Creates a Delta Lake table using Rust by defining a schema and writing data with RecordBatchWriter. Demonstrates lower-level operations including schema definition, record batch creation, and writing the data to storage.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/create-delta-lake-table.md#2025-04-16_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nlet delta_ops = DeltaOps::try_from_uri(\"tmp/some-table\").await?;\nlet mut table = delta_ops\n    .create()\n    .with_table_name(\"some-table\")\n    .with_save_mode(SaveMode::Overwrite)\n    .with_columns(\n        StructType::new(vec![\n            StructField::new(\n                \"num\".to_string(),\n                DataType::Primitive(PrimitiveType::Integer),\n                true,\n            ),\n            StructField::new(\n                \"letter\".to_string(),\n                DataType::Primitive(PrimitiveType::String),\n                true,\n            ),\n        ])\n        .fields()\n        .cloned(),\n    )\n    .await?;\n\nlet mut record_batch_writer =\n    deltalake::writer::RecordBatchWriter::for_table(&mut table)?;\nrecord_batch_writer\n    .write(\n        RecordBatch::try_new(\n            Arc::new(Schema::new(vec![\n                Field::new(\"num\", DataType::Int32, true),\n                Field::new(\"letter\", Utf8, true),\n            ])),\n            vec![\n                Arc::new(Int32Array::from(vec![1, 2, 3])),\n                Arc::new(StringArray::from(vec![\n                    \"a\", \"b\", \"c\",\n                ])),\n            ],\n        )?,\n    )\n    .await?;\nrecord_batch_writer.flush_and_commit(&mut table).await?;\n```\n\n----------------------------------------\n\nTITLE: Optimizing Merge Performance with Partition Predicates\nDESCRIPTION: Demonstrates how to optimize merge operations by using partition columns in predicates for better performance\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n(\n    dt.merge(\n        source=source_data,\n        predicate=\"s.id = t.id AND s.month_id = t.month_id AND t.month_id = 202501\",\n        source_alias=\"s\",\n        target_alias=\"t\")\n    .execute()\n)\n```\n\n----------------------------------------\n\nTITLE: Appending Data to a Delta Table in Python\nDESCRIPTION: Code that creates a new pandas DataFrame and appends it to an existing Delta table. This demonstrates how to add more data to a Delta table without overwriting existing content.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/architecture-of-delta-table.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"num\": [8, 9], \"letter\": [\"dd\", \"ee\"]})\nwrite_deltalake(\"tmp/some-table\", df, mode=\"append\")\n```\n\n----------------------------------------\n\nTITLE: Appending to Delta Lake Table in Python\nDESCRIPTION: This snippet demonstrates how to append two additional rows of data to an existing Delta table using Python. It uses the write_deltalake function with the 'append' mode.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/appending-overwriting-delta-lake-table.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import write_deltalake, DeltaTable\n\ndf = pd.DataFrame({\"num\": [8, 9], \"letter\": [\"dd\", \"ee\"]})\nwrite_deltalake(\"tmp/some-table\", df, mode=\"append\")\n```\n\n----------------------------------------\n\nTITLE: Running Optimize on Delta Table in Rust\nDESCRIPTION: Shows how to run the optimize command to compact small files in a Delta table using Rust.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"observation_data\").await?;\nlet (table, metrics) = DeltaOps(table).optimize().with_type(OptimizeType::Compact).await?;\nprintln!(\"{:?}\", metrics);\n```\n\n----------------------------------------\n\nTITLE: INSERT Operation in Python\nDESCRIPTION: Demonstrates how to perform an INSERT operation using when_not_matched_insert in delta-rs with Python. It shows creating target and source tables, and inserting new records without duplication.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n> target_data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n> write_deltalake(\"tmp_table\", target_data)\n> dt = DeltaTable(\"tmp_table\")\n\nx  y\n0  1  4\n1  2  5\n2  3  6\n\n> source_data = pa.table({\"x\": [2,3,7], \"y\": [4,5,8]})\n\nx  y\n0  2  5\n1  3  6\n2  7  8\n\n(\n    dt.merge(\n        source=source_data,\n        predicate=\"target.x = source.x\",\n        source_alias=\"source\",\n        target_alias=\"target\")\n    .when_not_matched_insert(\n        updates={\"x\": \"source.x\", \"y\":\"source.y\"})\n    .execute()\n)\n\n> # inspect result\n> print(dt.to_pandas().sort_values(\"x\", ignore_index=True))\n\nx  y\n0  1  4\n1  2  5\n2  3  6\n3  7  8\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Lake Table with Filtering\nDESCRIPTION: Example showing how to query a Delta Lake table with filtering to enable file skipping. This query runs in 8 seconds, faster than both CSV and Parquet approaches, demonstrating Delta Lake's out-of-the-box performance benefits.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n(\n    DeltaTable(f\"{Path.home()}/data/deltalake_baseline_G1_1e9_1e2_0_0\", version=0)\n    .to_pandas(filters=[(\"id1\", \"==\", \"id016\")], columns=[\"id1\", \"id2\", \"v1\"])\n    .query(\"id1 == 'id016'\")\n    .groupby(\"id2\")\n    .agg({\"v1\": \"sum\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Table Data into Pandas in Python\nDESCRIPTION: Shows how to load data from a Delta table into a Pandas DataFrame, with options for filtering partitions and selecting specific columns.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.8.0-partitioned\")\n>>> dt.schema().to_pyarrow()\nvalue: string\nyear: string\nmonth: string\nday: string\n>>> dt.to_pandas(partitions=[(\"year\", \"=\", \"2021\")], columns=[\"value\"])\n      value\n0     6\n1     7\n2     5\n3     4\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Lake Schema in Python\nDESCRIPTION: Gets the Delta Lake schema from a table using DeltaTable.schema() method, which returns schema information including field types and nullable properties.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.schema()\nSchema([Field(id, PrimitiveType(\"long\"), nullable=True)])\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table into Dask DataFrame in Python\nDESCRIPTION: Demonstrates how to read a Delta Lake table into a Dask DataFrame using dask-deltatable. It includes examples of reading the latest version, a specific version, and reading from a remote source.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dask.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask_deltatable as ddt\n\n# read delta table into Dask DataFrame\ndelta_path = \"path/to/data/people_countries_delta_dask\"\nddf = ddt.read_deltalake(delta_path)\n\n# with specific version\nddf = ddt.read_deltalake(delta_path, version=3)\n\n# with specific datetime\nddt.read_deltalake(delta_path, datetime=\"2018-12-19T16:39:57-08:00\")\n\n# reading from S3\nddt.read_deltalake(\"s3://bucket_name/delta_path\", version=3)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Delta Tables in Rust\nDESCRIPTION: Shows how to perform table optimization using compact operation in Rust with the DeltaOps interface.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/managing-tables.md#2025-04-16_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = open_table(\"./data/simple_table\").await?;\nlet (table, metrics) = DeltaOps(table).optimize().with_type(OptimizeType::Compact).await?;\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table with Polars\nDESCRIPTION: This snippet shows how to read a Delta table using Polars, including time travel to specific versions of the table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-polars.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(pl.read_delta(\"tmp/bear_delta_lake\"))\n\nprint(pl.read_delta(\"tmp/bear_delta_lake\", version=0))\n\nprint(pl.read_delta(\"tmp/bear_delta_lake\", version=1))\n\nprint(pl.read_delta(\"tmp/bear_delta_lake\"))\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Specific Partition in Delta Tables\nDESCRIPTION: Demonstrates how to overwrite a specific partition by using mode=\"overwrite\" with partition_filters. This removes all files within the matching partition and inserts new data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/index.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import write_deltalake\n>>> df = pd.DataFrame({'x': [1, 2, 3], 'y': ['a', 'a', 'b']})\n>>> write_deltalake('path/to/table', df, partition_by=['y'])\n\n>>> table = DeltaTable('path/to/table')\n>>> df2 = pd.DataFrame({'x': [100], 'y': ['b']})\n>>> write_deltalake(table, df2, partition_filters=[('y', '=', 'b')], mode=\"overwrite\")\n\n>>> table.to_pandas()\n     x  y\n0    1  a\n1    2  a\n2  100  b\n```\n\n----------------------------------------\n\nTITLE: Updating Delta Table Rows - Python\nDESCRIPTION: Examples of updating rows in a Delta table using DeltaTable.update method with and without predicates.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import write_deltalake, DeltaTable\n>>> df = pd.DataFrame({'x': [1, 2, 3], 'deleted': [False, False, False]})\n>>> write_deltalake('path/to/table', df)\n>>> dt = DeltaTable('path/to/table')\n>>> dt.update({\"processed\": \"True\"})\n>>> dt.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Table History in Python\nDESCRIPTION: Demonstrates how to create a DeltaTable object and retrieve its history, which includes details about past operations performed on the table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.history()\n[{'timestamp': 1587968626537, 'operation': 'DELETE', 'operationParameters': {'predicate': '[\"((`id` % CAST(2 AS BIGINT)) = CAST(0 AS BIGINT))\"]'}, 'readVersion': 3, 'isBlindAppend': False},\n {'timestamp': 1587968614187, 'operation': 'UPDATE', 'operationParameters': {'predicate': '((id#697L % cast(2 as bigint)) = cast(0 as bigint))'}, 'readVersion': 2, 'isBlindAppend': False},\n {'timestamp': 1587968604143, 'operation': 'WRITE', 'operationParameters': {'mode': 'Overwrite', 'partitionBy': '[]'}, 'readVersion': 1, 'isBlindAppend': False},\n {'timestamp': 1587968596254, 'operation': 'MERGE', 'operationParameters': {'predicate': '(oldData.`id` = newData.`id`)'}, 'readVersion': 0, 'isBlindAppend': False},\n {'timestamp': 1587968586154, 'operation': 'WRITE', 'operationParameters': {'mode': 'ErrorIfExists', 'partitionBy': '[]'}, 'isBlindAppend': True}]\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Table to PyArrow Table and Querying with DuckDB\nDESCRIPTION: This code shows how to convert a Delta table to a PyArrow table and query it using DuckDB. Unlike the dataset approach, this eagerly loads data into memory which impacts performance for large datasets.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-arrow.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquack = duckdb.arrow(table.to_pyarrow_table())\nquack.filter(\"id1 = 'id016' and v2 > 10\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Metadata in Python\nDESCRIPTION: Retrieves basic metadata from a Delta table including ID, name, description, partition columns, creation time, and configuration using the DeltaTable.metadata() method.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.metadata()\nMetadata(id: 5fba94ed-9794-4965-ba6e-6ee3c0d22af9, name: None, description: None, partitionColumns: [], created_time: 1587968585495, configuration={})\n```\n\n----------------------------------------\n\nTITLE: Loading DeltaTable with DataFusion\nDESCRIPTION: Python code snippet showing how to load and query a DeltaTable using DataFusion.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datafusion import SessionContext\n\nctx = SessionContext()\nctx.register_dataset(\"my_delta_table\", dt.to_pyarrow_dataset())\nctx.sql(\"select * from my_delta_table\")\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from Delta Table - Python\nDESCRIPTION: Example of removing rows from a Delta table using DeltaTable.delete method with predicates.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable, write_deltalake\n>>> df = pd.DataFrame({'a': [1, 2, 3], 'to_delete': [False, False, True]})\n>>> write_deltalake('path/to/table', df)\n\n>>> table = DeltaTable('path/to/table')\n>>> table.delete(predicate=\"to_delete = true\")\n\n>>> table.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Tables with DuckDB\nDESCRIPTION: Demonstrates how to use DuckDB to query data from a Delta table by passing a PyArrow dataset to the DuckDB engine.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/querying-delta-tables.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import duckdb\n>>> ex_data = duckdb.arrow(dataset)\n>>> ex_data.filter(\"year = 2021 and value > 4\").project(\"value\")\n---------------------\n-- Expression Tree --\n---------------------\nProjection [value]\n  Filter [year=2021 AND value>4]\n    arrow_scan(140409099470144, 4828104688, 1000000)\n\n---------------------\n-- Result Columns  --\n---------------------\n- value (VARCHAR)\n\n---------------------\n-- Result Preview  --\n---------------------\nvalue\nVARCHAR\n[ Rows: 3]\n6\n7\n5\n```\n\n----------------------------------------\n\nTITLE: Optimizing Delta Tables in Python\nDESCRIPTION: Demonstrates table optimization using compact operation in Python to address small file problems and improve query performance.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/managing-tables.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(...)\ndt.optimize.compact()\n```\n\n----------------------------------------\n\nTITLE: Opening Delta Table in Rust\nDESCRIPTION: Basic example of opening a Delta table using the Deltalake API and printing its files.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/core/README.md#2025-04-16_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"./tests/data/simple_table\").await.unwrap();\nprintln!(\"{}\", table.get_files());\n```\n\n----------------------------------------\n\nTITLE: Querying DeltaTable with DuckDB\nDESCRIPTION: Python code snippet demonstrating how to query a DeltaTable using DuckDB.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nduckdb.query(\"SELECT * FROM delta_scan('./delta_table_dir')\")\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Tables with Apache Datafusion (SQL Interface)\nDESCRIPTION: Demonstrates how to use Apache Datafusion's SQL interface to query Delta tables in Rust. This code opens a Delta table and runs SQL queries against it.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/querying-delta-tables.md#2025-04-16_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"../rust/tests/data/delta-0.8.0-partitioned\").await?;\nlet ctx = SessionContext::new();\nctx.register_table(\"simple_table\", Arc::new(table.clone()))?;\nlet df = ctx.sql(\"SELECT value FROM simple_table WHERE year = 2021\").await?;\ndf.show().await?;\n```\n\n----------------------------------------\n\nTITLE: Querying a Delta Lake Table with SQL in DataFusion\nDESCRIPTION: This code shows how to run a SQL query on a registered Delta Lake table. It demonstrates filtering and aggregation operations using SQL syntax.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-datafusion.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nctx.sql(\"select id1, sum(v1) as v1 from my_delta_table where id1='id096' group by id1\").show()\n```\n\n----------------------------------------\n\nTITLE: Creating and writing to a DeltaTable using Pandas\nDESCRIPTION: Python code snippet demonstrating how to create a Pandas DataFrame and write it to a DeltaTable using the deltalake library.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom deltalake import DeltaTable,write_deltalake\n\ndf = pd.DataFrame(\n    {\n        \"id\": [1, 2, 3],\n        \"name\": [\"Aadhya\", \"Bob\", \"Chen\"],\n    }\n)\n\n(\n    write_deltalake(\n        table_or_uri=\"delta_table_dir\",\n        data=df,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Column Pruning with Delta Lake and Dagster\nDESCRIPTION: Python code showing how to implement column pruning when loading Delta Lake tables in Dagster assets, improving computation efficiency by selecting specific columns.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nfrom dagster import AssetIn, asset\n\n@asset(\n       ins={\n           \"mammal_bool\": AssetIn(\n               key=\"clean_dataset\",\n               metadata={\"columns\": [\"is_mammal\", \"animals\"]},\n           )\n       }\n)\ndef mammal_data(mammal_bool: pa.Table) -> pa.Table:\n   mammals = mammal_bool[\"is_mammal\"].cast(\"bool\")\n   animals = mammal_bool[\"animals\"]\n   data = {\"mammal_bool\": mammals, \"animals\": animals}\n   return pa.Table.from_pydict(data)\n```\n\n----------------------------------------\n\nTITLE: Examining Delta Table Metadata in Python\nDESCRIPTION: Shows how to retrieve and inspect metadata from a Delta table, including table ID, name, description, and configuration.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.metadata()\nMetadata(id: 5fba94ed-9794-4965-ba6e-6ee3c0d22af9, name: None, description: None, partitionColumns: [], created_time: 1587968585495, configuration={})\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Tables with Apache Datafusion (DataFrame Interface)\nDESCRIPTION: Shows how to use Apache Datafusion's DataFrame interface to query Delta tables in Rust. This approach provides a programmatic alternative to SQL queries.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/querying-delta-tables.md#2025-04-16_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"../rust/tests/data/delta-0.8.0-partitioned\").await?;\nlet ctx = SessionContext::new();\nlet dataframe = ctx.read_table( Arc::new(table.clone()))?;\nlet df = dataframe.filter(col(\"year\").eq(lit(2021)))?.select(vec![col(\"value\")])?;\ndf.show().await?;\n```\n\n----------------------------------------\n\nTITLE: Running Optimize on Delta Table in Python\nDESCRIPTION: Demonstrates how to run the optimize command to compact small files in a Delta table using Python.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(\"observation_data\")\n\ndt.optimize.compact()\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Tables to GCS using Polars in Python\nDESCRIPTION: This snippet demonstrates how to create a sample dataframe using Polars and write it as a Delta table to Google Cloud Storage. It utilizes the native GCS support in delta-rs without requiring additional dependencies.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/gcs.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# create a toy dataframe\nimport polars as pl\ndf = pl.DataFrame({\"foo\": [1, 2, 3, 4, 5]})\n\n# define path\ntable_path = \"gs://bucket/delta-table\"\n\n# write Delta to GCS\ndf.write_delta(table_path)\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Table to PyArrow Dataset and Querying with DuckDB\nDESCRIPTION: This code demonstrates how to load a Delta table, convert it to a PyArrow dataset, and query it using DuckDB. The example uses predicate pushdown for efficient filtering of a large dataset.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-arrow.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nfrom deltalake import DeltaTable\n\ntable = DeltaTable(\"delta/G1_1e9_1e2_0_0\")\ndataset = table.to_pyarrow_dataset()\nquack = duckdb.arrow(dataset)\nquack.filter(\"id1 = 'id016' and v2 > 10\")\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Table to S3 with Polars\nDESCRIPTION: Example demonstrating how to write a Polars DataFrame to a Delta table in S3 using the storage_options parameter.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n   df.write_delta(\n       \"s3://bucket/delta_table\",\n       storage_options=storage_options,\n   )\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Schema to JSON in Python\nDESCRIPTION: Converts a Delta Lake schema to its JSON representation using the to_json() method, which is useful for schema inspection or serialization.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dt.schema().to_json()\n'{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}'\n```\n\n----------------------------------------\n\nTITLE: Vacuuming Delta Tables in Python\nDESCRIPTION: Demonstrates how to vacuum a Delta table to delete old files while preserving time travel capabilities within a retention window. Shows both dry-run and actual deletion modes.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/managing-tables.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.vacuum()\n['../rust/tests/data/simple_table/part-00006-46f2ff20-eb5d-4dda-8498-7bfb2940713b-c000.snappy.parquet',\n'../rust/tests/data/simple_table/part-00190-8ac0ae67-fb1d-461d-a3d3-8dc112766ff5-c000.snappy.parquet',\n'../rust/tests/data/simple_table/part-00164-bf40481c-4afd-4c02-befa-90f056c2d77a-c000.snappy.parquet',\n...]\n>>> dt.vacuum(dry_run=False) # Don't run this unless you are sure!\n```\n\n----------------------------------------\n\nTITLE: Using PyArrow Dataset with Delta Tables\nDESCRIPTION: Shows how to convert Delta tables to PyArrow datasets to enable advanced filtering and batch processing. This approach allows filtering on non-partition columns and streaming data in batches.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/querying-delta-tables.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.dataset as ds\n>>> dataset = dt.to_pyarrow_dataset()\n>>> condition = (ds.field(\"year\") == \"2021\") & (ds.field(\"value\") > \"4\")\n>>> dataset.to_table(filter=condition, columns=[\"value\"]).to_pandas()\n  value\n0     6\n1     7\n2     5\n>>> batch_iter = dataset.to_batches(filter=condition, columns=[\"value\"], batch_size=2)\n>>> for batch in batch_iter: print(batch.to_pandas())\n  value\n0     6\n1     7\n  value\n0     5\n```\n\n----------------------------------------\n\nTITLE: Z Ordering a Delta Table in Python\nDESCRIPTION: This snippet demonstrates how to perform Z Ordering on a Delta table using the 'country' column in Python. It uses the DeltaTable class to open the table and apply the Z Order optimization.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/delta-lake-z-order.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(\"tmp\")\ndt.optimize.z_order([\"country\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table from Pandas DataFrame in Python\nDESCRIPTION: Code that creates a pandas DataFrame with numeric and string columns and writes it to a new Delta table. This demonstrates the initial creation of a Delta table from an in-memory DataFrame.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/architecture-of-delta-table.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom deltalake import write_deltalake\n\ndf = pd.DataFrame({\"num\": [1, 2, 3], \"letter\": [\"a\", \"b\", \"c\"]})\nwrite_deltalake(\"tmp/some-table\", df)\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from a Delta Table in Python\nDESCRIPTION: Shows how to perform a delete operation on a Delta table. This example deletes all rows where the 'animal' column equals 'cat'.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/delta-lake-acid-transactions.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(\"tmp/my-delta-table\")\ndt.delete(\"animal = 'cat'\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Lake Table with Dagster Asset\nDESCRIPTION: Python code demonstrating how to create a Dagster asset that generates a PyArrow Table, which will be automatically stored as a Delta Lake table by the configured I/O Manager.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nfrom pyarrow import csv\n\nfrom dagster import asset\n\n@asset\ndef raw_dataset() -> pa.Table:\n   n_legs = pa.array([2, 4, None, 100])\n   animals = pa.array([\"Flamingo\", \"Horse\", \"Brittle stars\", \"Centipede\"])\n   data = {'n_legs': n_legs, 'animals': animals}\n\n   return pa.Table.from_pydict(data)\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Table with DataFusion via Arrow Dataset\nDESCRIPTION: This example demonstrates using DataFusion to query a Delta table by first converting it to a PyArrow dataset. It registers the dataset with a DataFusion SessionContext and executes an SQL query against it.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-arrow.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datafusion import SessionContext\n\nctx = SessionContext()\nctx.register_dataset(\"my_dataset\", table.to_pyarrow_dataset())\nctx.sql(\"select * from my_dataset where v2 > 5\")\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Schema to PyArrow Schema in Python\nDESCRIPTION: Converts a Delta Lake schema to a PyArrow schema using the to_pyarrow() method, which facilitates integration with Arrow-based libraries.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> dt.schema().to_pyarrow()\nid: int64\n```\n\n----------------------------------------\n\nTITLE: Examining Delta Table Schema in Python\nDESCRIPTION: Demonstrates how to retrieve and inspect the schema of a Delta table, both in Delta Lake format and as a PyArrow schema.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.schema()\nSchema([Field(id, PrimitiveType(\"long\"), nullable=True)])\n\n>>> dt.schema().json()\n'{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}'\n\n>>> dt.schema().to_pyarrow()\nid: int64\n```\n\n----------------------------------------\n\nTITLE: Overwriting Delta Lake Table in Rust\nDESCRIPTION: This snippet shows how to overwrite an existing Delta table using Rust. It uses DeltaOps with SaveMode::Overwrite to replace the entire table content.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/appending-overwriting-delta-lake-table.md#2025-04-16_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"tmp/some-table\").await?;\nDeltaOps(table).write(RecordBatch::try_new(\n    Arc::new(Schema::new(vec![\n        Field::new(\"num\", DataType::Int32, false),\n        Field::new(\"letter\", DataType::Utf8, false),\n    ])),\n    vec![\n        Arc::new(Int32Array::from(vec![1, 2, 3])),\n        Arc::new(StringArray::from(vec![\n            \"a\", \"b\", \"c\",\n        ])),\n    ])).with_save_mode(SaveMode::Overwrite).await?;\n```\n\n----------------------------------------\n\nTITLE: Restoring Delta Table Version - Python\nDESCRIPTION: Example of restoring a Delta table to a previous version using DeltaTable.restore method.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.restore(1)\n```\n\n----------------------------------------\n\nTITLE: Implementing UPSERT Operation\nDESCRIPTION: Shows how to perform an upsert operation by combining matched update and not matched insert conditions\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n(\n    dt.merge(\n        source=source_data,\n        predicate=\"target.x = source.x\",\n        source_alias=\"source\",\n        target_alias=\"target\")\n    .when_matched_update(\n        updates={\"x\": \"source.x\", \"y\":\"source.y\"})\n    .when_not_matched_insert(\n        updates={\"x\": \"source.x\", \"y\":\"source.y\"})\n    .execute()\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nDeltaOps(table)\n.merge(source_data, \"target.x = source.x\")\n.with_source_alias(\"source\")\n.with_target_alias(\"target\")\n.when_matched_update(\n    |update| update.update(\"x\", \"source.x\").update(\"y\", \"source.y\"),\n)?\n.when_not_matched_insert(\n    |insert| insert.set(\"x\", \"source.x\").set(\"y\", \"source.y\"),\n)?\n.await?;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Lake Schema in Rust\nDESCRIPTION: Gets the Delta Lake schema from a table in Rust using the get_schema() method on a DeltaTable instance.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"./data/simple_table\").await?;\nlet schema = table.get_schema()?;\nprintln!(\"schema: {:?}\", schema);\n```\n\n----------------------------------------\n\nTITLE: Verifying Delta Table Existence in Python\nDESCRIPTION: Demonstrates how to check if a Delta table exists at a given path using the is_deltatable() method.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>>from deltalake import DeltaTable\n>>>\n>>>table_path = \"<path/to/valid/table>\"\n>>>DeltaTable.is_deltatable(table_path)\n>>># True\n>>>\n>>>invalid_table_path = \"<path/to/nonexistent/table>\"\n>>>DeltaTable.is_deltatable(invalid_table_path)\n>>># False\n>>>\n>>>bucket_table_path = \"<path/to/valid/table/in/bucket>\"\n>>>storage_options = {\n>>>    \"AWS_ACCESS_KEY_ID\": \"THE_AWS_ACCESS_KEY_ID\",\n>>>    \"AWS_SECRET_ACCESS_KEY\": \"THE_AWS_SECRET_ACCESS_KEY\",\n>>>    ...\n>>>}\n>>>DeltaTable.is_deltatable(bucket_table_path)\n>>># True\n```\n\n----------------------------------------\n\nTITLE: Z Ordering a Delta Table in Rust\nDESCRIPTION: This snippet shows how to implement Z Ordering on a Delta table using the 'country' column in Rust. It opens the table, applies the Z Order optimization, and prints the optimization metrics.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/delta-lake-z-order.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"tmp\").await?;    \n\nlet (table, metrics) = DeltaOps(table)\n    .optimize()\n    .with_type(OptimizeType::ZOrder(vec![\"country\".to_string()]))\n    .await?;\nprintln!(\"{:?}\", metrics);\n```\n\n----------------------------------------\n\nTITLE: Vacuuming Delta Table in Python\nDESCRIPTION: Demonstrates how to vacuum a Delta table, which deletes files that have been marked for deletion. This operation can save storage space but may affect time travel capabilities.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.vacuum()\n['../rust/tests/data/simple_table/part-00006-46f2ff20-eb5d-4dda-8498-7bfb2940713b-c000.snappy.parquet',\n '../rust/tests/data/simple_table/part-00190-8ac0ae67-fb1d-461d-a3d3-8dc112766ff5-c000.snappy.parquet',\n '../rust/tests/data/simple_table/part-00164-bf40481c-4afd-4c02-befa-90f056c2d77a-c000.snappy.parquet',\n ...]\n>>> dt.vacuum(dry_run=False) # Don't run this unless you are sure!\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Lake for Python using pip\nDESCRIPTION: This command installs the deltalake package using pip, the Python package installer.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/installation.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deltalake\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Tables from Unity Catalog\nDESCRIPTION: Example of loading a Delta Table from Databricks or Open Source Unity Catalog using authentication tokens.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/loading-table.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom deltalake import DeltaTable\n\n# Set your Unity Catalog workspace URL in the\n# DATABRICKS_WORKSPACE_URL environment variable.\nos.environ[\"DATABRICKS_WORKSPACE_URL\"] = \"https://adb-1234567890.XX.azuredatabricks.net\"\n\n# Set your Unity Catalog access token in the\n# DATABRICKS_ACCESS_TOKEN environment variable.\nos.environ[\"DATABRICKS_ACCESS_TOKEN\"] = \"<unity-catalog-access-token-here>\"\n\n# Your Unity Catalog name here\ncatalog_name = \"unity\"\n\n# Your UC database name here\ndb_name = \"my_database\"\n\n# Your table name in the UC database here\ntable_name = \"my_table\"\n\n# Full UC URL in required format\nuc_full_url = f\"{catalog_name}.{db_name}.{table_name}\"\n\n# This should load the valid delta table at specified location,\n# and you can start using it.\ndt = DeltaTable(uc_full_url)\ndt.is_deltatable(dt.table_uri)\n# True\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage Options for Delta Table Loading\nDESCRIPTION: Example showing how to initialize a DeltaTable with AWS storage options for authentication.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/loading-table.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> storage_options = {\"AWS_ACCESS_KEY_ID\": \"THE_AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\":\"THE_AWS_SECRET_ACCESS_KEY\"}\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.2.0\", storage_options=storage_options)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet storage_options = HashMap::from_iter(vec![\n    (\"AWS_ACCESS_KEY_ID\".to_string(), \"THE_AWS_ACCESS_KEY_ID\".to_string()),\n    (\"AWS_SECRET_ACCESS_KEY\".to_string(), \"THE_AWS_SECRET_ACCESS_KEY\".to_string()),\n]);\nlet table = open_table_with_storage_options(\"../rust/tests/data/delta-0.2.0\", storage_options).await?;\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Dask Operations on Delta Lake Data in Python\nDESCRIPTION: Shows how to perform basic Dask operations on the DataFrame created from a Delta Lake, including viewing the first few rows, checking the number of partitions, and grouping data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dask.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# View first few rows\nddf.head(n=3)\n\n# Check number of partitions\nddf.npartitions\n\n# Inspect a single partition\nddf.get_partition(n=1).compute()\n\n# Group by country and count\nddf.groupby(['country']).count().compute()\n```\n\n----------------------------------------\n\nTITLE: Working with Partitioned Assets in Delta Lake and Dagster\nDESCRIPTION: Python code demonstrating how to work with partitioned Delta Lake tables in Dagster, using static partitions based on a specific column.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\nfrom dagster import StaticPartitionsDefinition, asset\n\n@asset(\n  partitions_def=StaticPartitionsDefinition(\n      [\"Human\", \"Horse\",]\n  ),\n  metadata={\"partition_expr\": \"n_legs\"},\n)\ndef dataset_partitioned(\n   context,\n   clean_dataset: pa.Table,\n   ) -> pa.Table:\n   animals = context.asset_partition_key_for_output()\n   table = clean_dataset\n\n   return table.filter(pc.field(\"animals\") == animals)\n```\n\n----------------------------------------\n\nTITLE: Querying Large Dataset with Polars and Different File Formats\nDESCRIPTION: This snippet compares query performance on a large dataset (1 billion rows) using different file formats: CSV, Parquet, and Delta Lake. It demonstrates the speed advantages of Delta Lake.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-polars.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npl.scan_csv(\"~/data/G1_1e9_1e2_0_0.csv\").filter(pl.col(\"id1\") < \"id016\").group_by(\n    [\"id1\", \"id2\"]\n).agg(pl.sum(\"v1\").alias(\"v1_sum\")).collect()\n\npl.scan_parquet(\"~/data/G1_1e9_1e2_0_0.parquet\").filter(\n    pl.col(\"id1\") < \"id016\"\n).group_by([\"id1\", \"id2\"]).agg(pl.sum(\"v1\").alias(\"v1_sum\")).collect()\n\npl.scan_delta(\"~/data/deltalake/G1_1e9_1e2_0_0\", version=1).filter(\n    pl.col(\"id1\") < \"id016\"\n).group_by([\"id1\", \"id2\"]).agg(pl.sum(\"v1\").alias(\"v1_sum\")).collect()\n\npl.scan_delta(\"~/data/deltalake/G1_1e9_1e2_0_0\", version=2).filter(\n    pl.col(\"id1\") < \"id016\"\n).group_by([\"id1\", \"id2\"]).agg(pl.sum(\"v1\").alias(\"v1_sum\")).collect()\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Table to ADLS with Polars\nDESCRIPTION: Example showing how to write a Delta table to Azure Data Lake Storage using Polars with explicit credentials. The code creates a DataFrame and writes it to ADLS using the write_delta method with storage options.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/adls.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\ndf = pl.DataFrame({\"foo\": [1, 2, 3, 4, 5]})\n\n# define container name\ncontainer = <container_name>\n\n# define credentials\nstorage_options = {\n    \"ACCOUNT_NAME\": <account_name>,\n    \"ACCESS_KEY\": <access_key>,\n}\n\n# write Delta to ADLS\ndf_pl.write_delta(\n    f\"abfs://{container}/delta_table\",\n    storage_options = storage_options\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Storage Backends with PyArrow\nDESCRIPTION: Examples of using custom storage backends with PyArrow FileSystem integration for Delta tables.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/loading-table.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs as fs\nfrom deltalake import DeltaTable\n\npath = \"<path/to/table>\"\nfilesystem = fs.SubTreeFileSystem(path, fs.LocalFileSystem())\n\ndt = DeltaTable(path)\nds = dt.to_pyarrow_dataset(filesystem=filesystem)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs as fs\nfrom deltalake import DeltaTable\n\ntable_uri = \"s3://<bucket>/<path>\"\nraw_fs, normalized_path = fs.FileSystem.from_uri(table_uri)\nfilesystem = fs.SubTreeFileSystem(normalized_path, raw_fs)\n\ndt = DeltaTable(table_uri)\nds = dt.to_pyarrow_dataset(filesystem=filesystem)\n```\n\n----------------------------------------\n\nTITLE: Mapping Custom Functions over Dask DataFrame Partitions in Python\nDESCRIPTION: Demonstrates how to define a custom Python function and map it over all partitions of a Dask DataFrame using map_partitions. The example replaces missing continent values.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dask.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define custom python function\ndef replace_proper(partition, na_string):\n    if [partition.country == \"Argentina\"]:\n        partition.loc[partition.country==\"Argentina\"] = partition.loc[partition.country==\"Argentina\"].replace(na_string, \"South America\")\n    if [partition.country == \"Germany\"]:\n        partition.loc[partition.country==\"Germany\"] = partition.loc[partition.country==\"Germany\"].replace(na_string, \"Europe\")\n    else:\n        pass\n    return partition\n\n# Map function over partitions\nmeta = dict(ddf.dtypes)\nddf3 = ddf.map_partitions(replace_proper, na_string, meta=meta)\nddf3.compute()\n```\n\n----------------------------------------\n\nTITLE: Time Travel to Previous Version in Rust\nDESCRIPTION: This snippet shows how to perform time travel to access a previous version of a Delta table in Rust. It uses the load_version method to retrieve a specific version of the table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/appending-overwriting-delta-lake-table.md#2025-04-16_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = open_table(\"tmp/some-table\").await?;\ntable.load_version(1).await?;\n```\n\n----------------------------------------\n\nTITLE: UPDATE Operation in Python\nDESCRIPTION: Illustrates how to perform an UPDATE operation using when_matched_update in delta-rs. It includes setting up target and source tables, and defining merge logic to update specific columns.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import DeltaTable, write_deltalake\nimport pyarrow as pa\n\n# define target table\n> target_data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n> write_deltalake(\"tmp_table\", target_data)\n> dt = DeltaTable(\"tmp_table\")\n> dt.to_pandas().sort_values(\"x\", ignore_index=True)\n\nx  y\n0  1  4\n1  2  5\n2  3  6\n\n# define source table\n> source_data = pa.table({\"x\": [2, 3], \"y\": [5,8]})\n> source_data\n\nx  y\n0  2  5\n1  3  8\n\n# define merge logic\n> (\n>     dt.merge(\n>         source=source_data,\n>         predicate=\"target.x = source.x\",\n>         source_alias=\"source\",\n>         target_alias=\"target\")\n>     .when_matched_update(\n>         updates={\"x\": \"source.x\", \"y\":\"source.y\"})\n>     .execute()\n> )\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Table from Unity Catalog in Python\nDESCRIPTION: Demonstrates how to load a Delta table from Databricks or Open Source Unity Catalog using a uc:// URL format.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> from deltalake import DeltaTable\n>>>\n>>> # Set your Unity Catalog workspace URL in the\n>>> # DATABRICKS_WORKSPACE_URL environment variable.\n>>> os.environ[\"DATABRICKS_WORKSPACE_URL\"] = \"https://adb-1234567890.XX.azuredatabricks.net\"\n>>>\n>>> # Set your Unity Catalog access token in the\n>>> # DATABRICKS_ACCESS_TOKEN environment variable.\n>>> os.environ[\"DATABRICKS_ACCESS_TOKEN\"] = \"<unity-catalog-access-token-here>\"\n>>>\n>>> # Your Unity Catalog name here\n>>> catalog_name = \"unity\"\n>>>\n>>> # Your UC database name here\n>>> db_name = \"my_database\"\n>>>\n>>> # Your table name in the UC database here\n>>> table_name = \"my_table\"\n>>>\n>>> # Full UC URL in required format\n>>> uc_full_url = f\"{catalog_name}.{db_name}.{table_name}\"\n>>>\n>>> # This should load the valid delta table at specified location,\n>>> # and you can start using it.\n>>> dt = DeltaTable(uc_full_url)\n>>> dt.is_deltatable(dt.table_uri)\n>>> # True\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying a Delta Table in Python\nDESCRIPTION: Demonstrates how to load a Delta table using the DeltaTable class, and query basic information like version and files.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.2.0\")\n>>> dt.version()\n3\n>>> dt.files()\n['part-00000-cb6b150b-30b8-4662-ad28-ff32ddab96d2-c000.snappy.parquet',\n 'part-00000-7c2deba3-1994-4fb8-bc07-d46c948aa415-c000.snappy.parquet',\n 'part-00001-c373a5bd-85f0-4758-815e-7eb62007a15c-c000.snappy.parquet']\n```\n\n----------------------------------------\n\nTITLE: Filtering a Delta Lake Table Using Partition Columns\nDESCRIPTION: Demonstrates how to filter a Delta Lake table using a partition column (country) for efficient partition pruning, which skips irrelevant partitions.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Filter on partition columns will result in efficient partition pruning; non-matching partitions will be skipped.\n> df.where(df[\"country\"] == \"Germany\").show()\n\n|    | first_name   | last_name   | country   |   continent |\n|---:|:-------------|:------------|:----------|------------:|\n|  0 | Wolfgang     | Manche      | Germany   |         nan |\n|  1 | Soraya       | Jala        | Germany   |         nan |\n```\n\n----------------------------------------\n\nTITLE: Writing a Polars DataFrame to a Delta table in LakeFS\nDESCRIPTION: This code demonstrates how to write a Polars DataFrame to a Delta table in LakeFS using the write_delta method. It specifies the LakeFS path and uses the previously defined storage_options for authentication.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/lakefs.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.write_delta(\n       \"lakefs://bucket/branch/table\",\n       storage_options=storage_options,\n   )\n```\n\n----------------------------------------\n\nTITLE: Using DataFusion's DataFrame API with Delta Lake\nDESCRIPTION: This code demonstrates how to use DataFusion's DataFrame API instead of SQL to query a Delta Lake table. It shows filtering and aggregation operations using the DataFrame syntax.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-datafusion.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nctx.table(\"my_delta_table\").filter(col(\"id1\") == \"id096\").aggregate(\n    col(\"id1\"), f.sum(col(\"v1\")).alias(\"v1\")\n).show()\n```\n\n----------------------------------------\n\nTITLE: Overwriting Delta Lake Table Schema\nDESCRIPTION: Example showing how to overwrite both data and schema of a Delta Lake table using schema_mode=\"overwrite\". This allows changing the table structure completely when required.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwrite_deltalake(\"tmp/some-table\", df, mode=\"overwrite\", schema_mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Delta Table Add Actions\nDESCRIPTION: Shows how to inspect table state and partition distribution using get_add_actions(). Helps identify partitions ready for optimization.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndt.get_add_actions(flatten=True).to_pandas()[\n    \"partition.date\"\n].value_counts().sort_index()\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"observation_data\").await?;\nlet batch = table.snapshot()?.add_actions_table(true)?;\nlet ctx = SessionContext::new();\nctx.register_batch(\"observations\", batch.clone())?;\nlet df = ctx.sql(\"\nSELECT \\\"partition.date\\\", \n        COUNT(*) \nFROM observations \nGROUP BY \\\"partition.date\\\" \nORDER BY \\\"partition.date\\\"\").await?;\ndf.show().await?\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Delta Lake Table\nDESCRIPTION: Example showing how to completely overwrite an existing Delta Lake table with new data. This creates version 2 of the Delta table by replacing all previous content with two new rows.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"num\": [11, 22], \"letter\": [\"aa\", \"bb\"]})\nwrite_deltalake(\"tmp/some-table\", df, mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Delta Lake Query Results with Python\nDESCRIPTION: Adds a WHERE clause to the query to filter results based on a specified condition. The condition should be a valid SQL expression.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/query.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfilter(condition: str) -> QueryBuilder\n```\n\n----------------------------------------\n\nTITLE: Optimizing Delta Table with File Compaction in Python\nDESCRIPTION: Shows how to optimize a Delta table by compacting small files into larger ones, which can improve read performance.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/simple_table\")\n>>> dt.optimize.compact()\n{'numFilesAdded': 1, 'numFilesRemoved': 5,\n 'filesAdded': {'min': 555, 'max': 555, 'avg': 555.0, 'totalFiles': 1, 'totalSize': 555},\n 'filesRemoved': {'min': 262, 'max': 429, 'avg': 362.2, 'totalFiles': 5, 'totalSize': 1811},\n 'partitionsOptimized': 1, 'numBatches': 1, 'totalConsideredFiles': 5,\n 'totalFilesSkipped': 0, 'preserveInsertionOrder': True}\n```\n\n----------------------------------------\n\nTITLE: Customizing Delta Table Writer Properties in Python\nDESCRIPTION: Shows how to customize the Rust Parquet writer using WriterProperties, BloomFilterProperties, and ColumnProperties classes for fine-grained control over how data is written.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/index.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import BloomFilterProperties, ColumnProperties, WriterProperties, write_deltalake\nimport pyarrow as pa\n\nwp = WriterProperties(\n        statistics_truncate_length=200,\n        default_column_properties=ColumnProperties(\n            bloom_filter_properties=BloomFilterProperties(True, 0.2, 30)\n        ),\n        column_properties={\n            \"value_non_bloom\": ColumnProperties(bloom_filter_properties=None),\n        },\n    )\n\ntable_path = \"/tmp/my_table\"\n\ndata = pa.table(\n        {\n            \"id\": pa.array([\"1\", \"1\"], pa.string()),\n            \"value\": pa.array([11, 12], pa.int64()),\n            \"value_non_bloom\": pa.array([11, 12], pa.int64()),\n        }\n    )\n\nwrite_deltalake(table_path, data, writer_properties=wp)\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Lake with Custom DynamoDB Settings in Python\nDESCRIPTION: This Python code example shows how to configure Delta Lake with custom DynamoDB settings, allowing the use of different credentials or endpoints for S3 and DynamoDB. It demonstrates overriding default configurations for scenarios like using S3-compatible storage on non-AWS platforms.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/writing-to-s3-with-locking-provider.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import write_deltalake\ndf = pd.DataFrame({'x': [1, 2, 3]})\nstorage_options = {\n    \"endpoint_url\": \"https://<your-s3-compatible-storage>\",\n    \"REGION\": \"<s3-region>\",\n    \"AWS_ACCESS_KEY_ID\": \"<s3-access-key-id>\",\n    \"AWS_SECRET_ACCESS_KEY\": \"<s3-secret-access-key>\",\n    # override dynamodb config\n    \"AWS_S3_LOCKING_PROVIDER\": \"dynamodb\",\n    \"AWS_ENDPOINT_URL_DYNAMODB\": \"https://dynamodb.<dynamodb-region>.amazonaws.com\",\n    \"AWS_REGION_DYNAMODB\": \"<dynamodb-region>\",\n    \"AWS_ACCESS_KEY_ID_DYNAMODB\": \"<dynamodb-access-key-id>\",\n    \"AWS_SECRET_ACCESS_KEY_DYNAMODB\": \"<dynamodb-secret-access-key>\",\n}\nwrite_deltalake(\n    's3a://path/to/table',\n    df,\n    storage_options=storage_options\n)\n```\n\n----------------------------------------\n\nTITLE: Vacuuming Delta Table\nDESCRIPTION: Shows how to physically remove deleted files from storage using the vacuum command with custom retention period settings.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndt.vacuum(retention_hours=0, enforce_retention_duration=False, dry_run=False)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"observation_data\").await?;\nlet (table, metrics) = DeltaOps(table)\n    .vacuum()\n    .with_retention_period(chrono::Duration::days(0))\n    .with_enforce_retention_duration(false)\n    .with_dry_run(false)\n    .await?;\nprintln!(\"{:?}\", metrics);\n```\n\n----------------------------------------\n\nTITLE: Performing DELETE Operation with Merge\nDESCRIPTION: Demonstrates how to delete matching rows based on a predicate condition using merge operation\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n(\n    dt.merge(\n        source=source_data,\n        predicate=\"target.x = source.x\",\n        source_alias=\"source\",\n        target_alias=\"target\")\n    .when_matched_delete(\n        predicate=\"source.deleted = true\")\n    .execute()\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nDeltaOps(table)\n.merge(source_data, \"target.x = source.x\")\n.with_source_alias(\"source\")\n.with_target_alias(\"target\")\n.when_matched_delete(\n    |delete| delete.predicate(\"source.deleted = true\")\n)?.await?;\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Data to S3 with pandas\nDESCRIPTION: Example showing how to write a Parquet file to S3 using pandas with boto3 credentials from local AWS config files.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    import pandas as pd\n    df = pd.DataFrame({'x': [1, 2, 3]})\n    df.to_parquet(\"s3://avriiil/parquet-test-pandas\")\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Storage Options for Delta Lake\nDESCRIPTION: Configuration of AWS credentials and DynamoDB locking provider settings for delta-rs operations.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\n    \"AWS_REGION\":<region_name>,\n    'AWS_ACCESS_KEY_ID': <key_id>,\n    'AWS_SECRET_ACCESS_KEY': <access_key>,\n    'AWS_S3_LOCKING_PROVIDER': 'dynamodb',\n    'DELTA_DYNAMO_TABLE_NAME': 'delta_log',\n}\n```\n\n----------------------------------------\n\nTITLE: Writing Dask DataFrame to Delta Lake in Python\nDESCRIPTION: Shows how to write a Dask DataFrame back to Delta Lake format using the to_deltalake function from dask-deltatable.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dask.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nddt.to_deltalake(\"tmp/test_write\", ddf)\n```\n\n----------------------------------------\n\nTITLE: Creating DynamoDB Table for Delta Lake Locking using AWS CLI\nDESCRIPTION: This shell command uses the AWS CLI to create a DynamoDB table suitable for use as a locking mechanism in Delta Lake. It sets up the table with the required attribute definitions and key schema for delta-rs to use.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/writing-to-s3-with-locking-provider.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\naws dynamodb create-table \\\n--table-name delta_log \\\n--attribute-definitions AttributeName=tablePath,AttributeType=S AttributeName=fileName,AttributeType=S \\\n--key-schema AttributeName=tablePath,KeyType=HASH AttributeName=fileName,KeyType=RANGE \\\n--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n```\n\n----------------------------------------\n\nTITLE: Overwriting Delta Table Partition - Python\nDESCRIPTION: Example of overwriting a specific partition in a Delta table using partition filters.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import write_deltalake\n>>> df = pd.DataFrame({'x': [1, 2, 3], 'y': ['a', 'a', 'b']})\n>>> write_deltalake('path/to/table', df, partition_by=['y'])\n\n>>> table = DeltaTable('path/to/table')\n>>> df2 = pd.DataFrame({'x': [100], 'y': ['b']})\n>>> write_deltalake(table, df2, partition_filters=[('y', '=', 'b')], mode=\"overwrite\")\n\n>>> table.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Reading a Delta Lake Table into a Daft DataFrame from Local Storage\nDESCRIPTION: Demonstrates how to read a Delta Lake table from a local file path into a Daft DataFrame using the read_delta_lake method.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\n# read delta table into Daft DataFrame\ndf = daft.read_delta_lake(\"path/to/delta_table\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing Single Partition in Delta Table\nDESCRIPTION: Demonstrates how to optimize a specific partition using partition filters to compact small files into larger ones.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndt.optimize.compact(partition_filters=[(\"date\", \"=\", \"2021-01-05\")])\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"observation_data\").await?;\nlet (table, metrics) = DeltaOps(table)\n    .optimize()\n    .with_type(OptimizeType::Compact)\n    .with_filters(&vec![(\"date\", \"=\", \"2021-01-05\").try_into()?])\n    .await?;\nprintln!(\"{:?}\", metrics);\n```\n\n----------------------------------------\n\nTITLE: Reading a Delta Lake Table from S3 Storage\nDESCRIPTION: Shows how to read a Delta Lake table from a remote S3 location into a Daft DataFrame. The example uses a commented-out reference to a public dataset.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# table_uri = (\n#     \"s3://daft-public-datasets/red-pajamas/\"\n#     \"stackexchange-sample-north-germanic-deltalake\"\n# )\n# df = daft.read_delta_lake(table_uri)\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Python\nDESCRIPTION: Demonstrates how to create a simple Delta table using pandas and deltalake library. The code creates a DataFrame and writes it to a Delta table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/delta-lake-acid-transactions.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom deltalake import write_deltalake, DeltaTable\n\ndf = pd.DataFrame({\"num\": [1, 2, 3], \"animal\": [\"cat\", \"dog\", \"snake\"]})\nwrite_deltalake(\"tmp/my-delta-table\", df)\n```\n\n----------------------------------------\n\nTITLE: Accessing ADLS with Azure CLI Authentication\nDESCRIPTION: Example showing how to read and write Delta tables to Azure Data Lake Storage using Azure CLI authentication. This approach uses existing Azure CLI credentials to access ADLS resources.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/adls.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import write_deltalake\n\n# build path\nstorage_account_name = \"<STORAGE_ACCOUNT_NAME>\"\ncontainer_name = \"<CONTAINER_NAME>\"\ntable_name = \"<TABLE_NAME>\" \nabfs_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{table_name}/\"\n\n# define credentials\nstorage_options = {\n    \"azure_tenant_id\": \"<TENANT_ID>\",\n    \"azure_use_azure_cli\": \"true\",\n}\n\ndt = DeltaTable(abfs_path,storage_options=storage_options)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Add Actions in Rust\nDESCRIPTION: Gets the current add actions from a Delta table in Rust, which show the active files in the table and their metadata.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"./data/simple_table\").await?;\nlet actions = table.snapshot()?.add_actions_table(true)?;\nprintln!(\"{}\", pretty_format_batches(&vec![actions])?);\n```\n\n----------------------------------------\n\nTITLE: Using DuckDB with Delta Table in Python\nDESCRIPTION: Demonstrates how to use DuckDB to query a Delta table that has been converted to a PyArrow dataset.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> import duckdb\n>>> ex_data = duckdb.arrow(dataset)\n>>> ex_data.filter(\"year = 2021 and value > 4\").project(\"value\")\n---------------------\n-- Expression Tree --\n---------------------\nProjection [value]\n  Filter [year=2021 AND value>4]\n    arrow_scan(140409099470144, 4828104688, 1000000)\n\n---------------------\n-- Result Columns  --\n---------------------\n- value (VARCHAR)\n\n---------------------\n-- Result Preview  --\n---------------------\nvalue\nVARCHAR\n[ Rows: 3]\n6\n7\n5\n```\n\n----------------------------------------\n\nTITLE: Loading a Delta Table with Storage Options in Python\nDESCRIPTION: Shows how to load a Delta table with custom storage options, such as AWS credentials for S3 access.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> storage_options = {\"AWS_ACCESS_KEY_ID\": \"THE_AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\":\"THE_AWS_SECRET_ACCESS_KEY\"}\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.2.0\", storage_options=storage_options)\n```\n\n----------------------------------------\n\nTITLE: Overwrite and Append Operations with Delta Tables\nDESCRIPTION: Shows how to perform overwrite and append operations on Delta tables using the mode parameter, which mirrors the behavior of Spark's DataFrameWriter.saveAsTable method.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/index.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> write_deltalake('path/to/table', df, mode='overwrite')\n>>> write_deltalake('path/to/table', df, mode='append')\n```\n\n----------------------------------------\n\nTITLE: UPDATE Operation in Rust\nDESCRIPTION: Shows how to perform an UPDATE operation using when_matched_update in delta-rs with Rust. It demonstrates creating a target table, preparing source data, and executing a merge operation.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n// define target table\nlet delta_ops = DeltaOps::try_from_uri(\"tmp/some-table\").await?;\nlet mut table = delta_ops\n    .create()\n    .with_table_name(\"some-table\")\n    .with_save_mode(SaveMode::Overwrite)\n    .with_columns(\n        StructType::new(vec![\n            StructField::new(\n                \"x\".to_string(),\n                DataType::Primitive(PrimitiveType::Integer),\n                true,\n            ),\n            StructField::new(\n                \"y\".to_string(),\n                DataType::Primitive(PrimitiveType::Integer),\n                true,\n            ),\n        ])\n        .fields()\n        .cloned(),\n    )\n    .await?;\n\nlet schema = Arc::new(Schema::new(vec![\n    Field::new(\"x\", arrow::datatypes::DataType::Int32, true),\n    Field::new(\"y\", arrow::datatypes::DataType::Int32, true),\n]));\nlet mut record_batch_writer = deltalake::writer::RecordBatchWriter::for_table(&mut table)?;\nrecord_batch_writer\n    .write(RecordBatch::try_new(\n        schema.clone(),\n        vec![\n            Arc::new(Int32Array::from(vec![1, 2, 3])),\n            Arc::new(Int32Array::from(vec![4, 5, 6])),\n        ],\n    )?)\n    .await?;\n\nrecord_batch_writer.flush_and_commit(&mut table).await?;\n\nlet ctx = SessionContext::new();\nlet source_data = ctx.read_batch(RecordBatch::try_new(\n    schema,\n    vec![\n        Arc::new(Int32Array::from(vec![2, 3])),\n        Arc::new(Int32Array::from(vec![5, 6])),\n    ],\n)?)?;\n\nDeltaOps(table)\n    .merge(source_data, \"target.x = source.x\")\n    .with_source_alias(\"source\")\n    .with_target_alias(\"target\")\n    .when_matched_update(|update| \n        update\n        .update(\"x\", \"source.x\")\n        .update(\"y\", \"source.y\"))?\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Configuring DeltaLakeIOManager in Dagster\nDESCRIPTION: Python code to configure the DeltaLakeIOManager in a Dagster project's __init__.py file. It sets up the root URI, storage options, and schema for Delta Lake tables.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n   assets=all_assets,\n   resources={\n        \"io_manager\": DeltaLakePyarrowIOManager(\n            root_uri=\"path/to/deltalake\",\n            storage_options=LocalConfig(),\n            schema=\"dagster_deltalake\",\n        ),\n   },\n)\n```\n\n----------------------------------------\n\nTITLE: Appending to Delta Lake Table in Rust\nDESCRIPTION: This snippet shows how to append two additional rows of data to an existing Delta table using Rust. It uses DeltaOps with SaveMode::Append to perform the operation.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/appending-overwriting-delta-lake-table.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet table = open_table(\"tmp/some-table\").await?;\nDeltaOps(table).write(RecordBatch::try_new(\n    Arc::new(Schema::new(vec![\n        Field::new(\"num\", DataType::Int32, false),\n        Field::new(\"letter\", DataType::Utf8, false),\n    ])),\n    vec![\n        Arc::new(Int32Array::from(vec![8, 9])),\n        Arc::new(StringArray::from(vec![\n            \"dd\", \"ee\"\n        ])),\n    ])).with_save_mode(SaveMode::Append).await?;\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Delta Table in Python\nDESCRIPTION: Code that creates a new pandas DataFrame and overwrites an existing Delta table with the new data. This demonstrates how to replace all data in a Delta table with new content.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/architecture-of-delta-table.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"num\": [11, 22], \"letter\": [\"aa\", \"bb\"]})\nwrite_deltalake(\"tmp/some-table\", df, mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Writing Initial Data to Delta Table in Python\nDESCRIPTION: This snippet demonstrates how to create a Delta table and write initial data using Polars DataFrame.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/delta-lake-file-skipping.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\nfrom deltalake import DeltaTable\n\ndf = pl.DataFrame({\"name\": [\"alice\", \"cat\", \"joy\"], \"age\": [12, 35, 46]})\ndf.write_delta(\"tmp/a_table\")\n```\n\n----------------------------------------\n\nTITLE: INSERT Operation in Rust\nDESCRIPTION: Shows how to perform an INSERT operation using when_not_matched_insert in delta-rs with Rust. It demonstrates creating a target table, preparing source data, and executing a merge operation to insert new records.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet delta_ops = DeltaOps::try_from_uri(\"./data/simple_table\").await?;\nlet mut table = delta_ops\n    .create()\n    .with_table_name(\"some-table\")\n    .with_save_mode(SaveMode::Overwrite)\n    .with_columns(\n        StructType::new(vec![\n            StructField::new(\n                \"x\".to_string(),\n                DataType::Primitive(PrimitiveType::Integer),\n                true,\n            ),\n            StructField::new(\n                \"y\".to_string(),\n                DataType::Primitive(PrimitiveType::Integer),\n                true,\n            ),\n        ])\n        .fields()\n        .cloned(),\n    )\n    .await?;\n\nlet schema = Arc::new(Schema::new(vec![\n    Field::new(\"x\", arrow::datatypes::DataType::Int32, true),\n    Field::new(\"y\", arrow::datatypes::DataType::Int32, true),\n]));\nlet mut record_batch_writer = deltalake::writer::RecordBatchWriter::for_table(&mut table)?;\nrecord_batch_writer\n    .write(RecordBatch::try_new(\n        schema.clone(),\n        vec![\n            Arc::new(Int32Array::from(vec![1, 2, 3])),\n            Arc::new(Int32Array::from(vec![4, 5, 6])),\n        ],\n    )?)\n    .await?;\n\nrecord_batch_writer.flush_and_commit(&mut table).await?;\n\nlet ctx = SessionContext::new();\nlet source_data = ctx.read_batch(RecordBatch::try_new(\n    schema,\n    vec![\n        Arc::new(Int32Array::from(vec![2, 3])),\n        Arc::new(Int32Array::from(vec![5, 6])),\n    ],\n)?)?;\n\nDeltaOps(table)\n.merge(source_data, \"target.x = source.x\")\n.with_source_alias(\"source\")\n.with_target_alias(\"target\")\n.when_not_matched_insert(\n    |insert| insert.set(\"x\", \"source.x\").set(\"y\", \"source.y\")\n)?.await?;\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table with Sample Data\nDESCRIPTION: Sets up initial target and source data tables using PyArrow for demonstrating merge operations\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntarget_data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\nwrite_deltalake(\"tmp_table\", target_data)\ndt = DeltaTable(\"tmp_table\")\nsource_data = pa.table({\"x\": [2, 3], \"deleted\": [False, True]})\n```\n\n----------------------------------------\n\nTITLE: Writing a Daft DataFrame to Delta Lake\nDESCRIPTION: Shows how to write a Daft DataFrame to a Delta Lake table using the write_deltalake method with overwrite mode.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.write_deltalake(\"tmp/daft-table\", mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Table with Age Filter in Python\nDESCRIPTION: This snippet demonstrates how to query a Delta table using Polars, filtering for records where age is less than 20.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/delta-lake-file-skipping.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npl.scan_delta(\"tmp/a_table\").filter(pl.col(\"age\") < 20).collect()\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Existing Delta Table in Python\nDESCRIPTION: This code snippet shows how to append additional data to an existing Delta table using Polars DataFrame.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/delta-lake-file-skipping.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pl.DataFrame({\"name\": [\"allan\", \"brian\", \"linda\"], \"age\": [34, 35, 78]})\ndf.write_delta(\"tmp/a_table\", mode=\"append\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DeltaLakePandasIOManager in Dagster\nDESCRIPTION: Python code to configure the DeltaLakePandasIOManager in a Dagster project for using Delta Lake with Pandas.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_deltalake_pandas import DeltaLakePandasIOManager\n\n\ndefs = Definitions(\n   assets=all_assets,\n   resources={\n        \"io_manager\": DeltaLakePandasIOManager(\n            root_uri=\"path/to/deltalake\",\n            storage_options=LocalConfig(),\n            schema=\"dagster_deltalake\",\n        ),\n   },\n)\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Pandas in Dagster Assets\nDESCRIPTION: Python code showing how to create a Dagster asset that reads data into a Pandas DataFrame, which will be stored as a Delta Lake table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\n\n@asset\ndef iris_dataset() -> pd.DataFrame:\n   return pd.read_csv(\n       \"https://docs.dagster.io/assets/iris.csv\",\n       names=[\n           \"sepal_length_cm\",\n           \"sepal_width_cm\",\n           \"petal_length_cm\",\n           \"petal_width_cm\",\n           \"species\",\n       ],\n   )\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Table to PyArrow Dataset in Python\nDESCRIPTION: Shows how to convert a Delta table to a PyArrow Dataset, which allows for more advanced filtering and batch processing of data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.dataset as ds\n>>> dataset = dt.to_pyarrow_dataset()\n>>> condition = (ds.field(\"year\") == \"2021\") & (ds.field(\"value\") > \"4\")\n>>> dataset.to_table(filter=condition, columns=[\"value\"]).to_pandas()\n  value\n0     6\n1     7\n2     5\n>>> batch_iter = dataset.to_batches(filter=condition, columns=[\"value\"], batch_size=2)\n>>> for batch in batch_iter: print(batch.to_pandas())\n  value\n0     6\n1     7\n  value\n0     5\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows with Rust using Delta Lake's SQL Predicate\nDESCRIPTION: Shows how to delete rows from a Delta table using Rust with a SQL WHERE clause as a predicate. This approach provides an alternative to using the Datafusion Expression API.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/deleting-rows-from-delta-lake-table.md#2025-04-16_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"./data/simple_table\").await?;\nlet (table, delete_metrics) = DeltaOps(table)\n    .delete()\n    .with_predicate(\"num > 2\")\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Table to ADLS with Pandas\nDESCRIPTION: Example demonstrating how to write a Delta table to Azure Data Lake Storage using Pandas dataframe and the deltalake library. The code creates a Pandas DataFrame and writes it to ADLS using the write_deltalake function.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/adls.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom deltalake import write_deltalake\n\ndf = pd.DataFrame({\"foo\": [1, 2, 3, 4, 5]})\n\nwrite_deltalake(\n    f\"abfs://{container}/delta_table_pandas\",\n    df,\n    storage_options=storage_options\n)\n```\n\n----------------------------------------\n\nTITLE: Loading a Delta Table from Databricks Unity Catalog in Python\nDESCRIPTION: Shows how to load a Delta table from Databricks Unity Catalog using environment variables for authentication.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> from deltalake import DataCatalog, DeltaTable\n>>> os.environ['DATABRICKS_WORKSPACE_URL'] = \"https://adb-62800498333851.30.azuredatabricks.net\"\n>>> os.environ['DATABRICKS_ACCESS_TOKEN'] = \"<DBAT>\"\n>>> catalog_name = 'main'\n>>> schema_name = 'db_schema'\n>>> table_name = 'db_table'\n>>> data_catalog = DataCatalog.UNITY\n>>> dt = DeltaTable.from_data_catalog(data_catalog=data_catalog, data_catalog_id=catalog_name, database_name=schema_name, table_name=table_name)\n```\n\n----------------------------------------\n\nTITLE: Initial Delta Table Transaction Log in JSON Format\nDESCRIPTION: Contents of the first transaction log file showing protocol information, metadata, file addition details, and commit information for the initial table creation operation. Includes schema definition and statistics for the added Parquet file.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/architecture-of-delta-table.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"protocol\": {\n    \"minReaderVersion\": 1,\n    \"minWriterVersion\": 1\n  }\n}\n{\n  \"metaData\": {\n    \"id\": \"b96ea1a2-1830-4da2-8827-5334cc6104ed\",\n    \"name\": null,\n    \"description\": null,\n    \"format\": {\n      \"provider\": \"parquet\",\n      \"options\": {}\n    },\n    \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"num\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"letter\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n    \"partitionColumns\": [],\n    \"createdTime\": 1701740315599,\n    \"configuration\": {}\n  }\n}\n{\n  \"add\": {\n    \"path\": \"0-62dffa23-bbe1-4496-8fb5-bff6724dc677-0.parquet\",\n    \"size\": 2208,\n    \"partitionValues\": {},\n    \"modificationTime\": 1701740315597,\n    \"dataChange\": true,\n    \"stats\": \"{\\\"numRecords\\\": 3, \\\"minValues\\\": {\\\"num\\\": 1, \\\"letter\\\": \\\"a\\\"}, \\\"maxValues\\\": {\\\"num\\\": 3, \\\"letter\\\": \\\"c\\\"}, \\\"nullCount\\\": {\\\"num\\\": 0, \\\"letter\\\": 0}}\"\n  }\n}\n{\n  \"commitInfo\": {\n    \"timestamp\": 1701740315602,\n    \"operation\": \"CREATE TABLE\",\n    \"operationParameters\": {\n      \"location\": \"file:///Users/matthew.powers/Documents/code/delta/delta-examples/notebooks/python-deltalake/tmp/some-table\",\n      \"metadata\": \"{\\\"configuration\\\":{},\\\"created_time\\\":1701740315599,\\\"description\\\":null,\\\"format\\\":{\\\"options\\\":{},\\\"provider\\\":\\\"parquet\\\"},\\\"id\\\":\\\"b96ea1a2-1830-4da2-8827-5334cc6104ed\\\",\\\"name\\\":null,\\\"partition_columns\\\":[],\\\"schema\\\":{\\\"fields\\\":[{\\\"metadata\\\":{},\\\"name\\\":\\\"num\\\",\\\"nullable\\\":true,\\\"type\\\":\\\"long\\\"},{\\\"metadata\\\":{},\\\"name\\\":\\\"letter\\\",\\\"nullable\\\":true,\\\"type\\\":\\\"string\\\"}],\\\"type\\\":\\\"struct\\\"}}\",\n      \"protocol\": \"{\\\"minReaderVersion\\\":1,\\\"minWriterVersion\\\":1}\",\n      \"mode\": \"ErrorIfExists\"\n    },\n    \"clientVersion\": \"delta-rs.0.17.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Table to PyArrow Table in Python\nDESCRIPTION: Demonstrates how to convert a Delta table to a PyArrow Table, with options for filtering partitions and selecting specific columns.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> dt.to_pyarrow_table(partitions=[(\"year\", \"=\", \"2021\")], columns=[\"value\"])\npyarrow.Table\nvalue: string\n```\n\n----------------------------------------\n\nTITLE: Loading a Delta Table from AWS Glue Data Catalog in Python\nDESCRIPTION: Demonstrates how to load a Delta table using AWS Glue Data Catalog references.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> from deltalake import DataCatalog\n>>> database_name = \"simple_database\"\n>>> table_name = \"simple_table\"\n>>> data_catalog = DataCatalog.AWS\n>>> dt = DeltaTable.from_data_catalog(data_catalog=data_catalog, database_name=database_name, table_name=table_name)\n>>> dt.to_pyarrow_table().to_pydict()\n{'id': [5, 7, 9, 5, 6, 7, 8, 9]}\n```\n\n----------------------------------------\n\nTITLE: Creating a DeltaTable object\nDESCRIPTION: Python code snippet showing how to create a DeltaTable object that holds the metadata for the Delta table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndt = DeltaTable(\"delta_table_dir\")\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Polars in Dagster Assets\nDESCRIPTION: Python code demonstrating how to create a Dagster asset that reads data into a Polars DataFrame, which will be stored as a Delta Lake table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\nfrom dagster import asset\n\n\n@asset\ndef iris_dataset() -> pl.DataFrame:\n   return pl.read_csv(\n       \"https://docs.dagster.io/assets/iris.csv\",\n       new_columns=[\n          \"sepal_length_cm\",\n          \"sepal_width_cm\",\n          \"petal_length_cm\",\n          \"petal_width_cm\",\n          \"species\",\n      ],\n   has_header=False\n)\n```\n\n----------------------------------------\n\nTITLE: Opening and Inspecting Delta Tables in Rust\nDESCRIPTION: Demonstrates how to open an existing Delta table and list its files using the Rust API. This shows the asynchronous nature of the Rust implementation and basic table inspection.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/README.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse deltalake::{open_table, DeltaTableError};\n\n#[tokio::main]\nasync fn main() -> Result<(), DeltaTableError> {\n    // open the table written in python\n    let table = open_table(\"./data/delta\").await?;\n\n    // show all active files in the table\n    let files: Vec<_> = table.get_file_uris()?.collect();\n    println!(\"{files:?}\");\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Add Actions for Specific Version in Python\nDESCRIPTION: Gets the add actions for a specific version of a Delta table by specifying the version parameter when creating the DeltaTable instance.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.8.0\", version=0)\n>>> dt.get_add_actions(flatten=True).to_pandas()\n                                                    path  size_bytes   modification_time  data_change  num_records  null_count.value  min.value  max.value\n0  part-00000-c9b90f86-73e6-46c8-93ba-ff6bfaf892a...         440 2021-03-06 15:16:07         True            2                 0          0          2\n1  part-00001-911a94a2-43f6-4acb-8620-5e68c265498...         445 2021-03-06 15:16:07         True            3                 0          2          4\n```\n\n----------------------------------------\n\nTITLE: Registering and Querying a Parquet File with DataFusion\nDESCRIPTION: This code illustrates how to register a Parquet file as a table in DataFusion and run a SQL query on it. It's used to compare performance with the Delta Lake equivalent.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-datafusion.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath = \"G1_1e9_1e2_0_0.parquet\"\nctx.register_parquet(\"my_parquet_table\", path)\nctx.sql(\"select id1, sum(v1) as v1 from my_parquet_table where id1='id096' group by id1\")\n```\n\n----------------------------------------\n\nTITLE: Initializing QueryBuilder for Delta Lake in Python\nDESCRIPTION: Creates a new QueryBuilder instance for a given Delta table. This is the starting point for constructing queries on Delta tables.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/query.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nQueryBuilder(table: DeltaTable)\n```\n\n----------------------------------------\n\nTITLE: Writing Hourly Data to Delta Table in Rust\nDESCRIPTION: Creates a Delta table and writes 100 hours worth of observation data, creating many small files.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = DeltaOps::try_from_uri(\"observation_data\")\n  .await?\n  .create()\n  .with_table_name(\"observations_data\")\n  .with_columns(\n      StructType::new(vec![\n          StructField::new(\n              \"date\".to_string(),\n              DataType::Primitive(PrimitiveType::Date),\n              false,\n          ),\n          StructField::new(\n              \"timestamp\".to_string(),\n              DataType::Primitive(PrimitiveType::Timestamp),\n              false,\n          ),\n          StructField::new(\n              \"value\".to_string(),\n              DataType::Primitive(PrimitiveType::Double),\n              false,\n          ),\n      ])\n      .fields()\n      .cloned(),\n  )\n  .with_partition_columns(vec![\"date\"])\n  .with_save_mode(SaveMode::Append)\n  .await?;\n\nlet hours_iter = (0..).map(|i| {\n    \"2021-01-01T00:00:00Z\".parse::<DateTime<Utc>>().unwrap() + chrono::Duration::hours(i)\n});\n\n// write 100 hours worth of data\nfor timestamp in hours_iter.take(100) {\n    let batch = record_observations(timestamp);\n    let mut writer =  deltalake::writer::RecordBatchWriter::for_table(&table)?;\n    writer\n        .write(batch)\n        .await?;\n    writer.flush_and_commit(&mut table).await?\n}\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Delta table on S3 using Polars\nDESCRIPTION: Python code to write a Polars DataFrame to a Delta table on S3, using the configured storage options.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3-like.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.write_delta(\n    \"s3://bucket/delta_table\",\n    storage_options=storage_options,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Lake Tables in Downstream Dagster Assets\nDESCRIPTION: Python code showing how to use a Delta Lake table as input to a downstream Dagster asset. The Delta Lake I/O Manager handles reading and writing the data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nfrom dagster import asset\n\n@asset\ndef clean_dataset(raw_dataset: pa.Table) -> pa.Table:\n   return raw_dataset.drop_null()\n```\n\n----------------------------------------\n\nTITLE: Querying Z-Ordered Delta Lake Table\nDESCRIPTION: Example demonstrating how to query a Z-Ordered Delta Lake table. This query executes in 2.4 seconds, showing the significant performance boost from Z-Ordering which enables more effective file skipping.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n(\n    DeltaTable(f\"{Path.home()}/data/deltalake_baseline_G1_1e9_1e2_0_0\", version=1)\n    .to_pandas(filters=[(\"id1\", \"==\", \"id016\")], columns=[\"id1\", \"id2\", \"v1\"])\n    .query(\"id1 == 'id016'\")\n    .groupby(\"id2\")\n    .agg({\"v1\": \"sum\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in Delta Lake Query with Python\nDESCRIPTION: Specifies which columns to include in the query result. If not called, all columns will be selected by default.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/query.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nselect(*columns: str) -> QueryBuilder\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Boost for TableAlterer Documentation\nDESCRIPTION: YAML configuration block that sets search boost parameters for the TableAlterer documentation page. The boost value of 10 increases the page's relevance in search results.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/delta_table/delta_table_alterer.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsearch:\n  boost: 10\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Tables into Dask\nDESCRIPTION: Shows how to load Delta table data into Dask DataFrames by passing the file paths directly to Dask's read_parquet function.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/querying-delta-tables.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n>>> df = dd.read_parquet(dt.file_uris())\n>>> df\nDask DataFrame Structure:\n                value             year            month              day\nnpartitions=6\n               object  category[known]  category[known]  category[known]\n                  ...              ...              ...              ...\n...               ...              ...              ...              ...\n                  ...              ...              ...              ...\n                  ...              ...              ...              ...\nDask Name: read-parquet, 6 tasks\n>>> df.compute()\n  value  year month day\n0     1  2020     1   1\n0     2  2020     2   3\n0     3  2020     2   5\n0     4  2021     4   5\n0     5  2021    12   4\n0     6  2021    12  20\n1     7  2021    12  20\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Delta Table\nDESCRIPTION: Demonstrates how to append 24 hours of data to a Delta table using partition-by-date strategy. Shows implementations in both Python and Rust with datetime handling.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor timestamp in itertools.islice(hours_iter, 24):\n    write_deltalake(\n        dt,\n        record_observations(timestamp),\n        partition_by=[\"date\"],\n        mode=\"append\",\n    )\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = open_table(\"observation_data\").await?;\nlet hours_iter = (0..).map(|i| {\n    \"2021-01-01T00:00:00Z\".parse::<DateTime<Utc>>().unwrap() + chrono::Duration::hours(i)\n});\nfor timestamp in hours_iter.skip(100).take(24) {\n    let batch = record_observations(timestamp);\n    let mut writer =  deltalake::writer::RecordBatchWriter::for_table(&table)?;\n    writer\n        .write(batch)\n        .await?;\n    writer.flush_and_commit(&mut table).await?;\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Parquet Data with Row Group Skipping\nDESCRIPTION: Example showing how to query Parquet data with row group skipping enabled using filters. This query runs in 19 seconds, significantly faster than the basic Parquet query by avoiding reading unnecessary row groups.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(\n    pd.read_parquet(\n        f\"{Path.home()}/data/G1_1e9_1e2_0_0.parquet\",\n        columns=[\"id1\", \"id2\", \"v1\"],\n        filters=[(\"id1\", \"==\", \"id016\")],\n    )\n    .query(\"id1 == 'id016'\")\n    .groupby(\"id2\")\n    .agg({\"v1\": \"sum\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Tables with Add Actions in Delta Lake Rust\nDESCRIPTION: The create_table_with_add_actions function creates a new Delta table using a series of add actions. It likely takes parameters for table properties and a list of AddAction instances.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/transaction.md#2025-04-16_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n::: deltalake.transaction.create_table_with_add_actions\n```\n\n----------------------------------------\n\nTITLE: Documenting DeltaError in Delta-rs Python Library\nDESCRIPTION: Base exception class for all Delta Lake related errors in the Delta-rs Python library.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/exceptions.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.exceptions.DeltaError\n```\n\n----------------------------------------\n\nTITLE: Overwrite Operation Transaction Log in JSON Format\nDESCRIPTION: Contents of the third transaction log file showing the overwrite operation, including file addition, removal of previous files (tombstoning), and commit information. Demonstrates Delta Lake's logical deletion approach for data replacement.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/architecture-of-delta-table.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"add\": {\n    \"path\": \"2-95ef2108-480c-4b89-96f0-ff9185dab9ad-0.parquet\",\n    \"size\": 2204,\n    \"partitionValues\": {},\n    \"modificationTime\": 1701740465102,\n    \"dataChange\": true,\n    \"stats\": \"{\\\"numRecords\\\": 2, \\\"minValues\\\": {\\\"num\\\": 11, \\\"letter\\\": \\\"aa\\\"}, \\\"maxValues\\\": {\\\"num\\\": 22, \\\"letter\\\": \\\"bb\\\"}, \\\"nullCount\\\": {\\\"num\\\": 0, \\\"letter\\\": 0}}\"\n  }\n}\n{\n  \"remove\": {\n    \"path\": \"0-62dffa23-bbe1-4496-8fb5-bff6724dc677-0.parquet\",\n    \"deletionTimestamp\": 1701740465102,\n    \"dataChange\": true,\n    \"extendedFileMetadata\": false,\n    \"partitionValues\": {},\n    \"size\": 2208\n  }\n}\n{\n  \"remove\": {\n    \"path\": \"1-57abb6fb-2249-43ba-a7be-cf09bcc230de-0.parquet\",\n    \"deletionTimestamp\": 1701740465102,\n    \"dataChange\": true,\n    \"extendedFileMetadata\": false,\n    \"partitionValues\": {},\n    \"size\": 2204\n  }\n}\n{\n  \"commitInfo\": {\n    \"timestamp\": 1701740465102,\n    \"operation\": \"WRITE\",\n    \"operationParameters\": {\n      \"mode\": \"Overwrite\",\n      \"partitionBy\": \"[]\"\n    },\n    \"clientVersion\": \"delta-rs.0.17.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Dask with Delta Table in Python\nDESCRIPTION: Shows how to load Delta table data into a Dask DataFrame for distributed processing.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask.dataframe as dd\n>>> df = dd.read_parquet(dt.file_uris())\n>>> df\nDask DataFrame Structure:\n                value             year            month              day\nnpartitions=6\n               object  category[known]  category[known]  category[known]\n                  ...              ...              ...              ...\n...               ...              ...              ...              ...\n                  ...              ...              ...              ...\n                  ...              ...              ...              ...\nDask Name: read-parquet, 6 tasks\n>>> df.compute()\n  value  year month day\n0     1  2020     1   1\n0     2  2020     2   3\n0     3  2020     2   5\n0     4  2021     4   5\n0     5  2021    12   4\n0     6  2021    12  20\n1     7  2021    12  20\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Merge Operation Logs with Partition Column Predicates\nDESCRIPTION: This snippet demonstrates the improved performance of a merge operation when partition columns are included in the predicates. It shows a significant reduction in scanned files and execution time.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nMerging table with predicates: {\n    'merge': 's.unique_constraint_hash = t.unique_constraint_hash AND s.month_id = t.month_id AND t.month_id = 202503',\n    'when_matched_update_all': 's.post_transform_row_hash != t.post_transform_row_hash AND s.month_id = t.month_id AND t.month_id = 202503'\n}\nFiles Scanned: 1\nFiles Skipped: 10\nFiles Added: 1\nExecution Time: 2964ms\n```\n\n----------------------------------------\n\nTITLE: Creating Write Transactions for Delta Tables in Rust\nDESCRIPTION: The create_write_transaction method on DeltaTable initiates a new write transaction. It likely returns a transaction object that can be used to perform write operations on the table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/transaction.md#2025-04-16_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n::: deltalake.DeltaTable.create_write_transaction\n```\n\n----------------------------------------\n\nTITLE: Documenting CommitFailedError in Delta-rs Python Library\nDESCRIPTION: Exception raised when a commit operation on a Delta table fails.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/exceptions.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.exceptions.CommitFailedError\n```\n\n----------------------------------------\n\nTITLE: Enabling Concurrent Writes for Alternative S3 Clients in Python\nDESCRIPTION: This Python code snippet demonstrates how to enable concurrent writes for alternative S3 clients like CloudFlare R2 or MinIO. It shows the storage options configuration required to support atomic renames using ETag headers.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/writing-to-s3-with-locking-provider.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\n    \"conditional_put\": \"etag\",\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Class for Delta Lake in Rust\nDESCRIPTION: This code snippet defines the Metadata class for Delta Lake operations in Rust. It includes various properties that describe the structure and characteristics of a Delta Lake table, such as schema, partition columns, and configuration details.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/delta_table/metadata.md#2025-04-16_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n::: deltalake.Metadata\n    options:\n        show_root_heading: true\n```\n\n----------------------------------------\n\nTITLE: Writing Hourly Data to Delta Table in Python\nDESCRIPTION: Writes 100 hours worth of observation data to a Delta table, creating many small files.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Every hour starting at midnight on 2021-01-01\nhours_iter = (datetime(2021, 1, 1) + timedelta(hours=i) for i in itertools.count())\n\n# Write 100 hours worth of data\nfor timestamp in itertools.islice(hours_iter, 100):\n    write_deltalake(\n        \"observation_data\",\n        record_observations(timestamp),\n        partition_by=[\"date\"],\n        mode=\"append\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Random Observation Data in Python\nDESCRIPTION: Defines a function to generate 1000 rows of random observation data for a given timestamp using PyArrow.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef record_observations(date: datetime) -> pa.Table:\n    \"\"\"Pulls data for a certain datetime\"\"\"\n    nrows = 1000\n    return pa.table(\n        {\n            \"date\": pa.array([date.date()] * nrows),\n            \"timestamp\": pa.array([date] * nrows),\n            \"value\": pc.random(nrows),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining CommitProperties for Delta Transactions in Rust\nDESCRIPTION: The CommitProperties struct defines properties for committing transactions in Delta Lake. It likely includes fields for specifying commit metadata and behavior.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/transaction.md#2025-04-16_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n::: deltalake.CommitProperties\n```\n\n----------------------------------------\n\nTITLE: Defining DynamoDB Table Schema for Delta Lake Locking\nDESCRIPTION: This JSON snippet defines the required schema for a DynamoDB table used as a locking mechanism in Delta Lake. It specifies the table name, attribute definitions, and key schema necessary for delta-rs to recognize and use the table for concurrent write operations.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/writing/writing-to-s3-with-locking-provider.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Table\": {\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"fileName\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"tablePath\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"TableName\": \"delta_log\",\n        \"KeySchema\": [\n            {\n                \"AttributeName\": \"tablePath\",\n                \"KeyType\": \"HASH\"\n            },\n            {\n                \"AttributeName\": \"fileName\",\n                \"KeyType\": \"RANGE\"\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Time Traveling with Delta Lake Tables\nDESCRIPTION: Three examples showing how to access different versions of a Delta Lake table using time travel. These snippets demonstrate accessing the latest version (default), version 0, and version 1 of the table.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nDeltaTable(\"tmp/some-table\").to_pandas()\n```\n\nLANGUAGE: python\nCODE:\n```\nDeltaTable(\"tmp/some-table\", version=0).to_pandas()\n```\n\nLANGUAGE: python\nCODE:\n```\nDeltaTable(\"tmp/some-table\", version=1).to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Configuring LakeFS storage options in Python\nDESCRIPTION: This snippet sets up the storage_options dictionary with LakeFS credentials, including the endpoint, access key ID, and secret access key. These options are required for authenticating with LakeFS.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/lakefs.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\n        \"endpoint\": \"https://mylakefs.intranet.com\", # LakeFS endpoint\n        \"access_key_id\": \"LAKEFSID\",\n        \"secret_access_key\": \"LAKEFSKEY\",\n    }\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Table Output Format\nDESCRIPTION: Shows the resulting table contents after creating a Delta Lake table. Displays the data in a tabular format with numeric and string columns.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/create-delta-lake-table.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n+-------+----------+\n|   num | letter   |\n|-------+----------|\n|     1 | a        |\n|     2 | b        |\n|     3 | c        |\n+-------+----------+\n```\n\n----------------------------------------\n\nTITLE: Schema Module Import Reference\nDESCRIPTION: Module import reference showing the namespace for accessing schema functionality\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/schema.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.schema\n```\n\n----------------------------------------\n\nTITLE: Querying CSV Data with pandas\nDESCRIPTION: Example showing how to query a large CSV dataset with pandas, including column selection and filtering by id1. This approach takes 234 seconds to execute and requires column pruning to avoid memory issues.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n(\n    pd.read_csv(f\"{Path.home()}/data/G1_1e9_1e2_0_0.csv\", usecols=[\"id1\", \"id2\", \"v1\"])\n    .query(\"id1 == 'id016'\")\n    .groupby(\"id2\")\n    .agg({\"v1\": \"sum\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table History in Rust\nDESCRIPTION: Gets the history of operations performed on a Delta table in Rust, showing timestamps, operations, parameters, and versions.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nlet table = deltalake::open_table(\"../rust/tests/data/simple_table\").await?;\nlet history = table.history(None).await?;\nprintln!(\"Table history: {:#?}\", history);\n```\n\n----------------------------------------\n\nTITLE: Running Python Tests for delta-rs Project\nDESCRIPTION: Command to run a specific Python test using pytest within the uv-managed environment.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv run pytest tests/test_writer.py -s -k \"test_with_deltalake_schema\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Add Actions for Specific Version in Rust\nDESCRIPTION: Gets the add actions for a specific version of a Delta table in Rust by loading a specific version before retrieving the add actions.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_13\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = deltalake::open_table(\"./data/simple_table\").await?;\ntable.load_version(0).await?;\nlet actions = table.snapshot()?.add_actions_table(true)?;\nprintln!(\"{}\", pretty_format_batches(&vec![actions])?);\n```\n\n----------------------------------------\n\nTITLE: Converting TPC-DS Web Returns CSV to Delta Table\nDESCRIPTION: This command converts a TPC-DS web_returns CSV file into a Delta table. It assumes the input file is pipe-delimited and records do not have a trailing delimiter.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/benchmarks/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release --bin merge -- convert data/tpcds/web_returns.dat data/web_returns\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Transaction Log Entry in JSON\nDESCRIPTION: Illustrates the structure of a Delta Lake transaction log entry. This JSON shows the details of a delete operation, including file additions, removals, and operation metrics.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/delta-lake-acid-transactions.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"add\": {\n    \"path\": \"part-00001-90312b96-b487-4a8f-9edc-1b9b3963f136-c000.snappy.parquet\",\n    \"partitionValues\": {},\n    \"size\": 858,\n    \"modificationTime\": 1705070631953,\n    \"dataChange\": true,\n    \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"num\\\":2,\\\"animal\\\":\\\"dog\\\"},\\\"maxValues\\\":{\\\"num\\\":3,\\\"animal\\\":\\\"snake\\\"},\\\"nullCount\\\":{\\\"num\\\":0,\\\"animal\\\":0}}\",\n    \"tags\": null,\n    \"deletionVector\": null,\n    \"baseRowId\": null,\n    \"defaultRowCommitVersion\": null,\n    \"clusteringProvider\": null\n  }\n}\n{\n  \"remove\": {\n    \"path\": \"0-fea2de92-861a-423e-9708-a9e91dafb27b-0.parquet\",\n    \"dataChange\": true,\n    \"deletionTimestamp\": 1705070631953,\n    \"extendedFileMetadata\": true,\n    \"partitionValues\": {},\n    \"size\": 895\n  }\n}\n{\n  \"commitInfo\": {\n    \"timestamp\": 1705070631953,\n    \"operation\": \"DELETE\",\n    \"operationParameters\": {\n      \"predicate\": \"animal = 'cat'\"\n    },\n    \"readVersion\": 0,\n    \"operationMetrics\": {\n      \"execution_time_ms\": 8013,\n      \"num_added_files\": 1,\n      \"num_copied_rows\": 2,\n      \"num_deleted_rows\": 1,\n      \"num_removed_files\": 1,\n      \"rewrite_time_ms\": 2,\n      \"scan_time_ms\": 5601\n    },\n    \"clientVersion\": \"delta-rs.0.17.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Parquet Data with pandas\nDESCRIPTION: Example demonstrating how to query a Parquet dataset with pandas, showing basic column selection. This query executes in 118 seconds, faster than the CSV approach but without utilizing Parquet's row group skipping capability.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n(\n    pd.read_parquet(\n        f\"{Path.home()}/data/G1_1e9_1e2_0_0.parquet\", columns=[\"id1\", \"id2\", \"v1\"]\n    )\n    .query(\"id1 == 'id016'\")\n    .groupby(\"id2\")\n    .agg({\"v1\": \"sum\"})\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Add Actions from Delta Table in Python\nDESCRIPTION: Shows how to get the add actions for a Delta table, which provide information about the files that are part of the table, including their paths, sizes, and statistics.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> from deltalake import DeltaTable\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.8.0\")\n>>> dt.get_add_actions(flatten=True).to_pandas()\n                                                        path  size_bytes   modification_time  data_change  num_records  null_count.value  min.value  max.value\n0  part-00000-c9b90f86-73e6-46c8-93ba-ff6bfaf892a...         440 2021-03-06 15:16:07         True            2                 0          0          2\n1  part-00000-04ec9591-0b73-459e-8d18-ba5711d6cbe...         440 2021-03-06 15:16:16         True            2                 0          2          4\n```\n\n----------------------------------------\n\nTITLE: Reading Existing Delta Lake Tables into Dagster\nDESCRIPTION: Python code demonstrating how to make existing Delta Lake tables available to Dagster assets using the SourceAsset object.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import SourceAsset\n\niris_harvest_data = SourceAsset(key=\"more_animal_data\")\n```\n\n----------------------------------------\n\nTITLE: Importing TableOptimizer in Python\nDESCRIPTION: This code snippet shows how to import the TableOptimizer class from the deltalake.table module. The TableOptimizer is used for optimizing Delta tables in the Delta Lake Rust implementation.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/delta_table/delta_table_optimizer.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake.table import TableOptimizer\n```\n\n----------------------------------------\n\nTITLE: Append Operation Transaction Log in JSON Format\nDESCRIPTION: Contents of the second transaction log file showing metadata about the append operation, including file addition details and commit information. Includes statistics for the newly added Parquet file.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/how-delta-lake-works/architecture-of-delta-table.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"add\": {\n    \"path\": \"1-57abb6fb-2249-43ba-a7be-cf09bcc230de-0.parquet\",\n    \"size\": 2204,\n    \"partitionValues\": {},\n    \"modificationTime\": 1701740386169,\n    \"dataChange\": true,\n    \"stats\": \"{\\\"numRecords\\\": 2, \\\"minValues\\\": {\\\"num\\\": 8, \\\"letter\\\": \\\"dd\\\"}, \\\"maxValues\\\": {\\\"num\\\": 9, \\\"letter\\\": \\\"ee\\\"}, \\\"nullCount\\\": {\\\"num\\\": 0, \\\"letter\\\": 0}}\"\n  }\n}\n{\n  \"commitInfo\": {\n    \"timestamp\": 1701740386169,\n    \"operation\": \"WRITE\",\n    \"operationParameters\": {\n      \"partitionBy\": \"[]\",\n      \"mode\": \"Append\"\n    },\n    \"clientVersion\": \"delta-rs.0.17.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting Delta Lake Query Results with Python\nDESCRIPTION: Limits the number of rows returned by the query to the specified count.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/query.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlimit(count: int) -> QueryBuilder\n```\n\n----------------------------------------\n\nTITLE: Verifying Delta Table Existence\nDESCRIPTION: Methods to check if a Delta table exists at a specified path with optional storage configuration.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/loading-table.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deltalake import DeltaTable\n\ntable_path = \"<path/to/valid/table>\"\nDeltaTable.is_deltatable(table_path)\n# True\n\ninvalid_table_path = \"<path/to/nonexistent/table>\"\nDeltaTable.is_deltatable(invalid_table_path)\n# False\n\nbucket_table_path = \"<path/to/valid/table/in/bucket>\"\nstorage_options = {\n    \"AWS_ACCESS_KEY_ID\": \"THE_AWS_ACCESS_KEY_ID\",\n    \"AWS_SECRET_ACCESS_KEY\": \"THE_AWS_SECRET_ACCESS_KEY\",\n    ...\n}\nDeltaTable.is_deltatable(bucket_table_path, storage_options)\n# True\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet table_path = \"<path/to/valid/table>\";\nlet builder = deltalake::DeltaTableBuilder::from_uri(table_path);\nbuilder.build()?.verify_deltatable_existence().await?;\n// true\n\nlet invalid_table_path = \"<path/to/nonexistent/table>\";\nlet builder = deltalake::DeltaTableBuilder::from_uri(invalid_table_path);\nbuilder.build()?.verify_deltatable_existence().await?;\n// false\n\nlet bucket_table_path = \"<path/to/valid/table/in/bucket>\";\nlet storage_options = HashMap::from_iter(vec![\n    (\"AWS_ACCESS_KEY_ID\".to_string(), \"THE_AWS_ACCESS_KEY_ID\".to_string()),\n    (\"AWS_SECRET_ACCESS_KEY\".to_string(), \"THE_AWS_SECRET_ACCESS_KEY\".to_string()),\n]);\nlet builder = deltalake::DeltaTableBuilder::from_uri(bucket_table_path).with_storage_options(storage_options);\nbuilder.build()?.verify_deltatable_existence().await?;\n// true\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment with Virtualenv\nDESCRIPTION: Sets up a local development environment for the Python deltalake package using the make setup command, which likely creates a virtual environment with required dependencies.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake setup\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 storage options in Python\nDESCRIPTION: Python dictionary setup for S3 storage options, including AWS credentials and conditional put flag for safe concurrency.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3-like.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\n    'AWS_SECRET_ACCESS_KEY': <access_key>,\n    'aws_conditional_put': 'etag', # Here we say to use conditional put, this provides safe concurrency.\n}\n```\n\n----------------------------------------\n\nTITLE: Schema Class Reference\nDESCRIPTION: Reference to the Schema class which represents Delta table schemas\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/schema.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.Schema\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Merge Operation Logs without Partition Column Predicates\nDESCRIPTION: This snippet shows the log output of a merge operation on a partitioned table without including partition columns in the predicates. It demonstrates inefficient performance with all files being scanned.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nMerging table with predicates: {\n    'merge': 's.unique_constraint_hash = t.unique_constraint_hash',\n    'when_matched_update_all': 's.post_transform_row_hash != t.post_transform_row_hash'\n}\nFiles Scanned: 24\nFiles Skipped: 0\nFiles Added: 1\nExecution Time: 23774ms\n```\n\n----------------------------------------\n\nTITLE: Configuring storage options for local Minio in Python\nDESCRIPTION: Python dictionary setup for storage options when using locally hosted Minio, including HTTP allowance and endpoint URL.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3-like.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\n    'AWS_ACCESS_KEY_ID': ...,\n    'AWS_SECRET_ACCESS_KEY': ...,\n    'AWS_ENDPOINT_URL': 'http://localhost:9000',\n    'AWS_ALLOW_HTTP': 'true',\n    'aws_conditional_put': 'etag'\n}\n```\n\n----------------------------------------\n\nTITLE: Vacuuming Delta Tables in Rust\nDESCRIPTION: Shows how to vacuum a Delta table using Rust, demonstrating both dry-run and actual deletion operations using the DeltaOps interface.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/managing-tables.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet mut table = open_table(\"./data/simple_table\").await?;\nlet (table, vacuum_metrics) = DeltaOps(table).vacuum().with_dry_run(true).await?;\nprintln!(\"Files deleted: {:?}\", vacuum_metrics.files_deleted);\n\nlet (table, vacuum_metrics) = DeltaOps(table).vacuum().with_dry_run(false).await?;\n```\n\n----------------------------------------\n\nTITLE: Defining AddAction for Delta Write Transactions in Rust\nDESCRIPTION: The AddAction struct represents an action to add data in a Delta Lake write transaction. It likely contains fields for specifying the data to be added and how it should be inserted.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/transaction.md#2025-04-16_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n::: deltalake.transaction.AddAction\n```\n\n----------------------------------------\n\nTITLE: Python Class Documentation Reference\nDESCRIPTION: Sphinx/MkDocs-style documentation reference directive for the DeltaStorageHandler class from the deltalake.fs module.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/storage.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n::: deltalake.fs.DeltaStorageHandler\n```\n\n----------------------------------------\n\nTITLE: DynamoDB Schema for S3 Locking - JSON\nDESCRIPTION: JSON schema definition for the required DynamoDB table structure used in S3 locking mechanism for Delta tables.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n\"Table\": {\n    \"AttributeDefinitions\": [\n        {\n            \"AttributeName\": \"fileName\",\n            \"AttributeType\": \"S\"\n        },\n        {\n            \"AttributeName\": \"tablePath\",\n            \"AttributeType\": \"S\"\n        }\n    ],\n    \"TableName\": \"delta_log\",\n    \"KeySchema\": [\n        {\n            \"AttributeName\": \"tablePath\",\n            \"KeyType\": \"HASH\"\n        },\n        {\n            \"AttributeName\": \"fileName\",\n            \"KeyType\": \"RANGE\"\n        }\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Schema to JSON in Rust\nDESCRIPTION: Converts a Delta Lake schema to its JSON representation in Rust using serde_json.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nprintln!(\"{}\", serde_json::to_string_pretty(&schema)?);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Add Actions for Specific Version in Python\nDESCRIPTION: Demonstrates how to get add actions for a specific version of a Delta table, allowing access to historical table states.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> dt = DeltaTable(\"../rust/tests/data/delta-0.8.0\", version=0)\n>>> dt.get_add_actions(flatten=True).to_pandas()\n                                                    path  size_bytes   modification_time  data_change  num_records  null_count.value  min.value  max.value\n0  part-00000-c9b90f86-73e6-46c8-93ba-ff6bfaf892a...         440 2021-03-06 15:16:07         True            2                 0          0          2\n1  part-00001-911a94a2-43f6-4acb-8620-5e68c265498...         445 2021-03-06 15:16:07         True            3                 0          2          4\n```\n\n----------------------------------------\n\nTITLE: Creating DynamoDB Table for Delta Locking Provider\nDESCRIPTION: AWS CLI command to create a DynamoDB table that serves as a locking provider for safe concurrent writes to Delta tables on S3.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n    aws dynamodb create-table \\\n    --table-name delta_log \\\n    --attribute-definitions AttributeName=tablePath,AttributeType=S AttributeName=fileName,AttributeType=S \\\n    --key-schema AttributeName=tablePath,KeyType=HASH AttributeName=fileName,KeyType=RANGE \\\n    --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n```\n\n----------------------------------------\n\nTITLE: CLI Commands for Delta Table Inspection\nDESCRIPTION: Examples of using the delta-inspect CLI tool to list files and get table information from Delta tables.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/core/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n cargo run --bin delta-inspect files ../rust/tests/data/delta-0.2.0\npart-00000-cb6b150b-30b8-4662-ad28-ff32ddab96d2-c000.snappy.parquet\npart-00000-7c2deba3-1994-4fb8-bc07-d46c948aa415-c000.snappy.parquet\npart-00001-c373a5bd-85f0-4758-815e-7eb62007a15c-c000.snappy.parquet\n cargo run --bin delta-inspect info ./tests/data/delta-0.2.0\nDeltaTable(./tests/data/delta-0.2.0)\n        version: 3\n        metadata: GUID=22ef18ba-191c-4c36-a606-3dad5cdf3830, name=None, description=None, partitionColumns=[], createdTime=1564524294376, configuration={}\n        min_version: read=1, write=2\n        files count: 3\n```\n\n----------------------------------------\n\nTITLE: Configuring MkDocs Documentation Dependencies\nDESCRIPTION: Specifies exact versions of Python packages required for documentation generation using MkDocs. Includes the core MkDocs package along with plugins for Python API documentation, material theme, macro processing, and code execution capabilities. Notable constraint is griffe package being pinned to version 0.44 due to build issues with native symbols in version 0.45.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmkdocs==1.5.3\n# 0.45 results in build errors on our native symbols\ngriffe==0.44\nmkdocstrings[python]==0.23.0\nmkdocs-autorefs==0.5.0\nmkdocs-material==9.4.5\nmkdocs-macros-plugin==1.0.4\nmarkdown-exec[ansi]==1.7.0\nmkdocs-simple-hooks==0.1.5\n```\n\n----------------------------------------\n\nTITLE: Field Class Reference\nDESCRIPTION: Reference to the Field class for defining table columns and their types\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/schema.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.Field\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Merge Operation Logs with Highly Specific Predicates\nDESCRIPTION: This snippet shows the optimal performance of a merge operation by using highly specific predicates including both partition columns and date-specific filters. It demonstrates maximum file skipping and minimal execution time.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/merging-tables.md#2025-04-16_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nMerging table with predicates: {\n    'merge': 's.unique_constraint_hash = t.unique_constraint_hash AND s.month_id = t.month_id AND t.month_id = 202503 AND s.date_id = t.date_id AND t.date_id IN (20250314, 20250315, 20250316)',\n    'when_matched_update_all': 's.post_transform_row_hash != t.post_transform_row_hash AND s.month_id = t.month_id AND t.month_id = 202503 AND s.date_id = t.date_id AND t.date_id IN (20250314, 20250315, 20250316)'\n}\nFiles Scanned: 0\nFiles Skipped: 18\nFiles Added: 1\nExecution Time: 416ms\n```\n\n----------------------------------------\n\nTITLE: Defining PostCommitHookProperties for Delta Transactions in Rust\nDESCRIPTION: The PostCommitHookProperties struct defines properties for post-commit hooks in Delta Lake transactions. These hooks are likely executed after a successful commit.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/transaction.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n::: deltalake.PostCommitHookProperties\n```\n\n----------------------------------------\n\nTITLE: Installing Deltalake-python\nDESCRIPTION: Shows different methods to install the deltalake package using pip, uv, and poetry. This snippet provides installation commands for various Python package managers.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# with pip\npip install deltalake\n# with uv\nuv add deltalake\n# with poetry\npoetry add deltalake\n```\n\n----------------------------------------\n\nTITLE: Docker Compose configuration for Minio\nDESCRIPTION: YAML configuration for setting up Minio locally using Docker Compose, exposing S3-compatible API on ports 9000 and 9001.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3-like.md#2025-04-16_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.8'\n\nservices:\n  minio:\n    image: minio/minio\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: ...\n      MINIO_ROOT_PASSWORD: ...\n    command: server /data --console-address \":9001\"\n```\n\n----------------------------------------\n\nTITLE: Installing deltalake Python package\nDESCRIPTION: Command to install the deltalake Python package using pip.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/index.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deltalake\n```\n\n----------------------------------------\n\nTITLE: Executing Standard Merge Benchmark Suite\nDESCRIPTION: This command runs the standard merge benchmark suite. Results can be saved to a Delta table for further analysis. The output table includes fields for group_id, name, sample, duration_ms, and additional data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/benchmarks/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release --bin merge -- standard data/web_returns 1 data/merge_results\n```\n\n----------------------------------------\n\nTITLE: Generating Random Observation Data in Rust\nDESCRIPTION: Defines a function to generate 1000 rows of random observation data for a given timestamp using Arrow in Rust.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/optimize/small-file-compaction-with-optimize.md#2025-04-16_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub fn record_observations(timestamp: DateTime<Utc>) -> RecordBatch {\n    let nrows = 1000;\n    let date = timestamp.date_naive();\n    let timestamp = timestamp;\n    let value = (0..nrows)\n        .map(|_| rand::random::<f64>())\n        .collect::<Vec<f64>>();\n    let date = (0..nrows).map(|_| date).collect::<Vec<NaiveDate>>();\n    let timestamp = (0..nrows)\n        .map(|_| timestamp.timestamp_micros())\n        .collect::<Vec<i64>>();\n\n    let schema = Schema::new(vec![\n        Field::new(\"date\", arrow::datatypes::DataType::Date32, false),\n        Field::new(\n            \"timestamp\",\n            arrow::datatypes::DataType::Timestamp(TimeUnit::Microsecond, Some(\"UTC\".to_string().into())),\n            false,\n        ),\n        Field::new(\"value\", arrow::datatypes::DataType::Float64, false),\n    ]);\n\n    RecordBatch::try_new(\n        Arc::new(schema),\n        vec![\n            Arc::new(Date32Array::from(\n                date.iter()\n                    .map(|d| Date32Type::from_naive_date(*d))\n                    .collect::<Vec<i32>>(),\n            )),\n            Arc::new(TimestampMicrosecondArray::from(timestamp).with_timezone(\"UTC\")),\n            Arc::new(Float64Array::from(value)),\n        ],\n    )\n    .unwrap()\n}\n```\n\n----------------------------------------\n\nTITLE: Building Optimized Wheels for Current System\nDESCRIPTION: Shows how to build wheels optimized for the current system using Rust flags. This snippet demonstrates building wheels with native CPU optimizations.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nRUSTFLAGS=\"-C target-cpu=native\" uvx maturin build --release --out wheels\n```\n\n----------------------------------------\n\nTITLE: Creating a sample DataFrame in Polars\nDESCRIPTION: Python code to create a simple DataFrame using Polars library with toy data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3-like.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pl.DataFrame({'x': [1, 2, 3]})\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Lake for Python using Conda\nDESCRIPTION: This command installs the deltalake package using Conda, specifying the conda-forge channel.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/installation.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge deltalake\n```\n\n----------------------------------------\n\nTITLE: Creating DynamoDB Table - AWS CLI\nDESCRIPTION: Shell command to create the required DynamoDB table for S3 locking using AWS CLI.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\naws dynamodb create-table \\\n--table-name delta_log \\\n--attribute-definitions AttributeName=tablePath,AttributeType=S AttributeName=fileName,AttributeType=S \\\n--key-schema AttributeName=tablePath,KeyType=HASH AttributeName=fileName,KeyType=RANGE \\\n--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n```\n\n----------------------------------------\n\nTITLE: Materializing a Daft DataFrame\nDESCRIPTION: Demonstrates how to materialize (execute) a lazy Daft DataFrame using the collect method to bring the data into memory.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n> df.collect()\n\n|    | first_name   | last_name   | country   | continent   |\n|---:|:-------------|:------------|:----------|:------------|\n|  0 | Ernesto      | Guevara     | Argentina | NaN         |\n|  1 | Bruce        | Lee         | China     | Asia        |\n|  2 | Jack         | Ma          | China     | Asia        |\n|  3 | Wolfgang     | Manche      | Germany   | NaN         |\n|  4 | Soraya       | Jala        | Germany   | NaN         |\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Lake for Rust using Cargo\nDESCRIPTION: This command adds the deltalake package to a Rust project using Cargo, the Rust package manager.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/installation.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo add deltalake\n```\n\n----------------------------------------\n\nTITLE: Installing libgssapi_krb5 on Debian-based systems for HDFS Kerberos Support\nDESCRIPTION: Command to install the libgssapi_krb5 library on Debian-based systems, which is required for Kerberos authentication with HDFS.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/hdfs.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napt-get install libgssapi-krb5-2\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Lake Library Using Pip\nDESCRIPTION: This code snippet shows how to install the Delta Lake library using the pip package manager. The package name is 'deltalake'.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/installation.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deltalake\n```\n\n----------------------------------------\n\nTITLE: Installing libgssapi_krb5 on RHEL-based systems for HDFS Kerberos Support\nDESCRIPTION: Command to install the libgssapi_krb5 library on RHEL-based systems, which is required for Kerberos authentication with HDFS.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/hdfs.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyum install krb5-libs\n```\n\n----------------------------------------\n\nTITLE: Cross-Compiling for Modern Systems\nDESCRIPTION: Shows how to build wheels optimized for newer CPUs and Linux systems. This snippet demonstrates setting Rust flags for Haswell CPUs and using a newer glibc compatibility.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nRUSTFLAGS=\"-C target-cpu=haswell\" uvx --from 'maturin[zig]' maturin build --release --zig \\\n    --target x86_64-unknown-linux-gnu \\\n    --compatibility manylinux_2_24 \\\n    --out wheels\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment for delta-rs in Bash\nDESCRIPTION: Commands to build the project for development, installing deltalake into a Python virtual environment managed by uv.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd python\nmake develop\n```\n\n----------------------------------------\n\nTITLE: Listing TPC-DS Dataset in S3 using AWS CLI\nDESCRIPTION: This command lists objects in a public S3 bucket containing the TPC-DS dataset with a 1GB scale factor. It demonstrates how to access the dataset where requesters must pay for data transfer.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/benchmarks/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naws s3api list-objects --bucket devrel-delta-datasets --request-payer requester --prefix tpcds-2.13/tpcds_sf1_parquet/web_returns/\n```\n\n----------------------------------------\n\nTITLE: Attempting Schema Mismatch Append in Delta Lake\nDESCRIPTION: Example demonstrating Delta Lake's schema enforcement by attempting to append a DataFrame with mismatched schema. This operation will be rejected with a ValueError to protect table integrity.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-pandas.md#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"num\": [5, 6], \"animal\": [\"cat\", \"dog\"]})\nwrite_deltalake(\"tmp/some-table\", df)\n```\n\n----------------------------------------\n\nTITLE: Documenting TableNotFoundError in Delta-rs Python Library\nDESCRIPTION: Exception raised when a Delta table cannot be found at the specified location.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/exceptions.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.exceptions.TableNotFoundError\n```\n\n----------------------------------------\n\nTITLE: Delta Protocol Support Tables in Markdown\nDESCRIPTION: Two markdown tables showing the implementation status of different Delta Lake protocol versions. The first table covers writer version requirements from version 2 through 7, while the second table shows reader version requirements.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/README.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Writer Version | Requirement                                   |              Status               |\n| -------------- | --------------------------------------------- | :-------------------------------: |\n| Version 2      | Append Only Tables                            |              ![done]              |\n| Version 2      | Column Invariants                             |              ![done]              |\n| Version 3      | Enforce `delta.checkpoint.writeStatsAsJson`   |              ![done]              |\n| Version 3      | Enforce `delta.checkpoint.writeStatsAsStruct` |              ![done]              |\n| Version 3      | CHECK constraints                             | [![done]][check-constraints]      |\n| Version 4      | Change Data Feed                              |              ![done]              |\n| Version 4      | Generated Columns                             |              ![done]              |\n| Version 5      | Column Mapping                                |                                   |\n| Version 6      | Identity Columns                              |                                   |\n| Version 7      | Table Features                                |              ![done]              |\n\n| Reader Version | Requirement                         |   Status   |\n| -------------- | ----------------------------------- |   ------   |\n| Version 2      | Column Mapping                      |            |\n| Version 3      | Table Features (requires reader V7) |  ![done]   |\n```\n\n----------------------------------------\n\nTITLE: Converting Delta Schema to Arrow Schema in Rust\nDESCRIPTION: Converts a Delta Lake schema to an Arrow schema in Rust, which facilitates integration with Arrow-based libraries.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/examining-table.md#2025-04-16_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet arrow_schema = table.snapshot()?.arrow_schema()?;\nprintln!(\"arrow_schema: {:?}\", schema);\n```\n\n----------------------------------------\n\nTITLE: Serving Local Documentation for delta-rs Project\nDESCRIPTION: Commands to set up and serve local documentation using mkdocs.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CONTRIBUTING.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n(cd python; make develop)\npip install -r docs/requirements.txt\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Building Custom Wheels with Maturin\nDESCRIPTION: Demonstrates how to build custom wheels using maturin. This snippet shows the command to build release wheels for the current platform.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuvx maturin build --release --out wheels\n```\n\n----------------------------------------\n\nTITLE: Installing libgssapi_krb5 on MacOS for HDFS Kerberos Support\nDESCRIPTION: Command to install the libgssapi_krb5 library on MacOS using Homebrew, which is required for Kerberos authentication with HDFS.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/hdfs.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install krb5\n```\n\n----------------------------------------\n\nTITLE: Documenting DeltaProtocolError in Delta-rs Python Library\nDESCRIPTION: Exception raised when there is an error related to the Delta protocol specification.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/exceptions.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.exceptions.DeltaProtocolError\n```\n\n----------------------------------------\n\nTITLE: Configuring DeltaLakePolarsIOManager in Dagster\nDESCRIPTION: Python code to configure the DeltaLakePolarsIOManager in a Dagster project for using Delta Lake with Polars.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_deltalake_polars import DeltaLakePolarsIOManager\n\ndefs = Definitions(\n   assets=all_assets,\n   resources={\n        \"io_manager\": DeltaLakePolarsIOManager(\n            root_uri=\"path/to/deltalake\",\n            storage_options=LocalConfig(),\n            schema=\"dagster_deltalake\",\n        ),\n   },\n)\n```\n\n----------------------------------------\n\nTITLE: Activating Virtual Environment\nDESCRIPTION: Activates the Python virtual environment created during setup, making the project's dependencies available in the current shell session.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource ./.venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Exploring Problem States with Stateright Visualization UI\nDESCRIPTION: Command to launch Stateright's exploration mode with visualization UI for debugging problematic states in delta-rs formal specifications. This provides an interactive way to analyze state issues.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/proofs/stateright/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release explore --worker-count 3\n```\n\n----------------------------------------\n\nTITLE: Cross-Compiling for manylinux2014\nDESCRIPTION: Demonstrates how to cross-compile wheels for manylinux2014 platform. This snippet shows the steps to add a Rust target and build wheels using maturin with Zig.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nrustup target add x86_64-unknown-linux-gnu\n```\n\nLANGUAGE: bash\nCODE:\n```\nuvx --from 'maturin[zig]' maturin build --release --zig \\\n    --target x86_64-unknown-linux-gnu \\\n    --compatibility manylinux2014 \\\n    --out wheels\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration Variables for S3DynamoDbLogStore\nDESCRIPTION: Environment variables required for configuring the new S3DynamoDbLogStore locking mechanism in Delta-RS v0.17.0. These settings specify the DynamoDB table and locking provider.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CHANGELOG.md#2025-04-16_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nAWS_S3_LOCKING_PROVIDER=dynamodb\nDELTA_DYNAMO_TABLE_NAME=<new-table-name>\n```\n\n----------------------------------------\n\nTITLE: Register AWS Handlers - Meta Crate\nDESCRIPTION: Code example showing how to register AWS handlers when using the deltalake meta crate\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CHANGELOG.md#2025-04-16_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\ndeltalake::aws::register_handlers(None);\n```\n\n----------------------------------------\n\nTITLE: Installing Package in Development Mode\nDESCRIPTION: Uses maturin via a make target to install the delta-rs package in development mode, allowing code changes to take effect without reinstallation.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake develop\n```\n\n----------------------------------------\n\nTITLE: Displaying a Daft DataFrame\nDESCRIPTION: Shows how to display a Daft DataFrame. Since Daft DataFrames are lazy, this only shows metadata without materializing the data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Installing Rust Compiler\nDESCRIPTION: Command to install the Rust compiler, which is a prerequisite for building custom wheels. This snippet shows how to download and run the Rustup installer script.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://sh.rustup.rs -sSf | sh -s\n```\n\n----------------------------------------\n\nTITLE: Using Custom S3 Storage Backend with Delta Tables in Python\nDESCRIPTION: Demonstrates how to use a custom S3 storage backend for reading Delta table data using pyarrow's filesystem factory.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs as fs\nfrom deltalake import DeltaTable\n\ntable_uri = \"s3://<bucket>/<path>\"\nraw_fs, normalized_path = fs.FileSystem.from_uri(table_uri)\nfilesystem = fs.SubTreeFileSystem(normalized_path, raw_fs)\n\ndt = DeltaTable(table_uri)\nds = dt.to_pyarrow_dataset(filesystem=filesystem)\n```\n\n----------------------------------------\n\nTITLE: Register AWS Handlers - Storage Crate\nDESCRIPTION: Code example showing how to register AWS handlers when using the deltalake-aws storage crate directly\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CHANGELOG.md#2025-04-16_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\ndeltalake_aws::register_handlers(None);\n```\n\n----------------------------------------\n\nTITLE: Listing Available Make Tasks\nDESCRIPTION: Shows all available make targets that can be used during development, likely displaying a help menu with descriptions for each task.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake help\n```\n\n----------------------------------------\n\nTITLE: Installing DeltaLakeIOManager in Python\nDESCRIPTION: Command to install the DeltaLakeIOManager package using pip.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-deltalake\n```\n\n----------------------------------------\n\nTITLE: Running Model Checking on Delta-rs Specs with Stateright\nDESCRIPTION: Command to execute model checking on the delta-rs formal specifications using Stateright with multiple worker threads. The '--release' flag ensures optimized performance for the checking process.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/proofs/stateright/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release check --worker-count 3\n```\n\n----------------------------------------\n\nTITLE: Using Custom Storage Backends with Delta Tables in Python\nDESCRIPTION: Shows how to use custom storage backends compatible with pyarrow.fs.FileSystem for reading Delta table data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/docs/source/usage.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs as fs\nfrom deltalake import DeltaTable\n\npath = \"<path/to/table>\"\nfilesystem = fs.SubTreeFileSystem(path, fs.LocalFileSystem())\n\ndt = DeltaTable(path)\nds = dt.to_pyarrow_dataset(filesystem=filesystem)\n```\n\n----------------------------------------\n\nTITLE: Installing Polars and deltalake packages in Python\nDESCRIPTION: Command to install the required Python packages Polars and deltalake using pip.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/s3-like.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install polars deltalake\n```\n\n----------------------------------------\n\nTITLE: Formatting Python Code\nDESCRIPTION: Runs code formatting tools to ensure the codebase adheres to project style guidelines, likely using tools like Black or isort.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake format\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda environment from YAML file\nDESCRIPTION: This command creates a Conda environment using the specifications defined in the 'deltalake-minimal.yml' file.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/installation.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda env create -f deltalake-minimal.yml\n```\n\n----------------------------------------\n\nTITLE: Installing DeltaLakePolarsIOManager in Python\nDESCRIPTION: Command to install the DeltaLakePolarsIOManager package for using Delta Lake with Polars in Dagster.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-deltalake-polars\n```\n\n----------------------------------------\n\nTITLE: Creating a sample DataFrame in Polars\nDESCRIPTION: This code creates a simple Polars DataFrame with a single column 'x' containing three values. It's used as sample data for the Delta Lake example.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/lakefs.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pl.DataFrame({'x': [1, 2, 3]})\n```\n\n----------------------------------------\n\nTITLE: Installing Dask-Deltatable for Python\nDESCRIPTION: Command to install the dask-deltatable library using pip, which enables the use of Delta Lake with Dask.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dask.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dask-deltatable\n```\n\n----------------------------------------\n\nTITLE: Checking Python Code Quality\nDESCRIPTION: Runs linting and type checking on the Python code to ensure quality standards are met, likely using tools like Flake8, Pylint, or MyPy.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake check-python\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda environment for Delta Lake and pandas\nDESCRIPTION: This YAML file defines a Conda environment named 'deltalake-minimal' with necessary dependencies for running Delta Lake and pandas in a Jupyter notebook.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/installation.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: deltalake-minimal\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.11\n  - ipykernel\n  - pandas\n  - polars\n  - jupyterlab\n  - deltalake\n```\n\n----------------------------------------\n\nTITLE: Comparing Benchmark Results Between Two Runs\nDESCRIPTION: This command compares the results of two different benchmark runs. It takes Delta table paths and group_ids for each run, and calculates the speedup for each test case.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/benchmarks/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release --bin merge -- compare data/benchmarks/ 1698636172801 data/benchmarks/ 1699759539902\n```\n\n----------------------------------------\n\nTITLE: Installing Polars and deltalake in Python\nDESCRIPTION: This command installs the Polars and deltalake libraries using pip, which are required for working with Delta Lake and LakeFS in Python.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/object-storage/lakefs.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install polars deltalake\n```\n\n----------------------------------------\n\nTITLE: Displaying All Benchmark Results from Delta Table\nDESCRIPTION: This command shows all benchmark results stored in a Delta table, providing a comprehensive view of the collected data.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/benchmarks/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release --bin merge -- show data/benchmark\n```\n\n----------------------------------------\n\nTITLE: Installing DeltaLakePandasIOManager in Python\nDESCRIPTION: Command to install the DeltaLakePandasIOManager package for using Delta Lake with Pandas in Dagster.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-dagster.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-deltalake-pandas\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Executes the project's unit test suite to verify functionality, likely using pytest or a similar testing framework.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/CONTRIBUTING.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake unit-test\n```\n\n----------------------------------------\n\nTITLE: Configuring MkDocs Search Boost for DeltaTable Documentation\nDESCRIPTION: This YAML configuration increases the search relevance of the DeltaTable documentation page in MkDocs. It sets a boost value of 2 for improved visibility in search results.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/delta_table/index.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsearch:\n  boost: 2\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Daft via Environment Variable\nDESCRIPTION: Sets an environment variable to disable telemetry collection in Daft. This allows users to opt out of sending non-identifiable metadata used for product improvement.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nDAFT_ANALYTICS_ENABLED=0\n```\n\n----------------------------------------\n\nTITLE: Error Example - Invalid S3 Scheme\nDESCRIPTION: Example error message when AWS S3 handlers are not properly registered in Delta Lake Rust\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CHANGELOG.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nthread 'main' panicked at /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/deltalake-core-0.17.0/src/table/builder.rs:189:48:\nThe specified table_uri is not valid: InvalidTableLocation(\"Unknown scheme: s3\")\n```\n\n----------------------------------------\n\nTITLE: Pre-Pull Request Checks for delta-rs Project\nDESCRIPTION: Series of commands to run checks and tests before submitting a pull request.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CONTRIBUTING.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncargo fmt -- --check\ncd python\nmake check-rust\nmake check-python\nmake develop\nmake unit-test\nmake build-docs\n```\n\n----------------------------------------\n\nTITLE: Function Reference for MIT License\nDESCRIPTION: Identifies the specific function '_convert_pa_schema_to_delta' in deltalake/schema.py that is covered under the MIT License, while the rest of the package uses Apache 2.0 License.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/python/licenses/README.md#2025-04-16_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n_convert_pa_schema_to_delta\n```\n\n----------------------------------------\n\nTITLE: Documenting SchemaMismatchError in Delta-rs Python Library\nDESCRIPTION: Exception raised when there is a mismatch between expected and actual schema during Delta table operations.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/exceptions.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.exceptions.SchemaMismatchError\n```\n\n----------------------------------------\n\nTITLE: Running Delta Table Example\nDESCRIPTION: Command to run the read_delta_table example from the examples directory.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/crates/core/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --example read_delta_table\n```\n\n----------------------------------------\n\nTITLE: Running Rust Examples for delta-rs Project\nDESCRIPTION: Command to run a Rust example with specific features enabled using cargo.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ../crates/deltalake\ncargo run --example basic_operations --features=\"datafusion\"\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with Delta Lake Support using pip\nDESCRIPTION: Installs the Daft library with Delta Lake integration support using pip. This adds the deltalake Python package needed to fetch Delta Lake table metadata.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/integrations/delta-lake-daft.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U \"getdaft[deltalake]\"\n```\n\n----------------------------------------\n\nTITLE: Data Type Class References\nDESCRIPTION: References to classes representing different data types supported in Delta schemas\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/schema.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndeltalake.schema.PrimitiveType\ndeltalake.schema.ArrayType\ndeltalake.schema.MapType\ndeltalake.schema.StructType\n```\n\n----------------------------------------\n\nTITLE: Version Header Markdown for Delta-RS v0.20.0\nDESCRIPTION: Markdown header formatting for version rust-v0.20.0 release with date and comparison link\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CHANGELOG.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## [rust-v0.20.0](https://github.com/delta-io/delta-rs/tree/rust-v0.20.0) (2024-09-18)\n\n[Full Changelog](https://github.com/delta-io/delta-rs/compare/rust-v0.19.1...rust-v0.20.0)\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Search Boost\nDESCRIPTION: YAML frontmatter configuration that sets a search boost value of 2 for this documentation page.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/storage.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsearch:\n  boost: 2\n```\n\n----------------------------------------\n\nTITLE: Version Header Markdown for Delta-RS v0.20.1\nDESCRIPTION: Markdown header formatting for version rust-v0.20.1 release with date and comparison link\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CHANGELOG.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## [rust-v0.20.1](https://github.com/delta-io/delta-rs/tree/rust-v0.20.1) (2024-09-27)\n\n[Full Changelog](https://github.com/delta-io/delta-rs/compare/rust-v0.20.0...rust-v0.20.1)\n```\n\n----------------------------------------\n\nTITLE: VSCode Debug Configuration for Python-to-Rust Debugging\nDESCRIPTION: JSON configuration for setting up LLDB debugger in VSCode to debug Rust code from Python.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/CONTRIBUTING.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n            \"type\": \"lldb\",\n            \"request\": \"attach\",\n            \"name\": \"LLDB Attach to Python'\",\n            \"program\": \"${command:python.interpreterPath}\",\n            \"pid\": \"${command:pickMyProcess}\",\n            \"args\": [],\n            \"stopOnEntry\": false,\n            \"environment\": [],\n            \"externalConsole\": true,\n            \"MIMode\": \"lldb\",\n            \"cwd\": \"${workspaceFolder}\"\n        }\n```\n\n----------------------------------------\n\nTITLE: Configuring TableAlterer Documentation Display\nDESCRIPTION: Documentation directive that configures how the TableAlterer class documentation should be displayed, with the option to show the root heading.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/delta_table/delta_table_alterer.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndeltalake.table.TableAlterer\n    options:\n        show_root_heading: true\n```\n\n----------------------------------------\n\nTITLE: Rendering DeltaTable Documentation in MkDocs\nDESCRIPTION: This MkDocs-specific directive instructs the documentation generator to render the full API documentation for the DeltaTable class from the deltalake module. It includes the root heading in the output.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/delta_table/index.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n::: deltalake.DeltaTable\n    options:\n        show_root_heading: true\n```\n\n----------------------------------------\n\nTITLE: Executing Delta Lake Query and Retrieving Results in Python\nDESCRIPTION: Executes the constructed query and returns the results as a list of dictionaries, where each dictionary represents a row with column names as keys.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/api/query.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncollect() -> List[Dict[str, Any]]\n```\n\n----------------------------------------\n\nTITLE: Linking to Spark Delta Lake Documentation\nDESCRIPTION: Provides a hyperlink to the official Spark Delta Lake documentation for users working with Delta Lake in a Spark environment.\nSOURCE: https://github.com/delta-io/delta-rs/blob/main/docs/usage/overview.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[Spark Delta Lake documentation](https://docs.delta.io/latest/index.html)\n```"
  }
]
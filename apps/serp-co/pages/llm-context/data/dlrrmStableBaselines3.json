[
  {
    "owner": "dlr-rm",
    "repo": "stable-baselines3",
    "content": "TITLE: Implementing a Custom Network Architecture for Actor-Critic Policy in PyTorch\nDESCRIPTION: This code demonstrates how to create a custom network architecture for policy and value functions in Stable Baselines3. It defines a CustomNetwork class with separate policy and value networks, and a CustomActorCriticPolicy class that uses this network.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_policy.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable, Dict, List, Optional, Tuple, Type, Union\n\nfrom gymnasium import spaces\nimport torch as th\nfrom torch import nn\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.policies import ActorCriticPolicy\n\n\nclass CustomNetwork(nn.Module):\n    \"\"\"\n    Custom network for policy and value function.\n    It receives as input the features extracted by the features extractor.\n\n    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_dim: int,\n        last_layer_dim_pi: int = 64,\n        last_layer_dim_vf: int = 64,\n    ):\n        super().__init__()\n\n        # IMPORTANT:\n        # Save output dimensions, used to create the distributions\n        self.latent_dim_pi = last_layer_dim_pi\n        self.latent_dim_vf = last_layer_dim_vf\n\n        # Policy network\n        self.policy_net = nn.Sequential(\n            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n        )\n        # Value network\n        self.value_net = nn.Sequential(\n            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n        )\n\n    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        \"\"\"\n        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n            If all layers are shared, then ``latent_policy == latent_value``\n        \"\"\"\n        return self.forward_actor(features), self.forward_critic(features)\n\n    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n        return self.policy_net(features)\n\n    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n        return self.value_net(features)\n\n\nclass CustomActorCriticPolicy(ActorCriticPolicy):\n    def __init__(\n        self,\n        observation_space: spaces.Space,\n        action_space: spaces.Space,\n        lr_schedule: Callable[[float], float],\n        *args,\n        **kwargs,\n    ):\n        # Disable orthogonal initialization\n        kwargs[\"ortho_init\"] = False\n        super().__init__(\n            observation_space,\n            action_space,\n            lr_schedule,\n            # Pass remaining arguments to base class\n            *args,\n            **kwargs,\n        )\n\n\n    def _build_mlp_extractor(self) -> None:\n        self.mlp_extractor = CustomNetwork(self.features_dim)\n\n\nmodel = PPO(CustomActorCriticPolicy, \"CartPole-v1\", verbose=1)\nmodel.learn(5000)\n```\n\n----------------------------------------\n\nTITLE: Training and Running PPO on CartPole Environment in Python\nDESCRIPTION: Example of how to train and run the Proximal Policy Optimization (PPO) algorithm on a CartPole environment. The code initializes the environment, creates a PPO model with a MLP policy, trains it, and then runs the trained model for 1000 steps with visualization.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nvec_env = model.get_env()\nobs = vec_env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = vec_env.step(action)\n    vec_env.render()\n    # VecEnv resets automatically\n    # if done:\n    #   obs = env.reset()\n\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Training and Running A2C on CartPole Environment with Stable-Baselines3\nDESCRIPTION: This code demonstrates how to train an A2C (Advantage Actor-Critic) model on the CartPole environment and then use the trained model to make predictions. It shows the complete workflow including environment creation, model initialization, training, and inference steps.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/quickstart.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import A2C\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n\nmodel = A2C(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nvec_env = model.get_env()\nobs = vec_env.reset()\nfor i in range(1000):\n    action, _state = model.predict(obs, deterministic=True)\n    obs, reward, done, info = vec_env.step(action)\n    vec_env.render(\"human\")\n    # VecEnv resets automatically\n    # if done:\n    #   obs = vec_env.reset()\n```\n\n----------------------------------------\n\nTITLE: Implementing CustomEnv Class for Stable Baselines 3 in Python\nDESCRIPTION: This class demonstrates how to create a custom environment that follows the Gymnasium interface, compatible with Stable Baselines 3. It includes the required methods and space definitions for actions and observations.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_env.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\nfrom gymnasium import spaces\n\n\nclass CustomEnv(gym.Env):\n    \"\"\"Custom Environment that follows gym interface.\"\"\"\n\n    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n\n    def __init__(self, arg1, arg2, ...):\n        super().__init__()\n        # Define action and observation space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n        # Example for using image as input (channel-first; channel-last also works):\n        self.observation_space = spaces.Box(low=0, high=255,\n                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n\n    def step(self, action):\n        ...\n        return observation, reward, terminated, truncated, info\n\n    def reset(self, seed=None, options=None):\n        ...\n        return observation, info\n\n    def render(self):\n        ...\n\n    def close(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: One-line A2C Training on Registered Environment\nDESCRIPTION: A simplified approach to train an A2C model on a registered Gymnasium environment in a single line of code. This works when both the environment (CartPole-v1) and policy (MlpPolicy) are properly registered.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/quickstart.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\n\nmodel = A2C(\"MlpPolicy\", \"CartPole-v1\").learn(10000)\n```\n\n----------------------------------------\n\nTITLE: Implementing HER with DQN, DDPG, SAC or TD3 in Stable Baselines3\nDESCRIPTION: This code snippet demonstrates how to use HerReplayBuffer with off-policy algorithms in Stable Baselines3. It shows initialization, training, saving, loading a model with HER, and running predictions in a BitFlippingEnv. The example showcases the key parameters for HER, including goal selection strategy.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/her.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import HerReplayBuffer, DDPG, DQN, SAC, TD3\nfrom stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\nfrom stable_baselines3.common.envs import BitFlippingEnv\n\nmodel_class = DQN  # works also with SAC, DDPG and TD3\nN_BITS = 15\n\nenv = BitFlippingEnv(n_bits=N_BITS, continuous=model_class in [DDPG, SAC, TD3], max_steps=N_BITS)\n\n# Available strategies (cf paper): future, final, episode\ngoal_selection_strategy = \"future\" # equivalent to GoalSelectionStrategy.FUTURE\n\n# Initialize the model\nmodel = model_class(\n    \"MultiInputPolicy\",\n    env,\n    replay_buffer_class=HerReplayBuffer,\n    # Parameters for HER\n    replay_buffer_kwargs=dict(\n        n_sampled_goal=4,\n        goal_selection_strategy=goal_selection_strategy,\n    ),\n    verbose=1,\n)\n\n# Train the model\nmodel.learn(1000)\n\nmodel.save(\"./her_bit_env\")\n# Because it needs access to `env.compute_reward()`\n# HER must be loaded with the env\nmodel = model_class.load(\"./her_bit_env\", env=env)\n\nobs, info = env.reset()\nfor _ in range(100):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, _ = env.step(action)\n    if terminated or truncated:\n        obs, info = env.reset()\n```\n\n----------------------------------------\n\nTITLE: Training, Saving and Loading a DQN Model in Python\nDESCRIPTION: This snippet demonstrates how to train a DQN model on the LunarLander environment, save it, load it back, evaluate its performance, and run the trained agent. It uses the Stable Baselines3 library and Gymnasium.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n\n# Create environment\nenv = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n\n# Instantiate the agent\nmodel = DQN(\"MlpPolicy\", env, verbose=1)\n# Train the agent and display a progress bar\nmodel.learn(total_timesteps=int(2e5), progress_bar=True)\n# Save the agent\nmodel.save(\"dqn_lunar\")\ndel model  # delete trained model to demonstrate loading\n\n# Load the trained agent\n# NOTE: if you have loading issue, you can pass `print_system_info=True`\n# to compare the system on which the model was trained vs the current one\n# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\nmodel = DQN.load(\"dqn_lunar\", env=env)\n\n# Evaluate the agent\n# NOTE: If you use wrappers with your environment that modify rewards,\n#       this will be reflected here. To evaluate with original rewards,\n#       wrap environment in a \"Monitor\" wrapper before other wrappers.\nmean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n\n# Enjoy trained agent\nvec_env = model.get_env()\nobs = vec_env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, rewards, dones, info = vec_env.step(action)\n    vec_env.render(\"human\")\n```\n\n----------------------------------------\n\nTITLE: Training PPO Agent on CartPole Environment in Python\nDESCRIPTION: This snippet demonstrates how to train a PPO agent on the CartPole-v1 environment using 4 parallel environments. It includes creating the environment, initializing the model, training, saving, loading, and running the trained agent.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/ppo.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Parallel environments\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n\nmodel = PPO(\"MlpPolicy\", vec_env, verbose=1)\nmodel.learn(total_timesteps=25000)\nmodel.save(\"ppo_cartpole\")\n\ndel model # remove to demonstrate saving and loading\n\nmodel = PPO.load(\"ppo_cartpole\")\n\nobs = vec_env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = vec_env.step(action)\n    vec_env.render(\"human\")\n```\n\n----------------------------------------\n\nTITLE: Setting up and Training TD3 Agent with Custom Callback for LunarLander\nDESCRIPTION: Creates a TD3 reinforcement learning agent for the LunarLanderContinuous-v2 environment with action noise, monitors the training process, and saves the best model using the custom callback.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create log dir\nlog_dir = \"tmp/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Create and wrap the environment\nenv = gym.make(\"LunarLanderContinuous-v2\")\nenv = Monitor(env, log_dir)\n\n# Add some action noise for exploration\nn_actions = env.action_space.shape[-1]\naction_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n# Because we use parameter noise, we should use a MlpPolicy with layer normalization\nmodel = TD3(\"MlpPolicy\", env, action_noise=action_noise, verbose=0)\n# Create the callback: check every 1000 steps\ncallback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n# Train the agent\ntimesteps = 1e5\nmodel.learn(total_timesteps=int(timesteps), callback=callback)\n\nplot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"TD3 LunarLander\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Training A2C with Vectorized Atari Environments in Stable Baselines3\nDESCRIPTION: Demonstrates how to train an A2C agent on Atari games using the make_atari_env helper function. It handles preprocessing, frame stacking, and multi-worker training to efficiently learn Atari gameplay.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\nfrom stable_baselines3 import A2C\n\nimport ale_py\n\n# There already exists an environment generator\n# that will make and wrap atari environments correctly.\n# Here we are also multi-worker training (n_envs=4 => 4 environments)\nvec_env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=4, seed=0)\n# Frame-stacking with 4 frames\nvec_env = VecFrameStack(vec_env, n_stack=4)\n\nmodel = A2C(\"CnnPolicy\", vec_env, verbose=1)\nmodel.learn(total_timesteps=25_000)\n\nobs = vec_env.reset()\nwhile True:\n    action, _states = model.predict(obs, deterministic=False)\n    obs, rewards, dones, info = vec_env.step(action)\n    vec_env.render(\"human\")\n```\n\n----------------------------------------\n\nTITLE: Implementing SaveOnBestTrainingRewardCallback in Python for Stable Baselines3\nDESCRIPTION: A custom callback class that saves the model when it achieves the best mean training reward. It checks performance at specified intervals, loads training results, and saves the best model to disk.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n    super().__init__(verbose)\n    self.check_freq = check_freq\n    self.log_dir = log_dir\n    self.save_path = os.path.join(log_dir, \"best_model\")\n    self.best_mean_reward = -np.inf\n\ndef _init_callback(self) -> None:\n    # Create folder if needed\n    if self.save_path is not None:\n        os.makedirs(self.save_path, exist_ok=True)\n\ndef _on_step(self) -> bool:\n    if self.n_calls % self.check_freq == 0:\n\n      # Retrieve training reward\n      x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n      if len(x) > 0:\n          # Mean training reward over the last 100 episodes\n          mean_reward = np.mean(y[-100:])\n          if self.verbose >= 1:\n            print(f\"Num timesteps: {self.num_timesteps}\")\n            print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n\n          # New best model, you could save the agent here\n          if mean_reward > self.best_mean_reward:\n              self.best_mean_reward = mean_reward\n              # Example for saving best model\n              if self.verbose >= 1:\n                print(f\"Saving new best model to {self.save_path}\")\n              self.model.save(self.save_path)\n\n    return True\n```\n\n----------------------------------------\n\nTITLE: Implementing TD3 with Action Noise for Pendulum Environment in Python\nDESCRIPTION: Example demonstrating how to use TD3 algorithm from Stable Baselines3 to train a reinforcement learning agent on the Pendulum environment. It shows initialization with action noise, training, saving/loading the model, and running inference.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/td3.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom stable_baselines3 import TD3\nfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\nenv = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n\n# The noise objects for TD3\nn_actions = env.action_space.shape[-1]\naction_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n\nmodel = TD3(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\nmodel.learn(total_timesteps=10000, log_interval=10)\nmodel.save(\"td3_pendulum\")\nvec_env = model.get_env()\n\ndel model # remove to demonstrate saving and loading\n\nmodel = TD3.load(\"td3_pendulum\")\n\nobs = vec_env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = vec_env.step(action)\n    vec_env.render(\"human\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Feature Extractor for PPO (Python)\nDESCRIPTION: This code defines a custom CNN feature extractor for use with PPO in Stable Baselines3. It processes image observations through convolutional layers followed by a linear layer to extract features.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_policy.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch as th\nimport torch.nn as nn\nfrom gymnasium import spaces\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n\nclass CustomCNN(BaseFeaturesExtractor):\n    \"\"\"\n    :param observation_space: (gym.Space)\n    :param features_dim: (int) Number of features extracted.\n        This corresponds to the number of unit for the last layer.\n    \"\"\"\n\n    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n        super().__init__(observation_space, features_dim)\n        # We assume CxHxW images (channels first)\n        # Re-ordering will be done by pre-preprocessing or wrapper\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n    features_extractor_kwargs=dict(features_dim=128),\n)\nmodel = PPO(\"CnnPolicy\", \"BreakoutNoFrameskip-v4\", policy_kwargs=policy_kwargs, verbose=1)\nmodel.learn(1000)\n```\n\n----------------------------------------\n\nTITLE: Loading Models from Hugging Face Hub\nDESCRIPTION: Example of loading a pre-trained model from Hugging Face Hub and evaluating it on CartPole environment. Includes environment setup and policy evaluation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/integrations.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport gymnasium as gym\n\nfrom huggingface_sb3 import load_from_hub\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n\n# Allow the use of `pickle.load()` when downloading model from the hub\n# Please make sure that the organization from which you download can be trusted\nos.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n\n# Retrieve the model from the hub\n## repo_id = id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})\n## filename = name of the model zip file from the repository\ncheckpoint = load_from_hub(\n    repo_id=\"sb3/demo-hf-CartPole-v1\",\n    filename=\"ppo-CartPole-v1.zip\",\n)\nmodel = PPO.load(checkpoint)\n\n# Evaluate the agent and watch it\neval_env = gym.make(\"CartPole-v1\")\nmean_reward, std_reward = evaluate_policy(\n    model, eval_env, render=True, n_eval_episodes=5, deterministic=True, warn=False\n)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```\n\n----------------------------------------\n\nTITLE: Training A2C Agent on CartPole with Multiple Environments\nDESCRIPTION: Example showing how to train an A2C agent on the CartPole-v1 environment using 4 parallel environments, save the model, load it back, and run inference with rendering.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/a2c.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Parallel environments\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n\nmodel = A2C(\"MlpPolicy\", vec_env, verbose=1)\nmodel.learn(total_timesteps=25000)\nmodel.save(\"a2c_cartpole\")\n\ndel model # remove to demonstrate saving and loading\n\nmodel = A2C.load(\"a2c_cartpole\")\n\nobs = vec_env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = vec_env.step(action)\n    vec_env.render(\"human\")\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing with Vectorized Environments in Python\nDESCRIPTION: This example shows how to use multiprocessing with vectorized environments in Stable Baselines3. It creates multiple parallel environments using SubprocVecEnv and trains a PPO agent on them.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.utils import set_random_seed\n\ndef make_env(env_id: str, rank: int, seed: int = 0):\n    \"\"\"\n    Utility function for multiprocessed env.\n\n    :param env_id: the environment ID\n    :param num_env: the number of environments you wish to have in subprocesses\n    :param seed: the initial seed for RNG\n    :param rank: index of the subprocess\n    \"\"\"\n    def _init():\n        env = gym.make(env_id, render_mode=\"human\")\n        env.reset(seed=seed + rank)\n        return env\n    set_random_seed(seed)\n    return _init\n\nif __name__ == \"__main__\":\n    env_id = \"CartPole-v1\"\n    num_cpu = 4  # Number of processes to use\n    # Create the vectorized environment\n    vec_env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n\n    # Stable Baselines provides you with make_vec_env() helper\n    # which does exactly the previous steps for you.\n    # You can choose between `DummyVecEnv` (usually faster) and `SubprocVecEnv`\n    # env = make_vec_env(env_id, n_envs=num_cpu, seed=0, vec_env_cls=SubprocVecEnv)\n\n    model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n    model.learn(total_timesteps=25_000)\n\n    obs = vec_env.reset()\n    for _ in range(1000):\n        action, _states = model.predict(obs)\n        obs, rewards, dones, info = vec_env.step(action)\n        vec_env.render()\n```\n\n----------------------------------------\n\nTITLE: Logging Agent Videos to Tensorboard with Custom Callback\nDESCRIPTION: Comprehensive example showing how to record agent behavior videos and log them to Tensorboard. This implementation captures screens during policy evaluation and converts them to video format.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\nimport gymnasium as gym\nimport torch as th\nimport numpy as np\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.logger import Video\n\n\nclass VideoRecorderCallback(BaseCallback):\n    def __init__(self, eval_env: gym.Env, render_freq: int, n_eval_episodes: int = 1, deterministic: bool = True):\n        \"\"\"\n        Records a video of an agent's trajectory traversing ``eval_env`` and logs it to TensorBoard\n\n        :param eval_env: A gym environment from which the trajectory is recorded\n        :param render_freq: Render the agent's trajectory every eval_freq call of the callback.\n        :param n_eval_episodes: Number of episodes to render\n        :param deterministic: Whether to use deterministic or stochastic policy\n        \"\"\"\n        super().__init__()\n        self._eval_env = eval_env\n        self._render_freq = render_freq\n        self._n_eval_episodes = n_eval_episodes\n        self._deterministic = deterministic\n\n    def _on_step(self) -> bool:\n        if self.n_calls % self._render_freq == 0:\n            screens = []\n\n            def grab_screens(_locals: Dict[str, Any], _globals: Dict[str, Any]) -> None:\n                \"\"\"\n                Renders the environment in its current state, recording the screen in the captured `screens` list\n\n                :param _locals: A dictionary containing all local variables of the callback's scope\n                :param _globals: A dictionary containing all global variables of the callback's scope\n                \"\"\"\n                # We expect `render()` to return a uint8 array with values in [0, 255] or a float array\n                # with values in [0, 1], as described in\n                # https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_video\n                screen = self._eval_env.render(mode=\"rgb_array\")\n                # PyTorch uses CxHxW vs HxWxC gym (and tensorflow) image convention\n                screens.append(screen.transpose(2, 0, 1))\n\n            evaluate_policy(\n                self.model,\n                self._eval_env,\n                callback=grab_screens,\n                n_eval_episodes=self._n_eval_episodes,\n                deterministic=self._deterministic,\n            )\n            self.logger.record(\n                \"trajectory/video\",\n                Video(th.from_numpy(np.asarray([screens])), fps=40),\n                exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n            )\n        return True\n\n\nmodel = A2C(\"MlpPolicy\", \"CartPole-v1\", tensorboard_log=\"runs/\", verbose=1)\nvideo_recorder = VideoRecorderCallback(gym.make(\"CartPole-v1\"), render_freq=5000)\nmodel.learn(total_timesteps=int(5e4), callback=video_recorder)\n```\n\n----------------------------------------\n\nTITLE: Implementing DDPG with Gymnasium Environment\nDESCRIPTION: Example demonstrating how to train a DDPG agent on the Pendulum-v1 environment using Stable Baselines3. Shows environment setup, noise configuration, model training, saving/loading, and inference.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/ddpg.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom stable_baselines3 import DDPG\nfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\nenv = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n\n# The noise objects for DDPG\nn_actions = env.action_space.shape[-1]\naction_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n\nmodel = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\nmodel.learn(total_timesteps=10000, log_interval=10)\nmodel.save(\"ddpg_pendulum\")\nvec_env = model.get_env()\n\ndel model # remove to demonstrate saving and loading\n\nmodel = DDPG.load(\"ddpg_pendulum\")\n\nobs = vec_env.reset()\nwhile True:\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = vec_env.step(action)\n    env.render(\"human\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Callback in Python for Stable Baselines 3\nDESCRIPTION: Example implementation of a custom callback class that extends BaseCallback. This template shows the structure with key lifecycle methods that are called at different stages of training, including training start, rollout start/end, each step, and training end.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.callbacks import BaseCallback\n\n\nclass CustomCallback(BaseCallback):\n    \"\"\"\n    A custom callback that derives from ``BaseCallback``.\n\n    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n    \"\"\"\n    def __init__(self, verbose: int = 0):\n        super().__init__(verbose)\n        # Those variables will be accessible in the callback\n        # (they are defined in the base class)\n        # The RL model\n        # self.model = None  # type: BaseAlgorithm\n        # An alias for self.model.get_env(), the environment used for training\n        # self.training_env # type: VecEnv\n        # Number of time the callback was called\n        # self.n_calls = 0  # type: int\n        # num_timesteps = n_envs * n times env.step() was called\n        # self.num_timesteps = 0  # type: int\n        # local and global variables\n        # self.locals = {}  # type: Dict[str, Any]\n        # self.globals = {}  # type: Dict[str, Any]\n        # The logger object, used to report things in the terminal\n        # self.logger # type: stable_baselines3.common.logger.Logger\n        # Sometimes, for event callback, it is useful\n        # to have access to the parent object\n        # self.parent = None  # type: Optional[BaseCallback]\n\n    def _on_training_start(self) -> None:\n        \"\"\"\n        This method is called before the first rollout starts.\n        \"\"\"\n        pass\n\n    def _on_rollout_start(self) -> None:\n        \"\"\"\n        A rollout is the collection of environment interaction\n        using the current policy.\n        This event is triggered before collecting new samples.\n        \"\"\"\n        pass\n\n    def _on_step(self) -> bool:\n        \"\"\"\n        This method will be called by the model after each call to `env.step()`.\n\n        For child callback (of an `EventCallback`), this will be called\n        when the event is triggered.\n\n        :return: If the callback returns False, training is aborted early.\n        \"\"\"\n        return True\n\n    def _on_rollout_end(self) -> None:\n        \"\"\"\n        This event is triggered before updating the policy.\n        \"\"\"\n        pass\n\n    def _on_training_end(self) -> None:\n        \"\"\"\n        This event is triggered before exiting the `learn()` method.\n        \"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Training DQN Agent with CartPole Environment in Python\nDESCRIPTION: Example demonstrating how to create, train, save, load and run a DQN agent on the CartPole environment using stable-baselines3. The example shows basic usage of the DQN implementation with an MLP policy.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/dqn.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import DQN\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\nmodel = DQN(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10000, log_interval=4)\nmodel.save(\"dqn_cartpole\")\n\ndel model # remove to demonstrate saving and loading\n\nmodel = DQN.load(\"dqn_cartpole\")\n\nobs, info = env.reset()\nwhile True:\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        obs, info = env.reset()\n```\n\n----------------------------------------\n\nTITLE: Implementing Hindsight Experience Replay (HER) with SAC for Goal-Conditioned Tasks\nDESCRIPTION: Demonstrates how to use Hindsight Experience Replay (HER) with SAC for the highway-parking-v0 environment. HER improves sample efficiency by creating artificial transitions with alternative goals to help learn from sparse rewards.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport highway_env\nimport numpy as np\n\nfrom stable_baselines3 import HerReplayBuffer, SAC, DDPG, TD3\nfrom stable_baselines3.common.noise import NormalActionNoise\n\nenv = gym.make(\"parking-v0\")\n\n# Create 4 artificial transitions per real transition\nn_sampled_goal = 4\n\n# SAC hyperparams:\nmodel = SAC(\n    \"MultiInputPolicy\",\n    env,\n    replay_buffer_class=HerReplayBuffer,\n    replay_buffer_kwargs=dict(\n      n_sampled_goal=n_sampled_goal,\n      goal_selection_strategy=\"future\",\n    ),\n    verbose=1,\n    buffer_size=int(1e6),\n    learning_rate=1e-3,\n    gamma=0.95,\n    batch_size=256,\n    policy_kwargs=dict(net_arch=[256, 256, 256]),\n)\n\nmodel.learn(int(2e5))\nmodel.save(\"her_sac_highway\")\n\n# Load saved model\n# Because it needs access to `env.compute_reward()`\n# HER must be loaded with the env\nenv = gym.make(\"parking-v0\", render_mode=\"human\") # Change the render mode\n```\n\n----------------------------------------\n\nTITLE: Advanced Saving and Loading of SAC Models and Replay Buffers in Python\nDESCRIPTION: This example shows how to save and load a Soft Actor-Critic (SAC) model, its policy, and replay buffer independently. It also demonstrates how to evaluate the saved policy and load it back for further use.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.sac.policies import MlpPolicy\n\n# Create the model and the training environment\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1,\n            learning_rate=1e-3)\n\n# train the model\nmodel.learn(total_timesteps=6000)\n\n# save the model\nmodel.save(\"sac_pendulum\")\n\n# the saved model does not contain the replay buffer\nloaded_model = SAC.load(\"sac_pendulum\")\nprint(f\"The loaded_model has {loaded_model.replay_buffer.size()} transitions in its buffer\")\n\n# now save the replay buffer too\nmodel.save_replay_buffer(\"sac_replay_buffer\")\n\n# load it into the loaded_model\nloaded_model.load_replay_buffer(\"sac_replay_buffer\")\n\n# now the loaded replay is not empty anymore\nprint(f\"The loaded_model has {loaded_model.replay_buffer.size()} transitions in its buffer\")\n\n# Save the policy independently from the model\n# Note: if you don't save the complete model with `model.save()`\n# you cannot continue training afterward\npolicy = model.policy\npolicy.save(\"sac_policy_pendulum\")\n\n# Retrieve the environment\nenv = model.get_env()\n\n# Evaluate the policy\nmean_reward, std_reward = evaluate_policy(policy, env, n_eval_episodes=10, deterministic=True)\n\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n\n# Load the policy independently from the model\nsaved_policy = MlpPolicy.load(\"sac_policy_pendulum\")\n\n# Evaluate the loaded policy\nmean_reward, std_reward = evaluate_policy(saved_policy, env, n_eval_episodes=10, deterministic=True)\n\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```\n\n----------------------------------------\n\nTITLE: Using SAC Algorithm with Pendulum Environment in Stable Baselines3\nDESCRIPTION: This code snippet demonstrates how to use the SAC algorithm from Stable Baselines3 to train an agent on the Pendulum environment, save the trained model, load it back, and run inference with it.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/sac.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\n\nenv = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n\nmodel = SAC(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10000, log_interval=4)\nmodel.save(\"sac_pendulum\")\n\ndel model # remove to demonstrate saving and loading\n\nmodel = SAC.load(\"sac_pendulum\")\n\nobs, info = env.reset()\nwhile True:\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        obs, info = env.reset()\n```\n\n----------------------------------------\n\nTITLE: Normalizing Features with VecNormalize for PyBullet Environments\nDESCRIPTION: Shows how to use VecNormalize wrapper with PyBullet environments to compute running averages and standard deviations of input features. This normalization technique is essential for successful training in continuous control tasks.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport pybullet_envs_gymnasium\n\nfrom stable_baselines3.common.vec_env import VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3 import PPO\n\n# Alternatively, you can use the MuJoCo equivalent \"HalfCheetah-v4\"\nvec_env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\n# Automatically normalize the input features and reward\nvec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n\nmodel = PPO(\"MlpPolicy\", vec_env)\nmodel.learn(total_timesteps=2000)\n\n# Don't forget to save the VecNormalize statistics when saving the agent\nlog_dir = Path(\"/tmp/\")\nmodel.save(log_dir / \"ppo_halfcheetah\")\nstats_path = log_dir / \"vec_normalize.pkl\"\nvec_env.save(stats_path)\n\n# To demonstrate loading\ndel model, vec_env\n\n# Load the saved statistics\nvec_env = make_vec_env(\"HalfCheetahBulletEnv-v0\", n_envs=1)\nvec_env = VecNormalize.load(stats_path, vec_env)\n#  do not update them at test time\nvec_env.training = False\n# reward normalization is not needed at test time\nvec_env.norm_reward = False\n\n# Load the agent\nmodel = PPO.load(log_dir / \"ppo_halfcheetah\", env=vec_env)\n```\n\n----------------------------------------\n\nTITLE: Exporting PPO Model to ONNX Format\nDESCRIPTION: Demonstrates how to export a trained PPO policy to ONNX format for inference. The code includes policy class definition, model loading, ONNX export, and validation using ONNX Runtime.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/export.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch as th\nfrom typing import Tuple\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.policies import BasePolicy\n\n\nclass OnnxableSB3Policy(th.nn.Module):\n    def __init__(self, policy: BasePolicy):\n        super().__init__()\n        self.policy = policy\n\n    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n        return self.policy(observation, deterministic=True)\n\n\nPPO(\"MlpPolicy\", \"Pendulum-v1\").save(\"PathToTrainedModel\")\nmodel = PPO.load(\"PathToTrainedModel.zip\", device=\"cpu\")\n\nonnx_policy = OnnxableSB3Policy(model.policy)\n\nobservation_size = model.observation_space.shape\ndummy_input = th.randn(1, *observation_size)\nth.onnx.export(\n    onnx_policy,\n    dummy_input,\n    \"my_ppo_model.onnx\",\n    opset_version=17,\n    input_names=[\"input\"],\n)\n\nimport onnx\nimport onnxruntime as ort\nimport numpy as np\n\nonnx_path = \"my_ppo_model.onnx\"\nonnx_model = onnx.load(onnx_path)\nonnx.checker.check_model(onnx_model)\n\nobservation = np.zeros((1, *observation_size)).astype(np.float32)\nort_sess = ort.InferenceSession(onnx_path)\nactions, values, log_prob = ort_sess.run(None, {\"input\": observation})\n\nprint(actions, values, log_prob)\n\nwith th.no_grad():\n    print(model.policy(th.as_tensor(observation), deterministic=True))\n```\n\n----------------------------------------\n\nTITLE: Creating GIF of Trained A2C Agent in LunarLander Environment\nDESCRIPTION: Generates a GIF animation of a trained A2C agent performing in the LunarLander-v2 environment. The script trains the agent for 100,000 steps, then captures frames of the agent's performance and saves them as a GIF using imageio.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport imageio\nimport numpy as np\n\nfrom stable_baselines3 import A2C\n\nmodel = A2C(\"MlpPolicy\", \"LunarLander-v2\").learn(100_000)\n\nimages = []\nobs = model.env.reset()\nimg = model.env.render(mode=\"rgb_array\")\nfor i in range(350):\n    images.append(img)\n    action, _ = model.predict(obs)\n    obs, _, _ ,_ = model.env.step(action)\n    img = model.env.render(mode=\"rgb_array\")\n\nimageio.mimsave(\"lander_a2c.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n```\n\n----------------------------------------\n\nTITLE: Training an A2C Agent with Custom Environment in Python\nDESCRIPTION: This snippet shows how to instantiate a custom environment and train an A2C agent using Stable Baselines 3. It demonstrates the simplicity of integrating custom environments with SB3's RL algorithms.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_env.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate the env\nenv = CustomEnv(arg1, ...)\n# Define and Train the agent\nmodel = A2C(\"CnnPolicy\", env).learn(total_timesteps=1000)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Network Architecture for SAC in Stable Baselines3\nDESCRIPTION: This code snippet shows how to configure a custom network architecture for the SAC (Soft Actor-Critic) algorithm with different layer structures for the policy and Q-function networks.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_policy.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import SAC\n\n# Custom actor architecture with two layers of 64 units each\n# Custom critic architecture with two layers of 400 and 300 units\npolicy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))\n# Create the agent\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", policy_kwargs=policy_kwargs, verbose=1)\nmodel.learn(5000)\n```\n\n----------------------------------------\n\nTITLE: Using EvalCallback in Python for Model Evaluation in Stable Baselines 3\nDESCRIPTION: Example of using EvalCallback to periodically evaluate a model's performance on a separate test environment during training. This callback can save the best model based on evaluation metrics and log evaluation results.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import EvalCallback\n\n# Separate evaluation env\neval_env = gym.make(\"Pendulum-v1\")\n# Use deterministic actions for evaluation\neval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n                             log_path=\"./logs/\", eval_freq=500,\n                             deterministic=True, render=False)\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\")\nmodel.learn(5000, callback=eval_callback)\n```\n\n----------------------------------------\n\nTITLE: Validating Custom Environment with SB3's Environment Checker in Python\nDESCRIPTION: This code snippet demonstrates how to use Stable Baselines 3's environment checker to validate a custom environment. It ensures that the environment follows the Gym interface supported by SB3.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_env.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = CustomEnv(arg1, ...)\n# It will check your custom environment and output additional warnings if needed\ncheck_env(env)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Linear Learning Rate Schedule in Python for PPO\nDESCRIPTION: This code defines a linear learning rate schedule function and demonstrates how to use it with the PPO algorithm in Stable Baselines3. The schedule decreases the learning rate linearly from an initial value to zero as training progresses.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable\n\nfrom stable_baselines3 import PPO\n\n\ndef linear_schedule(initial_value: float) -> Callable[[float], float]:\n    \"\"\"\n    Linear learning rate schedule.\n\n    :param initial_value: Initial learning rate.\n    :return: schedule that computes\n      current learning rate depending on remaining progress\n    \"\"\"\n    def func(progress_remaining: float) -> float:\n        \"\"\"\n        Progress will decrease from 1 (beginning) to 0.\n\n        :param progress_remaining:\n        :return: current learning rate\n        \"\"\"\n        return progress_remaining * initial_value\n\n    return func\n\n# Initial learning rate of 0.001\nmodel = PPO(\"MlpPolicy\", \"CartPole-v1\", learning_rate=linear_schedule(0.001), verbose=1)\nmodel.learn(total_timesteps=20_000)\n# By default, `reset_num_timesteps` is True, in which case the learning rate schedule resets.\n# progress_remaining = 1.0 - (num_timesteps / total_timesteps)\nmodel.learn(total_timesteps=10_000, reset_num_timesteps=True)\n```\n\n----------------------------------------\n\nTITLE: Modifying Environment Attributes in Vectorized Environments\nDESCRIPTION: Example implementation of a custom environment with a parameter that can be modified during training, demonstrating how to properly expose and update environment attributes when using vectorized environments in Stable Baselines3.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/vec_envs.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom gymnasium import spaces\n\nfrom stable_baselines3.common.env_util import make_vec_env\n\n\nclass MyMultiTaskEnv(gym.Env):\n\n  def __init__(self):\n      super().__init__()\n      \"\"\"\n      A state and action space for robotic locomotion.\n      The multi-task twist is that the policy would need to adapt to different terrains, each with its own\n      friction coefficient, mu.\n      The friction coefficient is the only parameter that changes between tasks.\n      mu is a scalar between 0 and 1, and during training a callback is used to update mu.\n      \"\"\"\n      ...\n\n  def step(self, action):\n    # Do something, depending on the action and current value of mu the next state is computed\n    return self._get_obs(), reward, done, truncated, info\n\n  def set_mu(self, new_mu: float) -> None:\n      # Note: this value should be used only at the next reset\n      self.mu = new_mu\n\n# Example of wrapped env\n# env is of type <TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>\nenv = gym.make(\"CartPole-v1\")\n# To access the base env, without wrapper, you should use `.unwrapped`\n# or env.get_wrapper_attr(\"gravity\") to include wrappers\nenv.unwrapped.gravity\n# SB3 uses VecEnv for training, where `env.unwrapped.x = new_value` cannot be used to set an attribute\n# therefore, you should expose a setter like `set_mu` to properly set an attribute\nvec_env = make_vec_env(MyMultiTaskEnv)\n# Print current mu value\n# Note: you should use vec_env.env_method(\"get_wrapper_attr\", \"mu\") in Gymnasium v1.0\nprint(vec_env.env_method(\"get_wrapper_attr\", \"mu\"))\n# Change `mu` attribute via the setter\nvec_env.env_method(\"set_mu\", \"mu\", 0.1)\n# If the variable exists, you can also use `set_wrapper_attr` to set it\nassert vec_env.has_attr(\"mu\")\nvec_env.env_method(\"set_wrapper_attr\", \"mu\", 0.1)\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing with Off-Policy Algorithms in Python\nDESCRIPTION: This snippet demonstrates how to use multiprocessing with off-policy algorithms in Stable Baselines3. It creates a vectorized environment and trains an SAC agent, adjusting the gradient steps parameter.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.env_util import make_vec_env\n\nvec_env = make_vec_env(\"Pendulum-v0\", n_envs=4, seed=0)\n\n# We collect 4 transitions per call to `env.step()`\n# and performs 2 gradient steps per call to `env.step()`\n# if gradient_steps=-1, then we would do 4 gradients steps per call to `env.step()`\nmodel = SAC(\"MlpPolicy\", vec_env, train_freq=1, gradient_steps=2, verbose=1)\nmodel.learn(total_timesteps=10_000)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Agent Performance with EvalCallback in Stable Baselines3\nDESCRIPTION: Uses EvalCallback to periodically evaluate a SAC agent's performance on a separate test environment. The callback saves the best model, logs evaluation results, and allows for environment customization.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.env_util import make_vec_env\n\nenv_id = \"Pendulum-v1\"\nn_training_envs = 1\nn_eval_envs = 5\n\n# Create log dir where evaluation results will be saved\neval_log_dir = \"./eval_logs/\"\nos.makedirs(eval_log_dir, exist_ok=True)\n\n# Initialize a vectorized training environment with default parameters\ntrain_env = make_vec_env(env_id, n_envs=n_training_envs, seed=0)\n\n# Separate evaluation env, with different parameters passed via env_kwargs\n# Eval environments can be vectorized to speed up evaluation.\neval_env = make_vec_env(env_id, n_envs=n_eval_envs, seed=0,\n                      env_kwargs={'g':0.7})\n\n# Create callback that evaluates agent for 5 episodes every 500 training environment steps.\n# When using multiple training environments, agent will be evaluated every\n# eval_freq calls to train_env.step(), thus it will be evaluated every\n# (eval_freq * n_envs) training steps. See EvalCallback doc for more information.\neval_callback = EvalCallback(eval_env, best_model_save_path=eval_log_dir,\n                            log_path=eval_log_dir, eval_freq=max(500 // n_training_envs, 1),\n                            n_eval_episodes=5, deterministic=True,\n                            render=False)\n\nmodel = SAC(\"MlpPolicy\", train_env)\nmodel.learn(5000, callback=eval_callback)\n```\n\n----------------------------------------\n\nTITLE: Exporting Model to C++ using PyTorch JIT\nDESCRIPTION: Demonstrates how to trace and export a model using PyTorch JIT for C++ inference. Includes model optimization and validation steps.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/export.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\njit_path = \"sac_traced.pt\"\n\n# Trace and optimize the module\ntraced_module = th.jit.trace(onnxable_model.eval(), dummy_input)\nfrozen_module = th.jit.freeze(traced_module)\nfrozen_module = th.jit.optimize_for_inference(frozen_module)\nth.jit.save(frozen_module, jit_path)\n\n##### Load and test with torch\n\nimport torch as th\n\ndummy_input = th.randn(1, *observation_size)\nloaded_module = th.jit.load(jit_path)\naction_jit = loaded_module(dummy_input)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Callback to Change Environment Parameters During Training\nDESCRIPTION: Implementation of a callback in Stable Baselines3 that cycles through different values of a parameter (mu) during training. This demonstrates how to properly modify environment attributes within the training loop.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/vec_envs.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom itertools import cycle\n\nclass ChangeMuCallback(BaseCallback):\n  \"\"\"\n  This callback changes the value of mu during training looping\n  through a list of values until training is aborted.\n  The environment is implemented so that the impact of changing\n  the value of mu mid-episode is visible only after the episode is over\n  and the reset method has been called.\n  \"\"\"\"\n  def __init__(self):\n    super().__init__()\n    # An iterator that contains the different of the friction coefficient\n    self.mus = cycle([0.1, 0.2, 0.5, 0.13, 0.9])\n\n  def _on_step(self):\n    # Note: in practice, you should not change this value at every step\n    # but rather depending on some events/metrics like agent performance/episode termination\n    # both accessible via the `self.logger` or `self.locals` variables\n    self.training_env.env_method(\"set_mu\", next(self.mus))\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Callbacks with CallbackList in SAC Model Training\nDESCRIPTION: This snippet demonstrates how to create and use multiple callbacks (CheckpointCallback and EvalCallback) with a SAC model using CallbackList. The callbacks save model checkpoints every 1000 steps and evaluate the model on a separate environment every 500 steps.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./logs/\")\n# Separate evaluation env\neval_env = gym.make(\"Pendulum-v1\")\neval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n                             log_path=\"./logs/results\", eval_freq=500)\n# Create the callback list\ncallback = CallbackList([checkpoint_callback, eval_callback])\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n# Equivalent to:\n# model.learn(5000, callback=[checkpoint_callback, eval_callback])\nmodel.learn(5000, callback=callback)\n```\n\n----------------------------------------\n\nTITLE: MLFlow Integration with Stable Baselines3\nDESCRIPTION: Implementation of a custom MLFlow logger for tracking Stable Baselines3 experiments. Shows how to create a custom output format and integrate with SAC algorithm.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/integrations.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nfrom typing import Any, Dict, Tuple, Union\n\nimport mlflow\nimport numpy as np\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n\n\nclass MLflowOutputFormat(KVWriter):\n    \"\"\"\n    Dumps key/value pairs into MLflow's numeric format.\n    \"\"\"\n\n    def write(\n        self,\n        key_values: Dict[str, Any],\n        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n        step: int = 0,\n    ) -> None:\n\n        for (key, value), (_, excluded) in zip(\n            sorted(key_values.items()), sorted(key_excluded.items())\n        ):\n\n            if excluded is not None and \"mlflow\" in excluded:\n                continue\n\n            if isinstance(value, np.ScalarType):\n                if not isinstance(value, str):\n                    mlflow.log_metric(key, value, step)\n\n\nloggers = Logger(\n    folder=None,\n    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n)\n\nwith mlflow.start_run():\n    model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=2)\n    # Set custom logger\n    model.set_logger(loggers)\n    model.learn(total_timesteps=10000, log_interval=1)\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating a Trained SAC Model in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained Soft Actor-Critic (SAC) model and evaluate it in an environment. It includes resetting the environment, running the model for multiple episodes, and printing the rewards and success status.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = SAC.load(\"her_sac_highway\", env=env)\n\nobs, info = env.reset()\n\n# Evaluate the agent\nepisode_reward = 0\nfor _ in range(100):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, terminated, truncated, info = env.step(action)\n    episode_reward += reward\n    if terminated or truncated or info.get(\"is_success\", False):\n        print(\"Reward:\", episode_reward, \"Success?\", info.get(\"is_success\", False))\n        episode_reward = 0.0\n        obs, info = env.reset()\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Logging Names in Tensorboard with Stable Baselines3\nDESCRIPTION: Demonstrates how to specify custom logging names when training a model with Tensorboard. This allows for organizing multiple training runs and continuing training curves by using the reset_num_timesteps parameter.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\n\nmodel = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\nmodel.learn(total_timesteps=10_000, tb_log_name=\"first_run\")\n# Pass reset_num_timesteps=False to continue the training curve in tensorboard\n# By default, it will create a new curve\n# Keep tb_log_name constant to have continuous curve (see note below)\nmodel.learn(total_timesteps=10_000, tb_log_name=\"second_run\", reset_num_timesteps=False)\nmodel.learn(total_timesteps=10_000, tb_log_name=\"third_run\", reset_num_timesteps=False)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Environment with Gymnasium in Python\nDESCRIPTION: This snippet shows how to register a custom environment with Gymnasium. Registration allows for easy instantiation of the environment using gym.make() and simplifies the creation of RL agents.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_env.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gymnasium.envs.registration import register\n# Example for the CartPole environment\nregister(\n    # unique identifier for the env `name-version`\n    id=\"CartPole-v1\",\n    # path to the class for creating the env\n    # Note: entry_point also accept a class as input (and not only a string)\n    entry_point=\"gym.envs.classic_control:CartPoleEnv\",\n    # Max number of steps per episode, using a `TimeLimitWrapper`\n    max_episode_steps=500,\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Custom Environment Validity with Stable Baselines3\nDESCRIPTION: This snippet demonstrates how to validate a custom environment using the check_env helper function from Stable Baselines3. This function verifies that your custom environment follows the Gym interface correctly and outputs warnings for potential issues.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_tips.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = CustomEnv(arg1, ...)\n# It will check your custom environment and output additional warnings if needed\ncheck_env(env)\n```\n\n----------------------------------------\n\nTITLE: Integrating Weights & Biases with Stable Baselines3\nDESCRIPTION: Example showing how to use Weights & Biases for experiment tracking with PPO algorithm. Demonstrates setup of wandb initialization, model training with callbacks, and tensorboard integration.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/integrations.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport wandb\nfrom wandb.integration.sb3 import WandbCallback\n\nfrom stable_baselines3 import PPO\n\nconfig = {\n    \"policy_type\": \"MlpPolicy\",\n    \"total_timesteps\": 25000,\n    \"env_id\": \"CartPole-v1\",\n}\nrun = wandb.init(\n    project=\"sb3\",\n    config=config,\n    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n    # monitor_gym=True,  # auto-upload the videos of agents playing the game\n    # save_code=True,  # optional\n)\n\nmodel = PPO(config[\"policy_type\"], config[\"env_id\"], verbose=1, tensorboard_log=f\"runs/{run.id}\")\nmodel.learn(\n    total_timesteps=config[\"total_timesteps\"],\n    callback=WandbCallback(\n        model_save_path=f\"models/{run.id}\",\n        verbose=2,\n    ),\n)\nrun.finish()\n```\n\n----------------------------------------\n\nTITLE: Optimizing A2C Training with SubprocVecEnv on CPU\nDESCRIPTION: Example demonstrating how to improve CPU utilization for A2C by using SubprocVecEnv instead of the default DummyVecEnv and explicitly setting the device to CPU.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/a2c.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\n\nif __name__==\"__main__\":\n    env = make_vec_env(\"CartPole-v1\", n_envs=8, vec_env_cls=SubprocVecEnv)\n    model = A2C(\"MlpPolicy\", env, device=\"cpu\")\n    model.learn(total_timesteps=25_000)\n```\n\n----------------------------------------\n\nTITLE: Handling Dict Observations in Python\nDESCRIPTION: This example shows how to use environments with dictionary observation spaces in Stable Baselines3. It uses the SimpleMultiObsEnv as an example and trains a PPO agent on it.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.envs import SimpleMultiObsEnv\n\n\n# Stable Baselines provides SimpleMultiObsEnv as an example environment with Dict observations\nenv = SimpleMultiObsEnv(random_start=False)\n\nmodel = PPO(\"MultiInputPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=100_000)\n```\n\n----------------------------------------\n\nTITLE: Initializing A2C Model with Tensorboard Logging in Python\nDESCRIPTION: Basic setup for Tensorboard integration with Stable Baselines3. This snippet creates an A2C model for the CartPole environment and configures a directory for Tensorboard logs.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\n\nmodel = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\nmodel.learn(total_timesteps=10_000)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Logger in Stable Baselines3\nDESCRIPTION: Shows how to set up a custom logger for a Stable Baselines3 reinforcement learning model. Demonstrates creating a new logger with multiple output formats and applying it to an A2C model.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/logger.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.logger import configure\n\ntmp_path = \"/tmp/sb3_log/\"\n# set up logger\nnew_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n\nmodel = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n# Set new logger\nmodel.set_logger(new_logger)\nmodel.learn(10000)\n```\n\n----------------------------------------\n\nTITLE: Using StopTrainingOnRewardThreshold with EvalCallback\nDESCRIPTION: This example shows how to use StopTrainingOnRewardThreshold with EvalCallback to automatically stop training when a model reaches a specified reward threshold (-200). The callback is triggered when a new best model is found during evaluation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n\n# Separate evaluation env\neval_env = gym.make(\"Pendulum-v1\")\n# Stop training when the model reaches the reward threshold\ncallback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-200, verbose=1)\neval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n# Almost infinite number of timesteps, but the training will stop\n# early as soon as the reward threshold is reached\nmodel.learn(int(1e10), callback=eval_callback)\n```\n\n----------------------------------------\n\nTITLE: Using CheckpointCallback in Python for Saving Models with Stable Baselines 3\nDESCRIPTION: Example of using the CheckpointCallback to periodically save model checkpoints during training. This callback saves the model every specified number of timesteps and can optionally save the replay buffer and normalization statistics.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import CheckpointCallback\n\n# Save a checkpoint every 1000 steps\ncheckpoint_callback = CheckpointCallback(\n  save_freq=1000,\n  save_path=\"./logs/\",\n  name_prefix=\"rl_model\",\n  save_replay_buffer=True,\n  save_vecnormalize=True,\n)\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\")\nmodel.learn(2000, callback=checkpoint_callback)\n```\n\n----------------------------------------\n\nTITLE: Optimizing PPO for CPU Performance in Python\nDESCRIPTION: This code snippet shows how to optimize PPO for CPU performance by using SubprocVecEnv instead of DummyVecEnv. It creates 8 parallel environments for the CartPole-v1 task and trains the PPO model on the CPU.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/ppo.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\n\nif __name__==\"__main__\":\n    env = make_vec_env(\"CartPole-v1\", n_envs=8, vec_env_cls=SubprocVecEnv)\n    model = PPO(\"MlpPolicy\", env, device=\"cpu\")\n    model.learn(total_timesteps=25_000)\n```\n\n----------------------------------------\n\nTITLE: Using EveryNTimesteps with CheckpointCallback\nDESCRIPTION: This example demonstrates the EveryNTimesteps EventCallback that triggers another callback (CheckpointCallback in this case) every specified number of timesteps (500). This allows for periodic saving of model checkpoints during training.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n\n# this is equivalent to defining CheckpointCallback(save_freq=500)\n# checkpoint_callback will be triggered every 500 steps\ncheckpoint_on_event = CheckpointCallback(save_freq=1, save_path=\"./logs/\")\nevent_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n\nmodel = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n\nmodel.learn(20_000, callback=event_callback)\n```\n\n----------------------------------------\n\nTITLE: Using CallbackList in Python to Chain Multiple Callbacks in Stable Baselines 3\nDESCRIPTION: Example of how to chain multiple callbacks using CallbackList in Stable Baselines 3. This allows for combining different callbacks like checkpoint saving and evaluation to be executed sequentially during training.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n```\n\n----------------------------------------\n\nTITLE: Using Stable Baselines3 with DeepMind Control (dm_control) in Python\nDESCRIPTION: This snippet demonstrates how to use Stable Baselines3 with the DeepMind Control Suite. It uses wrappers from shimmy to convert dm_control environments to Gymnasium-compatible environments, allowing them to be used with SB3's PPO algorithm.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport shimmy\nimport stable_baselines3 as sb3\nfrom dm_control import suite\nfrom gymnasium.wrappers import FlattenObservation\n\n# Available envs:\n# suite._DOMAINS and suite.dog.SUITE\n\nenv = suite.load(domain_name=\"dog\", task_name=\"run\")\ngym_env = FlattenObservation(shimmy.DmControlCompatibilityV0(env))\n\nmodel = sb3.PPO(\"MlpPolicy\", gym_env, verbose=1)\nmodel.learn(10_000, progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Using StopTrainingOnNoModelImprovement to Detect Training Plateaus\nDESCRIPTION: This example shows how to use StopTrainingOnNoModelImprovement to stop training if there's no improvement in the model's performance after a specified number of consecutive evaluations (3). It requires at least 5 evaluations before it can stop training.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n\n# Separate evaluation env\neval_env = gym.make(\"Pendulum-v1\")\n# Stop training if there is no improvement after more than 3 evaluations\nstop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\neval_callback = EvalCallback(eval_env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", learning_rate=1e-3, verbose=1)\n# Almost infinite number of timesteps, but the training will stop early\n# as soon as the the number of consecutive evaluations without model\n# improvement is greater than 3\nmodel.learn(int(1e10), callback=eval_callback)\n```\n\n----------------------------------------\n\nTITLE: Logging Environment Images to Tensorboard with Custom Callback\nDESCRIPTION: Demonstrates how to render and log environment images to Tensorboard during training. This example captures frames from the Pendulum environment and logs them at regular intervals.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.logger import Image\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", tensorboard_log=\"/tmp/sac/\", verbose=1)\n\n\nclass ImageRecorderCallback(BaseCallback):\n    def __init__(self, verbose=0):\n        super().__init__(verbose)\n\n    def _on_step(self):\n        image = self.training_env.render(mode=\"rgb_array\")\n        # \"HWC\" specify the dataformat of the image, here channel last\n        # (H for height, W for width, C for channel)\n        # See https://pytorch.org/docs/stable/tensorboard.html\n        # for supported formats\n        self.logger.record(\"trajectory/image\", Image(image, \"HWC\"), exclude=(\"stdout\", \"log\", \"json\", \"csv\"))\n        return True\n\n\nmodel.learn(50000, callback=ImageRecorderCallback())\n```\n\n----------------------------------------\n\nTITLE: Implementing VecExtractDictObs Wrapper in Python\nDESCRIPTION: A vectorized wrapper implementation that filters specific keys from dictionary observations in vectorized environments. It inherits from VecEnvWrapper and provides functionality to extract and process specific observation keys during environment steps.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/vec_envs.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n\n\nclass VecExtractDictObs(VecEnvWrapper):\n    \"\"\"\n    A vectorized wrapper for filtering a specific key from dictionary observations.\n    Similar to Gym's FilterObservation wrapper:\n        https://github.com/openai/gym/blob/master/gym/wrappers/filter_observation.py\n\n    :param venv: The vectorized environment\n    :param key: The key of the dictionary observation\n    \"\"\"\n\n    def __init__(self, venv: VecEnv, key: str):\n        self.key = key\n        super().__init__(venv=venv, observation_space=venv.observation_space.spaces[self.key])\n\n    def reset(self) -> np.ndarray:\n        obs = self.venv.reset()\n        return obs[self.key]\n\n    def step_async(self, actions: np.ndarray) -> None:\n        self.venv.step_async(actions)\n\n    def step_wait(self) -> VecEnvStepReturn:\n        obs, reward, done, info = self.venv.step_wait()\n        return obs[self.key], reward, done, info\n\nenv = DummyVecEnv([lambda: gym.make(\"FetchReach-v1\")])\n# Wrap the VecEnv\nenv = VecExtractDictObs(env, key=\"observation\")\n```\n\n----------------------------------------\n\nTITLE: Testing a Random Agent on a Custom Environment\nDESCRIPTION: This code demonstrates how to quickly test a custom environment with a random agent. It initializes the environment, takes random actions for a specified number of steps, and handles episode termination.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_tips.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nenv = YourEnv()\nobs, info = env.reset()\nn_steps = 10\nfor _ in range(n_steps):\n    # Random action\n    action = env.action_space.sample()\n    obs, reward, terminated, truncated, info = env.step(action)\n    if done:\n        obs, info = env.reset()\n```\n\n----------------------------------------\n\nTITLE: Logging Matplotlib Figures to Tensorboard with Custom Callback\nDESCRIPTION: Shows how to create and log matplotlib figures to Tensorboard. This example creates random plots and logs them during SAC training on the Pendulum environment.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.logger import Figure\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", tensorboard_log=\"/tmp/sac/\", verbose=1)\n\n\nclass FigureRecorderCallback(BaseCallback):\n    def __init__(self, verbose=0):\n        super().__init__(verbose)\n\n    def _on_step(self):\n        # Plot values (here a random variable)\n        figure = plt.figure()\n        figure.add_subplot().plot(np.random.random(3))\n        # Close the figure after logging it\n        self.logger.record(\"trajectory/figure\", Figure(figure, close=True), exclude=(\"stdout\", \"log\", \"json\", \"csv\"))\n        plt.close()\n        return True\n\n\nmodel.learn(50000, callback=FigureRecorderCallback())\n```\n\n----------------------------------------\n\nTITLE: Customizing Network Architecture in PPO (Python)\nDESCRIPTION: This snippet demonstrates how to customize the policy network architecture for PPO in Stable Baselines3. It defines custom actor and value function networks with two layers of 32 units each, using ReLU activation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_policy.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport torch as th\n\nfrom stable_baselines3 import PPO\n\n# Custom actor (pi) and value function (vf) networks\n# of two layers of size 32 each with Relu activation function\n# Note: an extra linear layer will be added on top of the pi and the vf nets, respectively\npolicy_kwargs = dict(activation_fn=th.nn.ReLU,\n                     net_arch=dict(pi=[32, 32], vf=[32, 32]))\n# Create the agent\nmodel = PPO(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)\n# Retrieve the environment\nenv = model.get_env()\n# Train the agent\nmodel.learn(total_timesteps=20_000)\n# Save the agent\nmodel.save(\"ppo_cartpole\")\n\ndel model\n# the policy_kwargs are automatically loaded\nmodel = PPO.load(\"ppo_cartpole\", env=env)\n```\n\n----------------------------------------\n\nTITLE: Implementing Evolution Strategy for CartPole using A2C in Python\nDESCRIPTION: This code demonstrates how to access and modify model parameters in Stable Baselines3. It implements a simple evolution strategy to solve the CartPole-v1 environment, starting with parameters obtained from A2C policy gradient updates.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\n\nimport gymnasium as gym\nimport numpy as np\nimport torch as th\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n\ndef mutate(params: Dict[str, th.Tensor]) -> Dict[str, th.Tensor]:\n    \"\"\"Mutate parameters by adding normal noise to them\"\"\"\n    return dict((name, param + th.randn_like(param)) for name, param in params.items())\n\n\n# Create policy with a small network\nmodel = A2C(\n    \"MlpPolicy\",\n    \"CartPole-v1\",\n    ent_coef=0.0,\n    policy_kwargs={\"net_arch\": [32]},\n    seed=0,\n    learning_rate=0.05,\n)\n\n# Use traditional actor-critic policy gradient updates to\n# find good initial parameters\nmodel.learn(total_timesteps=10_000)\n\n# Include only variables with \"policy\", \"action\" (policy) or \"shared_net\" (shared layers)\n# in their name: only these ones affect the action.\n# NOTE: you can retrieve those parameters using model.get_parameters() too\nmean_params = dict(\n    (key, value)\n    for key, value in model.policy.state_dict().items()\n    if (\"policy\" in key or \"shared_net\" in key or \"action\" in key)\n)\n\n# population size of 50 invdiduals\npop_size = 50\n# Keep top 10%\nn_elite = pop_size // 10\n# Retrieve the environment\nvec_env = model.get_env()\n\nfor iteration in range(10):\n    # Create population of candidates and evaluate them\n    population = []\n    for population_i in range(pop_size):\n        candidate = mutate(mean_params)\n        # Load new policy parameters to agent.\n        # Tell function that it should only update parameters\n        # we give it (policy parameters)\n        model.policy.load_state_dict(candidate, strict=False)\n        # Evaluate the candidate\n        fitness, _ = evaluate_policy(model, vec_env)\n        population.append((candidate, fitness))\n    # Take top 10% and use average over their parameters as next mean parameter\n    top_candidates = sorted(population, key=lambda x: x[1], reverse=True)[:n_elite]\n    mean_params = dict(\n        (\n            name,\n            th.stack([candidate[0][name] for candidate in top_candidates]).mean(dim=0),\n        )\n        for name in mean_params.keys()\n    )\n    mean_fitness = sum(top_candidate[1] for top_candidate in top_candidates) / n_elite\n    print(f\"Iteration {iteration + 1:<3} Mean top fitness: {mean_fitness:.2f}\")\n    print(f\"Best fitness: {top_candidates[0][1]:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Training Script for SBX with RL Zoo\nDESCRIPTION: This script creates a compatibility layer between Stable Baselines Jax and RL Zoo by registering SBX algorithms with RL Zoo's algorithm dictionary. It allows using the RL Zoo CLI to train SBX models with commands like 'python train_sbx.py --algo sac --env Pendulum-v1'.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/sbx.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport rl_zoo3\nimport rl_zoo3.train\nfrom rl_zoo3.train import train\nfrom sbx import DDPG, DQN, PPO, SAC, TD3, TQC, CrossQ\n\nrl_zoo3.ALGOS[\"ddpg\"] = DDPG\nrl_zoo3.ALGOS[\"dqn\"] = DQN\n# See SBX readme to use DroQ configuration\n# rl_zoo3.ALGOS[\"droq\"] = DroQ\nrl_zoo3.ALGOS[\"sac\"] = SAC\nrl_zoo3.ALGOS[\"ppo\"] = PPO\nrl_zoo3.ALGOS[\"td3\"] = TD3\nrl_zoo3.ALGOS[\"tqc\"] = TQC\nrl_zoo3.ALGOS[\"crossq\"] = CrossQ\nrl_zoo3.train.ALGOS = rl_zoo3.ALGOS\nrl_zoo3.exp_manager.ALGOS = rl_zoo3.ALGOS\n\n\nif __name__ == \"__main__\":\n    train()\n```\n\n----------------------------------------\n\nTITLE: Using StopTrainingOnMaxEpisodes to Limit Training Episodes\nDESCRIPTION: This example demonstrates how to use StopTrainingOnMaxEpisodes callback to stop training after a specified number of episodes (5), regardless of the total timesteps parameter. For multi-environment training, it will train for max_episodes * n_envs total episodes.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n\n# Stops training when the model reaches the maximum number of episodes\ncallback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=5, verbose=1)\n\nmodel = A2C(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n# Almost infinite number of timesteps, but the training will stop\n# early as soon as the max number of episodes is reached\nmodel.learn(int(1e10), callback=callback_max_episodes)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Training with Callbacks in Python\nDESCRIPTION: This snippet demonstrates how to use callbacks to monitor training in Stable Baselines3. It defines a custom callback for saving the best model during training.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom stable_baselines3 import TD3\nfrom stable_baselines3.common import results_plotter\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\nfrom stable_baselines3.common.noise import NormalActionNoise\nfrom stable_baselines3.common.callbacks import BaseCallback\n\n\nclass SaveOnBestTrainingRewardCallback(BaseCallback):\n    \"\"\"\n    Callback for saving a model (the check is done every ``check_freq`` steps)\n    based on the training reward (in practice, we recommend using ``EvalCallback``).\n\n    :param check_freq:\n    :param log_dir: Path to the folder where the model will be saved.\n      It must contains the file created by the ``Monitor`` wrapper.\n    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n    \"\"\"\n    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n        self.check_freq = check_freq\n        self.log_dir = log_dir\n        self.save_path = os.path.join(log_dir, 'best_model')\n        self.best_mean_reward = -np.inf\n\n    def _init_callback(self) -> None:\n        # Create folder if needed\n        if self.save_path is not None:\n            os.makedirs(self.save_path, exist_ok=True)\n\n    def _on_step(self) -> bool:\n        if self.n_calls % self.check_freq == 0:\n\n          # Retrieve training reward\n          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n          if len(x) > 0:\n              # Mean training reward over the last 100 episodes\n              mean_reward = np.mean(y[-100:])\n              if self.verbose >= 1:\n                print(f\"Num timesteps: {self.num_timesteps}\")\n                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n\n              # New best model, you could save the agent here\n              if mean_reward > self.best_mean_reward:\n                  self.best_mean_reward = mean_reward\n                  # Example for saving best model\n                  if self.verbose >= 1:\n                    print(f\"Saving new best model to {self.save_path}\")\n                  self.model.save(self.save_path)\n\n        return True\n```\n\n----------------------------------------\n\nTITLE: Implementing ShiftWrapper for Discrete Action Spaces in Python\nDESCRIPTION: This wrapper allows the use of Discrete action spaces with non-zero start values in Stable Baselines 3. It shifts the action space to start from 0 and adjusts the step method accordingly.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_env.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nclass ShiftWrapper(gym.Wrapper):\n    \"\"\"Allow to use Discrete() action spaces with start!=0\"\"\"\n    def __init__(self, env: gym.Env) -> None:\n        super().__init__(env)\n        assert isinstance(env.action_space, gym.spaces.Discrete)\n        self.action_space = gym.spaces.Discrete(env.action_space.n, start=0)\n\n    def step(self, action: int):\n        return self.env.step(action + self.env.action_space.start)\n```\n\n----------------------------------------\n\nTITLE: One-liner for Training PPO on CartPole Environment in Python\nDESCRIPTION: A simplified one-line example to train a PPO model on the CartPole environment. This requires the environment to be registered in Gymnasium and the policy to be registered in Stable Baselines3.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import PPO\n\nmodel = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Combined Feature Extractor for Multi-Input Observations (Python)\nDESCRIPTION: This example shows how to create a custom feature extractor for handling dictionary observations with multiple inputs in Stable Baselines3. It processes image and vector inputs separately before combining them.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/custom_policy.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport torch as th\nfrom torch import nn\n\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nclass CustomCombinedExtractor(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Dict):\n        # We do not know features-dim here before going over all the items,\n        # so put something dummy for now. PyTorch requires calling\n        # nn.Module.__init__ before adding modules\n        super().__init__(observation_space, features_dim=1)\n\n        extractors = {}\n\n        total_concat_size = 0\n        # We need to know size of the output of this extractor,\n        # so go over all the spaces and compute output feature sizes\n        for key, subspace in observation_space.spaces.items():\n            if key == \"image\":\n                # We will just downsample one channel of the image by 4x4 and flatten.\n                # Assume the image is single-channel (subspace.shape[0] == 0)\n                extractors[key] = nn.Sequential(nn.MaxPool2d(4), nn.Flatten())\n                total_concat_size += subspace.shape[1] // 4 * subspace.shape[2] // 4\n            elif key == \"vector\":\n                # Run through a simple MLP\n                extractors[key] = nn.Linear(subspace.shape[0], 16)\n                total_concat_size += 16\n\n        self.extractors = nn.ModuleDict(extractors)\n\n        # Update the features dim manually\n        self._features_dim = total_concat_size\n\n    def forward(self, observations) -> th.Tensor:\n        encoded_tensor_list = []\n\n        # self.extractors contain nn.Modules that do all the processing.\n        for key, extractor in self.extractors.items():\n```\n\n----------------------------------------\n\nTITLE: Custom Environment with NaN Handling\nDESCRIPTION: Implementation of a custom Gymnasium environment that demonstrates NaN and Infinity handling with VecCheckNan wrapper.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/checking_nan.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n\nclass NanAndInfEnv(gym.Env):\n    \"\"\"Custom Environment that raised NaNs and Infs\"\"\"\n    metadata = {\"render.modes\": [\"human\"]}\n\n    def __init__(self):\n        super(NanAndInfEnv, self).__init__()\n        self.action_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)\n\n    def step(self, _action):\n        randf = np.random.rand()\n        if randf > 0.99:\n            obs = float(\"NaN\")\n        elif randf > 0.98:\n            obs = float(\"inf\")\n        else:\n            obs = randf\n        return [obs], 0.0, False, {}\n\n    def reset(self):\n        return [0.0]\n\n    def render(self, close=False):\n        pass\n\n# Create environment\nenv = DummyVecEnv([lambda: NanAndInfEnv()])\nenv = VecCheckNan(env, raise_exception=True)\n\n# Instantiate the agent\nmodel = PPO(\"MlpPolicy\", env)\n\n# Train the agent\nmodel.learn(total_timesteps=int(2e5))  # this will crash explaining that the invalid value originated from the environment.\n```\n\n----------------------------------------\n\nTITLE: Training QR-DQN Agent Example\nDESCRIPTION: Example code demonstrating how to train a Quantile Regression DQN (QR-DQN) agent on the CartPole environment using SB3-Contrib. Shows initialization with custom policy parameters, training, and model saving.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/sb3_contrib.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sb3_contrib import QRDQN\n\npolicy_kwargs = dict(n_quantiles=50)\nmodel = QRDQN(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)\nmodel.learn(total_timesteps=10000, log_interval=4)\nmodel.save(\"qrdqn_cartpole\")\n```\n\n----------------------------------------\n\nTITLE: Exporting SAC Actor to ONNX Format\nDESCRIPTION: Shows how to export a trained SAC actor network to ONNX format. Includes policy wrapper class, model export, and inference validation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/export.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch as th\n\nfrom stable_baselines3 import SAC\n\n\nclass OnnxablePolicy(th.nn.Module):\n    def __init__(self, actor: th.nn.Module):\n        super().__init__()\n        self.actor = actor\n\n    def forward(self, observation: th.Tensor) -> th.Tensor:\n        return self.actor(observation, deterministic=True)\n\n\nSAC(\"MlpPolicy\", \"Pendulum-v1\").save(\"PathToTrainedModel.zip\")\nmodel = SAC.load(\"PathToTrainedModel.zip\", device=\"cpu\")\nonnxable_model = OnnxablePolicy(model.policy.actor)\n\nobservation_size = model.observation_space.shape\ndummy_input = th.randn(1, *observation_size)\nth.onnx.export(\n    onnxable_model,\n    dummy_input,\n    \"my_sac_actor.onnx\",\n    opset_version=17,\n    input_names=[\"input\"],\n)\n\nimport onnxruntime as ort\nimport numpy as np\n\nonnx_path = \"my_sac_actor.onnx\"\n\nobservation = np.zeros((1, *observation_size)).astype(np.float32)\nort_sess = ort.InferenceSession(onnx_path)\nscaled_action = ort_sess.run(None, {\"input\": observation})[0]\n\nprint(scaled_action)\n\nwith th.no_grad():\n    print(model.actor(th.as_tensor(observation), deterministic=True))\n```\n\n----------------------------------------\n\nTITLE: Displaying Stable Baselines3 Zip-Archive File Structure\nDESCRIPTION: A visual representation of the zip-archive file structure used by Stable Baselines3 to save trained models, showing the main components including JSON data file, PyTorch model files, optimizer files, and version information.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/save_format.rst#2025-04-16_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nsaved_model.zip/\n├── data              JSON file of class-parameters (dictionary)\n├── *.optimizer.pth   PyTorch optimizers serialized\n├── policy.pth        PyTorch state dictionary of the policy saved\n├── pytorch_variables.pth Additional PyTorch variables\n├── _stable_baselines3_version contains the SB3 version with which the model was saved\n├── system_info.txt contains system info (os, python version, ...) on which the model was saved\n```\n\n----------------------------------------\n\nTITLE: Using LogEveryNTimesteps for Periodic Logging\nDESCRIPTION: This snippet shows how to use LogEveryNTimesteps which is derived from EveryNTimesteps to dump logged training data every 1,000 timesteps. Auto-logging is disabled by setting log_interval=None in the learn method.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import LogEveryNTimesteps\n\nevent_callback = LogEveryNTimesteps(n_steps=1_000)\n\nmodel = PPO(\"MlpPolicy\", \"Pendulum-v1\", verbose=1)\n\n# Disable auto-logging by passing `log_interval=None`\nmodel.learn(10_000, callback=event_callback, log_interval=None)\n```\n\n----------------------------------------\n\nTITLE: Uploading Models to Hugging Face Hub using package_to_hub\nDESCRIPTION: Example demonstrating how to train a PPO agent and upload it to Hugging Face Hub using the package_to_hub function. Includes model training and packaging.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/integrations.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n# Create the environment\nenv_id = \"CartPole-v1\"\nenv = make_vec_env(env_id, n_envs=1)\n\n# Create the evaluation environment\neval_env = make_vec_env(env_id, n_envs=1)\n\n# Instantiate the agent\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\n\n# Train the agent\nmodel.learn(total_timesteps=int(5000))\n\n# This method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\npackage_to_hub(model=model,\n             model_name=\"ppo-CartPole-v1\",\n             model_architecture=\"PPO\",\n             env_id=env_id,\n             eval_env=eval_env,\n             repo_id=\"sb3/demo-hf-CartPole-v1\",\n             commit_message=\"Test commit\")\n```\n\n----------------------------------------\n\nTITLE: Training PPO on CartPole with Evaluation and Checkpoints\nDESCRIPTION: Example command for training a PPO agent on CartPole with periodic evaluation and checkpoint saving.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m rl_zoo3.train --algo ppo --env CartPole-v1 --eval-freq 10000 --save-freq 50000\n```\n\n----------------------------------------\n\nTITLE: Implementing an EventCallback in Python for Stable Baselines 3\nDESCRIPTION: Example of the EventCallback base class which is used to trigger child callbacks on specific events. This is a foundational component for creating complex callback chains in Stable Baselines 3.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass EventCallback(BaseCallback):\n    \"\"\"\n    Base class for triggering callback on event.\n\n    :param callback: Callback that will be called when an event is triggered.\n    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n    \"\"\"\n    def __init__(self, callback: BaseCallback, verbose: int = 0):\n        super().__init__(verbose=verbose)\n        self.callback = callback\n        # Give access to the parent\n        self.callback.parent = self\n    ...\n\n    def _on_event(self) -> bool:\n        return self.callback()\n```\n\n----------------------------------------\n\nTITLE: Creating VecEnv with RescaleAction Wrapper\nDESCRIPTION: Example showing how to create a vectorized environment with RescaleAction wrapper applied to each sub-environment. Demonstrates the use of make_vec_env with wrapper configuration.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/vec_envs.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gymnasium.wrappers import RescaleAction\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Use gym wrapper for each sub-env of the VecEnv\nwrapper_kwargs = dict(min_action=-1.0, max_action=1.0)\nvec_env = make_vec_env(\n    \"Pendulum-v1\", n_envs=2, wrapper_class=RescaleAction, wrapper_kwargs=wrapper_kwargs\n)\n```\n\n----------------------------------------\n\nTITLE: Enjoying a Trained A2C Agent on Breakout\nDESCRIPTION: Example command to run a trained A2C agent on Breakout for 5000 timesteps from a specified folder.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m rl_zoo3.enjoy --algo a2c --env BreakoutNoFrameskip-v4 --folder rl-trained-agents/ -n 5000\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Network Architecture for Off-Policy Algorithms\nDESCRIPTION: Example demonstrating how to specify custom actor/critic network architectures for off-policy algorithms like SAC, TD3, and DDPG using the net_arch parameter.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnet_arch=dict(qf=[400, 300], pi=[64, 64])\n```\n\n----------------------------------------\n\nTITLE: Recording MP4 Video of Random Agent with VecVideoRecorder\nDESCRIPTION: Creates an MP4 video recording of a random agent in the CartPole environment. Uses DummyVecEnv and VecVideoRecorder to capture RGB array frames and save them as a video file. Requires ffmpeg or avconv to be installed.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/examples.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n\nenv_id = \"CartPole-v1\"\nvideo_folder = \"logs/videos/\"\nvideo_length = 100\n\nvec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n\nobs = vec_env.reset()\n\n# Record the video starting at the first step\nvec_env = VecVideoRecorder(vec_env, video_folder,\n                     record_video_trigger=lambda x: x == 0, video_length=video_length,\n                     name_prefix=f\"random-agent-{env_id}\")\n\nvec_env.reset()\nfor _ in range(video_length + 1):\n  action = [vec_env.action_space.sample()]\n  obs, _, _, _ = vec_env.step(action)\n# Save the video\nvec_env.close()\n```\n\n----------------------------------------\n\nTITLE: Installing Development Version of Stable-Baselines3\nDESCRIPTION: Clones the Stable-Baselines3 repository and installs it in editable mode with additional dependencies for development, testing, and documentation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/stable-baselines3 && cd stable-baselines3\npip install -e .[docs,tests,extra]\n```\n\n----------------------------------------\n\nTITLE: Running TD3 Benchmarks with RL Baselines3 Zoo\nDESCRIPTION: Command-line instructions for replicating TD3 benchmark results using the RL Baselines3 Zoo repository. Shows how to clone the repository, train models, and generate performance plots.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/td3.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/rl-baselines3-zoo\ncd rl-baselines3-zoo/\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --algo td3 --env $ENV_ID --eval-episodes 10 --eval-freq 10000\n```\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/all_plots.py -a td3 -e HalfCheetah Ant Hopper Walker2D -f logs/ -o logs/td3_results\npython scripts/plot_from_file.py -i logs/td3_results.pkl -latex -l TD3\n```\n\n----------------------------------------\n\nTITLE: Using ProgressBarCallback in Python with Stable Baselines 3\nDESCRIPTION: Example of using ProgressBarCallback to display a progress bar with training information, including elapsed time and estimated remaining time. This provides a visual indication of training progress.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/callbacks.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import ProgressBarCallback\n\nmodel = PPO(\"MlpPolicy\", \"Pendulum-v1\")\n# Display progress bar using the progress bar callback\n# this is equivalent to model.learn(100_000, callback=ProgressBarCallback())\nmodel.learn(100_000, progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Bleeding-edge Stable-Baselines3 with Extras\nDESCRIPTION: Installs the latest development version of Stable-Baselines3 with additional features for testing and documentation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install \"stable_baselines3[extra,tests,docs] @ git+https://github.com/DLR-RM/stable-baselines3\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking A2C on RL-Zoo\nDESCRIPTION: Commands for replicating A2C benchmark results using the RL-Zoo repository, including training on environments and plotting performance results.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/a2c.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/rl-baselines3-zoo\ncd rl-baselines3-zoo/\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --algo a2c --env $ENV_ID --eval-episodes 10 --eval-freq 10000\n```\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/all_plots.py -a a2c -e HalfCheetah Ant Hopper Walker2D -f logs/ -o logs/a2c_results\npython scripts/plot_from_file.py -i logs/a2c_results.pkl -latex -l A2C\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Callback for Logging Additional Values to Tensorboard\nDESCRIPTION: Implementation of a custom callback to log arbitrary values to Tensorboard. This example logs random values during training of a SAC model on the Pendulum environment.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import BaseCallback\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", tensorboard_log=\"/tmp/sac/\", verbose=1)\n\n\nclass TensorboardCallback(BaseCallback):\n    \"\"\"\n    Custom callback for plotting additional values in tensorboard.\n    \"\"\"\n\n    def __init__(self, verbose=0):\n        super().__init__(verbose)\n\n    def _on_step(self) -> bool:\n        # Log scalar value (here a random variable)\n        value = np.random.random()\n        self.logger.record(\"random_value\", value)\n        return True\n\n\nmodel.learn(50000, callback=TensorboardCallback())\n```\n\n----------------------------------------\n\nTITLE: Enabling NaN Detection in PyTorch\nDESCRIPTION: Shows how to enable automatic NaN detection in PyTorch using autograd settings.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/checking_nan.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch as th\nth.autograd.set_detect_anomaly(True)\n```\n\n----------------------------------------\n\nTITLE: Replicating DDPG Benchmark Results\nDESCRIPTION: Bash commands for reproducing DDPG benchmark results using the RL Zoo repository. Includes steps for cloning the repository, training the agent, and plotting results.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/ddpg.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/rl-baselines3-zoo\ncd rl-baselines3-zoo/\n\npython train.py --algo ddpg --env $ENV_ID --eval-episodes 10 --eval-freq 10000\n\npython scripts/all_plots.py -a ddpg -e HalfCheetah Ant Hopper Walker2D -f logs/ -o logs/ddpg_results\npython scripts/plot_from_file.py -i logs/ddpg_results.pkl -latex -l DDPG\n```\n\n----------------------------------------\n\nTITLE: Logging Hyperparameters to TensorBoard using HParamCallback\nDESCRIPTION: Implements a custom callback that saves model hyperparameters and metrics to TensorBoard at the start of training. The callback tracks algorithm name, learning rate, gamma, and specified metrics.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.logger import HParam\n\n\nclass HParamCallback(BaseCallback):\n    \"\"\"\n    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.\n    \"\"\"\n\n    def _on_training_start(self) -> None:\n        hparam_dict = {\n            \"algorithm\": self.model.__class__.__name__,\n            \"learning rate\": self.model.learning_rate,\n            \"gamma\": self.model.gamma,\n        }\n        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag\n        # Tensorbaord will find & display metrics from the `SCALARS` tab\n        metric_dict = {\n            \"rollout/ep_len_mean\": 0,\n            \"train/value_loss\": 0.0,\n        }\n        self.logger.record(\n            \"hparams\",\n            HParam(hparam_dict, metric_dict),\n            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n        )\n\n    def _on_step(self) -> bool:\n        return True\n\n\nmodel = A2C(\"MlpPolicy\", \"CartPole-v1\", tensorboard_log=\"runs/\", verbose=1)\nmodel.learn(total_timesteps=int(5e4), callback=HParamCallback())\n```\n\n----------------------------------------\n\nTITLE: NumPy Division by Zero Error Handling\nDESCRIPTION: Demonstrates how to configure NumPy to raise exceptions for floating point errors using seterr.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/checking_nan.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nnp.seterr(all=\"raise\")  # define before your code.\n\nprint(\"numpy test:\")\n\na = np.float64(1.0)\nb = np.float64(0.0)\nval = a / b  # this will now raise an exception instead of a warning.\nprint(val)\n```\n\n----------------------------------------\n\nTITLE: Replicating SAC Benchmark Results Using RL-Baselines3-Zoo\nDESCRIPTION: Commands for replicating the SAC benchmark results using the RL-Baselines3-Zoo repository. This includes cloning the repository, running training with SAC on specified environments, and plotting the results.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/sac.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/rl-baselines3-zoo\ncd rl-baselines3-zoo/\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --algo sac --env $ENV_ID --eval-episodes 10 --eval-freq 10000\n```\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/all_plots.py -a sac -e HalfCheetah Ant Hopper Walker2D -f logs/ -o logs/sac_results\npython scripts/plot_from_file.py -i logs/sac_results.pkl -latex -l SAC\n```\n\n----------------------------------------\n\nTITLE: Direct TensorBoard SummaryWriter Access for Custom Logging\nDESCRIPTION: Shows how to directly access the TensorBoard SummaryWriter through a custom callback for logging arbitrary data. This approach allows for more flexibility in logging custom metrics and text data.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.logger import TensorBoardOutputFormat\n\n\n\nmodel = SAC(\"MlpPolicy\", \"Pendulum-v1\", tensorboard_log=\"/tmp/sac/\", verbose=1)\n\n\nclass SummaryWriterCallback(BaseCallback):\n\n    def _on_training_start(self):\n        self._log_freq = 1000  # log every 1000 calls\n\n        output_formats = self.logger.output_formats\n        # Save reference to tensorboard formatter object\n        # note: the failure case (not formatter found) is not handled here, should be done with try/except.\n        self.tb_formatter = next(formatter for formatter in output_formats if isinstance(formatter, TensorBoardOutputFormat))\n\n    def _on_step(self) -> bool:\n        if self.n_calls % self._log_freq == 0:\n            # You can have access to info from the env using self.locals.\n            # for instance, when using one env (index 0 of locals[\"infos\"]):\n            # lap_count = self.locals[\"infos\"][0][\"lap_count\"]\n            # self.tb_formatter.writer.add_scalar(\"train/lap_count\", lap_count, self.num_timesteps)\n\n            self.tb_formatter.writer.add_text(\"direct_access\", \"this is a value\", self.num_timesteps)\n            self.tb_formatter.writer.flush()\n\n\nmodel.learn(50000, callback=SummaryWriterCallback())\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch Model with Safety Measures in Python\nDESCRIPTION: Demonstrates loading a PyTorch model with weights_only=True for safety, and setting TRUST_REMOTE_CODE=True when using huggingface_sb3.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntorch.load(..., weights_only=True)\n```\n\n----------------------------------------\n\nTITLE: Continuing Training of a Pretrained Agent\nDESCRIPTION: Command to load a pretrained A2C agent for Breakout and continue its training for an additional 5000 steps.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m rl_zoo3.train --algo a2c --env BreakoutNoFrameskip-v4 -i trained_agents/a2c/BreakoutNoFrameskip-v4_1/BreakoutNoFrameskip-v4.zip -n 5000\n```\n\n----------------------------------------\n\nTITLE: Using RMSpropTFLike Optimizer with A2C in PyTorch\nDESCRIPTION: Example showing how to use the TensorFlow-like RMSprop optimizer in Stable-Baselines3 to match TensorFlow's implementation results. This is passed through policy_kwargs to the A2C constructor.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/migration.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nA2C(policy_kwargs=dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5)))\n```\n\n----------------------------------------\n\nTITLE: Initializing HER Replay Buffer\nDESCRIPTION: Example showing the updated way to initialize HER (Hindsight Experience Replay) with SAC in Stable Baselines 3 version 1.1.0+\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nher_kwargs = dict(n_sampled_goal=2, goal_selection_strategy=\"future\", online_sampling=True)\n# SB3 < 1.1.0\n# model = HER(\"MlpPolicy\", env, model_class=SAC, **her_kwargs)\n# SB3 >= 1.1.0:\nmodel = SAC(\"MultiInputPolicy\", env, replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=her_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Running a Trained RL Agent\nDESCRIPTION: Command to visualize and enjoy a trained agent using a specified algorithm and environment.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m rl_zoo3.enjoy --algo algo_name --env env_id\n```\n\n----------------------------------------\n\nTITLE: Creating a Visualization Script for SBX with RL Zoo\nDESCRIPTION: This script enables the visualization of trained SBX models using RL Zoo's enjoy functionality. It registers SBX algorithm implementations with RL Zoo's algorithm dictionary, allowing users to visualize model performance in simulation environments.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/sbx.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport rl_zoo3\nimport rl_zoo3.enjoy\nfrom rl_zoo3.enjoy import enjoy\nfrom sbx import DDPG, DQN, PPO, SAC, TD3, TQC, CrossQ\n\nrl_zoo3.ALGOS[\"ddpg\"] = DDPG\nrl_zoo3.ALGOS[\"dqn\"] = DQN\n# See SBX readme to use DroQ configuration\n# rl_zoo3.ALGOS[\"droq\"] = DroQ\nrl_zoo3.ALGOS[\"sac\"] = SAC\nrl_zoo3.ALGOS[\"ppo\"] = PPO\nrl_zoo3.ALGOS[\"td3\"] = TD3\nrl_zoo3.ALGOS[\"tqc\"] = TQC\nrl_zoo3.ALGOS[\"crossq\"] = CrossQ\nrl_zoo3.enjoy.ALGOS = rl_zoo3.ALGOS\nrl_zoo3.exp_manager.ALGOS = rl_zoo3.ALGOS\n\n\nif __name__ == \"__main__\":\n    enjoy()\n```\n\n----------------------------------------\n\nTITLE: Starting Tensorboard from Command Line to Monitor Training\nDESCRIPTION: Command to start Tensorboard for monitoring RL agent training. This allows viewing training metrics and performance during or after training.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir ./a2c_cartpole_tensorboard/\n```\n\n----------------------------------------\n\nTITLE: Device-Optimized Tensor Construction\nDESCRIPTION: Code change to create tensors directly on the specified device instead of creating them on CPU and then moving them, which provides approximately 8% speed boost on GPU operations.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Set tensors construction directly on the device (~8% speed boost on GPU)\n```\n\n----------------------------------------\n\nTITLE: Installing Stable-Baselines3 with Extra Dependencies\nDESCRIPTION: Installs Stable-Baselines3 with pip, including optional dependencies like Tensorboard, OpenCV, and ale-py for Atari games.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install stable-baselines3[extra]\n```\n\n----------------------------------------\n\nTITLE: Installing Stable-Baselines3 Core Package\nDESCRIPTION: Installs the core Stable-Baselines3 package without optional dependencies.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install stable-baselines3\n```\n\n----------------------------------------\n\nTITLE: Example of TD3/DDPG Hyperparameter Changes in SB3 v2.3.0\nDESCRIPTION: Comparison of default hyperparameters for TD3 between SB3 versions before and after 2.3.0, showing changes to train_freq, gradient_steps, and batch_size.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# SB3 < 2.3.0 default hyperparameters\n# model = TD3(\"MlpPolicy\", env, train_freq=(1, \"episode\"), gradient_steps=-1, batch_size=100)\n# SB3 >= 2.3.0:\nmodel = TD3(\"MlpPolicy\", env, train_freq=1, gradient_steps=1, batch_size=256)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Logging Directories in Tensorboard\nDESCRIPTION: Command to display multiple training runs in the same Tensorboard instance. This allows comparing different algorithms or hyperparameters visually.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/tensorboard.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir ./a2c_cartpole_tensorboard/;./ppo2_cartpole_tensorboard/\n```\n\n----------------------------------------\n\nTITLE: Installing Bleeding-edge Stable-Baselines3\nDESCRIPTION: Installs the latest development version of Stable-Baselines3 directly from the GitHub repository.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/DLR-RM/stable-baselines3\n```\n\n----------------------------------------\n\nTITLE: Converting Training Frequency Format in SAC Model Initialization\nDESCRIPTION: Demonstrates the change in API for specifying training frequency in SB3 models. Previously using n_episodes_rollout and train_freq separately, now using a tuple format (frequency, unit) for the train_freq parameter.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# SB3 < 0.11.0\n# model = SAC(\"MlpPolicy\", env, n_episodes_rollout=1, train_freq=-1)\n# SB3 >= 0.11.0:\nmodel = SAC(\"MlpPolicy\", env, train_freq=(1, \"episode\"))\n```\n\n----------------------------------------\n\nTITLE: Optimizing Hyperparameters with Optuna\nDESCRIPTION: Command to perform hyperparameter optimization for PPO on MountainCar using Optuna with random sampling and median pruning.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m rl_zoo3.train --algo ppo --env MountainCar-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 2 \\\n  --sampler random --pruner median\n```\n\n----------------------------------------\n\nTITLE: Installing SB3 Development Dependencies\nDESCRIPTION: Command to install Stable-Baselines3 with additional dependencies for documentation, testing, and extra features. This uses pip's editable mode to install the package from the current directory.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -e .[docs,tests,extra]\n```\n\n----------------------------------------\n\nTITLE: Importing Evaluation Module in Python for Stable Baselines3\nDESCRIPTION: This code snippet demonstrates how to import the evaluation module from Stable Baselines3 common package. It uses the automodule directive to automatically generate documentation for all members of the module.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/evaluation.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: stable_baselines3.common.evaluation\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Training an RL Agent with RL Baselines3 Zoo\nDESCRIPTION: Command to train an agent using a specified algorithm and environment with the RL Zoo training script.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m rl_zoo3.train --algo algo_name --env env_id\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RL Baselines3 Zoo\nDESCRIPTION: Commands to install system and Python dependencies for the RL Baselines3 Zoo, with options for full or minimal installation.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napt-get install swig cmake ffmpeg\n# full dependencies\npip install -r requirements.txt\n# minimal dependencies\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Example of LogEveryNTimesteps Callback Usage\nDESCRIPTION: A code comment showing that you need to pass log_interval=None when using the LogEveryNTimesteps callback to avoid interference with regular logging.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# note: you need to pass `log_interval=None` to avoid any interference\n```\n\n----------------------------------------\n\nTITLE: Sample Logger Output Format in Stable Baselines3\nDESCRIPTION: Example of the logger output when training a PPO agent, showing various metrics including evaluation stats, rollout information, timing data, and training parameters.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/logger.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n-----------------------------------------\n| eval/                   |             |\n|    mean_ep_length       | 200         |\n|    mean_reward          | -157        |\n| rollout/                |             |\n|    ep_len_mean          | 200         |\n|    ep_rew_mean          | -227        |\n| time/                   |             |\n|    fps                  | 972         |\n|    iterations           | 19          |\n|    time_elapsed         | 80          |\n|    total_timesteps      | 77824       |\n| train/                  |             |\n|    approx_kl            | 0.037781604 |\n|    clip_fraction        | 0.243       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.06       |\n|    explained_variance   | 0.999       |\n|    learning_rate        | 0.001       |\n|    loss                 | 0.245       |\n|    n_updates            | 180         |\n|    policy_gradient_loss | -0.00398    |\n|    std                  | 0.205       |\n|    value_loss           | 0.226       |\n-----------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Installing SB3-Contrib with pip\nDESCRIPTION: Commands for installing the stable version of SB3-Contrib using pip package manager.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/sb3_contrib.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install sb3-contrib\n```\n\n----------------------------------------\n\nTITLE: Initializing DQN Model in Python\nDESCRIPTION: Creates a DQN (Deep Q-Network) model with MlpPolicy and a custom learning start parameter.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = DQN(\"MlpPolicy\", env, learning_starts=100)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking DQN Performance with RL-Zoo\nDESCRIPTION: Shell commands for replicating DQN benchmark results using the RL-Zoo repository. Includes steps for cloning the repository, running training, and plotting results.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/modules/dqn.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/rl-baselines3-zoo\ncd rl-baselines3-zoo/\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --algo dqn --env $ENV_ID --eval-episodes 10 --eval-freq 10000\n```\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/all_plots.py -a dqn -e Pong Breakout -f logs/ -o logs/dqn_results\npython scripts/plot_from_file.py -i logs/dqn_results.pkl -latex -l DQN\n```\n\n----------------------------------------\n\nTITLE: Running Stable-Baselines3 GPU Docker Image\nDESCRIPTION: Runs the GPU-enabled Docker image for Stable-Baselines3, mounting the current directory and running tests.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --runtime=nvidia --rm --network host --ipc=host --name test --mount src=\"$(pwd)\",target=/home/mamba/stable-baselines3,type=bind stablebaselines/stable-baselines3 bash -c 'cd /home/mamba/stable-baselines3/ && pytest tests/'\n```\n\n----------------------------------------\n\nTITLE: NumPy Overflow Error Handling\nDESCRIPTION: Shows how NumPy handles overflow errors with seterr configuration.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/checking_nan.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nnp.seterr(all=\"raise\")  # define before your code.\n\nprint(\"numpy overflow test:\")\n\na = np.float64(10)\nb = np.float64(1000)\nval = a ** b  # this will now raise an exception\nprint(val)\n```\n\n----------------------------------------\n\nTITLE: Progress Bar Implementation in learn() Method\nDESCRIPTION: Addition of a progress bar parameter to the learn() method that displays training progress using TQDM and rich packages, improving user experience during model training.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Added `progress_bar` argument in the `learn()` method, displayed using TQDM and rich packages\n```\n\n----------------------------------------\n\nTITLE: Building Stable-Baselines3 CPU Docker Image\nDESCRIPTION: Builds the CPU-only Docker image for Stable-Baselines3 using the provided Makefile.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-cpu\n```\n\n----------------------------------------\n\nTITLE: Installing Sphinx Documentation Dependencies for Stable Baselines3\nDESCRIPTION: Command for installing Sphinx and the required theme for building the Stable Baselines3 documentation. This uses pip to install the package in development mode with the docs extras.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[docs]\"\n```\n\n----------------------------------------\n\nTITLE: Installing master versions of SB3 and SB3-Contrib\nDESCRIPTION: Commands for installing the latest master versions of both Stable-Baselines3 and SB3-Contrib directly from GitHub repositories.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/sb3_contrib.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/DLR-RM/stable-baselines3\\npip install git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n```\n\n----------------------------------------\n\nTITLE: Building Stable-Baselines3 GPU Docker Image\nDESCRIPTION: Builds the GPU-enabled Docker image for Stable-Baselines3 using the provided Makefile.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-gpu\n```\n\n----------------------------------------\n\nTITLE: Auto-rebuilding Stable Baselines3 Documentation on Changes\nDESCRIPTION: Command for automatically rebuilding the documentation whenever a file changes. This uses sphinx-autobuild to monitor for changes and update the build in real-time.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-autobuild . _build/html\n```\n\n----------------------------------------\n\nTITLE: Importing Utility Functions in Stable-Baselines3\nDESCRIPTION: Example of how to properly import utility functions in Stable-Baselines3, which now require absolute path imports rather than importing from the common module.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/migration.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.env_util import make_atari_env, make_vec_env\nfrom stable_baselines3.common.utils import set_random_seed\n```\n\n----------------------------------------\n\nTITLE: Pulling Stable-Baselines3 CPU Docker Image\nDESCRIPTION: Pulls the CPU-only Docker image for Stable-Baselines3 from Docker Hub.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull stablebaselines/stable-baselines3-cpu\n```\n\n----------------------------------------\n\nTITLE: Deprecation Warning Message Syntax\nDESCRIPTION: RST syntax example showing warning message format for upcoming HER algorithm changes\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_9\n\nLANGUAGE: rst\nCODE:\n```\n.. warning::\n\n    An update to the ``HER`` algorithm is planned to support multi-env training and remove the max episode length constrain.\n    (see `PR #704 <https://github.com/DLR-RM/stable-baselines3/pull/704>`_)\n    This will be a backward incompatible change (model trained with previous version of ``HER`` won't work with the new version).\n```\n\n----------------------------------------\n\nTITLE: Example of DQN Hyperparameter Changes in SB3 v2.3.0\nDESCRIPTION: Shows how the default learning_starts parameter of DQN has been changed to be consistent with other off-policy algorithms, updated from 50,000 which was based on Atari defaults.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# SB3 < 2.3.0 default hyperparameters, 50_000 corresponded to Atari defaults hyperparameters\n# model = DQN(\"MlpPolicy\", env, learning_starts=50_000)\n# SB3 >= 2.3.0:\n```\n\n----------------------------------------\n\nTITLE: Python Function Documentation Template - Python\nDESCRIPTION: Example template showing the required format for documenting Python functions, including type hints and docstring format.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef my_function(arg1: type1, arg2: type2) -> returntype:\n    \"\"\"\n    Short description of the function.\n\n    :param arg1: describe what is arg1\n    :param arg2: describe what is arg2\n    :return: describe what is returned\n    \"\"\"\n    ...\n    return my_variable\n```\n\n----------------------------------------\n\nTITLE: Citing Stable-Baselines3 in BibTeX Format\nDESCRIPTION: This code snippet provides the BibTeX citation for the Stable-Baselines3 project. It includes details such as authors, title, journal, year, volume, number, pages, and URL for referencing the project in academic publications.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{stable-baselines3,\n  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {268},\n  pages   = {1-8},\n  url     = {http://jmlr.org/papers/v22/20-1364.html}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Hugging Face Integration for Stable Baselines3\nDESCRIPTION: Command to install the Hugging Face integration package for Stable Baselines3.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/integrations.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_sb3\n```\n\n----------------------------------------\n\nTITLE: Installing Stable-Baselines3 in Development Mode - Bash\nDESCRIPTION: Command for installing Stable-Baselines3 in development mode with additional dependencies for documentation and testing.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[docs,tests,extra]\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for Stable-Baselines3\nDESCRIPTION: BibTeX entry for citing the Stable-Baselines3 library in academic publications. This references the Journal of Machine Learning Research paper published in 2021 that introduced the library.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_8\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{stable-baselines3,\n  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {268},\n  pages   = {1-8},\n  url     = {http://jmlr.org/papers/v22/20-1364.html}\n}\n```\n\n----------------------------------------\n\nTITLE: Pulling Stable-Baselines3 GPU Docker Image\nDESCRIPTION: Pulls the GPU-enabled Docker image for Stable-Baselines3 from Docker Hub.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull stablebaselines/stable-baselines3\n```\n\n----------------------------------------\n\nTITLE: Cloning Stable-Baselines3 Repository - Bash\nDESCRIPTION: Commands for cloning the Stable-Baselines3 repository and navigating to the project directory.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/DLR-RM/stable-baselines3\ncd stable-baselines3/\n```\n\n----------------------------------------\n\nTITLE: Configuring Autodoc for Stable Baselines3 Utilities in RST\nDESCRIPTION: A reStructuredText directive that configures the Sphinx autodoc extension to automatically generate documentation for all members of the stable_baselines3.common.utils module.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/utils.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: stable_baselines3.common.utils\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Running Stable-Baselines3 CPU Docker Image\nDESCRIPTION: Runs the CPU-only Docker image for Stable-Baselines3, mounting the current directory and running tests.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/install.rst#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm --network host --ipc=host --name test --mount src=\"$(pwd)\",target=/home/mamba/stable-baselines3,type=bind stablebaselines/stable-baselines3-cpu bash -c 'cd /home/mamba/stable-baselines3/ && pytest tests/'\n```\n\n----------------------------------------\n\nTITLE: Building Stable Baselines3 Documentation\nDESCRIPTION: Command to generate the HTML documentation for Stable Baselines3. This should be executed in the docs/ folder to build the documentation once.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Monitor Module\nDESCRIPTION: ReStructuredText documentation structure that sets up the monitor wrapper documentation page with automodule directive to generate API docs.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/monitor.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _monitor:\\n\\nMonitor Wrapper\\n===============\\n\\n.. automodule:: stable_baselines3.common.monitor\\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Cloning the RL Baselines3 Zoo Repository\nDESCRIPTION: Commands to clone the RL Baselines3 Zoo repository with the option to download trained agents using the recursive flag.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_zoo.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\ncd rl-baselines3-zoo/\n```\n\n----------------------------------------\n\nTITLE: Setting Up RST Documentation for Action Noise Module in Stable Baselines3\nDESCRIPTION: This RST code sets up documentation for the action noise module in Stable Baselines3. It creates a reference label for the noise section and specifies that the stable_baselines3.common.noise module should be autodocumented with all its members.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/noise.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _noise:\n\nAction Noise\n=============\n\n.. automodule:: stable_baselines3.common.noise\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Running All Unit Tests with Pytest\nDESCRIPTION: Command to run all the unit tests in Stable-Baselines3 using the pytest runner via a make command, which likely executes a predefined script in the project's Makefile.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmake pytest\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Ruff for Code Linting\nDESCRIPTION: Commands to install the ruff linter and check the code style of Stable-Baselines3 using a make command, which likely executes a predefined script in the project's Makefile.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npip install ruff\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Running a Single Test File with Pytest\nDESCRIPTION: Command to run a specific test file in Stable-Baselines3 using the pytest runner with verbose output. This example tests the environment checker functionality.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pytest -v tests/test_env_checker.py\n```\n\n----------------------------------------\n\nTITLE: Running a Single Test Case with Pytest\nDESCRIPTION: Command to run a specific test case in Stable-Baselines3 using the pytest runner with the -k flag to select tests by name. This example runs only the 'test_check_env_dict_action' test.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pytest -v -k 'test_check_env_dict_action'\n```\n\n----------------------------------------\n\nTITLE: Importing Environment Utility Functions in Python\nDESCRIPTION: Example showing how to properly import environment utilities after they were renamed from cmd_util to env_util in Stable Baselines3 version 0.9.0.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.cmd_util import make_atari_env, make_vec_env\nfrom stable_baselines3.common.utils import set_random_seed\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Mypy for Type Checking\nDESCRIPTION: Commands to install mypy and perform static type checking on the Stable-Baselines3 codebase using a make command, which likely executes a predefined script in the project's Makefile.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/README.md#2025-04-16_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install mypy\nmake type\n```\n\n----------------------------------------\n\nTITLE: Setting NumPy Boolean Type for Backward Compatibility\nDESCRIPTION: Code to monkey-patch NumPy's boolean type to maintain compatibility with Gym 0.21 when using NumPy 1.24+, addressing a breaking change in NumPy where np.bool was deprecated.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/misc/changelog.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnp.bool = bool\n```\n\n----------------------------------------\n\nTITLE: NumPy NaN Propagation Example\nDESCRIPTION: Demonstrates how NaN values propagate through NumPy operations without raising warnings.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/checking_nan.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nnp.seterr(all=\"raise\")  # define before your code.\n\nprint(\"numpy propagation test:\")\n\na = np.float64(\"NaN\")\nb = np.float64(1.0)\nval = a + b  # this will neither warn nor raise anything\nprint(val)\n```\n\n----------------------------------------\n\nTITLE: Importing Atari Wrappers Module in Python\nDESCRIPTION: This snippet demonstrates how to use the automodule directive to generate documentation for the Atari wrappers module in Stable Baselines3. It includes all members of the module.\nSOURCE: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/common/atari_wrappers.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: stable_baselines3.common.atari_wrappers\n  :members:\n```"
  }
]
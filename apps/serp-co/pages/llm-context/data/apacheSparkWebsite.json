[
  {
    "owner": "apache",
    "repo": "spark-website",
    "content": "TITLE: Initializing SparkSession in PySpark\nDESCRIPTION: This snippet initializes a SparkSession, the entry point for using PySpark. It creates an instance of SparkSession which can be used for DataFrame operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Explicit Schema in PySpark\nDESCRIPTION: This snippet illustrates creating a DataFrame with an explicit schema defined as a string. This is useful when you need to enforce a specific structure on the DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Rows\nDESCRIPTION: Demonstrates multiple ways to create a PySpark DataFrame using Row objects, explicit schemas, and pandas DataFrames with various data types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create DataFrame from Rows\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark DataFrame from Pandas DataFrame\nDESCRIPTION: This code snippet demonstrates how to create a PySpark DataFrame from a pandas DataFrame.  It first creates a pandas DataFrame with sample data. Then, it uses `spark.createDataFrame` to convert the pandas DataFrame into a PySpark DataFrame. This allows you to leverage existing pandas code within a PySpark environment.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: This snippet demonstrates how to initialize a SparkSession, which is the entry point to using PySpark functionalities. The session can be created using the builder pattern. It's required to run any PySpark code.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: Creates a SparkSession, which is the entry point for PySpark applications. This is typically the first step in any PySpark script.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Complete PySpark ETL Example with unittest\nDESCRIPTION: Provides a complete example of a PySpark ETL process, including the transformation function and a unittest class to test it. This snippet demonstrates how to set up the SparkSession, define the transformation, and write a test case.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# pkg/etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n\n# Define DataFrame transformation function\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column using regexp_replace\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n\n    return df_transformed\n```\n\nLANGUAGE: python\nCODE:\n```\n# pkg/test_etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \n\n# Define unit test base class\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n        \n# Define unit test\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                        {\"name\": \"Alice   G.\", \"age\": 25}, \n                        {\"name\": \"Bob  T.\", \"age\": 35}, \n                        {\"name\": \"Eve   A.\", \"age\": 28}] \n                \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n    \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n    \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\nLANGUAGE: python\nCODE:\n```\nunittest.main(argv=[''], verbosity=0, exit=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: This code snippet initializes a SparkSession, which is the entry point to PySpark functionality. It uses the builder pattern to get an existing SparkSession or create a new one if one doesn't already exist. This is necessary before performing any DataFrame operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Complete Example of PySpark ETL with Unit Test\nDESCRIPTION: This example integrates all concepts by defining an ETL process with DataFrame creation, transformation, and unit tests using unittest for validation. PySpark, unittest, and pyspark.testing.utils are required to check transformation integrity.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# pkg/etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n\n# Define DataFrame transformation function\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column using regexp_replace\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n\n    return df_transformed\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# pkg/test_etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \n\n# Define unit test base class\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n        \n# Define unit test\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                        {\"name\": \"Alice   G.\", \"age\": 25}, \n                        {\"name\": \"Bob  T.\", \"age\": 35}, \n                        {\"name\": \"Eve   A.\", \"age\": 28}] \n                \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n    \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n    \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n\n```\n\n----------------------------------------\n\nTITLE: Querying JSON Data with Spark SQL\nDESCRIPTION: Demonstrates reading JSON data from S3 and joining it with existing tables using Spark SQL. Shows how to register temporary tables and perform SQL joins across different data sources.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/sql/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nspark.read.json(\"s3n://...\").registerTempTable(\"json\")\nresults = spark.sql(\"\"\"\nSELECT * \n     FROM people\n     JOIN json ...\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Registering DataFrame as Temporary View for SQL Queries in PySpark\nDESCRIPTION: Creates a temporary view from a DataFrame and executes a SQL count query on it. This demonstrates how DataFrames can be easily accessed through SQL after registration.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Complete PySpark ETL Example with Unit Tests\nDESCRIPTION: Provides a complete example of a PySpark ETL process with corresponding unit tests using unittest. This includes the main ETL logic and the test suite.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# pkg/etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n\n# Define DataFrame transformation function\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column using regexp_replace\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n\n    return df_transformed\n```\n\nLANGUAGE: python\nCODE:\n```\n# pkg/test_etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \n\n# Define unit test base class\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n        \n# Define unit test\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                        {\"name\": \"Alice   G.\", \"age\": 25}, \n                        {\"name\": \"Bob  T.\", \"age\": 35}, \n                        {\"name\": \"Eve   A.\", \"age\": 28}] \n                \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n    \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n    \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\nLANGUAGE: python\nCODE:\n```\nunittest.main(argv=[''], verbosity=0, exit=False)\n```\n\n----------------------------------------\n\nTITLE: Registering and Invoking UDFs in PySpark SQL\nDESCRIPTION: Demonstrates registering a Python function as a UDF and invoking it in a SQL query within PySpark. Dependencies include PySpark and Pandas.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Getting DataFrame Columns and Schema in PySpark\nDESCRIPTION: This snippet illustrates how to retrieve a list of the column names and print the schema of a DataFrame, which is essential for understanding the structure of the data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.columns\n\ndf.printSchema()\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries with Python in Spark\nDESCRIPTION: Example showing how to execute SQL queries and map results using Spark SQL in Python. The code performs a simple SELECT query and maps over the results to extract names.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/sql/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nresults = spark.sql(\"SELECT * FROM people\")\nnames = results.map(lambda p: p.name)\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Views and Executing SQL Queries in PySpark\nDESCRIPTION: Demonstrates how to create a temporary view from a DataFrame and run SQL queries against it. This allows for utilizing SQL syntax to query DataFrame data in Spark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Applying Function to Grouped DataFrames\nDESCRIPTION: This snippet showcases applying a user-defined function that modifies values in a grouped DataFrame using `applyInPandas`.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: Create a SparkSession, which is the entry point for PySpark DataFrame operations. In PySpark shell, this is automatically created.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: Creates a new SparkSession which is the entry point for PySpark functionality\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Mixing SQL Expressions with PySpark Column Operations\nDESCRIPTION: This snippet demonstrates how to use SQL expressions directly with DataFrame operations using selectExpr and expr functions. It shows both using a registered UDF and a SQL aggregate function as DataFrame column expressions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas DataFrame to PySpark DataFrame\nDESCRIPTION: Demonstrates how to create a PySpark DataFrame from a pandas DataFrame with various data types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing Data Formats\nDESCRIPTION: Shows how to write and read DataFrames in different file formats including CSV, Parquet, and ORC, which are common in big data processing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# CSV Operations\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n\n# Parquet Operations\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n\n# ORC Operations\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Registering and Using Pandas UDFs in Spark SQL\nDESCRIPTION: Shows how to define a pandas UDF (User Defined Function) that adds one to each value in a Series, register it with Spark SQL, and invoke it in a SQL query. This enables efficient vectorized operations within SQL queries.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Grouping Data in PySpark DataFrame\nDESCRIPTION: This snippet demonstrates how to create a DataFrame and group data by a specified column using the split-apply-combine strategy.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark DataFrame with Explicit Schema\nDESCRIPTION: This code snippet creates a PySpark DataFrame from a list of tuples with an explicitly defined schema.  The `spark.createDataFrame` function is used with a list of tuples and a schema string that defines the column names and data types. This allows for more control over the DataFrame's structure.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Pandas UDFs\nDESCRIPTION: This snippet defines a Pandas UDF to apply a function that adds one to each element of a Series and uses it on a DataFrame column.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Data in PySpark\nDESCRIPTION: Demonstrates how to group data by a column and apply an aggregation function (average) to the grouped data. This is a common operation for data analysis.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()\n\ndf.groupby('color').avg().show()\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark DataFrame from list of Rows\nDESCRIPTION: This code snippet creates a PySpark DataFrame from a list of Row objects. It imports necessary modules like `datetime`, `date`, and `Row`. The `spark.createDataFrame` function is used to create the DataFrame from the provided list of Row objects. The Row object allows you to create a structured row of data with named columns.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating a SQL View from a DataFrame in PySpark\nDESCRIPTION: This snippet demonstrates how to register a DataFrame as a temporary view and execute SQL queries against it. It creates a temporary view named 'tableA' and runs a SQL query to count the number of rows.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: Creates the entry point SparkSession object for PySpark applications. This is the first step required for any PySpark application.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Writing PySpark Tests with Pytest\nDESCRIPTION: Shows how to use pytest to write tests for DataFrame transformations in PySpark. It validates transformation logic and compares expected results using assertions. Requires pytest fixtures and pyspark.testing.utils for testing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n\n```\n\n----------------------------------------\n\nTITLE: Complete ETL Module Implementation for Testing\nDESCRIPTION: A complete implementation of a PySpark ETL module including imports, SparkSession initialization, sample data creation, and the transformation function. This represents the code that would be tested by the unit tests.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# pkg/etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n\n# Define DataFrame transformation function\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column using regexp_replace\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n\n    return df_transformed\n```\n\n----------------------------------------\n\nTITLE: DataFrame Grouping and Aggregation\nDESCRIPTION: Demonstrates grouping data, applying aggregation functions, and using pandas-based group transformations in PySpark DataFrames.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('color').avg().show()\n\ndef plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns with Select Method\nDESCRIPTION: This snippet shows how to select specific columns from a DataFrame using the `select()` method, which will trigger the evaluation when `show()` is called.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.select(df.c).show()\n```\n\n----------------------------------------\n\nTITLE: Mixing SQL Expressions with PySpark Column Operations\nDESCRIPTION: Demonstrates how to use SQL expressions directly in DataFrame operations using selectExpr() and expr() functions. This allows combining the flexibility of SQL syntax with the programmatic interface of DataFrames.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Merging Ordered DataFrames in PySpark\nDESCRIPTION: This snippet showcases how to perform co-grouping and merging of two DataFrames based on common keys, leveraging pandas functionality for combined data processing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n    merge_ordered, schema='time int, id int, v1 double, v2 string').show()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Explicit Schema\nDESCRIPTION: This snippet shows how to create a PySpark DataFrame by defining an explicit schema, allowing control over the structure and types of the DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\n```\n\n----------------------------------------\n\nTITLE: Registering and Using UDF in Spark SQL\nDESCRIPTION: Demonstrates how to create a Pandas UDF that adds one to a value, register it for SQL use, and execute it in a SQL query. Uses the pandas_udf decorator for vectorized operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Showing Summary of DataFrame in PySpark\nDESCRIPTION: This snippet demonstrates how to show the summary statistics of specified columns in a DataFrame, which can provide valuable insights into the data distribution and characteristics.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.select(\"a\", \"b\", \"c\").describe().show()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Row Objects\nDESCRIPTION: Demonstrates creation of a PySpark DataFrame using Row objects with different data types including dates and timestamps.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Parquet Files\nDESCRIPTION: This snippet showcases how to write a DataFrame to a Parquet file format and read the data back into a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n```\n\n----------------------------------------\n\nTITLE: Using PySpark Built-in Test Utils\nDESCRIPTION: Demonstrates the use of PySpark's built-in testing utilities for DataFrame comparison.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Selecting Column from DataFrame in PySpark\nDESCRIPTION: This snippet illustrates how to access a specific column in a DataFrame by directly referencing it, which does not evaluate the underlying data but returns a Column instance.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.a\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Pandas UDF\nDESCRIPTION: Shows how to create and use a Pandas UDF (User Defined Function) to process DataFrame columns using pandas Series operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet with pandas API on Spark\nDESCRIPTION: Demonstrates how to read a Parquet file and perform group by aggregation using pandas API on Spark. This implementation leverages Spark's optimization and parallel processing capabilities.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/pandas-on-spark/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.pandas as ps\n\ndf = ps.read_parquet(\"G1_1e9_1e2_0_0.parquet\")[\n    [\"id1\", \"id2\", \"v3\"]\n]\ndf.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to Pandas DataFrame in PySpark\nDESCRIPTION: This snippet shows how to convert a PySpark DataFrame back into a pandas DataFrame, enabling the use of pandas-specific data manipulation functions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.toPandas()\n```\n\n----------------------------------------\n\nTITLE: Pandas UDF and Function Application\nDESCRIPTION: Shows how to apply custom functions to PySpark DataFrames using pandas UDFs and mapInPandas, enabling vectorized and flexible data transformations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Applying User Defined Function (UDF) in PySpark\nDESCRIPTION: This snippet shows how to define and apply a Pandas User Defined Function (UDF) to a DataFrame column. The UDF increments each value of the selected column by one.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Asserting DataFrame Equality Using PySpark Utilities\nDESCRIPTION: This snippet shows how to use PySpark's assertDataFrameEqual utility to test DataFrame equality, useful in ad-hoc validations. DataFrames are created and compared to ensure they are equivalent. Requires pyspark.testing.utils for assert function.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Pandas DataFrame in PySpark\nDESCRIPTION: This snippet showcases how to create a PySpark DataFrame directly from a pandas DataFrame, allowing the use of familiar pandas operations while leveraging Spark's distributed computing capabilities.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Rows in PySpark\nDESCRIPTION: Demonstrates creation of DataFrame using Row objects with various data types including dates and timestamps\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Explicit Schema in PySpark\nDESCRIPTION: Creates a DataFrame using tuple data and explicit schema definition\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Views and Running SQL Queries in PySpark\nDESCRIPTION: This snippet demonstrates how to create a temporary view from a DataFrame and execute a SQL query to count rows. It showcases the seamless integration between DataFrame and Spark SQL.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Spark Session with Spark Connect in Python\nDESCRIPTION: This snippet demonstrates how to create a remote Spark session using Spark Connect. It connects to the Spark server running on localhost:15002.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Working with DataFrame Groups\nDESCRIPTION: Demonstrates grouping operations on DataFrame including creation of grouped data and applying aggregation functions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.groupby('color').avg().show()\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas DataFrame to PySpark DataFrame\nDESCRIPTION: Demonstrates conversion from pandas DataFrame to PySpark DataFrame\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Parquet in PySpark\nDESCRIPTION: This snippet illustrates how to write a DataFrame to a Parquet file, which is an efficient columnar storage format suitable for large datasets.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n```\n\n----------------------------------------\n\nTITLE: Reading and Filtering CSV Data with PySpark\nDESCRIPTION: This snippet demonstrates how to read a CSV file into a Spark DataFrame, select specific columns, and filter the data based on a condition.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.read.csv(\"accounts.csv\", header=True)\n\n# Select subset of features and filter for balance > 0\nfiltered_df = df.select(\"AccountBalance\", \"CountOfDependents\").filter(\"AccountBalance > 0\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying a DataFrame with the Remote Spark Session\nDESCRIPTION: Creates a sample DataFrame using the remote Spark session with various data types including integers, floats, strings, dates, and timestamps. The DataFrame is then displayed to verify the connection is working.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Grouping Data and Calculating Average\nDESCRIPTION: This code snippet demonstrates how to group data in a PySpark DataFrame and calculate the average for each group. It groups the DataFrame by the 'color' column and calculates the average of the other numeric columns using the `avg()` function. The results are then displayed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('color').avg().show()\n```\n\n----------------------------------------\n\nTITLE: Showing Summary Statistics of DataFrame Columns\nDESCRIPTION: This code snippet calculates and displays summary statistics (count, mean, stddev, min, max) for specified columns ('a', 'b', 'c') of a DataFrame. It uses `df.select()` to choose the columns, `describe()` to generate the statistics, and `show()` to display the results.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.select(\"a\", \"b\", \"c\").describe().show()\n```\n\n----------------------------------------\n\nTITLE: Integrating SQL Expressions in DataFrame Operations\nDESCRIPTION: Demonstrates how to use SQL expressions directly within DataFrame operations using selectExpr and expr functions. This enables mixing SQL syntax with DataFrame API for more flexible data transformations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Creating Spark DataFrame with Sample Data\nDESCRIPTION: Creates a DataFrame with first_name and age columns containing four rows of sample data\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/examples.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame(\n    [\n        (\"sue\", 32),\n        (\"li\", 3),\n        (\"bob\", 75),\n        (\"heo\", 13),\n    ],\n    [\"first_name\", \"age\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Remote Spark Session\nDESCRIPTION: The snippet connects to a remote Spark session using Spark Connect configured at 'localhost:15002'. It assumes the remote Spark Connect server is running and accessible.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Creating Temp View and Executing SQL Query in PySpark\nDESCRIPTION: Shows how to create a temporary view from a DataFrame and execute a SQL count query using Spark SQL. Demonstrates the basic interoperability between DataFrames and SQL.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Adding New Column with Transformation in PySpark\nDESCRIPTION: This snippet shows how to create a new column in a DataFrame that applies a transformation to an existing column, demonstrating the mutability of DataFrames.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.withColumn('upper_c', upper(df.c)).show()\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Spark Connect Session\nDESCRIPTION: Establishes a remote connection to the Spark Connect server running on localhost:15002.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from List of Rows\nDESCRIPTION: This snippet illustrates how to create a PySpark DataFrame by passing a list of Row objects. Each Row represents a record and can include various data types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf\n```\n\n----------------------------------------\n\nTITLE: Setting Up Unit Tests for PySpark with unittest\nDESCRIPTION: Demonstrates how to set up a base test class for PySpark using Python's unittest framework. This includes methods for creating and tearing down a SparkSession.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Rows in PySpark\nDESCRIPTION: This snippet demonstrates how to create a PySpark DataFrame by passing a list of Row objects, providing a straightforward way to define data using columns from Python's datetime and date libraries.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf\n```\n\n----------------------------------------\n\nTITLE: Stop Existing Spark Session with PySpark\nDESCRIPTION: This code stops any existing local Spark sessions to ensure no conflict with the remote Spark Connect session. PySpark should be installed, and a Spark session should be active locally before executing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session in PySpark\nDESCRIPTION: Before starting a new remote Spark Connect session, this PySpark code snippet halts any existing local Spark sessions to avoid conflicts. It initializes a SparkSession and immediately stops it.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Data with MLlib in Python\nDESCRIPTION: Example showing how to load data in libsvm format from HDFS and train a K-means clustering model using Spark MLlib. The code demonstrates the simple API for machine learning tasks in Spark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/mllib/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = spark.read.format(\"libsvm\")\\\n    .load(\"hdfs://...\")\n\nmodel = KMeans(k=10).fit(data)\n```\n\n----------------------------------------\n\nTITLE: Writing unittest for PySpark Transformation\nDESCRIPTION: Shows how to write a unittest class to test the PySpark transformation function, using assertDataFrameEqual to compare the transformed DataFrame with the expected result.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Applying User-Defined Function in PySpark\nDESCRIPTION: Shows how to create and apply a Pandas UDF (User-Defined Function) to a PySpark DataFrame. This example adds 1 to each value in a column using pandas Series operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Spark Session in Python\nDESCRIPTION: Creates a new SparkSession or gets an existing one with the application name 'demo'\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/examples.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"demo\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Mixing SQL Expressions with DataFrame Operations\nDESCRIPTION: Shows how to use SQL expressions directly within DataFrame operations using selectExpr and expr functions. Demonstrates the flexibility of combining SQL and DataFrame APIs.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Printing DataFrame Schema\nDESCRIPTION: This code snippet prints the schema of a PySpark DataFrame using the `df.printSchema()` method. The schema includes column names, data types, and whether the columns are nullable. This provides detailed information about the structure of the DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.printSchema()\n```\n\n----------------------------------------\n\nTITLE: Applying Function on Grouped Data in PySpark\nDESCRIPTION: This snippet illustrates the application of a custom function on grouped data using the applyInPandas method, demonstrating the flexibility to utilize pandas operations on groups.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Parquet Files\nDESCRIPTION: This code snippet demonstrates writing a PySpark DataFrame to a Parquet file and reading it back. `df.write.parquet('bar.parquet')` writes the DataFrame to a Parquet file named 'bar.parquet'. `spark.read.parquet('bar.parquet').show()` reads the Parquet file back into a DataFrame and displays it.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows in DataFrame in PySpark\nDESCRIPTION: This snippet illustrates how to filter rows in a DataFrame based on column values using the filter() method, enabling selective data analysis.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.filter(df.a == 1).show()\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Server with Spark Connect Support in Bash\nDESCRIPTION: This snippet shows how to start a Spark server with Spark Connect support using the start-connect-server.sh script. It includes loading environment variables and specifying the Spark Connect package.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\nsource ~/.profile # Make sure environment variables are loaded.\n$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Registering and Using Pandas UDFs in Spark SQL\nDESCRIPTION: Defines a Pandas UDF to add one to each value in a Series, registers it with Spark SQL, and executes a SQL query that uses this function. Shows how to extend SQL capabilities with custom functions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Writing PySpark Tests with pytest\nDESCRIPTION: Shows how to write a test for a PySpark transformation function using pytest and assertDataFrameEqual. This test ensures the function removes extra spaces correctly.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading ORC Files\nDESCRIPTION: This code snippet demonstrates writing a PySpark DataFrame to an ORC file and reading it back. `df.write.orc('zoo.orc')` writes the DataFrame to an ORC file named 'zoo.orc'. `spark.read.orc('zoo.orc').show()` reads the ORC file back into a DataFrame and displays it.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Collecting Data to Driver\nDESCRIPTION: This code snippet demonstrates how to collect all the data from a PySpark DataFrame to the driver node using `df.collect()`.  This returns a list of Row objects containing the DataFrame's data.  Be cautious when using this on large DataFrames, as it can cause out-of-memory errors on the driver node.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Pandas DataFrame\nDESCRIPTION: This snippet illustrates how to convert a pandas DataFrame to a PySpark DataFrame, enabling the use of pandas functionalities in Spark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying DataFrame\nDESCRIPTION: Demonstrates creating a DataFrame with various data types including integers, floats, strings, dates, and timestamps, then displays it using the show() method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from List of Rows\nDESCRIPTION: This snippet creates a PySpark DataFrame from a list of Row objects, demonstrating how to initialize a DataFrame with complex data types such as dates and timestamps.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf\n```\n\n----------------------------------------\n\nTITLE: Writing PySpark Tests with unittest\nDESCRIPTION: Demonstrates how to write a test class using unittest that tests the remove_extra_spaces transformation function. The test creates input and expected DataFrames and uses assertDataFrameEqual to verify the transformation works correctly.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Spark Session in PySpark Application\nDESCRIPTION: This snippet illustrates the creation of a Spark Session in PySpark. The SparkSession is initialized with an application name, allowing for DataFrame operations. It requires the pyspark.sql library.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n```\n\n----------------------------------------\n\nTITLE: PySpark Schema Comparison Test\nDESCRIPTION: Shows how to compare DataFrame schemas using assertSchemaEqual\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Registering DataFrame as SQL Table and Executing Query in PySpark\nDESCRIPTION: This snippet shows how to create a temporary view from a DataFrame and run a SQL query against it. Ensure that PySpark is correctly set up and the DataFrame is properly initialized. The primary parameter is the query string within the sql method, and the output is the result set displayed via the show method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing Streams in Spark with Scala\nDESCRIPTION: This Scala snippet demonstrates a basic pipeline for reading, processing, and writing streams using Spark Structured Streaming. It involves reading a stream, selecting JSON data, casting it to a string, and writing the stream with a specific trigger interval. Dependencies include Apache Spark with Structured Streaming capabilities. Key parameters are the schema of the JSON data and the trigger interval.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/streaming/index.md#2025-04-21_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nspark\\n  .\\readStream\\n  .\\select($\"value\".cast(\"string\").alias(\"jsonData\"))\\n  .\\select(from_json($\"jsonData\", jsonSchema).alias(\"payload\"))\\n  .\\writeStream\\n  .\\trigger(\"1 seconds\")\\n  .\\start()\n```\n\n----------------------------------------\n\nTITLE: PySpark Built-in Test Utils Example\nDESCRIPTION: Demonstrates usage of PySpark's built-in assertDataFrameEqual for DataFrame comparison\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame by Removing Extra Spaces\nDESCRIPTION: This function demonstrates removing extra spaces within a specified column of a PySpark DataFrame using a regular expression. The transformation requires the pyspark.sql.functions module and a DataFrame as input. It outputs the transformed DataFrame with cleaned data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading ORC Files\nDESCRIPTION: This snippet demonstrates writing a DataFrame to ORC file format and then reading the contents back into a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Registering and Using a Pandas UDF in Spark SQL\nDESCRIPTION: This example shows how to define a Pandas UDF (User Defined Function), register it with Spark SQL, and then invoke it in a SQL query. The function 'add_one' increments values in a Pandas Series by 1.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to ORC in PySpark\nDESCRIPTION: This snippet demonstrates writing a DataFrame to an ORC file, showcasing another efficient file format for data storage and retrieval in PySpark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame Rows\nDESCRIPTION: This code snippet demonstrates how to filter rows in a PySpark DataFrame using `df.filter()`. It filters the DataFrame to include only rows where the value of column 'a' is equal to 1, and then displays the filtered DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf.filter(df.a == 1).show()\n```\n\n----------------------------------------\n\nTITLE: Testing PySpark Schema Equality\nDESCRIPTION: Demonstrates how to compare two DataFrame schemas for equality using PySpark's assertSchemaEqual function. This is useful when you need to verify that a transformation preserves the expected schema structure.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Grouping Data and Calculating Average in PySpark\nDESCRIPTION: This snippet demonstrates how to group data in a DataFrame by specific criteria and calculate the average for each group using the avg() function.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('color').avg().show()\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows in a DataFrame\nDESCRIPTION: This snippet demonstrates how to filter rows in a DataFrame based on a column condition using the `filter()` method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.filter(df.a == 1).show()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Data to/from Parquet\nDESCRIPTION: This snippet demonstrates how to write a DataFrame to a Parquet file, an efficient file format, and read it back into a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Remote Spark Session (Python)\nDESCRIPTION: This code demonstrates how to create a DataFrame using the remote Spark session. It includes importing necessary modules, defining data, and creating the DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Running Unittest in Python Notebook\nDESCRIPTION: Demonstrates how to run unittest tests within a Python notebook environment. This code is beneficial for integrating tests in notebook workflows. Uses unittest.main to execute tests with specified arguments.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nunittest.main(argv=[''], verbosity=0, exit=False)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame in Remote Spark Session with PySpark\nDESCRIPTION: The snippet creates a DataFrame from a list of Row objects in a remote Spark session. Each Row contains multiple fields of varying data types such as integers, floats, strings, dates, and datetimes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Running PySpark Tests\nDESCRIPTION: Commands for running PySpark tests using the run-tests script.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ python/run-tests --testnames pyspark.sql.tests.test_arrow\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python/run-tests --testnames 'pyspark.sql.tests.test_arrow ArrowTests'\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python/run-tests --testnames 'pyspark.sql.tests.test_arrow ArrowTests.test_null_conversion'\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python/run-tests --testnames pyspark.sql.dataframe\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python/run-tests-with-coverage --testnames pyspark.sql.tests.test_arrow --python-executables=python\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying DataFrame with Spark Connect\nDESCRIPTION: Demonstrates creation of a DataFrame with various data types including integers, floats, strings, dates, and timestamps, followed by displaying the results.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading CSV Files\nDESCRIPTION: This snippet demonstrates how to write a DataFrame to a CSV file and read the data back into a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n```\n\n----------------------------------------\n\nTITLE: Querying JSON Data with Spark SQL\nDESCRIPTION: Shows how to query JSON data using Spark SQL syntax. This example selects first name, last name, and age fields from a JSON file and filters records where age is greater than 21.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  name.first AS first_name,\n  name.last AS last_name,\n  age\nFROM json.`logs.json`\n  WHERE age > 21;\n```\n\n----------------------------------------\n\nTITLE: Creating a Remote Spark Connect Session\nDESCRIPTION: Establishes a remote connection to the Spark Connect server running on localhost:15002. This creates a SparkSession that communicates with the remote Spark server.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Adding New Column to DataFrame\nDESCRIPTION: This code snippet demonstrates how to add a new column to a PySpark DataFrame using `df.withColumn()`. It adds a new column named 'upper_c', which contains the uppercase version of the 'c' column, and then displays the resulting DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.withColumn('upper_c', upper(df.c)).show()\n```\n\n----------------------------------------\n\nTITLE: Defining and Applying Transformation Function in PySpark\nDESCRIPTION: Defines a function to remove extra spaces from a specified column in a DataFrame and applies it. This showcases a typical data cleaning operation in PySpark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Mixing SQL Expressions with DataFrame Operations in PySpark\nDESCRIPTION: Illustrates using SQL expressions directly in PySpark DataFrame operations. Requires PySpark library and access to DataFrame 'df'.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Writing Pytest Test Cases\nDESCRIPTION: Implements a pytest test case for validating DataFrame transformation function.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Enabling YourKit profiler for Spark unit tests\nDESCRIPTION: Instructs how to enable the YourKit Java Profiler for Spark unit tests by adding Java options in the SBT settings (`SparkBuild.scala`). This setup allows performance profiling of test executions to identify bottlenecks during Spark development. Ensure that the path to the profiler agent is correctly specified in the configuration.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\njavaOptions in Test += \\\"-agentpath:/path/to/yjp\\\"\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to CSV in PySpark\nDESCRIPTION: This snippet shows how to write a DataFrame to a CSV file and read it back, demonstrating file output and input functionality in PySpark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Content and Schema\nDESCRIPTION: This code demonstrates how to display the contents of a DataFrame and print its schema. The `df.show()` method displays the DataFrame's data, while `df.printSchema()` prints the schema, including column names and data types. These methods are useful for inspecting the structure and contents of a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# All DataFrames above result same.\ndf.show()\ndf.printSchema()\n```\n\n----------------------------------------\n\nTITLE: Writing PySpark Tests with pytest\nDESCRIPTION: Shows how to write a test function using pytest that tests the remove_extra_spaces transformation. The test uses the spark_fixture to get a SparkSession, creates test DataFrames, and verifies the transformation results.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark_fixture.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark_fixture.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Optimizing SBT Build Process for Apache Spark\nDESCRIPTION: This snippet demonstrates how to reduce build times in Apache Spark development by avoiding re-creation of the assembly JAR. It shows the process of creating an initial package, using compiled classes, and recompiling after local development.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ build/sbt clean package\n$ ./bin/spark-shell\n$ export SPARK_PREPEND_CLASSES=true\n$ ./bin/spark-shell # Now it's using compiled classes\n# ... do some local development ... #\n$ build/sbt compile\n$ unset SPARK_PREPEND_CLASSES\n$ ./bin/spark-shell\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Documentation with Jekyll in Shell\nDESCRIPTION: This script generates the HTML version of the Spark documentation using Jekyll. The `jekyll` command is executed in the `docs` directory to transform markdown files into HTML format. By using `SKIP_SCALADOC=1 jekyll`, it skips building the scaladoc to save time. This requires Jekyll installed as a Ruby Gem.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.6.0/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nSKIP_SCALADOC=1 jekyll\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading PySpark DataFrame as Parquet\nDESCRIPTION: Demonstrates writing a PySpark DataFrame to a Parquet file and reading it back. Parquet is an efficient columnar storage format often used in big data processing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading PySpark DataFrame as CSV\nDESCRIPTION: Shows how to write a PySpark DataFrame to a CSV file and then read it back. CSV is a common format for data exchange and storage.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n```\n\n----------------------------------------\n\nTITLE: Testing with Python Unittest\nDESCRIPTION: Demonstrates how to write PySpark tests using the unittest framework. Includes how to set up a Spark session using setUpClass and tearDownClass methods. Requires unittest and pyspark.sql. Example shows a test case for verifying DataFrame transformations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame in PySpark\nDESCRIPTION: This snippet shows how to display the top rows of a DataFrame using the show() method, which is commonly used for quick inspections of data within a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1)\n```\n\n----------------------------------------\n\nTITLE: Creating a PySpark Fixture for pytest\nDESCRIPTION: Defines a pytest fixture that creates and yields a SparkSession. Fixtures in pytest provide a way to set up and tear down resources that are needed for tests, ensuring proper cleanup after tests are complete.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\n----------------------------------------\n\nTITLE: Adding New Columns with Functions\nDESCRIPTION: This snippet illustrates how to add a new column to a DataFrame with the result of a function applied to an existing column.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.withColumn('upper_c', upper(df.c)).show()\n```\n\n----------------------------------------\n\nTITLE: Using PySpark Built-in Test Utils\nDESCRIPTION: Demonstrates usage of PySpark's built-in testing utilities for DataFrame comparison.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Getting DataFrame Schema and Column Names\nDESCRIPTION: This snippet retrieves the column names and schema of a DataFrame, facilitating understanding of the data structure.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.columns\n\ndf.printSchema()\n```\n\n----------------------------------------\n\nTITLE: Avoiding Out-of-Memory Errors\nDESCRIPTION: This snippet shows alternative methods to retrieve data from a DataFrame safely without risking out-of-memory errors, such as using `take()`.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.take(1)\n```\n\n----------------------------------------\n\nTITLE: Highlighting Scala Code in Markdown with Jekyll\nDESCRIPTION: A syntax example showing how to mark code blocks for syntax highlighting in markdown documentation using Jekyll's Liquid templating system. This allows for proper code formatting when the documentation is built.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.6.2/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% highlight scala %}\n// Your scala code goes here, you can replace scala with many other\n// supported languages too.\n{% endhighlight %}\n```\n\n----------------------------------------\n\nTITLE: Using PySpark Built-in Test Utility Functions\nDESCRIPTION: Demonstrates the use of PySpark's built-in test utility functions like assertDataFrameEqual and assertSchemaEqual for simple ad-hoc validation cases.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Highlighting Scala Code in Markdown for Jekyll Processing\nDESCRIPTION: This snippet demonstrates how to mark a block of Scala code in Markdown for syntax highlighting by Jekyll during the compile phase. It uses Liquid tags to specify the language for highlighting.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% highlight scala %}\n// Your Scala code goes here, you can replace Scala with many other\n// supported languages too.\n{% endhighlight %}\n```\n\n----------------------------------------\n\nTITLE: Applying Pandas Function API to DataFrame\nDESCRIPTION: This code snippet shows how to apply a Pandas Function API using `DataFrame.mapInPandas`.  A function `pandas_filter_func` filters a pandas DataFrame to include only rows where column 'a' is equal to 1. The function is then applied to the PySpark DataFrame `df` using `mapInPandas` with the DataFrame's schema. The filtered results are displayed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef pandas_filter_func(iterator):\n    for pandas_df in iterator:\n        yield pandas_df[pandas_df.a == 1]\n\ndf.mapInPandas(pandas_filter_func, schema=df.schema).show()\n```\n\n----------------------------------------\n\nTITLE: Displaying PySpark DataFrame Content and Schema\nDESCRIPTION: Shows how to view the contents of a PySpark DataFrame using show() and how to display its schema using printSchema(). These are essential for data exploration.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# All DataFrames above result same.\ndf.show()\ndf.printSchema()\n```\n\n----------------------------------------\n\nTITLE: Creating a Base Test Class for PySpark with unittest\nDESCRIPTION: Defines a base test class that sets up and tears down a SparkSession for unittest-based PySpark tests. The @classmethod decorator is used to ensure these operations happen once per test class rather than for each test method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\n----------------------------------------\n\nTITLE: Selecting a Column from DataFrame\nDESCRIPTION: This code snippet demonstrates how to select a column from a PySpark DataFrame. Selecting a column does not trigger computation; it returns a `Column` instance for lazy evaluation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf.a\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Column Names\nDESCRIPTION: This snippet shows how to retrieve the column names of a PySpark DataFrame using the `df.columns` attribute. This provides a list of strings representing the DataFrame's columns, which can be useful for programmatic access and manipulation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.columns\n```\n\n----------------------------------------\n\nTITLE: Grouping Data in DataFrame\nDESCRIPTION: This snippet demonstrates how to group data in a DataFrame by a specified column and apply the average function to each group.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('color').avg().show()\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns Using select() in PySpark\nDESCRIPTION: This snippet demonstrates how to select one or more specific columns from a DataFrame and display their values using the show() method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf.select(df.c).show()\n```\n\n----------------------------------------\n\nTITLE: Exporting and Adding GPG Key to KEYS File\nDESCRIPTION: Commands for exporting the ASCII-armored public key and checking out the Spark distribution repository for updating the KEYS file.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ gpg --export --armor 26A27D33\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Move dev/ to release/ when the voting is completed. See Finalize the Release below\nsvn co --depth=files \"https://dist.apache.org/repos/dist/dev/spark\" svn-spark\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading CSV Files\nDESCRIPTION: This code snippet demonstrates writing a PySpark DataFrame to a CSV file and reading it back.  `df.write.csv('foo.csv', header=True)` writes the DataFrame to a CSV file named 'foo.csv' with a header row.  `spark.read.csv('foo.csv', header=True).show()` reads the CSV file back into a DataFrame and displays it.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n```\n\n----------------------------------------\n\nTITLE: Implementing a PySpark Transformation Function\nDESCRIPTION: Defines a function that removes extra spaces from text in a specified DataFrame column using the regexp_replace function. The function takes a DataFrame and column name as parameters and returns the transformed DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Jekyll Front Matter for Spark Release Post\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 3.3.4 release. Includes layout, title, categories, tags and publishing metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2023-12-16-spark-3-3-4-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.3.4 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 2.4.0 release. Defines post metadata including layout, title, categories, tags, and publishing status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2018-11-02-spark-2-4-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.4.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating a Random Forest Regressor with PySpark\nDESCRIPTION: This example shows how to create a DataFrame, split it into training and test sets, initialize and train a Random Forest Regressor model, and generate predictions on the test set.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Every record contains a label and feature vector\ndf = spark.createDataFrame(data, [\"label\", \"features\"])\n\n# Split the data into train/test datasets\ntrain_df, test_df = df.randomSplit([.80, .20], seed=42)\n\n# Set hyperparameters for the algorithm\nrf = RandomForestRegressor(numTrees=100)\n\n# Fit the model to the training data\nmodel = rf.fit(train_df)\n\n# Generate predictions on the test dataset.\nmodel.transform(test_df).show()\n```\n\n----------------------------------------\n\nTITLE: Reading and Filtering JSON Data with PySpark\nDESCRIPTION: This snippet demonstrates how to read a JSON file into a Spark DataFrame, filter the data based on a condition, and select specific columns to display.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.read.json(\"logs.json\")\ndf.where(\"age > 21\").select(\"name.first\").show()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Data to/from CSV\nDESCRIPTION: This snippet illustrates how to write a DataFrame to a CSV file and subsequently read it back. The header option can be set to include column names in the output.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n```\n\n----------------------------------------\n\nTITLE: Executing Spark Connect Query in Go\nDESCRIPTION: Demonstrates a Spark Connect query execution using the spark-connect-go library, showing how non-JVM languages can interact with Spark\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/spark-connect/index.md#2025-04-21_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nspark, _ := sql.SparkSession.Builder.Remote(remote).Build()\ndf, _ := spark.Sql(\"select * from my_cool_table where age > 42\")\ndf.Show(100, false)\n```\n\n----------------------------------------\n\nTITLE: Adding Computed Column to Spark DataFrame\nDESCRIPTION: Adds a life_stage column based on age conditions using when/otherwise logic\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/examples.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, when\n\ndf1 = df.withColumn(\n    \"life_stage\",\n    when(col(\"age\") < 13, \"child\")\n    .when(col(\"age\").between(13, 19), \"teenager\")\n    .otherwise(\"adult\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Column-wise Operations in PySpark\nDESCRIPTION: This snippet shows that most operations applied to columns return Column instances, allowing for further operations to be chained without triggering computation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import Column\nfrom pyspark.sql.functions import upper\n\ntype(df.c) == type(upper(df.c)) == type(df.c.isNull())\n```\n\n----------------------------------------\n\nTITLE: Applying Pandas Function to Grouped Data\nDESCRIPTION: This code snippet demonstrates how to apply a Pandas function to grouped data in a PySpark DataFrame using `applyInPandas`. It defines a function `plus_mean` that subtracts the mean of the 'v1' column from each value in that column within each group. The data is grouped by the 'color' column, and the `plus_mean` function is applied to each group. The results are displayed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading PySpark DataFrame as ORC\nDESCRIPTION: Illustrates how to write a PySpark DataFrame to an ORC file and read it back. ORC is another columnar storage format optimized for large-scale data processing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Writing Unit Tests for PySpark Transformations\nDESCRIPTION: Shows how to write a unit test for a PySpark transformation function using unittest and assertDataFrameEqual. This test ensures the function removes extra spaces correctly.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Spark Session (Python)\nDESCRIPTION: This code creates a remote Spark session using Spark Connect, connecting to the server running on localhost:15002.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 2.0.0 release announcement blog post, including layout, title, categories, publication status and WordPress metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2016-07-26-spark-2-0-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.0.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame in PySpark\nDESCRIPTION: This code creates a DataFrame from sample data. This DataFrame can be used for various operations like transformations or assertions. Dependencies include a Spark session initialized from the pyspark.sql module.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n\n```\n\n----------------------------------------\n\nTITLE: Jekyll Post Frontmatter Configuration in YAML\nDESCRIPTION: YAML frontmatter block defining the Jekyll blog post metadata including layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2015-05-15-one-month-to-spark-summit-2015.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: One month to Spark Summit 2015 in San Francisco\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Creating and Transforming Graphs in GraphX with Scala\nDESCRIPTION: This snippet demonstrates basic GraphX operations including creating a graph from vertices and edges, loading data from HDFS, and using the joinVertices operation to combine graph data with external messages. It showcases GraphX's ability to seamlessly work with both graph structures and RDDs.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/graphx/index.md#2025-04-21_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\ngraph = Graph(vertices, edges)\nmessages = spark.textFile(\"hdfs://...\")\ngraph2 = graph.joinVertices(messages) {\n  (id, vertex, msg) => ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Explicit Schema\nDESCRIPTION: Shows how to create a DataFrame with an explicitly defined schema using tuples of data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame in PySpark\nDESCRIPTION: Demonstrates how to create a DataFrame from sample data in PySpark. This is a common operation when working with structured data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Collecting Data to Driver\nDESCRIPTION: This snippet illustrates how to collect the entire DataFrame to the driver side. Note that this can lead to out-of-memory errors with large datasets.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Initializing PySpark Session\nDESCRIPTION: Creates a new SparkSession for a PySpark application with a specified app name.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Views and Executing SQL Queries - Python\nDESCRIPTION: This snippet demonstrates how to register a DataFrame as a temporary view and execute a SQL query to count the entries in that view. It highlights the seamless integration between DataFrames and SQL within PySpark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Hive-Style Column Replacement Syntax\nDESCRIPTION: Added support for Hive-style REPLACE COLUMNS syntax in table alter operations, allowing more flexible schema modifications\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2021-03-02-spark-release-3-1-1.md#2025-04-21_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE table_name REPLACE COLUMNS (new_column_definition)\n```\n\n----------------------------------------\n\nTITLE: Testing DataFrames with Approximate Equality\nDESCRIPTION: Shows how to use assertDataFrameEqual with a relative tolerance parameter to test for approximate equality between DataFrames containing floating-point numbers. This is useful when exact comparisons are too strict.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Spark Session in PySpark\nDESCRIPTION: This snippet establishes a remote Spark session connected to a locally running Spark Connect server. The SparkSession.builder.remote method is used to connect to 'localhost:15002'.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Defining Sitemap URL for Apache Spark Website\nDESCRIPTION: Provides the canonical sitemap location for the Apache Spark project website, enabling web crawlers and search engines to discover and index site content\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/robots.txt#2025-04-21_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\nSitemap: https://spark.apache.org/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Core Package Structure in Scala\nDESCRIPTION: Lists the primary packages in the Scala standard library, including collections, concurrency, I/O, math, system interaction, and text processing utilities\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/bagel/lib/rootdoc.txt#2025-04-21_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nscala.collection\nscala.collection.immutable\nscala.collection.mutable\nscala.collection.parallel\nscala.actors\nscala.io\nscala.math\nscala.sys\nscala.util.matching\nscala.util.parsing.combinator\nscala.xml\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Pandas UDFs in PySpark\nDESCRIPTION: This snippet shows how to define a Pandas UDF (User Defined Function) that operates on pandas Series, allowing for vectorized operations within Spark DataFrames.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Showing Top Rows of DataFrame\nDESCRIPTION: This code snippet shows how to display the top rows of a PySpark DataFrame using the `show()` method.  The `show(1)` method displays the first row of the DataFrame.  The `show()` method can be used to view a sample of the data for inspection.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1)\n```\n\n----------------------------------------\n\nTITLE: Checking Binary Compatibility with MiMa\nDESCRIPTION: Command for running MiMa to check binary compatibility.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ dev/mima\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Vertically in PySpark\nDESCRIPTION: This snippet shows how to display rows of a DataFrame vertically, which is beneficial when rows contain long data that does not fit into horizontal tables.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1, vertical=True)\n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session in Python\nDESCRIPTION: This code stops the existing regular Spark session to prepare for creating a remote Spark Connect session. It's necessary because regular and remote Spark sessions cannot coexist.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Rows and Schema\nDESCRIPTION: This snippet demonstrates how to display the rows and schema of a DataFrame using `show()` and `printSchema()` methods. It also shows how to enable eager evaluation in PySpark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.show()\ndf.printSchema()\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Spark Session\nDESCRIPTION: Establishes a remote connection to the Spark Connect server running on localhost:15002.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Performing Column Operations\nDESCRIPTION: This snippet demonstrates the creation of new `Column` instances and using them to manipulate DataFrame columns without triggering computation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import Column\nfrom pyspark.sql.functions import upper\n\ntype(df.c) == type(upper(df.c)) == type(df.c.isNull())\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample PySpark DataFrame\nDESCRIPTION: Defines sample data as a list of dictionaries and creates a PySpark DataFrame from it. Each dictionary in the list becomes a row in the DataFrame with keys as column names.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Automatic Imports and Type Aliases\nDESCRIPTION: Demonstrates how Scala automatically imports identifiers from the scala package and Predef object, including type aliases that provide shortcuts to commonly used classes\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/bagel/lib/rootdoc.txt#2025-04-21_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n// Automatic imports\nList   // Alias for scala.collection.immutable.List\nString // Alias for java.lang.String\n```\n\n----------------------------------------\n\nTITLE: Referencing I/O Package\nDESCRIPTION: Shows how to reference the I/O package in Scala, which provides functionality for input and output operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_6\n\nLANGUAGE: Scala\nCODE:\n```\nscala.[[scala.io]]\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to Pandas DataFrame\nDESCRIPTION: This code snippet demonstrates how to convert a PySpark DataFrame to a pandas DataFrame using `df.toPandas()`.  This allows you to leverage the pandas API for data analysis and manipulation. Note that `toPandas()` collects all the data into the driver node, which can cause out-of-memory errors for large datasets.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf.toPandas()\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet with Standard pandas\nDESCRIPTION: Shows the basic pandas implementation for reading a Parquet file and performing group by aggregation. This version fails due to memory limitations with large datasets.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/pandas-on-spark/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.read_parquet(\"G1_1e9_1e2_0_0.parquet\")[\n    [\"id1\", \"id2\", \"v3\"]\n]\ndf.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n```\n\n----------------------------------------\n\nTITLE: Configuring YourKit profiler for Spark daemon\nDESCRIPTION: Sets up the YourKit Java Profiler agent for Spark daemons by modifying `SPARK_DAEMON_JAVA_OPTS` to include the agent path. This configuration ensures that the JVM is started with the profiler agent for performance monitoring. No external dependencies are required apart from a valid installation of YourKit Java Profiler.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nSPARK_DAEMON_JAVA_OPTS+=\\\" -agentpath:/root/YourKit-JavaProfiler-2017.02/bin/linux-x86-64/libyjpagent.so=sampling\\\"\nexport SPARK_DAEMON_JAVA_OPTS\n```\n\n----------------------------------------\n\nTITLE: Applying Pandas UDF to DataFrame\nDESCRIPTION: This code snippet demonstrates how to apply a Pandas UDF (User Defined Function) to a PySpark DataFrame. It defines a function `pandas_plus_one` that adds 1 to a pandas Series and then registers it as a Pandas UDF. The UDF is then applied to the 'a' column of the DataFrame, and the result is displayed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session\nDESCRIPTION: Stops any existing local Spark session before creating a remote Spark Connect session. This is necessary because regular and remote sessions cannot coexist.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Testing DataFrames with PySpark's Built-in Utility Functions\nDESCRIPTION: Demonstrates the use of PySpark's assertDataFrameEqual function to compare two DataFrames for equality. This is useful for simple validation cases where you want to verify that a transformation produces the expected results.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\n----------------------------------------\n\nTITLE: Setting Up Unittest Framework\nDESCRIPTION: Implements a base test class using unittest framework with SparkSession setup and teardown.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\n----------------------------------------\n\nTITLE: Referencing Math Package\nDESCRIPTION: Demonstrates how to reference the math package in Scala, which contains basic math functions and additional numeric types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_7\n\nLANGUAGE: Scala\nCODE:\n```\nscala.[[scala.math]]\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Contents\nDESCRIPTION: This snippet shows how to display the top rows of a DataFrame using the DataFrame.show() method, which is helpful for previewing the data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1)\n```\n\n----------------------------------------\n\nTITLE: Complete PySpark ETL and Unittest Example\nDESCRIPTION: Illustrates a complete PySpark ETL process with a Spark session setup, DataFrame transformation, and unittest integration. Dependencies include pyspark.sql and unittest. Combines all steps into a full example for educational purposes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# pkg/etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n\n# Define DataFrame transformation function\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column using regexp_replace\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n\n    return df_transformed\n```\n\nLANGUAGE: python\nCODE:\n```\n# pkg/test_etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \n\n# Define unit test base class\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n        \n# Define unit test\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                        {\"name\": \"Alice   G.\", \"age\": 25}, \n                        {\"name\": \"Bob  T.\", \"age\": 35}, \n                        {\"name\": \"Eve   A.\", \"age\": 28}] \n                \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n    \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n    \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Referencing Immutable List Class\nDESCRIPTION: Demonstrates how to reference the immutable List class in Scala, which is an alias for scala.collection.immutable.List.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_13\n\nLANGUAGE: Scala\nCODE:\n```\n[[scala.collection.immutable.List]]\n```\n\n----------------------------------------\n\nTITLE: Loading Fuse.js for Fuzzy Search\nDESCRIPTION: Imports the Fuse.js library which provides fuzzy search capabilities with lightweight implementation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_11\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/search-1.0.0/fuse.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns from DataFrame\nDESCRIPTION: This code snippet demonstrates how to select specific columns from a PySpark DataFrame using `df.select()`. It selects the column 'c' and displays the results using `show()`. This operation returns a new DataFrame containing only the selected column.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.select(df.c).show()\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame in Remote Spark Session\nDESCRIPTION: This code creates a DataFrame in a remote Spark session using PySpark's Row and datetime classes. The DataFrame is filled with sample data and can be used for data operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Active Voice Error Message Examples in Scala\nDESCRIPTION: Examples demonstrating the conversion of passive voice to active voice in error messages, making them clearer and more direct.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\n// Before\nDataType {} is not supported by {}.\n\n// After\n{} does not support datatype {}.\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Server with Spark Connect Support\nDESCRIPTION: Uses the start-connect-server.sh script to initialize a Spark server with Spark Connect capabilities. The script includes the necessary Spark Connect package for the current Spark version.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame in PySpark\nDESCRIPTION: Demonstrates how to create a DataFrame from a list of dictionaries containing sample data with name and age fields.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Taking First N Rows\nDESCRIPTION: This code snippet demonstrates how to retrieve the first row from a PySpark DataFrame using `df.take(1)`.  This returns a list containing the first Row object in the DataFrame. This is a safer alternative to `collect()` for large DataFrames, as it only retrieves a limited number of rows.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.take(1)\n```\n\n----------------------------------------\n\nTITLE: Column Type Check\nDESCRIPTION: This code verifies that column operations return `Column` objects. It shows that selecting a column (e.g., `df.c`), applying a function (e.g., `upper(df.c)`), and checking for null values (e.g., `df.c.isNull()`) all result in `Column` instances.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import Column\nfrom pyspark.sql.functions import upper\n\ntype(df.c) == type(upper(df.c)) == type(df.c.isNull())\n```\n\n----------------------------------------\n\nTITLE: Showing DataFrame Rows Vertically\nDESCRIPTION: This code snippet demonstrates how to display DataFrame rows vertically using `df.show(1, vertical=True)`. This is useful when rows have many columns or columns with long values that make horizontal display difficult to read.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1, vertical=True)\n```\n\n----------------------------------------\n\nTITLE: Include Bootstrap CSS\nDESCRIPTION: Includes the Bootstrap CSS framework for styling the website. It provides pre-built UI components and a responsive grid system.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<link href=\"deps/bootstrap-5.3.1/bootstrap.min.css\" rel=\"stylesheet\" />\n```\n\n----------------------------------------\n\nTITLE: Include Font Awesome CSS\nDESCRIPTION: Includes Font Awesome CSS for icons. It's used for adding visual elements to the website.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_4\n\nLANGUAGE: HTML\nCODE:\n```\n<link href=\"deps/font-awesome-6.4.2/css/all.min.css\" rel=\"stylesheet\" />\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark DataFrame from Pandas DataFrame\nDESCRIPTION: Illustrates the conversion of a pandas DataFrame to a PySpark DataFrame. This is useful when working with existing pandas data structures.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf\n```\n\n----------------------------------------\n\nTITLE: DataFrame mapInArrow Method in PySpark\nDESCRIPTION: Using the DataFrame.mapInArrow method to apply Python functions to batches of data with Apache Arrow format for better performance.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.mapInArrow\n```\n\n----------------------------------------\n\nTITLE: Enabling Eager Evaluation in Notebooks\nDESCRIPTION: This code snippet enables eager evaluation of PySpark DataFrames in notebooks like Jupyter. It sets the `spark.sql.repl.eagerEval.enabled` configuration to `True`.  When enabled, DataFrames are automatically displayed, making it easier to inspect data during development.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Running unittest Tests for PySpark\nDESCRIPTION: Shows how to run unittest tests programmatically with specific arguments. This is useful when you want to execute tests from within a script or notebook rather than from the command line.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nunittest.main(argv=[''], verbosity=0, exit=False)\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Server with Spark Connect (Bash)\nDESCRIPTION: This command starts the Spark Connect server using the start-connect-server.sh script. It includes the necessary package for Spark Connect support.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Using max_by/min_by Functions in PySpark\nDESCRIPTION: Using max_by and min_by functions to find the maximum or minimum value based on a specified column, simplifying complex aggregation operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmax_by/min_by\n```\n\n----------------------------------------\n\nTITLE: Showing Top Rows of DataFrame\nDESCRIPTION: This snippet uses the `DataFrame.show()` method to display the top rows of a DataFrame, illustrating how to limit the number of rows displayed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1)\n```\n\n----------------------------------------\n\nTITLE: Selecting and Accessing Data in DataFrame\nDESCRIPTION: This snippet illustrates how to select a specific column from a DataFrame. The selection creates a new Column instance without triggering computation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.a\n```\n\n----------------------------------------\n\nTITLE: Include jQuery Headroom Library\nDESCRIPTION: Includes the jQuery Headroom plugin, a jQuery wrapper for the Headroom library, enabling headroom functionality using jQuery.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_7\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/headroom-0.11.0/jQuery.headroom.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pytest Framework\nDESCRIPTION: Configures pytest fixture for PySpark testing environment.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\n----------------------------------------\n\nTITLE: Testing with PySpark Built-in Utilities\nDESCRIPTION: Utilizes PySpark built-in utilities like assertDataFrameEqual and assertSchemaEqual to validate DataFrames. Requires pyspark.testing and its utils. Compares DataFrames and schemas to assert equality. Inputs are two DataFrames or schemas.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Include Fuse.js Library\nDESCRIPTION: Includes Fuse.js, a fuzzy search library. It's used to perform approximate string matching for search functionality.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_11\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/search-1.0.0/fuse.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Include Mark.js Library\nDESCRIPTION: Includes Mark.js, a text highlighting library. It highlights search terms within the content of the page.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_12\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/search-1.0.0/mark.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Grouping Example\nDESCRIPTION: This code snippet creates a DataFrame to demonstrate grouping operations.  It defines a DataFrame with columns 'color', 'fruit', 'v1', and 'v2' and populates it with sample data. The DataFrame is then displayed using `show()`.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query on DataFrame TableView in PySpark\nDESCRIPTION: Shows how to register a DataFrame as a temporary view and execute a SQL query to count records in PySpark. Imports are implicit through Spark session context.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Pandas UDF in PySpark\nDESCRIPTION: Shows how to create and use a Pandas UDF (User Defined Function) for data transformation\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\n\ndf.select(pandas_plus_one(df.a)).show()\n```\n\n----------------------------------------\n\nTITLE: Using Take to Retrieve Data in PySpark\nDESCRIPTION: This snippet demonstrates the use of take() to retrieve a specified number of rows from a DataFrame, which is a safer alternative to collect() for managing memory usage.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.take(1)\n```\n\n----------------------------------------\n\nTITLE: Collecting DataFrame Data to Driver\nDESCRIPTION: This snippet collects all the data from the distributed DataFrame to the driver's local memory using DataFrame.collect(). Be cautious as it can lead to out-of-memory errors for large datasets.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Describing DataFrame Columns\nDESCRIPTION: This snippet provides a summary of specific columns in the DataFrame with descriptive statistics by using the `describe()` method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.select('a', 'b', 'c').describe().show()\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: This snippet initializes a Spark session, which is essential for executing any PySpark application. The session serves as the entry point where all PySpark functionalities can be accessed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Connect Server\nDESCRIPTION: Starts a Spark server with Spark Connect support using the start-connect-server.sh script and configures necessary packages.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\nsource ~/.profile # Make sure environment variables are loaded.\n$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Configuration for Eager Evaluation\nDESCRIPTION: This snippet configures Spark settings to enable eager evaluation, allowing DataFrames to be displayed directly without needing to call `show()`.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Building PySpark Epydoc\nDESCRIPTION: Command to build PySpark documentation using Epydoc from the PySpark directory.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.2/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nepydoc --config epydoc.conf\n```\n\n----------------------------------------\n\nTITLE: Co-grouping Data and Applying Function\nDESCRIPTION: This code demonstrates how to cogroup two PySpark DataFrames based on a common key ('id') and then apply a pandas function to the cogrouped data using `applyInPandas`.  The `merge_ordered` function merges two pandas DataFrames in an ordered fashion.  The result is a merged DataFrame displayed using `show()`.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    ('time', 'id', 'v1'))\n\ndf2 = spark.createDataFrame(\n    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n    ('time', 'id', 'v2'))\n\ndef merge_ordered(l, r):\n    return pd.merge_ordered(l, r)\n\ndf1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n    merge_ordered, schema='time int, id int, v1 double, v2 string').show()\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Connect Server\nDESCRIPTION: Starts the Spark Connect server using a shell script with specified package dependencies for Spark Connect.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.13:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame in Vertical Format\nDESCRIPTION: This snippet demonstrates how to display the DataFrame's contents in a vertical format, making it easier to read rows with many columns.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1, vertical=True)\n```\n\n----------------------------------------\n\nTITLE: Registering and Using UDFs in PySpark SQL Queries\nDESCRIPTION: This snippet demonstrates how to define a Pandas UDF and register it for usage within SQL queries in PySpark. The function add_one is decorated with @pandas_udf and registered to be accessible in SQL execution. Dependencies include Pandas and PySpark UDF functionality. The expected input is a Series, and the output is the SQL query result embedded with the registered UDF.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Streaming Data Processing with Kafka\nDESCRIPTION: Sets up a Spark Structured Streaming pipeline to read JSON data from Kafka and process it\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/examples.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = (\n    spark.readStream.format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n    .option(\"subscribe\", subscribeTopic)\n    .load()\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nschema = StructType([\n StructField(\"student_name\", StringType()),\n StructField(\"graduation_year\", StringType()),\n StructField(\"major\", StringType()),\n])\n\ndef with_normalized_names(df, schema):\n    parsed_df = (\n        df.withColumn(\"json_data\", from_json(col(\"value\").cast(\"string\"), schema))\n        .withColumn(\"student_name\", col(\"json_data.student_name\"))\n        .withColumn(\"graduation_year\", col(\"json_data.graduation_year\"))\n        .withColumn(\"major\", col(\"json_data.major\"))\n        .drop(col(\"json_data\"))\n        .drop(col(\"value\"))\n    )\n    split_col = split(parsed_df[\"student_name\"], \"XX\")\n    return (\n        parsed_df.withColumn(\"first_name\", split_col.getItem(0))\n        .withColumn(\"last_name\", split_col.getItem(1))\n        .drop(\"student_name\")\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Sample DataFrame\nDESCRIPTION: Creates a sample DataFrame with name and age data for testing purposes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Data with Scala in Spark\nDESCRIPTION: Demonstrates reading JSON data and performing filtering operations in Scala. This code reads a JSON file into a DataFrame, filters records where age is greater than 21, and displays only the first name field.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nval df = spark.read.json(\"logs.json\")\ndf.where(\"age > 21\")\n  .select(\"name.first\").show()\n```\n\n----------------------------------------\n\nTITLE: Building PySpark Epydoc\nDESCRIPTION: Command to build PySpark documentation using Epydoc.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nepydoc --config epydoc.conf\n```\n\n----------------------------------------\n\nTITLE: Word Count Using Spark RDD\nDESCRIPTION: Demonstrates word counting implementation using Spark's RDD API with text file input\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/examples.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntext_file = spark.sparkContext.textFile(\"some_words.txt\")\n\ncounts = (\n    text_file.flatMap(lambda line: line.split(\" \"))\n    .map(lambda word: (word, 1))\n    .reduceByKey(lambda a, b: a + b)\n)\n```\n\n----------------------------------------\n\nTITLE: Spark Streaming State Updates\nDESCRIPTION: Reference to state management in Spark Streaming using updateStateByKey operation for maintaining state across batches.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-07-16-spark-release-0-7-3.md#2025-04-21_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nupdateStateByKey\n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session\nDESCRIPTION: Stops any existing local Spark session to prepare for creating a remote Spark Connect session.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Creating Sample DataFrame\nDESCRIPTION: Creates a sample DataFrame with name and age data for testing purposes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrame Schemas in PySpark\nDESCRIPTION: Shows how to compare DataFrame schemas using PySpark's assertSchemaEqual function. This is useful for ensuring data structure consistency.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: Creates a SparkSession with a specified application name. This is the entry point for PySpark functionality.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\n----------------------------------------\n\nTITLE: DataFrame Transformation Function\nDESCRIPTION: Defines a function to remove extra spaces from specified column using regexp_replace\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing DataFrame Transformation\nDESCRIPTION: Defines a function to remove extra spaces from a specified column in a DataFrame using regexp_replace.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Setting Up pytest Fixture for PySpark Testing\nDESCRIPTION: Demonstrates how to set up a pytest fixture to create and share a SparkSession across tests, and tear it down when tests are complete.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\n----------------------------------------\n\nTITLE: Building Spark Scaladoc\nDESCRIPTION: Command to build Spark Scaladoc documentation using SBT from the project root directory.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.2/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsbt/sbt doc\n```\n\n----------------------------------------\n\nTITLE: Defining and Registering UDFs for SQL - Python\nDESCRIPTION: In this snippet, a Pandas UDF (User Defined Function) is defined and registered for use in SQL queries. It showcases how to create a UDF that adds one to the input values and how to execute it in a SQL statement.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: Data Persistence Configuration in Spark\nDESCRIPTION: Storage level configuration for persisting datasets directly to disk, optimizing memory usage for large datasets\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2014-05-30-spark-release-1-0-0.md#2025-04-21_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nDISK_ONLY\n```\n\n----------------------------------------\n\nTITLE: Configuring PySpark Batch Size\nDESCRIPTION: Example showing how to configure the PySpark batch size to restore pre-1.2 behavior. The default batch size was changed to auto in 1.2.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2014-12-18-spark-release-1-2-0.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSparkContext([... args... ], batchSize=1024)\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying DataFrame\nDESCRIPTION: Creates a sample DataFrame with various data types including integers, floats, strings, dates, and timestamps, then displays it.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Explicit Schema\nDESCRIPTION: This snippet shows how to create a DataFrame with an explicitly defined schema. The schema specifies the data types of each column directly.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\n```\n\n----------------------------------------\n\nTITLE: Initializing PySpark Session and Importing Required Libraries\nDESCRIPTION: Creates a SparkSession which is the entry point to programming with PySpark. This snippet imports necessary libraries and initializes a session with a specified application name.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\n----------------------------------------\n\nTITLE: Using PySpark Built-in Test Utility Functions\nDESCRIPTION: Demonstrates the use of PySpark's built-in test utility functions like assertDataFrameEqual for comparing DataFrames. These are useful for simple ad-hoc validations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.testing\nfrom pyspark.testing.utils import assertDataFrameEqual\n\n# Example 1\ndf1 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 1000), (\"2\", 3000)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2)  # pass, DataFrames are identical\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example 2\ndf1 = spark.createDataFrame(data=[(\"1\", 0.1), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\ndf2 = spark.createDataFrame(data=[(\"1\", 0.109), (\"2\", 3.23)], schema=[\"id\", \"amount\"])\nassertDataFrameEqual(df1, df2, rtol=1e-1)  # pass, DataFrames are approx equal by rtol\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Data with SparkR\nDESCRIPTION: Demonstrates how to work with JSON data using SparkR. The code reads a JSON file, filters records where age is greater than 21, and displays the first few rows with only the first name field selected.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_7\n\nLANGUAGE: r\nCODE:\n```\ndf <- read.json(path = \"logs.json\")\ndf <- filter(df, df$age > 21)\nhead(select(df, df$name.first))\n```\n\n----------------------------------------\n\nTITLE: Generating Scaladoc with sbt in Shell\nDESCRIPTION: This command generates the Spark Scaladoc for its subprojects using sbt in the Spark project root directory. It's executed through `sbt/sbt doc`, and Jekyll will copy these docs into the `_site` directory during the build.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.6.0/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsbt/sbt doc\n```\n\n----------------------------------------\n\nTITLE: Combining SQL Expressions with PySpark DataFrame Operations\nDESCRIPTION: This snippet demonstrates how to use SQL expressions directly within DataFrame operations. It shows both using 'selectExpr' to invoke the UDF and using the 'expr' function to create a SQL expression that can be used as a column.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Shuffle Consolidation\nDESCRIPTION: Configuration property to enable shuffle file consolidation for better filesystem performance. Recommended for filesystems newer than ext3 like ext4 or XFS.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-12-19-spark-release-0-8-1.md#2025-04-21_snippet_0\n\nLANGUAGE: Properties\nCODE:\n```\nspark.shuffle.consolidateFiles=true\n```\n\n----------------------------------------\n\nTITLE: Invalid Pivot Column Error Message in Scala\nDESCRIPTION: This error message concisely explains why a pivot column is invalid and implies how to fix it by using comparable columns.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nInvalid pivot column {}. Pivot columns must be comparable.\n```\n\n----------------------------------------\n\nTITLE: Running Unittest Main Function\nDESCRIPTION: Shows how to execute unittest test cases programmatically with specific arguments to control verbosity and exit behaviors. It serves as an entry point for the test execution. Requires unittest.main and configured test cases.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nunittest.main(argv=[''], verbosity=0, exit=False)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up PySpark Tests with pytest\nDESCRIPTION: Demonstrates how to set up PySpark tests using pytest, including a fixture for creating and sharing a SparkSession across tests.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\n----------------------------------------\n\nTITLE: Using Python Profiler with Pandas UDFs\nDESCRIPTION: Using the newly provided profiler for Python/Pandas UDFs to analyze performance and optimize code execution.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprofiler\n```\n\n----------------------------------------\n\nTITLE: Asserting Schema Equality with PySpark Utilities\nDESCRIPTION: Demonstrates how to use assertSchemaEqual from PySpark utilities to validate the equality of two DataFrame schemas. This is useful for ensuring schema compatibility between operations. Requires pyspark.sql.types for schema definitions and pyspark.testing.utils for assertions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertSchemaEqual\nfrom pyspark.sql.types import StructType, StructField, ArrayType, DoubleType\n\ns1 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\ns2 = StructType([StructField(\"names\", ArrayType(DoubleType(), True), True)])\n\nassertSchemaEqual(s1, s2)  # pass, schemas are identical\n\n```\n\n----------------------------------------\n\nTITLE: Collecting Data from DataFrame in PySpark\nDESCRIPTION: This snippet shows how to collect all rows from a distributed DataFrame to the driver side, which can lead to out-of-memory errors if the data size is too large.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Syntax Highlighting with Pygments in Markdown\nDESCRIPTION: Example of how to use Pygments for syntax highlighting in Markdown files. This snippet shows the syntax for highlighting Scala code.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/README.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{% highlight scala %}\n// Your scala code goes here, you can replace scala with many other\n// supported languages too.\n{% endhighlight %}\n```\n\n----------------------------------------\n\nTITLE: Registering and Using Pandas UDFs in Spark SQL\nDESCRIPTION: This snippet shows how to define a Pandas UDF, register it with Spark SQL, and then invoke it within a SQL query. The UDF adds one to each value in a Series.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\n\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n```\n\n----------------------------------------\n\nTITLE: RDD Persistence Method\nDESCRIPTION: New method for controlling RDD caching strategies on a per-RDD level in Spark 0.6.0.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-10-15-spark-release-0-6-0.md#2025-04-21_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\npersist()\n```\n\n----------------------------------------\n\nTITLE: Window Frame Specification Error Message in Scala\nDESCRIPTION: This improved error message explains why specifying a frame for a window expression is not allowed, detailing the mismatch between function and specification frames.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nCannot specify frame for window expression {}. Window expression contains mismatch between function frame {} and specification frame {}.\n```\n\n----------------------------------------\n\nTITLE: Co-grouping Two DataFrames\nDESCRIPTION: This snippet demonstrates how to co-group two DataFrames and merge their respective grouped data by using a custom function in `applyInPandas`.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    ('time', 'id', 'v1'))\n\ndf2 = spark.createDataFrame(\n    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n    ('time', 'id', 'v2'))\n\ndef merge_ordered(l, r):\n    return pd.merge_ordered(l, r)\n\ndf1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n    merge_ordered, schema='time int, id int, v1 double, v2 string').show()\n```\n\n----------------------------------------\n\nTITLE: Applying Aggregate Functions after Grouping\nDESCRIPTION: This snippet groups the DataFrame by color and calculates the average value for each group using the `avg()` function.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('color').avg().show()\n```\n\n----------------------------------------\n\nTITLE: Checking Function Existence in PySpark Catalog\nDESCRIPTION: Using the functionExists method from pyspark.sql.catalog to check if a function exists in the catalog before performing operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npyspark.sql.catalog.functionExists\n```\n\n----------------------------------------\n\nTITLE: Setting Up Unittest Framework\nDESCRIPTION: Defines base test class with setup and teardown methods for unittest-based PySpark testing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Connect Server Script in Bash\nDESCRIPTION: This Bash script snippet demonstrates how to start a Spark server supporting Spark Connect sessions using the script 'start-connect-server.sh' with specified packages and environment variables.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\nsource ~/.profile # Make sure environment variables are loaded.\n$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Syntax Highlighting in Markdown with Pygments\nDESCRIPTION: Example of how to use Pygments for syntax highlighting in markdown documentation. This snippet shows the syntax for highlighting Scala code.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.2/README.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{% highlight scala %}\n// Your scala code goes here, you can replace scala with many other\n// supported languages too.\n{% endhighlight %}\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Data to/from ORC\nDESCRIPTION: This snippet shows how to write a DataFrame to an ORC file, which is another optimized storage format, and read it back into the program.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Referencing Scala Collections Package\nDESCRIPTION: Demonstrates how to reference the Scala collections package, which contains the collections framework with higher-order functions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\nscala.[[scala.collection]]\n```\n\n----------------------------------------\n\nTITLE: Configuring Streaming Backpressure\nDESCRIPTION: Enable the experimental backpressure feature in Spark Streaming to dynamically control ingestion rates and handle bursty input streams.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2015-09-09-spark-release-1-5-0.md#2025-04-21_snippet_0\n\nLANGUAGE: configuration\nCODE:\n```\nspark.streaming.backpressure.enabled = true\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Pandas DataFrame\nDESCRIPTION: This snippet demonstrates how to create a PySpark DataFrame from an existing pandas DataFrame, allowing for easy integration with pandas data structures.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame in PySpark\nDESCRIPTION: Defines a function to remove extra spaces from a specified column in a DataFrame using regexp_replace, and applies this transformation to the 'name' column.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Starting PySpark Session and Creating DataFrame\nDESCRIPTION: Initializes a new SparkSession and creates a DataFrame with sample data. Dependencies include pyspark.sql for SparkSession and DataFrame operations. Input is list of dictionaries, output is a DataFrame object. No specific constraints.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Implementing Unittest Test Cases\nDESCRIPTION: Creates a test class with test cases for DataFrame transformation using unittest framework.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session (Python)\nDESCRIPTION: This code stops the existing regular Spark session to prepare for creating a remote Spark Connect session.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Skipping API Documentation Generation with Jekyll\nDESCRIPTION: Command to run Jekyll while skipping the generation of Scala and Python API documentation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nSKIP_API=1 jekyll\n```\n\n----------------------------------------\n\nTITLE: Applying Function on DataFrame Using mapInPandas\nDESCRIPTION: This snippet shows how to apply a custom function to filter data in a DataFrame using the `mapInPandas` function.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef pandas_filter_func(iterator):\n    for pandas_df in iterator:\n        yield pandas_df[pandas_df.a == 1]\n\ndf.mapInPandas(pandas_filter_func, schema=df.schema).show()\n```\n\n----------------------------------------\n\nTITLE: Referencing Parallel Collections Package\nDESCRIPTION: Shows how to reference the parallel collections package in Scala, which provides automatic parallel operations on collections.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_4\n\nLANGUAGE: Scala\nCODE:\n```\n[[scala.collection.parallel]]\n```\n\n----------------------------------------\n\nTITLE: Determining Closed PR Numbers for New Release - Shell\nDESCRIPTION: This command sequence retrieves and sorts pull request numbers that were closed in the new Apache Spark release compared to the previous one. It uses git log to find relevant entries, filters them with grep, and outputs the results to a file. The commands utilize redirection and sorting to list the closed PR numbers clearly.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ git log v1.1.1 | grep \"Closes #\" | cut -d \" \" -f 5,6 | grep Closes | sort > closed_1.1.1\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ git log v1.1.0 | grep \"Closes #\" | cut -d \" \" -f 5,6 | grep Closes | sort > closed_1.1.0\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ diff --new-line-format=\"\" --unchanged-line-format=\"\" closed_1.1.1 closed_1.1.0 > diff.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Spark Connect Session\nDESCRIPTION: Establishes a remote connection to the Spark Connect server running on localhost port 15002.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Using UDFs and SQL Expressions in DataFrame Selections - Python\nDESCRIPTION: This snippet illustrates how to use registered UDFs and SQL expressions within DataFrame selections in PySpark. It allows combining SQL operations with DataFrame API seamlessly.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Extended Pattern Matching in SQL\nDESCRIPTION: Introduction of new LIKE operators to support more complex pattern matching scenarios in SQL queries\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2021-03-02-spark-release-3-1-1.md#2025-04-21_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM table WHERE column LIKE ANY ('pattern1', 'pattern2')\n```\n\n----------------------------------------\n\nTITLE: Writing Unittest Test Cases\nDESCRIPTION: Implements a unittest test case for validating DataFrame transformation function.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Generating Encoder for Inner Class Error Message in Scala\nDESCRIPTION: This error message explains the inability to generate an encoder for an inner class due to scope access issues. It provides a clear explanation of the problem and suggests a solution.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nUnable to generate an encoder for inner class {} without access to the scope that this class was defined in. Try moving this class out of its parent class.\n```\n\n----------------------------------------\n\nTITLE: Optimized pandas Parquet Reading\nDESCRIPTION: Demonstrates manually optimized pandas code for reading Parquet files with column pruning and filtering. Shows how to reduce memory usage through manual optimizations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/pandas-on-spark/index.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet(\n    \"G1_1e9_1e2_0_0.parquet\",\n    columns=[\"id1\", \"id2\", \"v3\"],\n    filters=[(\"id1\", \">\", \"id098\")],\n    engine=\"pyarrow\",\n)\ndf.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n```\n\n----------------------------------------\n\nTITLE: AWS Environment Variables for S3 Access\nDESCRIPTION: Environment variable names used for S3 credentials in EC2 deployments.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-02-27-spark-release-0-7-0.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\n```\n\n----------------------------------------\n\nTITLE: Non-Accusatory Error Messages in Scala\nDESCRIPTION: Examples demonstrating how to write error messages without accusatory or judgmental language.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n// Before\nYou must specify an amount for {}.\n\n// After\n{} cannot be empty. Specify an amount for {}.\n```\n\n----------------------------------------\n\nTITLE: Checking DataFrame Emptiness in PySpark\nDESCRIPTION: Using the isEmpty method to quickly check if a DataFrame contains no rows, useful for conditional processing based on data presence.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.isEmpty\n```\n\n----------------------------------------\n\nTITLE: Using PySpark SQL Expressions as DataFrame Columns\nDESCRIPTION: This snippet illustrates how to integrate SQL expressions with DataFrame operations in PySpark. It uses selectExpr and expr to apply SQL logic directly as part of DataFrame transformations. PySpark needs to be configured, and required columns must exist in the DataFrame. The output consists of column transformations results displayed via the show method.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import expr\n\ndf.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') > 0).show()\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Vertically\nDESCRIPTION: This snippet shows how to display the contents of a DataFrame vertically, which is useful when the rows contain long data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.show(1, vertical=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Eager Evaluation in PySpark\nDESCRIPTION: Demonstrates how to enable eager evaluation of PySpark DataFrames in notebook environments. This allows for immediate display of DataFrame contents without calling show().\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Implementing Pytest Test Cases\nDESCRIPTION: Creates test cases for DataFrame transformation using pytest framework.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Invalid Function Name Error Message in Scala\nDESCRIPTION: This improved error message explains why a function name is invalid for temporary functions and suggests how to correctly specify the function name.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nFunction name {} is invalid. Temporary functions cannot belong to a catalog. Specify a function name with one or two parts.\n```\n\n----------------------------------------\n\nTITLE: Transforming PySpark DataFrame with Custom Function\nDESCRIPTION: Defines a custom function to remove extra spaces in a DataFrame column using regexp_replace. Requires pyspark.sql.functions for DataFrame transformations. Takes a DataFrame and column_name as parameters. Returns a transformed DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Deploy Spread Out Option\nDESCRIPTION: Configuration parameter to control how jobs are distributed across nodes in a standalone Spark cluster. When enabled, jobs are spread across more nodes rather than being concentrated on fewer nodes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-11-22-spark-release-0-6-1.md#2025-04-21_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nspark.deploy.spreadOut\n```\n\n----------------------------------------\n\nTITLE: Adding JIRA ID Test Case in Scala\nDESCRIPTION: Example of how to add a JIRA ID reference in a Scala test case. The test name should include the JIRA ticket number and a short description.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/contributing.md#2025-04-21_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\ntest(\"SPARK-12345: a short description of the test\") {\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to DataFrame in PySpark\nDESCRIPTION: Using the withMetadata API to add metadata to DataFrame columns, enabling additional information to be stored with the column.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.withMetadata\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark DataFrame from List of Rows\nDESCRIPTION: Demonstrates how to create a PySpark DataFrame using a list of Row objects. This method allows for explicit specification of data types and column names.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf\n```\n\n----------------------------------------\n\nTITLE: PySpark Pytest Fixture\nDESCRIPTION: Creates a pytest fixture for managing SparkSession lifecycle\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\n----------------------------------------\n\nTITLE: Setting Up PySpark Tests with Pytest\nDESCRIPTION: The code sets up a PySpark test fixture using pytest, allowing shared Spark sessions across tests. This setup facilitates efficient test execution by managing session lifecycle. Requires the pytest fixture mechanism and pyspark.sql for session handling.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n\n```\n\n----------------------------------------\n\nTITLE: User-Friendly Error Messages in Scala\nDESCRIPTION: Examples demonstrating how to write error messages without technical jargon for better user understanding.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\n// Before\nRENAME TABLE source and destination databases do not match: '{}' != '{}'.\n\n// After\nRENAME TABLE source and destination databases do not match. The source database is {}, but the destination database is {}.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pytest Framework\nDESCRIPTION: Implements a pytest fixture for SparkSession management.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\n----------------------------------------\n\nTITLE: Build Command Change Notice - SBT\nDESCRIPTION: Updated build command for compiling Spark from source code using SBT build tool.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-09-25-spark-release-0-8-0.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsbt/sbt assembly\n```\n\n----------------------------------------\n\nTITLE: Concrete Example Error Messages in Scala\nDESCRIPTION: Examples showing how to include specific examples in error messages to improve clarity.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\n// Before\n{} Hint expects a partition number as a parameter.\n\n// After\n{} Hint expects a partition number as a parameter. For example, specify 3 partitions with {}(3).\n```\n\n----------------------------------------\n\nTITLE: Using Observation Method in PySpark DataFrame\nDESCRIPTION: Using DataFrame.observation method to monitor metrics during execution of DataFrame operations, providing real-time insights into performance.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nDataframe.observation\n```\n\n----------------------------------------\n\nTITLE: Defining DataFrame Transformation Function\nDESCRIPTION: Implements a function to remove extra spaces from a specified column in a DataFrame using regexp_replace.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, regexp_replace\n\n# Remove additional spaces in name\ndef remove_extra_spaces(df, column_name):\n    # Remove extra spaces from the specified column\n    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n    \n    return df_transformed\n\ntransformed_df = remove_extra_spaces(df, \"name\")\n\ntransformed_df.show()\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Connect Server using Bash\nDESCRIPTION: Script to start the Spark Connect server by sourcing environment variables and executing the start-connect-server script.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\nsource ~/.profile # Make sure environment variables are loaded.\n$HOME/sbin/start-connect-server.sh\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Data with Java in Spark\nDESCRIPTION: Shows how to read and filter JSON data using Java API for Spark. The code reads a JSON file into a Dataset, filters by age greater than 21, and selects only the first name field for display.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nDataset df = spark.read().json(\"logs.json\");\ndf.where(\"age > 21\")\n  .select(\"name.first\").show();\n```\n\n----------------------------------------\n\nTITLE: New Java Operators in Spark 0.5.0\nDESCRIPTION: New operator methods added to Spark including sortByKey for parallel sorting, takeSample for sampling data, and optimized fold and aggregate operators. These operators can preserve RDD partitioning information to reduce network communication costs.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-06-12-spark-release-0-5-0.md#2025-04-21_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nsortByKey\ntakeSample\nfold\naggregate\njoin\n```\n\n----------------------------------------\n\nTITLE: Testing DataFrame Transformations with Unittest\nDESCRIPTION: Demonstrates writing a PySpark unit test case using unittest to verify DataFrame transformations. It checks that transformations like removing extra spaces in data are correctly applied. Requires pyspark.testing.utils for assert function and a pre-configured test base class.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.testing.utils import assertDataFrameEqual\n\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                       {\"name\": \"Alice   G.\", \"age\": 25}, \n                       {\"name\": \"Bob  T.\", \"age\": 35}, \n                       {\"name\": \"Eve   A.\", \"age\": 28}] \n                        \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n        \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n        \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n        \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n\n```\n\n----------------------------------------\n\nTITLE: Cloning Apache Spark Repository\nDESCRIPTION: Commands to clone the Apache Spark Git repository and navigate to the project directory\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/apache/spark.git\n$ cd spark\n```\n\n----------------------------------------\n\nTITLE: Referencing Whole Text Files Method in Spark\nDESCRIPTION: New SparkContext method introduced in Spark 1.0 that allows processing small text files as individual records\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2014-05-30-spark-release-1-0-0.md#2025-04-21_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nSparkContext.wholeTextFiles\n```\n\n----------------------------------------\n\nTITLE: Using merge_asof in Pandas API on Spark\nDESCRIPTION: Implementation of merge_asof function for asynchronous merge operations in Pandas API on Spark, similar to pandas.merge_asof functionality.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nps.merge_asof()\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to Pandas\nDESCRIPTION: This snippet demonstrates how to convert a PySpark DataFrame back to a pandas DataFrame, while also cautioning about potential memory limitations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.toPandas()\n```\n\n----------------------------------------\n\nTITLE: Creating Sample DataFrame\nDESCRIPTION: Creates a sample DataFrame with name and age data for testing\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n  {\"name\": \"Alice   G.\", \"age\": 25}, \n  {\"name\": \"Bob  T.\", \"age\": 35}, \n  {\"name\": \"Eve   A.\", \"age\": 28}] \n\ndf = spark.createDataFrame(sample_data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Eager Evaluation in PySpark\nDESCRIPTION: This snippet configures the Spark session to enable eager evaluation, allowing for immediate evaluation of DataFrame operations when used in interactive environments like Jupyter notebooks.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Saving Dataset as Text File in Spark\nDESCRIPTION: Demonstrates how to save a distributed dataset of numbers to HDFS as a text file using the saveAsTextFile operation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2011-07-14-spark-release-0-3.md#2025-04-21_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval numbers = spark.parallelize(1 to 100)\nnumbers.saveAsTextFile(\"hdfs://...\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up PySpark Unit Tests with Unittest\nDESCRIPTION: Illustrates configuring a PySpark TestCase using unittest, including setting up and tearing down a Spark session. This setup allows for structured test development in complex scenarios. Dependencies include the unittest module and a Spark session.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\n```\n\n----------------------------------------\n\nTITLE: Generating GPG Key for Spark Release Signing\nDESCRIPTION: Commands for generating a 4096-bit RSA GPG key pair for code signing purposes. Shows interactive process of key generation including setting user details and key expiration.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ gpg --full-gen-key\ngpg (GnuPG) 2.0.12; Copyright (C) 2009 Free Software Foundation, Inc.\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nPlease select what kind of key you want:\n   (1) RSA and RSA (default)\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\nYour selection? 1\nRSA keys may be between 1024 and 4096 bits long.\nWhat keysize do you want? (2048) 4096\nRequested keysize is 4096 bits\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      <n>  = key expires in n days\n      <n>w = key expires in n weeks\n      <n>m = key expires in n months\n      <n>y = key expires in n years\nKey is valid for? (0) \nKey does not expire at all\nIs this correct? (y/N) y\n\nGnuPG needs to construct a user ID to identify your key.\n\nReal name: Robert Burrell Donkin\nEmail address: rdonkin@apache.org\nComment: CODE SIGNING KEY\nYou selected this USER-ID:\n    \"Robert Burrell Donkin (CODE SIGNING KEY) <rdonkin@apache.org>\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\n```\n\n----------------------------------------\n\nTITLE: Setting JAR Dependencies in Spark Shell\nDESCRIPTION: Environment variable configuration for adding external JARs to Spark shell workers. This allows users to include additional dependencies that will be distributed to all cluster workers.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-07-16-spark-release-0-7-3.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nADD_JARS\n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session\nDESCRIPTION: Stops any existing local Spark session to prepare for creating a remote Spark Connect session.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Data I/O Operations in PySpark\nDESCRIPTION: Examples of reading and writing data in different formats (CSV, Parquet, ORC)\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n\ndf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n\ndf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n```\n\n----------------------------------------\n\nTITLE: Initializing PySpark Session\nDESCRIPTION: Creates a new SparkSession for running PySpark operations\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\n----------------------------------------\n\nTITLE: New Timestamp Conversion Functions in Spark SQL\nDESCRIPTION: Introduction of new timestamp conversion functions including timestamp_seconds, timestamp_millis, and timestamp_micros to provide more flexible time-based operations\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2021-03-02-spark-release-3-1-1.md#2025-04-21_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT timestamp_seconds(1234567890)\n```\n\n----------------------------------------\n\nTITLE: Python Column Resolution with Nested Fields\nDESCRIPTION: Updated Python DataFrame column resolution syntax to support dot notation and nested field access, with requirements for escaping columns containing dots.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2015-09-09-spark-release-1-5-0.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# New nested field access\ndf['table.column.nestedField']\n\n# Escaping columns with dots\ndf['table.`column.with.dots`.nested']\n```\n\n----------------------------------------\n\nTITLE: Setting Up unittest for PySpark Testing\nDESCRIPTION: Demonstrates how to set up a unittest class for PySpark testing, including methods to create and tear down a SparkSession for each test case.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\n----------------------------------------\n\nTITLE: Moving Spark Release Artifacts to Production\nDESCRIPTION: SVN commands to move release candidate artifacts from dev to release directory in Apache's distribution system. This is a critical, irreversible step that should only be performed by PMC members after a successful vote.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport SVN_EDITOR=vim\nsvn mv https://dist.apache.org/repos/dist/dev/spark/v1.1.1-rc2-bin https://dist.apache.org/repos/dist/release/spark/spark-1.1.1\n```\n\n----------------------------------------\n\nTITLE: Spark Shell Command\nDESCRIPTION: Command to launch the interactive Spark shell for running Spark commands interactively.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-07-16-spark-release-0-7-3.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nspark-shell\n```\n\n----------------------------------------\n\nTITLE: Generating Summary Statistics in PySpark\nDESCRIPTION: Demonstrates how to generate summary statistics for a DataFrame in PySpark. This code calls the summary() method on a filtered DataFrame and displays the results.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/index.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Generate summary statistics\nfiltered_df.summary().show()\n```\n\n----------------------------------------\n\nTITLE: Selecting DataFrame Columns\nDESCRIPTION: This snippet illustrates how to select columns from a DataFrame, showcasing the lazy evaluation behavior in PySpark.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf.a\n```\n\n----------------------------------------\n\nTITLE: Writing SequenceFile with Native Types in Spark\nDESCRIPTION: Demonstrates saving a dataset of key-value pairs as a Hadoop SequenceFile using native types that automatically convert to IntWritable.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2011-07-14-spark-release-0-3.md#2025-04-21_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n// Will write a SequenceFile of (IntWritable, IntWritable)\nval squares = spark.parallelize(1 to 100).map(n => (n, n*n))\nsquares.saveAsSequenceFile(\"hdfs://...\")\n```\n\n----------------------------------------\n\nTITLE: Writing pytest for PySpark Transformation\nDESCRIPTION: Shows how to write a pytest function to test the PySpark transformation, using the spark_fixture and assertDataFrameEqual to compare the transformed DataFrame with the expected result.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Uploading Artifacts to PyPI using Twine\nDESCRIPTION: This snippet demonstrates how to upload Python artifacts to the PyPI repository using the Twine tool. It requires a PyPI account with appropriate permissions and the twine package installed. The command includes placeholders for token-based authentication and the artifact file names, which should be adjusted according to the new release.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntwine upload -u __token__  -p $PYPI_API_TOKEN \\\n    --repository-url https://upload.pypi.org/legacy/ \\\n    \"pyspark-$PYSPARK_VERSION.tar.gz\" \\\n    \"pyspark-$PYSPARK_VERSION.tar.gz.asc\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Local Branch from Pull Request - Git Command\nDESCRIPTION: This shell command creates a new local branch from a remote pull request using Git. It checks out the specified pull request reference from the origin and creates a new branch named new-branch. This command is useful for testing changes from a pull request locally.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ git checkout origin/pr/112 -b new-branch\n```\n\n----------------------------------------\n\nTITLE: Updating Spark Apache Repository with Version Tags\nDESCRIPTION: This snippet illustrates how to tag a commit in the Spark repository with the new version after a release candidate passes. It involves using git tag to create a release tag and then pushing it to the Apache repository.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ git tag v1.1.1 v1.1.1-rc2 # the RC that passed\\n$ git push apache v1.1.1\n```\n\n----------------------------------------\n\nTITLE: Initializing PySpark Session\nDESCRIPTION: Creates a new SparkSession for a PySpark application with a specified app name.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\n----------------------------------------\n\nTITLE: Stopping Existing Spark Session\nDESCRIPTION: Code to stop any existing local Spark session before creating a remote Spark Connect session to avoid conflicts.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nSparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n```\n\n----------------------------------------\n\nTITLE: Building and Uploading Documentation to Spark Website\nDESCRIPTION: This snippet outlines the steps to build the latest documentation for Spark and upload it to the Spark website repository using Jekyll. It includes commands for checking out the right version, building docs, cloning the website repository, and copying the built site.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Build the latest docs\\n$ git checkout v1.1.1\\n$ cd docs\\n$ PRODUCTION=1 bundle exec jekyll build\\n\\n# Copy the new documentation to Apache\\n$ git clone https://github.com/apache/spark-website\\n...\\n$ cp -R _site spark-website/site/docs/1.1.1\\n\\n# Update the \"latest\" link\\n$ cd spark-website/site/docs\\n$ rm latest\\n$ ln -s 1.1.1 latest\n```\n\n----------------------------------------\n\nTITLE: Testing with Pytest Framework\nDESCRIPTION: Utilizes pytest fixtures to manage Spark session setup and teardown for tests. Requires pytest and pyspark.sql. Demonstrates writing tests that use shared fixtures to verify DataFrame transformations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef spark_fixture():\n    spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n    yield spark\n```\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom pyspark.testing.utils import assertDataFrameEqual\n\ndef test_single_space(spark_fixture):\n    sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                   {\"name\": \"Alice   G.\", \"age\": 25}, \n                   {\"name\": \"Bob  T.\", \"age\": 35}, \n                   {\"name\": \"Eve   A.\", \"age\": 28}] \n                    \n    # Create a Spark DataFrame\n    original_df = spark_fixture.createDataFrame(sample_data)\n    \n    # Apply the transformation function from before\n    transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n    expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n    {\"name\": \"Alice G.\", \"age\": 25}, \n    {\"name\": \"Bob T.\", \"age\": 35}, \n    {\"name\": \"Eve A.\", \"age\": 28}]\n    \n    expected_df = spark_fixture.createDataFrame(expected_data)\n\n    assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Removing Old Releases from Apache Mirror Network\nDESCRIPTION: This snippet demonstrates the process of removing an outdated release from the Apache mirror network using the svn rm command. It specifies the URL of the old release to be removed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ svn rm https://dist.apache.org/repos/dist/release/spark/spark-1.1.0\n```\n\n----------------------------------------\n\nTITLE: SVN Check-in KEYS File in Apache Spark\nDESCRIPTION: Commands to commit updates to the KEYS file in SVN using ASF credentials\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsvn ci --username $ASF_USERNAME --password \"$ASF_PASSWORD\" -m\"Update KEYS\"\n```\n\n----------------------------------------\n\nTITLE: Removing RC Artifacts from Apache Repositories\nDESCRIPTION: This snippet shows how to remove release candidate (RC) artifacts from the Apache staging repository after a vote passes. It utilizes the svn rm command to remove specified directories for binaries and docs. Proper backup of documentation is encouraged before executing this operation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nRC=v3.5.2-rc3 && \\\n  svn rm https://dist.apache.org/repos/dist/dev/spark/\"${RC}\"-bin/ \\\n  https://dist.apache.org/repos/dist/dev/spark/\"${RC}\"-docs/ \\\n  -m\"Removing RC artifacts.\"\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession in PySpark\nDESCRIPTION: Creates a SparkSession, which is the entry point for PySpark functionality. This session is named 'Testing PySpark Example'.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col \n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n```\n\n----------------------------------------\n\nTITLE: DataFrame Unresolved Logical Plan Example\nDESCRIPTION: Demonstrates how a simple DataFrame query is converted to an unresolved logical plan before transmission to the Spark Server\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/spark-connect/index.md#2025-04-21_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n== Parsed Logical Plan ==\nGlobalLimit 5\n+- LocalLimit 5\n   +- SubqueryAlias spark_catalog.default.some_table\n      +- UnresolvedRelation spark_catalog.default.some_table\n```\n\n----------------------------------------\n\nTITLE: Web UI Port Configuration\nDESCRIPTION: Default port configuration for Spark's monitoring web UI on the driver node.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-09-25-spark-release-0-8-0.md#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n4040\n```\n\n----------------------------------------\n\nTITLE: Checking Database Existence in PySpark Catalog\nDESCRIPTION: Using the databaseExists method from pyspark.sql.catalog to check if a database exists in the catalog before performing operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npyspark.sql.catalog.databaseExists\n```\n\n----------------------------------------\n\nTITLE: Creating Grep Expression for New Patches - Shell\nDESCRIPTION: This command constructs a regex expression from the diff.txt file that lists new patches between the two releases. It leverages cat, awk, tr, and sed to format the output appropriately for further filtering of the git logs.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ EXPR=$(cat diff.txt | awk '{ print \"\\(\"$1\" \"$2\" \\)\"; }' | tr \"\\n\" \"|\" | sed -e \"s/|/\\\\|/g\" | sed \"s/\\\\|$//\")\n```\n\n----------------------------------------\n\nTITLE: Configuration Property Change\nDESCRIPTION: Example of deprecated configuration property change in Spark 0.6.0. The spark.cache.class parameter is replaced with per-RDD storage level settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-10-15-spark-release-0-6-0.md#2025-04-21_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nspark.cache.class\n```\n\n----------------------------------------\n\nTITLE: DataFrameReader Table with Database Name - Spark SQL\nDESCRIPTION: This snippet demonstrates how to create a DataFrame from a table in a specific database using `DataFrameReader.table` in Spark SQL. It allows users to specify the database name along with the table name when creating a DataFrame.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2016-01-04-spark-release-1-6-0.md#2025-04-21_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n\"sqlContext.read.table(\\\"dbName.tableName\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Building Spark Core Module\nDESCRIPTION: Commands for building just the Spark Core module using SBT or Maven.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ # sbt\n$ build/sbt\n> project core\n> package\n\n$ # or you can build the spark-core module with sbt directly using:\n$ build/sbt core/package\n\n$ # Maven\n$ build/mvn package -DskipTests -pl core\n```\n\n----------------------------------------\n\nTITLE: Formatting Scala Code - Scalafmt\nDESCRIPTION: This shell command formats Scala source files that differ from the master branch using Scalafmt. It ensures consistent code formatting according to the project's guidelines before code submission. Users must rely on the existing script rather than a locally installed scalafmt.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$ ./dev/scalafmt\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Documentation with Jekyll\nDESCRIPTION: Command to generate HTML documentation using Jekyll, with an option to skip Scaladoc building. Also includes instructions for running a local web server to view the documentation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.2/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\njekyll\n```\n\nLANGUAGE: shell\nCODE:\n```\nSKIP_SCALADOC=1 jekyll\n```\n\nLANGUAGE: shell\nCODE:\n```\njekyll --server\n```\n\n----------------------------------------\n\nTITLE: Adding Archives to SparkContext in PySpark\nDESCRIPTION: Using SparkContext.addArchive method to add and distribute archive files to executors in a Spark application, simplifying dependency management.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nSparkContext.addArchive\n```\n\n----------------------------------------\n\nTITLE: Identifying Large Patch List - Shell\nDESCRIPTION: This command retrieves a list of significant patches (300+ lines) from the release logs. It employs filtering to find entries with substantial insertions, outputting the results to a designated file for easier review of large patches.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$ git log v1.1.1 --grep \"$expr\" --shortstat --oneline | grep -B 1 -e \"[3-9][0-9][0-9] insert\" -e \"[1-9][1-9][1-9][1-9] insert\" | grep SPARK > large-patches.txt\n```\n\n----------------------------------------\n\nTITLE: New Hadoop API Package Support\nDESCRIPTION: Support for the new Hadoop API package structure for reading and writing data to storage formats.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-06-12-spark-release-0-5-0.md#2025-04-21_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\norg.apache.mapreduce\n```\n\n----------------------------------------\n\nTITLE: Dynamic Resource Addition\nDESCRIPTION: Methods for dynamically adding files or JARs to be shipped to worker nodes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-10-15-spark-release-0-6-0.md#2025-04-21_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nSparkContext.addFile/Jar\n```\n\n----------------------------------------\n\nTITLE: Setting Local Spark Master URL Configuration\nDESCRIPTION: Configuration snippet showing how to specify the master URL for running Spark locally with parallel threads. The N parameter determines the number of parallel threads to use.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/faq.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlocal[N]\n```\n\n----------------------------------------\n\nTITLE: Running Tests with SBT\nDESCRIPTION: Commands for running individual tests or test suites using SBT.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ build/sbt\n> project core\n> test\n```\n\nLANGUAGE: bash\nCODE:\n```\n> testOnly org.apache.spark.scheduler.DAGSchedulerSuite\n```\n\nLANGUAGE: bash\nCODE:\n```\n> testOnly *DAGSchedulerSuite\n```\n\nLANGUAGE: bash\nCODE:\n```\n> testOnly org.apache.spark.scheduler.*\n```\n\nLANGUAGE: bash\nCODE:\n```\n> testOnly *DAGSchedulerSuite -- -z \"SPARK-12345\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ build/sbt \"core/testOnly *DAGSchedulerSuite -- -z SPARK-12345\"\n```\n\n----------------------------------------\n\nTITLE: Generating Contributor List - Shell\nDESCRIPTION: This command generates a list of contributors who contributed to the new release, filtered by the previously constructed regex expression. It utilizes git's shortlog feature to summarize contributions based on the specified expressions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n$ git shortlog v1.1.1 --grep \"$EXPR\" > contrib.txt\n```\n\n----------------------------------------\n\nTITLE: Adding JIRA ID Test Case in R\nDESCRIPTION: Example of how to add a JIRA ID reference in an R test case using testthat framework. The test description should include the JIRA ticket number and a short description.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/contributing.md#2025-04-21_snippet_3\n\nLANGUAGE: R\nCODE:\n```\ntest_that(\"SPARK-12345: a short description of the test\", {\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Version in SparkBuild.scala (Scala)\nDESCRIPTION: This snippet demonstrates how to configure Spark to compile against Hadoop 2 distributions.  It involves modifying the `HADOOP_VERSION` and `HADOOP_MAJOR_VERSION` variables within the `project/SparkBuild.scala` file. After modifying the variables, a recompile of Spark is necessary to apply the changes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-11-22-spark-release-0-5-2.md#2025-04-21_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nedit <code>project/SparkBuild.scala</code> and change both the <code>HADOOP_VERSION</code> and <code>HADOOP_MAJOR_VERSION</code> variables, then recompile Spark\n```\n\n----------------------------------------\n\nTITLE: Fetching and Checking Out Pull Requests\nDESCRIPTION: Commands to fetch and checkout remote pull requests for local testing\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n# Fetch remote pull requests\n$ git fetch origin\n# Checkout a remote pull request\n$ git checkout origin/pr/112\n```\n\n----------------------------------------\n\nTITLE: Generating Dependency Graphs - SBT and Maven\nDESCRIPTION: These shell commands generate dependency trees for a project using SBT and Maven. The SBT command outputs the project's dependency graph, while the Maven commands build the project and display the dependency tree. These analyses are useful for understanding project dependencies and detecting potential conflicts.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n$ build/sbt dependencyTree\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ build/mvn -DskipTests install\n$ build/mvn dependency:tree\n```\n\n----------------------------------------\n\nTITLE: Generating Scaladoc API Documentation\nDESCRIPTION: This snippet shows the command used to build the Scaladoc API documentation for Spark. It is executed from the SPARK_PROJECT_ROOT directory.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.3/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n\"sbt/sbt doc\"\n```\n\n----------------------------------------\n\nTITLE: Logging Example for Spark Block Manager UI\nDESCRIPTION: Example log output showing how to access the Block Manager web UI for monitoring memory usage of distributed datasets.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-02-27-spark-release-0-7-0.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n15:08:44 INFO BlockManagerUI: Started BlockManager web UI at http://mbk.local:63814\n```\n\n----------------------------------------\n\nTITLE: Referencing Predef Object\nDESCRIPTION: Shows how to reference the Predef object in Scala, which contains identifiers that are always in scope by default.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_12\n\nLANGUAGE: Scala\nCODE:\n```\n[[scala.Predef]]\n```\n\n----------------------------------------\n\nTITLE: Installing PySpark using pip\nDESCRIPTION: This snippet shows how to install PySpark using the pip package manager.  It's a simple command that downloads and installs PySpark and its dependencies from the Python Package Index (PyPI).\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2017-07-11-spark-release-2-2-0.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"pip install pyspark\"\n```\n\n----------------------------------------\n\nTITLE: Package Name Change Notice - Java/Scala\nDESCRIPTION: Important compatibility notice about the package name change from Spark 0.7 to 0.8. The main RDD class has been moved to org.apache.spark.rdd package.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-09-25-spark-release-0-8-0.md#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\norg.apache.spark.rdd\n```\n\n----------------------------------------\n\nTITLE: Running Jekyll Webserver in Shell\nDESCRIPTION: This command runs a Jekyll-built webserver on port 4000 for serving the Spark documentation locally as HTML. It uses the `--server` flag with the `jekyll` command. Required installation includes Jekyll as a Ruby Gem.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.6.0/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\njekyll --server\n```\n\n----------------------------------------\n\nTITLE: Maven Dependency Configuration for Spark 0.5.1\nDESCRIPTION: Maven coordinates for including Spark 0.5.1 as a dependency in Maven-based projects. Uses Scala 2.9.2 compatible version.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-10-11-spark-release-0-5-1.md#2025-04-21_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n    <groupId>org.spark-project</groupId>\n    <artifactId>spark-core_2.9.2</artifactId>\n    <version>0.5.1</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Referencing Mutable Collections Package\nDESCRIPTION: Demonstrates how to reference the mutable collections package in Scala for when mutable data structures are needed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\n[[scala.collection.mutable]]\n```\n\n----------------------------------------\n\nTITLE: Configuring YourKit profiler for Spark executor\nDESCRIPTION: Modifies `SPARK_EXECUTOR_OPTS` to include the YourKit Java Profiler agent, enabling profiling for Spark executors. This setup is necessary for detailed performance analysis of the executor processes in a Spark cluster. It requires YourKit Java Profiler to be installed.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nSPARK_EXECUTOR_OPTS+=\\\" -agentpath:/root/YourKit-JavaProfiler-2017.02/bin/linux-x86-64/libyjpagent.so=sampling\\\"\nexport SPARK_EXECUTOR_OPTS\n```\n\n----------------------------------------\n\nTITLE: Spark SQL Configuration Defaults\nDESCRIPTION: Configuration properties that changed defaults in Spark SQL 1.2\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2014-12-18-spark-release-1-2-0.md#2025-04-21_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nspark.sql.parquet.cacheMetadata=true\nspark.sql.parquet.compression.codec=gzip\nspark.sql.hive.convertMetastoreParquet=true\nspark.sql.inMemoryColumnarStorage.compressed=true\nspark.sql.inMemoryColumnarStorage.batchSize=10000\nspark.sql.autoBroadcastJoinThreshold=10485760\n```\n\n----------------------------------------\n\nTITLE: Incorrect pandas Filtering Example\nDESCRIPTION: Shows an example of incorrect manual optimization in pandas where the row-group filtering predicate is wrong, leading to incorrect results despite correct groupby logic.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/pandas-on-spark/index.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet(\n    \"G1_1e9_1e2_0_0.parquet\",\n    columns=[\"id1\", \"id2\", \"v3\"],\n    filters=[(\"id1\", \"==\", \"id001\")],\n    engine=\"pyarrow\",\n)\ndf.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n```\n\n----------------------------------------\n\nTITLE: Skipping API Documentation Generation with Jekyll\nDESCRIPTION: This snippet demonstrates how to skip the building and copying of Scala and Python API documentation during the Jekyll build process. This can be useful to reduce the build time.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.3/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n\"SKIP_API=1 jekyll\"\n```\n\n----------------------------------------\n\nTITLE: Building Spark with SBT\nDESCRIPTION: Commands for building Spark using SBT, including incremental builds on file changes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ build/sbt ~compile\n```\n\n----------------------------------------\n\nTITLE: Loading jQuery and Bootstrap Dependencies\nDESCRIPTION: HTML markup for importing jQuery 3.6.0, Bootstrap 5.3.1 CSS and JS bundles, and setting viewport meta tag for responsive design.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n<link href=\"deps/bootstrap-5.3.1/bootstrap.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/bootstrap-5.3.1/bootstrap.bundle.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Decimal Parsing Error Message in Scala\nDESCRIPTION: This improved error message provides more context about why a decimal couldn't be parsed, specifying the position where the error occurred.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nInvalid decimal {}; encountered error while parsing at position {}.\n```\n\n----------------------------------------\n\nTITLE: Spark Key Operations\nDESCRIPTION: Key-based fold operation for aggregating values by key in Spark RDDs.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-07-16-spark-release-0-7-3.md#2025-04-21_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nfoldByKey\n```\n\n----------------------------------------\n\nTITLE: Uploading GPG Key to Public Keyserver\nDESCRIPTION: Command to upload the generated GPG public key to the OpenPGP keyserver for distribution.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/release-process.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ gpg --keyserver hkps://keys.openpgp.org --send-key 26A27D33\n```\n\n----------------------------------------\n\nTITLE: HTML Links for Spark Release\nDESCRIPTION: HTML anchor tags linking to the Spark 3.4.3 release notes and download page, using Jekyll site.baseurl variable for URL construction.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2024-04-18-spark-3-4-3-released.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/releases/spark-release-3-4-3.html\" title=\"Spark Release 3.4.3\">Spark 3.4.3</a>\n```\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/releases/spark-release-3-4-3.html\" title=\"Spark Release 3.4.3\">release notes</a>\n```\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/downloads.html\">download</a>\n```\n\n----------------------------------------\n\nTITLE: Loading Frontend Dependencies in HTML\nDESCRIPTION: Configuration of essential frontend libraries and stylesheets including jQuery 3.6.0, Bootstrap 5.3.1, Font Awesome 6.5.2, Headroom 0.11.0, Bootstrap TOC 1.0.1, Clipboard.js 2.0.11, and search-related dependencies. These resources provide core functionality for the website's interface and user experience.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.5/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n<link href=\"deps/bootstrap-5.3.1/bootstrap.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/bootstrap-5.3.1/bootstrap.bundle.min.js\"></script>\n<link href=\"deps/font-awesome-6.5.2/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"deps/font-awesome-6.5.2/css/v4-shims.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/headroom-0.11.0/headroom.min.js\"></script>\n<script src=\"deps/headroom-0.11.0/jQuery.headroom.min.js\"></script>\n<script src=\"deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js\"></script>\n<script src=\"deps/clipboard.js-2.0.11/clipboard.min.js\"></script>\n<script src=\"deps/search-1.0.0/autocomplete.jquery.min.js\"></script>\n<script src=\"deps/search-1.0.0/fuse.min.js\"></script>\n<script src=\"deps/search-1.0.0/mark.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Direct Error Messages in Scala\nDESCRIPTION: Examples showing how to write clear and direct error messages without unnecessary politeness.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\n// Before\nLEGACY store assignment policy is disallowed in Spark data source V2. Please set the configuration spark.sql.storeAssignmentPolicy to other values.\n\n// After\nSpark data source V2 does not allow the LEGACY store assignment policy. Set the configuration spark.sql.storeAssignment to ANSI or STRICT.\n```\n\n----------------------------------------\n\nTITLE: Reading SequenceFile with Native Types in Spark\nDESCRIPTION: Shows how to read a Hadoop SequenceFile containing IntWritable and Text types using native Scala Int and String types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2011-07-14-spark-release-0-3.md#2025-04-21_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n// Will read a SequenceFile of (IntWritable, Text)\nval data = spark.sequenceFile[Int, String](\"hdfs://...\")\n```\n\n----------------------------------------\n\nTITLE: Include Clipboard.js Library\nDESCRIPTION: Includes the Clipboard.js library for copying text to the clipboard. It allows users to easily copy code snippets or other content.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_9\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/clipboard.js-2.0.11/clipboard.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Loading jQuery Library in HTML\nDESCRIPTION: Imports the jQuery 3.6.0 JavaScript library which provides DOM manipulation and event handling functionality.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Referencing XML Package\nDESCRIPTION: Demonstrates how to reference the XML package in Scala, which provides functionality for XML parsing, manipulation, and serialization.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_11\n\nLANGUAGE: Scala\nCODE:\n```\nscala.[[scala.xml]]\n```\n\n----------------------------------------\n\nTITLE: Including JavaScript and CSS Dependencies\nDESCRIPTION: The code snippet includes HTML tags to link external JavaScript and CSS files that are necessary for the front-end development of a web page. The dependencies include libraries for DOM manipulation, responsive design, and styling. All scripts and styles are loaded from local directories specified in the file paths.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n<link href=\"deps/bootstrap-5.3.1/bootstrap.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/bootstrap-5.3.1/bootstrap.bundle.min.js\"></script>\n<link href=\"deps/font-awesome-6.4.2/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"deps/font-awesome-6.4.2/css/v4-shims.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/headroom-0.11.0/headroom.min.js\"></script>\n<script src=\"deps/headroom-0.11.0/jQuery.headroom.min.js\"></script>\n<script src=\"deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js\"></script>\n<script src=\"deps/clipboard.js-2.0.11/clipboard.min.js\"></script>\n<script src=\"deps/search-1.0.0/autocomplete.jquery.min.js\"></script>\n<script src=\"deps/search-1.0.0/fuse.min.js\"></script>\n<script src=\"deps/search-1.0.0/mark.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Highlighting Code with Pygments in Markdown\nDESCRIPTION: This snippet demonstrates how to mark a block of code in a Markdown document to be syntax highlighted by Jekyll during the compilation process using Pygments. It uses Liquid syntax within the markdown.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.3/README.md#2025-04-21_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n\"    {% highlight scala %}\\n    // Your scala code goes here, you can replace scala with many other\\n    // supported languages too.\\n    {% endhighlight %}\"\n```\n\n----------------------------------------\n\nTITLE: Maven Profile Selection Commands\nDESCRIPTION: Maven command examples for selecting Hadoop versions and creating Debian packages during Spark build.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2013-02-27-spark-release-0-7-0.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n-Phadoop1\n-Phadoop2\nmvn -Phadoop1,deb install\n```\n\n----------------------------------------\n\nTITLE: Include Autocomplete jQuery Library\nDESCRIPTION: Includes the Autocomplete jQuery plugin for search suggestions. It provides real-time suggestions as the user types in a search box.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_10\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/search-1.0.0/autocomplete.jquery.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Loading Bootstrap CSS Framework\nDESCRIPTION: Imports the Bootstrap 5.3.1 CSS framework which provides responsive styling and layout components.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<link href=\"deps/bootstrap-5.3.1/bootstrap.min.css\" rel=\"stylesheet\" />\n```\n\n----------------------------------------\n\nTITLE: Referencing System Interaction Package\nDESCRIPTION: Shows how to reference the system interaction package in Scala, which provides functionality for interacting with other processes and the operating system.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_8\n\nLANGUAGE: Scala\nCODE:\n```\nscala.[[scala.sys]]\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration in YAML\nDESCRIPTION: YAML front matter block defining metadata for a Jekyll blog post announcing Spark 3.4.3 release, including layout, title, categories, and publication settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2024-04-18-spark-3-4-3-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.4.3 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Running Jekyll Web Server\nDESCRIPTION: This snippet shows how to build the Spark documentation and run a local web server using Jekyll to view the documentation. It specifies the command to start the server, which defaults to port 4000.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.3/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n\"jekyll --server\"\n```\n\n----------------------------------------\n\nTITLE: Maven Configuration for Spark 0.6.0\nDESCRIPTION: Maven coordinates for including Spark 0.6.0 as a dependency in Maven projects. Uses Spark core artifact for Scala 2.9.2.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2012-10-15-spark-release-0-6-0.md#2025-04-21_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\ngroupId: org.spark-project\nartifactId: spark-core_2.9.2\nversion: 0.6.0\n```\n\n----------------------------------------\n\nTITLE: HTML Links for Spark Release Documentation\nDESCRIPTION: HTML anchor tags linking to the Spark 1.6.0 release notes and download page, embedded within the announcement text.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2016-01-04-spark-1-6-0-released.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/releases/spark-release-1-6-0.html\" title=\"Spark Release 1.6.0\">Spark 1.6.0</a>\n```\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/releases/spark-release-1-6-0.html\" title=\"Spark Release 1.6.0\">release notes</a>\n```\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/downloads.html\">download</a>\n```\n\n----------------------------------------\n\nTITLE: Loading Headroom.js Library\nDESCRIPTION: Imports the Headroom.js 0.11.0 library which provides functionality for hiding/showing header elements based on scroll direction.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/headroom-0.11.0/headroom.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Front Matter for Spark 3.0.3 Release Post\nDESCRIPTION: YAML configuration for a Jekyll blog post announcing the release of Apache Spark 3.0.3. It sets metadata such as layout, title, categories, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2021-06-23-spark-3-0-3-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.0.3 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Referencing Immutable Collections Package\nDESCRIPTION: Shows how to reference the immutable collections package in Scala, which is the default collection type.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_2\n\nLANGUAGE: Scala\nCODE:\n```\n[[scala.collection.immutable]]\n```\n\n----------------------------------------\n\nTITLE: Displaying Git Remote Configuration\nDESCRIPTION: Shows the expected output of 'git remote -v' command for a properly configured Spark development environment. It includes remotes for the Apache repository, GitHub mirror, and personal fork.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/committers.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napache\tgit@github.com:apache/spark.git (fetch)\napache\tgit@github.com:apache/spark.git (push)\napache-github\tgit@github.com:apache/spark.git (fetch)\napache-github\tgit@github.com:apache/spark.git (push)\norigin\tgit@github.com:[your username]/spark.git (fetch)\norigin\tgit@github.com:[your username]/spark.git (push)\n```\n\n----------------------------------------\n\nTITLE: Jekyll Post Frontmatter Configuration in YAML\nDESCRIPTION: YAML frontmatter block defining metadata for a Jekyll blog post announcing Spark 2.3.2 release, including layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2018-09-24-spark-2-3-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.3.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading jQuery Autocomplete Plugin for Search\nDESCRIPTION: Imports the jQuery autocomplete plugin for search functionality, enabling dropdown suggestions during user input.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_10\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/search-1.0.0/autocomplete.jquery.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Blog Post\nDESCRIPTION: YAML configuration block defining metadata for a blog post announcing Spark 3.0.1 release, including layout, title, categories, and publishing settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2020-09-08-spark-3-0-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.0.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Referencing Scala Core Package\nDESCRIPTION: Shows how to reference the core Scala package, which contains fundamental types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\n[[scala]]\n```\n\n----------------------------------------\n\nTITLE: Time-Independent Error Messages in Scala\nDESCRIPTION: Examples showing how to remove time-based statements and promises of future support from error messages.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n// Before\nPandas UDF aggregate expressions are currently not supported in pivot.\nParquet type not yet supported: {}.\n\n// After\nPivot does not support Pandas UDF aggregate expressions.\n{} does not support Parquet type.\n```\n\n----------------------------------------\n\nTITLE: Configuring Jekyll Blog Post Front Matter in Markdown\nDESCRIPTION: This code snippet shows the front matter configuration for a Jekyll blog post. It sets various metadata fields including layout, title, categories, tags, status, type, and publication status. It also includes some WordPress-specific metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2013-06-21-spark-featured-in-wired.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: post\ntitle: Spark featured in Wired\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading Mark.js for Search Term Highlighting\nDESCRIPTION: Imports the Mark.js library which provides text highlighting functionality for search results.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_12\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/search-1.0.0/mark.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Defining Maven Coordinates for Spark Core\nDESCRIPTION: This snippet provides the Maven coordinates for including Spark Core as a dependency in a project. It specifies the groupId, artifactId, and version for Spark 3.5.5 with Scala 2.12.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/downloads.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ngroupId: org.apache.spark\nartifactId: spark-core_2.12\nversion: 3.5.5\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 2.4.2 release announcement blog post, including layout, categories, publication status and WordPress metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2019-04-23-spark-2-4-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.4.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading Bootstrap TOC Plugin\nDESCRIPTION: Imports the Bootstrap TOC 1.0.1 library which provides table of contents functionality for Bootstrap-based documentation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_8\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Jekyll Blog Post YAML Front Matter\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 3.3.1 release. Defines post metadata including layout, title, categories, tags and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2022-10-25-spark-3-3-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.3.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Jekyll Blog Post Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 2.3.1 release, including layout, title, categories, and metadata settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2018-06-08-spark-2-3-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.3.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 3.3.2 release announcement blog post, including layout, title, categories, and publication settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2023-02-17-spark-3-3-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.3.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 2.4.1 release announcement blog post, including layout, title, categories, and publishing settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2019-03-31-spark-2-4-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.4.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading Headroom.js jQuery Plugin\nDESCRIPTION: Imports the jQuery plugin for Headroom.js which allows for easier integration with jQuery-based projects.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_7\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/headroom-0.11.0/jQuery.headroom.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Front Matter Configuration in YAML\nDESCRIPTION: YAML configuration block defining the page layout, title, type and navigation properties for the Spark powered-by page.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/powered-by.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: global\ntitle: Powered By Spark\ntype: \"page singular\"\nnavigation:\n  weight: 5\n  show: true\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Jekyll Blog Post Front Matter\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post, specifying layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2014-05-11-spark-summit-agenda-posted.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark Summit agenda posted\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Maven\nDESCRIPTION: Commands for running individual Scala and Java tests using Maven.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbuild/mvn -Dtest=none -DwildcardSuites=org.apache.spark.scheduler.DAGSchedulerSuite test\n```\n\nLANGUAGE: bash\nCODE:\n```\nbuild/mvn test -DwildcardSuites=none -Dtest=org.apache.spark.streaming.JavaAPISuite test\n```\n\n----------------------------------------\n\nTITLE: Defining Jekyll Post Metadata in Markdown\nDESCRIPTION: This snippet defines the metadata for a Jekyll blog post, including layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2021-02-19-spark-3-0-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.0.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Generating News Articles List with Jekyll and Liquid\nDESCRIPTION: This snippet uses Jekyll and Liquid templating to create a list of news articles. It iterates through posts in the 'news' category, displaying each post's title as a link, publication date, and excerpt.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/index.md#2025-04-21_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n{% for post in site.categories.news %}\n  <article class=\"hentry\">\n    <header class=\"entry-header\">\n      <h3 class=\"entry-title\"><a href=\"{{ post.url }}\">{{ post.title }}</a></h3>\n      <div class=\"entry-date\">{{post.date | date: \"%B %-d, %Y\"}}</div>\n    </header>\n    <div class=\"entry-content\">{{post.excerpt}}</div>\n  </article>\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 1.6.2 release announcement blog post. Includes layout, title, categories, publication status and WordPress metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2016-06-25-spark-1-6-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 1.6.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading Clipboard.js Library\nDESCRIPTION: Imports the Clipboard.js 2.0.11 library which provides copy-to-clipboard functionality for web applications.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_9\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/clipboard.js-2.0.11/clipboard.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Defining 404 Page Layout and Metadata in Markdown\nDESCRIPTION: This snippet sets up the page layout, title, and license information for the 404 Not Found page using YAML front matter in Markdown.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/404.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: global\ntitle: 404 Not Found\nlicense: |\n  Licensed to the Apache Software Foundation (ASF) under one or more\n  contributor license agreements.  See the NOTICE file distributed with\n  this work for additional information regarding copyright ownership.\n  The ASF licenses this file to You under the Apache License, Version 2.0\n  (the \"License\"); you may not use this file except in compliance with\n  the License.  You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for a blog post announcing Spark 2.2.1 release. Includes layout, title, categories, publication status and WordPress metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2017-12-01-spark-2-2-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.2.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Remote for Pull Requests\nDESCRIPTION: Git configuration to enable fetching pull request data by modifying the .git/config file\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_9\n\nLANGUAGE: gitconfig\nCODE:\n```\n[remote \"origin\"]\n  url = git@github.com:apache/spark.git\n  fetch = +refs/heads/*:refs/remotes/origin/*\n  fetch = +refs/pull/*/head:refs/remotes/origin/pr/*   # Add this line\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the blog post about Spark 1.2.2 release, including layout, title, publishing status and WordPress-specific metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2015-04-17-spark-release-1-2-2.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark Release 1.2.2\ncategories: []\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Blog Post\nDESCRIPTION: YAML configuration block defining metadata for a blog post about the Spark+AI Summit 2020 announcement, including layout, title, categories, publication status and other WordPress-related settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2020-06-15-spark-ai-summit-june-2020-agenda-posted.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark+AI Summit (June 22-25th, 2020, VIRTUAL) agenda posted\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Blog Post YAML Frontmatter Configuration\nDESCRIPTION: YAML configuration metadata for the blog post defining the layout, title, categories, and publication settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2018-02-28-spark-2-3-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.3.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: HTML Announcement Content\nDESCRIPTION: HTML markup for the release announcement with links to release notes and details about the new version.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2012-10-15-spark-version-0-6-0-released.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"{{site.baseurl}}/releases/spark-release-0-6-0.html\">Spark version 0.6.0</a> was released today, a major release that brings a wide range of performance improvements and new features, including a simpler standalone deploy mode and a Java API. Read more about it in the <a href=\"{{site.baseurl}}/releases/spark-release-0-6-0.html\">release notes</a>.\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 1.6.1 release announcement blog post, including layout, title, categories, and publishing settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2016-03-09-spark-1-6-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 1.6.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Using Git to Amend Commits Before Merging in Apache Spark\nDESCRIPTION: Instructions for amending a commit before pushing to Apache. This process involves letting the merge script wait, making code changes in a separate window, using git rebase to squash commits, and verifying the result.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/committers.md#2025-04-21_snippet_1\n\nLANGUAGE: git\nCODE:\n```\ngit rebase -i HEAD~2\n```\n\n----------------------------------------\n\nTITLE: Configuring Page Redirection and Metadata in YAML for Apache Spark Website\nDESCRIPTION: This YAML configuration sets up a page redirection from 'Mailing Lists' to 'community.html'. It defines the layout, title, page type, and navigation properties for the page in the Apache Spark website project.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/mailing-lists.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: global\ntitle: Mailing Lists\nredirect: community.html\ntype: \"page singular\"\nnavigation:\n  weight: 5\n  show: true\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Release Post\nDESCRIPTION: YAML configuration block defining the blog post metadata including layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2018-06-29-spark-2-1-3-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.1.3 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: jQuery UI Software License Text\nDESCRIPTION: MIT-style open-source license granting broad permissions for software use, modification, and distribution with limited liability\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview1/api/java/legal/jqueryUI.md#2025-04-21_snippet_0\n\nLANGUAGE: license\nCODE:\n```\nCopyright jQuery Foundation and other contributors, https://jquery.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software...\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for History Page\nDESCRIPTION: YAML configuration block that defines the page layout, title, type, and navigation properties for the Apache Spark history page.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/history.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: global\ntitle: History\ntype: \"page singular\"\nnavigation:\n  weight: 5\n  show: true\n---\n```\n\n----------------------------------------\n\nTITLE: Adding JIRA ID Test Case in Python\nDESCRIPTION: Example of how to add a JIRA ID reference in a Python test case. The test should include a comment with the JIRA ticket number and description.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/contributing.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef test_case(self):\n    # SPARK-12345: a short description of the test\n    ...\n\n```\n\n----------------------------------------\n\nTITLE: jQuery License Text\nDESCRIPTION: This snippet contains the license text for jQuery v3.6.1. It grants permissions for use, modification, and distribution, subject to the conditions outlined in the notice.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/java/legal/jquery.md#2025-04-21_snippet_0\n\nLANGUAGE: None\nCODE:\n```\njQuery v 3.6.1\nCopyright OpenJS Foundation and other contributors, https://openjsf.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n******************************************\n\nThe jQuery JavaScript Library v3.6.1 also includes Sizzle.js\n\nSizzle.js includes the following license:\n\nCopyright JS Foundation and other contributors, https://js.foundation/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/sizzle\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n\n*********************\n\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 2.4.6 release announcement blog post, including layout, title, categories, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2020-06-05-spark-2-4-6.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.4.6 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Non-existent Path Error Message in Scala\nDESCRIPTION: This concise error message clearly states the problem and implicitly suggests how to fix it by using a different path.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nPath does not exist: {}\n```\n\n----------------------------------------\n\nTITLE: Verifying Squashed Commits in Apache Spark PR Merging\nDESCRIPTION: Command to verify that multiple commits have been properly squashed into a single change before completing the PR merge process.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/committers.md#2025-04-21_snippet_2\n\nLANGUAGE: git\nCODE:\n```\ngit log\n```\n\n----------------------------------------\n\nTITLE: jQuery UI License\nDESCRIPTION: This snippet presents the licensing terms for jQuery UI v1.13.2. It outlines the permissions granted to users regarding the software and associated documentation, including rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/java/legal/jqueryUI.md#2025-04-21_snippet_0\n\nLANGUAGE: None\nCODE:\n```\nCopyright jQuery Foundation and other contributors, https://jquery.org/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/jquery-ui\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nCopyright and related rights for sample code are waived via CC0. Sample\ncode is defined as all source code contained within the demos directory.\n\nCC0: http://creativecommons.org/publicdomain/zero/1.0/\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 3.2.0 release, including layout, title, categories, and publishing metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2021-10-13-spark-3-2-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.2.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Complete Unit Test Implementation for PySpark ETL\nDESCRIPTION: A complete implementation of unit tests for the PySpark ETL module. Includes the base PySparkTestCase class and a specific test case for the remove_extra_spaces function that verifies it correctly transforms the input data.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# pkg/test_etl.py\nimport unittest\n\nfrom pyspark.sql import SparkSession \n\n# Define unit test base class\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate() \n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n        \n# Define unit test\nclass TestTranformation(PySparkTestCase):\n    def test_single_space(self):\n        sample_data = [{\"name\": \"John    D.\", \"age\": 30}, \n                        {\"name\": \"Alice   G.\", \"age\": 25}, \n                        {\"name\": \"Bob  T.\", \"age\": 35}, \n                        {\"name\": \"Eve   A.\", \"age\": 28}] \n                \n        # Create a Spark DataFrame\n        original_df = spark.createDataFrame(sample_data)\n    \n        # Apply the transformation function from before\n        transformed_df = remove_extra_spaces(original_df, \"name\")\n    \n        expected_data = [{\"name\": \"John D.\", \"age\": 30}, \n        {\"name\": \"Alice G.\", \"age\": 25}, \n        {\"name\": \"Bob T.\", \"age\": 35}, \n        {\"name\": \"Eve A.\", \"age\": 28}]\n    \n        expected_df = spark.createDataFrame(expected_data)\n    \n        assertDataFrameEqual(transformed_df, expected_df)\n```\n\n----------------------------------------\n\nTITLE: Present Tense Error Messages in Scala\nDESCRIPTION: Examples demonstrating the use of present tense in error messages for clarity and immediacy.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/error-message-guidelines.md#2025-04-21_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n// Before\nCouldn't find the reference column for {} at {}.\nJoin strategy hint parameter should be an identifier or string but was {}.\n\n// After\nCannot find the reference column for {} at {}.\nCannot use join strategy hint parameter {}. Use a table name or identifier to specify the parameter.\n```\n\n----------------------------------------\n\nTITLE: Excluding Binary Incompatibility in MiMa\nDESCRIPTION: Scala code snippet for excluding a binary incompatibility in MiMa checks.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n// [SPARK-zz][CORE] Fix an issue\nProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.SomeClass.this\")\n```\n\n----------------------------------------\n\nTITLE: Including jQuery and Bootstrap Dependencies for Apache Spark Website\nDESCRIPTION: This HTML code snippet includes external JavaScript and CSS files for jQuery and Bootstrap, as well as setting the viewport meta tag for responsive design. These dependencies are crucial for the functionality and styling of the Apache Spark website.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.0/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n<link href=\"deps/bootstrap-5.2.2/bootstrap.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/bootstrap-5.2.2/bootstrap.bundle.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Triggering IntelliJ Remote Debugging - SBT Configuration\nDESCRIPTION: This shell command configures the JVM for remote debugging via IntelliJ using SBT. It sets the necessary javaOptions to facilitate the connection to the IntelliJ debugger, allowing breakpoints to be set and code execution to be analyzed during tests.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/developer-tools.md#2025-04-21_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsbt > set javaOptions in Test += \"-agentlib:jdwp=transport=dt_socket,server=n,suspend=n,address=localhost:5005\"\n```\n\n----------------------------------------\n\nTITLE: Loading Frontend Dependencies in HTML\nDESCRIPTION: HTML markup that loads essential frontend dependencies including jQuery, Bootstrap, Font Awesome icons, and various utility libraries for functionality like headroom scrolling, clipboard operations, and search features. The dependencies are loaded from a local deps directory.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.4/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n<link href=\"deps/bootstrap-5.3.1/bootstrap.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/bootstrap-5.3.1/bootstrap.bundle.min.js\"></script>\n<link href=\"deps/font-awesome-6.5.2/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"deps/font-awesome-6.5.2/css/v4-shims.min.css\" rel=\"stylesheet\" />\n<script src=\"deps/headroom-0.11.0/headroom.min.js\"></script>\n<script src=\"deps/headroom-0.11.0/jQuery.headroom.min.js\"></script>\n<script src=\"deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js\"></script>\n<script src=\"deps/clipboard.js-2.0.11/clipboard.min.js\"></script>\n<script src=\"deps/search-1.0.0/autocomplete.jquery.min.js\"></script>\n<script src=\"deps/search-1.0.0/fuse.min.js\"></script>\n<script src=\"deps/search-1.0.0/mark.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Include jQuery Library\nDESCRIPTION: Includes the jQuery library for DOM manipulation and event handling.  It's a prerequisite for many other JavaScript libraries.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Set Viewport Meta Tag\nDESCRIPTION: Sets the viewport meta tag for responsive design. It configures the viewport to the device width and disables initial scaling.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n```\n\n----------------------------------------\n\nTITLE: Include Headroom Library\nDESCRIPTION: Includes the Headroom JavaScript library for hiding/showing the header on scroll.  This enhances the user experience on long pages.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_6\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/headroom-0.11.0/headroom.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Adding JIRA ID Test Case in Java\nDESCRIPTION: Example of how to add a JIRA ID reference in a Java test case using JUnit annotations. The test should include a comment with the JIRA ticket number and description.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/contributing.md#2025-04-21_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@Test\npublic void testCase() {\n  // SPARK-12345: a short description of the test\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Remote Spark Session in Python\nDESCRIPTION: This code shows how to create a DataFrame using the remote Spark session. It creates a sample DataFrame with various data types including integers, floats, strings, dates, and datetimes.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark DataFrame with Explicit Schema\nDESCRIPTION: Shows how to create a PySpark DataFrame by providing data as a list of tuples and specifying the schema explicitly. This method offers more control over data types.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.3/api/python/getting_started/quickstart_df.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\n```\n\n----------------------------------------\n\nTITLE: Referencing spark-shell in Markdown\nDESCRIPTION: This snippet demonstrates how to reference the spark-shell command in Markdown format. It's used to highlight a new feature in Spark 0.7.3 that allows adding JARs to a spark-shell session.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2013-07-16-spark-0-7-3-released.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<code>spark-shell</code>\n```\n\n----------------------------------------\n\nTITLE: StackOverflow Tag Reference\nDESCRIPTION: Reference to the Apache Spark tag used on StackOverflow for getting community help and support.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/faq.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napache-spark\n```\n\n----------------------------------------\n\nTITLE: Structuring HTML Content for Apache Spark Documentation Page\nDESCRIPTION: This HTML snippet structures the content for the Apache Spark documentation page. It includes headers, lists of documentation links for different Spark versions, and sections for additional resources like videos and meetup talks.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/documentation.md#2025-04-21_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<h2><span class=\"text-capitalize\">Apache Spark<span class=\"tm\">&trade;</span></span> Documentation</h2>\n\n<p>Setup instructions, programming guides, and other documentation are available for each stable version of Spark below:</p>\n\n<ul>\n  <li><a href=\"{{site.baseurl}}/docs/3.5.5/\">Spark 3.5.5</a></li>\n  <li><a href=\"{{site.baseurl}}/docs/3.5.4/\">Spark 3.5.4</a></li>\n  <!-- ... more version links ... -->\n</ul>\n\n<p>Documentation for preview releases:</p>\n\n<ul>\n  <li><a href=\"{{site.baseurl}}/docs/4.0.0-preview2/\">Spark 4.0.0 preview2</a></li>\n  <!-- ... more preview version links ... -->\n</ul>\n\n<h3>Videos</h3>\n<!-- ... video section content ... -->\n\n<h4><a name=\"meetup-videos\"></a>Meetup Talk Videos</h4>\n<ul>\n  <li><a href=\"https://www.youtube.com/watch?v=NUQ-8to2XAk&list=PL-x35fyliRwiP3YteXbnhk0QGOtYLBT3a\">Spark 1.0 and Beyond</a> (<a href=\"http://files.meetup.com/3138542/Spark%201.0%20Meetup.ppt\">slides</a>) <span class=\"video-meta-info\">by Patrick Wendell, at Cisco in San Jose, 2014-04-23</span></li>\n  <!-- ... more meetup video links ... -->\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Jekyll Post Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for Jekyll blog post announcing Spark 3.3.0 release. Defines post metadata including layout, title, categories, and publishing status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2022-06-16-spark-3-3-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.3.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: Jekyll/blog post configuration metadata defining the post properties including layout, title, categories, publication status and metadata fields.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2020-06-18-spark-3-0-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.0.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Documentation using Jekyll\nDESCRIPTION: This snippet shows how to use Jekyll to generate the HTML version of the Spark documentation from Markdown files. It includes an option to skip building the Scaladoc to speed up the process.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.3/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n\"SKIP_SCALADOC=1 jekyll\"\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for a blog post announcing Spark 3.1.2 release, including layout, title, categories, and publication settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2021-06-01-spark-3-1-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.1.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 3.4.2 release announcement blog post, including layout, title, categories, and publishing settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2023-11-30-spark-3-4-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.4.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Generating PySpark Epydoc API Documentation\nDESCRIPTION: This snippet illustrates the command to build the PySpark Epydoc API documentation.  This should be run from the SPARK_PROJECT_ROOT/pyspark directory.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.3/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n\"epydoc --config epydoc.conf\"\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 3.2.2 release announcement blog post, including layout settings, categories, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2022-07-17-spark-3-2-2-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.2.2 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Defining Blog Post Metadata in Markdown\nDESCRIPTION: This code snippet defines the metadata for a blog post using YAML front matter in Markdown. It specifies the layout, title, categories, tags, status, type, and publication details for the Spark 3.2.4 release announcement.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2023-04-13-spark-3-2-4-released.md#2025-04-21_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.2.4 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Running Jekyll to Generate HTML Documentation\nDESCRIPTION: Commands for using Jekyll to generate HTML documentation from Markdown files. Includes options for skipping Scaladoc generation and running a local webserver.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\njekyll\n```\n\nLANGUAGE: shell\nCODE:\n```\nSKIP_SCALADOC=1 jekyll\n```\n\nLANGUAGE: shell\nCODE:\n```\njekyll --server\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation of Apache Spark Updates\nDESCRIPTION: Comprehensive documentation of feature updates and improvements in Apache Spark, organized by major components including Data Source V2 API, Kubernetes, Node Decommission, Push-based shuffle, and Structured Streaming.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n#### Data Source V2 API\n\n* New interfaces\n    * Introduce a new DataSource V2 interface HasPartitionKey (SPARK-37376)\n    * Add interface SupportsPushDownV2Filters (SPARK-36760)\n    * Support DataSource V2 CreateTempViewUsing (SPARK-35803)\n    * Add a class to represent general aggregate functions in DS V2 (SPARK-37789)\n    * A new framework to represent catalyst expressions in DS V2 APIs (SPARK-37960)\n    * Add APIs for group-based row-level operations (SPARK-38625)\n\n#### Kubernetes Enhancements\n\n* Executor Rolling in Kubernetes environment (SPARK-37810)\n* Support Customized Kubernetes Schedulers (SPARK-36057)\n* executorIdleTimeout is not working for pending pods on K8s (SPARK-37049)\n\n#### Node Decommission\n\n* FallbackStorage shouldn't attempt to resolve arbitrary \"remote\" hostname (SPARK-38062)\n* ExecutorMonitor.onExecutorRemoved should handle ExecutorDecommission as finished (SPARK-38023)\n\n#### Push-based shuffle\n\n* Adaptive shuffle merge finalization for push-based shuffle (SPARK-33701)\n* Adaptive fetch of shuffle mergers for Push based shuffle (SPARK-34826)\n* Skip diagnosis ob merged blocks from push-based shuffle (SPARK-37695)\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 2.2.3 release announcement blog post, including layout, title, categories, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2019-01-11-spark-release-2-2-3.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.2.3 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Connect Server with Shell Script\nDESCRIPTION: This snippet demonstrates how to launch a Spark server that supports Spark Connect sessions using a shell script. It requires setting the appropriate Spark version and the script should be run in a compatible shell environment.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.1/api/python/getting_started/quickstart_connect.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 3.5.3 release, including layout settings, categories, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2024-09-24-spark-3-5-3-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.5.3 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Building Spark Scaladoc\nDESCRIPTION: Command to build Spark Scaladoc documentation using SBT.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsbt/sbt doc\n```\n\n----------------------------------------\n\nTITLE: Adding Syntax Highlighting in Markdown with Pygments for Scala Code\nDESCRIPTION: This snippet demonstrates how to mark a block of code in markdown to be syntax highlighted by Jekyll during the compilation phase. The example shows the template for Scala code, but other supported languages can be used instead.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.6.1/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% highlight scala %}\n// Your scala code goes here, you can replace scala with many other\n// supported languages too.\n{% endhighlight %}\n```\n\n----------------------------------------\n\nTITLE: Checking Table Existence in PySpark Catalog\nDESCRIPTION: Using the tableExists method from pyspark.sql.catalog to check if a table exists in the catalog before performing operations.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/releases/_posts/2022-06-16-spark-release-3-3-0.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npyspark.sql.catalog.tableExists\n```\n\n----------------------------------------\n\nTITLE: Syntax Highlighting with Pygments in Markdown\nDESCRIPTION: This snippet illustrates how to use Pygments for syntax highlighting in Markdown files during Jekyll's compile phase. Encapsulate code with `{% highlight language %}` and `{% endhighlight %}` tags. Pygments must be installed as it requires Python.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.6.0/README.md#2025-04-21_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n{% highlight scala %}\n// Your scala code goes here, you can replace scala with many other\n// supported languages too.\n{% endhighlight %}\n```\n\n----------------------------------------\n\nTITLE: Referencing Regular Expression Package\nDESCRIPTION: Demonstrates how to reference the regular expression package in Scala, which provides pattern matching functionality for text using regular expressions.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_9\n\nLANGUAGE: Scala\nCODE:\n```\nscala.util.[[scala.util.matching]]\n```\n\n----------------------------------------\n\nTITLE: Skipping API Documentation Build in Jekyll\nDESCRIPTION: Command to run Jekyll without building and copying Scala and Python API documentation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.2/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nSKIP_API=1 jekyll\n```\n\n----------------------------------------\n\nTITLE: PySpark Unittest Setup\nDESCRIPTION: Defines a base test class with SparkSession setup and teardown using unittest\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/getting_started/testing_pyspark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport unittest\n\nclass PySparkTestCase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate() \n\n    \n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n```\n\n----------------------------------------\n\nTITLE: Bootstrap Library Header Comment\nDESCRIPTION: Version and licensing information for Bootstrap v5.2.3\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/python/_static/scripts/bootstrap.js.LICENSE.txt#2025-04-21_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*!\n  * Bootstrap v5.2.3 (https://getbootstrap.com/)\n  * Copyright 2011-2022 The Bootstrap Authors (https://github.com/twbs/bootstrap/graphs/contributors)\n  * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE)\n  */\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Front Matter for Spark 3.1.1 Release Post in Markdown\nDESCRIPTION: This snippet defines the YAML front matter for a blog post announcing the release of Apache Spark 3.1.1. It specifies metadata such as layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2021-03-02-spark-3-1-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.1.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Jekyll YAML Front Matter Configuration\nDESCRIPTION: YAML configuration block for a Jekyll blog post announcing Spark 3.5.5 release, including layout, title, categories, and publication settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2025-02-27-spark-3-5-5-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 3.5.5 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Jekyll Blog Post\nDESCRIPTION: Jekyll blog post metadata configuration defining the post title, categories, publication status, and other WordPress-related metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2021-04-28-new-repository-service.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: New repository service for spark-packages\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter for Spark Release Post\nDESCRIPTION: YAML front matter defining the metadata for the Spark 2.3.3 release announcement blog post. Includes post title, categories, tags, status and type.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2019-02-15-spark-2-3-3-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.3.3 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Blog Post\nDESCRIPTION: YAML front matter configuration block that defines metadata for a blog post about Spark Summit San Francisco call for presentations. Includes layout, title, categories, tags, status and publishing settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2016-02-11-submit-talks-to-spark-summit-2016.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Submission is open for Spark Summit San Francisco\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Front Matter for Jekyll Blog Post\nDESCRIPTION: This YAML front matter defines metadata for a Jekyll blog post, including layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2019-06-03-plan-for-dropping-python-2-support.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Plan for dropping Python 2 support\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Blog Post\nDESCRIPTION: YAML configuration block defining metadata for a blog post about Spark Summit East 2016, including layout, title, categories, publication status and other WordPress-related settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2015-10-14-submit-talks-to-spark-summit-east-2016.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Submission is open for Spark Summit East 2016\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Jekyll Blog Post Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 0.6.0 release, including layout, title, categories and publication settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2012-10-15-spark-version-0-6-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark version 0.6.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Blog Post\nDESCRIPTION: YAML configuration block defining metadata for a blog post about Spark Summit East 2015 videos, including layout, title, categories, publishing status and WordPress-specific metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2015-04-20-spark-summit-east-2015-videos-posted.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark Summit East 2015 Videos Posted\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the Spark 2.4.5 release announcement blog post, including layout, title, categories, publication status and WordPress metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2020-02-08-spark-2-4-5-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.4.5 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading Bootstrap JavaScript Bundle\nDESCRIPTION: Imports the Bootstrap 5.3.1 JavaScript bundle which includes all interactive components and utilities.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"deps/bootstrap-5.3.1/bootstrap.bundle.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Listing Spark Screencasts with Jinja\nDESCRIPTION: This snippet iterates through screencast posts using Jinja templating. For each post, it displays the title as a link, the date, and an excerpt of the content.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/screencasts/index.md#2025-04-21_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n{% for post in site.categories.screencasts %}\n  <article class=\"hentry\">\n    <header class=\"entry-header\">\n      <h1 class=\"entry-title\"><a href=\"{{ post.url }}\">{{ post.title }}</a></h1>\n      <div class=\"entry-meta\">{{post.date | date: \"%B %d, %Y\"}}</div>\n    </header>\n    <div class=\"entry-content\">{{post.excerpt}}</div>\n  </article>\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Jekyll Blog Post\nDESCRIPTION: YAML configuration block defining metadata for a Jekyll blog post about Spark screencasts, including layout, title, categories, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2013-04-16-spark-screencasts-published.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark screencasts published\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '2'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Initializing Download UI with JavaScript\nDESCRIPTION: This script initializes the download selection interface and release notes when the page loads. It calls two functions: initDownloads() and initReleaseNotes().\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/downloads.md#2025-04-21_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.onload = function () {\n  $(document).ready(function() {\n    initDownloads();\n    initReleaseNotes();\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for History Content\nDESCRIPTION: HTML markup that structures the history content, including the page heading with trademark symbol and paragraphs describing Spark's evolution with hyperlinks to related resources.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/history.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<h2><span class=\"text-capitalize\">Apache Spark<span class=\"tm\">&trade;</span></span> history</h2>\n\n<p>\nApache Spark started as a research project at the <a href=\"https://amplab.cs.berkeley.edu\">UC Berkeley AMPLab</a>\nin 2009, and was open sourced in early 2010.\nMany of the ideas behind the system were presented in various\n<a href=\"{{site.baseurl}}/research.html\">research papers</a> over the years.\n</p>\n\n<p>\nAfter being released, Spark grew into a broad developer community, and moved to the\n<a href=\"https://www.apache.org\">Apache Software Foundation</a> in 2013.\nToday, the project is developed collaboratively by a community of hundreds of developers from\nhundreds of organizations.\n</p>\n\n<p>\nYou can get involved in Apache Spark development by reading\n<a href=\"{{site.baseurl}}/contributing.html\">how to contribute</a>.\n</p>\n```\n\n----------------------------------------\n\nTITLE: Setting Responsive Viewport Meta Tag\nDESCRIPTION: Defines the viewport properties for responsive design, ensuring proper scaling and dimensions on different devices.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\n```\n\n----------------------------------------\n\nTITLE: Loading Font Awesome Icon Library\nDESCRIPTION: Imports the Font Awesome 6.4.2 CSS library which provides a comprehensive collection of icons.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<link href=\"deps/font-awesome-6.4.2/css/all.min.css\" rel=\"stylesheet\" />\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML configuration block defining metadata for the blog post announcing Spark 1.6.0 release, including layout, categories, publication status and WordPress metadata.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2016-01-04-spark-1-6-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 1.6.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Loading Font Awesome v4 Compatibility Shims\nDESCRIPTION: Imports the Font Awesome v4 shims CSS file that ensures backward compatibility with Font Awesome version 4 icon references.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/3.5.2/api/R/deps/data-deps.txt#2025-04-21_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<link href=\"deps/font-awesome-6.4.2/css/v4-shims.min.css\" rel=\"stylesheet\" />\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration for Spark Release Post\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 2.4.4 release. Defines post metadata including layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2019-09-01-spark-2-4-4-released.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlayout: post\ntitle: Spark 2.4.4 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Include Bootstrap Bundle\nDESCRIPTION: Includes the Bootstrap JavaScript bundle, which contains core Bootstrap components and Popper.js for tooltips and popovers.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_3\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/bootstrap-5.3.1/bootstrap.bundle.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Include Font Awesome Shims CSS\nDESCRIPTION: Includes Font Awesome CSS shims for compatibility with older versions. It provides support for legacy Font Awesome icons.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_5\n\nLANGUAGE: HTML\nCODE:\n```\n<link href=\"deps/font-awesome-6.4.2/css/v4-shims.min.css\" rel=\"stylesheet\" />\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Front Matter for Spark 1.5.0 Release Post\nDESCRIPTION: This snippet defines the YAML front matter for a Jekyll blog post announcing the release of Spark 1.5.0. It specifies metadata such as layout, title, categories, tags, and publication status.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2015-09-09-spark-1-5-0-released.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: post\ntitle: Spark 1.5.0 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Include Bootstrap Toc Library\nDESCRIPTION: Includes the Bootstrap Table of Contents (TOC) plugin.  This automatically generates a table of contents based on the headings in the document, improving navigation.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/R/deps/data-deps.txt#2025-04-21_snippet_8\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Markdown Blog Post Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for a Jekyll blog post announcing Spark 0.8.1 release, including metadata like categories, publication status, and WordPress integration settings.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/news/_posts/2013-12-19-spark-0-8-1-released.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: post\ntitle: Spark 0.8.1 released\ncategories:\n- News\ntags: []\nstatus: publish\ntype: post\npublished: true\nmeta:\n  _edit_last: '4'\n  _wpas_done_all: '1'\n---\n```\n\n----------------------------------------\n\nTITLE: Referencing Actors Package\nDESCRIPTION: Demonstrates how to reference the actors package in Scala, which provides a concurrency framework inspired by Erlang.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_5\n\nLANGUAGE: Scala\nCODE:\n```\nscala.[[scala.actors]]\n```\n\n----------------------------------------\n\nTITLE: jQuery UI MIT License Text\nDESCRIPTION: The complete license text for jQuery UI v1.13.2, including the MIT license terms, CC0 waiver for sample code, and notice about external dependencies.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/java/legal/jqueryUI.md#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright jQuery Foundation and other contributors, https://jquery.org/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/jquery-ui\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nCopyright and related rights for sample code are waived via CC0. Sample\ncode is defined as all source code contained within the demos directory.\n\nCC0: http://creativecommons.org/publicdomain/zero/1.0/\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n```\n\n----------------------------------------\n\nTITLE: Referencing Parser Combinator Package\nDESCRIPTION: Shows how to reference the parser combinator package in Scala, which provides composable combinators for parsing.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/0.7.0/api/core/lib/rootdoc.txt#2025-04-21_snippet_10\n\nLANGUAGE: Scala\nCODE:\n```\nscala.util.parsing.[[scala.util.parsing.combinator]]\n```\n\n----------------------------------------\n\nTITLE: jQuery and Sizzle.js License Text\nDESCRIPTION: The complete MIT license text for jQuery v3.6.1 and Sizzle.js, including copyright notices, permissions, conditions, and warranty disclaimers. Also includes information about externally maintained libraries in node_modules.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/site/docs/4.0.0-preview2/api/java/legal/jquery.md#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\njQuery v 3.6.1\nCopyright OpenJS Foundation and other contributors, https://openjsf.org/\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n******************************************\n\nThe jQuery JavaScript Library v3.6.1 also includes Sizzle.js\n\nSizzle.js includes the following license:\n\nCopyright JS Foundation and other contributors, https://js.foundation/\n\nThis software consists of voluntary contributions made by many\nindividuals. For exact contribution history, see the revision history\navailable at https://github.com/jquery/sizzle\n\nThe following license applies to all parts of this software except as\ndocumented below:\n\n====\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n====\n\nAll files located in the node_modules and external directories are\nexternally maintained libraries used by this software which have their\nown licenses; we recommend you read them, as their terms may differ from\nthe terms above.\n```\n\n----------------------------------------\n\nTITLE: Demonstrating XSS Vulnerability via MHTML in Spark Web UI\nDESCRIPTION: This code snippet shows an example HTTP request and response that demonstrates the XSS vulnerability (CVE-2017-7678) in Spark's web UI using MHTML. The request contains a malicious payload encoded in base64, which is then reflected in the response.\nSOURCE: https://github.com/apache/spark-website/blob/asf-site/security.md#2025-04-21_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET /app/?appId=Content-Type:%20multipart/related;%20boundary=_AppScan%0d%0a--_AppScan%0d%0aContent-Location:foo%0d%0aContent-Transfer-Encoding:base64%0d%0a%0d%0aPGh0bWw%2bPHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw%2b%0d%0a HTTP/1.1\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"row-fluid\">No running application with ID Content-Type: multipart/related;\nboundary=_AppScan\n--_AppScan\nContent-Location:foo\nContent-Transfer-Encoding:base64\nPGh0bWw+PHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw+\n</div>\n```\n\nLANGUAGE: html\nCODE:\n```\n<html><script>alert(\"XSS\")</script></html>\n```"
  }
]
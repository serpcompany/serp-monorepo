[
  {
    "owner": "canner",
    "repo": "wrenai",
    "content": "TITLE: Example of OpenAI LLM Configuration with Multiple Models\nDESCRIPTION: This YAML snippet demonstrates a specific configuration for OpenAI LLM with two models (gpt-4 and gpt-4o-mini). It includes detailed parameters for gpt-4 such as temperature, token count, and response format.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntype: llm\nprovider: openai_llm\nmodels:\n  - model: gpt-4\n    kwargs:\n      temperature: 0\n      n: 1\n      max_tokens: 4096\n      response_format:\n        type: \"json_object\"\n  - model: gpt-4o-mini\n    kwargs: {}\napi_base: https://api.openai.com/v1\n```\n\n----------------------------------------\n\nTITLE: Initializing FastAPI Lifespan with Service Components\nDESCRIPTION: FastAPI lifespan implementation that handles startup and shutdown events. Initializes pipeline components, service containers, and Langfuse integration. Configures caching and retrieval parameters through environment variables.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/code_design.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # startup events\n\n    pipe_components = generate_components()\n    app.state.service_container = create_service_container(\n        pipe_components,\n        column_indexing_batch_size=(\n            int(os.getenv(\"COLUMN_INDEXING_BATCH_SIZE\"))\n            if os.getenv(\"COLUMN_INDEXING_BATCH_SIZE\")\n            else 50\n        ),\n        table_retrieval_size=(\n            int(os.getenv(\"TABLE_RETRIEVAL_SIZE\"))\n            if os.getenv(\"TABLE_RETRIEVAL_SIZE\")\n            else 10\n        ),\n        table_column_retrieval_size=(\n            int(os.getenv(\"TABLE_COLUMN_RETRIEVAL_SIZE\"))\n            if os.getenv(\"TABLE_COLUMN_RETRIEVAL_SIZE\")\n            else 1000\n        ),\n        query_cache={\n            # the maxsize is a necessary parameter to init cache, but we don't want to expose it to the user\n            # so we set it to 1_000_000, which is a large number\n            \"maxsize\": 1_000_000,\n            \"ttl\": int(os.getenv(\"QUERY_CACHE_TTL\") or 120),\n        },\n    )\n    app.state.service_metadata = create_service_metadata(pipe_components)\n    init_langfuse()\n\n    yield\n\n    # shutdown events\n    langfuse_context.flush()\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipelines in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure pipelines that specify combinations of LLM, embedder, engine, and document store components. Each pipeline is named and references previously configured components.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ntype: pipeline\npipes:\n  - name: <pipe_name>\n    llm: <provider>.<model_name>\n    embedder: <provider>.<model_name>\n    engine: <provider_name>\n    document_store: <provider_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Component in YAML\nDESCRIPTION: This YAML snippet shows how to configure the Language Learning Model (LLM) component in the config.yaml file. It allows specifying the provider, model names, and model-specific parameters through the kwargs field.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntype: llm\nprovider: <provider_name>\nmodels:\n  - model: <model_name>\n    kwargs: {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Settings in YAML\nDESCRIPTION: This YAML snippet demonstrates general service settings configuration including network settings, batch sizes, cache parameters, logging, and development mode flags.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  host: <host_address>\n  port: <port_number>\n  column_indexing_batch_size: <batch_size>\n  table_retrieval_size: <retrieval_size>\n  table_column_retrieval_size: <column_retrieval_size>\n  query_cache_maxsize: <cache_size>\n  query_cache_ttl: <cache_ttl_in_seconds>\n  langfuse_host: <langfuse_endpoint>\n  langfuse_enable: <true/false>\n  logging_level: <log_level>\n  development: <true/false>\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedder Component in YAML\nDESCRIPTION: This YAML snippet shows how to configure the embedder component which converts text to numerical vectors. It allows specifying the provider, models, embedding dimensions, API endpoint, and timeout settings.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntype: embedder\nprovider: <provider_name>\nmodels:\n  - model: <model_name>\n    dimension: <embedding_size>\napi_base: <api_endpoint>\ntimeout: <timeout_in_seconds>\n```\n\n----------------------------------------\n\nTITLE: Configuring Engine Component in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the engine component responsible for generating SQL queries. It includes the provider and endpoint configuration options.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntype: engine\nprovider: <provider_name>\nendpoint: <engine_endpoint>\n```\n\n----------------------------------------\n\nTITLE: Configuring Document Store Component in YAML\nDESCRIPTION: This YAML snippet shows the configuration for a document store component which is responsible for storing and retrieving embeddings. It requires specifying the provider name (such as Qdrant).\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntype: document_store\nprovider: <provider_name>\n```\n\n----------------------------------------\n\nTITLE: Example of SQL Generation Pipeline Configuration\nDESCRIPTION: This YAML snippet shows a specific pipeline configuration for SQL generation. It references the OpenAI LLM with gpt-4o-mini model and uses a wren_ui engine.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/docs/configuration.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ntype: pipeline\npipes:\n  - name: sql_generation\n    llm: openai_llm.gpt-4o-mini\n    engine: wren_ui\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Sources for E2E Tests in TypeScript\nDESCRIPTION: Default test configuration object containing connection details for various data sources including BigQuery, DuckDB, PostgreSQL, MySQL, SQL Server, and Trino. This configuration should be customized in e2e.config.json.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/e2e/README.md#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst defaultTestConfig = {\n  bigQuery: {\n    projectId: 'wrenai',\n    datasetId: 'wrenai.tpch_sf1',\n    // The credential file should be under \"wren-ui\" folder\n    // For example: .tmp/credential.json\n    credentialPath: 'bigquery-credential-path',\n  },\n  duckDb: {\n    sqlCsvPath: 'https://duckdb.org/data/flights.csv',\n  },\n  postgreSql: {\n    host: 'postgresql-host',\n    port: '5432',\n    username: 'postgresql-username',\n    password: 'postgresql-password',\n    database: 'postgresql-database',\n    ssl: false,\n  },\n  mysql: {\n    host: 'mysql-host',\n    port: '3306',\n    username: 'mysql-username',\n    password: 'mysql-password',\n    database: 'mysql-database',\n  },\n  sqlServer: {\n    host: 'sqlServer-host',\n    port: '1433',\n    username: 'sqlServer-username',\n    password: 'sqlServer-password',\n    database: 'sqlServer-database',\n  },\n  trino: {\n    host: 'trino-host',\n    port: '8081',\n    catalog: 'trino-catalog',\n    schema: 'trino-schema',\n    username: 'trino-username',\n    password: 'trino-password',\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Minikube Setup for Wren AI\nDESCRIPTION: Configuration script for setting up Minikube environment with required addons and helm charts including external-dns and cert-manager. Prerequisites include Minikube and Helm installation.\nSOURCE: https://github.com/canner/wrenai/blob/main/deployment/kustomizations/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nminikube start\nminikube addons enable ingress\nminikube addons enable metallb\nminikube kubectl -- get nodes\nminikube kubectl -- get pods -A\n\nminikube update-context\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\nhelm install external-dns bitnami/external-dns\nhelm install \\\n  external-dns bitnami/external-dns \\\n  --namespace external-dns \\\n  --version 7.5.2 \\\n  --create-namespace \\\n  --set installCRDs=true\nkubectl get pods -n external-dns\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --version v1.13.6 \\\n  --create-namespace \\\n  --set installCRDs=true\nkubectl get pods -n cert-manager\n```\n\n----------------------------------------\n\nTITLE: Langfuse Client Initialization Function\nDESCRIPTION: Creates and configures a Langfuse client instance using provided credentials.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef init_langfuse_client(public_key: str, secret_key: str, host: str):\n    return Langfuse(\n        public_key=public_key,\n        secret_key=secret_key,\n        host=host,\n    )\n```\n\n----------------------------------------\n\nTITLE: Trace Fetching Utility\nDESCRIPTION: Retrieves all traces from Langfuse with optional filtering by name and timestamp range.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_all_traces(client: Langfuse, name: Optional[str]=None, from_timestamp: Optional[datetime]=None, to_timestamp: Optional[datetime]=None):\n    traces = []\n    page = 1\n\n    while True:\n        data = client.fetch_traces(name=name, page=page, from_timestamp=from_timestamp, to_timestamp=to_timestamp).data\n        if len(data) == 0:\n            break\n        traces += data\n        page += 1\n\n    return traces\n```\n\n----------------------------------------\n\nTITLE: Observations Fetching Utility\nDESCRIPTION: Retrieves all observations from Langfuse with optional filtering capabilities.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_all_observations(\n    client: Langfuse, \n    name: Optional[str]=None, \n    from_timestamp: Optional[datetime]=None, \n    to_timestamp: Optional[datetime]=None\n) -> list[ObservationsView]:\n    observations = []\n    page = 1\n\n    while True:\n        data = client.fetch_observations(name=name, page=page, from_start_time=from_timestamp, to_start_time=to_timestamp).data\n        if len(data) == 0:\n            break\n        observations += data\n        page += 1\n\n    return observations\n```\n\n----------------------------------------\n\nTITLE: BigQuery Environment Variables\nDESCRIPTION: Environment variable configuration for BigQuery credentials and settings\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_4\n\nLANGUAGE: env\nCODE:\n```\nBIGQUERY_PROJECT_ID=\"your_project_id\"\nBIGQUERY_DATASET_ID=\"your_dataset_id\"\nBIGQUERY_CREDENTIALS=\"your_credentials\"\n```\n\n----------------------------------------\n\nTITLE: BigQuery Configuration in YAML\nDESCRIPTION: Configuration template for BigQuery access credentials and project settings in YAML format\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nbigquery_project_id: \"your_project_id\"\nbigquery_dataset_id: \"your_dataset_id\"\nbigquery_credentials: \"your_credentials\"\n```\n\n----------------------------------------\n\nTITLE: Building and Running E2E Tests in Bash\nDESCRIPTION: Commands for building the UI, running E2E tests in different modes including headless, headed, UI mode, debug mode, and generating test scripts using Playwright.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/e2e/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:e2e\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:e2e --headed\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:e2e --ui\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:e2e --debug\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpx playwright codegen http://localhost:3000\n```\n\n----------------------------------------\n\nTITLE: Setting SQLite Database Environment Variables\nDESCRIPTION: Configures environment variables to use SQLite as the database for Wren UI, including database type and file path.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# windows\nSET DB_TYPE=sqlite\nSET SQLITE_FILE={your_sqlite_file_path} # default is ./db.sqlite3\n\n# linux or mac\nexport DB_TYPE=sqlite\nexport SQLITE_FILE={your_sqlite_file_path}\n```\n\n----------------------------------------\n\nTITLE: Setting Postgres Database Environment Variables\nDESCRIPTION: Configures environment variables to use Postgres as the database for Wren UI, including database type and connection string.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# windows\nSET DB_TYPE=pg\nSET PG_URL=postgres://user:password@localhost:5432/dbname \n\n# linux or mac\nexport DB_TYPE=pg\nexport PG_URL=postgres://user:password@localhost:5432/dbname\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Docker-based Services\nDESCRIPTION: Configures environment variables when using Docker for other Wren AI services, including experimental engine options.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Execute this if you start wren-engine and ibis-server via docker\n# Linux or MacOS\nexport OTHER_SERVICE_USING_DOCKER=true\nexport EXPERIMENTAL_ENGINE_RUST_VERSION=false # set to true if you want to use the experimental Rust version of the Wren Engine\n# Windows\nSET OTHER_SERVICE_USING_DOCKER=true\nSET EXPERIMENTAL_ENGINE_RUST_VERSION=false # set to true if you want to use the experimental Rust version of the Wren Engine\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration Files with Just in Bash\nDESCRIPTION: Command to initialize configuration files using the Just command runner. This creates both .env.dev and config.yaml files needed for the service.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njust init\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Services for WrenAI Development\nDESCRIPTION: Docker Compose commands to start the WrenAI services in development mode, including options for background execution and stopping services.\nSOURCE: https://github.com/canner/wrenai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# current directory is WrenAI/docker\ndocker-compose -f docker-compose-dev.yaml --env-file .env.example up\n\n# you can add the -d flag to run the services in the background\ndocker-compose -f docker-compose-dev.yaml --env-file .env.example up -d\n# to stop the services, use\ndocker-compose -f docker-compose-dev.yaml --env-file .env.example down\n```\n\n----------------------------------------\n\nTITLE: Starting Required Containers with Just in Bash\nDESCRIPTION: Command to start the required Docker containers for the Wren AI Service using the Just command runner. These containers provide necessary infrastructure services.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njust up\n```\n\n----------------------------------------\n\nTITLE: Launching the AI Service with Just in Bash\nDESCRIPTION: Command to start the Wren AI Service application using the Just command runner. This launches the main service after container dependencies are running.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\njust start\n```\n\n----------------------------------------\n\nTITLE: Starting Wrenai with OpenAI using Docker Compose\nDESCRIPTION: Command to start all Wrenai services using Docker Compose with environment variables from a .env file.\nSOURCE: https://github.com/canner/wrenai/blob/main/docker/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --env-file .env up -d\n```\n\n----------------------------------------\n\nTITLE: Restarting Wrenai AI Service with Custom LLM Configuration\nDESCRIPTION: Command to restart only the Wrenai AI service after modifying the configuration file for custom LLM integration.\nSOURCE: https://github.com/canner/wrenai/blob/main/docker/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --env-file .env up -d --force-recreate wren-ai-service\n```\n\n----------------------------------------\n\nTITLE: Starting Wren UI Development Server\nDESCRIPTION: Launches the development server for the Wren UI project using Yarn or npm.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Run the development server\nyarn dev\n# or\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Starting Wren AI Services with Docker Compose\nDESCRIPTION: Launches the Wren AI services using Docker Compose, with options for background execution and stopping services.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# current directory is WrenAI/docker\ndocker-compose -f docker-compose-dev.yaml --env-file .env.example up\n\n# you can add a -d flag to run the services in the background\ndocker-compose -f docker-compose-dev.yaml --env-file .env.example up -d\n# then stop the services via\ndocker-compose -f docker-compose-dev.yaml --env-file .env.example down\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for Module Development\nDESCRIPTION: Example of modifying the Docker Compose configuration to develop specific modules alongside Wren UI, such as disabling the AI service container.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# docker/docker-compose-dev.yaml\nwren-engine:\n    image: ghcr.io/canner/wren-engine:${WREN_ENGINE_VERSION}\n    pull_policy: always\n    platform: ${PLATFORM}\n    expose:\n      - ${WREN_ENGINE_SQL_PORT}\n    ports:\n      - ${WREN_ENGINE_PORT}:${WREN_ENGINE_PORT}\n    volumes:\n      - data:/usr/src/app/etc\n    networks:\n      - wren\n    depends_on:\n      - bootstrap\n    ...\n# comment out the ai-service service\nwren-ai-service:\n    image: ghcr.io/canner/wren-ai-service:${WREN_AI_SERVICE_VERSION}\n    pull_policy: always\n    platform: ${PLATFORM}\n    ports:\n      - ${AI_SERVICE_FORWARD_PORT}:${WREN_AI_SERVICE_PORT}\n    environment:\n      WREN_UI_ENDPOINT: http://docker.for.mac.localhost:${WREN_UI_PORT}\n      # sometimes the console won't show print messages,\n      # using PYTHONUNBUFFERED: 1 can fix this\n      PYTHONUNBUFFERED: 1\n      CONFIG_PATH: /app/data/config.yaml\n    env_file:\n      - ${PROJECT_DIR}/.env\n    volumes:\n      - ${PROJECT_DIR}/config.yaml:/app/data/config.yaml\n    networks:\n      - wren\n    depends_on:\n      - qdrant\n\nibis-server:\n    image: ghcr.io/canner/wren-engine-ibis:${IBIS_SERVER_VERSION}\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry in Bash\nDESCRIPTION: Command to install all project dependencies defined in the Poetry configuration file. This sets up all required packages for the Wren AI Service.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Deploying Wren AI to Kubernetes\nDESCRIPTION: Main deployment script for Wren AI including repository cloning, kustomization inflation, namespace creation, and secret configuration. Requires kubectl, kustomize, and necessary dependencies to be installed.\nSOURCE: https://github.com/canner/wrenai/blob/main/deployment/kustomizations/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Clone the repository with the kustomization\ngit clone https://github.com/Canner/WrenAI.git\ncd WrenAI\n\n# Inflate the manifest with kustomization\nkubectl kustomize deployment/kustomizations --enable-helm > deployment/kustomizations/wrenai.kustomized.yaml\n\n# Create namespace\nkubectl create namespace wren\n\n# !!!!!!!!!!!!!\n# MODIFY secret-wren_example.yaml manifest file FIRST\n# OPENAI_API_KEY is REQUIRED: without a valid key the wren-ai-service-deployment pod will not start\n# You must update PG_URL, otherwise wren-ui will not work\n#vi deployment/kustomizations/examples/secret-wren_example.yaml\nkubectl apply -f deployment/kustomizations/examples/secret-wren_example.yaml -n wren\n\n# Deploy the app:\nkubectl apply -f deployment/kustomizations/wrenai.kustomized.yaml\n\nkubectl get pods -n wren\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dry Run Failed Cases in Python\nDESCRIPTION: This function fetches dry run failed cases from observations. It processes the data to extract invalid generation results and their corresponding trace IDs.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_dry_run_failed_cases(client, from_timestamp = None, to_timestamp = None):\n    results = []\n    for data in get_all_observations(client, name=\"post_process\", from_timestamp=from_timestamp, to_timestamp=to_timestamp):\n        if invalid_generation_results := data.output.get('invalid_generation_results', []):\n            for result in invalid_generation_results:\n                result['trace_id'] = data.trace_id\n            results += invalid_generation_results\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Running Database Migrations for Wren UI\nDESCRIPTION: Executes database migrations to set up the required schema for the Wren UI project.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn migrate\n# or\nnpm run migrate\n```\n\n----------------------------------------\n\nTITLE: Filtering Traces by Conditions in Python\nDESCRIPTION: This snippet sets up conditions to filter traces for the 'Ask Question' feature and separates the results into error and non-error traces.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconditions = {\n    \"metadata\": {\n    },\n    \"name\": \"Ask Question\",\n    # \"release\": RELEASE,\n}\nerror_results, no_error_results = get_traces_by_conditions(traces, conditions)\n```\n\n----------------------------------------\n\nTITLE: Grouping and Counting Ask Question Traces by MDL Hash in Python\nDESCRIPTION: This code groups 'Ask Question' traces by MDL hash, sorts them by count, and prints the results. It provides insights into the distribution of traces across different MDL hashes.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n_traces_by_mdl_hash = get_traces_group_by_value(error_results, \"Ask Question\", \"mdl_hash\")\nsorted_traces_by_mdl_hash = sorted(_traces_by_mdl_hash.items(), key=lambda x: len(x[1]), reverse=True)\n\nprint(f'number of mdl_hash: {len(sorted_traces_by_mdl_hash)}')\nfor mdl_hash, traces in sorted_traces_by_mdl_hash:\n    print(f'mdl_hash: {mdl_hash}')\n    print(f'size of traces: {len(traces)}')\n```\n\n----------------------------------------\n\nTITLE: Fetching Recent Dry Run Failed Cases in Python\nDESCRIPTION: This snippet retrieves dry run failed cases from the past 14 days using the previously defined function and prints the count of failed cases.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndry_run_failed_cases = get_dry_run_failed_cases(client, from_timestamp=datetime.now() - timedelta(days=14))\nlen(dry_run_failed_cases)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dry Run Failed Cases in a Pandas DataFrame\nDESCRIPTION: This code creates a Pandas DataFrame from the dry run failed cases and displays it with search and export functionalities using the 'show' function.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nshow(\n    pd.DataFrame(dry_run_failed_cases),\n    buttons=[\"csvHtml5\"],\n    layout={\"top1\": \"searchPanes\"},\n    searchPanes={\"layout\": \"columns-3\", \"cascadePanes\": True}\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Required Python Libraries\nDESCRIPTION: Imports necessary Python packages for data analysis, including Langfuse client, pandas, plotly, and other utilities.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom dotenv import load_dotenv\nfrom itables import init_notebook_mode, show\nfrom langfuse import Langfuse\nfrom langfuse.api.resources.commons.types.observations_view import ObservationsView\nfrom pydantic import BaseModel\nimport pandas as pd\nimport plotly.express as px\n```\n\n----------------------------------------\n\nTITLE: Counting Error Results by Type in Python\nDESCRIPTION: This snippet processes error results and prints the count of errors for each error type. It provides an overview of the distribution of different error types.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nerror_results_details = get_error_result_details(error_results)\nfor key, value in error_results_details.items():\n    print(key)\n    print(len(value))\n```\n\n----------------------------------------\n\nTITLE: Printing Detailed Error Results by Type in Python\nDESCRIPTION: This code iterates through error results grouped by error type and prints the detailed JSON representation of each error result.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor key in error_results_details.keys():\n    print(f'Error Type: {key}')\n    for error_result in error_results_details[key]:\n        pprint_json(error_result.json())\n```\n\n----------------------------------------\n\nTITLE: Printing JSON Representation of Filtered Error Results in Python\nDESCRIPTION: This code iterates through the filtered error results and prints the JSON representation of each error result, providing detailed information about specific errors.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor _error_result in _error_results:\n    pprint_json(_error_result.json())\n```\n\n----------------------------------------\n\nTITLE: Command for Dataset Preparation\nDESCRIPTION: CLI command to prepare evaluation datasets, supporting Spider and Bird datasets\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_0\n\nLANGUAGE: cli\nCODE:\n```\njust prep <dataset-name>\n```\n\n----------------------------------------\n\nTITLE: Preparing Environment File for Docker Compose\nDESCRIPTION: Creates a local environment file for Docker Compose by copying the example file in the WrenAI/docker folder.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# assume current directory is wren-ui\ncd ../docker\ncp .env.example .env.local\n```\n\n----------------------------------------\n\nTITLE: Prediction Command\nDESCRIPTION: CLI commands for running predictions on evaluation datasets with optional pipeline specification\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_5\n\nLANGUAGE: cli\nCODE:\n```\njust predict <evaluation-dataset>\njust predict <evaluation-dataset> <pipeline-name>\n```\n\n----------------------------------------\n\nTITLE: Evaluation Command\nDESCRIPTION: CLI commands for evaluating prediction results with optional semantic comparison\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_6\n\nLANGUAGE: cli\nCODE:\n```\njust eval <prediction-result>\njust eval <prediction-result> --semantics\n```\n\n----------------------------------------\n\nTITLE: Building Wrenai Application for Different Platforms using Go\nDESCRIPTION: Commands for building the Wrenai application on macOS and Windows platforms. Shows how to compile the main.go file for the target operating system.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-launcher/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# mac\ngo build main.go\n# windows\nenv GOOS=windows GOARCH=amd64 go build main.go\n```\n\n----------------------------------------\n\nTITLE: Managing Go Dependencies for Wrenai Project\nDESCRIPTION: Commands for updating, maintaining, and verifying dependencies in the Wrenai Go project. Includes instructions for updating single or all dependencies, cleaning up module files, and verifying updates with tests.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-launcher/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Update a single dependency\ngo get example.com/some/package@latest\n\n# Update all dependencies\ngo get -u ./...\n\n# Clean up and ensure the module files are correct\ngo mod tidy\n\n# Verify the updates\ngo test ./...\n```\n\n----------------------------------------\n\nTITLE: Stopping the Service with Just in Bash\nDESCRIPTION: Command to stop the running Docker containers using the Just command runner. This cleanly shuts down all service components when development is complete.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njust down\n```\n\n----------------------------------------\n\nTITLE: Checking Node.js Version for Wren UI\nDESCRIPTION: Verifies that the installed Node.js version is 18, which is required for the Wren UI project.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode -v\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Wren UI\nDESCRIPTION: Installs the necessary dependencies for the Wren UI project using Yarn package manager.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn\n```\n\n----------------------------------------\n\nTITLE: Switching Between Wren AI Projects\nDESCRIPTION: Demonstrates how to switch between different Wren AI projects by changing the database configuration and restarting the server.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ui/README.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# start your first project using default database(sqlite by defulat)\nyarn migrate\nyarn dev\n\n# ... after onboarding and lots of hard work, you want to switch to another project \n# stop the server\n\n# set another sqlite file\nexport SQLITE_FILE=./new_project.sqlite\nyarn migrate\nyarn dev\n\n# In the Browser, ... after another onboarding process and hard work\n# you can switch back to the first project by setting the first sqlite file\nexport SQLITE_FILE=./first_project.sqlite\n\nyarn dev  # no need to do migration again\n\n# in the modeling page, click the deploy button to deploy the project to the wren-ai-service.\n# your Wren AI is ready to answer your question.\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry for Wren AI Service in Bash\nDESCRIPTION: Command to install Poetry 1.8.3 for Python package management using curl. Poetry is used for dependency management in the Wren AI Service project.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://install.python-poetry.org | python3 - --version 1.8.3\n```\n\n----------------------------------------\n\nTITLE: Stopping Wrenai Services using Docker Compose\nDESCRIPTION: Command to stop all Wrenai services that were started with Docker Compose.\nSOURCE: https://github.com/canner/wrenai/blob/main/docker/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --env-file .env down\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks in Bash\nDESCRIPTION: Command to set up pre-commit hooks using Poetry. These hooks help maintain code quality by automatically checking code before commits.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pre-commit install\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks in Bash\nDESCRIPTION: Command to manually run all pre-commit checks across all files in the project. This verifies code quality and formatting standards.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run pre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Just in Bash\nDESCRIPTION: Command to execute the test suite using the Just command runner. This verifies that the Wren AI Service is functioning correctly.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\njust test\n```\n\n----------------------------------------\n\nTITLE: Running Load Tests with Just in Bash\nDESCRIPTION: Command to execute load testing on the Wren AI Service using the Just command runner. This helps evaluate performance under various usage scenarios.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/README.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\njust load-test\n```\n\n----------------------------------------\n\nTITLE: Querying Error Results for Specific Question in Python\nDESCRIPTION: This snippet filters error results for a specific question about transportation routes in Taichung Park and prints the count of matching error results.\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/llm_trace_analysis/notebook.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n_error_results = get_error_results_by_query(error_results_details, '我在台中公園，有哪些路線我可以搭乘？')\nlen(_error_results)\n```\n\n----------------------------------------\n\nTITLE: Copying Environment File for Docker Setup\nDESCRIPTION: Commands to copy the example environment file to create a local configuration for Docker setup in the WrenAI project.\nSOURCE: https://github.com/canner/wrenai/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# assuming the current directory is wren-ui\ncd ../docker\ncp .env.example .env.local\n```\n\n----------------------------------------\n\nTITLE: Instructions for Applying Kustomization Patches to WrenAI Deployments\nDESCRIPTION: This markdown snippet describes how to use patches with Kustomization to modify the official unmodified WrenAI deployment. It recommends patching ConfigMap and Services as needed, and removing Certificate and Ingress if they're not required.\nSOURCE: https://github.com/canner/wrenai/blob/main/deployment/kustomizations/patches/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Example of usefull Patches for Kustomization\n\nPatches from this folder allows to utilize the official unmodified deployment/kustomization dirrectly from the repo as a base layer for your kustomization. And then add patches to update some values. This is usefull for your GitOps and can be combined with tools such as ArgoCD and FluxCD.\n\nPatch ConfigMap, and Service if needed.\nRemove Certificate and Ingress if not needed.\n```\n\n----------------------------------------\n\nTITLE: Listing Deployment Strategies in Markdown\nDESCRIPTION: This markdown snippet outlines the available deployment strategies for the Wrenai application. It includes links to Docker and Kubernetes Kustomizations deployment methods.\nSOURCE: https://github.com/canner/wrenai/blob/main/deployment/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Various deployemnt starategies of the app\n\n- [x] [Docker](../docker/)\n- [x] [Kubernetes: Kustomizations](./kustomizations/)\n```\n\n----------------------------------------\n\nTITLE: Markdown Contributors Link and Image\nDESCRIPTION: HTML elements embedded in markdown to display contributor avatars and provide navigation links\nSOURCE: https://github.com/canner/wrenai/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<a href=\"https://github.com/canner/wrenAI/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Canner/WrenAI\" />\n</a>\n\n<p align=\"right\">\n  <a href=\"#top\">⬆️ Back to Top</a>\n</p>\n```\n\n----------------------------------------\n\nTITLE: DuckDB Database Path Configuration\nDESCRIPTION: YAML configuration example for specifying the database path for DuckDB in Spider/Bird datasets\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndb_path_for_duckdb: \"etc/bird/minidev/MINIDEV/dev_databases\"\n```\n\n----------------------------------------\n\nTITLE: Base64 Credential Encoding Command\nDESCRIPTION: CLI command to encode BigQuery credentials to base64 format\nSOURCE: https://github.com/canner/wrenai/blob/main/wren-ai-service/eval/README.md#2025-04-23_snippet_2\n\nLANGUAGE: cli\nCODE:\n```\ncat <path/to/credentials.json> | base64\n```"
  }
]
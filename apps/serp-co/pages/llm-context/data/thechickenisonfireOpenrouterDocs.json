[
  {
    "owner": "thechickenisonfire",
    "repo": "openrouter-docs",
    "content": "TITLE: OpenAI SDK Integration with OpenRouter in TypeScript\nDESCRIPTION: This example shows how to use the OpenAI SDK with OpenRouter in TypeScript. It demonstrates setting the base URL, API key, and headers to interface with OpenRouter's API for chat completions.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_100\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from \"openai\"\n\nconst openai = new OpenAI({\n  baseURL: \"https://openrouter.ai/api/v1\",\n  apiKey: \"${API_KEY_REF}\",\n  defaultHeaders: {\n    ${getHeaderLines().join('\\n        ')}\n  },\n})\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: \"${Model.GPT_4_Omni}\",\n    messages: [\n      { role: \"user\", content: \"Say this is a test\" }\n    ],\n  })\n\n  console.log(completion.choices[0].message)\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: OpenAI SDK Integration with OpenRouter in Python\nDESCRIPTION: This example shows how to use the OpenAI Python SDK with OpenRouter. It demonstrates configuring the client with the OpenRouter base URL and API key, and making a basic chat completion request with optional headers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_101\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom os import getenv\n\n# gets API Key from environment variable OPENAI_API_KEY\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=getenv(\"OPENROUTER_API_KEY\"),\n)\n\ncompletion = client.chat.completions.create(\n  model=\"${Model.GPT_4_Omni}\",\n  extra_headers={\n    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  # pass extra_body to access OpenRouter-only arguments.\n  # extra_body={\n    # \"models\": [\n    #   \"${Model.GPT_4_Omni}\",\n    #   \"${Model.Mixtral_8x_22B_Instruct}\"\n    # ]\n  # },\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Say this is a test\",\n    },\n  ],\n)\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Direct API Call to OpenRouter in Python\nDESCRIPTION: This snippet demonstrates how to make a direct API call to OpenRouter using Python's requests library. It shows how to set headers, construct the request body, and send a POST request to the chat completions endpoint.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nresponse = requests.post(\n  url=\"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": \"Bearer <OPENROUTER_API_KEY>\",\n    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  data=json.dumps({\n    \"model\": \"openai/gpt-4o\", # Optional\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the meaning of life?\"\n      }\n    ]\n  })\n)\n```\n\n----------------------------------------\n\nTITLE: Direct API Call to OpenRouter in TypeScript\nDESCRIPTION: This snippet shows how to make a direct API call to OpenRouter using TypeScript's fetch API. It demonstrates setting headers, constructing the request body, and sending a POST request to the chat completions endpoint.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nfetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer <OPENROUTER_API_KEY>',\n    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.\n    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: 'openai/gpt-4o',\n    messages: [\n      {\n        role: 'user',\n        content: 'What is the meaning of life?',\n      },\n    ],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing LangChain with OpenRouter in Python\nDESCRIPTION: Demonstrates how to use LangChain with OpenRouter in Python. The code sets up a prompt template, initializes the ChatOpenAI model with OpenRouter configuration, and runs a chain to answer a question about the NFL Super Bowl.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_103\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom os import getenv\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ntemplate = \"\"\"Question: {question}\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nllm = ChatOpenAI(\n  openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n  openai_api_base=getenv(\"OPENROUTER_BASE_URL\"),\n  model_name=\"<model_name>\",\n  model_kwargs={\n    \"headers\": {\n      \"HTTP-Referer\": getenv(\"YOUR_SITE_URL\"),\n      \"X-Title\": getenv(\"YOUR_SITE_NAME\"),\n    }\n  },\n)\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n\nprint(llm_chain.run(question))\n```\n\n----------------------------------------\n\nTITLE: Making API Requests with OpenRouter in Python\nDESCRIPTION: This Python snippet demonstrates how to use the OpenAI library to make API requests to OpenRouter. It includes setting the API base URL and key, and creating a chat completion.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nopenai.api_base = \"https://openrouter.ai/api/v1\"\nopenai.api_key = \"<OPENROUTER_API_KEY>\"\n\nresponse = openai.ChatCompletion.create(\n  model=\"openai/gpt-4o\",\n  messages=[...],\n  headers={\n    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n  },\n)\n\nreply = response.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: OpenRouter API Request Schema Definition\nDESCRIPTION: TypeScript type definitions for the OpenRouter API request schema, including all supported parameters and options for chat completions endpoint.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_42\n\nLANGUAGE: typescript\nCODE:\n```\ntype Request = {\n  messages?: Message[];\n  prompt?: string;\n  model?: string;\n  response_format?: { type: 'json_object' };\n  stop?: string | string[];\n  stream?: boolean;\n  max_tokens?: number;\n  temperature?: number;\n  tools?: Tool[];\n  tool_choice?: ToolChoice;\n  seed?: number;\n  top_p?: number;\n  top_k?: number;\n  frequency_penalty?: number;\n  presence_penalty?: number;\n  repetition_penalty?: number;\n  logit_bias?: { [key: number]: number };\n  top_logprobs: number;\n  min_p?: number;\n  top_a?: number;\n  prediction?: { type: 'content'; content: string };\n  transforms?: string[];\n  models?: string[];\n  route?: 'fallback';\n  provider?: ProviderPreferences;\n};\n\ntype TextContent = {\n  type: 'text';\n  text: string;\n};\n\ntype ImageContentPart = {\n  type: 'image_url';\n  image_url: {\n    url: string;\n    detail?: string;\n  };\n};\n\ntype ContentPart = TextContent | ImageContentPart;\n\ntype Message =\n  | {\n      role: 'user' | 'assistant' | 'system';\n      content: string | ContentPart[];\n      name?: string;\n    }\n  | {\n      role: 'tool';\n      content: string;\n      tool_call_id: string;\n      name?: string;\n    };\n\ntype FunctionDescription = {\n  description?: string;\n  name: string;\n  parameters: object;\n};\n\ntype Tool = {\n  type: 'function';\n  function: FunctionDescription;\n};\n\ntype ToolChoice =\n  | 'none'\n  | 'auto'\n  | {\n      type: 'function';\n      function: {\n        name: string;\n      };\n    };\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI SDK with OpenRouter in TypeScript\nDESCRIPTION: This code example shows how to use the OpenAI TypeScript SDK with OpenRouter. It includes setting up the SDK with the correct base URL and API key, and making a chat completion request.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_56\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey: '<OPENROUTER_API_KEY>',\n  defaultHeaders: {\n    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.\n    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: 'openai/gpt-4o',\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n  });\n\n  console.log(completion.choices[0].message);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenRouter API with OpenAI SDK in TypeScript\nDESCRIPTION: This snippet shows how to use the OpenAI SDK in TypeScript to interact with OpenRouter's API. It demonstrates client setup, making a chat completion request, and handling the response.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey: '<OPENROUTER_API_KEY>',\n  defaultHeaders: {\n    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.\n    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.\n  },\n});\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: 'openai/gpt-4o',\n    messages: [\n      {\n        role: 'user',\n        content: 'What is the meaning of life?',\n      },\n    ],\n  });\n\n  console.log(completion.choices[0].message);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain-of-Thought Reasoning Between Models in Python\nDESCRIPTION: This code demonstrates using one model (deepseek-r1) to generate reasoning which is then injected into another model (gpt-4o-mini) to improve response quality. It shows how to make API requests to OpenRouter, extract reasoning tokens, and use them to enhance responses.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_96\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nquestion = \"Which is bigger: 9.11 or 9.9?\"\n\nurl = \"https://openrouter.ai/api/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer {{API_KEY_REF}}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef do_req(model, content, reasoning_config=None):\n    payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": content}\n        ],\n        \"stop\": \"</think>\"\n    }\n\n    return requests.post(url, headers=headers, data=json.dumps(payload))\n\n# Get reasoning from a capable model\ncontent = f\"{question} Please think this through, but don't output an answer\"\nreasoning_response = do_req(\"deepseek/deepseek-r1\", content)\nreasoning = reasoning_response.json()['choices'][0]['message']['reasoning']\n\n# Let's test! Here's the naive response:\nsimple_response = do_req(\"openai/gpt-4o-mini\", question)\nprint(simple_response.json()['choices'][0]['message']['content'])\n\n# Here's the response with the reasoning token injected:\ncontent = f\"{question}. Here is some context to help you: {reasoning}\"\nsmart_response = do_req(\"openai/gpt-4o-mini\", content)\nprint(smart_response.json()['choices'][0]['message']['content'])\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenRouter API with OpenAI SDK in Python\nDESCRIPTION: This snippet demonstrates how to use the OpenAI SDK in Python to interact with OpenRouter's API. It shows how to set up the client, make a chat completion request, and handle the response.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=\"<OPENROUTER_API_KEY>\",\n)\n\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  model=\"openai/gpt-4o\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the meaning of life?\"\n    }\n  ]\n)\n\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Streaming Mode with Reasoning Tokens for Anthropic Models in Python\nDESCRIPTION: This example shows how to use the OpenAI SDK to stream responses from Anthropic's Claude 3.7 Sonnet model with reasoning tokens. It demonstrates setting a reasoning token budget and handling both reasoning and content output in the streamed response.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_98\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=\"{{API_KEY_REF}}\",\n)\n\ndef chat_completion_with_reasoning(messages):\n    response = client.chat.completions.create(\n        model=\"{{MODEL}}\",\n        messages=messages,\n        max_tokens=10000,\n        reasoning={\n            \"max_tokens\": 8000  # Directly specify reasoning token budget\n        },\n        stream=True\n    )\n    return response\n\nfor chunk in chat_completion_with_reasoning([\n    {\"role\": \"user\", \"content\": \"What's bigger, 9.9 or 9.11?\"}\n]):\n    if hasattr(chunk.choices[0].delta, 'reasoning') and chunk.choices[0].delta.reasoning:\n        print(f\"REASONING: {chunk.choices[0].delta.reasoning}\")\n    elif chunk.choices[0].delta.content:\n        print(f\"CONTENT: {chunk.choices[0].delta.content}\")\n```\n\n----------------------------------------\n\nTITLE: Requiring Parameter Support in OpenRouter API\nDESCRIPTION: Example showing how to ensure requests only go to providers that support all specified parameters, specifically JSON formatting in this case.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"require_parameters\": true\n  },\n  \"response_format\": { \"type\": \"json_object\" }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenRouter with Vercel AI SDK for text streaming\nDESCRIPTION: Demonstrates how to use OpenRouter with Vercel AI SDK to stream text responses. The example includes two functions: one for generating a recipe and another for retrieving weather information using a tool function that simulates API calls.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_107\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createOpenRouter } from '@openrouter/ai-sdk-provider';\nimport { streamText } from 'ai';\nimport { z } from 'zod';\n\nexport const getLasagnaRecipe = async (modelName: string) => {\n  const openrouter = createOpenRouter({\n    apiKey: '${API_KEY_REF}',\n  });\n\n  const result = await streamText({\n    model: openrouter(modelName),\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n  return result.toAIStreamResponse();\n};\n\nexport const getWeather = async (modelName: string) => {\n  const openrouter = createOpenRouter({\n    apiKey: '${API_KEY_REF}',\n  });\n\n  const result = await streamText({\n    model: openrouter(modelName),\n    prompt: 'What is the weather in San Francisco, CA in Fahrenheit?',\n    tools: {\n      getCurrentWeather: {\n        description: 'Get the current weather in a given location',\n        parameters: z.object({\n          location: z\n            .string()\n            .describe('The city and state, e.g. San Francisco, CA'),\n          unit: z.enum(['celsius', 'fahrenheit']).optional(),\n        }),\n        execute: async ({ location, unit = 'celsius' }) => {\n          // Mock response for the weather\n          const weatherData = {\n            'Boston, MA': {\n              celsius: '15°C',\n              fahrenheit: '59°F',\n            },\n            'San Francisco, CA': {\n              celsius: '18°C',\n              fahrenheit: '64°F',\n            },\n          };\n\n          const weather = weatherData[location];\n          if (!weather) {\n            return `Weather data for ${location} is not available.`;\n          }\n\n          return `The current weather in ${location} is ${weather[unit]}.`;\n        },\n      },\n    },\n  });\n  return result.toAIStreamResponse();\n};\n```\n\n----------------------------------------\n\nTITLE: Using Auto Router with OpenRouter API\nDESCRIPTION: Example of using OpenRouter's Auto Router feature with the 'openrouter/auto' model ID. This allows OpenRouter to choose between selected high-quality models based on your prompt, powered by NotDiamond.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"openrouter/auto\",\n  ... // Other params\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming API Responses with Python\nDESCRIPTION: This code demonstrates how to make a streaming request to the OpenRouter API using Python, process the Server-Sent Events (SSE) stream, and handle the chunked response. It includes JSON parsing for each data chunk and printing the content as it arrives.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nquestion = \"How would you build the tallest building ever?\"\n\nurl = \"https://openrouter.ai/api/v1/chat/completions\"\nheaders = {\n  \"Authorization\": f\"Bearer {{API_KEY_REF}}\",\n  \"Content-Type\": \"application/json\"\n}\n\npayload = {\n  \"model\": \"{{MODEL}}\",\n  \"messages\": [{\"role\": \"user\", \"content\": question}],\n  \"stream\": True\n}\n\nbuffer = \"\"\nwith requests.post(url, headers=headers, json=payload, stream=True) as r:\n  for chunk in r.iter_content(chunk_size=1024, decode_unicode=True):\n    buffer += chunk\n    while True:\n      try:\n        # Find the next complete SSE line\n        line_end = buffer.find('\\n')\n        if line_end == -1:\n          break\n\n        line = buffer[:line_end].strip()\n        buffer = buffer[line_end + 1:]\n\n        if line.startswith('data: '):\n          data = line[6:]\n          if data == '[DONE]':\n            break\n\n          try:\n            data_obj = json.loads(data)\n            content = data_obj[\"choices\"][0][\"delta\"].get(\"content\")\n            if content:\n              print(content, end=\"\", flush=True)\n          except json.JSONDecodeError:\n            pass\n      except Exception:\n        break\n```\n\n----------------------------------------\n\nTITLE: Configuring ChatOpenAI with OpenRouter in TypeScript\nDESCRIPTION: Sets up a ChatOpenAI instance using LangChain.js to connect to OpenRouter API. Configuration includes model name, temperature, streaming capability, and required headers for OpenRouter.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_102\n\nLANGUAGE: typescript\nCODE:\n```\nconst chat = new ChatOpenAI(\n  {\n    modelName: '<model_name>',\n    temperature: 0.8,\n    streaming: true,\n    openAIApiKey: '${API_KEY_REF}',\n  },\n  {\n    basePath: 'https://openrouter.ai/api/v1',\n    baseOptions: {\n      headers: {\n        'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.\n        'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.\n      },\n    },\n  },\n);\n```\n\n----------------------------------------\n\nTITLE: Managing API Keys with Python\nDESCRIPTION: Demonstrates key management operations including listing, creating, retrieving, updating, and deleting API keys using Python requests library. Requires a Provisioning API key for authentication.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nPROVISIONING_API_KEY = \"your-provisioning-key\"\nBASE_URL = \"https://openrouter.ai/api/v1/keys\"\n\n# List the most recent 100 API keys\nresponse = requests.get(\n    BASE_URL,\n    headers={\n        \"Authorization\": f\"Bearer {PROVISIONING_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n)\n\n# You can paginate using the offset parameter\nresponse = requests.get(\n    f\"{BASE_URL}?offset=100\",\n    headers={\n        \"Authorization\": f\"Bearer {PROVISIONING_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n)\n\n# Create a new API key\nresponse = requests.post(\n    f\"{BASE_URL}/\",\n    headers={\n        \"Authorization\": f\"Bearer {PROVISIONING_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"name\": \"Customer Instance Key\",\n        \"label\": \"customer-123\",\n        \"limit\": 1000  # Optional credit limit\n    }\n)\n\n# Get a specific key\nkey_hash = \"<YOUR_KEY_HASH>\"\nresponse = requests.get(\n    f\"{BASE_URL}/{key_hash}\",\n    headers={\n        \"Authorization\": f\"Bearer {PROVISIONING_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n)\n\n# Update a key\nresponse = requests.patch(\n    f\"{BASE_URL}/{key_hash}\",\n    headers={\n        \"Authorization\": f\"Bearer {PROVISIONING_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"name\": \"Updated Key Name\",\n        \"disabled\": True  # Disable the key\n    }\n)\n\n# Delete a key\nresponse = requests.delete(\n    f\"{BASE_URL}/{key_hash}\",\n    headers={\n        \"Authorization\": f\"Bearer {PROVISIONING_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming API Responses with TypeScript\nDESCRIPTION: This TypeScript example shows how to make a streaming request to the OpenRouter API, read the response using a ReadableStream, and process the Server-Sent Events. It includes handling of chunked data, buffer management, and parsing the JSON data from each chunk.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_51\n\nLANGUAGE: typescript\nCODE:\n```\nconst question = 'How would you build the tallest building ever?';\nconst response = await fetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: `Bearer ${API_KEY_REF}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: '{{MODEL}}',\n    messages: [{ role: 'user', content: question }],\n    stream: true,\n  }),\n});\n\nconst reader = response.body?.getReader();\nif (!reader) {\n  throw new Error('Response body is not readable');\n}\n\nconst decoder = new TextDecoder();\nlet buffer = '';\n\ntry {\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    // Append new chunk to buffer\n    buffer += decoder.decode(value, { stream: true });\n\n    // Process complete lines from buffer\n    while (true) {\n      const lineEnd = buffer.indexOf('\\n');\n      if (lineEnd === -1) break;\n\n      const line = buffer.slice(0, lineEnd).trim();\n      buffer = buffer.slice(lineEnd + 1);\n\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n        if (data === '[DONE]') break;\n\n        try {\n          const parsed = JSON.parse(data);\n          const content = parsed.choices[0].delta.content;\n          if (content) {\n            console.log(content);\n          }\n        } catch (e) {\n          // Ignore invalid JSON\n        }\n      }\n    }\n  }\n} finally {\n  reader.cancel();\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling Example\nDESCRIPTION: JavaScript example showing how to handle and log API errors.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_62\n\nLANGUAGE: typescript\nCODE:\n```\nconst request = await fetch('https://openrouter.ai/...');\nconsole.log(request.status);\nconst response = await request.json();\nconsole.error(response.error?.status);\nconsole.error(response.error?.message);\n```\n\n----------------------------------------\n\nTITLE: Using Fallback Models with OpenAI Python SDK\nDESCRIPTION: Implementation of the 'models' parameter fallback feature using the OpenAI Python SDK. Uses the extra_body parameter to specify fallback models from different providers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nopenai_client = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key={{API_KEY_REF}},\n)\n\ncompletion = openai_client.chat.completions.create(\n    model=\"openai/gpt-4o\",\n    extra_body={\n        \"models\": [\"anthropic/claude-3.5-sonnet\", \"gryphe/mythomax-l2-13b\"],\n    },\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the meaning of life?\"\n        }\n    ]\n)\n\nprint(completion.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Making Final OpenRouter API Call in Python\nDESCRIPTION: Demonstrates the second API call to the OpenRouter API, which includes the results of the tool execution. This call aims to get the final response from the language model.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nrequest_2 = {\n  \"model\": MODEL,\n  \"messages\": messages,\n  \"tools\": tools\n}\n\nresponse_2 = openai_client.chat.completions.create(**request_2)\n\nprint(response_2.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agentic Loop in Python and TypeScript\nDESCRIPTION: Implementation of a simple agentic loop for handling AI model interactions and tool calls. Includes functions for making LLM calls and processing tool responses with error handling and recursive execution.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef call_llm(msgs):\n    resp = openai_client.chat.completions.create(\n        model={{MODEL}},\n        tools=tools,\n        messages=msgs\n    )\n    msgs.append(resp.choices[0].message.dict())\n    return resp\n\ndef get_tool_response(response):\n    tool_call = response.choices[0].message.tool_calls[0]\n    tool_name = tool_call.function.name\n    tool_args = json.loads(tool_call.function.arguments)\n\n    # Look up the correct tool locally, and call it with the provided arguments\n    # Other tools can be added without changing the agentic loop\n    tool_result = TOOL_MAPPING[tool_name](**tool_args)\n\n    return {\n        \"role\": \"tool\",\n        \"tool_call_id\": tool_call.id,\n        \"name\": tool_name,\n        \"content\": tool_result,\n    }\n\nwhile True:\n    resp = call_llm(_messages)\n\n    if resp.choices[0].message.tool_calls is not None:\n        messages.append(get_tool_response(resp))\n    else:\n        break\n\nprint(messages[-1]['content'])\n```\n\nLANGUAGE: typescript\nCODE:\n```\nasync function callLLM(messages: Message[]): Promise<Message> {\n  const response = await fetch(\n    'https://openrouter.ai/api/v1/chat/completions',\n    {\n      method: 'POST',\n      headers: {\n        Authorization: `Bearer {{API_KEY_REF}}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        model: '{{MODEL}}',\n        tools,\n        messages,\n      }),\n    },\n  );\n\n  const data = await response.json();\n  messages.push(data.choices[0].message);\n  return data;\n}\n\nasync function getToolResponse(response: Message): Promise<Message> {\n  const toolCall = response.toolCalls[0];\n  const toolName = toolCall.function.name;\n  const toolArgs = JSON.parse(toolCall.function.arguments);\n\n  // Look up the correct tool locally, and call it with the provided arguments\n  // Other tools can be added without changing the agentic loop\n  const toolResult = await TOOL_MAPPING[toolName](toolArgs);\n\n  return {\n    role: 'tool',\n    toolCallId: toolCall.id,\n    name: toolName,\n    content: toolResult,\n  };\n}\n\nwhile (true) {\n  const response = await callLLM(messages);\n\n  if (response.toolCalls) {\n    messages.push(await getToolResponse(response));\n  } else {\n    break;\n  }\n}\n\nconsole.log(messages[messages.length - 1].content);\n```\n\n----------------------------------------\n\nTITLE: Making Final OpenRouter API Call in TypeScript\nDESCRIPTION: Shows how to make the second API call to the OpenRouter API using fetch in TypeScript. This call includes the results of the tool execution and aims to get the final response from the language model.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_35\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: `Bearer {{API_KEY_REF}}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: '{{MODEL}}',\n    messages,\n    tools,\n  }),\n});\n\nconst data = await response.json();\nconsole.log(data.choices[0].message.content);\n```\n\n----------------------------------------\n\nTITLE: Processing Tool Calls in Python\nDESCRIPTION: Handles the response from the language model, processes any tool calls, and executes the requested function. The results are then appended to the messages array for context in subsequent API calls.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# Append the response to the messages array so the LLM has the full context\n# It's easy to forget this step!\nmessages.append(response_1)\n\n# Now we process the requested tool calls, and use our book lookup tool\nfor tool_call in response_1.tool_calls:\n    '''\n    In this case we only provided one tool, so we know what function to call.\n    When providing multiple tools, you can inspect `tool_call.function.name`\n    to figure out what function you need to call locally.\n    '''\n    tool_name = tool_call.function.name\n    tool_args = json.loads(tool_call.function.arguments)\n    tool_response = TOOL_MAPPING[tool_name](**tool_args)\n    messages.append({\n      \"role\": \"tool\",\n      \"tool_call_id\": tool_call.id,\n      \"name\": tool_name,\n      \"content\": json.dumps(tool_response),\n    })\n```\n\n----------------------------------------\n\nTITLE: Direct API Call to OpenRouter using cURL\nDESCRIPTION: This snippet demonstrates how to make a direct API call to OpenRouter using cURL in a shell environment. It shows how to set headers, construct the request body, and send a POST request to the chat completions endpoint.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENROUTER_API_KEY\" \\\n  -d '{\n  \"model\": \"openai/gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the meaning of life?\"\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring PydanticAI with OpenRouter\nDESCRIPTION: Example of configuring PydanticAI to work with OpenRouter through its OpenAI-compatible interface. The code initializes an OpenAI model pointed to OpenRouter's endpoint with a specific model and creates an agent to process requests.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_105\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\n\nmodel = OpenAIModel(\n    \"anthropic/claude-3.5-sonnet\",  # or any other OpenRouter model\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=\"sk-or-...\",\n)\n\nagent = Agent(model)\nresult = await agent.run(\"What is the meaning of life?\")\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Managing API Keys with TypeScript\nDESCRIPTION: Shows key management operations using TypeScript and the Fetch API. Includes examples for listing, creating, retrieving, updating, and deleting API keys.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_40\n\nLANGUAGE: typescript\nCODE:\n```\nconst PROVISIONING_API_KEY = 'your-provisioning-key';\nconst BASE_URL = 'https://openrouter.ai/api/v1/keys';\n\n// List the most recent 100 API keys\nconst listKeys = await fetch(BASE_URL, {\n  headers: {\n    Authorization: `Bearer ${PROVISIONING_API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n});\n\n// You can paginate using the `offset` query parameter\nconst listKeys = await fetch(`${BASE_URL}?offset=100`, {\n  headers: {\n    Authorization: `Bearer ${PROVISIONING_API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n});\n\n// Create a new API key\nconst createKey = await fetch(`${BASE_URL}`, {\n  method: 'POST',\n  headers: {\n    Authorization: `Bearer ${PROVISIONING_API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    name: 'Customer Instance Key',\n    label: 'customer-123',\n    limit: 1000, // Optional credit limit\n  }),\n});\n\n// Get a specific key\nconst keyHash = '<YOUR_KEY_HASH>';\nconst getKey = await fetch(`${BASE_URL}/${keyHash}`, {\n  headers: {\n    Authorization: `Bearer ${PROVISIONING_API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n});\n\n// Update a key\nconst updateKey = await fetch(`${BASE_URL}/${keyHash}`, {\n  method: 'PATCH',\n  headers: {\n    Authorization: `Bearer ${PROVISIONING_API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    name: 'Updated Key Name',\n    disabled: true, // Disable the key\n  }),\n});\n\n// Delete a key\nconst deleteKey = await fetch(`${BASE_URL}/${keyHash}`, {\n  method: 'DELETE',\n  headers: {\n    Authorization: `Bearer ${PROVISIONING_API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Stream Cancellation with TypeScript\nDESCRIPTION: Implementation of stream cancellation in TypeScript using AbortController. This approach allows cancelling streaming requests by aborting the fetch operation, stopping model processing and billing for supported providers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_54\n\nLANGUAGE: typescript\nCODE:\n```\nconst controller = new AbortController();\n\ntry {\n  const response = await fetch(\n    'https://openrouter.ai/api/v1/chat/completions',\n    {\n      method: 'POST',\n      headers: {\n        Authorization: `Bearer ${{{API_KEY_REF}}}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        model: '{{MODEL}}',\n        messages: [{ role: 'user', content: 'Write a story' }],\n        stream: true,\n      }),\n      signal: controller.signal,\n    },\n  );\n\n  // Process the stream...\n} catch (error) {\n  if (error.name === 'AbortError') {\n    console.log('Stream cancelled');\n  } else {\n    throw error;\n  }\n}\n\n// To cancel the stream:\ncontroller.abort();\n```\n\n----------------------------------------\n\nTITLE: Implementing Chain-of-Thought Reasoning Between Models in TypeScript\nDESCRIPTION: This TypeScript code uses the OpenAI SDK to implement chain-of-thought reasoning with OpenRouter. It extracts reasoning from the deepseek-r1 model and injects it into gpt-4o-mini to compare response quality with and without reasoning assistance.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_97\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey,\n});\n\nasync function doReq(model, content, reasoningConfig) {\n  const payload = {\n    model,\n    messages: [{ role: 'user', content }],\n    stop: '</think>',\n    ...reasoningConfig,\n  };\n\n  return openai.chat.completions.create(payload);\n}\n\nasync function getResponseWithReasoning() {\n  const question = 'Which is bigger: 9.11 or 9.9?';\n  const reasoningResponse = await doReq(\n    'deepseek/deepseek-r1',\n    `${question} Please think this through, but don't output an answer`,\n  );\n  const reasoning = reasoningResponse.choices[0].message.reasoning;\n\n  // Let's test! Here's the naive response:\n  const simpleResponse = await doReq('openai/gpt-4o-mini', question);\n  console.log(simpleResponse.choices[0].message.content);\n\n  // Here's the response with the reasoning token injected:\n  const content = `${question}. Here is some context to help you: ${reasoning}`;\n  const smartResponse = await doReq('openai/gpt-4o-mini', content);\n  console.log(smartResponse.choices[0].message.content);\n}\n\ngetResponseWithReasoning();\n```\n\n----------------------------------------\n\nTITLE: CompletionsResponse Format Type Definition in TypeScript\nDESCRIPTION: This snippet defines the TypeScript type for the OpenRouter API response format, including the main Response type and its subtypes for different response scenarios.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_47\n\nLANGUAGE: typescript\nCODE:\n```\n// Definitions of subtypes are below\ntype Response = {\n  id: string;\n  // Depending on whether you set \"stream\" to \"true\" and\n  // whether you passed in \"messages\" or a \"prompt\", you\n  // will get a different output shape\n  choices: (NonStreamingChoice | StreamingChoice | NonChatChoice)[];\n  created: number; // Unix timestamp\n  model: string;\n  object: 'chat.completion' | 'chat.completion.chunk';\n\n  system_fingerprint?: string; // Only present if the provider supports it\n\n  // Usage data is always returned for non-streaming.\n  // When streaming, you will get one usage object at\n  // the end accompanied by an empty choices array.\n  usage?: ResponseUsage;\n};\n\n// If the provider returns usage, we pass it down\n// as-is. Otherwise, we count using the GPT-4 tokenizer.\n\ntype ResponseUsage = {\n  /** Including images and tools if any */\n  prompt_tokens: number;\n  /** The tokens generated */\n  completion_tokens: number;\n  /** Sum of the above two fields */\n  total_tokens: number;\n};\n\n// Subtypes:\ntype NonChatChoice = {\n  finish_reason: string | null;\n  text: string;\n  error?: ErrorResponse;\n};\n\ntype NonStreamingChoice = {\n  finish_reason: string | null;\n  native_finish_reason: string | null;\n  message: {\n    content: string | null;\n    role: string;\n    tool_calls?: ToolCall[];\n  };\n  error?: ErrorResponse;\n};\n\ntype StreamingChoice = {\n  finish_reason: string | null;\n  native_finish_reason: string | null;\n  delta: {\n    content: string | null;\n    role?: string;\n    tool_calls?: ToolCall[];\n  };\n  error?: ErrorResponse;\n};\n\ntype ErrorResponse = {\n  code: number; // See \"Error Handling\" section\n  message: string;\n  metadata?: Record<string, unknown>; // Contains additional error information such as provider details, the raw error message, etc.\n};\n\ntype ToolCall = {\n  id: string;\n  type: 'function';\n  function: FunctionCall;\n};\n```\n\n----------------------------------------\n\nTITLE: Streaming Mode with Reasoning Tokens for Anthropic Models in TypeScript\nDESCRIPTION: This TypeScript example demonstrates streaming responses from Anthropic's Claude 3.7 Sonnet with reasoning tokens. It shows how to configure the reasoning token budget and handle both reasoning and content in the streamed chunks.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_99\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey,\n});\n\nasync function chatCompletionWithReasoning(messages) {\n  const response = await openai.chat.completions.create({\n    model: '{{MODEL}}',\n    messages,\n    maxTokens: 10000,\n    reasoning: {\n      maxTokens: 8000, // Directly specify reasoning token budget\n    },\n    stream: true,\n  });\n\n  return response;\n}\n\n(async () => {\n  for await (const chunk of chatCompletionWithReasoning([\n    { role: 'user', content: \"What's bigger, 9.9 or 9.11?\" },\n  ])) {\n    if (chunk.choices[0].delta.reasoning) {\n      console.log(`REASONING: ${chunk.choices[0].delta.reasoning}`);\n    } else if (chunk.choices[0].delta.content) {\n      console.log(`CONTENT: ${chunk.choices[0].delta.content}`);\n    }\n  }\n})();\n```\n\n----------------------------------------\n\nTITLE: Converting MCP Tool Definitions to OpenAI Format in Python\nDESCRIPTION: Helper function to convert MCP tool definitions to OpenAI-compatible tool definitions. This allows the use of MCP servers with OpenRouter.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_88\n\nLANGUAGE: python\nCODE:\n```\ndef convert_tool_format(tool):\n    converted_tool = {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": tool.inputSchema[\"properties\"],\n                \"required\": tool.inputSchema[\"required\"]\n            }\n        }\n    }\n    return converted_tool\n```\n\n----------------------------------------\n\nTITLE: Assistant Prefill in Chat Completion Request using TypeScript\nDESCRIPTION: This code shows how to use the assistant prefill feature in OpenRouter. It includes a partial assistant response in the messages array, which can guide the model to respond in a specific way.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_44\n\nLANGUAGE: typescript\nCODE:\n```\nfetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer <OPENROUTER_API_KEY>',\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: 'openai/gpt-4o',\n    messages: [\n      { role: 'user', content: 'What is the meaning of life?' },\n      { role: 'assistant', content: \"I'm not sure, but my best guess is\" },\n    ],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing MCP Client for OpenRouter Integration in Python\nDESCRIPTION: Implements an MCP client class that connects to the MCP server, processes queries, and manages the chat loop. It uses OpenRouter API for model interactions and converts MCP tool calls to OpenAI format.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_89\n\nLANGUAGE: python\nCODE:\n```\nclass MCPClient:\n    def __init__(self):\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.openai = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\"\n        )\n\n    async def connect_to_server(self, server_config):\n        server_params = StdioServerParameters(**server_config)\n        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n        self.stdio, self.write = stdio_transport\n        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n\n        await self.session.initialize()\n\n        # List available tools from the MCP server\n        response = await self.session.list_tools()\n        print(\"\\nConnected to server with tools:\", [tool.name for tool in response.tools])\n\n        self.messages = []\n\n    async def process_query(self, query: str) -> str:\n\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": query\n        })\n\n        response = await self.session.list_tools()\n        available_tools = [convert_tool_format(tool) for tool in response.tools]\n\n        response = self.openai.chat.completions.create(\n            model=MODEL,\n            tools=available_tools,\n            messages=self.messages\n        )\n        self.messages.append(response.choices[0].message.model_dump())\n\n        final_text = []\n        content = response.choices[0].message\n        if content.tool_calls is not None:\n            tool_name = content.tool_calls[0].function.name\n            tool_args = content.tool_calls[0].function.arguments\n            tool_args = json.loads(tool_args) if tool_args else {}\n\n            # Execute tool call\n            try:\n                result = await self.session.call_tool(tool_name, tool_args)\n                final_text.append(f\"[Calling tool {tool_name} with args {tool_args}]\")\n            except Exception as e:\n                print(f\"Error calling tool {tool_name}: {e}\")\n                result = None\n\n            self.messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": content.tool_calls[0].id,\n                \"name\": tool_name,\n                \"content\": result.content\n            })\n\n            response = self.openai.chat.completions.create(\n                model=MODEL,\n                max_tokens=1000,\n                messages=self.messages,\n            )\n\n            final_text.append(response.choices[0].message.content)\n        else:\n            final_text.append(content.content)\n\n        return \"\\n\".join(final_text)\n\n    async def chat_loop(self):\n        \"\"\"Run an interactive chat loop\"\"\"\n        print(\"\\nMCP Client Started!\")\n        print(\"Type your queries or 'quit' to exit.\")\n\n        while True:\n            try:\n                query = input(\"\\nQuery: \").strip()\n                result = await self.process_query(query)\n                print(\"Result:\")\n                print(result)\n\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n\n    async def cleanup(self):\n        await self.exit_stack.aclose()\n\nasync def main():\n    client = MCPClient()\n    try:\n        await client.connect_to_server(SERVER_CONFIG)\n        await client.chat_loop()\n    finally:\n        await client.cleanup()\n\nif __name__ == \"__main__\":\n    import sys\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Uploading Base64 Encoded Images in TypeScript\nDESCRIPTION: This code demonstrates how to read a local image file, convert it to a base64 string, and include it in a multimodal request to the OpenRouter API.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_46\n\nLANGUAGE: typescript\nCODE:\n```\nimport { readFile } from \"fs/promises\";\n\nconst getFlowerImage = async (): Promise<string> => {\n  const imagePath = new URL(\"flower.jpg\", import.meta.url);\n  const imageBuffer = await readFile(imagePath);\n  const base64Image = imageBuffer.toString(\"base64\");\n  return `data:image/jpeg;base64,${base64Image}`;\n};\n\n...\n\n\"messages\": [\n  {\n    role: \"user\",\n    content: [\n      {\n        type: \"text\",\n        text: \"What's in this image?\",\n      },\n      {\n        type: \"image_url\",\n        image_url: {\n          url: `${await getFlowerImage()}`,\n        },\n      },\n    ],\n  },\n];\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Setup in TypeScript\nDESCRIPTION: Basic setup for implementing tool calling functionality using the OpenRouter API with TypeScript.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_27\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: `Bearer {{API_KEY_REF}}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: '{{MODEL}}',\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant.' },\n      {\n        role: 'user',\n        content: 'What are the titles of some James Joyce books?',\n      },\n    ],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Message Transforms in TypeScript\nDESCRIPTION: Example of using the transforms parameter to enable middle-out compression for handling large prompts that exceed model context windows.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_37\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  transforms: [\"middle-out\"], // Compress prompts that are > context size.\n  messages: [...],\n  model // Works with any model\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Setup in Python\nDESCRIPTION: Basic setup for implementing tool calling functionality using the OpenRouter API with Python.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport json, requests\nfrom openai import OpenAI\n\nOPENROUTER_API_KEY = f\"{{API_KEY_REF}}\"\n\n# You can use any model that supports tool calling\nMODEL = {{MODEL}}\n\nopenai_client = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=OPENROUTER_API_KEY,\n)\n\ntask = \"What are the titles of some James Joyce books?\"\n\nmessages = [\n  {\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant.\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": task,\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Specifying Provider Order with Fallbacks in OpenRouter API\nDESCRIPTION: Example showing how to specify provider order with fallbacks enabled when making requests to OpenRouter. Orders Together after OpenAI for Mixtral model with fallback to other providers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"mistralai/mixtral-8x7b-instruct\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"order\": [\"OpenAI\", \"Together\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Model List Endpoint Response Format in JSON\nDESCRIPTION: Specifies the required JSON format for the List Models endpoint that providers must implement to integrate with OpenRouter. Includes model details, pricing, and quantization information.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_91\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"id\": \"anthropic/claude-2.0\",\n      \"name\": \"Anthropic: Claude v2.0\",\n      \"created\": 1690502400,\n      \"description\": \"Anthropic's flagship model...\",\n      \"context_length\": 100000,\n      \"max_completion_tokens\": 4096,\n      \"quantization\": \"fp8\",\n      \"pricing\": {\n        \"prompt\": \"0.000008\",\n        \"completion\": \"0.000024\",\n        \"image\": \"0\",\n        \"request\": \"0\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using High Reasoning Effort with OpenAI Models\nDESCRIPTION: Examples showing how to set high reasoning effort in API requests to OpenAI's o3-mini model. The code demonstrates setting up headers, payload with the reasoning parameter, and extracting the reasoning from the response.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_93\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nurl = \"https://openrouter.ai/api/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer {{API_KEY_REF}}\",\n    \"Content-Type\": \"application/json\"\n}\npayload = {\n    \"model\": \"{{MODEL}}\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"How would you build the world's tallest skyscraper?\"}\n    ],\n    \"reasoning\": {\n        \"effort\": \"high\"  # Use high reasoning effort\n    }\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(payload))\nprint(response.json()['choices'][0]['message']['reasoning'])\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey: '{{API_KEY_REF}}',\n});\n\nasync function getResponseWithReasoning() {\n  const response = await openai.chat.completions.create({\n    model: '{{MODEL}}',\n    messages: [\n      {\n        role: 'user',\n        content: \"How would you build the world's tallest skyscraper?\",\n      },\n    ],\n    reasoning: {\n      effort: 'high', // Use high reasoning effort\n    },\n  });\n\n  console.log('REASONING:', response.choices[0].message.reasoning);\n  console.log('CONTENT:', response.choices[0].message.content);\n}\n\ngetResponseWithReasoning();\n```\n\n----------------------------------------\n\nTITLE: Configuring Provider Preferences for Throughput Optimization\nDESCRIPTION: Example of using the provider object to configure throughput-based sorting for model providers. This prioritizes providers with higher throughput rather than using the default price-based load balancing.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"meta-llama/llama-3.1-70b-instruct\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"sort\": \"throughput\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Tool Calls in TypeScript\nDESCRIPTION: Handles the response from the language model, processes any tool calls, and executes the requested function asynchronously. The results are then appended to the messages array for context in subsequent API calls.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_33\n\nLANGUAGE: typescript\nCODE:\n```\n// Append the response to the messages array so the LLM has the full context\n// It's easy to forget this step!\nmessages.push(response);\n\n// Now we process the requested tool calls, and use our book lookup tool\nfor (const toolCall of response.toolCalls) {\n  const toolName = toolCall.function.name;\n  const toolArgs = JSON.parse(toolCall.function.arguments);\n  const toolResponse = await TOOL_MAPPING[toolName](toolArgs);\n  messages.push({\n    role: 'tool',\n    toolCallId: toolCall.id,\n    name: toolName,\n    content: JSON.stringify(toolResponse),\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Reasoning Parameter Configuration in JSON\nDESCRIPTION: JSON configuration showing how to control reasoning tokens using the 'reasoning' parameter, including options for setting effort level, maximum tokens, and whether to exclude reasoning from the response.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_92\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"your-model\",\n  \"messages\": [],\n  \"reasoning\": {\n    // One of the following (not both):\n    \"effort\": \"high\", // Can be \"high\", \"medium\", or \"low\" (OpenAI-style)\n    \"max_tokens\": 2000, // Specific token limit (Anthropic-style)\n\n    // Optional: Default is false. All models support this.\n    \"exclude\": false // Set to true to exclude reasoning tokens from response\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making API Requests with Bearer Token in TypeScript\nDESCRIPTION: This snippet demonstrates how to make an API request to OpenRouter using a Bearer token for authentication in TypeScript. It includes setting the necessary headers and sending a chat completion request.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_55\n\nLANGUAGE: typescript\nCODE:\n```\nfetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer <OPENROUTER_API_KEY>',\n    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.\n    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: 'openai/gpt-4o',\n    messages: [\n      {\n        role: 'user',\n        content: 'What is the meaning of life?',\n      },\n    ],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Max Tokens for Reasoning with Anthropic Models\nDESCRIPTION: Examples demonstrating how to allocate a specific number of tokens for reasoning with Anthropic's Claude 3.7 Sonnet model. The code shows how to use the max_tokens parameter within the reasoning configuration.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nurl = \"https://openrouter.ai/api/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer {{API_KEY_REF}}\",\n    \"Content-Type\": \"application/json\"\n}\npayload = {\n    \"model\": \"{{MODEL}}\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What's the most efficient algorithm for sorting a large dataset?\"}\n    ],\n    \"reasoning\": {\n        \"max_tokens\": 2000  # Allocate 2000 tokens (or approximate effort) for reasoning\n    }\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(payload))\nprint(response.json()['choices'][0]['message']['reasoning'])\nprint(response.json()['choices'][0]['message']['content'])\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey: '{{API_KEY_REF}}',\n});\n\nasync function getResponseWithReasoning() {\n  const response = await openai.chat.completions.create({\n    model: '{{MODEL}}',\n    messages: [\n      {\n        role: 'user',\n        content: \"How would you build the world's tallest skyscraper?\",\n      },\n    ],\n    reasoning: {\n      max_tokens: 2000, // Allocate 2000 tokens (or approximate effort) for reasoning\n    },\n  });\n\n  console.log('REASONING:', response.choices[0].message.reasoning);\n  console.log('CONTENT:', response.choices[0].message.content);\n}\n\ngetResponseWithReasoning();\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Search Integration in JSON\nDESCRIPTION: Examples of enabling and customizing web search capabilities using the web plugin or model slug suffix.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"openai/gpt-4o:online\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"openrouter/auto\",\n  \"plugins\": [{ \"id\": \"web\" }]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"openai/gpt-4o:online\",\n  \"plugins\": [\n    {\n      \"id\": \"web\",\n      \"max_results\": 1,\n      \"search_prompt\": \"Some relevant web results:\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Gutenberg Book Search Tool in Python\nDESCRIPTION: Implements a function to search for books in the Project Gutenberg library and defines the tool specification for the language model. It includes a mapping of tool names to functions for execution.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef search_gutenberg_books(search_terms):\n    search_query = \" \".join(search_terms)\n    url = \"https://gutendex.com/books\"\n    response = requests.get(url, params={\"search\": search_query})\n\n    simplified_results = []\n    for book in response.json().get(\"results\", []):\n        simplified_results.append({\n            \"id\": book.get(\"id\"),\n            \"title\": book.get(\"title\"),\n            \"authors\": book.get(\"authors\")\n        })\n\n    return simplified_results\n\ntools = [\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"search_gutenberg_books\",\n      \"description\": \"Search for books in the Project Gutenberg library based on specified search terms\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"search_terms\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"List of search terms to find books in the Gutenberg library (e.g. ['dickens', 'great'] to search for books by Dickens with 'great' in the title)\"\n          }\n        },\n        \"required\": [\"search_terms\"]\n      }\n    }\n  }\n]\n\nTOOL_MAPPING = {\n    \"search_gutenberg_books\": search_gutenberg_books\n}\n```\n\n----------------------------------------\n\nTITLE: Stream Cancellation with Python\nDESCRIPTION: Implementation of stream cancellation in Python using threading and events. This allows cancelling streaming requests by aborting the connection, which stops model processing and billing for supported providers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom threading import Event, Thread\n\ndef stream_with_cancellation(prompt: str, cancel_event: Event):\n    with requests.Session() as session:\n        response = session.post(\n            \"https://openrouter.ai/api/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {{API_KEY_REF}}\"},\n            json={\"model\": \"{{MODEL}}\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"stream\": True},\n            stream=True\n        )\n\n        try:\n            for line in response.iter_lines():\n                if cancel_event.is_set():\n                    response.close()\n                    return\n                if line:\n                    print(line.decode(), end=\"\", flush=True)\n        finally:\n            response.close()\n\n# Example usage:\ncancel_event = Event()\nstream_thread = Thread(target=lambda: stream_with_cancellation(\"Write a story\", cancel_event))\nstream_thread.start()\n\n# To cancel the stream:\ncancel_event.set()\n```\n\n----------------------------------------\n\nTITLE: Making Initial OpenRouter API Call in Python\nDESCRIPTION: Demonstrates how to make the first API call to the OpenRouter API using the defined tool and messages. This call initiates the conversation with the language model.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nrequest_1 = {\n    \"model\": {{MODEL}},\n    \"tools\": tools,\n    \"messages\": messages\n}\n\nresponse_1 = openai_client.chat.completions.create(**request_1).message\n```\n\n----------------------------------------\n\nTITLE: Defining Gutenberg Book Search Tool in TypeScript\nDESCRIPTION: Implements an asynchronous function to search for books in the Project Gutenberg library and defines the tool specification for the language model. It includes a mapping of tool names to functions for execution.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_29\n\nLANGUAGE: typescript\nCODE:\n```\nasync function searchGutenbergBooks(searchTerms: string[]): Promise<Book[]> {\n  const searchQuery = searchTerms.join(' ');\n  const url = 'https://gutendex.com/books';\n  const response = await fetch(`${url}?search=${searchQuery}`);\n  const data = await response.json();\n\n  return data.results.map((book: any) => ({\n    id: book.id,\n    title: book.title,\n    authors: book.authors,\n  }));\n}\n\nconst tools = [\n  {\n    type: 'function',\n    function: {\n      name: 'search_gutenberg_books',\n      description:\n        'Search for books in the Project Gutenberg library based on specified search terms',\n      parameters: {\n        type: 'object',\n        properties: {\n          search_terms: {\n            type: 'array',\n            items: {\n              type: 'string',\n            },\n            description:\n              \"List of search terms to find books in the Gutenberg library (e.g. ['dickens', 'great'] to search for books by Dickens with 'great' in the title)\",\n          },\n        },\n        required: ['search_terms'],\n      },\n    },\n  },\n];\n\nconst TOOL_MAPPING = {\n  searchGutenbergBooks,\n};\n```\n\n----------------------------------------\n\nTITLE: Checking OpenRouter Credits Balance\nDESCRIPTION: API call example for checking available credits balance using the OpenRouter API. Returns total credits and usage information.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_81\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/credits', {\n  method: 'GET',\n  headers: { Authorization: 'Bearer <OPENROUTER_API_KEY>' },\n});\nconst { data } = await response.json();\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": {\n    \"total_credits\": 50.0,\n    \"total_usage\": 42.0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making Initial OpenRouter API Call in TypeScript\nDESCRIPTION: Shows how to make the first API call to the OpenRouter API using fetch in TypeScript. This call initiates the conversation with the language model using the defined tool and messages.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_31\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: `Bearer {{API_KEY_REF}}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: '{{MODEL}}',\n    tools,\n    messages,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Using Fallback Models with OpenAI TypeScript SDK\nDESCRIPTION: Implementation of the 'models' parameter fallback feature using the OpenAI TypeScript SDK. The example tries gpt-4o first, then falls back to claude-3.5-sonnet and mythomax-l2-13b in order if needed.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openrouterClient = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  // API key and headers\n});\n\nasync function main() {\n  // @ts-expect-error\n  const completion = await openrouterClient.chat.completions.create({\n    model: 'openai/gpt-4o',\n    models: ['anthropic/claude-3.5-sonnet', 'gryphe/mythomax-l2-13b'],\n    messages: [\n      {\n        role: 'user',\n        content: 'What is the meaning of life?',\n      },\n    ],\n  });\n  console.log(completion.choices[0].message);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Complete Weather API Implementation in TypeScript\nDESCRIPTION: Full implementation example using Fetch API to make structured output requests with TypeScript.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer {{API_KEY_REF}}',\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: '{{MODEL}}',\n    messages: [\n      { role: 'user', content: 'What is the weather like in London?' },\n    ],\n    response_format: {\n      type: 'json_schema',\n      json_schema: {\n        name: 'weather',\n        strict: true,\n        schema: {\n          type: 'object',\n          properties: {\n            location: {\n              type: 'string',\n              description: 'City or location name',\n            },\n            temperature: {\n              type: 'number',\n              description: 'Temperature in Celsius',\n            },\n            conditions: {\n              type: 'string',\n              description: 'Weather conditions description',\n            },\n          },\n          required: ['location', 'temperature', 'conditions'],\n          additionalProperties: false,\n        },\n      },\n    },\n  }),\n});\n\nconst data = await response.json();\nconst weatherInfo = data.choices[0].message.content;\n```\n\n----------------------------------------\n\nTITLE: Using Fallback Models with the 'models' Parameter\nDESCRIPTION: Example of specifying fallback models with the 'models' parameter. This allows OpenRouter to try alternative models if the primary model's providers are down, rate-limited, or refuse to reply due to content moderation.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\"anthropic/claude-3.5-sonnet\", \"gryphe/mythomax-l2-13b\"],\n  ... // Other params\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Configuration with Structured Outputs\nDESCRIPTION: Example of enabling streaming responses with structured outputs using JSON configuration.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_25\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  \"stream\": true,\n  \"response_format\": {\n    \"type\": \"json_schema\",\n    // ... rest of your schema\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Anthropic Claude User Message Caching in JSON\nDESCRIPTION: This example shows how to implement prompt caching with Anthropic Claude in a user message. The JSON request structures the content to include a large text body with cache_control setting of ephemeral type, allowing for efficient reuse of the cached content.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Given the book below:\"\n        },\n        {\n          \"type\": \"text\",\n          \"text\": \"HUGE TEXT BODY\",\n          \"cache_control\": {\n            \"type\": \"ephemeral\"\n          }\n        },\n        {\n          \"type\": \"text\",\n          \"text\": \"Name all the characters in the above book\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking API Key Status in TypeScript and Python\nDESCRIPTION: Examples of how to check rate limits and credits remaining for an API key using both TypeScript and Python implementations.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_59\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/auth/key', {\n  method: 'GET',\n  headers: {\n    Authorization: 'Bearer {{API_KEY_REF}}',\n  },\n});\n```\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nresponse = requests.get(\n  url=\"https://openrouter.ai/api/v1/auth/key\",\n  headers={\n    \"Authorization\": f\"Bearer {{API_KEY_REF}}\"\n  }\n)\n\nprint(json.dumps(response.json(), indent=2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Anthropic Claude System Message Caching in JSON\nDESCRIPTION: This example demonstrates how to implement prompt caching with Anthropic Claude using cache_control breakpoints in a system message. It shows how to structure a JSON request that includes a large text body marked for caching with an ephemeral type.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"You are a historian studying the fall of the Roman Empire. You know the following book very well:\"\n        },\n        {\n          \"type\": \"text\",\n          \"text\": \"HUGE TEXT BODY\",\n          \"cache_control\": {\n            \"type\": \"ephemeral\"\n          }\n        }\n      ]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What triggered the collapse?\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Provider Fallbacks in OpenRouter API\nDESCRIPTION: Example demonstrating how to disable fallbacks to ensure requests are only served by the primary provider.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"allow_fallbacks\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Generation Stats in TypeScript\nDESCRIPTION: This code demonstrates how to query the OpenRouter API for generation stats using the generation ID. This allows retrieving detailed information about the request, including token counts and cost.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_49\n\nLANGUAGE: typescript\nCODE:\n```\nconst generation = await fetch(\n  'https://openrouter.ai/api/v1/generation?id=$GENERATION_ID',\n  { headers },\n);\n\nconst stats = await generation.json();\n```\n\n----------------------------------------\n\nTITLE: Ignoring Specific Providers in OpenRouter API\nDESCRIPTION: Example showing how to ignore specific providers (Azure in this case) when making requests to GPT-4 Omni model.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"openai/gpt-4o\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"ignore\": [\"Azure\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making OpenRouter API Requests\nDESCRIPTION: Example of making an API request to OpenRouter's chat completions endpoint using the obtained API key.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_86\n\nLANGUAGE: typescript\nCODE:\n```\nfetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer <API_KEY>',\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: 'openai/gpt-4o',\n    messages: [\n      {\n        role: 'user',\n        content: 'Hello!',\n      },\n    ],\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Quantization Levels in OpenRouter API\nDESCRIPTION: Example demonstrating how to specify quantization levels when making requests, filtering for providers that support FP8 quantization.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"meta-llama/llama-3.1-8b-instruct\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"quantizations\": [\"fp8\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making API Requests with cURL in Shell\nDESCRIPTION: This Shell script shows how to make an API request to OpenRouter using cURL. It demonstrates setting the necessary headers and sending a chat completion request.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_58\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENROUTER_API_KEY\" \\\n  -d '{\n  \"model\": \"openai/gpt-4o\",\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Provider Order without Fallbacks in OpenRouter API\nDESCRIPTION: Example demonstrating how to specify provider order with fallbacks disabled, restricting the request to only use specified providers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"mistralai/mixtral-8x7b-instruct\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"order\": [\"OpenAI\", \"Together\"],\n    \"allow_fallbacks\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example CompletionsResponse JSON\nDESCRIPTION: This JSON snippet illustrates an example response from the OpenRouter API, showing the structure of a non-streaming completion response with usage information.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_48\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"gen-xxxxxxxxxxxxxx\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\", // Normalized finish_reason\n      \"native_finish_reason\": \"stop\", // The raw finish_reason from the provider\n      \"message\": {\n        // will be \"delta\" if streaming\n        \"role\": \"assistant\",\n        \"content\": \"Hello there!\"\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 0,\n    \"completion_tokens\": 4,\n    \"total_tokens\": 4\n  },\n  \"model\": \"openai/gpt-3.5-turbo\" // Could also be \"anthropic/claude-2.1\", etc, depending on the \"model\" that ends up being used\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Structured Output Request Schema in TypeScript\nDESCRIPTION: Example request structure for enforcing JSON Schema validation on model responses, including weather information schema definition.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"What's the weather like in London?\" }\n  ],\n  \"response_format\": {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n      \"name\": \"weather\",\n      \"strict\": true,\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"City or location name\"\n          },\n          \"temperature\": {\n            \"type\": \"number\",\n            \"description\": \"Temperature in Celsius\"\n          },\n          \"conditions\": {\n            \"type\": \"string\",\n            \"description\": \"Weather conditions description\"\n          }\n        },\n        \"required\": [\"location\", \"temperature\", \"conditions\"],\n        \"additionalProperties\": false\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Bedrock API Keys for OpenRouter BYOK\nDESCRIPTION: JSON configuration format for using Amazon Bedrock with OpenRouter. Requires AWS access key ID, secret access key, and region where Bedrock models are deployed.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_74\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"accessKeyId\": \"your-aws-access-key-id\",\n  \"secretAccessKey\": \"your-aws-secret-access-key\",\n  \"region\": \"your-aws-region\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"bedrock:InvokeModel\",\n        \"bedrock:InvokeModelWithResponseStream\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Weather Response Format Example in JSON\nDESCRIPTION: Example of structured JSON response format following the defined weather schema.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"location\": \"London\",\n  \"temperature\": 18,\n  \"conditions\": \"Partly cloudy with light drizzle\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up MCP Server Configuration and Dependencies in Python\nDESCRIPTION: Initializes necessary imports, environment variables, and server configuration for using MCP servers with OpenRouter. Requires pip installation of packages and a .env file with OPENAI_API_KEY set.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_87\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom typing import Optional\nfrom contextlib import AsyncExitStack\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport json\n\nload_dotenv()  # load environment variables from .env\n\nMODEL = \"anthropic/claude-3-7-sonnet\"\n\nSERVER_CONFIG = {\n    \"command\": \"npx\",\n    \"args\": [\"-y\",\n              \"@modelcontextprotocol/server-filesystem\",\n              f\"/Applications/\"],\n    \"env\": None\n}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenRouter provider for Vercel AI SDK\nDESCRIPTION: Command to install the OpenRouter provider package for the Vercel AI SDK, which allows integration of OpenRouter with Next.js applications.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_106\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @openrouter/ai-sdk-provider\n```\n\n----------------------------------------\n\nTITLE: Complete Weather API Implementation in Python\nDESCRIPTION: Full implementation example using requests library to make structured output requests with Python.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nresponse = requests.post(\n  \"https://openrouter.ai/api/v1/chat/completions\",\n  headers={\n    \"Authorization\": f\"Bearer {{API_KEY_REF}}\",\n    \"Content-Type\": \"application/json\",\n  },\n\n  json={\n    \"model\": \"{{MODEL}}\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is the weather like in London?\"},\n    ],\n    \"response_format\": {\n      \"type\": \"json_schema\",\n      \"json_schema\": {\n        \"name\": \"weather\",\n        \"strict\": True,\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"City or location name\",\n            },\n            \"temperature\": {\n              \"type\": \"number\",\n              \"description\": \"Temperature in Celsius\",\n            },\n            \"conditions\": {\n              \"type\": \"string\",\n              \"description\": \"Weather conditions description\",\n            },\n          },\n          \"required\": [\"location\", \"temperature\", \"conditions\"],\n          \"additionalProperties\": False,\n        },\n      },\n    },\n  },\n)\n\ndata = response.json()\nweather_info = data[\"choices\"][0][\"message\"][\"content\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Data Collection Policy in OpenRouter API\nDESCRIPTION: Example showing how to configure data collection policies to restrict requests to providers based on their data handling practices.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }],\n  \"provider\": {\n    \"data_collection\": \"deny\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multimodal Request with Image URL in TypeScript\nDESCRIPTION: This snippet demonstrates how to send a multimodal request to the OpenRouter API, including both text and an image URL in the messages array.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_45\n\nLANGUAGE: typescript\nCODE:\n```\n\"messages\": [\n  {\n    \"role\": \"user\",\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"What's in this image?\"\n      },\n      {\n        \"type\": \"image_url\",\n        \"image_url\": {\n          \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n        }\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: SSE Comment Example\nDESCRIPTION: Example of an SSE comment that OpenRouter occasionally sends to prevent connection timeouts. These comments can be safely ignored per SSE specifications but can also be leveraged to improve user experience.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_52\n\nLANGUAGE: text\nCODE:\n```\n: OPENROUTER PROCESSING\n```\n\n----------------------------------------\n\nTITLE: Excluding Reasoning Tokens from Response with DeepSeek Models\nDESCRIPTION: Examples showing how to use internal reasoning without including it in the response for DeepSeek R1 models. The code demonstrates setting the exclude parameter to true within the reasoning configuration.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_95\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport json\n\nurl = \"https://openrouter.ai/api/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer {{API_KEY_REF}}\",\n    \"Content-Type\": \"application/json\"\n}\npayload = {\n    \"model\": \"{{MODEL}}\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n    ],\n    \"reasoning\": {\n        \"effort\": \"high\",\n        \"exclude\": true  # Use reasoning but don't include it in the response\n    }\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(payload))\n# No reasoning field in the response\nprint(response.json()['choices'][0]['message']['content'])\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'https://openrouter.ai/api/v1',\n  apiKey: '{{API_KEY_REF}}',\n});\n\nasync function getResponseWithReasoning() {\n  const response = await openai.chat.completions.create({\n    model: '{{MODEL}}',\n    messages: [\n      {\n        role: 'user',\n        content: \"How would you build the world's tallest skyscraper?\",\n      },\n    ],\n    reasoning: {\n      effort: 'high',\n      exclude: true, // Use reasoning but don't include it in the response\n    },\n  });\n\n  console.log('REASONING:', response.choices[0].message.reasoning);\n  console.log('CONTENT:', response.choices[0].message.content);\n}\n\ngetResponseWithReasoning();\n```\n\n----------------------------------------\n\nTITLE: API Endpoints Example Requests\nDESCRIPTION: Example cURL requests for various API endpoints including completions, chat, and model information.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_63\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/completions \\\n     -H \"Authorization: Bearer <token>\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"model\": \"model\",\n  \"prompt\": \"prompt\"\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/chat/completions \\\n     -H \"Authorization: Bearer <token>\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"model\": \"openai/gpt-3.5-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the meaning of life?\"\n    }\n  ]\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -G https://openrouter.ai/api/v1/generation \\\n     -H \"Authorization: Bearer <token>\" \\\n     -d id=id\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/models\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/models/author/slug/endpoints\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure API Keys for OpenRouter BYOK\nDESCRIPTION: JSON configuration format for using Azure AI Services with OpenRouter. Requires endpoint URL, API key, model ID, and model slug mapped to OpenRouter.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_73\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model_slug\": \"the-openrouter-model-slug\",\n  \"endpoint_url\": \"https://<resource>.services.ai.azure.com/deployments/<model-id>/chat/completions?api-version=<api-version>\",\n  \"api_key\": \"your-azure-api-key\",\n  \"model_id\": \"the-azure-model-id\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"model_slug\": \"mistralai/mistral-large\",\n    \"endpoint_url\": \"https://example-project.openai.azure.com/openai/deployments/mistral-large/chat/completions?api-version=2024-08-01-preview\",\n    \"api_key\": \"your-azure-api-key\",\n    \"model_id\": \"mistral-large\"\n  },\n  {\n    \"model_slug\": \"openai/gpt-4o\",\n    \"endpoint_url\": \"https://example-project.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n    \"api_key\": \"your-azure-api-key\",\n    \"model_id\": \"gpt-4o\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Getting Current API Key Information\nDESCRIPTION: Makes a GET request to retrieve information about the API key associated with the current authentication session. Requires authentication via Bearer token.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_67\n\nLANGUAGE: http\nCODE:\n```\nGET https://openrouter.ai/api/v1/key\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/key \\\n     -H \"Authorization: Bearer <token>\"\n```\n\n----------------------------------------\n\nTITLE: Getting Credits from OpenRouter API\nDESCRIPTION: Makes a GET request to retrieve the total credits purchased and used for the authenticated user. Requires authentication via Bearer token.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_64\n\nLANGUAGE: http\nCODE:\n```\nGET https://openrouter.ai/api/v1/credits\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/credits \\\n     -H \"Authorization: Bearer <token>\"\n```\n\n----------------------------------------\n\nTITLE: Listing All API Keys\nDESCRIPTION: Makes a GET request to retrieve a list of all API keys associated with the account. Requires a Provisioning API key for authentication. Optional query parameters for offset and including disabled keys.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_68\n\nLANGUAGE: http\nCODE:\n```\nGET https://openrouter.ai/api/v1/keys\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/keys \\\n     -H \"Authorization: Bearer <token>\"\n```\n\n----------------------------------------\n\nTITLE: Creating New API Key\nDESCRIPTION: Makes a POST request to create a new API key. Requires a Provisioning API key for authentication and JSON payload with the name parameter.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_69\n\nLANGUAGE: http\nCODE:\n```\nPOST https://openrouter.ai/api/v1/keys\nContent-Type: application/json\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/keys \\\n     -H \"Authorization: Bearer <token>\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"name\": \"name\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Deleting API Key\nDESCRIPTION: Makes a DELETE request to remove an API key identified by its hash. Requires a Provisioning API key for authentication.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_71\n\nLANGUAGE: http\nCODE:\n```\nDELETE https://openrouter.ai/api/v1/keys/{hash}\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X DELETE https://openrouter.ai/api/v1/keys/hash \\\n     -H \"Authorization: Bearer <token>\"\n```\n\n----------------------------------------\n\nTITLE: Getting Specific API Key Details\nDESCRIPTION: Makes a GET request to retrieve details about a specific API key identified by its hash. Requires a Provisioning API key for authentication.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_70\n\nLANGUAGE: http\nCODE:\n```\nGET https://openrouter.ai/api/v1/keys/{hash}\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://openrouter.ai/api/v1/keys/hash \\\n     -H \"Authorization: Bearer <token>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Viem Clients for Ethereum Transactions\nDESCRIPTION: Setup of viem public and wallet clients for interacting with the Base network. Includes configuration of HTTP transport and account initialization.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_79\n\nLANGUAGE: typescript\nCODE:\n```\nconst publicClient = createPublicClient({\n  chain: base,\n  transport: http(),\n});\nconst account = privateKeyToAccount('0x...');\nconst walletClient = createWalletClient({\n  chain: base,\n  transport: http(),\n  account,\n});\n```\n\n----------------------------------------\n\nTITLE: Updating Existing API Key\nDESCRIPTION: Makes a PATCH request to update an existing API key identified by its hash. Requires a Provisioning API key for authentication and JSON payload with the updated parameters.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_72\n\nLANGUAGE: http\nCODE:\n```\nPATCH https://openrouter.ai/api/v1/keys/{hash}\nContent-Type: application/json\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X PATCH https://openrouter.ai/api/v1/keys/hash \\\n     -H \"Authorization: Bearer <token>\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{}'\n```\n\n----------------------------------------\n\nTITLE: Error Response Type Definition\nDESCRIPTION: TypeScript type definition for API error responses showing structure and metadata.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_61\n\nLANGUAGE: typescript\nCODE:\n```\ntype ErrorResponse = {\n  error: {\n    code: number;\n    message: string;\n    metadata?: Record<string, unknown>;\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: API Key Response Type Definition\nDESCRIPTION: TypeScript type definition for the API key response object showing available properties.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_60\n\nLANGUAGE: typescript\nCODE:\n```\ntype Key = {\n  data: {\n    label: string;\n    usage: number;\n    limit: number | null;\n    is_free_tier: boolean;\n    rate_limit: {\n      requests: number;\n      interval: string;\n    };\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: Extracting Auth Code from URL Parameters\nDESCRIPTION: Code to extract the authentication code from URL parameters after user authorization.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_84\n\nLANGUAGE: typescript\nCODE:\n```\nconst urlParams = new URLSearchParams(window.location.search);\nconst code = urlParams.get('code');\n```\n\n----------------------------------------\n\nTITLE: OpenRouter API Response Format\nDESCRIPTION: Example JSON response format for API key operations showing the structure of returned data including creation timestamp, key hash, label, name, status, and usage information.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"created_at\": \"2025-02-19T20:52:27.363244+00:00\",\n      \"updated_at\": \"2025-02-19T21:24:11.708154+00:00\",\n      \"hash\": \"<YOUR_KEY_HASH>\",\n      \"label\": \"sk-or-v1-customkey\",\n      \"name\": \"Customer Key\",\n      \"disabled\": false,\n      \"limit\": 10,\n      \"usage\": 0\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Generating SHA-256 Code Challenge in TypeScript\nDESCRIPTION: Function to generate a secure code challenge using SHA-256 hashing and Base64URL encoding. Requires Buffer API and must be used with a bundler in web browsers.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_83\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Buffer } from 'buffer';\n\nasync function createSHA256CodeChallenge(input: string) {\n  const encoder = new TextEncoder();\n  const data = encoder.encode(input);\n  const hash = await crypto.subtle.digest('SHA-256', data);\n  return Buffer.from(hash).toString('base64url');\n}\n\nconst codeVerifier = 'your-random-string';\nconst generatedCodeChallenge = await createSHA256CodeChallenge(codeVerifier);\n```\n\n----------------------------------------\n\nTITLE: Using the Nitro Shortcut for Provider Throughput Optimization\nDESCRIPTION: Example of using the ':nitro' suffix shortcut to prioritize providers by throughput. This is equivalent to setting provider.sort to 'throughput'.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"meta-llama/llama-3.1-70b-instruct:nitro\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenRouter Auth URLs with PKCE\nDESCRIPTION: Different authentication URL formats for OpenRouter with varying security levels - S256 code challenge (recommended), plain code challenge, and without code challenge.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_82\n\nLANGUAGE: txt\nCODE:\n```\nhttps://openrouter.ai/auth?callback_url=<YOUR_SITE_URL>&code_challenge=<CODE_CHALLENGE>&code_challenge_method=S256\n```\n\nLANGUAGE: txt\nCODE:\n```\nhttps://openrouter.ai/auth?callback_url=<YOUR_SITE_URL>&code_challenge=<CODE_CHALLENGE>&code_challenge_method=plain\n```\n\nLANGUAGE: txt\nCODE:\n```\nhttps://openrouter.ai/auth?callback_url=<YOUR_SITE_URL>\n```\n\n----------------------------------------\n\nTITLE: Using the Floor Price Shortcut for Provider Price Optimization\nDESCRIPTION: Example of using the ':floor' suffix shortcut to explicitly prioritize providers by price. This is equivalent to setting provider.sort to 'price'.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"meta-llama/llama-3.1-70b-instruct:floor\",\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }]\n}\n```\n\n----------------------------------------\n\nTITLE: Exchanging Auth Code for API Key\nDESCRIPTION: API call to exchange the authorization code for a user-controlled API key, including optional code verifier parameters.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_85\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/auth/keys', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    code: '<CODE_FROM_QUERY_PARAM>',\n    code_verifier: '<CODE_VERIFIER>', // If code_challenge was used\n    code_challenge_method: '<CODE_CHALLENGE_METHOD>', // If code_challenge was used\n  }),\n});\n\nconst { key } = await response.json();\n```\n\n----------------------------------------\n\nTITLE: Exchanging Authorization Code for API Key\nDESCRIPTION: Makes a POST request to exchange an authorization code from the PKCE flow for a user-controlled API key. Requires a JSON payload with the code parameter.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_66\n\nLANGUAGE: http\nCODE:\n```\nPOST https://openrouter.ai/api/v1/auth/keys\nContent-Type: application/json\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/auth/keys \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"code\": \"code\"\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/auth/keys \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"code\": \"string\"\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/auth/keys \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"code\": \"string\"\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/auth/keys \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"code\": \"string\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing PydanticAI for OpenRouter integration\nDESCRIPTION: Installation command for PydanticAI with OpenAI compatibility to use with OpenRouter.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_104\n\nLANGUAGE: bash\nCODE:\n```\npip install 'pydantic-ai-slim[openai]'\n```\n\n----------------------------------------\n\nTITLE: Creating Coinbase Charge via OpenRouter API\nDESCRIPTION: Makes a POST request to create and hydrate a Coinbase Commerce charge for cryptocurrency payments. Requires authentication and JSON payload with amount, sender, and chain_id.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_65\n\nLANGUAGE: http\nCODE:\n```\nPOST https://openrouter.ai/api/v1/credits/coinbase\nContent-Type: application/json\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST https://openrouter.ai/api/v1/credits/coinbase \\\n     -H \"Authorization: Bearer <token>\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"amount\": 1.1,\n  \"sender\": \"sender\",\n  \"chain_id\": 1\n}'\n```\n\n----------------------------------------\n\nTITLE: Demonstrating MCP Client Usage with OpenRouter in Bash\nDESCRIPTION: Shows an example interaction with the MCP client using OpenRouter, demonstrating how to check for installed applications and process queries.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_90\n\nLANGUAGE: bash\nCODE:\n```\n% python mcp-client.py\n\nSecure MCP Filesystem Server running on stdio\nAllowed directories: [ '/Applications' ]\n\nConnected to server with tools: ['read_file', 'read_multiple_files', 'write_file'...]\n\nMCP Client Started!\nType your queries or 'quit' to exit.\n\nQuery: Do I have microsoft office installed?\n\nResult:\n[Calling tool list_allowed_directories with args {}]\nI can check if Microsoft Office is installed in the Applications folder:\n\nQuery: continue\n\nResult:\n[Calling tool search_files with args {'path': '/Applications', 'pattern': 'Microsoft'}]\nNow let me check specifically for Microsoft Office applications:\n\nQuery: continue\n\nResult:\nI can see from the search results that Microsoft Office is indeed installed on your system.\nThe search found the following main Microsoft Office applications:\n\n1. Microsoft Excel - /Applications/Microsoft Excel.app\n2. Microsoft PowerPoint - /Applications/Microsoft PowerPoint.app\n3. Microsoft Word - /Applications/Microsoft Word.app\n4. OneDrive - /Applications/OneDrive.app (which includes Microsoft SharePoint integration)\n```\n\n----------------------------------------\n\nTITLE: Creating Coinbase Credit Purchase Request in TypeScript\nDESCRIPTION: Makes a POST request to OpenRouter's Coinbase API endpoint to initiate a credit purchase transaction. Requires API key authentication and includes amount in USD, sender address, and chain ID parameters.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_75\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await fetch('https://openrouter.ai/api/v1/credits/coinbase', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer <OPENROUTER_API_KEY>',\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    amount: 10, // Target credit amount in USD\n    sender: '0x9a85CB3bfd494Ea3a8C9E50aA6a3c1a7E8BACE11',\n    chain_id: 8453,\n  }),\n});\nconst responseJSON = await response.json();\n```\n\n----------------------------------------\n\nTITLE: Coinbase Credit Purchase Response Structure\nDESCRIPTION: JSON response structure from the credit purchase API endpoint containing charge details and transaction data for executing the on-chain payment. Includes transfer intent metadata and calldata required for blockchain transaction.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_76\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": {\n    \"id\": \"...\",\n    \"created_at\": \"2024-01-01T00:00:00Z\",\n    \"expires_at\": \"2024-01-01T01:00:00Z\",\n    \"web3_data\": {\n      \"transfer_intent\": {\n        \"metadata\": {\n          \"chain_id\": 8453,\n          \"contract_address\": \"0x03059433bcdb6144624cc2443159d9445c32b7a8\",\n          \"sender\": \"0x9a85CB3bfd494Ea3a8C9E50aA6a3c1a7E8BACE11\"\n        },\n        \"call_data\": {\n          \"recipient_amount\": \"...\",\n          \"deadline\": \"...\",\n          \"recipient\": \"...\",\n          \"recipient_currency\": \"...\",\n          \"refund_destination\": \"...\",\n          \"fee_amount\": \"...\",\n          \"id\": \"...\",\n          \"operator\": \"...\",\n          \"signature\": \"...\",\n          \"prefix\": \"...\"\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Solidity Contract Interface Definition\nDESCRIPTION: Definition of a smart contract interface containing functions for token transfers, Uniswap V3 swaps, and administrative operations. Includes structured types for transfer intents and permit data.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_78\n\nLANGUAGE: solidity\nCODE:\n```\n{\n  components: [\n    { internalType: 'uint256', name: 'recipientAmount', type: 'uint256' },\n    { internalType: 'uint256', name: 'deadline', type: 'uint256' },\n    { internalType: 'address payable', name: 'recipient', type: 'address' },\n    { internalType: 'address', name: 'recipientCurrency', type: 'address' },\n    { internalType: 'address', name: 'refundDestination', type: 'address' },\n    { internalType: 'uint256', name: 'feeAmount', type: 'uint256' },\n    { internalType: 'bytes16', name: 'id', type: 'bytes16' },\n    { internalType: 'address', name: 'operator', type: 'address' },\n    { internalType: 'bytes', name: 'signature', type: 'bytes' },\n    { internalType: 'bytes', name: 'prefix', type: 'bytes' }\n  ],\n  internalType: 'struct TransferIntent',\n  name: '_intent',\n  type: 'tuple'\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Viem Client for Coinbase Onchain Payment Protocol Transaction\nDESCRIPTION: This code snippet demonstrates how to import necessary viem functions and set up the ABI for Coinbase's onchain payment protocol. It includes the complete contract ABI for interacting with Coinbase's payment protocol, focusing on the swapAndTransferUniswapV3Native function for fulfilling charges.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_77\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createPublicClient, createWalletClient, http, parseEther } from 'viem';\nimport { privateKeyToAccount } from 'viem/accounts';\nimport { base } from 'viem/chains';\n\n// The ABI for Coinbase's onchain payment protocol\nconst abi = [\n  {\n    inputs: [\n      {\n        internalType: 'contract IUniversalRouter',\n        name: '_uniswap',\n        type: 'address',\n      },\n      { internalType: 'contract Permit2', name: '_permit2', type: 'address' },\n      { internalType: 'address', name: '_initialOperator', type: 'address' },\n      {\n        internalType: 'address',\n        name: '_initialFeeDestination',\n        type: 'address',\n      },\n      {\n        internalType: 'contract IWrappedNativeCurrency',\n        name: '_wrappedNativeCurrency',\n        type: 'address',\n      },\n    ],\n    stateMutability: 'nonpayable',\n    type: 'constructor',\n  },\n  { inputs: [], name: 'AlreadyProcessed', type: 'error' },\n  { inputs: [], name: 'ExpiredIntent', type: 'error' },\n  {\n    inputs: [\n      { internalType: 'address', name: 'attemptedCurrency', type: 'address' },\n    ],\n    name: 'IncorrectCurrency',\n    type: 'error',\n  },\n  { inputs: [], name: 'InexactTransfer', type: 'error' },\n  {\n    inputs: [{ internalType: 'uint256', name: 'difference', type: 'uint256' }],\n    name: 'InsufficientAllowance',\n    type: 'error',\n  },\n  {\n    inputs: [{ internalType: 'uint256', name: 'difference', type: 'uint256' }],\n    name: 'InsufficientBalance',\n    type: 'error',\n  },\n  {\n    inputs: [{ internalType: 'int256', name: 'difference', type: 'int256' }],\n    name: 'InvalidNativeAmount',\n    type: 'error',\n  },\n  { inputs: [], name: 'InvalidSignature', type: 'error' },\n  { inputs: [], name: 'InvalidTransferDetails', type: 'error' },\n  {\n    inputs: [\n      { internalType: 'address', name: 'recipient', type: 'address' },\n      { internalType: 'uint256', name: 'amount', type: 'uint256' },\n      { internalType: 'bool', name: 'isRefund', type: 'bool' },\n      { internalType: 'bytes', name: 'data', type: 'bytes' },\n    ],\n    name: 'NativeTransferFailed',\n    type: 'error',\n  },\n  { inputs: [], name: 'NullRecipient', type: 'error' },\n  { inputs: [], name: 'OperatorNotRegistered', type: 'error' },\n  { inputs: [], name: 'PermitCallFailed', type: 'error' },\n  {\n    inputs: [{ internalType: 'bytes', name: 'reason', type: 'bytes' }],\n    name: 'SwapFailedBytes',\n    type: 'error',\n  },\n  {\n    inputs: [{ internalType: 'string', name: 'reason', type: 'string' }],\n    name: 'SwapFailedString',\n    type: 'error',\n  },\n  {\n    anonymous: false,\n    inputs: [\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'operator',\n        type: 'address',\n      },\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'feeDestination',\n        type: 'address',\n      },\n    ],\n    name: 'OperatorRegistered',\n    type: 'event',\n  },\n  {\n    anonymous: false,\n    inputs: [\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'operator',\n        type: 'address',\n      },\n    ],\n    name: 'OperatorUnregistered',\n    type: 'event',\n  },\n  {\n    anonymous: false,\n    inputs: [\n      {\n        indexed: true,\n        internalType: 'address',\n        name: 'previousOwner',\n        type: 'address',\n      },\n      {\n        indexed: true,\n        internalType: 'address',\n        name: 'newOwner',\n        type: 'address',\n      },\n    ],\n    name: 'OwnershipTransferred',\n    type: 'event',\n  },\n  {\n    anonymous: false,\n    inputs: [\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'account',\n        type: 'address',\n      },\n    ],\n    name: 'Paused',\n    type: 'event',\n  },\n  {\n    anonymous: false,\n    inputs: [\n      {\n        indexed: true,\n        internalType: 'address',\n        name: 'operator',\n        type: 'address',\n      },\n      { indexed: false, internalType: 'bytes16', name: 'id', type: 'bytes16' },\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'recipient',\n        type: 'address',\n      },\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'sender',\n        type: 'address',\n      },\n      {\n        indexed: false,\n        internalType: 'uint256',\n        name: 'spentAmount',\n        type: 'uint256',\n      },\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'spentCurrency',\n        type: 'address',\n      },\n    ],\n    name: 'Transferred',\n    type: 'event',\n  },\n  {\n    anonymous: false,\n    inputs: [\n      {\n        indexed: false,\n        internalType: 'address',\n        name: 'account',\n        type: 'address',\n      },\n    ],\n    name: 'Unpaused',\n    type: 'event',\n  },\n  {\n    inputs: [],\n    name: 'owner',\n    outputs: [{ internalType: 'address', name: '', type: 'address' }],\n    stateMutability: 'view',\n    type: 'function',\n  },\n  {\n    inputs: [],\n    name: 'pause',\n    outputs: [],\n    stateMutability: 'nonpayable',\n    type: 'function',\n  },\n  {\n    inputs: [],\n    name: 'paused',\n    outputs: [{ internalType: 'bool', name: '', type: 'bool' }],\n    stateMutability: 'view',\n    type: 'function',\n  },\n  {\n    inputs: [],\n    name: 'permit2',\n    outputs: [{ internalType: 'contract Permit2', name: '', type: 'address' }],\n    stateMutability: 'view',\n    type: 'function',\n  },\n  {\n    inputs: [],\n    name: 'registerOperator',\n    outputs: [],\n    stateMutability: 'nonpayable',\n    type: 'function',\n  },\n  {\n    inputs: [\n      { internalType: 'address', name: '_feeDestination', type: 'address' },\n    ],\n    name: 'registerOperatorWithFeeDestination',\n    outputs: [],\n    stateMutability: 'nonpayable',\n    type: 'function',\n  },\n  {\n    inputs: [],\n    name: 'renounceOwnership',\n    outputs: [],\n    stateMutability: 'nonpayable',\n    type: 'function',\n  },\n  {\n    inputs: [{ internalType: 'address', name: 'newSweeper', type: 'address' }],\n    name: 'setSweeper',\n    outputs: [],\n    stateMutability: 'nonpayable',\n    type: 'function',\n  },\n  {\n    inputs: [\n      {\n        components: [\n          { internalType: 'uint256', name: 'recipientAmount', type: 'uint256' },\n          { internalType: 'uint256', name: 'deadline', type: 'uint256' },\n          {\n            internalType: 'address payable',\n            name: 'recipient',\n            type: 'address',\n          },\n          {\n            internalType: 'address',\n            name: 'recipientCurrency',\n            type: 'address',\n          },\n          {\n            internalType: 'address',\n            name: 'refundDestination',\n            type: 'address',\n          },\n          { internalType: 'uint256', name: 'feeAmount', type: 'uint256' },\n          { internalType: 'bytes16', name: 'id', type: 'bytes16' },\n          { internalType: 'address', name: 'operator', type: 'address' },\n          { internalType: 'bytes', name: 'signature', type: 'bytes' },\n          { internalType: 'bytes', name: 'prefix', type: 'bytes' },\n        ],\n        internalType: 'struct TransferIntent',\n        name: '_intent',\n        type: 'tuple',\n      },\n      {\n        components: [\n          { internalType: 'address', name: 'owner', type: 'address' },\n          { internalType: 'bytes', name: 'signature', type: 'bytes' },\n        ],\n        internalType: 'struct EIP2612SignatureTransferData',\n        name: '_signatureTransferData',\n        type: 'tuple',\n      },\n    ],\n    name: 'subsidizedTransferToken',\n    outputs: [],\n    stateMutability: 'nonpayable',\n    type: 'function',\n  },\n  {\n    inputs: [\n      {\n        components: [\n          { internalType: 'uint256', name: 'recipientAmount', type: 'uint256' },\n          { internalType: 'uint256', name: 'deadline', type: 'uint256' },\n          {\n            internalType: 'address payable',\n            name: 'recipient',\n            type: 'address',\n          },\n          {\n            internalType: 'address',\n            name: 'recipientCurrency',\n            type: 'address',\n          },\n          {\n            internalType: 'address',\n            name: 'refundDestination',\n            type: 'address',\n          },\n          { internalType: 'uint256', name: 'feeAmount', type: 'uint256' },\n          { internalType: 'bytes16', name: 'id', type: 'bytes16' },\n          { internalType: 'address', name: 'operator', type: 'address' },\n          { internalType: 'bytes', name: 'signature', type: 'bytes' },\n          { internalType: 'bytes', name: 'prefix', type: 'bytes' },\n        ],\n        internalType: 'struct TransferIntent',\n        name: '_intent',\n        type: 'tuple',\n      },\n      { internalType: 'uint24', name: 'poolFeesTier', type: 'uint24' },\n    ],\n    name: 'swapAndTransferUniswapV3Native',\n    outputs: [],\n    stateMutability: 'payable',\n    type: 'function',\n  },\n  {\n    inputs: [\n      {\n        components: [\n          { internalType: 'uint256', name: 'recipientAmount', type: 'uint256' },\n          { internalType: 'uint256', name: 'deadline', type: 'uint256' },\n          {\n            internalType: 'address payable',\n            name: 'recipient',\n            type: 'address',\n          },\n          {\n            internalType: 'address',\n            name: 'recipientCurrency',\n            type: 'address',\n          }\n```\n\n----------------------------------------\n\nTITLE: Simulating and Executing Ethereum Contract Transaction\nDESCRIPTION: Example of simulating and executing a contract transaction using viem clients. Includes contract call setup, parameter configuration, and transaction execution.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_80\n\nLANGUAGE: typescript\nCODE:\n```\nconst { request } = await publicClient.simulateContract({\n  abi,\n  account,\n  address: contract_address,\n  functionName: 'swapAndTransferUniswapV3Native',\n  args: [\n    {\n      recipientAmount: BigInt(call_data.recipient_amount),\n      deadline: BigInt(\n        Math.floor(new Date(call_data.deadline).getTime() / 1000),\n      ),\n      recipient: call_data.recipient,\n      recipientCurrency: call_data.recipient_currency,\n      refundDestination: call_data.refund_destination,\n      feeAmount: BigInt(call_data.fee_amount),\n      id: call_data.id,\n      operator: call_data.operator,\n      signature: call_data.signature,\n      prefix: call_data.prefix,\n    },\n    poolFeesTier,\n  ],\n  value: parseEther('0.004'),\n});\n\nconst txHash = await walletClient.writeContract(request);\nconsole.log('Transaction hash:', txHash);\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request with Headers in TypeScript\nDESCRIPTION: This snippet demonstrates how to send a chat completion request to the OpenRouter API using TypeScript. It includes optional headers for app identification and specifies the model and message content.\nSOURCE: https://github.com/thechickenisonfire/openrouter-docs/blob/main/llms-full.txt#2025-04-20_snippet_43\n\nLANGUAGE: typescript\nCODE:\n```\nfetch('https://openrouter.ai/api/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    Authorization: 'Bearer <OPENROUTER_API_KEY>',\n    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.\n    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    model: 'openai/gpt-4o',\n    messages: [\n      {\n        role: 'user',\n        content: 'What is the meaning of life?',\n      },\n    ],\n  }),\n});\n```"
  }
]
[
  {
    "owner": "daveebbelaar",
    "repo": "ai-cookbook",
    "content": "TITLE: Basic Usage of Instructor with OpenAI\nDESCRIPTION: Shows how to use Instructor to extract structured data from natural language. It defines a Pydantic model for the expected output structure, patches the OpenAI client, and processes a simple text input to extract name and age information.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n)\n\nprint(user_info.name)\n#> John Doe\nprint(user_info.age)\n#> 30\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple MCP Server in Python\nDESCRIPTION: This snippet shows how to create a basic MCP server with a single tool using the FastMCP class. It defines a 'say_hello' function as a tool and sets up the server to run.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"DemoServer\")\n\n# Simple tool\n@mcp.tool()\ndef say_hello(name: str) -> str:\n    \"\"\"Say hello to someone\n\n    Args:\n        name: The person's name to greet\n    \"\"\"\n    return f\"Hello, {name}! Nice to meet you.\"\n\n# Run the server\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Mode with OpenAI API in Python\nDESCRIPTION: This snippet demonstrates how to use JSON mode with the OpenAI API to generate structured responses. It includes setting up the query, defining the system message, and parsing the JSON response.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Hi there, I have a question about my bill. Can you help me?\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\n        You're a helpful customer care assistant that can classify incoming messages and create a response.\n        Always response in the following JSON format: {\"content\": <response>, \"category\": <classification>}\n        Available categories: 'general', 'order', 'billing'\n        \"\"\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": query,\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=messages,\n    response_format={\"type\": \"json_object\"},\n)\nmessage = response.choices[0].message.content\n\ntype(message)  # str\n\nmessage_json = json.loads(message)\ntype(message_json)  # dict\n```\n\n----------------------------------------\n\nTITLE: Implementing Function Calling with OpenAI API in Python\nDESCRIPTION: This snippet shows how to use Function Calling with the OpenAI API for structured output. It includes defining the function, setting up the query, and parsing the function call response.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Hi there, I have a question about my bill. Can you help me?\"\n\nfunction_name = \"chat\"\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": function_name,\n            \"description\": f\"Function to respond to a customer query.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"content\": {\n                        \"type\": \"string\",\n                        \"description\": \"Your reply that we send to the customer.\",\n                    },\n                    \"category\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"general\", \"order\", \"billing\"],\n                        \"description\": \"Category of the ticket.\",\n                    },\n                },\n                \"required\": [\"content\", \"category\"],\n            },\n        },\n    }\n]\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You're a helpful customer care assistant that can classify incoming messages and create a response.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": query,\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=messages,\n    tools=tools,\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": function_name}},\n)\n\ntool_call = response.choices[0].message.tool_calls[0]\ntype(tool_call)  # ChatCompletionMessageToolCall\n\nfunction_args = json.loads(tool_call.function.arguments)\ntype(function_args)  # dict\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Lifecycle Management with Lifespan Object in MCP\nDESCRIPTION: Demonstrates how to create and use a lifespan object in MCP for managing application-level resources throughout the server's lifecycle, including initialization, operation, and proper cleanup.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/7-lifecycle-management/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\nfrom dataclasses import dataclass\n\nfrom mcp.server.fastmcp import Context, FastMCP\n\n# Define a type-safe context class\n@dataclass\nclass AppContext:\n    db: Database  # Replace with your actual resource type\n\n# Create the lifespan context manager\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    # Initialize resources on startup\n    db = await Database.connect()\n    try:\n        # Make resources available during operation\n        yield AppContext(db=db)\n    finally:\n        # Clean up resources on shutdown\n        await db.disconnect()\n\n# Create the MCP server with the lifespan\nmcp = FastMCP(\"My App\", lifespan=app_lifespan)\n\n# Use the lifespan context in tools\n@mcp.tool()\ndef query_db(ctx: Context) -> str:\n    \"\"\"Tool that uses initialized resources\"\"\"\n    db = ctx.request_context.lifespan_context.db\n    return db.query()\n```\n\n----------------------------------------\n\nTITLE: Content Filtering using llm_validator with Pydantic\nDESCRIPTION: Shows how to implement content filtering with the llm_validator function. It defines a ValidatedReply model that uses BeforeValidator and Annotated to attach a validator that ensures responses don't harm company reputation.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\nclass ValidatedReply(BaseModel):\n    content: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\n                statement=\"Never say things that could hurt the reputation of the company.\",\n                client=client,\n                allow_override=True,\n            )\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: OpenAI Request with Confidence Score Range Validation\nDESCRIPTION: Demonstrates a request to OpenAI that validates confidence scores are within the expected range. The example shows how to configure system prompts to guide the model and how to use max_retries for automatic retry on validation failure.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nreply = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Reply,\n    max_retries=3,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a helpful customer care assistant that can classify incoming messages and create a response. Set confidence between 1-100.\",\n        },\n        {\"role\": \"user\", \"content\": query},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing MCP Client with Server-Sent Events\nDESCRIPTION: This snippet illustrates how to create a client that connects to an MCP server using Server-Sent Events (SSE). It establishes a connection over HTTP, lists available tools, and calls a specific tool.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport nest_asyncio\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client\n\nasync def main():\n    # Connect to the server using SSE\n    async with sse_client(\"http://localhost:8050/sse\") as (read_stream, write_stream):\n        async with ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available tools\n            tools_result = await session.list_tools()\n            print(\"Available tools:\")\n            for tool in tools_result.tools:\n                print(f\"  - {tool.name}: {tool.description}\")\n\n            # Call our calculator tool\n            result = await session.call_tool(\"add\", arguments={\"a\": 2, \"b\": 3})\n            print(f\"2 + 3 = {result.content[0].text}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining Enum Categories with Pydantic for Validation\nDESCRIPTION: Demonstrates how to define an Enum class for validating ticket categories and a Pydantic model that includes validation rules for the response content. The model uses Field to add descriptions and constraints.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TicketCategory(str, Enum):\n    \"\"\"Enumeration of categories for incoming tickets.\"\"\"\n\n    GENERAL = \"general\"\n    ORDER = \"order\"\n    BILLING = \"billing\"\n\nclass Reply(BaseModel):\n    content: str = Field(description=\"Your reply that we send to the customer.\")\n    category: TicketCategory\n    confidence: float = Field(\n        ge=0, le=1, description=\"Confidence in the category prediction.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Visualizing MCP Architecture with Mermaid Diagram\nDESCRIPTION: This Mermaid diagram illustrates the MCP architecture, showing the relationships between the host with MCP client, multiple MCP servers, local data sources, and remote services. It demonstrates the modular and composable nature of the MCP system.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/2-understanding-mcp/README.md#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n---\nconfig:\n  theme: neutral\n  look: classic\n  layout: dagre\n---\nflowchart LR\n subgraph Computer[\"Your Computer\"]\n        Client[\"Host with MCP Client<br>(Claude, IDEs, Tools)\"]\n        ServerA[\"MCP Server A\"]\n        ServerB[\"MCP Server B\"]\n        ServerC[\"MCP Server C\"]\n        DataA[(\"Local<br>Data Source A\")]\n        DataB[(\"Local<br>Data Source B\")]\n  end\n subgraph Internet[\"Internet\"]\n        RemoteC[(\"Remote<br>Service C\")]\n  end\n    Client -- MCP Protocol --> ServerA & ServerB & ServerC\n    ServerA <--> DataA\n    ServerB <--> DataB\n    ServerC -- Web APIs --> RemoteC\n```\n\n----------------------------------------\n\nTITLE: Comparing Stdio and SSE Transport Mechanisms with Mermaid Diagram\nDESCRIPTION: This Mermaid diagram compares the Stdio and SSE transport mechanisms used in MCP. It shows the communication flow for both local (Stdio) and remote (SSE) deployments, highlighting the differences in data transmission methods between MCP clients and servers.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/2-understanding-mcp/README.md#2025-04-21_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\n---\nconfig:\n  theme: neutral\n  look: classic\n  layout: dagre\n---\nflowchart LR\n subgraph Stdio[\"Stdio Transport\"]\n        Client1[\"MCP Client\"]\n        Server1[\"MCP Server\"]\n  end\n subgraph SSE[\"SSE Transport\"]\n        Client2[\"MCP Client\"]\n        Server2[\"MCP Server\"]\n  end\n subgraph Local[\"Local Deployment\"]\n        Stdio\n  end\n subgraph Remote[\"Remote Deployment\"]\n        SSE\n  end\n    Client1 -- stdin/stdout<br>(bidirectional) --> Server1\n    Client2 -- HTTP POST<br>(client to server) --> Server2\n    Server2 -- SSE<br>(server to client) --> Client2\n    style Client1 fill:#BBDEFB\n    style Server1 fill:#BBDEFB\n    style Client2 fill:#BBDEFB\n    style Server2 fill:#E1BEE7\n```\n\n----------------------------------------\n\nTITLE: Implementing MCP Client with Standard I/O\nDESCRIPTION: This code demonstrates how to create a client that connects to an MCP server using standard I/O. It establishes a connection, lists available tools, and calls a specific tool.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport nest_asyncio\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync def main():\n    # Define server parameters\n    server_params = StdioServerParameters(\n        command=\"python\",  # The command to run your server\n        args=[\"server.py\"],  # Arguments to the command\n    )\n\n    # Connect to the server\n    async with stdio_client(server_params) as (read_stream, write_stream):\n        async with ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available tools\n            tools_result = await session.list_tools()\n            print(\"Available tools:\")\n            for tool in tools_result.tools:\n                print(f\"  - {tool.name}: {tool.description}\")\n\n            # Call our calculator tool\n            result = await session.call_tool(\"add\", arguments={\"a\": 2, \"b\": 3})\n            print(f\"2 + 3 = {result.content[0].text}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: MCP Tool Discovery and Execution in Python\nDESCRIPTION: Shows how to discover available tools from an MCP server and execute tool calls during the operation phase of the MCP lifecycle.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/7-lifecycle-management/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Tool discovery example\ntools_result = await session.list_tools()\nprint(\"Available tools:\")\nfor tool in tools_result.tools:\n    print(f\"  - {tool.name}: {tool.description}\")\n\n# Tool execution example\nresult = await session.call_tool(\n    tool_call.function.name,\n    arguments=json.loads(tool_call.function.arguments),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Server for SSE Transport\nDESCRIPTION: This snippet shows how to modify the MCP server to use Server-Sent Events (SSE) transport, allowing it to communicate over HTTP on a specific port.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"MyServer\", host=\"127.0.0.1\", port=8050)\n\n# Add your tools and resources here...\n\nif __name__ == \"__main__\":\n    # Run with SSE transport on port 8000\n    mcp.run(transport=\"sse\")\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API key in .env file for Python\nDESCRIPTION: This snippet shows how to set up the OpenAI API key in a .env file for use in a Python application. It's a crucial step for authenticating requests to the OpenAI API.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/4-openai-integration/README.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nSet up your OpenAI API key in the .env file\n```\n\n----------------------------------------\n\nTITLE: MCP CLI Development Commands\nDESCRIPTION: Essential CLI commands for MCP development, including testing servers with MCP Inspector, installing servers in Claude Desktop, and running servers directly.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Test a server with the MCP Inspector\nmcp dev server.py\n\n# Install a server in Claude Desktop\nmcp install server.py\n\n# Run a server directly\nmcp run server.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing Blog Writing Orchestrator-Workers Pattern in Mermaid\nDESCRIPTION: This Mermaid diagram demonstrates the orchestrator-workers pattern for a blog writing system. It shows the flow from topic input through planning, writing, and review phases, with the writing phase highlighted as the core worker process.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/patterns/workflows/README.md#2025-04-21_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    A[Topic Input] --> B[Orchestrator]\n    B --> C[Planning Phase]\n    C --> D[Writing Phase]\n    D --> E[Review Phase]\n    style D fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n----------------------------------------\n\nTITLE: Visualizing Calendar Assistant Prompt Chain in Mermaid\nDESCRIPTION: This Mermaid diagram illustrates a 3-step prompt chain for a calendar assistant. It shows the flow from user input through extraction, validation, parsing details, and generating a confirmation.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/patterns/workflows/README.md#2025-04-21_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    A[User Input] --> B[LLM 1: Extract]\n    B --> C{Gate Check}\n    C -->|Pass| D[LLM 2: Parse Details]\n    C -->|Fail| E[Exit]\n    D --> F[LLM 3: Generate Confirmation]\n    F --> G[Final Output]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Calendar Assistant Parallelization in Mermaid\nDESCRIPTION: This Mermaid diagram illustrates the parallelization pattern for a calendar assistant. It shows how multiple checks (calendar and security) are run concurrently before aggregating results for validation.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/patterns/workflows/README.md#2025-04-21_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    A[User Input] --> B[Calendar Check]\n    A --> C[Security Check]\n    B --> D{Aggregate}\n    C --> D\n    D -->|Valid| E[Continue]\n    D -->|Invalid| F[Exit]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Calendar Assistant Routing in Mermaid\nDESCRIPTION: This Mermaid diagram shows the routing pattern for a calendar assistant. It demonstrates how user input is classified and directed to specialized handlers for new events or event modifications.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/patterns/workflows/README.md#2025-04-21_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    A[User Input] --> B[LLM Router]\n    B --> C{Route}\n    C -->|New Event| D[New Event Handler]\n    C -->|Modify Event| E[Modify Event Handler]\n    C -->|Other| F[Exit]\n    D --> G[Response]\n    E --> G\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI Request with Content Filtering and Error Handling\nDESCRIPTION: Demonstrates how to create a chat completion with the ValidatedReply model for content filtering. It includes a try-except block to catch validation errors that might occur if the content violates the specified validation rules.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    reply = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=ValidatedReply,\n        max_retries=1,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You're a helpful customer care assistant that can classify incoming messages and create a response.\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n    )\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Initializing MCP Client Session in Python\nDESCRIPTION: Demonstrates how to initialize an MCP client connection using a context manager that establishes a session with the server and initializes the connection.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/7-lifecycle-management/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Client initialization example\nasync with stdio_client(server_params) as (read, write):\n    async with ClientSession(read, write) as session:\n        # Initialize the connection\n        await session.initialize()\n```\n\n----------------------------------------\n\nTITLE: Automatic Termination of MCP Session using Context Managers in Python\nDESCRIPTION: Illustrates how MCP session termination happens automatically when exiting the context manager, ensuring proper cleanup of resources.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/7-lifecycle-management/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Termination happens automatically when exiting the context manager\nasync with ClientSession(read, write) as session:\n    # Session operations\n    # ...\n# Session is automatically terminated here\n```\n\n----------------------------------------\n\nTITLE: Key Features and Implementation Details\nDESCRIPTION: Detailed markdown documentation describing the important aspects of the Responses API, including backward compatibility, migration timeline, key features, available tools, migration considerations, and implementation details.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/05-responses/README.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Most important things to know\n\n1. **Backward Compatibility**: The Responses API is a superset of Chat Completions - everything you can do with Chat Completions can be done with Responses API, plus additional features.\n\n2. **Migration Timeline**: The Chat Completions API is not being deprecated and will continue to be supported indefinitely as an industry standard for building AI applications, while the Assistants API (not Chat Completions) is the one planned for eventual deprecation in 2026.\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Main documentation structure outlining the topics covered in the OpenAI Responses API guide, including introduction, text prompting, conversation states, function calling, structured output, web search, reasoning, and file search.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/05-responses/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# OpenAI Responses API\n\n## What we will cover\n\n1. Introduction\n2. Text prompting\n3. Conversation states\n4. Function calling\n5. Structured output\n6. Web search\n7. Reasoning\n8. File search\n```\n\n----------------------------------------\n\nTITLE: Installing MCP Dependencies with Package Managers\nDESCRIPTION: Commands for installing required MCP dependencies using either uv (recommended) or pip package managers from a requirements.txt file.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Using uv (recommended)\nuv pip install -r requirements.txt\n\n# Or using pip\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MCP Server\nDESCRIPTION: This command builds a Docker image for the MCP server using the Dockerfile in the current directory. The image is tagged as 'mcp-server'.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/6-run-with-docker/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t mcp-server .\n```\n\n----------------------------------------\n\nTITLE: Running MCP Server Docker Container\nDESCRIPTION: This command runs the MCP server Docker container, mapping port 8050 from the container to the host. This allows the server to be accessed on port 8050 of the host machine.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/6-run-with-docker/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8050:8050 mcp-server\n```\n\n----------------------------------------\n\nTITLE: Running MCP Client\nDESCRIPTION: This command runs the Python client script that connects to the MCP server. The client lists available tools and calls the calculator tool to add 2 and 3.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/6-run-with-docker/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython client.py\n```\n\n----------------------------------------\n\nTITLE: Running MCP Server in Development Mode\nDESCRIPTION: This command demonstrates how to run the MCP server in development mode using the MCP Inspector, which provides a web-based interface for interacting with the server's tools and resources.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmcp dev server.py\n```\n\n----------------------------------------\n\nTITLE: Installing MCP Server for Claude Desktop\nDESCRIPTION: This command shows how to install the MCP server for use with Claude Desktop, making it available within the Claude environment.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmcp install server.py\n```\n\n----------------------------------------\n\nTITLE: Running MCP Server Directly\nDESCRIPTION: These commands illustrate how to run the MCP server directly, either as a Python script or using the UV package manager (recommended).\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/3-simple-server-setup/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Method 1: Running as a Python script\npython server.py\n\n# Method 2: Using UV (recommended)\nuv run server.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for AI Project\nDESCRIPTION: This snippet lists the required Python packages for an AI project. It includes OpenAI's main library (version 1.66.2), an OpenAI agents package, Pydantic for data validation and serialization, and Instructor for additional functionality.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nopenai==1.66.2\nopenai-agents\npydantic\ninstructor\n```\n\n----------------------------------------\n\nTITLE: Dependencies List for AI Project\nDESCRIPTION: Required Python packages for an AI-focused application including API clients (requests, openai), development tools (ipykernel), environment management (python-dotenv), data validation (pydantic), text processing (docling), vector database (lancedb), web interface (streamlit), and token counting (tiktoken).\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/knowledge/docling/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nrequests\nipykernel\npython-dotenv\nopenai\npydantic\ndocling\nlancedb\nstreamlit\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for AI Project\nDESCRIPTION: This snippet lists the required Python packages for an AI-related project. It includes 'requests' for HTTP operations, 'ipykernel' for Jupyter notebook kernel, 'python-dotenv' for environment variable management, 'openai' for OpenAI API integration, and 'pydantic' for data validation.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/patterns/workflows/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nrequests\nipykernel\npython-dotenv\nopenai\npydantic\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for AI Cookbook\nDESCRIPTION: This snippet lists the required Python packages and their versions for an AI cookbook project. It includes mcp[cli], openai, python-dotenv, ipykernel, and httpx. These packages provide essential functionality for machine learning, API interactions, and development environment setup.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmcp[cli]==1.6.0 \nopenai==1.75.0\npython-dotenv\nipykernel\nhttpx\n```\n\n----------------------------------------\n\nTITLE: Installing Docling Dependencies\nDESCRIPTION: Command to install required packages for the Docling pipeline using pip package manager.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/knowledge/docling/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: Configuration for setting up the OpenAI API key in a .env file for use with Docling.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/knowledge/docling/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Running Python client for OpenAI-MCP integration\nDESCRIPTION: This command runs the Python client script that handles the integration between OpenAI and the MCP server. It's the entry point for executing the example.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/4-openai-integration/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython client.py\n```\n\n----------------------------------------\n\nTITLE: Installing Instructor Package with pip\nDESCRIPTION: Command to install the Instructor package using pip. The -U flag ensures you get the latest version of the package.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U instructor\n```\n\n----------------------------------------\n\nTITLE: CLI Command Entry\nDESCRIPTION: A basic command line interface input showing 'mcp' command.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/mcp/crash-course/6-run-with-docker/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: cli\nCODE:\n```\nmcp[cli]\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI Request with Enum Validation and Retries\nDESCRIPTION: Shows how to create a chat completion that validates the response against the Reply model. It specifies max_retries to automatically retry if validation fails, and includes system and user messages to guide the model.\nSOURCE: https://github.com/daveebbelaar/ai-cookbook/blob/main/models/openai/04-structured-output/Instructor/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nreply = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Reply,\n    max_retries=1,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a helpful customer care assistant that can classify incoming messages and create a response. Set the category to 'banana'.\",\n        },\n        {\"role\": \"user\", \"content\": query},\n    ],\n)\n```"
  }
]
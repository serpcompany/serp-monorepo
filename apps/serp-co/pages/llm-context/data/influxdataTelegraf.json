[
  {
    "owner": "influxdata",
    "repo": "telegraf",
    "content": "TITLE: Configuring InfluxDB v2.x Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the InfluxDB v2.x output plugin for Telegraf. It includes options for specifying InfluxDB URLs, authentication, organization, bucket, timeouts, HTTP headers, and various other settings for fine-tuning the plugin's behavior.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/influxdb_v2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for sending metrics to InfluxDB 2.0\n[[outputs.influxdb_v2]]\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://127.0.0.1:8086\"]\n\n  ## Local address to bind when connecting to the server\n  ## If empty or not set, the local address is automatically chosen.\n  # local_address = \"\"\n\n  ## Token for authentication.\n  token = \"\"\n\n  ## Organization is the name of the organization you wish to write to.\n  organization = \"\"\n\n  ## Destination bucket to write into.\n  bucket = \"\"\n\n  ## The value of this tag will be used to determine the bucket.  If this\n  ## tag is not set the 'bucket' option is used as the default.\n  # bucket_tag = \"\"\n\n  ## If true, the bucket tag will not be added to the metric.\n  # exclude_bucket_tag = false\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## Enable or disable uint support for writing uints influxdb 2.0.\n  # influx_uint_support = false\n\n  ## When true, Telegraf will omit the timestamp on data to allow InfluxDB\n  ## to set the timestamp of the data during ingestion. This is generally NOT\n  ## what you want as it can lead to data points captured at different times\n  ## getting omitted due to similar data.\n  # influx_omit_timestamp = false\n\n  ## HTTP/2 Timeouts\n  ## The following values control the HTTP/2 client's timeouts. These settings\n  ## are generally not required unless a user is seeing issues with client\n  ## disconnects. If a user does see issues, then it is suggested to set these\n  ## values to \"15s\" for ping timeout and \"30s\" for read idle timeout and\n  ## retry.\n  ##\n  ## Note that the timer for read_idle_timeout begins at the end of the last\n  ## successful write and not at the beginning of the next write.\n  # ping_timeout = \"0s\"\n  # read_idle_timeout = \"0s\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Rate limits for sending data (disabled by default)\n  ## Available, uncompressed payload size e.g. \"5Mb\"\n  # rate_limit = \"unlimited\"\n  ## Fixed time-window for the available payload size e.g. \"5m\"\n  # rate_limit_period = \"0s\"\n```\n\n----------------------------------------\n\nTITLE: Adding Data Format Support to Output Configuration\nDESCRIPTION: TOML configuration snippet for output plugins that support multiple data formats. This should be added to the plugin's SampleConfig() function to document serialization options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/OUTPUTS.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Generating Telegraf Configuration File\nDESCRIPTION: Commands to generate a default or filtered Telegraf configuration file using the telegraf command-line tool.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf config > telegraf.conf\n```\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf config --input-filter cpu:mem:net:swap --output-filter influxdb:kafka\n```\n\n----------------------------------------\n\nTITLE: Basic Telegraf Configuration\nDESCRIPTION: Simple TOML configuration file that enables CPU and Memory monitoring inputs and a file output that prints metrics to STDOUT.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/QUICK_START.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n[[inputs.mem]]\n[[outputs.file]]\n```\n\n----------------------------------------\n\nTITLE: Setting Agent Interval in Telegraf Configuration\nDESCRIPTION: Example of setting the agent interval in the Telegraf configuration, demonstrating the time duration format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[agent]\n  interval = \"10s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Client TLS in Telegraf using TOML\nDESCRIPTION: This snippet shows the standard client-side TLS configuration options for Telegraf plugins. It includes options for enabling TLS, specifying certificates and keys, setting server name for SNI, and controlling verification behavior.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TLS.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n## Enable/disable TLS\n## Set to true/false to enforce TLS being enabled/disabled. If not set,\n## enable TLS only if any of the other options are specified.\n# tls_enable =\n\n## Root certificates for verifying server certificates encoded in PEM format.\n# tls_ca = \"/etc/telegraf/ca.pem\"\n\n## The public and private key pairs for the client encoded in PEM format.  May\n## contain intermediate certificates.\n# tls_cert = \"/etc/telegraf/cert.pem\"\n# tls_key = \"/etc/telegraf/key.pem\"\n# passphrase for encrypted private key, if it is in PKCS#8 format. Encrypted PKCS#1 private keys are not supported.\n# tls_key_pwd = \"changeme\"\n## Skip TLS verification.\n# insecure_skip_verify = false\n## Send the specified TLS server name via SNI.\n# tls_server_name = \"foo.example.com\"\n#\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Additional Tags\nDESCRIPTION: Example showing how to configure the CPU input plugin with additional tags using TOML table syntax.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  [inputs.cpu.tags]\n    tag1 = \"foo\"\n    tag2 = \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for InfluxDB 1.x Configuration\nDESCRIPTION: Shell script defining environment variables for Telegraf to connect to InfluxDB 1.x, including URL, credentials and database settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nUSER=\"alice\"\nINFLUX_URL=\"http://localhost:8086\"\nINFLUX_SKIP_DATABASE_CREATION=\"true\"\nINFLUX_PASSWORD=\"monkey123\"\n```\n\n----------------------------------------\n\nTITLE: Telegraf Configuration with Environment Variables\nDESCRIPTION: TOML configuration file for Telegraf using environment variables for global tags and multiple InfluxDB output configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[global_tags]\n  user = \"${USER}\"\n\n[[inputs.mem]]\n\n# For InfluxDB 1.x:\n[[outputs.influxdb]]\n  urls = [\"${INFLUX_URL}\"]\n  skip_database_creation = ${INFLUX_SKIP_DATABASE_CREATION}\n  password = \"${INFLUX_PASSWORD}\"\n\n# For InfluxDB OSS 2:\n[[outputs.influxdb_v2]]\n  urls = [\"${INFLUX_HOST}\"]\n  token = \"${INFLUX_TOKEN}\"\n  organization = \"${INFLUX_ORG}\"\n  bucket = \"${INFLUX_BUCKET}\"\n\n# For InfluxDB Cloud 2:\n[[outputs.influxdb_v2]]\n  urls = [\"${INFLUX_HOST}\"]\n  token = \"${INFLUX_TOKEN}\"\n  organization = \"${INFLUX_ORG}\"\n  bucket = \"${INFLUX_BUCKET}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Listener v2 Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the HTTP Listener v2 Input Plugin for Telegraf. It specifies the service address, paths, HTTP methods, timeouts, TLS settings, authentication, and data format for incoming metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/http_listener_v2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Generic HTTP write listener\n[[inputs.http_listener_v2]]\n  ## Address to host HTTP listener on\n  ## can be prefixed by protocol tcp, or unix if not provided defaults to tcp\n  ## if unix network type provided it should be followed by absolute path for unix socket\n  service_address = \"tcp://:8080\"\n  ## service_address = \"tcp://:8443\"\n  ## service_address = \"unix:///tmp/telegraf.sock\"\n\n  ## Permission for unix sockets (only available for unix sockets)\n  ## This setting may not be respected by some platforms. To safely restrict\n  ## permissions it is recommended to place the socket into a previously\n  ## created directory with the desired permissions.\n  ##   ex: socket_mode = \"777\"\n  # socket_mode = \"\"\n\n  ## Paths to listen to.\n  # paths = [\"/telegraf\"]\n\n  ## Save path as http_listener_v2_path tag if set to true\n  # path_tag = false\n\n  ## HTTP methods to accept.\n  # methods = [\"POST\", \"PUT\"]\n\n  ## Optional HTTP headers\n  ## These headers are applied to the server that is listening for HTTP\n  ## requests and included in responses.\n  # http_headers = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n\n  ## HTTP Return Success Code\n  ## This is the HTTP code that will be returned on success\n  # http_success_code = 204\n\n  ## maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Part of the request to consume.  Available options are \"body\" and\n  ## \"query\".\n  # data_source = \"body\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Minimal TLS version accepted by the server\n  # tls_min_version = \"TLS12\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n  ## Optional setting to map http headers into tags\n  ## If the http header is not present on the request, no corresponding tag will be added\n  ## If multiple instances of the http header are present, only the first value will be used\n  # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Output Plugin in Telegraf (TOML)\nDESCRIPTION: This snippet provides a comprehensive TOML configuration for the HTTP Output Plugin in Telegraf. It includes settings for URL, timeout, authentication methods, TLS, data formatting, and various HTTP-specific options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/http/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin that can transmit metrics over HTTP\n[[outputs.http]]\n  ## URL is the address to send metrics to\n  url = \"http://127.0.0.1:8080/telegraf\"\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP method, one of: \"POST\" or \"PUT\" or \"PATCH\"\n  # method = \"POST\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## OAuth2 Client Credentials Grant\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # audience = \"\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n\n  ## Goole API Auth\n  # google_application_credentials = \"/etc/telegraf/example_secret.json\"\n\n  ## HTTP Proxy support\n  # use_system_proxy = false\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional Cookie authentication\n  # cookie_auth_url = \"https://localhost/authMe\"\n  # cookie_auth_method = \"POST\"\n  # cookie_auth_username = \"username\"\n  # cookie_auth_password = \"pa$$word\"\n  # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n  # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n  ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n  # cookie_auth_renewal = \"5m\"\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## Use batch serialization format (default) instead of line based format.\n  ## Batch format is more efficient and should be used unless line based\n  ## format is really needed.\n  # use_batch_format = true\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## MaxIdleConns controls the maximum number of idle (keep-alive)\n  ## connections across all hosts. Zero means no limit.\n  # max_idle_conn = 0\n\n  ## MaxIdleConnsPerHost, if non-zero, controls the maximum idle\n  ## (keep-alive) connections to keep per-host. If zero,\n  ## DefaultMaxIdleConnsPerHost is used(2).\n  # max_idle_conn_per_host = 2\n\n  ## Idle (keep-alive) connection timeout.\n  ## Maximum amount of time before idle connection is closed.\n  ## Zero means no limit.\n  # idle_conn_timeout = 0\n\n  ## Amazon Region\n  #region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Amazon Credentials are not built unless the following aws_service\n  ## setting is set to a non-empty string. It may need to match the name of\n  ## the service output to as well\n  #aws_service = \"execute-api\"\n\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Optional list of statuscodes (<200 or >300) upon which requests should not be retried\n  # non_retryable_statuscodes = [409, 413]\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Additional HTTP headers\n  # [outputs.http.headers]\n  #   ## Should be set manually to \"application/json\" for json data_format\n  #   Content-Type = \"text/plain; charset=utf-8\"\n```\n\n----------------------------------------\n\nTITLE: Setting up InfluxData Repository and Installing Telegraf on Debian-based Systems\nDESCRIPTION: Shell commands to add the InfluxData repository GPG key, create a new sources list entry, and install Telegraf on Debian-based systems like Ubuntu. Includes verification of the GPG key fingerprint.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INSTALL_GUIDE.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# influxdata-archive_compat.key GPG fingerprint:\n#     9D53 9D90 D332 8DC7 D6C8 D3B9 D8FF 8E1F 7DF8 B07E\nwget -q https://repos.influxdata.com/influxdata-archive_compat.key\necho '393e8779c89ac8d958f81f942f9ad7fb82a25e133faddaf92e15b16e6ac9ce4c influxdata-archive_compat.key' | sha256sum -c && cat influxdata-archive_compat.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/influxdata-archive_compat.gpg > /dev/null\necho 'deb [signed-by=/etc/apt/trusted.gpg.d/influxdata-archive_compat.gpg] https://repos.influxdata.com/debian stable main' | sudo tee /etc/apt/sources.list.d/influxdata.list\nsudo apt-get update && sudo apt-get install telegraf\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the HTTP input plugin for Telegraf. It includes options for specifying URLs, HTTP methods, headers, authentication, TLS settings, and data formatting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/http/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read formatted metrics from one or more HTTP endpoints\n[[inputs.http]]\n  ## One or more URLs from which to read formatted metrics.\n  urls = [\n    \"http://localhost/metrics\",\n    \"http+unix:///run/user/420/podman/podman.sock:/d/v4.0.0/libpod/pods/json\"\n  ]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP entity-body to send with POST/PUT requests.\n  # body = \"\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Optional Bearer token settings to use for the API calls.\n  ## Use either the token itself or the token file if you need a token.\n  # token = \"eyJhbGc...Qssw5c\"\n  # token_file = \"/path/to/file\"\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## OAuth2 Client Credentials. The options 'client_id', 'client_secret', and 'token_url' are required to use OAuth2.\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n\n  ## HTTP Proxy support\n  # use_system_proxy = false\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable =\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Password for the key file if it is encrypted\n  # tls_key_pwd = \"\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Minimal TLS version to accept by the client\n  # tls_min_version = \"TLS12\"\n  ## List of ciphers to accept, by default all secure ciphers will be accepted\n  ## See https://pkg.go.dev/crypto/tls#pkg-constants for supported values.\n  ## Use \"all\", \"secure\" and \"insecure\" to add all support ciphers, secure\n  ## suites or insecure suites respectively.\n  # tls_cipher_suites = [\"secure\"]\n  ## Renegotiation method, \"never\", \"once\" or \"freely\"\n  # tls_renegotiation_method = \"never\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional Cookie authentication\n  # cookie_auth_url = \"https://localhost/authMe\"\n  # cookie_auth_method = \"POST\"\n  # cookie_auth_username = \"username\"\n  # cookie_auth_password = \"pa$$word\"\n  # cookie_auth_headers = { Content-Type = \"application/json\", X-MY-HEADER = \"hello\" }\n  # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n  ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n  # cookie_auth_renewal = \"5m\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## List of success status codes\n  # success_status_codes = [200]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the MySQL input plugin that defines server connections, metric output format, and controls which specific MySQL metrics to collect. The configuration includes options for gathering statistics from performance schema, replication status, global variables, and other MySQL components.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mysql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or many mysql servers\n[[inputs.mysql]]\n  ## specify servers via a url matching:\n  ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n  ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n  ##  e.g.\n  ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n  ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n  #\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"tcp(127.0.0.1:3306)/\"]\n\n  ## Selects the metric output format.\n  ##\n  ## This option exists to maintain backwards compatibility, if you have\n  ## existing metrics do not set or change this value until you are ready to\n  ## migrate to the new format.\n  ##\n  ## If you do not have existing metrics from this plugin set to the latest\n  ## version.\n  ##\n  ## Telegraf >=1.6: metric_version = 2\n  ##           <1.6: metric_version = 1 (or unset)\n  metric_version = 2\n\n  ## if the list is empty, then metrics are gathered from all database tables\n  # table_schema_databases = []\n\n  ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided\n  ## in the list above\n  # gather_table_schema = false\n\n  ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n  # gather_process_list = false\n\n  ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n  # gather_user_statistics = false\n\n  ## gather auto_increment columns and max values from information schema\n  # gather_info_schema_auto_inc = false\n\n  ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n  # gather_innodb_metrics = false\n\n  ## gather metrics from all channels from SHOW SLAVE STATUS command output\n  # gather_all_slave_channels = false\n\n  ## gather metrics from SHOW SLAVE STATUS command output\n  # gather_slave_status = false\n\n  ## gather metrics from SHOW REPLICA STATUS command output\n  # gather_replica_status = false\n\n  ## use SHOW ALL SLAVES STATUS command output for MariaDB\n  ## use SHOW ALL REPLICAS STATUS command if enable gather replica status\n  # mariadb_dialect = false\n\n  ## gather metrics from SHOW BINARY LOGS command output\n  # gather_binary_logs = false\n\n  ## gather metrics from SHOW GLOBAL VARIABLES command output\n  # gather_global_variables = true\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n  # gather_table_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n  # gather_table_lock_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n  # gather_index_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n  # gather_event_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n  # gather_file_events_stats = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n  # gather_perf_events_statements             = false\n  #\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_ACCOUNT_BY_EVENT_NAME\n  # gather_perf_sum_per_acc_per_event         = false\n  #\n  ## list of events to be gathered for gather_perf_sum_per_acc_per_event\n  ## in case of empty list all events will be gathered\n  # perf_summary_events                       = []\n\n  ## the limits for metrics form perf_events_statements\n  # perf_events_statements_digest_text_limit = 120\n  # perf_events_statements_limit = 250\n  # perf_events_statements_time_limit = 86400\n\n  ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n  ##   example: interval_slow = \"30m\"\n  # interval_slow = \"\"\n\n  ## Optional TLS Config (used if tls=custom parameter specified in server uri)\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Generating Basic Telegraf Configuration File\nDESCRIPTION: Command to generate a default Telegraf configuration file and write it to telegraf.conf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf config > telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: Implementing a Standard Processor Plugin in Telegraf\nDESCRIPTION: Complete implementation of a basic 'printer' processor plugin that conforms to the telegraf.Processor interface. This example shows how to structure a processor that processes metrics sequentially.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROCESSORS.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n//go:generate ../../../tools/readme_config_includer/generator\npackage printer\n\nimport (\n    _ \"embed\"\n    \"fmt\"\n\n    \"github.com/influxdata/telegraf\"\n    \"github.com/influxdata/telegraf/plugins/processors\"\n)\n\n//go:embed sample.conf\nvar sampleConfig string\n\ntype Printer struct {\n    Log telegraf.Logger `toml:\"-\"`\n}\n\nfunc (*Printer) SampleConfig() string {\n    return sampleConfig\n}\n\n// Init is for setup, and validating config.\nfunc (p *Printer) Init() error {\n    return nil\n}\n\nfunc (p *Printer) Apply(in ...telegraf.Metric) []telegraf.Metric {\n    for _, metric := range in {\n        fmt.Println(metric.String())\n    }\n    return in\n}\n\nfunc init() {\n    processors.Add(\"printer\", func() telegraf.Processor {\n        return &Printer{}\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Routing Metrics to Different InfluxDB Outputs in Telegraf\nDESCRIPTION: This configuration demonstrates how to route metrics to different InfluxDB outputs based on metric names and tags. It separates aerospike and CPU-specific data into different databases.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_25\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf\"\n  # Drop all measurements that start with \"aerospike\"\n  namedrop = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-aerospike-data\"\n  # Only accept aerospike data:\n  namepass = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-cpu0-data\"\n  # Only store measurements where the tag \"cpu\" matches the value \"cpu0\"\n  [outputs.influxdb.tagpass]\n    cpu = [\"cpu0\"]\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for InfluxDB OSS 2 Configuration\nDESCRIPTION: Shell script defining environment variables for Telegraf to connect to InfluxDB OSS 2, including host, token, organization, and bucket.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nINFLUX_HOST=\"http://localhost:8086\" # used to be 9999\nINFLUX_TOKEN=\"replace_with_your_token\"\nINFLUX_ORG=\"your_username\"\nINFLUX_BUCKET=\"replace_with_your_bucket_name\"\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes Metrics Output\nDESCRIPTION: Sample output showing the format of metrics collected by the plugin including node, container, volume, and network metrics with their respective tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kubernetes/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nkubernetes_node\nkubernetes_pod_container,container_name=deis-controller,namespace=deis,node_name=ip-10-0-0-0.ec2.internal,pod_name=deis-controller-3058870187-xazsr cpu_usage_core_nanoseconds=2432835i,cpu_usage_nanocores=0i,logsfs_available_bytes=121128271872i,logsfs_capacity_bytes=153567944704i,logsfs_used_bytes=20787200i,memory_major_page_faults=0i,memory_page_faults=175i,memory_rss_bytes=0i,memory_usage_bytes=0i,memory_working_set_bytes=0i,rootfs_available_bytes=121128271872i,rootfs_capacity_bytes=153567944704i,rootfs_used_bytes=1110016i 1476477530000000000\nkubernetes_pod_network,namespace=deis,node_name=ip-10-0-0-0.ec2.internal,pod_name=deis-controller-3058870187-xazsr rx_bytes=120671099i,rx_errors=0i,tx_bytes=102451983i,tx_errors=0i 1476477530000000000\nkubernetes_pod_volume,volume_name=default-token-f7wts,namespace=default,node_name=ip-172-17-0-1.internal,pod_name=storage-7 available_bytes=8415240192i,capacity_bytes=8415252480i,used_bytes=12288i 1546910783000000000\nkubernetes_system_container\n```\n\n----------------------------------------\n\nTITLE: Routing Metrics to Different Outputs Based on Input Tags in Telegraf\nDESCRIPTION: This configuration shows how to route metrics to different InfluxDB outputs based on tags set in the input plugin. It uses the 'influxdb_database' tag to determine the output and removes the tag before writing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_20\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.influxdb]]\n  urls = [\"http://influxdb.example.com\"]\n  database = \"db_default\"\n  [outputs.influxdb.tagdrop]\n    influxdb_database = [\"*\"]\n\n[[outputs.influxdb]]\n  urls = [\"http://influxdb.example.com\"]\n  database = \"db_other\"\n  tagexclude = [\"influxdb_database\"]\n  [outputs.influxdb.tagpass]\n    influxdb_database = [\"other\"]\n\n[[inputs.disk]]\n  [inputs.disk.tags]\n    influxdb_database = \"other\"\n```\n\n----------------------------------------\n\nTITLE: Filtering Tags with taginclude and tagexclude in Telegraf\nDESCRIPTION: This example shows how to use taginclude and tagexclude to filter specific tags from CPU and disk metrics. It includes only the 'cpu' tag for CPU metrics and excludes the 'fstype' tag from disk metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_24\n\nLANGUAGE: toml\nCODE:\n```\n# Only include the \"cpu\" tag in the measurements for the cpu plugin.\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  taginclude = [\"cpu\"]\n\n# Exclude the \"fstype\" tag from the measurements for the disk plugin.\n[[inputs.disk]]\n  tagexclude = [\"fstype\"]\n```\n\n----------------------------------------\n\nTITLE: Example Prometheus Metrics Source Format\nDESCRIPTION: Sample Prometheus metrics output showing various metric types including summary, gauge, and custom metrics with labels. This demonstrates the source format that Telegraf will parse.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/prometheus/README.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# HELP go_gc_duration_seconds A summary of the GC invocation durations.\n# TYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 7.4545e-05\ngo_gc_duration_seconds{quantile=\"0.25\"} 7.6999e-05\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000277935\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000706591\ngo_gc_duration_seconds{quantile=\"1\"} 0.000706591\ngo_gc_duration_seconds_sum 0.00113607\ngo_gc_duration_seconds_count 4\n# HELP go_goroutines Number of goroutines that currently exist.\n# TYPE go_goroutines gauge\ngo_goroutines 15\n# HELP cpu_usage_user Telegraf collected metric\n# TYPE cpu_usage_user gauge\ncpu_usage_user{cpu=\"cpu0\"} 1.4112903225816156\ncpu_usage_user{cpu=\"cpu1\"} 0.702106318955865\ncpu_usage_user{cpu=\"cpu2\"} 2.0161290322588776\ncpu_usage_user{cpu=\"cpu3\"} 1.5045135406226022\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Extension Setup\nDESCRIPTION: SQL commands for creating required PostgreSQL extensions in the monitoring database.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate extension pg_stat_statements;\ncreate extension pg_stat_kcache;\ncreate extension pg_proctab;\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Input Plugin in Telegraf (TOML)\nDESCRIPTION: This snippet shows the complete configuration options for the Docker input plugin in Telegraf. It includes settings for endpoint configuration, service gathering, container filtering, tag customization, and various collection options for metrics related to containers, images, and volumes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/docker/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about docker containers\n[[inputs.docker]]\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  endpoint = \"unix:///var/run/docker.sock\"\n\n  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n  ## Note: configure this in one of the manager nodes in a Swarm cluster.\n  ## configuring in multiple Swarm managers results in duplication of metrics.\n  gather_services = false\n\n  ## Only collect metrics for these containers. Values will be appended to\n  ## container_name_include.\n  ## Deprecated (1.4.0), use container_name_include\n  container_names = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Containers to include and exclude. Collect all if empty. Globs accepted.\n  container_name_include = []\n  container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## Objects to include for disk usage query\n  ## Allowed values are \"container\", \"image\", \"volume\" \n  ## When empty disk usage is excluded\n  storage_objects = []\n\n  ## Timeout for docker list, info, and stats commands\n  timeout = \"5s\"\n\n  ## Specifies for which classes a per-device metric should be issued\n  ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)\n  ## Please note that this setting has no effect if 'perdevice' is set to 'true'\n  # perdevice_include = [\"cpu\"]\n\n  ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.\n  ## Possible values are 'cpu', 'blkio' and 'network'\n  ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.\n  ## Please note that this setting has no effect if 'total' is set to 'false'\n  # total_include = [\"cpu\", \"blkio\", \"network\"]\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  docker_label_include = []\n  docker_label_exclude = []\n\n  ## Which environment variables should we use as a tag\n  tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Output Plugin for Telegraf\nDESCRIPTION: Complete example of a simple output plugin implementation for Telegraf. The plugin demonstrates conforming to the telegraf.Output interface including config handling, connection management, and metric writing functionality.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/OUTPUTS.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n//go:generate ../../../tools/readme_config_includer/generator\npackage simpleoutput\n\n// simpleoutput.go\n\nimport (\n    _ \"embed\"\n\n    \"github.com/influxdata/telegraf\"\n    \"github.com/influxdata/telegraf/plugins/outputs\"\n)\n\n//go:embed sample.conf\nvar sampleConfig string\n\ntype Simple struct {\n    Ok  bool            `toml:\"ok\"`\n    Log telegraf.Logger `toml:\"-\"`\n}\n\nfunc (*Simple) SampleConfig() string {\n    return sampleConfig\n}\n\n// Init is for setup, and validating config.\nfunc (s *Simple) Init() error {\n    return nil\n}\n\nfunc (s *Simple) Connect() error {\n    // Make any connection required here\n    return nil\n}\n\nfunc (s *Simple) Close() error {\n    // Close any connections here.\n    // Write will not be called once Close is called, so there is no need to synchronize.\n    return nil\n}\n\n// Write should write immediately to the output, and not buffer writes\n// (Telegraf manages the buffer for you). Returning an error will fail this\n// batch of writes and the entire batch will be retried automatically.\nfunc (s *Simple) Write(metrics []telegraf.Metric) error {\n    for _, metric := range metrics {\n        // write `metric` to the output sink here\n    }\n    return nil\n}\n\nfunc init() {\n    outputs.Add(\"simpleoutput\", func() telegraf.Output { return &Simple{} })\n}\n```\n\n----------------------------------------\n\nTITLE: Example Docker Metrics Output in Telegraf\nDESCRIPTION: Sample output showing various Docker metrics collected by Telegraf, including container memory, CPU, network, block I/O, health status, swarm statistics, and disk usage measurements. The output is in InfluxDB line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/docker/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndocker,engine_host=debian-stretch-docker,server_version=17.09.0-ce n_containers=6i,n_containers_paused=0i,n_containers_running=1i,n_containers_stopped=5i,n_cpus=2i,n_goroutines=41i,n_images=2i,n_listener_events=0i,n_used_file_descriptors=27i 1524002041000000000\ndocker,engine_host=debian-stretch-docker,server_version=17.09.0-ce,unit=bytes memory_total=2101661696i 1524002041000000000\ndocker_container_mem,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,engine_host=debian-stretch-docker,server_version=17.09.0-ce active_anon=8327168i,active_file=2314240i,cache=27402240i,container_id=\"adc4ba9593871bf2ab95f3ffde70d1b638b897bb225d21c2c9c84226a10a8cf4\",hierarchical_memory_limit=9223372036854771712i,inactive_anon=0i,inactive_file=25088000i,limit=2101661696i,mapped_file=20582400i,max_usage=36646912i,pgfault=4193i,pgmajfault=214i,pgpgin=9243i,pgpgout=520i,rss=8327168i,rss_huge=0i,total_active_anon=8327168i,total_active_file=2314240i,total_cache=27402240i,total_inactive_anon=0i,total_inactive_file=25088000i,total_mapped_file=20582400i,total_pgfault=4193i,total_pgmajfault=214i,total_pgpgin=9243i,total_pgpgout=520i,total_rss=8327168i,total_rss_huge=0i,total_unevictable=0i,total_writeback=0i,unevictable=0i,usage=36528128i,usage_percent=0.4342225020025297,writeback=0i 1524002042000000000\ndocker_container_cpu,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,cpu=cpu-total,engine_host=debian-stretch-docker,server_version=17.09.0-ce container_id=\"adc4ba9593871bf2ab95f3ffde70d1b638b897bb225d21c2c9c84226a10a8cf4\",throttling_periods=0i,throttling_throttled_periods=0i,throttling_throttled_time=0i,usage_in_kernelmode=40000000i,usage_in_usermode=100000000i,usage_percent=0,usage_system=6394210000000i,usage_total=117319068i 1524002042000000000\ndocker_container_cpu,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,cpu=cpu0,engine_host=debian-stretch-docker,server_version=17.09.0-ce container_id=\"adc4ba9593871bf2ab95f3ffde70d1b638b897bb225d21c2c9c84226a10a8cf4\",usage_total=20825265i 1524002042000000000\ndocker_container_cpu,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,cpu=cpu1,engine_host=debian-stretch-docker,server_version=17.09.0-ce container_id=\"adc4ba9593871bf2ab95f3ffde70d1b638b897bb225d21c2c9c84226a10a8cf4\",usage_total=96493803i 1524002042000000000\ndocker_container_net,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,engine_host=debian-stretch-docker,network=eth0,server_version=17.09.0-ce container_id=\"adc4ba9593871bf2ab95f3ffde70d1b638b897bb225d21c2c9c84226a10a8cf4\",rx_bytes=1576i,rx_dropped=0i,rx_errors=0i,rx_packets=20i,tx_bytes=0i,tx_dropped=0i,tx_errors=0i,tx_packets=0i 1524002042000000000\ndocker_container_blkio,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,device=254:0,engine_host=debian-stretch-docker,server_version=17.09.0-ce container_id=\"adc4ba9593871bf2ab95f3ffde70d1b638b897bb225d21c2c9c84226a10a8cf4\",io_service_bytes_recursive_async=27398144i,io_service_bytes_recursive_read=27398144i,io_service_bytes_recursive_sync=0i,io_service_bytes_recursive_total=27398144i,io_service_bytes_recursive_write=0i,io_serviced_recursive_async=529i,io_serviced_recursive_read=529i,io_serviced_recursive_sync=0i,io_serviced_recursive_total=529i,io_serviced_recursive_write=0i 1524002042000000000\ndocker_container_health,container_image=telegraf,container_name=zen_ritchie,container_status=running,container_version=unknown,engine_host=debian-stretch-docker,server_version=17.09.0-ce failing_streak=0i,health_status=\"healthy\" 1524007529000000000\ndocker_swarm,service_id=xaup2o9krw36j2dy1mjx1arjw,service_mode=replicated,service_name=test tasks_desired=3,tasks_running=3 1508968160000000000\ndocker_disk_usage,engine_host=docker-desktop,server_version=24.0.5 layers_size=17654519107i 1695742041000000000\ndocker_disk_usage,container_image=influxdb,container_name=frosty_wright,container_version=1.8,engine_host=docker-desktop,server_version=24.0.5 size_root_fs=286593526i,size_rw=538i 1695742041000000000\ndocker_disk_usage,engine_host=docker-desktop,image_id=7f4a1cc74046,image_name=telegraf,image_version=latest,server_version=24.0.5 shared_size=0i,size=425484494i 1695742041000000000\ndocker_disk_usage,engine_host=docker-desktop,server_version=24.0.5,volume_name=docker_influxdb-data size=91989940i 1695742041000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring CloudWatch Logs Output in Telegraf using TOML\nDESCRIPTION: Complete configuration example for setting up the AWS CloudWatch Logs output plugin in Telegraf. Includes region selection, authentication options, log group/stream settings, and log data source configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/cloudwatch_logs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for AWS CloudWatchLogs output.\n[[outputs.cloudwatch_logs]]\n  ## The region is the Amazon region that you wish to connect to.\n  ## Examples include but are not limited to:\n  ## - us-west-1\n  ## - us-west-2\n  ## - us-east-1\n  ## - ap-southeast-1\n  ## - ap-southeast-2\n  ## ...\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and\n  ##    web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default, e.g endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront!\n  ## For example, you can specify the name of the k8s cluster here to group logs\n  ## from all cluster in oine place\n  log_group = \"my-group-name\"\n\n  ## Log stream in log group\n  ## Either log group name or reference to metric attribute, from which it can\n  ## be parsed, tag:<TAG_NAME> or field:<FIELD_NAME>. If the log stream is not\n  ## exist, it will be created. Since AWS is not automatically delete logs\n  ## streams with expired logs entries (i.e. empty log stream) you need to put\n  ## in place appropriate house-keeping (https://forums.aws.amazon.com/thread.jspa?threadID=178855)\n  log_stream = \"tag:location\"\n\n  ## Source of log data - metric name\n  ## specify the name of the metric, from which the log data should be\n  ## retrieved. I.e., if you are using docker_log plugin to stream logs from\n  ## container, then specify log_data_metric_name = \"docker_log\"\n  log_data_metric_name  = \"docker_log\"\n\n  ## Specify from which metric attribute the log data should be retrieved:\n  ## tag:<TAG_NAME> or field:<FIELD_NAME>.\n  ## I.e., if you are using docker_log plugin to stream logs from container,\n  ## then specify log_data_source = \"field:message\"\n  log_data_source  = \"field:message\"\n```\n\n----------------------------------------\n\nTITLE: Resolved Telegraf Configuration After Environment Variable Substitution\nDESCRIPTION: The effective TOML configuration after environment variables have been substituted, showing resolved values for InfluxDB 1.x, OSS 2, and Cloud 2 outputs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[global_tags]\n  user = \"alice\"\n\n[[inputs.mem]]\n\n# For InfluxDB 1.x:\n[[outputs.influxdb]]\n  urls = \"http://localhost:8086\"\n  skip_database_creation = true\n  password = \"monkey123\"\n\n# For InfluxDB OSS 2:\n[[outputs.influxdb_v2]]\n  urls = [\"http://127.0.0.1:8086\"] # double check the port. could be 9999 if using OSS Beta\n  token = \"replace_with_your_token\"\n  organization = \"your_username\"\n  bucket = \"replace_with_your_bucket_name\"\n\n# For InfluxDB Cloud 2:\n[[outputs.influxdb_v2]]\n  # For AWS West (Oregon)\n  INFLUX_HOST=\"https://us-west-2-1.aws.cloud2.influxdata.com\"\n  # Other Cloud URLs at https://v2.docs.influxdata.com/v2.0/reference/urls/#influxdb-cloud-urls\n  token = \"replace_with_your_token\"\n  organization = \"yourname@yourcompany.com\"\n  bucket = \"replace_with_your_bucket_name\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Output Plugin in TOML\nDESCRIPTION: Complete configuration options for the OpenSearch output plugin including URL settings, authentication, TLS configuration, template management, and document handling parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opensearch/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.opensearch]]\n  urls = [\"http://node1.os.example.com:9200\"]\n  index_name = \"\"\n  # timeout = \"5s\"\n  # enable_sniffer = false\n  # enable_gzip = false\n  # health_check_interval = \"10s\"\n  # health_check_timeout = \"1s\"\n  # username = \"\"\n  # password = \"\"\n  # auth_bearer_token = \"\"\n  # tls_enable =\n  # tls_ca = \"/path/to/cafile\"\n  # tls_cert = \"/path/to/certfile\"\n  # tls_key = \"/path/to/keyfile\"\n  # tls_server_name = \"kubernetes.example.com\"\n  # insecure_skip_verify = false\n  # manage_template = true\n  # template_name = \"telegraf\"\n  # overwrite_template = false\n  # force_document_id = false\n  # float_handling = \"none\"\n  # float_replacement_value = 0.0\n  # use_pipeline = \"my_pipeline\"\n  # default_pipeline = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple CPU Input Plugin Instances\nDESCRIPTION: Example showing how to configure multiple CPU input plugin instances with different settings to avoid measurement collisions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  name_override = \"percpu_usage\"\n  fieldexclude = [\"cpu_time*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the SQL Input Plugin for Telegraf. It specifies the database driver, connection string, timeout settings, connection limits, and query details including measurement naming, time formatting, and column mappings for tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from SQL queries\n[[inputs.sql]]\n  ## Database Driver\n  ## See https://github.com/influxdata/telegraf/blob/master/docs/SQL_DRIVERS_INPUT.md for\n  ## a list of supported drivers.\n  driver = \"mysql\"\n\n  ## Data source name for connecting\n  ## The syntax and supported options depends on selected driver.\n  dsn = \"username:password@tcp(mysqlserver:3307)/dbname?param=value\"\n\n  ## Timeout for any operation\n  ## Note that the timeout for queries is per query not per gather.\n  # timeout = \"5s\"\n\n  ## Connection time limits\n  ## By default the maximum idle time and maximum lifetime of a connection is unlimited, i.e. the connections\n  ## will not be closed automatically. If you specify a positive time, the connections will be closed after\n  ## idleing or existing for at least that amount of time, respectively.\n  # connection_max_idle_time = \"0s\"\n  # connection_max_life_time = \"0s\"\n\n  ## Connection count limits\n  ## By default the number of open connections is not limited and the number of maximum idle connections\n  ## will be inferred from the number of queries specified. If you specify a positive number for any of the\n  ## two options, connections will be closed when reaching the specified limit. The number of idle connections\n  ## will be clipped to the maximum number of connections limit if any.\n  # connection_max_open = 0\n  # connection_max_idle = auto\n\n  ## Specifies plugin behavior regarding disconnected servers\n  ## Available choices :\n  ##   - error: telegraf will return an error on startup if one the servers is unreachable\n  ##   - ignore: telegraf will ignore unreachable servers on both startup and gather\n  # disconnected_servers_behavior = \"error\"\n\n  [[inputs.sql.query]]\n    ## Query to perform on the server\n    query=\"SELECT user,state,latency,score FROM Scoreboard WHERE application > 0\"\n    ## Alternatively to specifying the query directly you can select a file here containing the SQL query.\n    ## Only one of 'query' and 'query_script' can be specified!\n    # query_script = \"/path/to/sql/script.sql\"\n\n    ## Name of the measurement\n    ## In case both measurement and 'measurement_col' are given, the latter takes precedence.\n    # measurement = \"sql\"\n\n    ## Column name containing the name of the measurement\n    ## If given, this will take precedence over the 'measurement' setting. In case a query result\n    ## does not contain the specified column, we fall-back to the 'measurement' setting.\n    # measurement_column = \"\"\n\n    ## Column name containing the time of the measurement\n    ## If omitted, the time of the query will be used.\n    # time_column = \"\"\n\n    ## Format of the time contained in 'time_col'\n    ## The time must be 'unix', 'unix_ms', 'unix_us', 'unix_ns', or a golang time format.\n    ## See https://golang.org/pkg/time/#Time.Format for details.\n    # time_format = \"unix\"\n\n    ## Column names containing tags\n    ## An empty include list will reject all columns and an empty exclude list will not exclude any column.\n    ## I.e. by default no columns will be returned as tag and the tags are empty.\n    # tag_columns_include = []\n    # tag_columns_exclude = []\n\n    ## Column names containing fields (explicit types)\n    ## Convert the given columns to the corresponding type. Explicit type conversions take precedence over\n    ## the automatic (driver-based) conversion below.\n    ## NOTE: Columns should not be specified for multiple types or the resulting type is undefined.\n    # field_columns_float = []\n    # field_columns_int = []\n    # field_columns_uint = []\n    # field_columns_bool = []\n    # field_columns_string = []\n\n    ## Column names containing fields (automatic types)\n    ## An empty include list is equivalent to '[*]' and all returned columns will be accepted. An empty\n    ## exclude list will not exclude any column. I.e. by default all columns will be returned as fields.\n    ## NOTE: We rely on the database driver to perform automatic datatype conversion.\n    # field_columns_include = []\n    # field_columns_exclude = []\n```\n\n----------------------------------------\n\nTITLE: Visual Studio Code Debug Configuration for Telegraf\nDESCRIPTION: VSCode launch.json configuration for debugging Telegraf. This setup enables the Go debugger extension to launch and debug Telegraf with a specified configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEBUG.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Launch Package\",\n            \"type\": \"go\",\n            \"request\": \"launch\",\n            \"mode\": \"auto\",\n            \"program\": \"${fileDirname}\",\n            \"args\": [\"--config\", \"/path/to/config\"]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Output of PostgreSQL Metrics in Telegraf\nDESCRIPTION: Example of the metrics output from the PostgreSQL input plugin, showing various database statistics and performance metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\npostgresql,db=postgres_global,server=dbname\\=postgres\\ host\\=localhost\\ port\\=5432\\ statement_timeout\\=10000\\ user\\=postgres tup_fetched=1271i,tup_updated=5i,session_time=1451414320768.855,xact_rollback=2i,conflicts=0i,blk_write_time=0,temp_bytes=0i,datid=0i,sessions_fatal=0i,tup_returned=1339i,sessions_abandoned=0i,blk_read_time=0,blks_read=88i,idle_in_transaction_time=0,sessions=0i,active_time=0,tup_inserted=24i,tup_deleted=0i,temp_files=0i,numbackends=0i,xact_commit=4i,sessions_killed=0i,blks_hit=5616i,deadlocks=0i 1672399790000000000\npostgresql,db=postgres,host=oss_cluster_host,server=dbname\\=postgres\\ host\\=localhost\\ port\\=5432\\ statement_timeout\\=10000\\ user\\=postgres conflicts=0i,sessions_abandoned=2i,active_time=460340.823,tup_returned=119382i,tup_deleted=0i,blk_write_time=0,xact_commit=305i,blks_hit=16358i,deadlocks=0i,sessions=12i,numbackends=1i,temp_files=0i,xact_rollback=5i,sessions_fatal=0i,datname=\"postgres\",blk_read_time=0,idle_in_transaction_time=0,temp_bytes=0i,tup_inserted=3i,tup_updated=0i,blks_read=299i,datid=5i,session_time=469056.613,sessions_killed=0i,tup_fetched=5550i 1672399790000000000\npostgresql,db=template1,host=oss_cluster_host,server=dbname\\=postgres\\ host\\=localhost\\ port\\=5432\\ statement_timeout\\=10000\\ user\\=postgres active_time=0,idle_in_transaction_time=0,blks_read=1352i,sessions_abandoned=0i,tup_fetched=28544i,session_time=0,sessions_killed=0i,temp_bytes=0i,tup_returned=188541i,xact_commit=1168i,blk_read_time=0,sessions_fatal=0i,datid=1i,datname=\"template1\",conflicts=0i,xact_rollback=0i,numbackends=0i,deadlocks=0i,sessions=0i,tup_inserted=17520i,temp_files=0i,tup_updated=743i,blk_write_time=0,blks_hit=99487i,tup_deleted=34i 1672399790000000000\npostgresql,db=template0,host=oss_cluster_host,server=dbname\\=postgres\\ host\\=localhost\\ port\\=5432\\ statement_timeout\\=10000\\ user\\=postgres sessions=0i,datid=4i,tup_updated=0i,sessions_abandoned=0i,blk_write_time=0,numbackends=0i,blks_read=0i,blks_hit=0i,sessions_fatal=0i,temp_files=0i,deadlocks=0i,conflicts=0i,xact_commit=0i,xact_rollback=0i,session_time=0,datname=\"template0\",tup_returned=0i,tup_inserted=0i,idle_in_transaction_time=0,tup_fetched=0i,active_time=0,temp_bytes=0i,tup_deleted=0i,blk_read_time=0,sessions_killed=0i 1672399790000000000\npostgresql,db=postgres,host=oss_cluster_host,server=dbname\\=postgres\\ host\\=localhost\\ port\\=5432\\ statement_timeout\\=10000\\ user\\=postgres buffers_clean=0i,buffers_alloc=426i,checkpoints_req=1i,buffers_checkpoint=50i,buffers_backend_fsync=0i,checkpoint_write_time=5053,checkpoints_timed=26i,checkpoint_sync_time=26,maxwritten_clean=0i,buffers_backend=9i 1672399790000000000\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Telegraf with Config File\nDESCRIPTION: Shows how to run Telegraf by specifying a configuration file using the --config flag. This is the minimal required command to run Telegraf with custom configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/cmd/telegraf/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf --config config.toml\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Output Plugin in TOML\nDESCRIPTION: Complete configuration example for the InfluxDB output plugin showing all available options including connection URLs, authentication, database settings, retention policies, TLS configuration, and HTTP options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/influxdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb]]\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## Local address to bind when connecting to the server\n  ## If empty or not set, the local address is automatically chosen.\n  # local_address = \"\"\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the 'database_tag' will not be included in the written metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## The value of this tag will be used to determine the retention policy.  If this\n  ## tag is not set the 'retention_policy' option is used as the default.\n  # retention_policy_tag = \"\"\n\n  ## If true, the 'retention_policy_tag' will not be included in the written metric.\n  # exclude_retention_policy_tag = false\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n  ## When true, Telegraf will omit the timestamp on data to allow InfluxDB\n  ## to set the timestamp of the data during ingestion. This is generally NOT\n  ## what you want as it can lead to data points captured at different times\n  ## getting omitted due to similar data.\n  # influx_omit_timestamp = false\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Telegraf\nDESCRIPTION: Example of setting environment variables for Telegraf configuration, including InfluxDB connection details for different versions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nUSER=\"alice\"\nINFLUX_URL=\"http://localhost:8086\"\nINFLUX_SKIP_DATABASE_CREATION=\"true\"\nINFLUX_PASSWORD=\"monkey123\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nINFLUX_HOST=\"http://localhost:8086\" # used to be 9999\nINFLUX_TOKEN=\"replace_with_your_token\"\nINFLUX_ORG=\"your_username\"\nINFLUX_BUCKET=\"replace_with_your_bucket_name\"\n```\n\nLANGUAGE: shell\nCODE:\n```\n# For AWS West (Oregon)\nINFLUX_HOST=\"https://us-west-2-1.aws.cloud2.influxdata.com\"\n# Other Cloud URLs at https://v2.docs.influxdata.com/v2.0/reference/urls/#influxdb-cloud-urls\nINFLUX_TOKEN=\"replace_with_your_token\"\nINFLUX_ORG=\"yourname@yourcompany.com\"\nINFLUX_BUCKET=\"replace_with_your_bucket_name\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Inventory Plugin in TOML\nDESCRIPTION: Main configuration file for the Kubernetes Inventory Telegraf plugin. Defines connection settings, resource filtering, authentication, and TLS options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.kube_inventory]]\n  ## URL for the Kubernetes API.\n  ## If empty in-cluster config with POD's service account token will be used.\n  # url = \"\"\n\n  ## URL for the kubelet, if set it will be used to collect the pods resource metrics\n  # url_kubelet = \"http://127.0.0.1:10255\"\n\n  ## Namespace to use. Set to \"\" to use all namespaces.\n  # namespace = \"default\"\n\n  ## Node name to filter to. No filtering by default.\n  # node_name = \"\"\n\n  ## Use bearer token for authorization.\n  ## Ignored if url is empty and in-cluster config is used.\n  # bearer_token = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional Resources to exclude from gathering\n  # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## Optional Resources to include when gathering\n  # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## selectors to include and exclude as tags.\n  # selector_include = []\n  # selector_exclude = [\"*\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/path/to/cafile\"\n  # tls_cert = \"/path/to/certfile\"\n  # tls_key = \"/path/to/keyfile\"\n  # tls_server_name = \"kubernetes.example.com\"\n  # insecure_skip_verify = false\n\n  ## Uncomment to remove deprecated metrics.\n  # fieldexclude = [\"terminated_reason\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Input Plugin for Telegraf in TOML\nDESCRIPTION: Sample configuration for the Apache input plugin in Telegraf that collects performance metrics from Apache HTTP Servers. It includes settings for URLs, authentication, timeout, and TLS configuration options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/apache/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Apache status information (mod_status)\n[[inputs.apache]]\n  ## An array of URLs to gather from, must be directed at the machine\n  ## readable version of the mod_status page including the auto query string.\n  ## Default is \"http://localhost/server-status?auto\".\n  urls = [\"http://localhost/server-status?auto\"]\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Implementing a Streaming Processor Plugin in Telegraf\nDESCRIPTION: Complete implementation of a 'printer' streaming processor that conforms to the telegraf.StreamingProcessor interface. This pattern is useful for processors that need to handle metrics concurrently or maintain background processes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROCESSORS.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n//go:generate ../../../tools/readme_config_includer/generator\npackage printer\n\nimport (\n    _ \"embed\"\n    \"fmt\"\n\n    \"github.com/influxdata/telegraf\"\n    \"github.com/influxdata/telegraf/plugins/processors\"\n)\n\n//go:embed sample.conf\nvar sampleConfig string\n\ntype Printer struct {\n    Log telegraf.Logger `toml:\"-\"`\n}\n\nfunc (*Printer) SampleConfig() string {\n    return sampleConfig\n}\n\n// Init is for setup, and validating config.\nfunc (p *Printer) Init() error {\n    return nil\n}\n\n// Start is called once when the plugin starts; it is only called once per\n// plugin instance, and never in parallel.\n// Start should return once it is ready to receive metrics.\n// The passed in accumulator is the same as the one passed to Add(), so you\n// can choose to save it in the plugin, or use the one received from Add().\nfunc (p *Printer) Start(acc telegraf.Accumulator) error {\n}\n\n// Add is called for each metric to be processed. The Add() function does not\n// need to wait for the metric to be processed before returning, and it may\n// be acceptable to let background goroutine(s) handle the processing if you\n// have slow processing you need to do in parallel.\n// Keep in mind Add() should not spawn unbounded goroutines, so you may need\n// to use a semaphore or pool of workers (eg: reverse_dns plugin does this).\n// Metrics you don't want to pass downstream should have metric.Drop() called,\n// rather than simply omitting the acc.AddMetric() call\nfunc (p *Printer) Add(metric telegraf.Metric, acc telegraf.Accumulator) error {\n    // print!\n    fmt.Println(metric.String())\n    // pass the metric downstream, or metric.Drop() it.\n    // Metric will be dropped if this function returns an error.\n    acc.AddMetric(metric)\n\n    return nil\n}\n\n// Stop gives you an opportunity to gracefully shut down the processor.\n// Once Stop() is called, Add() will not be called any more. If you are using\n// goroutines, you should wait for any in-progress metrics to be processed\n// before returning from Stop().\n// When stop returns, you should no longer be writing metrics to the\n// accumulator.\nfunc (p *Printer) Stop() error {\n}\n\nfunc init() {\n    processors.AddStreaming(\"printer\", func() telegraf.StreamingProcessor {\n        return &Printer{}\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Outputs in Telegraf\nDESCRIPTION: This snippet demonstrates how to configure outputs in Telegraf. It includes examples for InfluxDB and Graphite outputs, showing how to specify URLs, databases, retention policies, and write consistency settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/migrations/inputs_jolokia/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb]]\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the 'database_tag' will not be included in the written metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## The value of this tag will be used to determine the retention policy.  If this\n  ## tag is not set the 'retention_policy' option is used as the default.\n  # retention_policy_tag = \"\"\n\n  ## If true, the 'retention_policy_tag' will not be included in the written metric.\n  # exclude_retention_policy_tag = false\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n# # Configuration for Graphite server to send metrics to\n# [[outputs.graphite]]\n#   ## TCP endpoint for your graphite instance.\n#   ## If multiple endpoints are configured, output will be load balanced.\n#   ## Only one of the endpoints will be written to with each iteration.\n#   servers = [\"localhost:2003\"]\n#   ## Prefix metrics name\n#   prefix = \"\"\n#   ## Graphite output template\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   template = \"host.tags.measurement.field\"\n#\n#   ### Enable Graphite tags support\n#   # graphite_tag_support = false\n#\n#   ### Character for separating metric name and field for Graphite tags\n#   # graphite_separator = \".\"\n#\n#   ## timeout in seconds for the write connection to graphite\n#   timeout = 2\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain & host verification\n#   # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring gNMI Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Telegraf gNMI input plugin that consumes telemetry data based on gNMI subscriptions. The configuration includes connection details, authentication, subscription settings, and various optional parameters for TLS, keepalive settings, and vendor-specific options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/gnmi/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# gNMI telemetry input plugin\n[[inputs.gnmi]]\n  ## Address and port of the gNMI GRPC server\n  addresses = [\"10.49.234.114:57777\"]\n\n  ## define credentials\n  username = \"cisco\"\n  password = \"cisco\"\n\n  ## gNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\", \"bytes\")\n  # encoding = \"proto\"\n\n  ## redial in case of failures after\n  # redial = \"10s\"\n\n  ## gRPC Keepalive settings\n  ## See https://pkg.go.dev/google.golang.org/grpc/keepalive\n  ## The client will ping the server to see if the transport is still alive if it has\n  ## not see any activity for the given time.\n  ## If not set, none of the keep-alive setting (including those below) will be applied.\n  ## If set and set below 10 seconds, the gRPC library will apply a minimum value of 10s will be used instead.\n  # keepalive_time = \"\"\n\n  ## Timeout for seeing any activity after the keep-alive probe was\n  ## sent. If no activity is seen the connection is closed.\n  # keepalive_timeout = \"\"\n\n  ## gRPC Maximum Message Size\n  # max_msg_size = \"4MB\"\n\n  ## Subtree depth for depth extension (disables if < 1)\n  ## see https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-depth.md\n  # depth = 0\n\n  ## Enable to get the canonical path as field-name\n  # canonical_field_names = false\n\n  ## Remove leading slashes and dots in field-name\n  # trim_field_names = false\n\n  ## Only receive updates for the state, also suppresses receiving the initial state\n  # updates_only = false\n\n  ## Enforces the namespace of the first element as origin for aliases and\n  ## response paths, required for backward compatibility.\n  ## NOTE: Set to 'false' if possible but be aware that this might change the path tag!\n  # enforce_first_namespace_as_origin = true\n\n  ## Guess the path-tag if an update does not contain a prefix-path\n  ## Supported values are\n  ##   none         -- do not add a 'path' tag\n  ##   common path  -- use the common path elements of all fields in an update\n  ##   subscription -- use the subscription path\n  # path_guessing_strategy = \"none\"\n\n  ## Prefix tags from path keys with the path element\n  # prefix_tag_key_with_path = false\n\n  ## Optional client-side TLS to authenticate the device\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable =\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Password for the key file if it is encrypted\n  # tls_key_pwd = \"\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Minimal TLS version to accept by the client\n  # tls_min_version = \"TLS12\"\n  ## List of ciphers to accept, by default all secure ciphers will be accepted\n  ## See https://pkg.go.dev/crypto/tls#pkg-constants for supported values.\n  ## Use \"all\", \"secure\" and \"insecure\" to add all support ciphers, secure\n  ## suites or insecure suites respectively.\n  # tls_cipher_suites = [\"secure\"]\n  ## Renegotiation method, \"never\", \"once\" or \"freely\"\n  # tls_renegotiation_method = \"never\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## gNMI subscription prefix (optional, can usually be left empty)\n  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n  # origin = \"\"\n  # prefix = \"\"\n  # target = \"\"\n\n  ## Vendor specific options\n  ## This defines what vendor specific options to load.\n  ## * Juniper Header Extension (juniper_header): some sensors are directly managed by\n  ##   Linecard, which adds the Juniper GNMI Header Extension. Enabling this\n  ##   allows the decoding of the Extension header if present. Currently this knob\n  ##   adds component, component_id & sub_component_id as additional tags\n  # vendor_specific = []\n\n  ## YANG model paths for decoding IETF JSON payloads\n  ## Model files are loaded recursively from the given directories. Disabled if\n  ## no models are specified.\n  # yang_model_paths = []\n\n  ## Define additional aliases to map encoding paths to measurement names\n  # [inputs.gnmi.aliases]\n  #   ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n\n  [[inputs.gnmi.subscription]]\n    ## Name of the measurement that will be emitted\n    name = \"ifcounters\"\n\n    ## Origin and path of the subscription\n    ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n    ##\n    ## origin usually refers to a (YANG) data model implemented by the device\n    ## and path to a specific substructure inside it that should be subscribed\n    ## to (similar to an XPath). YANG models can be found e.g. here:\n    ## https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n    origin = \"openconfig-interfaces\"\n    path = \"/interfaces/interface/state/counters\"\n\n    ## Subscription mode (\"target_defined\", \"sample\", \"on_change\") and interval\n    subscription_mode = \"sample\"\n    sample_interval = \"10s\"\n\n    ## Suppress redundant transmissions when measured values are unchanged\n    # suppress_redundant = false\n\n    ## If suppression is enabled, send updates at least every X seconds anyway\n    # heartbeat_interval = \"60s\"\n\n  ## Tag subscriptions are applied as tags to other subscriptions.\n  # [[inputs.gnmi.tag_subscription]]\n  #  ## When applying this value as a tag to other metrics, use this tag name\n  #  name = \"descr\"\n  #\n  #  ## All other subscription fields are as normal\n  #  origin = \"openconfig-interfaces\"\n  #  path = \"/interfaces/interface/state\"\n  #  subscription_mode = \"on_change\"\n  #\n  #  ## Match strategy to use for the tag.\n  #  ## Tags are only applied for metrics of the same address. The following\n  #  ## settings are valid:\n  #  ##   unconditional -- always match\n  #  ##   name          -- match by the \"name\" key\n  #  ##                    This resembles the previous 'tag-only' behavior.\n  #  ##   elements      -- match by the keys in the path filtered by the path\n  #  ##                    parts specified `elements` below\n  #  ## By default, 'elements' is used if the 'elements' option is provided,\n  #  ## otherwise match by 'name'.\n  #  # match = \"\"\n  #\n  #  ## For the 'elements' match strategy, at least one path-element name must\n  #  ## be supplied containing at least one key to match on. Multiple path\n  #  ## elements can be specified in any order. All given keys must be equal\n  #  ## for a match.\n  #  # elements = [\"description\", \"interface\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring MQTT Consumer Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the MQTT Consumer input plugin that reads metrics from MQTT topics. Includes server connection, topic subscription, QoS settings, authentication, TLS configuration, and data format options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mqtt_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from MQTT topic(s)\n[[inputs.mqtt_consumer]]\n  ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n  ## clusters or standalone servers, use a separate plugin instance.\n  ##   example: servers = [\"tcp://localhost:1883\"]\n  ##            servers = [\"ssl://localhost:1883\"]\n  ##            servers = [\"ws://localhost:1883\"]\n  servers = [\"tcp://127.0.0.1:1883\"]\n\n  ## Topics that will be subscribed to.\n  topics = [\n    \"telegraf/host01/cpu\",\n    \"telegraf/+/mem\",\n    \"sensors/#\",\n  ]\n\n  ## The message topic will be stored in a tag specified by this value.  If set\n  ## to the empty string no topic tag will be created.\n  # topic_tag = \"topic\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  ##\n  ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n  ## resuming unacknowledged messages.\n  # qos = 0\n\n  ## Connection timeout for initial connection in seconds\n  # connection_timeout = \"30s\"\n\n  ## Interval and ping timeout for keep-alive messages\n  ## The sum of those options defines when a connection loss is detected.\n  ## Note: The keep-alive interval needs to be greater or equal one second and\n  ## fractions of a second are not supported.\n  # keepalive = \"60s\"\n  # ping_timeout = \"10s\"\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Persistent session disables clearing of the client session on connection.\n  ## In order for this option to work you must also set client_id to identify\n  ## the client.  To receive messages that arrived while the client is offline,\n  ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n  ## publishing. Finally, using a persistent session will use the initial\n  ## connection topics and not subscribe to any new topics even after\n  ## reconnecting or restarting without a change in client ID.\n  # persistent_session = false\n\n  ## If unset, a random client ID will be generated.\n  # client_id = \"\"\n\n  ## Username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Client trace messages\n  ## When set to true, and debug mode enabled in the agent settings, the MQTT\n  ## client's messages are included in telegraf logs. These messages are very\n  ## noisey, but essential for debugging issues.\n  # client_trace = false\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Enable extracting tag values from MQTT topics\n  ## _ denotes an ignored entry in the topic path,\n  ## # denotes a variable length path element (can only be used once per setting)\n  # [[inputs.mqtt_consumer.topic_parsing]]\n  #   topic = \"\"\n  #   measurement = \"\"\n  #   tags = \"\"\n  #   fields = \"\"\n  ## Value supported is int, float, unit\n  #   [inputs.mqtt_consumer.topic_parsing.types]\n  #      key = type\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Plugin Flush Parameters in TOML\nDESCRIPTION: Example showing how to override default flush parameters for specific output plugins. Demonstrates setting different flush intervals and batch sizes for influxdb and file outputs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[agent]\n  flush_interval = \"10s\"\n  flush_jitter = \"5s\"\n  metric_batch_size = 1000\n\n[[outputs.influxdb]]\n  urls = [ \"http://example.org:8086\" ]\n  database = \"telegraf\"\n\n[[outputs.file]]\n  files = [ \"stdout\" ]\n  flush_interval = \"1s\"\n  flush_jitter = \"1s\"\n  metric_batch_size = 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Taginclude and Tagexclude in Telegraf CPU and Disk Inputs\nDESCRIPTION: This snippet shows how to use taginclude and tagexclude filters with CPU and disk input plugins in Telegraf. It demonstrates including only specific tags and excluding others from the measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n# Only include the \"cpu\" tag in the measurements for the cpu plugin.\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  taginclude = [\"cpu\"]\n\n# Exclude the \"fstype\" tag from the measurements for the disk plugin.\n[[inputs.disk]]\n  tagexclude = [\"fstype\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring NATS Consumer Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the NATS Consumer Input Plugin that specifies server connection details, subjects to consume, authentication options, TLS settings, and message handling parameters. The configuration supports both standard NATS subjects and JetStream subjects for persistent message delivery.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nats_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from NATS subject(s)\n[[inputs.nats_consumer]]\n  ## urls of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## subject(s) to consume\n  ## If you use jetstream you need to set the subjects\n  ## in jetstream_subjects\n  subjects = [\"telegraf\"]\n\n  ## jetstream subjects\n  ## jetstream is a streaming technology inside of nats.\n  ## With jetstream the nats-server persists messages and\n  ## a consumer can consume historical messages. This is\n  ## useful when telegraf needs to restart it don't miss a\n  ## message. You need to configure the nats-server.\n  ## https://docs.nats.io/nats-concepts/jetstream.\n  jetstream_subjects = [\"js_telegraf\"]\n\n  ## name a queue group\n  queue_group = \"telegraf_consumers\"\n\n  ## Optional authentication with username and password credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional authentication with NATS credentials file (NATS 2.0)\n  # credentials = \"/etc/telegraf/nats.creds\"\n\n  ## Optional authentication with nkey seed file (NATS 2.0)\n  # nkey_seed = \"/etc/telegraf/seed.txt\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Sets the limits for pending msgs and bytes for each subscription\n  ## These shouldn't need to be adjusted except in very high throughput scenarios\n  # pending_message_limit = 65536\n  # pending_bytes_limit = 67108864\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Graylog Input Plugin in TOML\nDESCRIPTION: Configuration example for the Graylog input plugin showing server endpoints, timeout settings, metric specifications, authentication credentials, and TLS configuration options. Supports both multiple and namespace API endpoints for collecting Graylog metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/graylog/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read flattened metrics from one or more GrayLog HTTP endpoints\n[[inputs.graylog]]\n  ## API endpoint, currently supported API:\n  ##\n  ##   - multiple  (e.g. http://<host>:9000/api/system/metrics/multiple)\n  ##   - namespace (e.g. http://<host>:9000/api/system/metrics/namespace/{namespace})\n  ##\n  ## For namespace endpoint, the metrics array will be ignored for that call.\n  ## Endpoint can contain namespace and multiple type calls.\n  ##\n  ## Please check http://[graylog-server-ip]:9000/api/api-browser for full list\n  ## of endpoints\n  servers = [\n    \"http://[graylog-server-ip]:9000/api/system/metrics/multiple\",\n  ]\n\n  ## Set timeout (default 5 seconds)\n  # timeout = \"5s\"\n\n  ## Metrics list\n  ## List of metrics can be found on Graylog webservice documentation.\n  ## Or by hitting the web service api at:\n  ##   http://[graylog-host]:9000/api/system/metrics\n  metrics = [\n    \"jvm.cl.loaded\",\n    \"jvm.memory.pools.Metaspace.committed\"\n  ]\n\n  ## Username and password\n  username = \"\"\n  password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Client Output in TOML\nDESCRIPTION: Complete configuration example for the Prometheus client output plugin in Telegraf. Includes settings for listener address, timeouts, authentication, IP range restrictions, TLS configuration, metric versioning, and custom headers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/prometheus_client/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.prometheus_client]]\n  ## Address to listen on.\n  ##   ex:\n  ##     listen = \":9273\"\n  ##     listen = \"vsock://:9273\"\n  listen = \":9273\"\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.\n  ## See \"Metric Format Configuration\" in plugins/inputs/prometheus/README.md for details.\n  ## Valid options: 1, 2\n  # metric_version = 1\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"Foo\"\n  # basic_password = \"Bar\"\n\n  ## If set, the IP Ranges which are allowed to access metrics.\n  ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n  # ip_range = []\n\n  ## Path to publish the metrics on.\n  # path = \"/metrics\"\n\n  ## Expiration interval for each metric. 0 == no expiration\n  # expiration_interval = \"60s\"\n\n  ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n  ## If unset, both are enabled.\n  # collectors_exclude = [\"gocollector\", \"process\"]\n\n  ## Send string metrics as Prometheus labels.\n  ## Unless set to false all string metrics will be sent as labels.\n  # string_as_label = true\n\n  ## If set, enable TLS with the given certificate.\n  # tls_cert = \"/etc/ssl/telegraf.crt\"\n  # tls_key = \"/etc/ssl/telegraf.key\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Export metric collection time.\n  # export_timestamp = false\n\n  ## Set custom headers for HTTP responses.\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Specify the metric type explicitly.\n  ## This overrides the metric-type of the Telegraf metric. Globbing is allowed.\n  # [outputs.prometheus_client.metric_types]\n  #   counter = []\n  #   gauge = []\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Input Plugin in Telegraf\nDESCRIPTION: Configuration example for the GitHub input plugin in Telegraf. It demonstrates how to specify repositories to monitor, set up API access tokens, configure enterprise URLs, and request additional fields like pull request statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/github/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather repository information from GitHub hosted repositories.\n[[inputs.github]]\n  ## List of repositories to monitor\n  repositories = [\n    \"influxdata/telegraf\",\n    \"influxdata/influxdb\"\n  ]\n\n  ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n  # access_token = \"\"\n\n  ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n  # enterprise_base_url = \"\"\n\n  ## Timeout for HTTP requests.\n  # http_timeout = \"5s\"\n\n  ## List of additional fields to query.\n  ## NOTE: Getting those fields might involve issuing additional API-calls, so please\n  ##       make sure you do not exceed the rate-limit of GitHub.\n  ##\n  ## Available fields are:\n  ##  - pull-requests -- number of open and closed pull requests (2 API-calls per repository)\n  # additional_fields = []\n```\n\n----------------------------------------\n\nTITLE: Pulling Official Telegraf Docker Images\nDESCRIPTION: Docker commands to pull the latest Telegraf images from DockerHub. Includes commands for both Debian-based and Alpine-based images.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INSTALL_GUIDE.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# latest Debian-based image\ndocker pull telegraf\n# latest Alpine-based image\ndocker pull telegraf:alpine\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Input in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to configure the Telegraf Prometheus input plugin to scrape metrics from a Prometheus endpoint. It includes options for specifying URLs, metric version, Kubernetes service discovery, security, and other advanced settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/prometheus/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Read metrics from one or many prometheus clients\n[[inputs.prometheus]]\n  ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:9100/metrics\"]\n\n  ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.\n  ## See \"Metric Format Configuration\" in plugins/inputs/prometheus/README.md for details.\n  ## Valid options: 1, 2\n  # metric_version = 1\n\n  ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n  # url_tag = \"url\"\n\n  ## Whether the timestamp of the scraped metrics will be ignored.\n  ## If set to true, the gather time will be used.\n  # ignore_timestamp = false\n\n  ## Override content-type of the returned message\n  ## Available options are for prometheus:\n  ##   text, protobuf-delimiter, protobuf-compact, protobuf-text,\n  ## and for openmetrics:\n  ##   openmetrics-text, openmetrics-protobuf\n  ## By default the content-type of the response is used.\n  # content_type_override = \"\"\n\n  ## An array of Kubernetes services to scrape metrics from.\n  # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n\n  ## Kubernetes config file to create client from.\n  # kube_config = \"/path/to/kubernetes.config\"\n\n  ## Scrape Pods\n  ## Enable scraping of k8s pods. Further settings as to which pods to scape\n  ## are determiend by the 'method' option below. When enabled, the default is\n  ## to use annotations to determine whether to scrape or not.\n  # monitor_kubernetes_pods = false\n\n  ## Scrape Pods Method\n  ## annotations: default, looks for specific pod annotations documented below\n  ## settings: only look for pods matching the settings provided, not\n  ##   annotations\n  ## settings+annotations: looks at pods that match annotations using the user\n  ##   defined settings\n  # monitor_kubernetes_pods_method = \"annotations\"\n\n  ## Scrape Pods 'annotations' method options\n  ## If set method is set to 'annotations' or 'settings+annotations', these\n  ## annotation flags are looked for:\n  ## - prometheus.io/scrape: Required to enable scraping for this pod. Can also\n  ##     use 'prometheus.io/scrape=false' annotation to opt-out entirely.\n  ## - prometheus.io/scheme: If the metrics endpoint is secured then you will\n  ##     need to set this to 'https' & most likely set the tls config\n  ## - prometheus.io/path: If the metrics path is not /metrics, define it with\n  ##     this annotation\n  ## - prometheus.io/port: If port is not 9102 use this annotation\n\n  ## Scrape Pods 'settings' method options\n  ## When using 'settings' or 'settings+annotations', the default values for\n  ## annotations can be modified using with the following options:\n  # monitor_kubernetes_pods_scheme = \"http\"\n  # monitor_kubernetes_pods_port = \"9102\"\n  # monitor_kubernetes_pods_path = \"/metrics\"\n\n  ## Get the list of pods to scrape with either the scope of\n  ## - cluster: the kubernetes watch api (default, no need to specify)\n  ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.\n  # pod_scrape_scope = \"cluster\"\n\n  ## Only for node scrape scope: node IP of the node that telegraf is running on.\n  ## Either this config or the environment variable NODE_IP must be set.\n  # node_ip = \"10.180.1.1\"\n\n  ## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.\n  ## Default is 60 seconds.\n  # pod_scrape_interval = 60\n\n  ## Content length limit\n  ## When set, telegraf will drop responses with length larger than the configured value.\n  ## Default is \"0KB\" which means unlimited.\n  # content_length_limit = \"0KB\"\n\n  ## Restricts Kubernetes monitoring to a single namespace\n  ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n  # monitor_kubernetes_pods_namespace = \"\"\n  ## The name of the label for the pod that is being scraped.\n  ## Default is 'namespace' but this can conflict with metrics that have the label 'namespace'\n  # pod_namespace_label_name = \"namespace\"\n  # label selector to target pods which have the label\n  # kubernetes_label_selector = \"env=dev,app=nginx\"\n  # field selector to target pods\n  # eg. To scrape pods on a specific node\n  # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n\n  ## Filter which pod annotations and labels will be added to metric tags\n  #\n  # pod_annotation_include = [\"annotation-key-1\"]\n  # pod_annotation_exclude = [\"exclude-me\"]\n  # pod_label_include = [\"label-key-1\"]\n  # pod_label_exclude = [\"exclude-me\"]\n\n  # cache refresh interval to set the interval for re-sync of pods list.\n  # Default is 60 minutes.\n  # cache_refresh_interval = 60\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## HTTP Basic Authentication username and password. ('bearer_token' and\n  ## 'bearer_token_string' take priority)\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional custom HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Specify timeout duration for slower prometheus clients (default is 5s)\n  # timeout = \"5s\"\n\n  ## This option is now used by the HTTP client to set the header response\n  ## timeout, not the overall HTTP timeout.\n  # response_timeout = \"5s\"\n\n  ## HTTP Proxy support\n  # use_system_proxy = false\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Use the given name as the SNI server name on each URL\n  # tls_server_name = \"myhost.example.org\"\n\n  ## TLS renegotiation method, choose from \"never\", \"once\", \"freely\"\n  # tls_renegotiation_method = \"never\"\n\n  ## Enable/disable TLS\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable = true\n\n  ## This option allows you to report the status of prometheus requests.\n  # enable_request_metrics = false\n\n  ## Scrape Services available in Consul Catalog\n  # [inputs.prometheus.consul]\n  #   enabled = true\n  #   agent = \"http://localhost:8500\"\n  #   query_interval = \"5m\"\n\n  #   [[inputs.prometheus.consul.query]]\n  #     name = \"a service name\"\n  #     tag = \"a service tag\"\n  #     url = 'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'\n  #     [inputs.prometheus.consul.query.tags]\n  #       host = \"{{.Node}}\"\n\n  ## Control pod scraping based on pod namespace annotations\n  ## Pass and drop here act like tagpass and tagdrop, but instead\n  ## of filtering metrics they filters pod candidates for scraping\n  #[inputs.prometheus.namespace_annotation_pass]\n  # annotation_key = [\"value1\", \"value2\"]\n  #[inputs.prometheus.namespace_annotation_drop]\n  # some_annotation_key = [\"dont-scrape\"]\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Event Hub Consumer in TOML\nDESCRIPTION: Complete configuration example for the Azure Event Hub Consumer input plugin. Includes settings for connection strings, persistence, consumer groups, message handling, and metadata field mapping. Supports both Azure Event Hubs and IoT Hub integration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/eventhub_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Azure Event Hubs service input plugin\n[[inputs.eventhub_consumer]]\n  ## The default behavior is to create a new Event Hub client from environment variables.\n  ## This requires one of the following sets of environment variables to be set:\n  ##\n  ## 1) Expected Environment Variables:\n  ##    - \"EVENTHUB_CONNECTION_STRING\"\n  ##\n  ## 2) Expected Environment Variables:\n  ##    - \"EVENTHUB_NAMESPACE\"\n  ##    - \"EVENTHUB_NAME\"\n  ##    - \"EVENTHUB_KEY_NAME\"\n  ##    - \"EVENTHUB_KEY_VALUE\"\n\n  ## 3) Expected Environment Variables:\n  ##    - \"EVENTHUB_NAMESPACE\"\n  ##    - \"EVENTHUB_NAME\"\n  ##    - \"AZURE_TENANT_ID\"\n  ##    - \"AZURE_CLIENT_ID\"\n  ##    - \"AZURE_CLIENT_SECRET\"\n\n  ## Uncommenting the option below will create an Event Hub client based solely on the connection string.\n  ## This can either be the associated environment variable or hard coded directly.\n  ## If this option is uncommented, environment variables will be ignored.\n  ## Connection string should contain EventHubName (EntityPath)\n  # connection_string = \"\"\n\n  ## Set persistence directory to a valid folder to use a file persister instead of an in-memory persister\n  # persistence_dir = \"\"\n\n  ## Change the default consumer group\n  # consumer_group = \"\"\n\n  ## By default the event hub receives all messages present on the broker, alternative modes can be set below.\n  ## The timestamp should be in https://github.com/toml-lang/toml#offset-date-time format (RFC 3339).\n  ## The 3 options below only apply if no valid persister is read from memory or file (e.g. first run).\n  # from_timestamp =\n  # latest = true\n\n  ## Set a custom prefetch count for the receiver(s)\n  # prefetch_count = 1000\n\n  ## Add an epoch to the receiver(s)\n  # epoch = 0\n\n  ## Change to set a custom user agent, \"telegraf\" is used by default\n  # user_agent = \"telegraf\"\n\n  ## To consume from a specific partition, set the partition_ids option.\n  ## An empty array will result in receiving from all partitions.\n  # partition_ids = [\"0\",\"1\"]\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Set either option below to true to use a system property as timestamp.\n  ## You have the choice between EnqueuedTime and IoTHubEnqueuedTime.\n  ## It is recommended to use this setting when the data itself has no timestamp.\n  # enqueued_time_as_ts = true\n  # iot_hub_enqueued_time_as_ts = true\n\n  ## Tags or fields to create from keys present in the application property bag.\n  ## These could for example be set by message enrichments in Azure IoT Hub.\n  # application_property_tags = []\n  # application_property_fields = []\n\n  ## Tag or field name to use for metadata\n  ## By default all metadata is disabled\n  # sequence_number_field = \"SequenceNumber\"\n  # enqueued_time_field = \"EnqueuedTime\"\n  # offset_field = \"Offset\"\n  # partition_id_tag = \"PartitionID\"\n  # partition_key_tag = \"PartitionKey\"\n  # iot_hub_device_connection_id_tag = \"IoTHubDeviceConnectionID\"\n  # iot_hub_auth_generation_id_tag = \"IoTHubAuthGenerationID\"\n  # iot_hub_connection_auth_method_tag = \"IoTHubConnectionAuthMethod\"\n  # iot_hub_connection_module_id_tag = \"IoTHubConnectionModuleID\"\n  # iot_hub_enqueued_time_field = \"IoTHubEnqueuedTime\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Output Plugin in TOML\nDESCRIPTION: Configuration example for the OpenTelemetry output plugin showing all available options including service address, TLS configuration, compression settings, Coralogix dialect options, and additional attributes and headers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opentelemetry/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.opentelemetry]]\n  ## Override the default (localhost:4317) OpenTelemetry gRPC service\n  ## address:port\n  # service_address = \"localhost:4317\"\n\n  ## Override the default (5s) request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config.\n  ##\n  ## Root certificates for verifying server certificates encoded in PEM format.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  ## The public and private key pairs for the client encoded in PEM format.\n  ## May contain intermediate certificates.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS, but skip TLS chain and host verification.\n  # insecure_skip_verify = false\n  ## Send the specified TLS server name via SNI.\n  # tls_server_name = \"foo.example.com\"\n\n  ## Override the default (gzip) compression used to send data.\n  ## Supports: \"gzip\", \"none\"\n  # compression = \"gzip\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Configuration options for the Coralogix dialect\n  ## Enable the following section of you use this plugin with a Coralogix endpoint\n  # [outputs.opentelemetry.coralogix]\n  #   ## Your Coralogix private key (required).\n  #   ## Please note that this is sensitive data!\n  #   private_key = \"your_coralogix_key\"\n  #\n  #   ## Application and subsystem names for the metrics (required)\n  #   application = \"$NAMESPACE\"\n  #   subsystem = \"$HOSTNAME\"\n\n  ## Additional OpenTelemetry resource attributes\n  # [outputs.opentelemetry.attributes]\n  # \"service.name\" = \"demo\"\n\n  ## Additional gRPC request metadata\n  # [outputs.opentelemetry.headers]\n  # key1 = \"value1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Server Input Plugin for Telegraf in TOML\nDESCRIPTION: Configuration block for the Telegraf sqlserver input plugin that collects metrics from Microsoft SQL Server. This specifies connection parameters, timeout settings, authentication methods, database type, and which queries to include or exclude.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from Microsoft SQL Server\n[[inputs.sqlserver]]\n  ## Specify instances to monitor with a list of connection strings.\n  ## All connection parameters are optional.\n  ## By default, the host is localhost, listening on default port, TCP 1433.\n  ##   for Windows, the user is the currently running AD user (SSO).\n  ##   See https://github.com/microsoft/go-mssqldb for detailed connection\n  ##   parameters, in particular, tls connections can be created like so:\n  ##   \"encrypt=true;certificate=<cert>;hostNameInCertificate=<SqlServer host fqdn>\"\n  servers = [\n    \"Server=192.168.1.10;Port=1433;User Id=<user>;Password=<pw>;app name=telegraf;log=1;\",\n  ]\n\n  ## Timeout for query execution operation\n  ## Note that the timeout for queries is per query not per gather.\n  ## 0 value means no timeout\n  # query_timeout = \"0s\"\n\n  ## Authentication method\n  ## valid methods: \"connection_string\", \"AAD\"\n  # auth_method = \"connection_string\"\n\n  ## ClientID is the is the client ID of the user assigned identity of the VM\n  ## that should be used to authenticate to the Azure SQL server.\n  # client_id = \"\"\n\n  ## \"database_type\" enables a specific set of queries depending on the database type. If specified, it replaces azuredb = true/false and query_version = 2\n  ## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n  ## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n  database_type = \"SQLServer\"\n\n  ## A list of queries to include. If not specified, all the below listed queries are used.\n  include_query = []\n\n  ## A list of queries to explicitly ignore.\n  exclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n\n  ## Queries enabled by default for database_type = \"SQLServer\" are -\n  ## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n  ## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates,\n  ## SQLServerRecentBackups\n\n  ## Queries enabled by default for database_type = \"AzureSQLDB\" are -\n  ## AzureSQLDBResourceStats, AzureSQLDBResourceGovernance, AzureSQLDBWaitStats, AzureSQLDBDatabaseIO, AzureSQLDBServerProperties,\n  ## AzureSQLDBOsWaitstats, AzureSQLDBMemoryClerks, AzureSQLDBPerformanceCounters, AzureSQLDBRequests, AzureSQLDBSchedulers\n\n  ## Queries enabled by default for database_type = \"AzureSQLManagedInstance\" are -\n  ## AzureSQLMIResourceStats, AzureSQLMIResourceGovernance, AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,\n  ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers\n\n  ## Queries enabled by default for database_type = \"AzureSQLPool\" are -\n  ## AzureSQLPoolResourceStats, AzureSQLPoolResourceGovernance, AzureSQLPoolDatabaseIO, AzureSQLPoolWaitStats,\n  ## AzureSQLPoolMemoryClerks, AzureSQLPoolPerformanceCounters, AzureSQLPoolSchedulers\n\n  ## Queries enabled by default for database_type = \"AzureArcSQLManagedInstance\" are -\n  ## AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,\n  ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers\n\n  ## Following are old config settings\n  ## You may use them only if you are using the earlier flavor of queries, however it is recommended to use\n  ## the new mechanism of identifying the database_type there by use it's corresponding queries\n\n  ## Optional parameter, setting this to 2 will use a new version\n  ## of the collection queries that break compatibility with the original\n  ## dashboards.\n  ## Version 2 - is compatible from SQL Server 2012 and later versions and also for SQL Azure DB\n  # query_version = 2\n\n  ## If you are using AzureDB, setting this to true will gather resource utilization metrics\n  # azuredb = false\n\n  ## Toggling this to true will emit an additional metric called \"sqlserver_telegraf_health\".\n  ## This metric tracks the count of attempted queries and successful queries for each SQL instance specified in \"servers\".\n  ## The purpose of this metric is to assist with identifying and diagnosing any connectivity or query issues.\n  ## This setting/metric is optional and is disabled by default.\n  # health_metric = false\n\n  ## Possible queries across different versions of the collectors\n  ## Queries enabled by default for specific Database Type\n\n  ## database_type =  AzureSQLDB  by default collects the following queries\n  ## - AzureSQLDBWaitStats\n  ## - AzureSQLDBResourceStats\n  ## - AzureSQLDBResourceGovernance\n  ## - AzureSQLDBDatabaseIO\n  ## - AzureSQLDBServerProperties\n  ## - AzureSQLDBOsWaitstats\n  ## - AzureSQLDBMemoryClerks\n  ## - AzureSQLDBPerformanceCounters\n  ## - AzureSQLDBRequests\n  ## - AzureSQLDBSchedulers\n\n  ## database_type =  AzureSQLManagedInstance by default collects the following queries\n  ## - AzureSQLMIResourceStats\n  ## - AzureSQLMIResourceGovernance\n  ## - AzureSQLMIDatabaseIO\n  ## - AzureSQLMIServerProperties\n  ## - AzureSQLMIOsWaitstats\n  ## - AzureSQLMIMemoryClerks\n  ## - AzureSQLMIPerformanceCounters\n  ## - AzureSQLMIRequests\n  ## - AzureSQLMISchedulers\n\n  ## database_type =  AzureSQLPool by default collects the following queries\n  ## - AzureSQLPoolResourceStats\n  ## - AzureSQLPoolResourceGovernance\n  ## - AzureSQLPoolDatabaseIO\n  ## - AzureSQLPoolOsWaitStats,\n  ## - AzureSQLPoolMemoryClerks\n  ## - AzureSQLPoolPerformanceCounters\n  ## - AzureSQLPoolSchedulers\n\n  ## database_type =  SQLServer by default collects the following queries\n  ## - SQLServerPerformanceCounters\n  ## - SQLServerWaitStatsCategorized\n  ## - SQLServerDatabaseIO\n  ## - SQLServerProperties\n  ## - SQLServerMemoryClerks\n  ## - SQLServerSchedulers\n  ## - SQLServerRequests\n  ## - SQLServerVolumeSpace\n  ## - SQLServerCpu\n  ## - SQLServerRecentBackups\n  ## and following as optional (if mentioned in the include_query list)\n  ## - SQLServerAvailabilityReplicaStates\n  ## - SQLServerDatabaseReplicaStates\n```\n\n----------------------------------------\n\nTITLE: Configuring Server TLS in Telegraf using TOML\nDESCRIPTION: This snippet details the server-side TLS configuration for Telegraf, supporting mutual authentication. It includes settings for allowed client CA certificates, DNS name verification, and server certificate configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TLS.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n## Set one or more allowed client CA certificate file names to\n## enable mutually authenticated TLS connections.\n# tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n## Set one or more allowed DNS name to enable a whitelist\n## to verify incoming client certificates.\n## It will go through all available SAN in the certificate,\n## if of them matches the request is accepted.\n# tls_allowed_dns_names = [\"client.example.org\"]\n\n## Add service certificate and key.\n# tls_cert = \"/etc/telegraf/cert.pem\"\n# tls_key = \"/etc/telegraf/key.pem\"\n# passphrase for encrypted private key, if it is in PKCS#8 format. Encrypted PKCS#1 private keys are not supported.\n# tls_key_pwd = \"changeme\"\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Telegraf Input Plugin\nDESCRIPTION: Complete example of a simple Telegraf input plugin implementation. Shows the required interface methods, configuration handling, metric collection, and plugin registration process.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INPUTS.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n//go:generate ../../../tools/readme_config_includer/generator\npackage simple\n\nimport (\n    _ \"embed\"\n\n    \"github.com/influxdata/telegraf\"\n    \"github.com/influxdata/telegraf/plugins/inputs\"\n)\n\n//go:embed sample.conf\nvar sampleConfig string\n\ntype Simple struct {\n    Ok  bool            `toml:\"ok\"`\n    Log telegraf.Logger `toml:\"-\"`\n}\n\nfunc (*Simple) SampleConfig() string {\n    return sampleConfig\n}\n\n// Init is for setup, and validating config.\nfunc (s *Simple) Init() error {\n    return nil\n}\n\nfunc (s *Simple) Gather(acc telegraf.Accumulator) error {\n    if s.Ok {\n        acc.AddFields(\"state\", map[string]interface{}{\"value\": \"pretty good\"}, nil)\n    } else {\n        acc.AddFields(\"state\", map[string]interface{}{\"value\": \"not great\"}, nil)\n    }\n\n    return nil\n}\n\nfunc init() {\n    inputs.Add(\"simple\", func() telegraf.Input { return &Simple{} })\n}\n```\n\n----------------------------------------\n\nTITLE: Routing Metrics to Different InfluxDB Outputs in Telegraf\nDESCRIPTION: This configuration demonstrates how to route metrics to different InfluxDB outputs based on metric names and tags. It shows filtering for specific databases and CPU cores.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_19\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf\"\n  # Drop all measurements that start with \"aerospike\"\n  namedrop = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-aerospike-data\"\n  # Only accept aerospike data:\n  namepass = [\"aerospike*\"]\n\n[[outputs.influxdb]]\n  urls = [ \"http://localhost:8086\" ]\n  database = \"telegraf-cpu0-data\"\n  # Only store measurements where the tag \"cpu\" matches the value \"cpu0\"\n  [outputs.influxdb.tagpass]\n    cpu = [\"cpu0\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Tail Input Plugin\nDESCRIPTION: Sample configuration for the Telegraf Tail plugin that specifies how to tail files, handle offsets, manage file updates, and process multiline logs. Includes options for character encoding and data formats.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tail/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Parse the new lines appended to a file\n[[inputs.tail]]\n  ## File names or a pattern to tail.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -> recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -> find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -> just tail the apache log file\n  ##   \"/var/log/log[!1-2]*  -> tail files without 1-2\n  ##   \"/var/log/log[^1-2]*  -> identical behavior as above\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/mymetrics.out\"]\n\n  ## Offset to start reading at\n  ## The following methods are available:\n  ##   beginning          -- start reading from the beginning of the file ignoring any persisted offset\n  ##   end                -- start reading from the end of the file ignoring any persisted offset\n  ##   saved-or-beginning -- use the persisted offset of the file or, if no offset persisted, start from the beginning of the file\n  ##   saved-or-end       -- use the persisted offset of the file or, if no offset persisted, start from the end of the file\n  # initial_read_offset = \"saved-or-end\"\n\n  ## Whether file is a named pipe\n  # pipe = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  ## inotify is supported on linux, *bsd, and macOS, while Windows requires\n  ## using poll. Poll checks for changes every 250ms.\n  # watch_method = \"inotify\"\n\n  ## Maximum lines of the file to process that have not yet be written by the\n  ## output.  For best throughput set based on the number of metrics on each\n  ## line and the size of the output's metric_batch_size.\n  # max_undelivered_lines = 1000\n\n  ## Character encoding to use when interpreting the file contents.  Invalid\n  ## characters are replaced using the unicode replacement character.  When set\n  ## to the empty string the data is not decoded to text.\n  ##   ex: character_encoding = \"utf-8\"\n  ##       character_encoding = \"utf-16le\"\n  ##       character_encoding = \"utf-16be\"\n  ##       character_encoding = \"\"\n  # character_encoding = \"\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.\n  # path_tag = \"path\"\n\n  ## Filters to apply to files before generating metrics\n  ## \"ansi_color\" removes ANSI colors\n  # filters = []\n\n  ## multiline parser/codec\n  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html\n  #[inputs.tail.multiline]\n    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.\n    #pattern = \"^\\s\"\n\n    ## The field's value must be previous or next and indicates the relation to the\n    ## multi-line event.\n    #match_which_line = \"previous\"\n\n    ## The invert_match can be true or false (defaults to false).\n    ## If true, a message not matching the pattern will constitute a match of the multiline filter and the what will be applied. (vice-versa is also true)\n    #invert_match = false\n\n    ## The handling method for quoted text (defaults to 'ignore').\n    ## The following methods are available:\n    ##   ignore  -- do not consider quotation (default)\n    ##   single-quotes -- consider text quoted by single quotes (')\n    ##   double-quotes -- consider text quoted by double quotes (\")\n    ##   backticks     -- consider text quoted by backticks (`)\n    ## When handling quotes, escaped quotes (e.g. \\\") are handled correctly.\n    #quotation = \"ignore\"\n\n    ## The preserve_newline option can be true or false (defaults to false).\n    ## If true, the newline character is preserved for multiline elements,\n    ## this is useful to preserve message-structure e.g. for logging outputs.\n    #preserve_newline = false\n\n    #After the specified timeout, this plugin sends the multiline event even if no new pattern is found to start a new event. The default is 5s.\n    #timeout = 5s\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Plugin in TOML\nDESCRIPTION: Main configuration block for the PostgreSQL extensible input plugin, including connection settings and query definitions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.postgresql_extensible]]\n  address = \"host=localhost user=postgres sslmode=disable\"\n  prepared_statements = true\n\n  [[inputs.postgresql_extensible.query]]\n    measurement=\"pg_stat_database\"\n    sqlquery=\"SELECT * FROM pg_stat_database WHERE datname\"\n    min_version=901\n    tagvalue=\"\"\n  [[inputs.postgresql_extensible.query]]\n    script=\"your_sql-filepath.sql\"\n    min_version=901\n    max_version=1300\n    tagvalue=\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logz.io Output Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Logz.io output plugin for Telegraf. It includes settings for connection timeout, TLS configuration, Logz.io account token, and listener URL. The token is required, while other parameters are optional.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/logzio/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin that can send metrics over HTTPs to Logz.io\n[[outputs.logzio]]\n  ## Connection timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Logz.io account token\n  token = \"your logz.io token\" # required\n\n  ## Use your listener URL for your Logz.io account region.\n  # url = \"https://listener.logz.io:8071\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Input in Telegraf\nDESCRIPTION: This configuration snippet shows how to configure the MongoDB input plugin in Telegraf. It includes setting the server URLs, enabling/disabling collection of cluster, database, and collection statistics, TLS configuration, and specifying the behavior for disconnected servers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Read metrics from one or many MongoDB servers\n[[inputs.mongodb]]\n  ## An array of URLs of the form:\n  ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n  ## For example:\n  ##   mongodb://user:auth_key@10.10.3.30:27017,\n  ##   mongodb://10.10.3.33:18832,\n  ##\n  ## If connecting to a cluster, users must include the \"?connect=direct\" in\n  ## the URL to ensure that the connection goes directly to the specified node\n  ## and not have all connections passed to the master node.\n  servers = [\"mongodb://127.0.0.1:27017/?connect=direct\"]\n\n  ## When true, collect cluster status.\n  ## Note that the query that counts jumbo chunks triggers a COLLSCAN, which\n  ## may have an impact on performance.\n  # gather_cluster_status = true\n\n  ## When true, collect per database stats\n  # gather_perdb_stats = false\n\n  ## When true, collect per collection stats\n  # gather_col_stats = false\n\n  ## When true, collect usage statistics for each collection\n  ## (insert, update, queries, remove, getmore, commands etc...).\n  # gather_top_stat = false\n\n  ## List of db where collections stats are collected\n  ## If empty, all db are concerned\n  # col_stats_dbs = [\"local\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Specifies plugin behavior regarding disconnected servers\n  ## Available choices :\n  ##   - error: telegraf will return an error on startup if one the servers is unreachable\n  ##   - skip: telegraf will skip unreachable servers on both startup and gather\n  # disconnected_servers_behavior = \"error\"\n\n```\n\n----------------------------------------\n\nTITLE: Example Libvirt Configuration in TOML\nDESCRIPTION: This TOML configuration provides a concrete example of configuring the `inputs.libvirt` plugin. It specifies the `domain_names` to monitor, the `libvirt_uri` for connecting to the hypervisor, the `libvirt_metrics` to gather (state and interface), and `additional_statistics` (vcpu_mapping).  This configuration allows targeted metric collection from specific VMs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/libvirt/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.libvirt]]\n  domain_names = [\"ubuntu_20\"]\n  libvirt_uri = \"qemu:///system\"\n  libvirt_metrics = [\"state\", \"interface\"]\n  additional_statistics = [\"vcpu_mapping\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring vSAN Metrics Collection in Telegraf\nDESCRIPTION: Configuration example for collecting vSAN metrics in Telegraf. Includes settings for vCenter connections, metric filtering, concurrency controls, and SSL configuration options. Demonstrates how to enable specific vSAN metrics while excluding other VSphere metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.vsphere]]\n  interval = \"300s\"\n  vcenters = [\"https://<vcenter-ip>/sdk\", \"https://<vcenter2-ip>/sdk\"]\n  username = \"<user>\"\n  password = \"<pwd>\"\n\n  # Exclude all other metrics\n  vm_metric_exclude = [\"*\"]\n  datastore_metric_exclude = [\"*\"]\n  datacenter_metric_exclude = [\"*\"]\n  host_metric_exclude = [\"*\"]\n  cluster_metric_exclude = [\"*\"]\n\n  # By default all supported entity will be included\n  vsan_metric_include = [\n    \"summary.disk-usage\",\n    \"summary.health\",\n    \"summary.resync\",\n    \"performance.cluster-domclient\",\n    \"performance.cluster-domcompmgr\",\n    \"performance.host-domclient\",\n    \"performance.host-domcompmgr\",\n    \"performance.cache-disk\",\n    \"performance.disk-group\",\n    \"performance.capacity-disk\",\n    \"performance.disk-group\",\n    \"performance.virtual-machine\",\n    \"performance.vscsi\",\n    \"performance.virtual-disk\",\n    \"performance.vsan-host-net\",\n    \"performance.vsan-vnic-net\",\n    \"performance.vsan-pnic-net\",\n    \"performance.vsan-iscsi-host\",\n    \"performance.vsan-iscsi-target\",\n    \"performance.vsan-iscsi-lun\",\n    \"performance.lsom-world-cpu\",\n    \"performance.nic-world-cpu\",\n    \"performance.dom-world-cpu\",\n    \"performance.cmmds-world-cpu\",\n    \"performance.host-cpu\",\n    \"performance.host-domowner\",\n    \"performance.host-memory-slab\",\n    \"performance.host-memory-heap\",\n    \"performance.system-mem\"\n  ]\n  vsan_metric_skip_verify = true\n  vsan_metric_exclude = [ ]\n\n  collect_concurrency = 5\n  discover_concurrency = 5\n\n  ## Optional SSL Config\n  # ssl_ca = \"/path/to/cafile\"\n  # ssl_cert = \"/path/to/certfile\"\n  # ssl_key = \"/path/to/keyfile\"\n  ## Use SSL but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring the Template Processor Plugin in Telegraf\nDESCRIPTION: Basic configuration for the template processor plugin that defines a tag name and template value to generate new tags from existing metric data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Uses a Go template to create a new tag\n[[processors.template]]\n  ## Go template used to create the tag name of the output. In order to\n  ## ease TOML escaping requirements, you should use single quotes around\n  ## the template string.\n  tag = \"topic\"\n\n  ## Go template used to create the tag value of the output. In order to\n  ## ease TOML escaping requirements, you should use single quotes around\n  ## the template string.\n  template = '{{ .Tag \"hostname\" }}.{{ .Tag \"level\" }}'\n```\n\n----------------------------------------\n\nTITLE: Sample Output for CPU Input Plugin in Telegraf\nDESCRIPTION: This example shows the output format for the CPU input plugin in Telegraf. It includes metrics for individual CPUs and the total CPU, with various time and usage measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cpu/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncpu,cpu=cpu0,host=loaner time_active=202224.15999999992,time_guest=30250.35,time_guest_nice=0,time_idle=1527035.04,time_iowait=1352,time_irq=0,time_nice=169.28,time_softirq=6281.4,time_steal=0,time_system=40097.14,time_user=154324.34 1568760922000000000\ncpu,cpu=cpu0,host=loaner usage_active=31.249999981810106,usage_guest=2.083333333080696,usage_guest_nice=0,usage_idle=68.7500000181899,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=4.166666666161392,usage_user=25.000000002273737 1568760922000000000\ncpu,cpu=cpu1,host=loaner time_active=201890.02000000002,time_guest=30508.41,time_guest_nice=0,time_idle=264641.18,time_iowait=210.44,time_irq=0,time_nice=181.75,time_softirq=4537.88,time_steal=0,time_system=39480.7,time_user=157479.25 1568760922000000000\ncpu,cpu=cpu1,host=loaner usage_active=12.500000010610771,usage_guest=2.0833333328280585,usage_guest_nice=0,usage_idle=87.49999998938922,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=2.0833333332070145,usage_steal=0,usage_system=4.166666665656117,usage_user=4.166666666414029 1568760922000000000\ncpu,cpu=cpu2,host=loaner time_active=201382.78999999998,time_guest=30325.8,time_guest_nice=0,time_idle=264686.63,time_iowait=202.77,time_irq=0,time_nice=162.81,time_softirq=3378.34,time_steal=0,time_system=39270.59,time_user=158368.28 1568760922000000000\ncpu,cpu=cpu2,host=loaner usage_active=15.999999993480742,usage_guest=1.9999999999126885,usage_guest_nice=0,usage_idle=84.00000000651926,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=2.0000000002764864,usage_steal=0,usage_system=3.999999999825377,usage_user=7.999999998923158 1568760922000000000\ncpu,cpu=cpu3,host=loaner time_active=198953.51000000007,time_guest=30344.43,time_guest_nice=0,time_idle=265504.09,time_iowait=187.64,time_irq=0,time_nice=197.47,time_softirq=2301.47,time_steal=0,time_system=39313.73,time_user=156953.2 1568760922000000000\ncpu,cpu=cpu3,host=loaner usage_active=10.41666667424579,usage_guest=0,usage_guest_nice=0,usage_idle=89.58333332575421,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=4.166666666666667,usage_user=6.249999998484175 1568760922000000000\ncpu,cpu=cpu-total,host=loaner time_active=804450.5299999998,time_guest=121429,time_guest_nice=0,time_idle=2321866.96,time_iowait=1952.86,time_irq=0,time_nice=711.32,time_softirq=16499.1,time_steal=0,time_system=158162.17,time_user=627125.08 1568760922000000000\ncpu,cpu=cpu-total,host=loaner usage_active=17.616580305880305,usage_guest=1.036269430422946,usage_guest_nice=0,usage_idle=82.3834196941197,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=1.0362694300459534,usage_steal=0,usage_system=4.145077721691784,usage_user=11.398963731636465 1568760922000000000\n```\n\n----------------------------------------\n\nTITLE: Routing Metrics Based on Input Tags in Telegraf\nDESCRIPTION: This example shows how to route metrics to different outputs based on tags set in the input plugin. It uses the 'influxdb_database' tag to determine the output database and removes the tag before writing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_26\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.influxdb]]\n  urls = [\"http://influxdb.example.com\"]\n  database = \"db_default\"\n  [outputs.influxdb.tagdrop]\n    influxdb_database = [\"*\"]\n\n[[outputs.influxdb]]\n  urls = [\"http://influxdb.example.com\"]\n  database = \"db_other\"\n  tagexclude = [\"influxdb_database\"]\n  [outputs.influxdb.tagpass]\n    influxdb_database = [\"other\"]\n\n[[inputs.disk]]\n  [inputs.disk.tags]\n    influxdb_database = \"other\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PHP-FPM Input Plugin in TOML\nDESCRIPTION: TOML configuration for the PHP-FPM input plugin in Telegraf. It defines options for gathering stats via HTTP status page or socket, including URL configuration, format settings, and TLS options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/phpfpm/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics of phpfpm, via HTTP status page or socket\n[[inputs.phpfpm]]\n  ## An array of addresses to gather stats about. Specify an ip or hostname\n  ## with optional port and path\n  ##\n  ## Plugin can be configured in three modes (either can be used):\n  ##   - http: the URL must start with http:// or https://, ie:\n  ##       \"http://localhost/status\"\n  ##       \"http://192.168.130.1/status?full\"\n  ##\n  ##   - unixsocket: path to fpm socket, ie:\n  ##       \"/var/run/php5-fpm.sock\"\n  ##      or using a custom fpm status path:\n  ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n  ##      glob patterns are also supported:\n  ##       \"/var/run/php*.sock\"\n  ##\n  ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n  ##       \"fcgi://10.0.0.12:9000/status\"\n  ##       \"cgi://10.0.10.12:9001/status\"\n  ##\n  ## Example of multiple gathering from local socket and remote host\n  ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n  urls = [\"http://localhost/status\"]\n\n  ## Format of stats to parse, set to \"status\" or \"json\"\n  ## If the user configures the URL to return JSON (e.g.\n  ## http://localhost/status?json), set to JSON. Otherwise, will attempt to\n  ## parse line-by-line. The JSON mode will produce additional metrics.\n  # format = \"status\"\n\n  ## Duration allowed to complete HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-node TimescaleDB with Telegraf\nDESCRIPTION: Configuration for using TimescaleDB in a multi-node setup with Telegraf. Creates a distributed hypertable with data partitioning by tag_id and replication across nodes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/postgresql/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\ntags_as_foreign_keys = true\ncreate_templates = [\n    '''CREATE TABLE {{ .table }} ({{ .columns }})''',\n    '''SELECT create_distributed_hypertable({{ .table|quoteLiteral }}, 'time', partitioning_column => 'tag_id', number_partitions => (SELECT count(*) FROM timescaledb_information.data_nodes)::integer, replication_factor => 2, chunk_time_interval => INTERVAL '7d')''',\n    '''ALTER TABLE {{ .table }} SET (timescaledb.compress, timescaledb.compress_segmentby = 'tag_id')''',\n]\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for InfluxDB Cloud 2 Configuration\nDESCRIPTION: Shell script defining environment variables for Telegraf to connect to InfluxDB Cloud 2, with AWS West region example.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# For AWS West (Oregon)\nINFLUX_HOST=\"https://us-west-2-1.aws.cloud2.influxdata.com\"\n# Other Cloud URLs at https://v2.docs.influxdata.com/v2.0/reference/urls/#influxdb-cloud-urls\nINFLUX_TOKEN=\"replace_with_your_token\"\nINFLUX_ORG=\"yourname@yourcompany.com\"\nINFLUX_BUCKET=\"replace_with_your_bucket_name\"\n```\n\n----------------------------------------\n\nTITLE: Sample HAProxy Metrics Output\nDESCRIPTION: Example output showing the metrics collected by the HAProxy input plugin, including various HTTP response codes, connection stats, and proxy status information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/haproxy/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhaproxy,server=/run/haproxy/admin.sock,proxy=public,sv=FRONTEND,type=frontend http_response.other=0i,req_rate_max=1i,comp_byp=0i,status=\"OPEN\",rate_lim=0i,dses=0i,req_rate=0i,comp_rsp=0i,bout=9287i,comp_in=0i,mode=\"http\",smax=1i,slim=2000i,http_response.1xx=0i,conn_rate=0i,dreq=0i,ereq=0i,iid=2i,rate_max=1i,http_response.2xx=1i,comp_out=0i,intercepted=1i,stot=2i,pid=1i,http_response.5xx=1i,http_response.3xx=0i,http_response.4xx=0i,conn_rate_max=1i,conn_tot=2i,dcon=0i,bin=294i,rate=0i,sid=0i,req_tot=2i,scur=0i,dresp=0i 1513293519000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon CloudWatch Output Plugin for Telegraf\nDESCRIPTION: This TOML configuration snippet defines the settings for the Amazon CloudWatch output plugin in Telegraf. It includes options for specifying AWS credentials, region, namespace, and various CloudWatch-specific settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/cloudwatch/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for AWS CloudWatch output.\n[[outputs.cloudwatch]]\n  ## Amazon REGION\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and\n  ##    web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Set http_proxy\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## Namespace for the CloudWatch MetricDatums\n  namespace = \"InfluxData/Telegraf\"\n\n  ## If you have a large amount of metrics, you should consider to send\n  ## statistic values instead of raw metrics which could not only improve\n  ## performance but also save AWS API cost. If enable this flag, this plugin\n  ## would parse the required CloudWatch statistic fields (count, min, max, and\n  ## sum) and send them to CloudWatch. You could use basicstats aggregator to\n  ## calculate those fields. If not all statistic fields are available, all\n  ## fields would still be sent as raw metrics.\n  # write_statistics = false\n\n  ## Enable high resolution metrics of 1 second (if not enabled, standard\n  ## resolution are of 60 seconds precision)\n  # high_resolution_metrics = false\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU and Disk Inputs with Tagpass and Tagdrop in Telegraf\nDESCRIPTION: This snippet demonstrates how to use tagpass and tagdrop filters with CPU and disk inputs in Telegraf. It excludes specific CPU cores and filters disk metrics based on filesystem type and path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  fieldexclude = [\"cpu_time\"]\n  # Don't collect CPU data for cpu6 & cpu7\n  [inputs.cpu.tagdrop]\n    cpu = [ \"cpu6\", \"cpu7\" ]\n\n[[inputs.disk]]\n  [inputs.disk.tagpass]\n    # tagpass conditions are OR, not AND.\n    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)\n    # then the metric passes\n    fstype = [ \"ext4\", \"xfs\" ]\n    # Globs can also be used on the tag values\n    path = [ \"/opt\", \"/home*\" ]\n\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"Network Interface\"\n    Instances = [\"*\"]\n    Counters = [\n      \"Bytes Received/sec\",\n      \"Bytes Sent/sec\"\n    ]\n    Measurement = \"win_net\"\n  # Do not send metrics where the Windows interface name (instance) begins with\n  # 'isatap' or 'Local'\n  [inputs.win_perf_counters.tagdrop]\n    instance = [\"isatap*\", \"Local*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Storage Queue Input Plugin in TOML\nDESCRIPTION: Sample configuration for the Azure Storage Queue input plugin. It requires an account name and access key, and includes an optional setting to disable peeking at the oldest message age for improved performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/azure_storage_queue/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather Azure Storage Queue metrics\n[[inputs.azure_storage_queue]]\n  ## Azure Storage Account name and shared access key (required)\n  account_name = \"mystorageaccount\"\n  account_key = \"storageaccountaccesskey\"\n\n  ## Disable peeking age of oldest message (faster)\n  # peek_oldest_message_age = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon Timestream Output in Telegraf\nDESCRIPTION: Configuration template for the Timestream output plugin including authentication, database settings, data organization modes, and table management options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/timestream/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for sending metrics to Amazon Timestream.\n[[outputs.timestream]]\n  ## Amazon Region\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order:\n  ## 1) Web identity provider credentials via STS if role_arn and\n  ##    web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Timestream database where the metrics will be inserted.\n  ## The database must exist prior to starting Telegraf.\n  database_name = \"yourDatabaseNameHere\"\n\n  ## Specifies if the plugin should describe the Timestream database upon\n  ## starting to validate if it has access, necessary permissions, connection,\n  ## etc., as a safety check. If the describe operation fails, the plugin will\n  ## not start and therefore the Telegraf agent will not start.\n  describe_database_on_start = false\n\n  ## Specifies how the data is organized in Timestream.\n  ## Valid values are: single-table, multi-table.\n  ## When mapping_mode is set to single-table, all of the data is stored in a\n  ## single table. When mapping_mode is set to multi-table, the data is\n  ## organized and stored in multiple tables. The default is multi-table.\n  mapping_mode = \"multi-table\"\n\n  ## Specifies if the plugin should create the table, if it doesn't exist.\n  create_table_if_not_exists = true\n\n  ## Specifies the Timestream table magnetic store retention period in days.\n  ## Check Timestream documentation for more details.\n  ## NOTE: This property is valid when create_table_if_not_exists = true.\n  create_table_magnetic_store_retention_period_in_days = 365\n\n  ## Specifies the Timestream table memory store retention period in hours.\n  ## Check Timestream documentation for more details.\n  ## NOTE: This property is valid when create_table_if_not_exists = true.\n  create_table_memory_store_retention_period_in_hours = 24\n\n  ## Specifies how the data is written into Timestream.\n  ## Valid values are: true, false\n  ## When use_multi_measure_records is set to true, all of the tags and fields\n  ## are stored as a single row in a Timestream table.\n  ## When use_multi_measure_record is set to false, Timestream stores each field\n  ## in a separate table row, thereby storing the tags multiple times (once for\n  ## each field). The recommended setting is true. The default is false.\n  use_multi_measure_records = \"false\"\n\n  ## Specifies the measure_name to use when sending multi-measure records.\n  ## NOTE: This property is valid when use_multi_measure_records=true and\n  ## mapping_mode=multi-table\n  measure_name_for_multi_measure_records = \"telegraf_measure\"\n\n  ## Specifies the name of the table to write data into\n  ## NOTE: This property is valid when mapping_mode=single-table.\n  # single_table_name = \"\"\n\n  ## Specifies the name of dimension when all of the data is being stored in a\n  ## single table and the measurement name is transformed into the dimension\n  ## value (see Mapping data from Influx to Timestream for details)\n  ## NOTE: This property is valid when mapping_mode=single-table.\n  # single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n\n  ## Only valid and optional if create_table_if_not_exists = true\n  ## Specifies the Timestream table tags.\n  ## Check Timestream documentation for more details\n  # create_table_tags = { \"foo\" = \"bar\", \"environment\" = \"dev\"}\n\n  ## Specify the maximum number of parallel go routines to ingest/write data\n  ## If not specified, defaulted to 1 go routines\n  max_write_go_routines = 25\n```\n\n----------------------------------------\n\nTITLE: Configuring Redfish Input Plugin in TOML\nDESCRIPTION: TOML configuration for the Redfish input plugin in Telegraf. It specifies the API endpoint, credentials, system ID, and various optional settings for metric collection and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/redfish/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read CPU, Fans, Powersupply and Voltage metrics of hardware server through redfish APIs\n[[inputs.redfish]]\n  ## Redfish API Base URL.\n  address = \"https://127.0.0.1:5000\"\n\n  ## Credentials for the Redfish API. Can also use secrets.\n  username = \"root\"\n  password = \"password123456\"\n\n  ## System Id to collect data for in Redfish APIs.\n  computer_system_id=\"System.Embedded.1\"\n\n  ## Metrics to collect\n  ## The metric collects to gather. Choose from \"power\" and \"thermal\".\n  # include_metrics = [\"power\", \"thermal\"]\n\n  ## Tag sets allow you to include redfish OData link parent data\n  ## For Example.\n  ## Thermal data is an OData link with parent Chassis which has a link of Location.\n  ## For more info see the Redfish Resource and Schema Guide at DMTFs website.\n  ## Available sets are: \"chassis.location\" and \"chassis\"\n  # include_tag_sets = [\"chassis.location\"]\n\n  ## Workarounds\n  ## Defines workarounds for certain hardware vendors. Choose from:\n  ## * ilo4-thermal - Do not pass 0Data-Version header to Thermal endpoint\n  # workarounds = []\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Basic Telegraf Usage with Config File\nDESCRIPTION: Shows how to run Telegraf with a configuration file. This is the minimum required command to start Telegraf with your specified plugins.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/COMMANDS_AND_FLAGS.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf --config config.toml\n```\n\n----------------------------------------\n\nTITLE: Launching Telegraf Docker Container\nDESCRIPTION: Docker command to run Telegraf with a mounted configuration file. The --rm flag automatically removes the container when it exits, and the volume mount maps the local config file to the container's configuration path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/QUICK_START.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndocker run --rm --volume $PWD/config.toml:/etc/telegraf/telegraf.conf telegraf\n```\n\n----------------------------------------\n\nTITLE: Configuring Kinesis Consumer Input Plugin in TOML\nDESCRIPTION: Complete configuration example for the AWS Kinesis Consumer input plugin, including AWS credentials, stream settings, polling intervals, and DynamoDB checkpoint configuration options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kinesis_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.kinesis_consumer]]\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # web_identity_token_file = \"\"\n  # role_session_name = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n\n  ## Shard iterator type\n  ## Available options: 'TRIM_HORIZON' (first in non-expired) and 'LATEST'\n  # shard_iterator_type = \"TRIM_HORIZON\"\n\n  ## Interval for checking for new records\n  ## Please consider limits for getting records documented here:\n  ## https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html\n  # poll_interval = \"250ms\"\n\n  ## Interval for scanning for new shards created when resharding\n  ## If set to zero, shards are only scanned once on startup.\n  # shard_update_interval = \"30s\"\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Content encoding of the record data\n  ## If you are processing a cloudwatch logs kinesis stream then set this to\n  ## \"gzip\" as AWS compresses cloudwatch log data before it is sent to kinesis.\n  # content_encoding = \"identity\"\n\n  ## Data format of the records to consume\n  ## See https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n  ## Optional: Configuration for DynamoDB backend to store positions in the stream\n  # [inputs.kinesis_consumer.checkpoint_dynamodb]\n  #   ## Unique name for this consumer\n  #   app_name = \"default\"\n  #   ## Table to store the sequence numbers in\n  #   table_name = \"default\"\n  #   ## Interval for persisting data to limit write operations\n  #   # interval = \"10s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the SNMP input plugin in Telegraf. It includes options for specifying SNMP agents, timeout, version, community string, authentication, and defining fields and tables for data collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Retrieves SNMP values from remote agents\n[[inputs.snmp]]\n  ## Agent addresses to retrieve values from.\n  ##   format:  agents = [\"<scheme://><hostname>:<port>\"]\n  ##   scheme:  optional, either udp, udp4, udp6, tcp, tcp4, tcp6.\n  ##            default is udp\n  ##   port:    optional\n  ##   example: agents = [\"udp://127.0.0.1:161\"]\n  ##            agents = [\"tcp://127.0.0.1:161\"]\n  ##            agents = [\"udp4://v4only-snmp-agent\"]\n  agents = [\"udp://127.0.0.1:161\"]\n\n  ## Timeout for each request.\n  # timeout = \"5s\"\n\n  ## SNMP version; can be 1, 2, or 3.\n  # version = 2\n\n  ## Unconnected UDP socket\n  ## When true, SNMP responses are accepted from any address not just\n  ## the requested address. This can be useful when gathering from\n  ## redundant/failover systems.\n  # unconnected_udp_socket = false\n\n  ## Path to mib files\n  ## Used by the gosmi translator.\n  ## To add paths when translating with netsnmp, use the MIBDIRS environment variable\n  # path = [\"/usr/share/snmp/mibs\"]\n\n  ## SNMP community string.\n  # community = \"public\"\n\n  ## Agent host tag; should be set to \"source\" for consistent usage across plugins\n  ##   example: agent_host_tag = \"source\"\n  ## The default value is inconsistent with other plugins. Users will get a\n  ## warning that can be ignored if this is not changed. However, to have a\n  ## consistent experience, set this to \"source\" in your config to align with\n  ## other plugins.\n  # agent_host_tag = \"agent_host\"\n\n  ## Number of retries to attempt.\n  # retries = 3\n\n  ## The GETBULK max-repetitions parameter.\n  # max_repetitions = 10\n\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\", \"SHA224\", \"SHA256\", \"SHA384\", \"SHA512\" or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Context Name.\n  # context_name = \"\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\", or \"\".\n  ### Protocols \"AES192\", \"AES192\", \"AES256\", and \"AES256C\" require the underlying net-snmp tools\n  ### to be compiled with --enable-blumenthal-aes (http://www.net-snmp.org/docs/INSTALL.html)\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n\n  ## Add fields and tables defining the variables you wish to collect.  This\n  ## example collects the system uptime and interface variables.  Reference the\n  ## full plugin documentation for configuration details.\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysUpTime.0\"\n    name = \"sysUptime\"\n    conversion = \"float(2)\"\n\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysName.0\"\n    name = \"sysName\"\n    is_tag = true\n\n  [[inputs.snmp.table]]\n    oid = \"IF-MIB::ifTable\"\n    name = \"interface\"\n    inherit_tags = [\"sysName\"]\n\n    [[inputs.snmp.table.field]]\n      oid = \"IF-MIB::ifDescr\"\n      name = \"ifDescr\"\n      is_tag = true\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the MongoDB output plugin for Telegraf. It includes options for connection string, authentication, database selection, time series granularity, and document expiration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mongodb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin that can transmit logs to mongodb\n[[outputs.mongodb]]\n  # connection string examples for mongodb\n  dsn = \"mongodb://localhost:27017\"\n  # dsn = \"mongodb://mongod1:27017,mongod2:27017,mongod3:27017/admin&replicaSet=myReplSet&w=1\"\n\n  # overrides serverSelectionTimeoutMS in dsn if set\n  # timeout = \"30s\"\n\n  # default authentication, optional\n  # authentication = \"NONE\"\n\n  # for SCRAM-SHA-256 authentication\n  # authentication = \"SCRAM\"\n  # username = \"root\"\n  # password = \"***\"\n\n  # for x509 certificate authentication\n  # authentication = \"X509\"\n  # tls_ca = \"ca.pem\"\n  # tls_key = \"client.pem\"\n  # # tls_key_pwd = \"changeme\" # required for encrypted tls_key\n  # insecure_skip_verify = false\n\n  # database to store measurements and time series collections\n  # database = \"telegraf\"\n\n  # granularity can be seconds, minutes, or hours.\n  # configuring this value will be based on your input collection frequency.\n  # see https://docs.mongodb.com/manual/core/timeseries-collections/#create-a-time-series-collection\n  # granularity = \"seconds\"\n\n  # optionally set a TTL to automatically expire documents from the measurement collections.\n  # ttl = \"360h\"\n```\n\n----------------------------------------\n\nTITLE: MQTT Consumer with Field Pivoting Using Pivot Processor\nDESCRIPTION: Configuration example showing how to use the pivot processor with MQTT Consumer to transform single-valued metrics into multi-field metrics. Useful for handling multiple sensor readings from different topics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mqtt_consumer/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.mqtt_consumer]]\n    .....\n    topics = \"/sensors/#\"\n    [[inputs.mqtt_consumer.topic_parsing]]\n        measurement = \"/measurement/_/_/_/_\"\n        tags = \"/_/site/version/device_name/field\"\n[[processors.pivot]]\n    tag_key = \"field\"\n    value_key = \"value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sysstat Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the Sysstat Input Plugin that collects system metrics via sysstat utilities. The configuration includes paths to sadc/sadf commands, activity selection, metric grouping options, and detailed metric collection settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sysstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Sysstat metrics collector\n# This plugin ONLY supports Linux\n[[inputs.sysstat]]\n  ## Path to the sadc command.\n  #\n  ## Common Defaults:\n  ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n  ##   Arch:          /usr/lib/sa/sadc\n  ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n\n  ## Path to the sadf command, if it is not in PATH\n  # sadf_path = \"/usr/bin/sadf\"\n\n  ## Activities is a list of activities, that are passed as argument to the\n  ## sadc collector utility (e.g: DISK, SNMP etc...)\n  ## The more activities that are added, the more data is collected.\n  # activities = [\"DISK\"]\n\n  ## Group metrics to measurements.\n  ##\n  ## If group is false each metric will be prefixed with a description\n  ## and represents itself a measurement.\n  ##\n  ## If Group is true, corresponding metrics are grouped to a single measurement.\n  # group = true\n\n  ## Options for the sadf command. The values on the left represent the sadf options and\n  ## the values on the right their description (which are used for grouping and prefixing metrics).\n  ##\n  ## Run 'sar -h' or 'man sar' to find out the supported options for your sysstat version.\n  [inputs.sysstat.options]\n    -C = \"cpu\"\n    -B = \"paging\"\n    -b = \"io\"\n    -d = \"disk\"             # requires DISK activity\n    \"-n ALL\" = \"network\"\n    \"-P ALL\" = \"per_cpu\"\n    -q = \"queue\"\n    -R = \"mem\"\n    -r = \"mem_util\"\n    -S = \"swap_util\"\n    -u = \"cpu_util\"\n    -v = \"inode\"\n    -W = \"swap\"\n    -w = \"task\"\n  # -H = \"hugepages\"        # only available for newer linux distributions\n  # \"-I ALL\" = \"interrupts\" # requires INT activity\n\n  ## Device tags can be used to add additional tags for devices. For example the configuration below\n  ## adds a tag vg with value rootvg for all metrics with sda devices.\n  # [[inputs.sysstat.device_tags.sda]]\n  #  vg = \"rootvg\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Plugins with Custom Flush Parameters in TOML\nDESCRIPTION: Example configuration for customizing flush parameters on specific outputs in Telegraf. Shows how to set global agent parameters and override them for individual outputs, such as the 'file' output.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n[agent]\n  flush_interval = \"10s\"\n  flush_jitter = \"5s\"\n  metric_batch_size = 1000\n\n[[outputs.influxdb]]\n  urls = [ \"http://example.org:8086\" ]\n  database = \"telegraf\"\n\n[[outputs.file]]\n  files = [ \"stdout\" ]\n  flush_interval = \"1s\"\n  flush_jitter = \"1s\"\n  metric_batch_size = 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud PubSub Push Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Google Cloud PubSub Push HTTP listener plugin. Includes options for service address, security tokens, timeouts, TLS settings, and message handling parameters. The plugin acts as an endpoint for receiving push messages from Google Cloud PubSub service.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloud_pubsub_push/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cloud_pubsub_push]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Application secret to verify messages originate from Cloud Pub/Sub\n  # token = \"\"\n\n  ## Path to listen to.\n  # path = \"/\"\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response. This should be\n  ## set to a value large enough that you can send at least 'metric_batch_size'\n  ## number of messages within the duration.\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Whether to add the pubsub metadata, such as message attributes and\n  ## subscription as a tag.\n  # add_meta = false\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Parser Version 2 in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the JSON Parser Version 2 plugin for Telegraf. It includes options for defining measurement names, timestamps, tags, fields, and object parsing rules.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json_v2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n    urls = []\n    data_format = \"json_v2\"\n    [[inputs.file.json_v2]]\n        measurement_name = \"\" # A string that will become the new measurement name\n        measurement_name_path = \"\" # A string with valid GJSON path syntax, will override measurement_name\n        timestamp_path = \"\" # A string with valid GJSON path syntax to a valid timestamp (single value)\n        timestamp_format = \"\" # A string with a valid timestamp format (see below for possible values)\n        timestamp_timezone = \"\" # A string with a valid timezone (see below for possible values)\n        [[inputs.file.json_v2.tag]]\n            path = \"\" # A string with valid GJSON path syntax to a non-array/non-object value\n            rename = \"new name\" # A string with a new name for the tag key\n            ## Setting optional to true will suppress errors if the configured Path doesn't match the JSON\n            optional = false\n        [[inputs.file.json_v2.field]]\n            path = \"\" # A string with valid GJSON path syntax to a non-array/non-object value\n            rename = \"new name\" # A string with a new name for the tag key\n            type = \"int\" # A string specifying the type (int,uint,float,string,bool)\n            ## Setting optional to true will suppress errors if the configured Path doesn't match the JSON\n            optional = false\n        [[inputs.file.json_v2.object]]\n            path = \"\" # A string with valid GJSON path syntax, can include array's and object's\n\n            ## Setting optional to true will suppress errors if the configured Path doesn't match the JSON\n            optional = false\n\n            ## Configuration to define what JSON keys should be used as timestamps ##\n            timestamp_key = \"\" # A JSON key (for a nested key, prepend the parent keys with underscores) to a valid timestamp\n            timestamp_format = \"\" # A string with a valid timestamp format (see below for possible values)\n            timestamp_timezone = \"\" # A string with a valid timezone (see below for possible values)\n\n            ### Configuration to define what JSON keys should be included and how (field/tag) ###\n            tags = [] # List of JSON keys (for a nested key, prepend the parent keys with underscores) to be a tag instead of a field, when adding a JSON key in this list you don't have to define it in the included_keys list\n            included_keys = [] # List of JSON keys (for a nested key, prepend the parent keys with underscores) that should be only included in result\n            excluded_keys = [] # List of JSON keys (for a nested key, prepend the parent keys with underscores) that shouldn't be included in result\n            # When a tag/field sub-table is defined, they will be the only field/tag's along with any keys defined in the included_keys list.\n            # If the resulting values aren't included in the object/array returned by the root object path, it won't be included.\n            # You can define as many tag/field sub-tables as you want.\n            [[inputs.file.json_v2.object.tag]]\n                path = \"\" # # A string with valid GJSON path syntax, can include array's and object's\n                rename = \"new name\" # A string with a new name for the tag key\n            [[inputs.file.json_v2.object.field]]\n                path = \"\" # # A string with valid GJSON path syntax, can include array's and object's\n                rename = \"new name\" # A string with a new name for the tag key\n                type = \"int\" # A string specifying the type (int,uint,float,string,bool)\n\n            ### Configuration to modify the resulting line protocol ###\n            disable_prepend_keys = false (or true, just not both)\n            [inputs.file.json_v2.object.renames] # A map of JSON keys (for a nested key, prepend the parent keys with underscores) with a new name for the tag key\n                key = \"new name\"\n            [inputs.file.json_v2.object.fields] # A map of JSON keys (for a nested key, prepend the parent keys with underscores) with a type (int,uint,float,string,bool)\n                key = \"int\"\n```\n\n----------------------------------------\n\nTITLE: MongoDB Telegraf Metrics Output\nDESCRIPTION: Example output showing MongoDB metrics in InfluxDB line protocol format. Includes server statistics, database statistics, collection statistics, shard statistics and operation timing metrics. Each line represents a different measurement with various fields and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nmongodb,hostname=127.0.0.1:27017 active_reads=1i,active_writes=0i,aggregate_command_failed=0i,aggregate_command_total=0i,assert_msg=0i,assert_regular=0i,assert_rollovers=0i,assert_user=0i,assert_warning=0i,available_reads=127i,available_writes=128i,commands=65i,commands_per_sec=4i,connections_available=51199i,connections_current=1i,connections_total_created=5i,count_command_failed=0i,count_command_total=7i,cursor_no_timeout=0i,cursor_no_timeout_count=0i,cursor_pinned=0i,cursor_pinned_count=0i,cursor_timed_out=0i,cursor_timed_out_count=0i,cursor_total=0i,cursor_total_count=0i,delete_command_failed=0i,delete_command_total=1i,deletes=1i,deletes_per_sec=0i,distinct_command_failed=0i,distinct_command_total=0i,document_deleted=0i,document_inserted=0i,document_returned=0i,document_updated=0i,find_and_modify_command_failed=0i,find_and_modify_command_total=0i,find_command_failed=0i,find_command_total=1i,flushes=52i,flushes_per_sec=0i,flushes_total_time_ns=364000000i,get_more_command_failed=0i,get_more_command_total=0i,getmores=0i,getmores_per_sec=0i,insert_command_failed=0i,insert_command_total=0i,inserts=0i,inserts_per_sec=0i,jumbo_chunks=0i,latency_commands=5740i,latency_commands_count=46i,latency_reads=348i,latency_reads_count=7i,latency_writes=0i,latency_writes_count=0i,net_in_bytes=296i,net_in_bytes_count=4262i,net_out_bytes=29322i,net_out_bytes_count=242103i,open_connections=1i,operation_scan_and_order=0i,operation_write_conflicts=0i,page_faults=1i,percent_cache_dirty=0,percent_cache_used=0,queries=1i,queries_per_sec=0i,queued_reads=0i,queued_writes=0i,resident_megabytes=33i,storage_freelist_search_bucket_exhausted=0i,storage_freelist_search_requests=0i,storage_freelist_search_scanned=0i,tcmalloc_central_cache_free_bytes=0i,tcmalloc_current_allocated_bytes=0i,tcmalloc_current_total_thread_cache_bytes=0i,tcmalloc_heap_size=0i,tcmalloc_max_total_thread_cache_bytes=0i,tcmalloc_pageheap_commit_count=0i,tcmalloc_pageheap_committed_bytes=0i,tcmalloc_pageheap_decommit_count=0i,tcmalloc_pageheap_free_bytes=0i,tcmalloc_pageheap_reserve_count=0i,tcmalloc_pageheap_scavenge_count=0i,tcmalloc_pageheap_total_commit_bytes=0i,tcmalloc_pageheap_total_decommit_bytes=0i,tcmalloc_pageheap_total_reserve_bytes=0i,tcmalloc_pageheap_unmapped_bytes=0i,tcmalloc_spinlock_total_delay_ns=0i,tcmalloc_thread_cache_free_bytes=0i,tcmalloc_total_free_bytes=0i,tcmalloc_transfer_cache_free_bytes=0i,total_available=0i,total_created=0i,total_docs_scanned=0i,total_in_use=0i,total_keys_scanned=0i,total_refreshing=0i,total_tickets_reads=128i,total_tickets_writes=128i,ttl_deletes=0i,ttl_deletes_per_sec=0i,ttl_passes=51i,ttl_passes_per_sec=0i,update_command_failed=0i,update_command_total=0i,updates=0i,updates_per_sec=0i,uptime_ns=6135152000000i,version=\"4.0.19\",vsize_megabytes=5088i,wt_connection_files_currently_open=13i,wt_data_handles_currently_active=18i,wtcache_app_threads_page_read_count=99i,wtcache_app_threads_page_read_time=44528i,wtcache_app_threads_page_write_count=19i,wtcache_bytes_read_into=3248195i,wtcache_bytes_written_from=170612i,wtcache_current_bytes=3648788i,wtcache_internal_pages_evicted=0i,wtcache_max_bytes_configured=8053063680i,wtcache_modified_pages_evicted=0i,wtcache_pages_evicted_by_app_thread=0i,wtcache_pages_queued_for_eviction=0i,wtcache_pages_read_into=234i,wtcache_pages_requested_from=18235i,wtcache_server_evicting_pages=0i,wtcache_tracked_dirty_bytes=0i,wtcache_unmodified_pages_evicted=0i,wtcache_worker_thread_evictingpages=0i 1595691605000000000\nmongodb,hostname=127.0.0.1:27017,node_type=PRI,rs_name=rs0 active_reads=1i,active_writes=0i,aggregate_command_failed=0i,aggregate_command_total=0i,assert_msg=0i,assert_regular=0i,assert_rollovers=0i,assert_user=25i,assert_warning=0i,available_reads=127i,available_writes=128i,commands=345i,commands_per_sec=4i,connections_available=838853i,connections_current=7i,connections_total_created=13i,count_command_failed=0i,count_command_total=5i,cursor_no_timeout=0i,cursor_no_timeout_count=0i,cursor_pinned=0i,cursor_pinned_count=2i,cursor_timed_out=0i,cursor_timed_out_count=0i,cursor_total=0i,cursor_total_count=4i,delete_command_failed=0i,delete_command_total=0i,deletes=0i,deletes_per_sec=0i,distinct_command_failed=0i,distinct_command_total=0i,document_deleted=0i,document_inserted=2i,document_returned=56i,document_updated=0i,find_and_modify_command_failed=0i,find_and_modify_command_total=0i,find_command_failed=0i,find_command_total=23i,flushes=4i,flushes_per_sec=0i,flushes_total_time_ns=43000000i,get_more_command_failed=0i,get_more_command_total=88i,getmores=88i,getmores_per_sec=0i,insert_command_failed=0i,insert_command_total=2i,inserts=2i,inserts_per_sec=0i,jumbo_chunks=0i,latency_commands=82532i,latency_commands_count=337i,latency_reads=30633i,latency_reads_count=111i,latency_writes=0i,latency_writes_count=0i,member_status=\"PRI\",net_in_bytes=636i,net_in_bytes_count=172300i,net_out_bytes=38849i,net_out_bytes_count=335459i,open_connections=7i,operation_scan_and_order=1i,operation_write_conflicts=0i,page_faults=1i,percent_cache_dirty=0,percent_cache_used=0,queries=23i,queries_per_sec=2i,queued_reads=0i,queued_writes=0i,repl_apply_batches_num=0i,repl_apply_batches_total_millis=0i,repl_apply_ops=0i,repl_buffer_count=0i,repl_buffer_size_bytes=0i,repl_commands=0i,repl_commands_per_sec=0i,repl_deletes=0i,repl_deletes_per_sec=0i,repl_executor_pool_in_progress_count=0i,repl_executor_queues_network_in_progress=0i,repl_executor_queues_sleepers=3i,repl_executor_unsignaled_events=0i,repl_getmores=0i,repl_getmores_per_sec=0i,repl_inserts=0i,repl_inserts_per_sec=0i,repl_lag=0i,repl_network_bytes=0i,repl_network_getmores_num=0i,repl_network_getmores_total_millis=0i,repl_network_ops=0i,repl_oplog_window_sec=140i,repl_queries=0i,repl_queries_per_sec=0i,repl_state=1i,repl_updates=0i,repl_updates_per_sec=0i,resident_megabytes=81i,state=\"PRIMARY\",storage_freelist_search_bucket_exhausted=0i,storage_freelist_search_requests=0i,storage_freelist_search_scanned=0i,tcmalloc_central_cache_free_bytes=322128i,tcmalloc_current_allocated_bytes=143566680i,tcmalloc_current_total_thread_cache_bytes=1098968i,tcmalloc_heap_size=181317632i,tcmalloc_max_total_thread_cache_bytes=260046848i,tcmalloc_pageheap_commit_count=53i,tcmalloc_pageheap_committed_bytes=149106688i,tcmalloc_pageheap_decommit_count=1i,tcmalloc_pageheap_free_bytes=3244032i,tcmalloc_pageheap_reserve_count=51i,tcmalloc_pageheap_scavenge_count=1i,tcmalloc_pageheap_total_commit_bytes=183074816i,tcmalloc_pageheap_total_decommit_bytes=33968128i,tcmalloc_pageheap_total_reserve_bytes=181317632i,tcmalloc_pageheap_unmapped_bytes=32210944i,tcmalloc_spinlock_total_delay_ns=0i,tcmalloc_thread_cache_free_bytes=1098968i,tcmalloc_total_free_bytes=2295976i,tcmalloc_transfer_cache_free_bytes=874880i,total_available=0i,total_created=0i,total_docs_scanned=56i,total_in_use=0i,total_keys_scanned=2i,total_refreshing=0i,total_tickets_reads=128i,total_tickets_writes=128i,ttl_deletes=0i,ttl_deletes_per_sec=0i,ttl_passes=2i,ttl_passes_per_sec=0i,update_command_failed=0i,update_command_total=0i,updates=0i,updates_per_sec=0i,uptime_ns=166481000000i,version=\"4.0.19\",vsize_megabytes=1482i,wt_connection_files_currently_open=26i,wt_data_handles_currently_active=44i,wtcache_app_threads_page_read_count=0i,wtcache_app_threads_page_read_time=0i,wtcache_app_threads_page_write_count=56i,wtcache_bytes_read_into=0i,wtcache_bytes_written_from=130403i,wtcache_current_bytes=100312i,wtcache_internal_pages_evicted=0i,wtcache_max_bytes_configured=506462208i,wtcache_modified_pages_evicted=0i,wtcache_pages_evicted_by_app_thread=0i,wtcache_pages_queued_for_eviction=0i,wtcache_pages_read_into=0i,wtcache_pages_requested_from=2085i,wtcache_server_evicting_pages=0i,wtcache_tracked_dirty_bytes=63929i,wtcache_unmodified_pages_evicted=0i,wtcache_worker_thread_evictingpages=0i 1595691605000000000\nmongodb_db_stats,db_name=admin,hostname=127.0.0.1:27017 avg_obj_size=241,collections=2i,data_size=723i,index_size=49152i,indexes=3i,num_extents=0i,objects=3i,ok=1i,storage_size=53248i,type=\"db_stat\" 1547159491000000000\nmongodb_db_stats,db_name=local,hostname=127.0.0.1:27017 avg_obj_size=813.9705882352941,collections=6i,data_size=55350i,index_size=102400i,indexes=5i,num_extents=0i,objects=68i,ok=1i,storage_size=204800i,type=\"db_stat\" 1547159491000000000\nmongodb_col_stats,collection=foo,db_name=local,hostname=127.0.0.1:27017 size=375005928i,avg_obj_size=5494,type=\"col_stat\",storage_size=249307136i,total_index_size=2138112i,ok=1i,count=68251i 1547159491000000000\nmongodb_shard_stats,hostname=127.0.0.1:27017,in_use=3i,available=3i,created=4i,refreshing=0i 1522799074000000000\nmongodb_top_stats,collection=foo,total_time=1471,total_count=158,read_lock_time=49614,read_lock_count=657,write_lock_time=49125456,write_lock_count=9841,queries_time=174,queries_count=495,get_more_time=498,get_more_count=46,insert_time=2651,insert_count=1265,update_time=0,update_count=0,remove_time=0,remove_count=0,commands_time=498611,commands_count=4615\n```\n\n----------------------------------------\n\nTITLE: Using namepass Filter with Aggregator Plugins in Telegraf\nDESCRIPTION: Example showing how to use the namepass filter to selectively apply an aggregator to specific metrics. The minmax aggregator processes only swap metrics while ignoring system load metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_19\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.swap]]\n\n[[inputs.system]]\n  fieldinclude = [\"load1\"] # collects system load1 metric.\n\n[[aggregators.minmax]]\n  period = \"30s\"        # send & clear the aggregate every 30s.\n  drop_original = true  # drop the original metrics.\n  namepass = [\"swap\"]   # only \"pass\" swap metrics through the aggregator.\n\n[[outputs.file]]\n  files = [\"stdout\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring the Reverse DNS Processor Plugin in Telegraf\nDESCRIPTION: Complete configuration example for the reverse_dns processor showing all available options including cache settings, timeouts, and lookup configurations for both fields and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/reverse_dns/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# ReverseDNS does a reverse lookup on IP addresses to retrieve the DNS name\n[[processors.reverse_dns]]\n  ## For optimal performance, you may want to limit which metrics are passed to this\n  ## processor. eg:\n  ## namepass = [\"my_metric_*\"]\n\n  ## cache_ttl is how long the dns entries should stay cached for.\n  ## generally longer is better, but if you expect a large number of diverse lookups\n  ## you'll want to consider memory use.\n  cache_ttl = \"24h\"\n\n  ## lookup_timeout is how long should you wait for a single dns request to respond.\n  ## this is also the maximum acceptable latency for a metric travelling through\n  ## the reverse_dns processor. After lookup_timeout is exceeded, a metric will\n  ## be passed on unaltered.\n  ## multiple simultaneous resolution requests for the same IP will only make a\n  ## single rDNS request, and they will all wait for the answer for this long.\n  lookup_timeout = \"3s\"\n\n  ## max_parallel_lookups is the maximum number of dns requests to be in flight\n  ## at the same time. Requesting hitting cached values do not count against this\n  ## total, and neither do mulptiple requests for the same IP.\n  ## It's probably best to keep this number fairly low.\n  max_parallel_lookups = 10\n\n  ## ordered controls whether or not the metrics need to stay in the same order\n  ## this plugin received them in. If false, this plugin will change the order\n  ## with requests hitting cached results moving through immediately and not\n  ## waiting on slower lookups. This may cause issues for you if you are\n  ## depending on the order of metrics staying the same. If so, set this to true.\n  ## keeping the metrics ordered may be slightly slower.\n  ordered = false\n\n  [[processors.reverse_dns.lookup]]\n    ## get the ip from the field \"source_ip\", and put the result in the field \"source_name\"\n    field = \"source_ip\"\n    dest = \"source_name\"\n\n  [[processors.reverse_dns.lookup]]\n    ## get the ip from the tag \"destination_ip\", and put the result in the tag\n    ## \"destination_name\".\n    tag = \"destination_ip\"\n    dest = \"destination_name\"\n\n    ## If you would prefer destination_name to be a field instead, you can use a\n    ## processors.converter after this one, specifying the order attribute.\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Response Input Plugin in Telegraf\nDESCRIPTION: Complete configuration template for the HTTP response input plugin showing all available options including URL settings, proxy configuration, authentication, TLS settings, and cookie authentication parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/http_response/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# HTTP/HTTPS request given an address a method and a timeout\n[[inputs.http_response]]\n  ## List of urls to query.\n  # urls = [\"http://localhost\"]\n\n  ## Set http_proxy.\n  ## Telegraf uses the system wide proxy settings if it's is not set.\n  # http_proxy = \"http://localhost:8888\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## HTTP Request Method\n  # method = \"GET\"\n\n  ## Whether to follow redirects from the server (defaults to false)\n  # follow_redirects = false\n\n  ## Optional file with Bearer token\n  ## file content is added as an Authorization header\n  # bearer_token = \"/path/to/file\"\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional HTTP Request Body\n  # body = '''\n  # {'fake':'data'}\n  # '''\n\n  ## Optional HTTP Request Body Form\n  ## Key value pairs to encode and set at URL form. Can be used with the POST\n  ## method + application/x-www-form-urlencoded content type to replicate the\n  ## POSTFORM method.\n  # body_form = { \"key\": \"value\" }\n\n  ## Optional name of the field that will contain the body of the response.\n  ## By default it is set to an empty String indicating that the body's\n  ## content won't be added\n  # response_body_field = ''\n\n  ## Maximum allowed HTTP response body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  ## If the response body size exceeds this limit a \"body_read_error\" will\n  ## be raised.\n  # response_body_max_size = \"32MiB\"\n\n  ## Optional substring or regex match in body of the response (case sensitive)\n  # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n  # response_string_match = \"ok\"\n  # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n\n  ## Expected response status code.\n  ## The status code of the response is compared to this value. If they match,\n  ## the field \"response_status_code_match\" will be 1, otherwise it will be 0.\n  ## If the expected status code is 0, the check is disabled and the field\n  ## won't be added.\n  # response_status_code = 0\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n  ## Use the given name as the SNI server name on each URL\n  # tls_server_name = \"\"\n  ## TLS renegotiation method, choose from \"never\", \"once\", \"freely\"\n  # tls_renegotiation_method = \"never\"\n\n  ## HTTP Request Headers (all values must be strings)\n  # [inputs.http_response.headers]\n  #   Host = \"github.com\"\n\n  ## Optional setting to map response http headers into tags\n  ## If the http header is not present on the request, no corresponding tag will\n  ## be added. If multiple instances of the http header are present, only the\n  ## first value will be used.\n  # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n\n  ## Interface to use when dialing an address\n  # interface = \"eth0\"\n\n  ## Optional Cookie authentication\n  # cookie_auth_url = \"https://localhost/authMe\"\n  # cookie_auth_method = \"POST\"\n  # cookie_auth_username = \"username\"\n  # cookie_auth_password = \"pa$$word\"\n  # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n  ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n  # cookie_auth_renewal = \"5m\"\n```\n\n----------------------------------------\n\nTITLE: Telegraf Configuration with Secret-Store Integration\nDESCRIPTION: Example of configuring Telegraf with secret-stores for secure credential management, showing both local and cloud secret configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[global_tags]\n  user = \"alice\"\n\n[[secretstores.os]]\n  id = \"local_secrets\"\n\n[[secretstores.jose]]\n  id = \"cloud_secrets\"\n  path = \"/etc/telegraf/secrets\"\n  # Optional reference to another secret store to unlock this one.\n  password = \"@{local_secrets:cloud_store_passwd}\"\n\n[[inputs.http]]\n  urls = [\"http://server.company.org/metrics\"]\n  username = \"@{local_secrets:company_server_http_metric_user}\"\n  password = \"@{local_secrets:company_server_http_metric_pass}\"\n\n[[outputs.influxdb_v2]]\n  urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  token = \"@{cloud_secrets:influxdb_token}\"\n  organization = \"yourname@yourcompany.com\"\n  bucket = \"replace_with_your_bucket_name\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring IIS and ASP.NET with Telegraf\nDESCRIPTION: Configuration for tracking IIS and ASP.NET performance metrics. Monitors HTTP service request queues, ASP.NET applications, web service performance, and web service cache.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    # HTTP Service request queues in the Kernel before being handed over to User Mode.\n    ObjectName = \"HTTP Service Request Queues\"\n    Instances = [\"*\"]\n    Counters = [\"CurrentQueueSize\",\"RejectedRequests\"]\n    Measurement = \"win_http_queues\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # IIS, ASP.NET Applications\n    ObjectName = \"ASP.NET Applications\"\n    Counters = [\"Cache Total Entries\",\"Cache Total Hit Ratio\",\"Cache Total Turnover Rate\",\"Output Cache Entries\",\"Output Cache Hits\",\"Output Cache Hit Ratio\",\"Output Cache Turnover Rate\",\"Compilations Total\",\"Errors Total/Sec\",\"Pipeline Instance Count\",\"Requests Executing\",\"Requests in Application Queue\",\"Requests/Sec\"]\n    Instances = [\"*\"]\n    Measurement = \"win_aspnet_app\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # IIS, ASP.NET\n    ObjectName = \"ASP.NET\"\n    Counters = [\"Application Restarts\",\"Request Wait Time\",\"Requests Current\",\"Requests Queued\",\"Requests Rejected\"]\n    Instances = [\"*\"]\n    Measurement = \"win_aspnet\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # IIS, Web Service\n    ObjectName = \"Web Service\"\n    Counters = [\"Get Requests/sec\",\"Post Requests/sec\",\"Connection Attempts/sec\",\"Current Connections\",\"ISAPI Extension Requests/sec\"]\n    Instances = [\"*\"]\n    Measurement = \"win_websvc\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # Web Service Cache / IIS\n    ObjectName = \"Web Service Cache\"\n    Counters = [\"URI Cache Hits %\",\"Kernel: URI Cache Hits %\",\"File Cache Hits %\"]\n    Instances = [\"*\"]\n    Measurement = \"win_websvc_cache\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n```\n\n----------------------------------------\n\nTITLE: Configuring Socket Writer Output Plugin in TOML\nDESCRIPTION: Configuration example for the Socket Writer output plugin showing various socket types (TCP, UDP, Unix), TLS settings, keep-alive options, and data format specifications. Includes comprehensive connection options for IPv4, IPv6, and Unix sockets with optional security configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/socket_writer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Generic socket writer capable of handling multiple socket types.\n[[outputs.socket_writer]]\n  ## URL to connect to\n  # address = \"tcp://127.0.0.1:8094\"\n  # address = \"tcp://example.com:http\"\n  # address = \"tcp4://127.0.0.1:8094\"\n  # address = \"tcp6://127.0.0.1:8094\"\n  # address = \"tcp6://[2001:db8::1]:8094\"\n  # address = \"udp://127.0.0.1:8094\"\n  # address = \"udp4://127.0.0.1:8094\"\n  # address = \"udp6://127.0.0.1:8094\"\n  # address = \"unix:///tmp/telegraf.sock\"\n  # address = \"unixgram:///tmp/telegraf.sock\"\n  # address = \"vsock://cid:port\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Content encoding for message payloads, can be set to \"gzip\" or to\n  ## \"identity\" to apply no encoding.\n  ##\n  # content_encoding = \"identity\"\n\n  ## Data format to generate.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Fibaro Input Plugin in Telegraf\nDESCRIPTION: Configuration example for the Telegraf Fibaro plugin showing connection settings for a Fibaro controller including URL, authentication credentials, timeout, and device type specification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fibaro/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read devices value(s) from a Fibaro controller\n[[inputs.fibaro]]\n  ## Required Fibaro controller address/hostname.\n  ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n  url = \"http://<controller>:80\"\n\n  ## Required credentials to access the API (http://<controller/api/<component>)\n  username = \"<username>\"\n  password = \"<password>\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## Fibaro Device Type\n  ## By default, this plugin will attempt to read using the HC2 API. For HC3\n  ## devices, set this to \"HC3\"\n  # device_type = \"HC2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Output Plugin in Telegraf\nDESCRIPTION: Complete configuration template for the Kafka output plugin in Telegraf. Includes broker connection, topic management, authentication, compression, delivery settings, and various optional features like TLS and SASL.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/kafka/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.kafka]]\n  ## URLs of kafka brokers\n  ## The brokers listed here are used to connect to collect metadata about a\n  ## cluster. However, once the initial metadata collect is completed, telegraf\n  ## will communicate solely with the kafka leader and not all defined brokers.\n  brokers = [\"localhost:9092\"]\n\n  ## Kafka topic for producer messages\n  topic = \"telegraf\"\n\n  ## The value of this tag will be used as the topic.  If not set the 'topic'\n  ## option is used.\n  # topic_tag = \"\"\n\n  ## If true, the 'topic_tag' will be removed from to the metric.\n  # exclude_topic_tag = false\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Of particular interested, lz4 compression\n  ## requires at least version 0.10.0.0.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## The routing tag specifies a tagkey on the metric whose value is used as\n  ## the message key.  The message key is used to determine which partition to\n  ## send the message to.  This tag is preferred over the routing_key option.\n  routing_tag = \"host\"\n\n  ## The routing key is set as the message key and used to determine which\n  ## partition to send the message to.  This value is only used when no\n  ## routing_tag is set or as a fallback when the tag specified in routing tag\n  ## is not found.\n  ##\n  ## If set to \"random\", a random value will be generated for each message.\n  ##\n  ## When unset, no message key is added and each message is routed to a random\n  ## partition.\n  ##\n  ##   ex: routing_key = \"random\"\n  ##       routing_key = \"telegraf\"\n  # routing_key = \"\"\n\n  ## Compression codec represents the various compression codecs recognized by\n  ## Kafka in messages.\n  ##  0 : None\n  ##  1 : Gzip\n  ##  2 : Snappy\n  ##  3 : LZ4\n  ##  4 : ZSTD\n  # compression_codec = 0\n\n  ## Idempotent Writes\n  ## If enabled, exactly one copy of each message is written.\n  # idempotent_writes = false\n\n  ##  RequiredAcks is used in Produce Requests to tell the broker how many\n  ##  replica acknowledgements it must see before responding\n  ##   0 : the producer never waits for an acknowledgement from the broker.\n  ##       This option provides the lowest latency but the weakest durability\n  ##       guarantees (some data will be lost when a server fails).\n  ##   1 : the producer gets an acknowledgement after the leader replica has\n  ##       received the data. This option provides better durability as the\n  ##       client waits until the server acknowledges the request as successful\n  ##       (only messages that were written to the now-dead leader but not yet\n  ##       replicated will be lost).\n  ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n  ##       received the data. This option provides the best durability, we\n  ##       guarantee that no messages will be lost as long as at least one in\n  ##       sync replica remains.\n  # required_acks = -1\n\n  ## The maximum number of times to retry sending a metric before failing\n  ## until the next flush.\n  # max_retry = 3\n\n  ## The maximum permitted size of a message. Should be set equal to or\n  ## smaller than the broker's 'message.max.bytes'.\n  # max_message_bytes = 1000000\n\n  ## Producer timestamp\n  ## This option sets the timestamp of the kafka producer message, choose from:\n  ##   * metric: Uses the metric's timestamp\n  ##   * now: Uses the time of write\n  # producer_timestamp = metric\n\n  ## Add metric name as specified kafka header if not empty\n  # metric_name_header = \"\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Defaults to the OS configuration if not specified or zero.\n  # keep_alive_period = \"15s\"\n\n  ## Optional SOCKS5 proxy to use when connecting to brokers\n  # socks5_enabled = true\n  # socks5_address = \"127.0.0.1:1080\"\n  # socks5_username = \"alice\"\n  # socks5_password = \"pass123\"\n\n  ## Optional SASL Config\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Optional SASL:\n  ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n  ## (defaults to PLAIN)\n  # sasl_mechanism = \"\"\n\n  ## used if sasl_mechanism is GSSAPI\n  # sasl_gssapi_service_name = \"\"\n  # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n  # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n  # sasl_gssapi_kerberos_config_path = \"/\"\n  # sasl_gssapi_realm = \"realm\"\n  # sasl_gssapi_key_tab_path = \"\"\n  # sasl_gssapi_disable_pafxfast = false\n\n  ## Access token used if sasl_mechanism is OAUTHBEARER\n  # sasl_access_token = \"\"\n\n  ## Arbitrary key value string pairs to pass as a TOML table. For example:\n  # {logicalCluster = \"cluster-042\", poolId = \"pool-027\"}\n  # sasl_extensions = {}\n\n  ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n  # sasl_version = 1\n\n  # Disable Kafka metadata full fetch\n  # metadata_full = false\n\n  ## Maximum number of retries for metadata operations including\n  ## connecting. Sets Sarama library's Metadata.Retry.Max config value. If 0 or\n  ## unset, use the Sarama default of 3,\n  # metadata_retry_max = 0\n\n  ## Type of retry backoff. Valid options: \"constant\", \"exponential\"\n  # metadata_retry_type = \"constant\"\n\n  ## Amount of time to wait before retrying. When metadata_retry_type is\n  ## \"constant\", each retry is delayed this amount. When \"exponential\", the\n  ## first retry is delayed this amount, and subsequent delays are doubled. If 0\n  ## or unset, use the Sarama default of 250 ms\n  # metadata_retry_backoff = 0\n\n  ## Maximum amount of time to wait before retrying when metadata_retry_type is\n  ## \"exponential\". Ignored for other retry types. If 0, there is no backoff\n  ## limit.\n  # metadata_retry_max_duration = 0\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Optional topic suffix configuration.\n  ## If the section is omitted, no suffix is used.\n  ## Following topic suffix methods are supported:\n  ##   measurement - suffix equals to separator + measurement's name\n  ##   tags        - suffix equals to separator + specified tags' values\n  ##                 interleaved with separator\n\n  ## Suffix equals to \"_\" + measurement name\n  # [outputs.kafka.topic_suffix]\n  #   method = \"measurement\"\n  #   separator = \"_\"\n\n  ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n  ## If there's no such a tag, suffix equals to an empty string\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\"]\n  #   separator = \"__\"\n\n  ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n  ## tag values, separated by \"_\". If there is no such tags,\n  ## their values treated as empty strings.\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\", \"bar\"]\n  #   separator = \"_\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Format in Telegraf Input Plugin\nDESCRIPTION: This example shows how to configure the 'exec' input plugin to parse command output in JSON format. The configuration specifies commands to execute and sets a measurement name suffix along with the data format option.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.exec]]\n  ## Commands array\n  commands = [\"/tmp/test.sh\", \"/usr/bin/mycollector --foo=bar\"]\n\n  ## measurement name suffix (for separating different commands)\n  name_suffix = \"_mycollector\"\n\n  ## Data format to consume.\n  data_format = \"json\"\n```\n\n----------------------------------------\n\nTITLE: Telegraf Configuration with Secret-Store Secrets\nDESCRIPTION: Example of a Telegraf configuration using secret-store plugins for managing sensitive information like credentials and tokens.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[global_tags]\n  user = \"alice\"\n\n[[secretstores.os]]\n  id = \"local_secrets\"\n\n[[secretstores.jose]]\n  id = \"cloud_secrets\"\n  path = \"/etc/telegraf/secrets\"\n  # Optional reference to another secret store to unlock this one.\n  password = \"@{local_secrets:cloud_store_passwd}\"\n\n[[inputs.http]]\n  urls = [\"http://server.company.org/metrics\"]\n  username = \"@{local_secrets:company_server_http_metric_user}\"\n  password = \"@{local_secrets:company_server_http_metric_pass}\"\n\n[[outputs.influxdb_v2]]\n  urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  token = \"@{cloud_secrets:influxdb_token}\"\n  organization = \"yourname@yourcompany.com\"\n  bucket = \"replace_with_your_bucket_name\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Table Collection in Telegraf with TOML\nDESCRIPTION: This TOML configuration demonstrates how to set up SNMP table collection in Telegraf. It shows the basic structure for defining a table, fields, and various options for controlling table behavior including inheritance, indexing, and translation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.snmp]]\n  # ... snip ...\n\n  [[inputs.snmp.table]]\n    ## Object identifier of the SNMP table as a numeric or textual OID.\n    oid = \"IF-MIB::ifTable\"\n\n    ## Name of the field or tag to create.  If not specified, it defaults to\n    ## the value of 'oid'.  If 'oid' is numeric an attempt to translate the\n    ## numeric OID into a textual OID will be made.\n    # name = \"\"\n\n    ## Which tags to inherit from the top-level config and to use in the output\n    ## of this table's measurement.\n    ## example: inherit_tags = [\"source\"]\n    # inherit_tags = []\n\n    ## Add an 'index' tag with the table row number.  Use this if the table has\n    ## no indexes or if you are excluding them.  This option is normally not\n    ## required as any index columns are automatically added as tags.\n    # index_as_tag = false\n\n    [[inputs.snmp.table.field]]\n      ## OID to get. May be a numeric or textual module-qualified OID.\n      oid = \"IF-MIB::ifDescr\"\n\n      ## Name of the field or tag to create.  If not specified, it defaults to\n      ## the value of 'oid'. If 'oid' is numeric an attempt to translate the\n      ## numeric OID into a textual OID will be made.\n      # name = \"\"\n\n      ## Output this field as a tag.\n      # is_tag = false\n\n      ## The OID sub-identifier to strip off so that the index can be matched\n      ## against other fields in the table.\n      # oid_index_suffix = \"\"\n\n      ## Specifies the length of the index after the supplied table OID (in OID\n      ## path segments). Truncates the index after this point to remove non-fixed\n      ## value or length index suffixes.\n      # oid_index_length = 0\n\n      ## Specifies if the value of given field should be snmptranslated\n      ## by default no field values are translated\n      # translate = true\n\n      ## Secondary index table allows to merge data from two tables with\n      ## different index that this filed will be used to join them. There can\n      ## be only one secondary index table.\n      # secondary_index_table = false\n\n      ## This field is using secondary index, and will be later merged with\n      ## primary index using SecondaryIndexTable. SecondaryIndexTable and\n      ## SecondaryIndexUse are exclusive.\n      # secondary_index_use = false\n\n      ## Controls if entries from secondary table should be added or not\n      ## if joining index is present or not. I set to true, means that join\n      ## is outer, and index is prepended with \"Secondary.\" for missing values\n      ## to avoid overlapping indexes from both tables. Can be set per field or\n      ## globally with SecondaryIndexTable, global true overrides per field false.\n      # secondary_outer_join = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Parsing with Telegraf Plugin - TOML\nDESCRIPTION: This code snippet demonstrates the configuration setup for the Telegraf Parser Processor Plugin using the TOML format. It highlights parameters such as 'parse_fields', 'merge', and 'data_format'. Key functionality includes parsing specified field/tags and applying base64 decoding if necessary. These settings dictate how incoming data metrics are manipulated and emitted.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/parser/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.parser]]\n  parse_fields = [\"message\"]\n  merge = \"override\"\n  data_format = \"logfmt\"\n\n```\n\n----------------------------------------\n\nTITLE: Generating Telegraf Configuration with Specific Filters\nDESCRIPTION: Command to generate a Telegraf configuration file with specific input and output plugins filtered.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf config --input-filter cpu:mem:net:swap --output-filter influxdb:kafka\n```\n\n----------------------------------------\n\nTITLE: Redis Input Plugin Configuration\nDESCRIPTION: Sample configuration for the Redis input plugin showing server connection settings, optional commands, authentication, and TLS configuration options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/redis/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or many redis servers\n[[inputs.redis]]\n  ## specify servers via a url matching:\n  ##  [protocol://][username:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:6379\n  ##    tcp://username:password@192.168.99.100\n  ##    unix:///var/run/redis.sock\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 6379 is used\n  servers = [\"tcp://localhost:6379\"]\n\n  ## Optional. Specify redis commands to retrieve values\n  # [[inputs.redis.commands]]\n  #   # The command to run where each argument is a separate element\n  #   command = [\"get\", \"sample-key\"]\n  #   # The field to store the result in\n  #   field = \"sample-key-value\"\n  #   # The type of the result\n  #   # Can be \"string\", \"integer\", or \"float\"\n  #   type = \"string\"\n\n  ## Specify username and password for ACL auth (Redis 6.0+). You can add this\n  ## to the server URI above or specify it here. The values here take\n  ## precedence.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  ## Check tls/config.go ClientConfig for more options\n  # tls_enable = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Monitoring Output in TOML\nDESCRIPTION: Sample configuration for the Stackdriver output plugin showing all available options including project settings, namespace configuration, metric formatting, and resource labeling. Includes detailed comments explaining each option.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/stackdriver/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.stackdriver]]\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Quota Project\n  ## Specifies the Google Cloud project that should be billed for metric ingestion.\n  ## If omitted, the quota is charged to the service account's default project.\n  ## This is useful when sending metrics to multiple projects using a single service account.\n  ## The caller must have the `serviceusage.services.use` permission on the specified project.\n  # quota_project = \"\"\n\n  ## The namespace for the metric descriptor\n  ## This is optional and users are encouraged to set the namespace as a\n  ## resource label instead. If omitted it is not included in the metric name.\n  namespace = \"telegraf\"\n\n  ## Metric Type Prefix\n  ## The DNS name used with the metric type as a prefix.\n  # metric_type_prefix = \"custom.googleapis.com\"\n\n  ## Metric Name Format\n  ## Specifies the layout of the metric name, choose from:\n  ##  * path: 'metric_type_prefix_namespace_name_key'\n  ##  * official: 'metric_type_prefix/namespace_name_key/kind'\n  # metric_name_format = \"path\"\n\n  ## Metric Data Type\n  ## By default, telegraf will use whatever type the metric comes in as.\n  ## However, for some use cases, forcing int64, may be preferred for values:\n  ##   * source: use whatever was passed in\n  ##   * double: preferred datatype to allow queries by PromQL.\n  # metric_data_type = \"source\"\n\n  ## Tags as resource labels\n  ## Tags defined in this option, when they exist, are added as a resource\n  ## label and not included as a metric label. The values from tags override\n  ## the values defined under the resource_labels config options.\n  # tags_as_resource_label = []\n\n  ## Custom resource type\n  # resource_type = \"generic_node\"\n\n  ## Override metric type by metric name\n  ## Metric names matching the values here, globbing supported, will have the\n  ## metric type set to the corresponding type.\n  # metric_counter = []\n  # metric_gauge = []\n  # metric_histogram = []\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Additional resource labels\n  # [outputs.stackdriver.resource_labels]\n  #   node_id = \"$HOSTNAME\"\n  #   namespace = \"myapp\"\n  #   location = \"eu-north0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring the Lookup Processor Plugin in Telegraf\nDESCRIPTION: Configuration example for the Lookup Processor Plugin in Telegraf. This shows how to specify lookup files, format, and a template for generating the lookup key from metric properties.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.lookup]]\n  ## List of files containing the lookup-table\n  files = [\"path/to/lut.json\", \"path/to/another_lut.json\"]\n\n  ## Format of the lookup file(s)\n  ## Available formats are:\n  ##    json               -- JSON file with 'key: {tag-key: tag-value, ...}' mapping\n  ##    csv_key_name_value -- CSV file with 'key,tag-key,tag-value,...,tag-key,tag-value' mapping\n  ##    csv_key_values     -- CSV file with a header containing tag-names and\n  ##                          rows with 'key,tag-value,...,tag-value' mappings\n  # format = \"json\"\n\n  ## Template for generating the lookup-key from the metric.\n  ## This is a Golang template (see https://pkg.go.dev/text/template) to\n  ## access the metric name (`{{.Name}}`), a tag value (`{{.Tag \"name\"}}`) or\n  ## a field value (`{{.Field \"name\"}}`).\n  key = '{{.Tag \"host\"}}'\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Aggregator Plugins for Metric Collection in Telegraf\nDESCRIPTION: Example of configuring the minmax aggregator to collect and emit min/max values of the system load1 metric every 30 seconds. This configuration drops original metrics after aggregation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.system]]\n  fieldinclude = [\"load1\"] # collects system load1 metric.\n\n[[aggregators.minmax]]\n  period = \"30s\"        # send & clear the aggregate every 30s.\n  drop_original = true  # drop the original metrics.\n\n[[outputs.file]]\n  files = [\"stdout\"]\n```\n\n----------------------------------------\n\nTITLE: Adding and Searching InfluxData Helm Repository\nDESCRIPTION: Helm commands to add the InfluxData repository and search for available charts. This is useful for deploying Telegraf in Kubernetes environments.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INSTALL_GUIDE.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add influxdata https://helm.influxdata.com/\nhelm search repo influxdata\n```\n\n----------------------------------------\n\nTITLE: Configuring the Filter Processor in TOML\nDESCRIPTION: This TOML configuration demonstrates the basic structure for configuring the Telegraf Filter Processor plugin. It shows the top-level `[[processors.filter]]` section, the default action, and a nested `[[processors.filter.rule]]` section where specific filtering rules can be defined based on metric names, tags, and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.filter]]\\n    ## Default action if no rule applies\\n    # default = \\\"pass\\\"\\n\\n    ## Rules to apply on the incoming metrics (multiple rules are possible)\\n    ## The rules are evaluated in order and the first matching rule is applied.\\n    ## In case no rule matches the \\\"default\\\" is applied.\\n    ## All filter criteria in a rule must apply for the rule to match the metric\\n    ## i.e. the criteria are combined by a logical AND. If a criterion is\\n    ## omitted it is NOT applied at all and ignored.\\n    [[processors.filter.rule]]\\n        ## List of metric names to match including glob expressions\\n        # name = []\\n\\n        ## List of tag key/values pairs to match including glob expressions\\n        ## ALL given tags keys must exist and at least one value must match\\n        ## for the metric to match the rule.\\n        # tags = {}\\n\\n        ## List of field keys to match including glob expressions\\n        ## At least one field must exist for the metric to match the rule.\\n        # fields = []\\n\\n        ## Action to apply for this rule\\n        ## \\\"pass\\\" will keep the metric and pass it on, while \\\"drop\\\" will remove\\n        ## the metric\\n        # action = \\\"drop\\\"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Output in Telegraf\nDESCRIPTION: Main configuration block for setting up Elasticsearch output in Telegraf. Includes settings for connection, authentication, index management, and template configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.elasticsearch]]\n  urls = [ \"http://node1.es.example.com:9200\" ]\n  timeout = \"5s\"\n  enable_sniffer = false\n  enable_gzip = false\n  health_check_interval = \"10s\"\n  index_name = \"telegraf-%Y.%m.%d\"\n  manage_template = true\n  template_name = \"telegraf\"\n  overwrite_template = false\n  force_document_id = false\n```\n\n----------------------------------------\n\nTITLE: Enabling Compatibility Mode for OpenSearch in JSON\nDESCRIPTION: JSON request to enable compatibility mode for OpenSearch, allowing existing Elasticsearch clients to work properly with version checks.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"persistent\" : {\n    \"compatibility.override_main_response_version\" : true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Output in Telegraf\nDESCRIPTION: Configuration settings for the PostgreSQL output plugin in Telegraf. Includes connection parameters, schema settings, tag storage options, timestamp configurations, and templating options for table creation and modification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/postgresql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Publishes metrics to a postgresql database\n[[outputs.postgresql]]\n  ## Specify connection address via the standard libpq connection string:\n  ##   host=... user=... password=... sslmode=... dbname=...\n  ## Or a URL:\n  ##   postgres://[user[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]\n  ## See https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING\n  ##\n  ## All connection parameters are optional. Environment vars are also supported.\n  ## e.g. PGPASSWORD, PGHOST, PGUSER, PGDATABASE\n  ## All supported vars can be found here:\n  ##  https://www.postgresql.org/docs/current/libpq-envars.html\n  ##\n  ## Non-standard parameters:\n  ##   pool_max_conns (default: 1) - Maximum size of connection pool for parallel (per-batch per-table) inserts.\n  ##   pool_min_conns (default: 0) - Minimum size of connection pool.\n  ##   pool_max_conn_lifetime (default: 0s) - Maximum connection age before closing.\n  ##   pool_max_conn_idle_time (default: 0s) - Maximum idle time of a connection before closing.\n  ##   pool_health_check_period (default: 0s) - Duration between health checks on idle connections.\n  # connection = \"\"\n\n  ## Postgres schema to use.\n  # schema = \"public\"\n\n  ## Store tags as foreign keys in the metrics table. Default is false.\n  # tags_as_foreign_keys = false\n\n  ## Suffix to append to table name (measurement name) for the foreign tag table.\n  # tag_table_suffix = \"_tag\"\n\n  ## Deny inserting metrics if the foreign tag can't be inserted.\n  # foreign_tag_constraint = false\n\n  ## Store all tags as a JSONB object in a single 'tags' column.\n  # tags_as_jsonb = false\n\n  ## Store all fields as a JSONB object in a single 'fields' column.\n  # fields_as_jsonb = false\n\n  ## Name of the timestamp column\n  ## NOTE: Some tools (e.g. Grafana) require the default name so be careful!\n  # timestamp_column_name = \"time\"\n\n  ## Type of the timestamp column\n  ## Currently, \"timestamp without time zone\" and \"timestamp with time zone\"\n  ## are supported\n  # timestamp_column_type = \"timestamp without time zone\"\n\n  ## Templated statements to execute when creating a new table.\n  # create_templates = [\n  #   '''CREATE TABLE {{ .table }} ({{ .columns }})''',\n  # ]\n\n  ## Templated statements to execute when adding columns to a table.\n  ## Set to an empty list to disable. Points containing tags for which there is\n  ## no column will be skipped. Points containing fields for which there is no\n  ## column will have the field omitted.\n  # add_column_templates = [\n  #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join \", ADD COLUMN IF NOT EXISTS \" }}''',\n  # ]\n\n  ## Templated statements to execute when creating a new tag table.\n  # tag_table_create_templates = [\n  #   '''CREATE TABLE {{ .table }} ({{ .columns }}, PRIMARY KEY (tag_id))''',\n  # ]\n\n  ## Templated statements to execute when adding columns to a tag table.\n  ## Set to an empty list to disable. Points containing tags for which there is\n  ## no column will be skipped.\n  # tag_table_add_column_templates = [\n  #   '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join \", ADD COLUMN IF NOT EXISTS \" }}''',\n  # ]\n\n  ## The postgres data type to use for storing unsigned 64-bit integer values\n  ## (Postgres does not have a native unsigned 64-bit integer type).\n  ## The value can be one of:\n  ##   numeric - Uses the PostgreSQL \"numeric\" data type.\n  ##   uint8 - Requires pguint extension (https://github.com/petere/pguint)\n  # uint64_type = \"numeric\"\n\n  ## When using pool_max_conns > 1, and a temporary error occurs, the query is\n  ## retried with an incremental backoff. This controls the maximum duration.\n  # retry_max_backoff = \"15s\"\n\n  ## Approximate number of tag IDs to store in in-memory cache (when using\n  ## tags_as_foreign_keys). This is an optimization to skip inserting known\n  ## tag IDs. Each entry consumes approximately 34 bytes of memory.\n  # tag_cache_size = 100000\n\n  ## Cut column names at the given length to not exceed PostgreSQL's\n  ## 'identifier length' limit (default: no limit)\n  ## (see https://www.postgresql.org/docs/current/limits.html)\n  ## Be careful to not create duplicate column names!\n  # column_name_length_limit = 0\n\n  ## Enable & set the log level for the Postgres driver.\n  # log_level = \"warn\" # trace, debug, info, warn, error, none\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Input Plugin in Telegraf (TOML)\nDESCRIPTION: Configuration snippet for the Elasticsearch input plugin in Telegraf. Includes settings for server connections, authentication, metrics collection options, TLS configuration, and proxy settings. Allows customization of which statistics to collect from nodes, clusters, and indices.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.elasticsearch]]\n  ## specify a list of one or more Elasticsearch servers\n  ## you can add username and password to your url to use basic authentication:\n  ## servers = [\"http://user:pass@localhost:9200\"]\n  servers = [\"http://localhost:9200\"]\n\n  ## HTTP headers to send with each request\n  # headers = { \"X-Custom-Header\" = \"Custom\" }\n\n  ## When local is true (the default), the node will read only its own stats.\n  ## Set local to false when you want to read the node stats from all nodes\n  ## of the cluster.\n  local = true\n\n  ## Set cluster_health to true when you want to obtain cluster health stats\n  cluster_health = false\n\n  ## Adjust cluster_health_level when you want to obtain detailed health stats\n  ## The options are\n  ##  - indices (default)\n  ##  - cluster\n  # cluster_health_level = \"indices\"\n\n  ## Set cluster_stats to true when you want to obtain cluster stats.\n  cluster_stats = false\n\n  ## Only gather cluster_stats from the master node.\n  ## To work this require local = true\n  cluster_stats_only_from_master = true\n\n  ## Gather stats from the enrich API\n  # enrich_stats = false\n\n  ## Indices to collect; can be one or more indices names or _all\n  ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index\n  ## names that end with a changing value, like a date.\n  indices_include = [\"_all\"]\n\n  ## One of \"shards\", \"cluster\", \"indices\"\n  ## Currently only \"shards\" is implemented\n  indices_level = \"shards\"\n\n  ## node_stats is a list of sub-stats that you want to have gathered.\n  ## Valid options are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\",\n  ## \"fs\", \"transport\", \"http\", \"breaker\". Per default, all stats are gathered.\n  # node_stats = [\"jvm\", \"http\"]\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## If 'use_system_proxy' is set to true, Telegraf will check env vars such as\n  ## HTTP_PROXY, HTTPS_PROXY, and NO_PROXY (or their lowercase counterparts).\n  ## If 'use_system_proxy' is set to false (default) and 'http_proxy_url' is\n  ## provided, Telegraf will use the specified URL as HTTP proxy.\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## Sets the number of most recent indices to return for indices that are\n  ## configured with a date-stamped suffix. Each 'indices_include' entry\n  ## ending with a wildcard (*) or glob matching pattern will group together\n  ## all indices that match it, and  sort them by the date or number after\n  ## the wildcard. Metrics then are gathered for only the\n  ## 'num_most_recent_indices' amount of most  recent indices.\n  # num_most_recent_indices = 0\n```\n\n----------------------------------------\n\nTITLE: Formatting System Metrics as JSON\nDESCRIPTION: This snippet represents system-related metrics formatted in JSON for Telegraf events. It details load averages and CPU/user statistics, providing a clear example of how to structure system data for processing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opensearch/README.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"@timestamp\": \"2017-01-01T00:00:00+00:00\",\n  \"measurement_name\": \"system\",\n  \"system\": {\n    \"load1\": 0.78,\n    \"load15\": 0.8,\n    \"load5\": 0.8,\n    \"n_cpus\": 2,\n    \"n_users\": 2\n  },\n  \"tag\": {\n    \"host\": \"opensearhhost\",\n    \"dc\": \"datacenter1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Output in Telegraf\nDESCRIPTION: Configuration block for setting up the SignalFx output plugin in Telegraf. Specifies the access token, realm, custom ingest URL, and event filtering options. Supports secret-store integration for secure token management.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/signalfx/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics and events to SignalFx\n[[outputs.signalfx]]\n  ## SignalFx Org Access Token\n  access_token = \"my-secret-token\"\n\n  ## The SignalFx realm that your organization resides in\n  signalfx_realm = \"us9\"  # Required if ingest_url is not set\n\n  ## You can optionally provide a custom ingest url instead of the\n  ## signalfx_realm option above if you are using a gateway or proxy\n  ## instance.  This option takes precedence over signalfx_realm.\n  ingest_url = \"https://my-custom-ingest/\"\n\n  ## Event typed metrics are omitted by default,\n  ## If you require an event typed metric you must specify the\n  ## metric name in the following list.\n  included_event_names = [\"plugin.metric_name\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PowerStat with All Available Metrics\nDESCRIPTION: Comprehensive configuration that enables all available package and CPU metrics. It also specifies a custom path to the PMU event definitions file, which is needed for some of the advanced metrics collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_powerstat]]\n  package_metrics = [\"current_power_consumption\", \"current_dram_power_consumption\", \"thermal_design_power\", \"max_turbo_frequency\", \"uncore_frequency\", \"cpu_base_frequency\"]\n  cpu_metrics = [\"cpu_frequency\", \"cpu_c0_state_residency\", \"cpu_c1_state_residency\", \"cpu_c3_state_residency\", \"cpu_c6_state_residency\", \"cpu_c7_state_residency\", \"cpu_temperature\", \"cpu_busy_frequency\", \"cpu_c0_substate_c01\", \"cpu_c0_substate_c02\", \"cpu_c0_substate_c0_wait\"]\n  event_definitions = \"/home/telegraf/.cache/pmu-events/GenuineIntel-6-8F-core.json\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Input Plugin in TOML\nDESCRIPTION: TOML configuration block for the OpenTelemetry input plugin, including settings for service address, timeout, message size, dimensions, metrics schema and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opentelemetry/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opentelemetry]]\n  ## Override the default (0.0.0.0:4317) destination OpenTelemetry gRPC service\n  ## address:port\n  # service_address = \"0.0.0.0:4317\"\n\n  ## Override the default (5s) new connection timeout\n  # timeout = \"5s\"\n\n  ## gRPC Maximum Message Size\n  # max_msg_size = \"4MB\"\n\n  ## Override the default span attributes to be used as line protocol tags.\n  # span_dimensions = [\"service.name\", \"span.name\"]\n\n  ## Override the default log record attributes to be used as line protocol tags.\n  # log_record_dimensions = [\"service.name\"]\n\n  ## Override the default profile attributes to be used as line protocol tags.\n  # profile_dimensions = []\n\n  ## Override the default (prometheus-v1) metrics schema.\n  # metrics_schema = \"prometheus-v1\"\n\n  ## Optional TLS Config.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n```\n\n----------------------------------------\n\nTITLE: Displaying ECS Task and Container Metrics in InfluxDB Line Protocol Format\nDESCRIPTION: This code snippet shows the output of ECS task and container metrics in InfluxDB line protocol format. It includes metrics for task status, memory usage, CPU utilization, network traffic, and block I/O operations for various containers and devices.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ecs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\necs_task,cluster=test,family=nginx,host=c4b301d4a123,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a desired_status=\"RUNNING\",known_status=\"RUNNING\",limit_cpu=0.5,limit_mem=512 1542641488000000000\necs_container_mem,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a active_anon=40960i,active_file=8192i,cache=790528i,pgpgin=1243i,total_pgfault=1298i,total_rss=40960i,limit=1033658368i,max_usage=4825088i,hierarchical_memory_limit=536870912i,rss=40960i,total_active_file=8192i,total_mapped_file=618496i,usage_percent=0.05349543109392212,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",pgfault=1298i,pgmajfault=6i,pgpgout=1040i,total_active_anon=40960i,total_inactive_file=782336i,total_pgpgin=1243i,usage=552960i,inactive_file=782336i,mapped_file=618496i,total_cache=790528i,total_pgpgout=1040i 1542642001000000000\necs_container_cpu,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,cpu=cpu-total,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a usage_in_kernelmode=0i,throttling_throttled_periods=0i,throttling_periods=0i,throttling_throttled_time=0i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",usage_percent=0,usage_total=26426156i,usage_in_usermode=20000000i,usage_system=2336100000000i 1542642001000000000\necs_container_cpu,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,cpu=cpu0,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",usage_total=26426156i 1542642001000000000\necs_container_net,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,network=eth0,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a rx_errors=0i,rx_packets=36i,tx_errors=0i,tx_bytes=648i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",rx_dropped=0i,rx_bytes=5338i,tx_packets=8i,tx_dropped=0i 1542642001000000000\necs_container_net,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,network=eth5,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a rx_errors=0i,tx_packets=9i,rx_packets=26i,tx_errors=0i,rx_bytes=4641i,tx_dropped=0i,tx_bytes=690i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",rx_dropped=0i 1542642001000000000\necs_container_net,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,network=total,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a rx_dropped=0i,rx_bytes=9979i,rx_errors=0i,rx_packets=62i,tx_bytes=1338i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",tx_packets=17i,tx_dropped=0i,tx_errors=0i 1542642001000000000\necs_container_blkio,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,device=253:1,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a io_service_bytes_recursive_sync=790528i,io_service_bytes_recursive_total=790528i,io_serviced_recursive_sync=10i,io_serviced_recursive_write=0i,io_serviced_recursive_async=0i,io_serviced_recursive_total=10i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",io_service_bytes_recursive_read=790528i,io_service_bytes_recursive_write=0i,io_service_bytes_recursive_async=0i,io_serviced_recursive_read=10i 1542642001000000000\necs_container_blkio,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,device=253:2,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a io_service_bytes_recursive_sync=790528i,io_service_bytes_recursive_total=790528i,io_serviced_recursive_async=0i,io_serviced_recursive_total=10i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",io_service_bytes_recursive_read=790528i,io_service_bytes_recursive_write=0i,io_service_bytes_recursive_async=0i,io_serviced_recursive_read=10i,io_serviced_recursive_write=0i,io_serviced_recursive_sync=10i 1542642001000000000\necs_container_blkio,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,device=253:4,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a io_service_bytes_recursive_write=0i,io_service_bytes_recursive_sync=790528i,io_service_bytes_recursive_async=0i,io_service_bytes_recursive_total=790528i,io_serviced_recursive_async=0i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",io_service_bytes_recursive_read=790528i,io_serviced_recursive_read=10i,io_serviced_recursive_write=0i,io_serviced_recursive_sync=10i,io_serviced_recursive_total=10i 1542642001000000000\necs_container_blkio,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,device=202:26368,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a io_serviced_recursive_read=10i,io_serviced_recursive_write=0i,io_serviced_recursive_sync=10i,io_serviced_recursive_async=0i,io_serviced_recursive_total=10i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",io_service_bytes_recursive_sync=790528i,io_service_bytes_recursive_total=790528i,io_service_bytes_recursive_async=0i,io_service_bytes_recursive_read=790528i,io_service_bytes_recursive_write=0i 1542642001000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the PostgreSQL input plugin in Telegraf. Specifies connection details, database selection, and query options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.postgresql]]\n  address = \"host=localhost user=postgres sslmode=disable\"\n  # outputaddress = \"db01\"\n  # max_lifetime = \"0s\"\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n  # databases = [\"app_production\", \"testing\"]\n  prepared_statements = true\n```\n\n----------------------------------------\n\nTITLE: Basic JSON Parsing Example\nDESCRIPTION: Demonstrates basic JSON parsing configuration with sample input and output.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n  name_override = \"myjsonmetric\"\n  data_format = \"json\"\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"a\": 5,\n    \"b\": {\n        \"c\": 6\n    },\n    \"ignored\": \"I'm a string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Directory Monitor Input Plugin in TOML\nDESCRIPTION: Configuration template for the directory monitor input plugin. Specifies settings for directory monitoring, file processing, filtering, and data format handling.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/directory_monitor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.directory_monitor]]\n  ## The directory to monitor and read files from (including sub-directories if \"recursive\" is true).\n  directory = \"\"\n  #\n  ## The directory to move finished files to (maintaining directory hierarchy from source).\n  finished_directory = \"\"\n  #\n  ## Setting recursive to true will make the plugin recursively walk the directory and process all sub-directories.\n  # recursive = false\n  #\n  ## The directory to move files to upon file error.\n  ## If not provided, erroring files will stay in the monitored directory.\n  # error_directory = \"\"\n  #\n  ## The amount of time a file is allowed to sit in the directory before it is picked up.\n  ## This time can generally be low but if you choose to have a very large file written to the directory and it's potentially slow,\n  ## set this higher so that the plugin will wait until the file is fully copied to the directory.\n  # directory_duration_threshold = \"50ms\"\n  #\n  ## A list of the only file names to monitor, if necessary. Supports regex. If left blank, all files are ingested.\n  # files_to_monitor = [\"^.*\\\\.csv\"]\n  #\n  ## A list of files to ignore, if necessary. Supports regex.\n  # files_to_ignore = [\".DS_Store\"]\n  #\n  ## Maximum lines of the file to process that have not yet be written by the\n  ## output. For best throughput set to the size of the output's metric_buffer_limit.\n  ## Warning: setting this number higher than the output's metric_buffer_limit can cause dropped metrics.\n  # max_buffered_metrics = 10000\n  #\n  ## The maximum amount of file paths to queue up for processing at once, before waiting until files are processed to find more files.\n  ## Lowering this value will result in *slightly* less memory use, with a potential sacrifice in speed efficiency, if absolutely necessary.\n  # file_queue_size = 100000\n  #\n  ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n  ## to disable. Cautious when file name variation is high, this can increase the cardinality\n  ## significantly. Read more about cardinality here:\n  ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n  # file_tag = \"\"\n  #\n  ## Specify if the file can be read completely at once or if it needs to be read line by line (default).\n  ## Possible values: \"line-by-line\", \"at-once\"\n  # parse_method = \"line-by-line\"\n  #\n  ## The dataformat to be read from the files.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Socket Listener Input Plugin (TOML)\nDESCRIPTION: This snippet provides a sample configuration for the Telegraf Socket Listener plugin, allowing it to listen on various specified socket protocols. It includes options for address settings, permissions, and buffer sizes to optimize performance for different socket types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socket_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"\"\"\n[[inputs.socket_listener]]\n  ## URL to listen on\n  # service_address = \"tcp://:8094\"\n  # service_address = \"tcp://127.0.0.1:http\"\n  # service_address = \"tcp4://:8094\"\n  # service_address = \"tcp6://:8094\"\n  # service_address = \"tcp6://[2001:db8::1]:8094\"\n  # service_address = \"udp://:8094\"\n  # service_address = \"udp4://:8094\"\n  # service_address = \"udp6://:8094\"\n  # service_address = \"unix:///tmp/telegraf.sock\"\n  # service_address = \"unixgram:///tmp/telegraf.sock\"\n  # service_address = \"vsock://cid:port\"\n\n  ## Permission for unix sockets (only available on unix sockets)\n  ## This setting may not be respected by some platforms. To safely restrict\n  ## permissions it is recommended to place the socket into a previously\n  ## created directory with the desired permissions.\n  ##   ex: socket_mode = \"777\"\n  # socket_mode = \"\"\n\n  ## Maximum number of concurrent connections (only available on stream sockets like TCP)\n  ## Zero means unlimited.\n  # max_connections = 0\n\n  ## Read timeout (only available on stream sockets like TCP)\n  ## Zero means unlimited.\n  # read_timeout = \"0s\"\n\n  ## Optional TLS configuration (only available on stream sockets like TCP)\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Maximum socket buffer size (in bytes when no unit specified)\n  ## For stream sockets, once the buffer fills up, the sender will start\n  ## backing up. For datagram sockets, once the buffer fills up, metrics will\n  ## start dropping. Defaults to the OS default.\n  # read_buffer_size = \"64KiB\"\n\n  ## Period between keep alive probes (only applies to TCP sockets)\n  ## Zero disables keep alive probes. Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Content encoding for message payloads\n  ## Can be set to \"gzip\" for compressed payloads or \"identity\" for no encoding.\n  # content_encoding = \"identity\"\n\n  ## Maximum size of decoded packet (in bytes when no unit specified)\n  # max_decompression_size = \"500MB\"\n\n  ## Message splitting strategy and corresponding settings for stream sockets\n  ## (tcp, tcp4, tcp6, unix or unixpacket). The setting is ignored for packet\n  ## listeners such as udp.\n  ## Available strategies are:\n  ##   newline         -- split at newlines (default)\n  ##   null            -- split at null bytes\n  ##   delimiter       -- split at delimiter byte-sequence in hex-format\n  ##                      given in `splitting_delimiter`\n  ##   fixed length    -- split after number of bytes given in `splitting_length`\n  ##   variable length -- split depending on length information received in the\n  ##                      data. The length field information is specified in\n  ##                      `splitting_length_field`.\n  # splitting_strategy = \"newline\"\n\n  ## Delimiter used to split received data to messages consumed by the parser.\n  ## The delimiter is a hex byte-sequence marking the end of a message\n  ## e.g. \"0x0D0A\", \"x0d0a\" or \"0d0a\" marks a Windows line-break (CR LF).\n  ## The value is case-insensitive and can be specified with \"0x\" or \"x\" prefix\n  ## or without.\n  ## Note: This setting is only used for splitting_strategy = \"delimiter\".\n  # splitting_delimiter = \"\"\n\n  ## Fixed length of a message in bytes.\n  ## Note: This setting is only used for splitting_strategy = \"fixed length\".\n  # splitting_length = 0\n\n  ## Specification of the length field contained in the data to split messages\n  ## with variable length. The specification contains the following fields:\n  ##  offset        -- start of length field in bytes from begin of data\n  ##  bytes         -- length of length field in bytes\n  ##  endianness    -- endianness of the value, either \"be\" for big endian or\n  ##                   \"le\" for little endian\n  ##  header_length -- total length of header to be skipped when passing\n  ##                   data on to the parser. If zero (default), the header\n  ##                   is passed on to the parser together with the message.\n  ## Note: This setting is only used for splitting_strategy = \"variable length\".\n  # splitting_length_field = {offset = 0, bytes = 0, endianness = \"be\", header_length = 0}\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Systemd Secret-Store in Telegraf\nDESCRIPTION: Sample TOML configuration for setting up the systemd secret-store plugin in Telegraf. Defines the unique identifier, optional path to credentials directory, and prefix handling for credential filenames.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Secret-store to access systemd secrets\n[[secretstores.systemd]]\n  ## Unique identifier for the secretstore.\n  ## This id can later be used in plugins to reference the secrets\n  ## in this secret-store via @{<id>:<secret_key>} (mandatory)\n  id = \"systemd\"\n\n  ## Path to systemd credentials directory\n  ## This should not be required as systemd indicates this directory\n  ## via the CREDENTIALS_DIRECTORY environment variable.\n  # path = \"${CREDENTIALS_DIRECTORY}\"\n\n  ## Prefix to remove from systemd credential-filenames to derive secret names\n  # prefix = \"telegraf.\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Burrow Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Burrow Input Plugin in Telegraf. It includes various options such as server endpoints, API prefix, response timeout, concurrent connections, filtering, authentication, and SSL configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/burrow/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collect Kafka topics and consumers status from Burrow HTTP API.\n[[inputs.burrow]]\n  ## Burrow API endpoints in format \"schema://host:port\".\n  ## Default is \"http://localhost:8000\".\n  servers = [\"http://localhost:8000\"]\n\n  ## Override Burrow API prefix.\n  ## Useful when Burrow is behind reverse-proxy.\n  # api_prefix = \"/v3/kafka\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Limit per-server concurrent connections.\n  ## Useful in case of large number of topics or consumer groups.\n  # concurrent_connections = 20\n\n  ## Filter clusters, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # clusters_include = []\n  # clusters_exclude = []\n\n  ## Filter consumer groups, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # groups_include = []\n  # groups_exclude = []\n\n  ## Filter topics, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # topics_include = []\n  # topics_exclude = []\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional SSL config\n  # ssl_ca = \"/etc/telegraf/ca.pem\"\n  # ssl_cert = \"/etc/telegraf/cert.pem\"\n  # ssl_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Generic Windows Performance Counter Queries in TOML\nDESCRIPTION: This snippet demonstrates how to configure generic queries for Windows performance counters, including CPU, disk, system, memory, and network interface metrics. It shows how to specify object names, instances, and counters for each measurement.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    # Processor usage, alternative to native, reports on a per core.\n    ObjectName = \"Processor\"\n    Instances = [\"*\"]\n    Counters = [\"% Idle Time\", \"% Interrupt Time\", \"% Privileged Time\", \"% User Time\", \"% Processor Time\"]\n    Measurement = \"win_cpu\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # Disk times and queues\n    ObjectName = \"LogicalDisk\"\n    Instances = [\"*\"]\n    Counters = [\"% Idle Time\", \"% Disk Time\",\"% Disk Read Time\", \"% Disk Write Time\", \"% User Time\", \"Current Disk Queue Length\"]\n    Measurement = \"win_disk\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"System\"\n    Counters = [\"Context Switches/sec\",\"System Calls/sec\", \"Processor Queue Length\"]\n    Instances = [\"------\"]\n    Measurement = \"win_system\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # Example query where the Instance portion must be removed to get data back, such as from the Memory object.\n    ObjectName = \"Memory\"\n    Counters = [\"Available Bytes\",\"Cache Faults/sec\",\"Demand Zero Faults/sec\",\"Page Faults/sec\",\"Pages/sec\",\"Transition Faults/sec\",\"Pool Nonpaged Bytes\",\"Pool Paged Bytes\"]\n    Instances = [\"------\"] # Use 6 x - to remove the Instance bit from the query.\n    Measurement = \"win_mem\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # more counters for the Network Interface Object can be found at\n    # https://msdn.microsoft.com/en-us/library/ms803962.aspx\n    ObjectName = \"Network Interface\"\n    Counters = [\"Bytes Received/sec\",\"Bytes Sent/sec\",\"Packets Received/sec\",\"Packets Sent/sec\"]\n    Instances = [\"*\"] # Use 6 x - to remove the Instance bit from the query.\n    Measurement = \"win_net\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n```\n\n----------------------------------------\n\nTITLE: Configuring Wireguard Input Plugin for Telegraf - Toml\nDESCRIPTION: This configuration snippet defines the Wireguard input plugin settings for Telegraf. It specifies how to collect statistics from Wireguard server interfaces and peers, with an option to limit the querying to specific interfaces.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/wireguard/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[inputs.wireguard]]\\n  ## Optional list of Wireguard device/interface names to query.\\n  ## If omitted, all Wireguard interfaces are queried.\\n  # devices = [\\\"wg0\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Monitor Input Plugin in TOML\nDESCRIPTION: Complete configuration example for the Azure Monitor input plugin showing how to set up authentication credentials, resource targets, resource group targets, and subscription targets. Includes options for metric collection and aggregation types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/azure_monitor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather Azure resources metrics from Azure Monitor API\n[[inputs.azure_monitor]]\n  # can be found under Overview->Essentials in the Azure portal for your application/service\n  subscription_id = \"<<SUBSCRIPTION_ID>>\"\n  # can be obtained by registering an application under Azure Active Directory\n  client_id = \"<<CLIENT_ID>>\"\n  # can be obtained by registering an application under Azure Active Directory.\n  # If not specified Default Azure Credentials chain will be attempted:\n  # - Environment credentials (AZURE_*)\n  # - Workload Identity in Kubernetes cluster\n  # - Managed Identity\n  # - Azure CLI auth\n  # - Developer Azure CLI auth\n  client_secret = \"<<CLIENT_SECRET>>\"\n  # can be found under Azure Active Directory->Properties\n  tenant_id = \"<<TENANT_ID>>\"\n  # Define the optional Azure cloud option e.g. AzureChina, AzureGovernment or AzurePublic. The default is AzurePublic.\n  # cloud_option = \"AzurePublic\"\n\n  # resource target #1 to collect metrics from\n  [[inputs.azure_monitor.resource_target]]\n    # can be found under Overview->Essentials->JSON View in the Azure portal for your application/service\n    # must start with 'resourceGroups/...' ('/subscriptions/xxxxxxxx-xxxx-xxxx-xxx-xxxxxxxxxxxx'\n    # must be removed from the beginning of Resource ID property value)\n    resource_id = \"<<RESOURCE_ID>>\"\n    # the metric names to collect\n    # leave the array empty to use all metrics available to this resource\n    metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n    # metrics aggregation type value to collect\n    # can be 'Total', 'Count', 'Average', 'Minimum', 'Maximum'\n    # leave the array empty to collect all aggregation types values for each metric\n    aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n\n  # resource target #2 to collect metrics from\n  [[inputs.azure_monitor.resource_target]]\n    resource_id = \"<<RESOURCE_ID>>\"\n    metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n    aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n\n  # resource group target #1 to collect metrics from resources under it with resource type\n  [[inputs.azure_monitor.resource_group_target]]\n    # the resource group name\n    resource_group = \"<<RESOURCE_GROUP_NAME>>\"\n\n    # defines the resources to collect metrics from\n    [[inputs.azure_monitor.resource_group_target.resource]]\n      # the resource type\n      resource_type = \"<<RESOURCE_TYPE>>\"\n      metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n      aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n\n    # defines the resources to collect metrics from\n    [[inputs.azure_monitor.resource_group_target.resource]]\n      resource_type = \"<<RESOURCE_TYPE>>\"\n      metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n      aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n\n  # resource group target #2 to collect metrics from resources under it with resource type\n  [[inputs.azure_monitor.resource_group_target]]\n    resource_group = \"<<RESOURCE_GROUP_NAME>>\"\n\n    [[inputs.azure_monitor.resource_group_target.resource]]\n      resource_type = \"<<RESOURCE_TYPE>>\"\n      metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n      aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n\n  # subscription target #1 to collect metrics from resources under it with resource type\n  [[inputs.azure_monitor.subscription_target]]\n    resource_type = \"<<RESOURCE_TYPE>>\"\n    metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n    aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n\n  # subscription target #2 to collect metrics from resources under it with resource type\n  [[inputs.azure_monitor.subscription_target]]\n    resource_type = \"<<RESOURCE_TYPE>>\"\n    metrics = [ \"<<METRIC>>\", \"<<METRIC>>\" ]\n    aggregations = [ \"<<AGGREGATION>>\", \"<<AGGREGATION>>\" ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Kafka Consumer Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Kafka Consumer input plugin for Telegraf. It includes options for broker connections, topic selection, authentication, compression, and message processing settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kafka_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from Kafka topics\n[[inputs.kafka_consumer]]\n  ## Kafka brokers.\n  brokers = [\"localhost:9092\"]\n\n  ## Set the minimal supported Kafka version. Should be a string contains\n  ## 4 digits in case if it is 0 version and 3 digits for versions starting\n  ## from 1.0.0 separated by dot. This setting enables the use of new\n  ## Kafka features and APIs.  Must be 0.10.2.0(used as default) or greater.\n  ## Please, check the list of supported versions at\n  ## https://pkg.go.dev/github.com/Shopify/sarama#SupportedVersions\n  ##   ex: kafka_version = \"2.6.0\"\n  ##   ex: kafka_version = \"0.10.2.0\"\n  # kafka_version = \"0.10.2.0\"\n\n  ## Topics to consume.\n  topics = [\"telegraf\"]\n\n  ## Topic regular expressions to consume.  Matches will be added to topics.\n  ## Example: topic_regexps = [ \"*test\", \"metric[0-9A-z]*\" ]\n  # topic_regexps = [ ]\n\n  ## When set this tag will be added to all metrics with the topic as the value.\n  # topic_tag = \"\"\n\n  ## The list of Kafka message headers that should be pass as metric tags\n  ## works only for Kafka version 0.11+, on lower versions the message headers\n  ## are not available\n  # msg_headers_as_tags = []\n\n  ## The name of kafka message header which value should override the metric name.\n  ## In case when the same header specified in current option and in msg_headers_as_tags\n  ## option, it will be excluded from the msg_headers_as_tags list.\n  # msg_header_as_metric_name = \"\"\n\n  ## Set metric(s) timestamp using the given source.\n  ## Available options are:\n  ##   metric -- do not modify the metric timestamp\n  ##   inner  -- use the inner message timestamp (Kafka v0.10+)\n  ##   outer  -- use the outer (compressed) block timestamp (Kafka v0.10+)\n  # timestamp_source = \"metric\"\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Defaults to the OS configuration if not specified or zero.\n  # keep_alive_period = \"15s\"\n\n  ## SASL authentication credentials.  These settings should typically be used\n  ## with TLS encryption enabled\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Optional SASL:\n  ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n  ## (defaults to PLAIN)\n  # sasl_mechanism = \"\"\n\n  ## used if sasl_mechanism is GSSAPI\n  # sasl_gssapi_service_name = \"\"\n  # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n  # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n  # sasl_gssapi_kerberos_config_path = \"/\"\n  # sasl_gssapi_realm = \"realm\"\n  # sasl_gssapi_key_tab_path = \"\"\n  # sasl_gssapi_disable_pafxfast = false\n\n  ## used if sasl_mechanism is OAUTHBEARER\n  # sasl_access_token = \"\"\n\n  ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n  # sasl_version = 1\n\n  # Disable Kafka metadata full fetch\n  # metadata_full = false\n\n  ## Name of the consumer group.\n  # consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Compression codec represents the various compression codecs recognized by\n  ## Kafka in messages.\n  ##  0 : None\n  ##  1 : Gzip\n  ##  2 : Snappy\n  ##  3 : LZ4\n  ##  4 : ZSTD\n  # compression_codec = 0\n  ## Initial offset position; one of \"oldest\" or \"newest\".\n  # offset = \"oldest\"\n\n  ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n  # balance_strategy = \"range\"\n\n  ## Maximum number of retries for metadata operations including\n  ## connecting. Sets Sarama library's Metadata.Retry.Max config value. If 0 or\n  ## unset, use the Sarama default of 3,\n  # metadata_retry_max = 0\n\n  ## Type of retry backoff. Valid options: \"constant\", \"exponential\"\n  # metadata_retry_type = \"constant\"\n\n  ## Amount of time to wait before retrying. When metadata_retry_type is\n  ## \"constant\", each retry is delayed this amount. When \"exponential\", the\n  ## first retry is delayed this amount, and subsequent delays are doubled. If 0\n  ## or unset, use the Sarama default of 250 ms\n  # metadata_retry_backoff = 0\n\n  ## Maximum amount of time to wait before retrying when metadata_retry_type is\n  ## \"exponential\". Ignored for other retry types. If 0, there is no backoff\n  ## limit.\n  # metadata_retry_max_duration = 0\n\n  ## When set to true, this turns each bootstrap broker address into a set of\n  ## IPs, then does a reverse lookup on each one to get its canonical hostname.\n  ## This list of hostnames then replaces the original address list.\n  ## resolve_canonical_bootstrap_servers_only = false\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 1000000\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Maximum amount of time the consumer should take to process messages. If\n  ## the debug log prints messages from sarama about 'abandoning subscription\n  ## to [topic] because consuming was taking too long', increase this value to\n  ## longer than the time taken by the output plugin(s).\n  ##\n  ## Note that the effective timeout could be between 'max_processing_time' and\n  ## '2 * max_processing_time'.\n  # max_processing_time = \"100ms\"\n\n  ## The default number of message bytes to fetch from the broker in each\n  ## request (default 1MB). This should be larger than the majority of\n  ## your messages, or else the consumer will spend a lot of time\n  ## negotiating sizes and not actually consuming. Similar to the JVM's\n  ## `fetch.message.max.bytes`.\n  # consumer_fetch_default = \"1MB\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Filtering CPU and Disk Metrics with tagpass and tagdrop in Telegraf\nDESCRIPTION: This TOML configuration demonstrates how to use tagpass and tagdrop to filter CPU and disk metrics. It excludes specific CPU cores and includes only certain filesystems and paths.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_20\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  fieldexclude = [\"cpu_time\"]\n  # Don't collect CPU data for cpu6 & cpu7\n  [inputs.cpu.tagdrop]\n    cpu = [ \"cpu6\", \"cpu7\" ]\n\n[[inputs.disk]]\n  [inputs.disk.tagpass]\n    # tagpass conditions are OR, not AND.\n    # If the (filesystem is ext4 or xfs) OR (the path is /opt or /home)\n    # then the metric passes\n    fstype = [ \"ext4\", \"xfs\" ]\n    # Globs can also be used on the tag values\n    path = [ \"/opt\", \"/home*\" ]\n\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"Network Interface\"\n    Instances = [\"*\"]\n    Counters = [\n      \"Bytes Received/sec\",\n      \"Bytes Sent/sec\"\n    ]\n    Measurement = \"win_net\"\n  # Do not send metrics where the Windows interface name (instance) begins with\n  # 'isatap' or 'Local'\n  [inputs.win_perf_counters.tagdrop]\n    instance = [\"isatap*\", \"Local*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Output Plugin in Telegraf\nDESCRIPTION: TOML configuration for the SQL output plugin including database driver selection, connection settings, table templates, and type conversions. Supports multiple database types including MSSQL, MySQL, PostgreSQL, SQLite, Snowflake, and ClickHouse.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.sql]]\n  ## Database driver\n  ## Valid options: mssql (Microsoft SQL Server), mysql (MySQL), pgx (Postgres),\n  ##  sqlite (SQLite3), snowflake (snowflake.com) clickhouse (ClickHouse)\n  driver = \"\"\n\n  ## Data source name\n  ## The format of the data source name is different for each database driver.\n  ## See the plugin readme for details.\n  data_source_name = \"\"\n\n  ## Timestamp column name, set to empty to ignore the timestamp\n  # timestamp_column = \"timestamp\"\n\n  ## Table creation template\n  ## Available template variables:\n  ##  {TABLE} - table name as a quoted identifier\n  ##  {TABLELITERAL} - table name as a quoted string literal\n  ##  {COLUMNS} - column definitions (list of quoted identifiers and types)\n  ##  {TAG_COLUMN_NAMES} - tag column definitions (list of quoted identifiers)\n  ##  {TIMESTAMP_COLUMN_NAME} - the name of the time stamp column, as configured above\n  # table_template = \"CREATE TABLE {TABLE}({COLUMNS})\"\n  ## NOTE: For the clickhouse driver the default is:\n  # table_template = \"CREATE TABLE {TABLE}({COLUMNS}) ORDER BY ({TAG_COLUMN_NAMES}, {TIMESTAMP_COLUMN_NAME})\"\n\n  ## Table existence check template\n  ## Available template variables:\n  ##  {TABLE} - tablename as a quoted identifier\n  # table_exists_template = \"SELECT 1 FROM {TABLE} LIMIT 1\"\n\n  ## Table update template, available template variables:\n  ##  {TABLE} - table name as a quoted identifier\n  ##  {COLUMN} - column definition (quoted identifier and type)\n  ## NOTE: Ensure the user (you're using to write to the database) has necessary permissions\n  ##\n  ## Use the following setting for automatically adding columns:\n  ## table_update_template = \"ALTER TABLE {TABLE} ADD COLUMN {COLUMN}\"\n  # table_update_template = \"\"\n\n  ## Initialization SQL\n  # init_sql = \"\"\n\n  ## Maximum amount of time a connection may be idle. \"0s\" means connections are\n  ## never closed due to idle time.\n  # connection_max_idle_time = \"0s\"\n\n  ## Maximum amount of time a connection may be reused. \"0s\" means connections\n  ## are never closed due to age.\n  # connection_max_lifetime = \"0s\"\n\n  ## Maximum number of connections in the idle connection pool. 0 means unlimited.\n  # connection_max_idle = 2\n\n  ## Maximum number of open connections to the database. 0 means unlimited.\n  # connection_max_open = 0\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Metric type to SQL type conversion\n  ## The values on the left are the data types Telegraf has and the values on\n  ## the right are the data types Telegraf will use when sending to a database.\n  ##\n  ## The database values used must be data types the destination database\n  ## understands. It is up to the user to ensure that the selected data type is\n  ## available in the database they are using. Refer to your database\n  ## documentation for what data types are available and supported.\n  #[outputs.sql.convert]\n  #  integer              = \"INT\"\n  #  real                 = \"DOUBLE\"\n  #  text                 = \"TEXT\"\n  #  timestamp            = \"TIMESTAMP\"\n  #  defaultvalue         = \"TEXT\"\n  #  unsigned             = \"UNSIGNED\"\n  #  bool                 = \"BOOL\"\n  #  ## This setting controls the behavior of the unsigned value. By default the\n  #  ## setting will take the integer value and append the unsigned value to it. The other\n  #  ## option is \"literal\", which will use the actual value the user provides to\n  #  ## the unsigned option. This is useful for a database like ClickHouse where\n  #  ## the unsigned value should use a value like \"uint64\".\n  #  # conversion_style = \"unsigned_suffix\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Format for File Output Plugin in Telegraf\nDESCRIPTION: This snippet demonstrates how to configure the data format for the file output plugin in Telegraf. It specifies the output files and sets the data format to InfluxDB Line Protocol.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\"]\n\n  ## Data format to output.\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Vault Input Plugin\nDESCRIPTION: This TOML configuration sets up the Telegraf Vault input plugin to collect metrics from a Vault agent. It specifies the Vault agent's URL, authentication token, and optional TLS settings. Response timeout can also be configured.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vault/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from the Vault API\n[[inputs.vault]]\n  ## URL for the Vault agent\n  # url = \"http://127.0.0.1:8200\"\n\n  ## Use Vault token for authorization.\n  ## Vault token configuration is mandatory.\n  ## If both are empty or both are set, an error is thrown.\n  # token_file = \"/path/to/auth/token\"\n  ## OR\n  token = \"s.CDDrgg5zPv5ssI0Z2P4qxJj2\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n```\n\n----------------------------------------\n\nTITLE: Debugging Telegraf with Delve CLI\nDESCRIPTION: Command to attach the Delve debugger to Telegraf with a configuration file. Note the double dash separator for passing arguments to Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEBUG.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndlv debug ./cmd/telegraf -- --config config.toml\n```\n\n----------------------------------------\n\nTITLE: Generating Filtered Telegraf Configuration\nDESCRIPTION: Shows how to generate a configuration file that includes only specific input and output plugins using the --input-filter and --output-filter flags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/cmd/telegraf/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf config --input-filter cpu --output-filter influxdb\n```\n\n----------------------------------------\n\nTITLE: Configuring AMQP Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet defines the settings for the AMQP output plugin in Telegraf. It includes options for brokers, exchange settings, authentication, routing, TLS, proxy, and data formatting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/amqp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Publishes metrics to an AMQP broker\n[[outputs.amqp]]\n  ## Brokers to publish to.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Maximum messages to send over a connection.  Once this is reached, the\n  ## connection is closed and a new connection is made.  This can be helpful for\n  ## load balancing when not using a dedicated load balancer.\n  # max_messages = 0\n\n  ## Exchange to declare and publish to.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Metric tag to use as a routing key.\n  ##   ie, if this tag exists, its value will be used as the routing key\n  # routing_tag = \"host\"\n\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.\n  # routing_key = \"\"\n  # routing_key = \"telegraf\"\n\n  ## Delivery Mode controls if a published message is persistent.\n  ##   One of \"transient\" or \"persistent\".\n  # delivery_mode = \"transient\"\n\n  ## Static headers added to each published message.\n  # headers = { }\n  # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n\n  ## Connection timeout.  If not provided, will default to 5s.  0s means no\n  ## timeout (not recommended).\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional Proxy Configuration\n  # use_proxy = false\n  # proxy_url = \"localhost:8888\"\n\n  ## If true use batch serialization format instead of line based delimiting.\n  ## Only applies to data formats which are not line based such as JSON.\n  ## Recommended to set to true.\n  # use_batch_format = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  ##\n  ## Please note that when use_batch_format = false each amqp message contains only\n  ## a single metric, it is recommended to use compression with batch format\n  ## for best results.\n  # content_encoding = \"identity\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Parsing CSV Data in Telegraf\nDESCRIPTION: Configuration for parsing CSV data with Telegraf, specifying header row, column names, tag columns, and timestamp format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PARSING_DATA.md#2025-04-16_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nnode,temp,humidity,alarm,time\nnode1,32.3,23,false,2023-03-06T16:52:23Z\nnode2,22.6,44,false,2023-03-06T16:52:23Z\nnode3,17.9,56,true,2023-03-06T16:52:23Z\n```\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\nfiles = [\"test.csv\"]\ndata_format = \"csv\"\n\ncsv_header_row_count = 1\ncsv_column_names = [\"node\",\"temp\",\"humidity\",\"alarm\",\"time\"]\ncsv_tag_columns = [\"node\"]\ncsv_timestamp_column = \"time\"\ncsv_timestamp_format = \"2006-01-02T15:04:05Z\"\n```\n\n----------------------------------------\n\nTITLE: Building Customized Telegraf with Go Build\nDESCRIPTION: Shows how to build a customized version of Telegraf using Go's native build tools with the -tags option. This example builds Telegraf including all inputs, the InfluxDB v2 output, and the JSON parser.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CUSTOMIZATION.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngo build -tags \"custom,inputs,outputs.influxdb_v2,parsers.json\" ./cmd/telegraf\n```\n\n----------------------------------------\n\nTITLE: Configuring Jenkins Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Jenkins input plugin in Telegraf. It includes options for specifying the Jenkins URL, authentication, TLS settings, and various filters for jobs and nodes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jenkins/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read jobs and cluster metrics from Jenkins instances\n[[inputs.jenkins]]\n  ## The Jenkins URL in the format \"schema://host:port\"\n  url = \"http://my-jenkins-instance:8080\"\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Set response_timeout\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use SSL but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional Max Job Build Age filter\n  ## Default 1 hour, ignore builds older than max_build_age\n  # max_build_age = \"1h\"\n\n  ## Optional Sub Job Depth filter\n  ## Jenkins can have unlimited layer of sub jobs\n  ## This config will limit the layers of pulling, default value 0 means\n  ## unlimited pulling until no more sub jobs\n  # max_subjob_depth = 0\n\n  ## Optional Sub Job Per Layer\n  ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n  ## This config will limit to call only the lasted branches in each layer,\n  ## empty will use default value 10\n  # max_subjob_per_layer = 10\n\n  ## Jobs to include or exclude from gathering\n  ## When using both lists, job_exclude has priority.\n  ## Wildcards are supported: [ \"jobA/*\", \"jobB/subjob1/*\"]\n  # job_include = [ \"*\" ]\n  # job_exclude = [ ]\n\n  ## Nodes to include or exclude from gathering\n  ## When using both lists, node_exclude has priority.\n  # node_include = [ \"*\" ]\n  # node_exclude = [ ]\n\n  ## Worker pool for jenkins plugin only\n  ## Empty this field will use default value 5\n  # max_connections = 5\n\n  ## When set to true will add node labels as a comma-separated tag. If none,\n  ## are found, then a tag with the value of 'none' is used. Finally, if a\n  ## label contains a comma it is replaced with an underscore.\n  # node_labels_as_tag = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Beat Input Plugin in Telegraf with TOML\nDESCRIPTION: Sample TOML configuration for the Telegraf Beat input plugin. It shows how to specify the Beat URL endpoint, collection options, HTTP parameters, authentication, and TLS settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/beat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics exposed by Beat\n[[inputs.beat]]\n  ## An URL from which to read Beat-formatted JSON\n  ## Default is \"http://127.0.0.1:5066\".\n  url = \"http://127.0.0.1:5066\"\n\n  ## Enable collection of the listed stats\n  ## An empty list means collect all. Available options are currently\n  ## \"beat\", \"libbeat\", \"system\" and \"filebeat\".\n  # include = [\"beat\", \"libbeat\", \"filebeat\"]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"logstash.example.com\"\n\n  ## Timeout for HTTP requests\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Plus Input Plugin in TOML\nDESCRIPTION: This snippet shows the configuration options for the Nginx Plus input plugin in Telegraf. It specifies the URLs to gather stats from, response timeout, and optional TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Nginx Plus' advanced status information\n[[inputs.nginx_plus]]\n  ## An array of Nginx status URIs to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PMU Plugin in TOML\nDESCRIPTION: Sample configuration for the Intel PMU Telegraf plugin showing core and uncore event monitoring setup. Includes event definitions paths, core events configuration with optional grouping, and uncore events with socket specifications.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmu/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_pmu]]\n  event_definitions = [\"/var/cache/pmu/GenuineIntel-6-55-4-core.json\", \"/var/cache/pmu/GenuineIntel-6-55-4-uncore.json\"]\n\n  [[inputs.intel_pmu.core_events]]\n    events = [\"INST_RETIRED.ANY\", \"CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k\"]\n    cores = [\"0\"]\n    # perf_group = false\n    # events_tag = \"\"\n\n  [[inputs.intel_pmu.uncore_events]]\n    events = [\"UNC_CHA_CLOCKTICKS\", \"UNC_CHA_TOR_OCCUPANCY.IA_MISS\"]\n    sockets = [\"0\"]\n    # aggregate_uncore_units = false\n    # events_tag = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Data Firehose Input Plugin in TOML\nDESCRIPTION: Configuration settings for the AWS Data Firehose input plugin including service address, paths, timeouts, TLS settings, authentication, and data format options. This configuration allows customization of the HTTP listener and security parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/firehose/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.firehose]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Paths to listen to.\n  # paths = [\"/telegraf\"]\n\n  ## maximum duration before timing out read of the request\n  # read_timeout = \"5s\"\n  ## maximum duration before timing out write of the response\n  # write_timeout = \"5s\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Minimal TLS version accepted by the server\n  # tls_min_version = \"TLS12\"\n\n  ## Optional access key to accept for authentication.\n  ## AWS Data Firehose uses \"x-amz-firehose-access-key\" header to set the access key.\n  ## If no access_key is provided (default), authentication is completely disabled and\n  ## this plugin will accept all request ignoring the provided access-key in the request!\n  # access_key = \"foobar\"\n\n  ## Optional setting to add parameters as tags\n  ## If the http header \"x-amz-firehose-common-attributes\" is not present on the\n  ## request, no corresponding tag will be added. The header value should be a\n  ## json and should follow the schema as describe in the official documentation:\n  ## https://docs.aws.amazon.com/firehose/latest/dev/httpdeliveryrequestresponse.html#requestformat\n  # parameter_tags = [\"env\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kapacitor Input Plugin in Telegraf (TOML)\nDESCRIPTION: TOML configuration for the Kapacitor input plugin in Telegraf. Specifies URLs to collect metrics from, timeout settings, and optional TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kapacitor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.kapacitor]]\n  ## Multiple URLs from which to read Kapacitor-formatted JSON\n  ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n  urls = [\n    \"http://localhost:9092/kapacitor/v1/debug/vars\"\n  ]\n\n  ## Time limit for http requests\n  timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Advanced Tag Key Processing Example Configuration\nDESCRIPTION: Example configuration demonstrating how to process tag keys with multiple transformations. This shows converting tag keys to lowercase and then replacing characters within the transformed key.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/strings/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.strings]]\n  [[processors.strings.lowercase]]\n    tag_key = \"URI-Stem\"\n\n  [[processors.strings.replace]]\n    tag_key = \"uri-stem\"\n    old = \"-\"\n    new = \"_\"\n\n```\n\n----------------------------------------\n\nTITLE: Monitoring DNS Server with Telegraf\nDESCRIPTION: Configuration for tracking DNS server metrics on Domain Controllers. Monitors query and response statistics, dynamic updates, and recursive query performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"DNS\"\n    Counters = [\"Dynamic Update Received\",\"Dynamic Update Rejected\",\"Recursive Queries\",\"Recursive Queries Failure\",\"Secure Update Failure\",\"Secure Update Received\",\"TCP Query Received\",\"TCP Response Sent\",\"UDP Query Received\",\"UDP Response Sent\",\"Total Query Received\",\"Total Response Sent\"]\n    Instances = [\"------\"]\n    Measurement = \"win_dns\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n```\n\n----------------------------------------\n\nTITLE: Configuring S7 Protocol Input Plugin in TOML\nDESCRIPTION: Complete configuration example for the S7 protocol input plugin, including server connection parameters, protocol settings, and metric definitions. Shows how to define fields for reading different data types from PLC memory areas.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/s7comm/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.s7comm]]\n  server = \"127.0.0.1:102\"\n  rack = 0\n  slot = 0\n\n  # connection_type = \"PD\"\n  # pdu_size = 20\n  # timeout = \"10s\"\n  # log_level = \"trace\"\n\n  [[inputs.s7comm.metric]]\n    fields = [\n      { name=\"rpm\",             address=\"DB1.R4\"    },\n      { name=\"status_ok\",       address=\"DB1.X2.1\"  },\n      { name=\"last_error\",      address=\"DB2.S1.32\" },\n      { name=\"last_error_time\", address=\"DB2.DT2\"   }\n    ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Log Input Plugin in Telegraf\nDESCRIPTION: A sample configuration for the Docker Log Input Plugin that shows all available options including endpoint settings, container filters, docker label options, and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/docker_log/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read logging output from the Docker engine\n[[inputs.docker_log]]\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  # endpoint = \"unix:///var/run/docker.sock\"\n\n  ## When true, container logs are read from the beginning; otherwise reading\n  ## begins at the end of the log. If state-persistence is enabled for Telegraf,\n  ## the reading continues at the last previously processed timestamp.\n  # from_beginning = false\n\n  ## Timeout for Docker API calls.\n  # timeout = \"5s\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  # docker_label_include = []\n  # docker_label_exclude = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Registering an Input Plugin in Telegraf\nDESCRIPTION: Example of how to register a custom input plugin in Telegraf's build system via the all package. The build tags allow for selective inclusion/exclusion of the plugin when customizing Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INPUTS.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n//go:build !custom || inputs || inputs.simple\n\npackage all\n\nimport _ \"github.com/influxdata/telegraf/plugins/inputs/simple\" // register plugin\n```\n\n----------------------------------------\n\nTITLE: Documenting InfluxDB Metrics Structure\nDESCRIPTION: Detailed documentation of InfluxDB metrics organized by component categories. Includes metrics for query execution, RPC operations, runtime statistics, shard management, subscriber operations, TSM cache, engine operations, file store, WAL, and write operations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb/README.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- **queriesFinished**: Number of queries that have finished executing.\n- **queryDurationNs**: Total duration, in nanoseconds, of executed queries.\n- **recoveredPanics**: Number of panics recovered by the Query Executor.\n- **influxdb_rpc** _(Enterprise Only)_ : Statistics related to the use of RPC calls within InfluxDB Enterprise clusters.\n[...remainder of metrics documentation...]\n```\n\n----------------------------------------\n\nTITLE: AWS CloudWatch Get Metric Data Command\nDESCRIPTION: AWS CLI command to manually retrieve metric data for a specific time period, demonstrating how to query CPU credit balance for an EC2 instance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\naws cloudwatch get-metric-data \\\n  --start-time 2018-07-01T00:00:00Z \\\n  --end-time 2018-07-01T00:15:00Z \\\n  --metric-data-queries '[\n  {\n    \"Id\": \"avgCPUCreditBalance\",\n    \"MetricStat\": {\n      \"Metric\": {\n        \"Namespace\": \"AWS/EC2\",\n        \"MetricName\": \"CPUCreditBalance\",\n        \"Dimensions\": [\n          {\n            \"Name\": \"InstanceId\",\n            \"Value\": \"i-deadbeef\"\n          }\n        ]\n      },\n      \"Period\": 300,\n      \"Stat\": \"Average\"\n    },\n    \"Label\": \"avgCPUCreditBalance\"\n  }\n]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Dropwizard Parser Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up the Dropwizard parser plugin for Telegraf. It includes options for input files, data format, separator, templates, and various paths for metric registry, time, and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/dropwizard/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"dropwizard\"\n\n  ## Used by the templating engine to join matched values when cardinality is > 1\n  separator = \"_\"\n\n  ## Each template line requires a template pattern. It can have an optional\n  ## filter before the template and separated by spaces. It can also have optional extra\n  ## tags following the template. Multiple tags should be separated by commas and no spaces\n  ## similar to the line protocol format. There can be only one default template.\n  ## Templates support below format:\n  ## 1. filter + template\n  ## 2. filter + template + extra tag(s)\n  ## 3. filter + template with field key\n  ## 4. default template\n  ## By providing an empty template array, templating is disabled and measurements are parsed as influxdb line protocol keys (measurement<,tag_set>)\n  templates = []\n\n  ## You may use an appropriate [gjson path](https://github.com/tidwall/gjson#path-syntax)\n  ## to locate the metric registry within the JSON document\n  # dropwizard_metric_registry_path = \"metrics\"\n\n  ## You may use an appropriate [gjson path](https://github.com/tidwall/gjson#path-syntax)\n  ## to locate the default time of the measurements within the JSON document\n  # dropwizard_time_path = \"time\"\n  # dropwizard_time_format = \"2006-01-02T15:04:05Z07:00\"\n\n  ## You may use an appropriate [gjson path](https://github.com/tidwall/gjson#path-syntax)\n  ## to locate the tags map within the JSON document\n  # dropwizard_tags_path = \"tags\"\n\n  ## You may even use tag paths per tag\n  # [inputs.exec.dropwizard_tag_paths]\n  #   tag1 = \"tags.tag1\"\n  #   tag2 = \"tags.tag2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Output Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Datadog Output Plugin in Telegraf. It includes settings for API key, timeout, URL override, proxy configuration, compression, and rate interval conversion.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/datadog/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for DataDog API to send metrics to.\n[[outputs.datadog]]\n  ## Datadog API key\n  apikey = \"my-secret-key\"\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n  ## Write URL override; useful for debugging.\n  ## This plugin only supports the v1 API currently due to the authentication\n  ## method used.\n  # url = \"https://app.datadoghq.com/api/v1/series\"\n\n  ## Set http_proxy\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## Override the default (none) compression used to send data.\n  ## Supports: \"zlib\", \"none\"\n  # compression = \"none\"\n\n  ## When non-zero, converts count metrics submitted by inputs.statsd\n  ## into rate, while dividing the metric value by this number.\n  ## Note that in order for metrics to be submitted simultaenously alongside\n  ## a Datadog agent, rate_interval has to match the interval used by the\n  ## agent - which defaults to 10s\n  # rate_interval = 0s\n```\n\n----------------------------------------\n\nTITLE: Configuring CloudEvents Serializer in Telegraf with TOML\nDESCRIPTION: A comprehensive configuration example for the CloudEvents serializer in Telegraf. It demonstrates how to set up file outputs with the CloudEvents format, including options for specifying the CloudEvents version, source, event type, time header, and batch format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/cloudevents/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"cloudevents\"\n\n  ## Specification version to use for events\n  ## Currently versions \"0.3\" and \"1.0\" are supported.\n  # cloudevents_version = \"1.0\"\n\n  ## Event source specifier\n  ## This allows to overwrite the source header-field with the given value.\n  # cloudevents_source = \"telegraf\"\n\n  ## Tag to use as event source specifier\n  ## This allows to overwrite the source header-field with the value of the\n  ## specified tag. If both 'cloudevents_source' and 'cloudevents_source_tag'\n  ## are set, the this setting will take precedence. In case the specified tag\n  ## value does not exist for a metric, the serializer will fallback to\n  ## 'cloudevents_source'.\n  # cloudevents_source_tag = \"\"\n\n  ## Event-type specifier to overwrite the default value\n  ## By default, events (and event batches) containing a single metric will\n  ## set the event-type to 'com.influxdata.telegraf.metric' while events\n  ## containing a batch of metrics will set the event-type to\n  ## 'com.influxdata.telegraf.metric' (plural).\n  # cloudevents_event_type = \"\"\n\n  ## Set time header of the event\n  ## Supported values are:\n  ##   none     -- do not set event time\n  ##   earliest -- use timestamp of the earliest metric\n  ##   latest   -- use timestamp of the latest metric\n  ##   creation -- use timestamp of event creation\n  ## For events containing only a single metric, earliest and latest are\n  ## equivalent.\n  # cloudevents_event_time = \"latest\"\n\n  ## Batch format of the output when running in batch mode\n  ## If set to 'events' the resulting output will contain a list of events,\n  ## each with a single metric according to the JSON Batch Format of the\n  ## specification. Use 'application/cloudevents-batch+json' for this format.\n  ##\n  ## When set to 'metrics', a single event will be generated containing a list\n  ## of metrics as payload. Use 'application/cloudevents+json' for this format.\n  # cloudevents_batch_format = \"events\"\n```\n\n----------------------------------------\n\nTITLE: Example Unbound Plugin Output\nDESCRIPTION: This is an example output from the Telegraf Unbound plugin. It shows various metrics collected from Unbound, including query statistics, cache hit/miss rates, and recursion times. The metrics are tagged with the host and thread ID.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/unbound/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nunbound,host=localhost total_requestlist_avg=0,total_requestlist_exceeded=0,total_requestlist_overwritten=0,total_requestlist_current_user=0,total_recursion_time_avg=0.029186,total_tcpusage=0,total_num_queries=51,total_num_queries_ip_ratelimited=0,total_num_recursivereplies=6,total_requestlist_max=0,time_now=1522804978.784814,time_elapsed=310.435217,total_num_cachemiss=6,total_num_zero_ttl=0,time_up=310.435217,total_num_cachehits=45,total_num_prefetch=0,total_requestlist_current_all=0,total_recursion_time_median=0.016384 1522804979000000000\nunbound_threads,host=localhost,thread=0 num_queries_ip_ratelimited=0,requestlist_current_user=0,recursion_time_avg=0.029186,num_prefetch=0,requestlist_overwritten=0,requestlist_exceeded=0,requestlist_current_all=0,tcpusage=0,num_cachehits=37,num_cachemiss=6,num_recursivereplies=6,requestlist_avg=0,num_queries=43,num_zero_ttl=0,requestlist_max=0,recursion_time_median=0.032768 1522804979000000000\nunbound_threads,host=localhost,thread=1 num_zero_ttl=0,recursion_time_avg=0,num_queries_ip_ratelimited=0,num_cachehits=8,num_prefetch=0,requestlist_exceeded=0,recursion_time_median=0,tcpusage=0,num_cachemiss=0,num_recursivereplies=0,requestlist_max=0,requestlist_overwritten=0,requestlist_current_user=0,num_queries=8,requestlist_avg=0,requestlist_current_all=0 1522804979000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CloudWatch Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Telegraf CloudWatch input plugin. It includes settings for AWS authentication, region selection, metric collection periods, request throttling, and metric filtering. The configuration allows for pulling specific metrics with dimension filters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Pull Metric Statistics from Amazon CloudWatch\n[[inputs.cloudwatch]]\n  ## Amazon Region\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and\n  ##    web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # web_identity_token_file = \"\"\n  # role_session_name = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## If you are using CloudWatch cross-account observability, you can\n  ## set IncludeLinkedAccounts to true in a monitoring account\n  ## and collect metrics from the linked source accounts\n  # include_linked_accounts = false\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Set http_proxy\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## The minimum period for Cloudwatch metrics is 1 minute (60s). However not\n  ## all metrics are made available to the 1 minute period. Some are collected\n  ## at 3 minute, 5 minute, or larger intervals.\n  ## See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n  ## Note that if a period is configured that is smaller than the minimum for a\n  ## particular metric, that metric will not be returned by the Cloudwatch API\n  ## and will not be collected by Telegraf.\n  #\n  ## Requested CloudWatch aggregation Period (required)\n  ## Must be a multiple of 60s.\n  period = \"5m\"\n\n  ## Collection Delay (required)\n  ## Must account for metrics availability via CloudWatch API\n  delay = \"5m\"\n\n  ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"5m\"\n\n  ## Recommended if \"delay\" and \"period\" are both within 3 hours of request\n  ## time. Invalid values will be ignored. Recently Active feature will only\n  ## poll for CloudWatch ListMetrics values that occurred within the last 3h.\n  ## If enabled, it will reduce total API usage of the CloudWatch ListMetrics\n  ## API and require less memory to retain.\n  ## Do not enable if \"period\" or \"delay\" is longer than 3 hours, as it will\n  ## not return data more than 3 hours old.\n  ## See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html\n  # recently_active = \"PT3H\"\n\n  ## Configure the TTL for the internal cache of metrics.\n  # cache_ttl = \"1h\"\n\n  ## Metric Statistic Namespaces, wildcards are allowed\n  # namespaces = [\"*\"]\n\n  ## Metric Format\n  ## This determines the format of the produces metrics. 'sparse', the default\n  ## will produce a unique field for each statistic. 'dense' will report all\n  ## statistics will be in a field called value and have a metric_name tag\n  ## defining the name of the statistic. See the plugin README for examples.\n  # metric_format = \"sparse\"\n\n  ## Maximum requests per second. Note that the global default AWS rate limit\n  ## is 50 reqs/sec, so if you define multiple namespaces, these should add up\n  ## to a maximum of 50.\n  ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n  # ratelimit = 25\n\n  ## Timeout for http requests made by the cloudwatch client.\n  # timeout = \"5s\"\n\n  ## Batch Size\n  ## The size of each batch to send requests to Cloudwatch. 500 is the\n  ## suggested largest size. If a request gets to large (413 errors), consider\n  ## reducing this amount.\n  # batch_size = 500\n\n  ## Namespace-wide statistic filters. These allow fewer queries to be made to\n  ## cloudwatch.\n  # statistic_include = [\"average\", \"sum\", \"minimum\", \"maximum\", sample_count]\n  # statistic_exclude = []\n\n  ## Metrics to Pull\n  ## Defaults to all Metrics in Namespace if nothing is provided\n  ## Refreshes Namespace available metrics every 1h\n  #[[inputs.cloudwatch.metrics]]\n  #  names = [\"Latency\", \"RequestCount\"]\n  #\n  #  ## Statistic filters for Metric.  These allow for retrieving specific\n  #  ## statistics for an individual metric.\n  #  # statistic_include = [\"average\", \"sum\", \"minimum\", \"maximum\", sample_count]\n  #  # statistic_exclude = []\n  #\n  #  ## Dimension filters for Metric.\n  #  ## All dimensions defined for the metric names must be specified in order\n  #  ## to retrieve the metric statistics.\n  #  ## 'value' has wildcard / 'glob' matching support such as 'p-*'.\n  #  [[inputs.cloudwatch.metrics.dimensions]]\n  #    name = \"LoadBalancerName\"\n  #    value = \"p-example\"\n```\n\n----------------------------------------\n\nTITLE: Configuring IPtables Plugin in Telegraf\nDESCRIPTION: Configuration template for the IPtables input plugin specifying sudo usage, lock settings, binary selection, and chain monitoring options. Requires setting table and chains parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/iptables/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.iptables]]\n  ## iptables require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n  ## Users must configure sudo to allow telegraf user to run iptables with\n  ## no password.\n  ## iptables can be restricted to only list command \"iptables -nvL\".\n  # use_sudo = false\n\n  ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n  ## Adjust your sudo settings appropriately if using this option\n  ## (\"iptables -w 5 -nvl\")\n  # use_lock = false\n\n  ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n  # binary = \"ip6tables\"\n  ## defines the table to monitor:\n  table = \"filter\"\n\n  ## defines the chains to monitor.\n  ## NOTE: iptables rules without a comment will not be monitored.\n  ## Read the plugin documentation for more information.\n  chains = [ \"INPUT\" ]\n```\n\n----------------------------------------\n\nTITLE: MongoDB Server Metrics Structure\nDESCRIPTION: Defines the core MongoDB server metrics including connections, operations, replication, storage, and performance counters. Contains both active metrics and deprecated fields that were changed in version 1.10.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- mongodb\n  - tags:\n    - hostname\n    - node_type\n    - rs_name\n  - fields:\n    - active_reads (integer)\n    - active_writes (integer)\n    [...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Starlark Processor in Telegraf\nDESCRIPTION: Sample configuration for the Starlark processor plugin, demonstrating basic script configuration and constant definition\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/starlark/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.starlark]]\n  source = '''\ndef apply(metric):\n  return metric\n'''\n\n  # File containing a Starlark script\n  # script = \"/usr/local/bin/myscript.star\"\n\n  # Constants for the Starlark script\n  # [processors.starlark.constants]\n  #   max_size = 10\n  #   threshold = 0.75\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache IoTDB Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the Apache IoTDB output plugin for Telegraf. It includes options for server connection, authentication, timeout, data type conversion, timestamp precision, and tag handling.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/iotdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Save metrics to an IoTDB Database\n[[outputs.iotdb]]\n  ## Configuration of IoTDB server connection\n  host = \"127.0.0.1\"\n  # port = \"6667\"\n\n  ## Configuration of authentication\n  # user = \"root\"\n  # password = \"root\"\n\n  ## Timeout to open a new session.\n  ## A value of zero means no timeout.\n  # timeout = \"5s\"\n\n  ## Configuration of type conversion for 64-bit unsigned int\n  ## IoTDB currently DOES NOT support unsigned integers (version 13.x).\n  ## 32-bit unsigned integers are safely converted into 64-bit signed integers by the plugin,\n  ## however, this is not true for 64-bit values in general as overflows may occur.\n  ## The following setting allows to specify the handling of 64-bit unsigned integers.\n  ## Available values are:\n  ##   - \"int64\"       --  convert to 64-bit signed integers and accept overflows\n  ##   - \"int64_clip\"  --  convert to 64-bit signed integers and clip the values on overflow to 9,223,372,036,854,775,807\n  ##   - \"text\"        --  convert to the string representation of the value\n  # uint64_conversion = \"int64_clip\"\n\n  ## Configuration of TimeStamp\n  ## TimeStamp is always saved in 64bits int. timestamp_precision specifies the unit of timestamp.\n  ## Available value:\n  ## \"second\", \"millisecond\", \"microsecond\", \"nanosecond\"(default)\n  # timestamp_precision = \"nanosecond\"\n\n  ## Handling of tags\n  ## Tags are not fully supported by IoTDB.\n  ## A guide with suggestions on how to handle tags can be found here:\n  ##     https://iotdb.apache.org/UserGuide/Master/API/InfluxDB-Protocol.html\n  ##\n  ## Available values are:\n  ##   - \"fields\"     --  convert tags to fields in the measurement\n  ##   - \"device_id\"  --  attach tags to the device ID\n  ##\n  ## For Example, a metric named \"root.sg.device\" with the tags `tag1: \"private\"`  and  `tag2: \"working\"` and\n  ##  fields `s1: 100`  and `s2: \"hello\"` will result in the following representations in IoTDB\n  ##   - \"fields\"     --  root.sg.device, s1=100, s2=\"hello\", tag1=\"private\", tag2=\"working\"\n  ##   - \"device_id\"  --  root.sg.device.private.working, s1=100, s2=\"hello\"\n  # convert_tags_to = \"device_id\"\n\n  ## Handling of unsupported characters\n  ## Some characters in different versions of IoTDB are not supported in path name\n  ## A guide with suggetions on valid paths can be found here:\n  ## for iotdb 0.13.x           -> https://iotdb.apache.org/UserGuide/V0.13.x/Reference/Syntax-Conventions.html#identifiers\n  ## for iotdb 1.x.x and above  -> https://iotdb.apache.org/UserGuide/V1.3.x/User-Manual/Syntax-Rule.html#identifier\n  ##\n  ## Available values are:\n  ##   - \"1.0\", \"1.1\", \"1.2\", \"1.3\"  -- use backticks to enclose tags with forbidden characters\n  ##                                    such as @$#:[]{}() and space\n  ##   - \"0.13\"                      -- use backticks to enclose tags with forbidden characters\n  ##                                    such as space\n  ## Keep this section commented if you don't want to sanitize the path\n  # sanitize_tag = \"1.3\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Parser Plugin in Telegraf (TOML)\nDESCRIPTION: This snippet shows the complete configuration options for the CSV parser plugin in Telegraf. It includes settings for header rows, column names, data types, metadata handling, and various parsing options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/csv/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"csv\"\n\n  ## Indicates how many rows to treat as a header. By default, the parser assumes\n  ## there is no header and will parse the first row as data. If set to anything more\n  ## than 1, column names will be concatenated with the name listed in the next header row.\n  ## If `csv_column_names` is specified, the column names in header will be overridden.\n  csv_header_row_count = 0\n\n  ## For assigning custom names to columns\n  ## If this is specified, all columns should have a name\n  ## Unnamed columns will be ignored by the parser.\n  ## If `csv_header_row_count` is set to 0, this config must be used\n  csv_column_names = []\n\n  ## For assigning explicit data types to columns.\n  ## Supported types: \"int\", \"float\", \"bool\", \"string\".\n  ## Specify types in order by column (e.g. `[\"string\", \"int\", \"float\"]`)\n  ## If this is not specified, type conversion will be done on the types above.\n  csv_column_types = []\n\n  ## Indicates the number of rows to skip before looking for metadata and header information.\n  csv_skip_rows = 0\n\n  ## Indicates the number of rows to parse as metadata before looking for header information.\n  ## By default, the parser assumes there are no metadata rows to parse.\n  ## If set, the parser would use the provided separators in the csv_metadata_separators to look for metadata.\n  ## Please note that by default, the (key, value) pairs will be added as tags.\n  ## If fields are required, use the converter processor.\n  csv_metadata_rows = 0\n\n  ## A list of metadata separators. If csv_metadata_rows is set,\n  ## csv_metadata_separators must contain at least one separator.\n  ## Please note that separators are case sensitive and the sequence of the separators are respected.\n  csv_metadata_separators = [\":\", \"=\"]\n\n  ## A set of metadata trim characters.\n  ## If csv_metadata_trim_set is not set, no trimming is performed.\n  ## Please note that the trim cutset is case sensitive.\n  csv_metadata_trim_set = \"\"\n\n  ## Indicates the number of columns to skip before looking for data to parse.\n  ## These columns will be skipped in the header as well.\n  csv_skip_columns = 0\n\n  ## The separator between csv fields\n  ## By default, the parser assumes a comma (\",\")\n  ## Please note that if you use invalid delimiters (e.g. \"\\u0000\"), commas\n  ## will be changed to \"\\ufffd\", the invalid delimiters changed to a comma\n  ## during parsing, and afterwards the invalid characters and commas are\n  ## returned to their original values.\n  csv_delimiter = \",\"\n\n  ## The character reserved for marking a row as a comment row\n  ## Commented rows are skipped and not parsed\n  csv_comment = \"\"\n\n  ## If set to true, the parser will remove leading whitespace from fields\n  ## By default, this is false\n  csv_trim_space = false\n\n  ## Columns listed here will be added as tags. Any other columns\n  ## will be added as fields.\n  csv_tag_columns = []\n\n  ## Set to true to let the column tags overwrite the metadata and default tags.\n  csv_tag_overwrite = false\n\n  ## The column to extract the name of the metric from. Will not be\n  ## included as field in metric.\n  csv_measurement_column = \"\"\n\n  ## The column to extract time information for the metric\n  ## `csv_timestamp_format` must be specified if this is used.\n  ## Will not be included as field in metric.\n  csv_timestamp_column = \"\"\n\n  ## The format of time data extracted from `csv_timestamp_column`\n  ## this must be specified if `csv_timestamp_column` is specified\n  csv_timestamp_format = \"\"\n\n  ## The timezone of time data extracted from `csv_timestamp_column`\n  ## in case of there is no timezone information.\n  ## It follows the  IANA Time Zone database.\n  csv_timezone = \"\"\n\n  ## Indicates values to skip, such as an empty string value \"\".\n  ## The field will be skipped entirely where it matches any values inserted here.\n  csv_skip_values = []\n\n  ## If set to true, the parser will skip csv lines that cannot be parsed.\n  ## By default, this is false\n  csv_skip_errors = false\n\n  ## Reset the parser on given conditions.\n  ## This option can be used to reset the parser's state e.g. when always reading a\n  ## full CSV structure including header etc. Available modes are\n  ##    \"none\"   -- do not reset the parser (default)\n  ##    \"always\" -- reset the parser with each call (ignored in line-wise parsing)\n  ##                Helpful when e.g. reading whole files in each gather-cycle.\n  # csv_reset_mode = \"none\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Aggregator Plugin with MinMax in TOML\nDESCRIPTION: Example showing how to configure the MinMax aggregator to collect system metrics. Demonstrates period setting and original metric dropping.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.system]]\n  fieldinclude = [\"load1\"] # collects system load1 metric.\n\n[[aggregators.minmax]]\n  period = \"30s\"        # send & clear the aggregate every 30s.\n  drop_original = true  # drop the original metrics.\n\n[[outputs.file]]\n  files = [\"stdout\"]\n```\n\n----------------------------------------\n\nTITLE: Example Telegraf Libvirt Metrics Output\nDESCRIPTION: Sample output from the Telegraf Libvirt plugin showing various metrics collected from a virtual machine named 'U22', including CPU affinity, balloon statistics, VCPU usage, network statistics, block device information, performance metrics, and memory bandwidth monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/libvirt/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nlibvirt_cpu_affinity,domain_name=U22,host=localhost,vcpu_id=0 cpu_id=\"1,2,3\" 1662383707000000000\nlibvirt_cpu_affinity,domain_name=U22,host=localhost,vcpu_id=1 cpu_id=\"1,2,3,4,5,6,7,8,9,10\" 1662383707000000000\nlibvirt_balloon,domain_name=U22,host=localhost current=4194304i,maximum=4194304i,swap_in=0i,swap_out=0i,major_fault=0i,minor_fault=0i,unused=3928628i,available=4018480i,rss=1036012i,usable=3808724i,last_update=1654611373i,disk_caches=68820i,hugetlb_pgalloc=0i,hugetlb_pgfail=0i 1662383709000000000\nlibvirt_vcpu_total,domain_name=U22,host=localhost maximum=2i,current=2i 1662383709000000000\nlibvirt_vcpu,domain_name=U22,host=localhost,vcpu_id=0 state=1i,time=17943740000000i,wait=0i,halted=\"no\",halted_i=0i,delay=14246609424i,cpu_id=1i 1662383709000000000\nlibvirt_vcpu,domain_name=U22,host=localhost,vcpu_id=1 state=1i,time=18288400000000i,wait=0i,halted=\"yes\",halted_i=1i,delay=12902231142i,cpu_id=3i 1662383709000000000\nlibvirt_net_total,domain_name=U22,host=localhost count=1i 1662383709000000000\nlibvirt_net,domain_name=U22,host=localhost,interface_id=0 name=\"vnet0\",rx_bytes=110i,rx_pkts=1i,rx_errs=0i,rx_drop=31007i,tx_bytes=0i,tx_pkts=0i,tx_errs=0i,tx_drop=0i 1662383709000000000\nlibvirt_block_total,domain_name=U22,host=localhost count=1i 1662383709000000000\nlibvirt_block,domain_name=U22,host=localhost,block_id=0 rd=17337818234i,path=name=\"vda\",backingIndex=1i,path=\"/tmp/ubuntu_image.img\",rd_reqs=11354i,rd_bytes=330314752i,rd_times=6240559566i,wr_reqs=52440i,wr_bytes=1183828480i,wr_times=21887150375i,fl_reqs=32250i,fl_times=23158998353i,errors=0i,allocation=770048000i,capacity=2361393152i,physical=770052096i,threshold=2147483648i\nlibvirt_perf,domain_name=U22,host=localhost cmt=19087360i,mbmt=77168640i,mbml=67788800i,cpu_cycles=29858995122i,instructions=0i,cache_references=3053301695i,cache_misses=609441024i,branch_instructions=2623890194i,branch_misses=103707961i,bus_cycles=188105628i,stalled_cycles_frontend=0i,stalled_cycles_backend=0i,ref_cpu_cycles=30766094039i,cpu_clock=25166642695i,task_clock=25263578917i,page_faults=2670i,context_switches=294284i,cpu_migrations=17949i,page_faults_min=2670i,page_faults_maj=0i,alignment_faults=0i,emulation_faults=0i 1662383709000000000\nlibvirt_dirtyrate,domain_name=U22,host=localhost calc_status=2i,calc_start_time=348414i,calc_period=1i,dirtyrate.megabytes_per_second=4i,calc_mode=\"dirty-ring\" 1662383709000000000\nlibvirt_dirtyrate_vcpu,domain_name=U22,host=localhost,vcpu_id=0 megabytes_per_second=2i 1662383709000000000\nlibvirt_dirtyrate_vcpu,domain_name=U22,host=localhost,vcpu_id=1 megabytes_per_second=2i 1662383709000000000\nlibvirt_state,domain_name=U22,host=localhost state=1i,reason=5i 1662383709000000000\nlibvirt_cpu,domain_name=U22,host=localhost time=67419144867000i,user=63886161852000i,system=3532983015000i,haltpoll_success_time=516907915i,haltpoll_fail_time=2727253643i 1662383709000000000\nlibvirt_cpu_cache_monitor_total,domain_name=U22,host=localhost count=1i 1662383709000000000\nlibvirt_cpu_cache_monitor,domain_name=U22,host=localhost,cache_monitor_id=0 name=\"any_name_vcpus_0-3\",vcpus=\"0-3\",bank_count=1i 1662383709000000000\nlibvirt_cpu_cache_monitor_bank,domain_name=U22,host=localhost,cache_monitor_id=0,bank_index=0 id=0i,bytes=5406720i 1662383709000000000\nlibvirt_iothread_total,domain_name=U22,host=localhost count=1i 1662383709000000000\nlibvirt_iothread,domain_name=U22,host=localhost,iothread_id=0 poll_max_ns=32768i,poll_grow=0i,poll_shrink=0i 1662383709000000000\nlibvirt_memory_bandwidth_monitor_total,domain_name=U22,host=localhost count=2i 1662383709000000000\nlibvirt_memory_bandwidth_monitor,domain_name=U22,host=localhost,memory_bandwidth_monitor_id=0 name=\"any_name_vcpus_0-4\",vcpus=\"0-4\",node_count=2i 1662383709000000000\nlibvirt_memory_bandwidth_monitor,domain_name=U22,host=localhost,memory_bandwidth_monitor_id=1 name=\"vcpus_7\",vcpus=\"7\",node_count=2i 1662383709000000000\nlibvirt_memory_bandwidth_monitor_node,domain_name=U22,host=localhost,memory_bandwidth_monitor_id=0,controller_index=0 id=0i,bytes_total=10208067584i,bytes_local=4807114752i 1662383709000000000\nlibvirt_memory_bandwidth_monitor_node,domain_name=U22,host=localhost,memory_bandwidth_monitor_id=0,controller_index=1 id=1i,bytes_total=8693735424i,bytes_local=5850161152i 1662383709000000000\nlibvirt_memory_bandwidth_monitor_node,domain_name=U22,host=localhost,memory_bandwidth_monitor_id=1,controller_index=0 id=0i,bytes_total=853811200i,bytes_local=290701312i 1662383709000000000\nlibvirt_memory_bandwidth_monitor_node,domain_name=U22,host=localhost,memory_bandwidth_monitor_id=1,controller_index=1 id=1i,bytes_total=406044672i,bytes_local=229425152i 1662383709000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Monitor Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Azure Monitor output plugin for Telegraf. It includes settings for timeout, namespace prefix, string-to-dimension conversion, region and resource ID specification, and timestamp limitations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/azure_monitor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send aggregate metrics to Azure Monitor\n[[outputs.azure_monitor]]\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Set the namespace prefix, defaults to \"Telegraf/<input-name>\".\n  # namespace_prefix = \"Telegraf/\"\n\n  ## Azure Monitor doesn't have a string value type, so convert string\n  ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n  ## a maximum of 10 dimensions so Telegraf will only send the first 10\n  ## alphanumeric dimensions.\n  # strings_as_dimensions = false\n\n  ## Both region and resource_id must be set or be available via the\n  ## Instance Metadata service on Azure Virtual Machines.\n  #\n  ## Azure Region to publish metrics against.\n  ##   ex: region = \"southcentralus\"\n  # region = \"\"\n  #\n  ## The Azure Resource ID against which metric will be logged, e.g.\n  ##   ex: resource_id = \"/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>\"\n  # resource_id = \"\"\n\n  ## Optionally, if in Azure US Government, China, or other sovereign\n  ## cloud environment, set the appropriate REST endpoint for receiving\n  ## metrics. (Note: region may be unused in this context)\n  # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n\n  ## Time limitations of metric to send\n  ## Documentation can be found here:\n  ##   https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-store-custom-rest-api?tabs=rest#timestamp\n  ## However, the returned (400) error message might document more strict or\n  ## relaxed settings. By default, only past metrics witin the limit are sent.\n  # timestamp_limit_past = \"30m\"\n  # timestamp_limit_future = \"-1m\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OPC UA Client Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet defines the settings for the OPC UA client input plugin in Telegraf. It includes options for endpoint URL, timeouts, security settings, authentication, and node configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Retrieve data from OPCUA devices\n[[inputs.opcua]]\n  ## Metric name\n  # name = \"opcua\"\n\n  ## OPC UA Endpoint URL\n  # endpoint = \"opc.tcp://localhost:4840\"\n\n  ## Maximum time allowed to establish a connect to the endpoint.\n  # connect_timeout = \"10s\"\n\n  ## Maximum time allowed for a request over the established connection.\n  # request_timeout = \"5s\"\n\n  ## Maximum time that a session shall remain open without activity.\n  # session_timeout = \"20m\"\n\n  ## Retry options for failing reads e.g. due to invalid sessions\n  ## If the retry count is zero, the read will fail after the initial attempt.\n  # read_retry_timeout = \"100ms\"\n  # read_retry_count = 0\n\n  ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n  ## \"Basic256Sha256\", or \"auto\"\n  # security_policy = \"auto\"\n\n  ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n  # security_mode = \"auto\"\n\n  ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n  ## If cert path is not supplied, self-signed cert and key will be generated.\n  # certificate = \"/etc/telegraf/cert.pem\"\n\n  ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n  ## If key path is not supplied, self-signed cert and key will be generated.\n  # private_key = \"/etc/telegraf/key.pem\"\n\n  ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n  ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n  # auth_method = \"Anonymous\"\n\n  ## Username and password required for auth_method = \"UserName\"\n  # username = \"\"\n  # password = \"\"\n\n  ## Option to select the metric timestamp to use. Valid options are:\n  ##     \"gather\" -- uses the time of receiving the data in telegraf\n  ##     \"server\" -- uses the timestamp provided by the server\n  ##     \"source\" -- uses the timestamp provided by the source\n  # timestamp = \"gather\"\n\n  ## Client trace messages\n  ## When set to true, and debug mode enabled in the agent settings, the OPCUA\n  ## client's messages are included in telegraf logs. These messages are very\n  ## noisey, but essential for debugging issues.\n  # client_trace = false\n\n  ## Include additional Fields in each metric\n  ## Available options are:\n  ##   DataType -- OPC-UA Data Type (string)\n  # optional_fields = []\n\n  ## Node ID configuration\n  ## name              - field name to use in the output\n  ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\n  ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\n  ## identifier        - OPC UA ID (tag as shown in opcua browser)\n  ## default_tags      - extra tags to be added to the output metric (optional)\n  ##\n  ## Use either the inline notation or the bracketed notation, not both.\n\n  ## Inline notation (default_tags not supported yet)\n  # nodes = [\n  #   {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  # ]\n\n  ## Bracketed notation\n  # [[inputs.opcua.nodes]]\n  #   name = \"node1\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n  #   default_tags = { tag1 = \"value1\", tag2 = \"value2\" }\n  #\n  # [[inputs.opcua.nodes]]\n  #   name = \"node2\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n\n  ## Node Group\n  ## Sets defaults so they aren't required in every node.\n  ## Default values can be set for:\n  ## * Metric name\n  ## * OPC UA namespace\n  ## * Identifier\n  ## * Default tags\n  ##\n  ## Multiple node groups are allowed\n  #[[inputs.opcua.group]]\n  ## Group Metric name. Overrides the top level name.  If unset, the\n  ## top level name is used.\n  # name =\n\n  ## Group default namespace. If a node in the group doesn't set its\n  ## namespace, this is used.\n  # namespace =\n\n  ## Group default identifier type. If a node in the group doesn't set its\n  ## namespace, this is used.\n  # identifier_type =\n\n  ## Default tags that are applied to every node in this group. Can be\n  ## overwritten in a node by setting a different value for the tag name.\n  ##   example: default_tags = { tag1 = \"value1\" }\n  # default_tags = {}\n\n  ## Node ID Configuration. Array of nodes with the same settings as above.\n  ## Use either the inline notation or the bracketed notation, not both.\n\n  ## Inline notation (default_tags not supported yet)\n  # nodes = [\n  #  {name=\"node1\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #  {name=\"node2\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #]\n\n  ## Bracketed notation\n  # [[inputs.opcua.group.nodes]]\n  #   name = \"node1\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n  #   default_tags = { tag1 = \"override1\", tag2 = \"value2\" }\n  #\n  # [[inputs.opcua.group.nodes]]\n  #   name = \"node2\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n\n  ## Enable workarounds required by some devices to work correctly\n  # [inputs.opcua.workarounds]\n  #   ## Set additional valid status codes, StatusOK (0x0) is always considered valid\n  #   # additional_valid_status_codes = [\"0xC0\"]\n\n  # [inputs.opcua.request_workarounds]\n  #   ## Use unregistered reads instead of registered reads\n  #   # use_unregistered_reads = false\n```\n\n----------------------------------------\n\nTITLE: Configuring File Output Plugin in Telegraf using TOML\nDESCRIPTION: This snippet provides a sample configuration for the File Output Plugin in Telegraf. It demonstrates various options including file paths, batch formatting, rotation settings, data format selection, and compression algorithms.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/file/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send telegraf metrics to file(s)\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Use batch serialization format instead of line based delimiting.  The\n  ## batch format allows for the production of non line based output formats and\n  ## may more efficiently encode and write metrics.\n  # use_batch_format = false\n\n  ## The file will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.\n  # rotation_interval = \"0h\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # rotation_max_archives = 5\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## Compress output data with the specified algorithm.\n  ## If empty, compression will be disabled and files will be plain text.\n  ## Supported algorithms are \"zstd\", \"gzip\" and \"zlib\".\n  # compression_algorithm = \"\"\n\n  ## Compression level for the algorithm above.\n  ## Please note that different algorithms support different levels:\n  ##   zstd  -- supports levels 1, 3, 7 and 11.\n  ##   gzip -- supports levels 0, 1 and 9.\n  ##   zlib -- supports levels 0, 1, and 9.\n  ## By default the default compression level for each algorithm is used.\n  # compression_level = -1\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Input Plugin for Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Consul input plugin for Telegraf. It includes options for server address, URI scheme, metric version, authentication, and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/consul/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather health check statuses from services registered in Consul\n[[inputs.consul]]\n  ## Consul server address\n  # address = \"localhost:8500\"\n\n  ## URI scheme for the Consul server, one of \"http\", \"https\"\n  # scheme = \"http\"\n\n  ## Metric version controls the mapping from Consul metrics into\n  ## Telegraf metrics. Version 2 moved all fields with string values\n  ## to tags.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.16\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## ACL token used in every request\n  # token = \"\"\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Data center to query the health checks from\n  # datacenter = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = true\n\n  ## Consul checks' tag splitting\n  # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n  # they will be split and reported as proper key:value in Telegraf\n  # tag_delimiter = \":\"\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Objects in Telegraf\nDESCRIPTION: Configuration for parsing JSON objects with nested data in Telegraf, specifying object path, timestamp key, tag and field paths, and data types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PARSING_DATA.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"metrics\": [\n        { \"node\": \"node1\", \"temp\": 32.3, \"humidity\": 23, \"alarm\": \"false\", \"time\": \"1678121543\"},\n        { \"node\": \"node2\", \"temp\": 22.6, \"humidity\": 44, \"alarm\": \"false\", \"time\": \"1678121543\"},\n        { \"node\": \"node3\", \"temp\": 17.9, \"humidity\": 56, \"alarm\": \"true\", \"time\": \"1678121543\"}\n    ]\n}\n```\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\nfiles = [\"test.json\"]\ndata_format = \"json_v2\"\n\n[[inputs.file.json_v2]]\n[[inputs.file.json_v2.object]]\n  path = \"metrics\"\n  timestamp_key = \"time\"\n  timestamp_format = \"unix\"\n  [[inputs.file.json_v2.object.tag]]\n    path = \"#.node\"\n  [[inputs.file.json_v2.object.field]]\n    path = \"#.temp\"\n    type = \"float\"\n  [[inputs.file.json_v2.object.field]]\n    path = \"#.humidity\"\n    type = \"int\"\n  [[inputs.file.json_v2.object.field]]\n    path = \"#.alarm\"\n    type = \"bool\"\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Input Plugin in TOML\nDESCRIPTION: TOML configuration for the RabbitMQ input plugin in Telegraf. It includes options for URL, credentials, TLS config, timeouts, and filtering of nodes, exchanges, metrics, queues, and federation upstreams.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/rabbitmq/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Reads metrics from RabbitMQ servers via the Management Plugin\n[[inputs.rabbitmq]]\n  ## Management Plugin url. (default: http://localhost:15672)\n  # url = \"http://localhost:15672\"\n\n  ## Credentials\n  # username = \"guest\"\n  # password = \"guest\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional request timeouts\n  ##\n  ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## A list of nodes to gather as the rabbitmq_node measurement. If not\n  ## specified, metrics for all nodes are gathered.\n  # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n  ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n  ## specified, metrics for all exchanges are gathered.\n  # exchanges = [\"telegraf\"]\n\n  ## Metrics to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all metrics\n  ## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n  # metric_include = []\n  # metric_exclude = []\n\n  ## Queues to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all queues\n  # queue_name_include = []\n  # queue_name_exclude = []\n\n  ## Federation upstreams to include and exclude specified as an array of glob\n  ## pattern strings.  Federation links can also be limited by the queue and\n  ## exchange filters.\n  # federation_upstream_include = []\n  # federation_upstream_exclude = []\n```\n\n----------------------------------------\n\nTITLE: Configuring Multifile Input Plugin with Base Directory and Files\nDESCRIPTION: This snippet demonstrates the configuration of the Multifile input plugin using TOML. The configuration specifies the base directory for file paths and lists individual files to be parsed. Each file includes parameters for the destination field name and data conversion type.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/multifile/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n\"[[inputs.multifile]]\\n  base_dir = \\\"/sys/bus/i2c/devices/1-0076/iio:device0\\\"\\n  [[inputs.multifile.file]]\\n    file = \\\"in_pressure_input\\\"\\n    dest = \\\"pressure\\\"\\n    conversion = \\\"float\\\"\\n  [[inputs.multifile.file]]\\n    file = \\\"in_temp_input\\\"\\n    dest = \\\"temperature\\\"\\n    conversion = \\\"float(3)\\\"\\n  [[inputs.multifile.file]]\\n    file = \\\"in_humidityrelative_input\\\"\\n    dest = \\\"humidityrelative\\\"\\n    conversion = \\\"float(3)\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Active Directory Domain Controller Monitoring with Telegraf\nDESCRIPTION: Configuration for monitoring Active Directory services using win_perf_counters. Tracks DirectoryServices performance metrics, security authentication statistics, and database cache performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [inputs.win_perf_counters.tags]\n    monitorgroup = \"ActiveDirectory\"\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"DirectoryServices\"\n    Instances = [\"*\"]\n    Counters = [\"Base Searches/sec\",\"Database adds/sec\",\"Database deletes/sec\",\"Database modifys/sec\",\"Database recycles/sec\",\"LDAP Client Sessions\",\"LDAP Searches/sec\",\"LDAP Writes/sec\"]\n    Measurement = \"win_ad\" # Set an alternative measurement to win_perf_counters if wanted.\n    #Instances = [\"\"] # Gathers all instances by default, specify to only gather these\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"Security System-Wide Statistics\"\n    Instances = [\"*\"]\n    Counters = [\"NTLM Authentications\",\"Kerberos Authentications\",\"Digest Authentications\"]\n    Measurement = \"win_ad\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    ObjectName = \"Database\"\n    Instances = [\"*\"]\n    Counters = [\"Database Cache % Hit\",\"Database Cache Page Fault Stalls/sec\",\"Database Cache Page Faults/sec\",\"Database Cache Size\"]\n    Measurement = \"win_db\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Google Cloud Storage input plugin in Telegraf. It demonstrates setting up bucket access, key prefixes, offset tracking, and data format options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/google_cloud_storage/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather metrics by iterating the files located on a Cloud Storage Bucket.\n[[inputs.google_cloud_storage]]\n  ## Required. Name of Cloud Storage bucket to ingest metrics from.\n  bucket = \"my-bucket\"\n\n  ## Optional. Prefix of Cloud Storage bucket keys to list metrics from.\n  # key_prefix = \"my-bucket\"\n\n  ## Key that will store the offsets in order to pick up where the ingestion was left.\n  offset_key = \"offset_key\"\n\n  ## Key that will store the offsets in order to pick up where the ingestion was left.\n  objects_per_iteration = 10\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## Google Cloud Storage APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n```\n\n----------------------------------------\n\nTITLE: PowerDNS Metrics Output Example\nDESCRIPTION: Sample output showing the format and types of metrics collected by the PowerDNS plugin, including various counters and performance metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/powerdns/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\npowerdns,server=/var/run/pdns.controlsocket corrupt-packets=0i,deferred-cache-inserts=0i,deferred-cache-lookup=0i,dnsupdate-answers=0i,dnsupdate-changes=0i,dnsupdate-queries=0i,dnsupdate-refused=0i,key-cache-size=0i,latency=26i,meta-cache-size=0i,packetcache-hit=0i,packetcache-miss=1i,packetcache-size=0i,qsize-q=0i,query-cache-hit=0i,query-cache-miss=6i,rd-queries=1i,recursing-answers=0i,recursing-questions=0i,recursion-unanswered=0i,security-status=3i,servfail-packets=0i,signature-cache-size=0i,signatures=0i,sys-msec=4349i,tcp-answers=0i,tcp-queries=0i,timedout-packets=0i,udp-answers=1i,udp-answers-bytes=50i,udp-do-queries=0i,udp-queries=0i,udp4-answers=1i,udp4-queries=1i,udp6-answers=0i,udp6-queries=0i,uptime=166738i,user-msec=3036i 1454078624932715706\n```\n\n----------------------------------------\n\nTITLE: Configuring vSphere Input Settings - TOML\nDESCRIPTION: This section configures the vSphere input plugin for Telegraf, specifying vCenter connection parameters and the metrics to collect for VMs and hosts. It allows flexibility in metric inclusion or exclusion based on user requirements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.vsphere]]\n  vcenters = [ \"https://vcenter.local/sdk\" ]\n  username = \"user@corp.local\"\n  password = \"secret\"\n  vm_metric_include = [\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.run.summation\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.wait.summation\",\n    \"mem.active.average\",\n    \"mem.granted.average\",\n    \"mem.latency.average\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"virtualDisk.numberReadAveraged.average\",\n    \"virtualDisk.numberWriteAveraged.average\",\n    \"virtualDisk.read.average\",\n    \"virtualDisk.readOIO.latest\",\n    \"virtualDisk.throughput.usage.average\",\n    \"virtualDisk.totalReadLatency.average\",\n    \"virtualDisk.totalWriteLatency.average\",\n    \"virtualDisk.write.average\",\n    \"virtualDisk.writeOIO.latest\",\n    \"sys.uptime.latest\",\n  ]\n  host_metric_include = [\n    \"cpu.coreUtilization.average\",\n    \"cpu.costop.summation\",\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.swapwait.summation\",\n    \"cpu.usage.average\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.utilization.average\",\n    \"cpu.wait.summation\",\n    \"disk.deviceReadLatency.average\",\n    \"disk.deviceWriteLatency.average\",\n    \"disk.kernelReadLatency.average\",\n    \"disk.kernelWriteLatency.average\",\n    \"disk.numberReadAveraged.average\",\n    \"disk.numberWriteAveraged.average\",\n    \"disk.read.average\",\n    \"disk.totalReadLatency.average\",\n    \"disk.totalWriteLatency.average\",\n    \"disk.write.average\",\n    \"mem.active.average\",\n    \"mem.latency.average\",\n    \"mem.state.latest\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.totalCapacity.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.errorsRx.summation\",\n    \"net.errorsTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"storageAdapter.numberReadAveraged.average\",\n    \"storageAdapter.numberWriteAveraged.average\",\n    \"storageAdapter.read.average\",\n    \"storageAdapter.write.average\",\n    \"sys.uptime.latest\",\n  ]\n```\n\nLANGUAGE: toml\nCODE:\n```\n## To disable collection of a specific resource type, simply exclude all\n## metrics using the XX_metric_exclude. For example, to disable collection of VMs,\nvm_metric_exclude = [ \"*\" ]\n```\n\nLANGUAGE: toml\nCODE:\n```\n## number of objects to retrieve per query for realtime resources (VMs and hosts)\n## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n# max_query_objects = 256\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Performance Counters Input Plugin in TOML\nDESCRIPTION: This snippet shows the full configuration options for the win_perf_counters input plugin. It includes settings for CPU, disk, network, system, and memory performance counters, as well as global plugin options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n# Input plugin to counterPath Performance Counters on Windows operating systems\n# This plugin ONLY supports Windows\n[[inputs.win_perf_counters]]\n  ## By default this plugin returns basic CPU and Disk statistics. See the\n  ## README file for more examples. Uncomment examples below or write your own\n  ## as you see fit. If the system being polled for data does not have the\n  ## Object at startup of the Telegraf agent, it will not be gathered.\n\n  ## Print All matching performance counters\n  # PrintValid = false\n\n  ## Whether request a timestamp along with the PerfCounter data or use current\n  ## time\n  # UsePerfCounterTime = true\n\n  ## If UseWildcardsExpansion params is set to true, wildcards (partial\n  ## wildcards in instance names and wildcards in counters names) in configured\n  ## counter paths will be expanded and in case of localized Windows, counter\n  ## paths will be also localized. It also returns instance indexes in instance\n  ## names. If false, wildcards (not partial) in instance names will still be\n  ## expanded, but instance indexes will not be returned in instance names.\n  # UseWildcardsExpansion = false\n\n  ## When running on a localized version of Windows and with\n  ## UseWildcardsExpansion = true, Windows will localize object and counter\n  ## names. When LocalizeWildcardsExpansion = false, use the names in\n  ## object.Counters instead of the localized names. Only Instances can have\n  ## wildcards in this case. ObjectName and Counters must not have wildcards\n  ## when this setting is false.\n  # LocalizeWildcardsExpansion = true\n\n  ## Period after which counters will be reread from configuration and\n  ## wildcards in counter paths expanded\n  # CountersRefreshInterval=\"1m\"\n\n  ## Accepts a list of PDH error codes which are defined in pdh.go, if this\n  ## error is encountered it will be ignored. For example, you can provide\n  ## \"PDH_NO_DATA\" to ignore performance counters with no instances. By default\n  ## no errors are ignored You can find the list here:\n  ##   https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/pdh.go\n  ## e.g. IgnoredErrors = [\"PDH_NO_DATA\"]\n  # IgnoredErrors = []\n\n  ## Maximum size of the buffer for values returned by the API\n  ## Increase this value if you experience \"buffer limit reached\" errors.\n  # MaxBufferSize = \"4MiB\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"\"\n    # ObjectName = \"\"\n    # Instances = [\"\"]\n    # Counters = []\n    ## Additional Object Settings\n    ##   * IncludeTotal: set to true to include _Total instance when querying\n    ##                   for all metrics via '*'\n    ##   * WarnOnMissing: print out when the performance counter is missing\n    ##                    from object, counter or instance\n    ##   * UseRawValues: gather raw values instead of formatted. Raw values are\n    ##                   stored in the field name with the \"_Raw\" suffix, e.g.\n    ##                   \"Disk_Read_Bytes_sec_Raw\".\n    # IncludeTotal = false\n    # WarnOnMissing = false\n    # UseRawValues = false\n\n  ## Processor usage, alternative to native, reports on a per core.\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_cpu\"\n    # ObjectName = \"Processor\"\n    # Instances = [\"*\"]\n    # UseRawValues = true\n    # Counters = [\n    #   \"% Idle Time\",\n    #   \"% Interrupt Time\",\n    #   \"% Privileged Time\",\n    #   \"% User Time\",\n    #   \"% Processor Time\",\n    #   \"% DPC Time\",\n    # ]\n\n  ## Disk times and queues\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_disk\"\n    # ObjectName = \"LogicalDisk\"\n    # Instances = [\"*\"]\n    # Counters = [\n    #   \"% Idle Time\",\n    #   \"% Disk Time\",\n    #   \"% Disk Read Time\",\n    #   \"% Disk Write Time\",\n    #   \"% User Time\",\n    #   \"% Free Space\",\n    #   \"Current Disk Queue Length\",\n    #   \"Free Megabytes\",\n    # ]\n\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_diskio\"\n    # ObjectName = \"PhysicalDisk\"\n    # Instances = [\"*\"]\n    # Counters = [\n    #   \"Disk Read Bytes/sec\",\n    #   \"Disk Write Bytes/sec\",\n    #   \"Current Disk Queue Length\",\n    #   \"Disk Reads/sec\",\n    #   \"Disk Writes/sec\",\n    #   \"% Disk Time\",\n    #   \"% Disk Read Time\",\n    #   \"% Disk Write Time\",\n    # ]\n\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_net\"\n    # ObjectName = \"Network Interface\"\n    # Instances = [\"*\"]\n    # Counters = [\n    # \"Bytes Received/sec\",\n    # \"Bytes Sent/sec\",\n    # \"Packets Received/sec\",\n    # \"Packets Sent/sec\",\n    # \"Packets Received Discarded\",\n    # \"Packets Outbound Discarded\",\n    # \"Packets Received Errors\",\n    # \"Packets Outbound Errors\",\n    # ]\n\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_system\"\n    # ObjectName = \"System\"\n    # Instances = [\"------\"]\n    # Counters = [\n    #   \"Context Switches/sec\",\n    #   \"System Calls/sec\",\n    #   \"Processor Queue Length\",\n    #   \"System Up Time\",\n    # ]\n\n  ## Example counterPath where the Instance portion must be removed to get\n  ## data back, such as from the Memory object.\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_mem\"\n    # ObjectName = \"Memory\"\n    ## Use 6 x - to remove the Instance bit from the counterPath.\n    # Instances = [\"------\"]\n    # Counters = [\n    #   \"Available Bytes\",\n    #   \"Cache Faults/sec\",\n    #   \"Demand Zero Faults/sec\",\n    #   \"Page Faults/sec\",\n    #   \"Pages/sec\",\n    #   \"Transition Faults/sec\",\n    #   \"Pool Nonpaged Bytes\",\n    #   \"Pool Paged Bytes\",\n    #   \"Standby Cache Reserve Bytes\",\n    #   \"Standby Cache Normal Priority Bytes\",\n    #   \"Standby Cache Core Bytes\",\n    # ]\n\n  ## Example query where the Instance portion must be removed to get data back,\n  ## such as from the Paging File object.\n  # [[inputs.win_perf_counters.object]]\n    # Measurement = \"win_swap\"\n    # ObjectName = \"Paging File\"\n    # Instances = [\"_Total\"]\n    # Counters = [\n    #   \"% Usage\",\n    # ]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS EC2 Metadata Processor\nDESCRIPTION: Complete configuration sample showing all available options for the AWS EC2 metadata processor plugin including IMDS tags, EC2 tags, metadata paths, timeouts, caching, and performance settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/aws_ec2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.aws_ec2]]\n  ## Instance identity document tags to attach to metrics.\n  ## For more information see:\n  ## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html\n  ##\n  ## Available tags:\n  ## * accountId\n  ## * architecture\n  ## * availabilityZone\n  ## * billingProducts\n  ## * imageId\n  ## * instanceId\n  ## * instanceType\n  ## * kernelId\n  ## * pendingTime\n  ## * privateIp\n  ## * ramdiskId\n  ## * region\n  ## * version\n  # imds_tags = []\n\n  ## EC2 instance tags retrieved with DescribeTags action.\n  ## In case tag is empty upon retrieval it's omitted when tagging metrics.\n  ## Note that in order for this to work, role attached to EC2 instance or AWS\n  ## credentials available from the environment must have a policy attached, that\n  ## allows ec2:DescribeTags.\n  ##\n  ## For more information see:\n  ## https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html\n  # ec2_tags = []\n\n  ## Paths to instance metadata information to attach to the metrics.\n  ## Specify the full path without the base-path e.g. `tags/instance/Name`.\n  ##\n  ## For more information see:\n  ## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\n  # metadata_paths = []\n\n  ## Allows to convert metadata tag-names to canonical names representing the\n  ## full path with slashes ('/') being replaces with underscores. By default,\n  ## only the last path element is used to name the tag.\n  # canonical_metadata_tags = false\n\n  ## Timeout for http requests made by against aws ec2 metadata endpoint.\n  # timeout = \"10s\"\n\n  ## ordered controls whether or not the metrics need to stay in the same order\n  ## this plugin received them in. If false, this plugin will change the order\n  ## with requests hitting cached results moving through immediately and not\n  ## waiting on slower lookups. This may cause issues for you if you are\n  ## depending on the order of metrics staying the same. If so, set this to true.\n  ## Keeping the metrics ordered may be slightly slower.\n  # ordered = false\n\n  ## max_parallel_calls is the maximum number of AWS API calls to be in flight\n  ## at the same time.\n  ## It's probably best to keep this number fairly low.\n  # max_parallel_calls = 10\n\n  ## cache_ttl determines how long each cached item will remain in the cache before\n  ## it is removed and subsequently needs to be queried for from the AWS API. By\n  ## default, no items are cached.\n  # cache_ttl = \"0s\"\n\n  ## tag_cache_size determines how many of the values which are found in imds_tags\n  ## or ec2_tags will be kept in memory for faster lookup on successive processing\n  ## of metrics. You may want to adjust this if you have excessively large numbers\n  ## of tags on your EC2 instances, and you are using the ec2_tags field. This\n  ## typically does not need to be changed when using the imds_tags field.\n  # tag_cache_size = 1000\n\n  ## log_cache_stats will emit a log line periodically to stdout with details of\n  ## cache entries, hits, misses, and evacuations since the last time stats were\n  ## emitted. This can be helpful in determining whether caching is being effective\n  ## in your environment. Stats are emitted every 30 seconds. By default, this\n  ## setting is disabled.\n  # log_cache_stats = false\n```\n\n----------------------------------------\n\nTITLE: Example Telegraf Output for Fail2ban Metrics\nDESCRIPTION: Sample output showing the metrics collected by the Fail2ban input plugin, including jail tag, failed and banned counts.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fail2ban/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nfail2ban,jail=sshd failed=5i,banned=2i 1495868667000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring NSD Input Plugin in Telegraf\nDESCRIPTION: TOML configuration block for the NSD input plugin defining server connection settings, binary locations, and timeouts. Includes optional sudo access configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nsd]]\n  ## Address of server to connect to, optionally ':port'. Defaults to the\n  ## address in the nsd config file.\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the nsd-control binary can be overridden with:\n  # binary = \"/usr/sbin/nsd-control\"\n\n  ## The default location of the nsd config file can be overridden with:\n  # config_file = \"/etc/nsd/nsd.conf\"\n\n  ## The default timeout of 1s can be overridden with:\n  # timeout = \"1s\"\n```\n\n----------------------------------------\n\nTITLE: Complete Dynatrace Output Configuration\nDESCRIPTION: Full configuration example showing all available options including TLS settings, timeout, counter configurations, and custom dimensions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/dynatrace/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.dynatrace]]\n  ## For usage with the Dynatrace OneAgent you can omit any configuration,\n  ## the only requirement is that the OneAgent is running on the same host.\n  ## Only setup environment url and token if you want to monitor a Host without the OneAgent present.\n  ##\n  ## Your Dynatrace environment URL.\n  ## For Dynatrace OneAgent you can leave this empty or set it to \"http://127.0.0.1:14499/metrics/ingest\" (default)\n  ## For Dynatrace SaaS environments the URL scheme is \"https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest\"\n  ## For Dynatrace Managed environments the URL scheme is \"https://{your-domain}/e/{your-environment-id}/api/v2/metrics/ingest\"\n  url = \"\"\n\n  ## Your Dynatrace API token.\n  ## Create an API token within your Dynatrace environment, by navigating to Settings > Integration > Dynatrace API\n  ## The API token needs data ingest scope permission. When using OneAgent, no API token is required.\n  api_token = \"\"\n\n  ## Optional prefix for metric names (e.g.: \"telegraf\")\n  prefix = \"telegraf\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Optional flag for ignoring tls certificate check\n  # insecure_skip_verify = false\n\n  ## Connection timeout, defaults to \"5s\" if not set.\n  timeout = \"5s\"\n\n  ## If you want metrics to be treated and reported as delta counters, add the metric names here\n  additional_counters = [ ]\n\n  ## In addition or as an alternative to additional_counters, if you want metrics to be treated and\n  ## reported as delta counters using regular expression pattern matching\n  additional_counters_patterns = [ ]\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Optional dimensions to be added to every metric\n  # [outputs.dynatrace.default_dimensions]\n  # default_key = \"default value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ClickHouse Input Plugin in TOML\nDESCRIPTION: TOML configuration block for setting up the ClickHouse input plugin in Telegraf. Includes options for authentication, server endpoints, cluster discovery, and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/clickhouse/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or many ClickHouse servers\n[[inputs.clickhouse]]\n  ## Username for authorization on ClickHouse server\n  username = \"default\"\n\n  ## Password for authorization on ClickHouse server\n  # password = \"\"\n\n  ## HTTP(s) timeout while getting metrics values\n  ## The timeout includes connection time, any redirects, and reading the\n  ## response body.\n  # timeout = 5s\n\n  ## List of servers for metrics scraping\n  ## metrics scrape via HTTP(s) clickhouse interface\n  ## https://clickhouse.tech/docs/en/interfaces/http/\n  servers = [\"http://127.0.0.1:8123\"]\n\n  ## Server Variant\n  ## When set to \"managed\", some queries are excluded from being run. This is\n  ## useful for instances hosted in ClickHouse Cloud where certain tables are\n  ## not available.\n  # variant = \"self-hosted\"\n\n  ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers\n  ## available in the cluster with using same \"user:password\" described in\n  ## \"user\" and \"password\" parameters and get this server hostname list from\n  ## \"system.clusters\" table.\n  # auto_discovery = true\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  # cluster_include = []\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  # cluster_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring SFlow Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up the SFlow Input Plugin in Telegraf. It includes settings for the service address and read buffer size.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sflow/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# SFlow V5 Protocol Listener\n[[inputs.sflow]]\n  ## Address to listen for sFlow packets.\n  ##   example: service_address = \"udp://:6343\"\n  ##            service_address = \"udp4://:6343\"\n  ##            service_address = \"udp6://:6343\"\n  service_address = \"udp://:6343\"\n\n  ## Set the size of the operating system's receive buffer.\n  ##   example: read_buffer_size = \"64KiB\"\n  # read_buffer_size = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Execd Input Plugin in Telegraf\nDESCRIPTION: This configuration snippet demonstrates how to set up the execd input plugin in Telegraf's TOML configuration. It includes options for command execution, environment variables, signaling method, restart behavior, buffer size, error handling, and data format selection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/execd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n## One program to run as daemon.\n## NOTE: process and each argument should each be their own string\ncommand = [\"telegraf-smartctl\", \"-d\", \"/dev/sda\"]\n\n## Environment variables\n## Array of \"key=value\" pairs to pass as environment variables\n## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n# environment = []\n\n## Define how the process is signaled on each collection interval.\n## Valid values are:\n##   \"none\"    : Do not signal anything. (Recommended for service inputs)\n##               The process must output metrics by itself.\n##   \"STDIN\"   : Send a newline on STDIN. (Recommended for gather inputs)\n##   \"SIGHUP\"  : Send a HUP signal. Not available on Windows. (not recommended)\n##   \"SIGUSR1\" : Send a USR1 signal. Not available on Windows.\n##   \"SIGUSR2\" : Send a USR2 signal. Not available on Windows.\n# signal = \"none\"\n\n## Delay before the process is restarted after an unexpected termination\n# restart_delay = \"10s\"\n\n## Buffer size used to read from the command output stream\n## Optional parameter. Default is 64 Kib, minimum is 16 bytes\n# buffer_size = \"64Kib\"\n\n## Disable automatic restart of the program and stop if the program exits\n## with an error (i.e. non-zero error code)\n# stop_on_error = false\n\n## Data format to consume.\n## Each data format has its own unique set of configuration options, read\n## more about them here:\n## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n# data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Filtered Aggregator Plugin in TOML\nDESCRIPTION: Example showing aggregator configuration with metric filtering. Demonstrates using namepass to selectively apply aggregation to swap metrics only.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.swap]]\n\n[[inputs.system]]\n  fieldinclude = [\"load1\"] # collects system load1 metric.\n\n[[aggregators.minmax]]\n  period = \"30s\"        # send & clear the aggregate every 30s.\n  drop_original = true  # drop the original metrics.\n  namepass = [\"swap\"]   # only \"pass\" swap metrics through the aggregator.\n\n[[outputs.file]]\n  files = [\"stdout\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD Server Options in Telegraf\nDESCRIPTION: This TOML configuration snippet is for setting up a StatsD server input plugin in Telegraf. It specifies the protocol for communication, the maximum TCP connections, and cache reset behaviors among other parameters. These configurations play a crucial role in how metrics are collected and processed by Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Statsd Server\\n[[inputs.statsd]]\\n  ## Protocol, must be \\\"tcp\\\", \\\"udp4\\\", \\\"udp6\\\" or \\\"udp\\\" (default=udp)\\n  protocol = \\\"udp\\\"\\n\\n  ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\\n  max_tcp_connections = 250\\n\\n  ## Enable TCP keep alive probes (default=false)\\n  tcp_keep_alive = false\\n\\n  ## Specifies the keep-alive period for an active network connection.\\n  ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\\n  ## Defaults to the OS configuration.\\n  # tcp_keep_alive_period = \\\"2h\\\"\\n\\n  ## Address and port to host UDP listener on\\n  service_address = \\\":8125\\\"\\n\\n  ## The following configuration options control when telegraf clears it's cache\\n  ## of previous values. If set to false, then telegraf will only clear it's\\n  ## cache when the daemon is restarted.\\n  ## Reset gauges every interval (default=true)\\n  delete_gauges = true\\n  ## Reset counters every interval (default=true)\\n  delete_counters = true\\n  ## Reset sets every interval (default=true)\\n  delete_sets = true\\n  ## Reset timings & histograms every interval (default=true)\\n  delete_timings = true\\n\\n  ## Enable aggregation temporality adds temporality=delta or temporality=commulative tag, and\\n  ## start_time field, which adds the start time of the metric accumulation.\\n  ## You should use this when using OpenTelemetry output.\\n  # enable_aggregation_temporality = false\\n\\n  ## Percentiles to calculate for timing & histogram stats.\\n  percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\\n\\n  ## separator to use between elements of a statsd metric\\n  metric_separator = \\\"_\\\"\\n\\n  ## Parses extensions to statsd in the datadog statsd format\\n  ## currently supports metrics and datadog tags.\\n  ## http://docs.datadoghq.com/guides/dogstatsd/\\n  datadog_extensions = false\\n\\n  ## Parses distributions metric as specified in the datadog statsd format\\n  ## https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#definition\\n  datadog_distributions = false\\n\\n  ## Keep or drop the container id as tag. Included as optional field\\n  ## in DogStatsD protocol v1.2 if source is running in Kubernetes\\n  ## https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/?tab=metrics#dogstatsd-protocol-v12\\n  datadog_keep_container_tag = false\\n\\n  ## Statsd data translation templates, more info can be read here:\\n  ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\\n  # templates = [\\n  #     \\\"cpu.* measurement*\\\"\\n  # ]\\n\\n  ## Number of UDP messages allowed to queue up, once filled,\\n  ## the statsd server will start dropping packets\\n  allowed_pending_messages = 10000\\n\\n  ## Number of worker threads used to parse the incoming messages.\\n  # number_workers_threads = 5\\n\\n  ## Number of timing/histogram values to track per-measurement in the\\n  ## calculation of percentiles. Raising this limit increases the accuracy\\n  ## of percentiles but also increases the memory usage and cpu time.\\n  percentile_limit = 1000\\n\\n  ## Maximum socket buffer size in bytes, once the buffer fills up, metrics\\n  ## will start dropping.  Defaults to the OS default.\\n  # read_buffer_size = 65535\\n\\n  ## Max duration (TTL) for each metric to stay cached/reported without being updated.\\n  # max_ttl = \\\"10h\\\"\\n\\n  ## Sanitize name method\\n  ## By default, telegraf will pass names directly as they are received.\\n  ## However, upstream statsd now does sanitization of names which can be\\n  ## enabled by using the \\\"upstream\\\" method option. This option will a) replace\\n  ## white space with '_', replace '/' with '-', and remove characters not\\n  ## matching 'a-zA-Z_\\-0-9\\.;='.\\n  #sanitize_name_method = \\\"\\\"\\n\\n  ## Replace dots (.) with underscore (_) and dashes (-) with\\n  ## double underscore (__) in metric names.\\n  # convert_names = false\\n\\n  ## Convert all numeric counters to float\\n  ## Enabling this would ensure that both counters and guages are both emitted\\n  ## as floats.\\n  # float_counters = false\\n\\n  ## Emit timings `metric_<name>_count` field as float, the same as all other\\n  ## histogram fields\\n  # float_timings = false\\n\\n  ## Emit sets as float\\n  # float_sets = false\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Internal Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Telegraf Internal Input Plugin. It includes options to collect memory stats and Go runtime metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/internal/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collect statistics about itself\n[[inputs.internal]]\n  ## If true, collect telegraf memory stats.\n  # collect_memstats = true\n\n  ## If true, collect metrics from Go's runtime.metrics. For a full list see:\n  ##   https://pkg.go.dev/runtime/metrics\n  # collect_gostats = false\n```\n\n----------------------------------------\n\nTITLE: Configuring PgBouncer Input Plugin in TOML\nDESCRIPTION: Configuration settings for the PgBouncer input plugin, specifying connection parameters and show commands to gather metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pgbouncer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.pgbouncer]]\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@host:port[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost port=5432 user=pqgotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  address = \"host=localhost user=pgbouncer sslmode=disable\"\n\n  ## Specify which \"show\" commands to gather metrics for.\n  ## Choose from: \"stats\", \"pools\", \"lists\", \"databases\"\n  # show_commands = [\"stats\", \"pools\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring DNS Query Plugin in TOML\nDESCRIPTION: Configuration example for the DNS query input plugin. Specifies DNS servers, network protocol, domains to query, record types, port settings, timeout, and additional field options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dns_query/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.dns_query]]\n  ## servers to query\n  servers = [\"8.8.8.8\"]\n\n  ## Network is the network protocol name.\n  # network = \"udp\"\n\n  ## Domains or subdomains to query.\n  # domains = [\".\"]\n\n  ## Query record type.\n  ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n  # record_type = \"A\"\n\n  ## Dns server port.\n  # port = 53\n\n  ## Query timeout\n  # timeout = \"2s\"\n\n  ## Include the specified additional properties in the resulting metric.\n  ## The following values are supported:\n  ##    \"first_ip\" -- return IP of the first A and AAAA answer\n  ##    \"all_ips\"  -- return IPs of all A and AAAA answers\n  # include_fields = []\n```\n\n----------------------------------------\n\nTITLE: Configuring ZFS Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to configure the ZFS input plugin in Telegraf to collect ZFS metrics. It includes options to specify the kstat path (Linux), override the default list of kstat metrics, and enable the collection of pool and dataset metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zfs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, pools and datasets\n# This plugin ONLY supports Linux & FreeBSD\n[[inputs.zfs]]\n  ## ZFS kstat path. Ignored on FreeBSD\n  ## If not specified, then default is:\n  # kstatPath = \"/proc/spl/kstat/zfs\"\n\n  ## By default, telegraf gather all zfs stats\n  ## Override the stats list using the kstatMetrics array:\n  ## For FreeBSD, the default is:\n  # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n  ## For Linux, the default is:\n  # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n  #     \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n\n  ## By default, don't gather zpool stats\n  # poolMetrics = false\n\n  ## By default, don't gather dataset stats\n  # datasetMetrics = false\n\n```\n\n----------------------------------------\n\nTITLE: Advanced TLS Configuration for Telegraf Servers using TOML\nDESCRIPTION: This snippet provides advanced TLS configuration options for Telegraf server implementations. It includes settings for cipher suite selection and TLS version constraints, allowing fine-grained control over the TLS implementation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TLS.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n## Define list of allowed ciphers suites.  If not defined the default ciphers\n## supported by Go will be used.\n##   ex: tls_cipher_suites = [\n##           \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\",\n##           \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\",\n##           \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n##           \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\n##           \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\n##           \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\n##           \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\",\n##           \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\",\n##           \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\",\n##           \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\",\n##           \"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\",\n##           \"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\",\n##           \"TLS_RSA_WITH_AES_128_GCM_SHA256\",\n##           \"TLS_RSA_WITH_AES_256_GCM_SHA384\",\n##           \"TLS_RSA_WITH_AES_128_CBC_SHA256\",\n##           \"TLS_RSA_WITH_AES_128_CBC_SHA\",\n##           \"TLS_RSA_WITH_AES_256_CBC_SHA\"\n##       ]\n# tls_cipher_suites = []\n\n## Minimum TLS version that is acceptable.\n# tls_min_version = \"TLS10\"\n\n## Maximum SSL/TLS version that is acceptable.\n# tls_max_version = \"TLS13\"\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf via Homebrew on macOS\nDESCRIPTION: Commands to update Homebrew and install Telegraf using the Homebrew package manager on macOS. Note that Homebrew builds Telegraf with CGO, which may result in differences from official binaries.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INSTALL_GUIDE.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbrew update\nbrew install telegraf\n```\n\n----------------------------------------\n\nTITLE: Configuring VMware vSphere Input Plugin with TOML\nDESCRIPTION: This snippet shows an example configuration of the VMware vSphere Input Plugin using TOML format. The configuration should be set up according to the specified plugin-specific options as well as any global settings that may be required.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"\\[inputs.vsphere\\]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Listener Plugin in TOML\nDESCRIPTION: Complete configuration template for the InfluxDB Listener input plugin, including service address, timeouts, TLS settings, authentication options, and parser configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.influxdb_listener]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8186\"\n\n  ## maximum duration before timing out read of the request\n  read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  write_timeout = \"10s\"\n\n  ## Maximum allowed HTTP request body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  max_body_size = 0\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional tag name used to store the database name.\n  # database_tag = \"\"\n\n  ## If set the retention policy specified in the write query will be added as\n  ## the value of this tag name.\n  # retention_policy_tag = \"\"\n\n  ## Optional username and password to accept for HTTP basic authentication\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n  ## Optional JWT token authentication for HTTP requests\n  # token_shared_secret = \"\"\n  # token_username = \"\"\n\n  ## Influx line protocol parser\n  # parser_type = \"internal\"\n```\n\n----------------------------------------\n\nTITLE: Example Output from System Input Plugin\nDESCRIPTION: Sample output showing the metrics collected by the system plugin, including load averages, number of users, CPU count, and uptime information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/system/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsystem,host=tyrion load1=3.72,load5=2.4,load15=2.1,n_users=3i,n_cpus=4i 1483964144000000000\nsystem,host=tyrion uptime=1249632i 1483964144000000000\nsystem,host=tyrion uptime_format=\"14 days, 11:07\" 1483964144000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring WebSocket Output Plugin in Telegraf\nDESCRIPTION: Configuration template for the WebSocket output plugin in Telegraf. Includes settings for URL endpoint, timeouts, TLS configuration, proxy settings, data format options, and custom HTTP headers. Supports both ws and wss schemes for secure connections.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/websocket/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.websocket]]\n  ## URL is the address to send metrics to. Make sure ws or wss scheme is used.\n  url = \"ws://127.0.0.1:3000/telegraf\"\n\n  ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).\n  # connect_timeout = \"30s\"\n  # write_timeout = \"30s\"\n  # read_timeout = \"30s\"\n\n  ## Optionally turn on using text data frames (binary by default).\n  # use_text_frames = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional SOCKS5 proxy to use\n  # socks5_enabled = true\n  # socks5_address = \"127.0.0.1:1080\"\n  # socks5_username = \"alice\"\n  # socks5_password = \"pass123\"\n\n  ## Optional HTTP proxy to use\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Additional HTTP Upgrade headers\n  # [outputs.websocket.headers]\n  #   Authorization = \"Bearer <TOKEN>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Executable Daemon Output in TOML\nDESCRIPTION: Configuration example for the execd output plugin that specifies command execution parameters, environment variables, restart behavior, error handling, and data format settings. The plugin runs a daemon process and streams metrics to it via stdin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/execd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.execd]]\n  ## One program to run as daemon.\n  ## NOTE: process and each argument should each be their own string\n  command = [\"my-telegraf-output\", \"--some-flag\", \"value\"]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Delay before the process is restarted after an unexpected termination\n  restart_delay = \"10s\"\n\n  ## Flag to determine whether execd should throw error when part of metrics is unserializable\n  ## Setting this to true will skip the unserializable metrics and process the rest of metrics\n  ## Setting this to false will throw error when encountering unserializable metrics and none will be processed\n  ## This setting does not apply when use_batch_format is set.\n  # ignore_serialization_error = false\n\n  ## Use batch serialization instead of per metric. The batch format allows for the\n  ## production of batch output formats and may more efficiently encode and write metrics.\n  # use_batch_format = false\n\n  ## Data format to export.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Running Unix Tail Command Equivalent\nDESCRIPTION: Shows the equivalent Unix tail command that the plugin uses by default, which follows the file (-F) and starts at the end of the file (--lines=0).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tail/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntail -F --lines=0 myfile.log\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Output to InfluxDB in TOML\nDESCRIPTION: This code snippet configures the output of Telegraf to write metrics to a specified file or stdout in InfluxDB Line Protocol format. It includes parameters for file selection, data format, maximum line length, field sorting options, and support for unsigned integers. Dependencies include having InfluxDB configured to accept the specified data formats and version compatibility for unsigned integer support.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/influx/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## Maximum line length in bytes.  Useful only for debugging.\n  influx_max_line_bytes = 0\n\n  ## When true, fields will be output in ascending lexical order.  Enabling\n  ## this option will result in decreased performance and is only recommended\n  ## when you need predictable ordering while debugging.\n  influx_sort_fields = false\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: `42u`.  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  influx_uint_support = false\n\n  ## When true, Telegraf will omit the timestamp on data to allow InfluxDB\n  ## to set the timestamp of the data during ingestion. This is generally NOT\n  ## what you want as it can lead to data points captured at different times\n  ## getting omitted due to similar data.\n  # influx_omit_timestamp = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Tags in Telegraf\nDESCRIPTION: Example of configuring global tags in Telegraf, which will be added to all metrics collected by the agent.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[global_tags]\n  dc = \"us-east-1\"\n```\n\n----------------------------------------\n\nTITLE: Basic OpenStack Interval Configuration in TOML\nDESCRIPTION: Example configuration showing how to set different polling intervals for OpenStack services to optimize performance and manage load.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openstack/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.openstack]]\n  interval = \"5m\"\n  ....\n  authentication_endpoint = \"https://my.openstack.cloud:5000\"\n  ...\n  enabled_services = [\"nova_services\"]\n  ....\n\n[[inputs.openstack]]\n  interval = \"30m\"\n  ....\n  authentication_endpoint = \"https://my.openstack.cloud:5000\"\n  ...\n  enabled_services = [\"services\", \"projects\", \"hypervisors\", \"flavors\", \"networks\", \"volumes\"]\n  ....\n```\n\n----------------------------------------\n\nTITLE: Configuring Execd Processor Plugin in Toml\nDESCRIPTION: This TOML configuration snippet defines the setup for the execd processor plugin in Telegraf, detailing how to run a command as a daemon with specific environment variables and data formats.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/execd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Run executable as long-running processor plugin\n[[processors.execd]]\n  ## One program to run as daemon.\n  ## NOTE: process and each argument should each be their own string\n  ## eg: command = [\"/path/to/your_program\", \"arg1\", \"arg2\"]\n  command = [\"cat\"]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Delay before the process is restarted after an unexpected termination\n  # restart_delay = \"10s\"\n\n  ## Serialization format for communicating with the executed program\n  ## Please note that the corresponding data-format must exist both in\n  ## parsers and serializers\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxmox Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Proxmox input plugin including API connection details, node name, additional tags, TLS configuration, and timeout settings. Requires PVEAuditor role permissions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/proxmox/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.proxmox]]\n  base_url = \"https://localhost:8006/api2/json\"\n  api_token = \"USER@REALM!TOKENID=UUID\"\n\n  ## Node name, defaults to OS hostname\n  ## Unless Telegraf is on the same host as Proxmox, setting this is required.\n  # node_name = \"\"\n\n  ## Additional tags of the VM stats data to add as a tag\n  ## Supported values are \"vmid\" and \"status\"\n  # additional_vmstats_tags = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP response timeout (default: 5s)\n  # response_timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Trap Input Plugin in TOML\nDESCRIPTION: Configuration settings for the SNMP Trap input plugin including service address, MIB paths, timeouts, SNMP versions, and authentication options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp_trap/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.snmp_trap]]\n  ## Transport, local address, and port to listen on.  Transport must\n  ## be \"udp://\".  Omit local address to listen on all interfaces.\n  ##   example: \"udp://127.0.0.1:1234\"\n  ##\n  ## Special permissions may be required to listen on a port less than\n  ## 1024.  See README.md for details\n  ##\n  # service_address = \"udp://:162\"\n  ##\n  ## Path to mib files\n  ## Used by the gosmi translator.\n  ## To add paths when translating with netsnmp, use the MIBDIRS environment variable\n  # path = [\"/usr/share/snmp/mibs\"]\n  ##\n  ## Timeout running snmptranslate command\n  ## Used by the netsnmp translator only\n  # timeout = \"5s\"\n  ## Snmp version; one of \"1\", \"2c\" or \"3\".\n  # version = \"2c\"\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\", \"SHA224\", \"SHA256\", \"SHA384\", \"SHA512\" or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\" or \"\".\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n```\n\n----------------------------------------\n\nTITLE: Running Local Development Checks in Telegraf\nDESCRIPTION: Commands to run various checks and tests locally before submitting a pull request to ensure CI compliance. Includes linting, dependency checks, testing, and documentation generation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake lint\nmake check\nmake check-deps\nmake test\nmake docs\n```\n\n----------------------------------------\n\nTITLE: Configuring Wavefront Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the Wavefront output plugin for Telegraf. It includes options for specifying the Wavefront URL, authentication methods, metric formatting, and various HTTP and TLS settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/wavefront/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.wavefront]]\n  ## URL for Wavefront API or Wavefront proxy instance\n  ## Direct Ingestion via Wavefront API requires authentication. See below.\n  url = \"https://metrics.wavefront.com\"\n\n  ## Maximum number of metrics to send per HTTP request. This value should be\n  ## higher than the `metric_batch_size`. Values higher than 40,000 are not\n  ## recommended.\n  # http_maximum_batch_size = 10000\n\n  ## Prefix for metrics keys\n  # prefix = \"my.specific.prefix.\"\n\n  ## Use \"value\" for name of simple fields\n  # simple_fields = false\n\n  ## character to use between metric and field name\n  # metric_separator = \".\"\n\n  ## Convert metric name paths to use metricSeparator character\n  ## When true will convert all _ (underscore) characters in final metric name.\n  # convert_paths = true\n\n  ## Use Strict rules to sanitize metric and tag names from invalid characters\n  ## When enabled forward slash (/) and comma (,) will be accepted\n  # use_strict = false\n\n  ## Use Regex to sanitize metric and tag names from invalid characters\n  ## Regex is more thorough, but significantly slower.\n  # use_regex = false\n\n  ## Tags to use as the source name for Wavefront (\"host\" if none is found)\n  # source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n\n  ## Convert boolean values to numeric values, with false -> 0.0 and true -> 1.0\n  # convert_bool = true\n\n  ## Truncate metric tags to a total of 254 characters for the tag name value\n  ## Wavefront will reject any data point exceeding this limit if not truncated\n  ## Defaults to 'false' to provide backwards compatibility.\n  # truncate_tags = false\n\n  ## Flush the internal buffers after each batch. This effectively bypasses the\n  ## background sending of metrics normally done by the Wavefront SDK. This can\n  ## be used if you are experiencing buffer overruns. The sending of metrics\n  ## will block for a longer time, but this will be handled gracefully by\n  ## internal buffering in Telegraf.\n  # immediate_flush = true\n\n  ## Send internal metrics (starting with `~sdk.go`) for valid, invalid, and\n  ## dropped metrics\n  # send_internal_metrics = true\n\n  ## Optional TLS Config\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable =\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Timeout\n  # timeout=\"10s\"\n\n  ## MaxIdleConns controls the maximum number of idle (keep-alive) connections\n  ## across all hosts. Zero means unlimited.\n  # max_idle_conn = 0\n\n  ## MaxIdleConnsPerHost, if non-zero, controls the maximum idle (keep-alive)\n  ## connections to keep per-host. If zero, DefaultMaxIdleConnsPerHost is used.\n  # max_idle_conn_per_host = 2\n\n  ## Idle (keep-alive) connection timeout\n  # idle_conn_timeout = 0\n\n  ## Authentication for Direct Ingestion.\n  ## Direct Ingestion requires one of: `token`,`auth_csp_api_token`, or\n  ## `auth_csp_client_credentials` (see https://docs.wavefront.com/csp_getting_started.html)\n  ## to learn more about using CSP credentials with Wavefront.\n  ## Not required if using a Wavefront proxy.\n\n  ## Wavefront API Token Authentication, ignored if using a Wavefront proxy\n  ## 1. Click the gear icon at the top right in the Wavefront UI.\n  ## 2. Click your account name (usually your email)\n  ## 3. Click *API access*.\n  # token = \"YOUR_TOKEN\"\n\n  ## Base URL used for authentication, ignored if using a Wavefront proxy or a\n  ## Wavefront API token.\n  # auth_csp_base_url=https://console.cloud.vmware.com\n\n  ## CSP API Token Authentication, ignored if using a Wavefront proxy\n  # auth_csp_api_token=CSP_API_TOKEN_HERE\n\n  ## CSP Client Credentials Authentication Information, ignored if using a\n  ## Wavefront proxy.\n  ## See also: https://docs.wavefront.com/csp_getting_started.html#whats-a-server-to-server-app\n  # [outputs.wavefront.auth_csp_client_credentials]\n  #  app_id=CSP_APP_ID_HERE\n  #  app_secret=CSP_APP_SECRET_HERE\n  #  org_id=CSP_ORG_ID_HERE\n```\n\n----------------------------------------\n\nTITLE: Configuring NSQ Output Plugin in Telegraf\nDESCRIPTION: Configuration example for setting up NSQ output plugin in Telegraf. Specifies the NSQ server location, topic name, and data format for sending metrics. The server defaults to localhost:4150 and supports various data output formats.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/nsq/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send telegraf measurements to NSQD\n[[outputs.nsq]]\n  ## Location of nsqd instance listening on TCP\n  server = \"localhost:4150\"\n  ## NSQ topic for producer messages\n  topic = \"telegraf\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Entity Table Collection in Telegraf\nDESCRIPTION: TOML configuration for collecting data from the ENTITY-MIB table. This configuration retrieves physical entity names which will be joined with the CISCO power table in a subsequent example.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.snmp.table]]\nname = \"entityTable\"\nindex_as_tag = true\n\n[[inputs.snmp.table.field]]\nname = \"EntPhysicalName\"\noid = \"ENTITY-MIB::entPhysicalName\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HAProxy Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the HAProxy input plugin. Specifies server endpoints, TLS settings, and field name handling options. Supports both HTTP and socket-based collection of HAProxy statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/haproxy/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics of HAProxy, via stats socket or http endpoints\n[[inputs.haproxy]]\n  ## List of stats endpoints. Metrics can be collected from both http and socket\n  ## endpoints. Examples of valid endpoints:\n  ##   - http://myhaproxy.com:1936/haproxy?stats\n  ##   - https://myhaproxy.com:8000/stats\n  ##   - socket:/run/haproxy/admin.sock\n  ##   - /run/haproxy/*.sock\n  ##   - tcp://127.0.0.1:1936\n  ##\n  ## Server addresses not starting with 'http://', 'https://', 'tcp://' will be\n  ## treated as possible sockets. When specifying local socket, glob patterns are\n  ## supported.\n  servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n\n  ## By default, some of the fields are renamed from what haproxy calls them.\n  ## Setting this option to true results in the plugin keeping the original\n  ## field names.\n  # keep_field_names = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Exec Input Plugin in TOML\nDESCRIPTION: Sample configuration for the Exec input plugin showing available options including commands array, environment variables, timeout settings, and data format specifications.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/exec/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.exec]]\n  ## Commands array\n  commands = []\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Timeout for each command to complete.\n  # timeout = \"5s\"\n\n  ## Measurement name suffix\n  ## Used for separating different commands\n  # name_suffix = \"\"\n\n  ## Ignore Error Code\n  ## If set to true, a non-zero error code in not considered an error and the\n  ## plugin will continue to parse the output.\n  # ignore_error = false\n\n  ## Data format\n  ## By default, exec expects JSON. This was done for historical reasons and is\n  ## different than other inputs that use the influx line protocol. Each data\n  ## format has its own unique set of configuration options, read more about\n  ## them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"json\"\n```\n\n----------------------------------------\n\nTITLE: Multi-node Selection for XML Metrics in Telegraf\nDESCRIPTION: Demonstrates how to generate metrics for each 'Sensor' node within an XML document using XPath and TOML configurations in Telegraf. It showcases selecting multiple nodes for parsing metrics, handling node-relative field definitions, and deriving common timestamp information from the document.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.xml\"]\n  data_format = \"xml\"\n\n  [[inputs.file.xpath]]\n    metric_selection = \"/Bus/child::Sensor\"\n\n    metric_name = \"string('sensors')\"\n\n    timestamp = \"/Gateway/Timestamp\"\n    timestamp_format = \"2006-01-02T15:04:05Z\"\n\n    [inputs.file.xpath.tags]\n      name = \"substring-after(@name, ' ')\"\n\n    [inputs.file.xpath.fields_int]\n      consumers = \"Variable/@consumers\"\n\n    [inputs.file.xpath.fields]\n      temperature = \"number(Variable/@temperature)\"\n      power       = \"number(Variable/@power)\"\n      frequency   = \"number(Variable/@frequency)\"\n      ok          = \"Mode != 'error'\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PowerStat for CPU Temperature and Frequency on Specific CPU\nDESCRIPTION: Configuration that collects default processor package metrics plus CPU temperature and frequency, but only for CPU ID 0. This demonstrates using the included_cpus parameter to limit metric collection to specific CPUs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_powerstat]]\n  cpu_metrics = [\"cpu_frequency\", \"cpu_temperature\"]\n  included_cpus = [\"0\"]\n```\n\n----------------------------------------\n\nTITLE: Telegraf Startup Error Configuration Values\nDESCRIPTION: Defines the possible values for the startup_error_behavior configuration option, including 'error' (default), 'retry', 'ignore', and 'probe' behaviors. Each option specifies how Telegraf should handle startup errors in plugins.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-006-startup-error-behavior.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nstartup_error_behavior = \"error\"   # Fail and exit on startup errors (default)\nstartup_error_behavior = \"retry\"   # Continue running and retry startup indefinitely\nstartup_error_behavior = \"ignore\"  # Continue running and ignore the failed plugin\nstartup_error_behavior = \"probe\"   # Continue running, ignore plugin, and probe for availability\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC for Kubernetes Service Discovery\nDESCRIPTION: YAML configuration for setting up the necessary RBAC permissions for Telegraf to discover and watch Kubernetes pods. Includes ClusterRole, ClusterRoleBinding, and ServiceAccount definitions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/prometheus/README.md#2025-04-16_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: telegraf-k8s-role-{{.Release.Name}}\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n---\n# Rolebinding for namespace to cluster-admin\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: telegraf-k8s-role-{{.Release.Name}}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: telegraf-k8s-role-{{.Release.Name}}\nsubjects:\n- kind: ServiceAccount\n  name: telegraf-k8s-{{ .Release.Name }}\n  namespace: {{ .Release.Namespace }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: telegraf-k8s-{{ .Release.Name }}\n```\n\n----------------------------------------\n\nTITLE: Monitoring IIS Process Metrics with Telegraf\nDESCRIPTION: Configuration for tracking specific process metrics for IIS worker processes (w3wp). Monitors processor time, handle count, memory usage, and thread count.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    # Process metrics, in this case for IIS only\n    ObjectName = \"Process\"\n    Counters = [\"% Processor Time\",\"Handle Count\",\"Private Bytes\",\"Thread Count\",\"Virtual Bytes\",\"Working Set\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_proc\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n```\n\n----------------------------------------\n\nTITLE: Configuring Strings Processor Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Strings processor plugin in Telegraf. This configuration shows all available string functions that can be applied to metrics, including lowercase, uppercase, titlecase, trim, replace, and others.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/strings/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Perform string processing on tags, fields, and measurements\n[[processors.strings]]\n  ## Convert a field value to lowercase and store in a new field\n  # [[processors.strings.lowercase]]\n  #   field = \"uri_stem\"\n  #   dest = \"uri_stem_normalised\"\n\n  ## Convert a tag value to uppercase\n  # [[processors.strings.uppercase]]\n  #   tag = \"method\"\n\n  ## Convert a field value to titlecase\n  # [[processors.strings.titlecase]]\n  #   field = \"status\"\n\n  ## Trim leading and trailing whitespace using the default cutset\n  # [[processors.strings.trim]]\n  #   field = \"message\"\n\n  ## Trim leading characters in cutset\n  # [[processors.strings.trim_left]]\n  #   field = \"message\"\n  #   cutset = \"\\t\"\n\n  ## Trim trailing characters in cutset\n  # [[processors.strings.trim_right]]\n  #   field = \"message\"\n  #   cutset = \"\\r\\n\"\n\n  ## Trim the given prefix from the field\n  # [[processors.strings.trim_prefix]]\n  #   field = \"my_value\"\n  #   prefix = \"my_\"\n\n  ## Trim the given suffix from the field\n  # [[processors.strings.trim_suffix]]\n  #   field = \"read_count\"\n  #   suffix = \"_count\"\n\n  ## Replace all non-overlapping instances of old with new\n  # [[processors.strings.replace]]\n  #   measurement = \"*\"\n  #   old = \":\"\n  #   new = \"_\"\n\n  ## Trims strings based on width\n  # [[processors.strings.left]]\n  #   field = \"message\"\n  #   width = 10\n\n  ## Decode a base64 encoded utf-8 string\n  # [[processors.strings.base64decode]]\n  #   field = \"message\"\n\n  ## Sanitize a string to ensure it is a valid utf-8 string\n  ## Each run of invalid UTF-8 byte sequences is replaced by the replacement string, which may be empty\n  # [[processors.strings.valid_utf8]]\n  #   field = \"message\"\n  #   replacement = \"\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring MarkLogic Input in Telegraf\nDESCRIPTION: This TOML configuration snippet defines the settings for the `marklogic` input plugin in Telegraf. It specifies the base URL of the MarkLogic HTTP Server and optionally allows configuring authentication via username and password, and TLS settings. The `url` parameter is required, and you may also need to specify the list of `hosts`.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/marklogic/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Retrieves information on a specific host in a MarkLogic Cluster\n[[inputs.marklogic]]\n  ## Base URL of the MarkLogic HTTP Server.\n  url = \"http://localhost:8002\"\n\n  ## List of specific hostnames to retrieve information. At least (1) required.\n  # hosts = [\"hostname1\", \"hostname2\"]\n\n  ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf HTTP Output for Prometheus Remote Write\nDESCRIPTION: This snippet shows how to configure the `outputs.http` section in Telegraf's configuration file to send metrics in the Prometheus remote write format. It includes settings for the URL, TLS configuration (optional), data format, and HTTP headers required by the Prometheus remote write endpoint.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/prometheusremotewrite/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.http]]\n  ## URL is the address to send metrics to\n  url = \"https://cortex/api/prom/push\"\n\n  ## Optional TLS Config\n  tls_ca = \"/etc/telegraf/ca.pem\"\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Data format to output.\n  data_format = \"prometheusremotewrite\"\n\n  [outputs.http.headers]\n     Content-Type = \"application/x-protobuf\"\n     Content-Encoding = \"snappy\"\n     X-Prometheus-Remote-Write-Version = \"0.1.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring System Input Plugin in Telegraf\nDESCRIPTION: Basic TOML configuration for the system input plugin. This plugin requires no additional configuration parameters to function.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/system/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about system load & uptime\n[[inputs.system]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Configuring Fail2ban Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the Fail2ban input plugin in Telegraf. Includes options for enabling sudo and specifying a custom socket path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fail2ban/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from fail2ban.\n[[inputs.fail2ban]]\n  ## Use sudo to run fail2ban-client\n  # use_sudo = false\n\n  ## Use the given socket instead of the default one\n  # socket = \"/var/run/fail2ban/fail2ban.sock\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Histogram Aggregator Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up the Histogram Aggregator Plugin in Telegraf. It includes options for flushing period, original metric handling, histogram resetting, cumulative mode, expiration interval, and example configurations for aggregating fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/histogram/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[aggregators.histogram]]\n  ## The period in which to flush the aggregator.\n  # period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  # drop_original = false\n\n  ## If true, the histogram will be reset on flush instead\n  ## of accumulating the results.\n  reset = false\n\n  ## Whether bucket values should be accumulated. If set to false, \"gt\" tag will be added.\n  ## Defaults to true.\n  cumulative = true\n\n  ## Expiration interval for each histogram. The histogram will be expired if\n  ## there are no changes in any buckets for this time interval. 0 == no expiration.\n  # expiration_interval = \"0m\"\n\n  ## If true, aggregated histogram are pushed to output only if it was updated since\n  ## previous push. Defaults to false.\n  # push_only_on_update = false\n\n  ## Example config that aggregates all fields of the metric.\n  # [[aggregators.histogram.config]]\n  #   ## Right borders of buckets (with +Inf implicitly added).\n  #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]\n  #   ## The name of metric.\n  #   measurement_name = \"cpu\"\n\n  ## Example config that aggregates only specific fields of the metric.\n  # [[aggregators.histogram.config]]\n  #   ## Right borders of buckets (with +Inf implicitly added).\n  #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n  #   ## The name of metric.\n  #   measurement_name = \"diskio\"\n  #   ## The concrete fields of metric\n  #   fields = [\"io_time\", \"read_time\", \"write_time\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin for Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the CPU input plugin for Telegraf. It defines options for reporting per-CPU and total CPU stats, collecting raw CPU time, reporting active time, and adding core tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cpu/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  ## NOTE: The resulting 'time_active' field INCLUDES 'iowait'!\n  report_active = false\n  ## If true and the info is available then add core_id and physical_id tags\n  core_tags = false\n```\n\n----------------------------------------\n\nTITLE: HTTP Access Log Parsing Configuration Example\nDESCRIPTION: Complete example showing how to parse HTTP access logs with logparser input and valuecounter aggregator to count response codes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/valuecounter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.logparser]]\n  files = [\"/tmp/tst.log\"]\n  [inputs.logparser.grok]\n    patterns = ['%{DATA:url:tag} %{NUMBER:response:string}']\n    measurement = \"access\"\n\n[[aggregators.valuecounter]]\n  namepass = [\"access\"]\n  fields = [\"response\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring DPDK Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the DPDK input plugin in Telegraf. It includes options for socket path, timeout, device types, additional commands, and other settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dpdk/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Reads metrics from DPDK applications using v2 telemetry interface.\n# This plugin ONLY supports Linux\n[[inputs.dpdk]]\n  ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK\n  ## telemetry interface.\n  # socket_path = \"/var/run/dpdk/rte/dpdk_telemetry.v2\"\n\n  ## Duration that defines how long the connected socket client will wait for\n  ## a response before terminating connection.\n  ## This includes both writing to and reading from socket. Since it's local\n  ## socket access to a fast packet processing application, the timeout should\n  ## be sufficient for most users.\n  ## Setting the value to 0 disables the timeout (not recommended)\n  # socket_access_timeout = \"200ms\"\n\n  ## Enables telemetry data collection for selected device types.\n  ## Adding \"ethdev\" enables collection of telemetry from DPDK NICs (stats, xstats, link_status, info).\n  ## Adding \"rawdev\" enables collection of telemetry from DPDK Raw Devices (xstats).\n  # device_types = [\"ethdev\"]\n\n  ## List of custom, application-specific telemetry commands to query\n  ## The list of available commands depend on the application deployed.\n  ## Applications can register their own commands via telemetry library API\n  ## https://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands\n  ## For L3 Forwarding with Power Management Sample Application this could be:\n  ##   additional_commands = [\"/l3fwd-power/stats\"]\n  # additional_commands = []\n\n  ## List of plugin options.\n  ## Supported options:\n  ##  - \"in_memory\" option enables reading for multiple sockets when a dpdk application is running with --in-memory option.\n  ##    When option is enabled plugin will try to find additional socket paths related to provided socket_path.\n  ##    Details: https://doc.dpdk.org/guides/howto/telemetry.html#connecting-to-different-dpdk-processes\n  # plugin_options = [\"in_memory\"]\n\n  ## Specifies plugin behavior regarding unreachable socket (which might not have been initialized yet).\n  ## Available choices:\n  ##   - error: Telegraf will return an error during the startup and gather phases if socket is unreachable\n  ##   - ignore: Telegraf will ignore error regarding unreachable socket on both startup and gather\n  # unreachable_socket_behavior = \"error\"\n\n  ## List of metadata fields which will be added to every metric produced by the plugin.\n  ## Supported options:\n  ##  - \"pid\" - exposes PID of DPDK process. Example: pid=2179660i\n  ##  - \"version\" - exposes version of DPDK. Example: version=\"DPDK 21.11.2\"\n  # metadata_fields = [\"pid\", \"version\"]\n\n  ## Allows turning off collecting data for individual \"ethdev\" commands.\n  ## Remove \"/ethdev/link_status\" from list to gather link status metrics.\n  [inputs.dpdk.ethdev]\n    exclude_commands = [\"/ethdev/link_status\"]\n\n  ## When running multiple instances of the plugin it's recommended to add a\n  ## unique tag to each instance to identify metrics exposed by an instance\n  ## of DPDK application. This is useful when multiple DPDK apps run on a\n  ## single host.\n  ##  [inputs.dpdk.tags]\n  ##    dpdk_instance = \"my-fwd-app\"\n```\n\n----------------------------------------\n\nTITLE: Kapacitor Example Output\nDESCRIPTION: This snippet shows example data emitted by Kapacitor. The data is in line protocol format. It describes the state and activity of various Kapacitor components, including memory usage, task execution, and data flow between nodes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kapacitor/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n```text\nkapacitor_memstats,host=hostname.local,kap_version=1.1.0~rc2,url=http://localhost:9092/kapacitor/v1/debug/vars alloc_bytes=6974808i,buck_hash_sys_bytes=1452609i,frees=207281i,gc_sys_bytes=802816i,gc_cpu_fraction=0.00004693548939673313,heap_alloc_bytes=6974808i,heap_idle_bytes=6742016i,heap_in_use_bytes=9183232i,heap_objects=23216i,heap_released_bytes=0i,heap_sys_bytes=15925248i,last_gc_ns=1478791460012676997i,lookups=88i,mallocs=230497i,mcache_in_use_bytes=9600i,mcache_sys_bytes=16384i,mspan_in_use_bytes=98560i,mspan_sys_bytes=131072i,next_gc_ns=11467528i,num_gc=8i,other_sys_bytes=2236087i,pause_total_ns=2994110i,stack_in_use_bytes=1900544i,stack_sys_bytes=1900544i,sys_bytes=22464760i,total_alloc_bytes=35023600i 1478791462000000000\nkapacitor,host=hostname.local,kap_version=1.1.0~rc2,url=http://localhost:9092/kapacitor/v1/debug/vars num_enabled_tasks=5i,num_subscriptions=5i,num_tasks=5i 1478791462000000000\nkapacitor_edges,child=stream0,host=hostname.local,parent=stream,task=deadman-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=shard,retention_policy=monitor,task_master=main points_received=120 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=subscriber,retention_policy=monitor,task_master=main points_received=60 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=http_out,node=http_out3,task=sys-stats,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=window6,host=hostname.local,parent=derivative5,task=deadman-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=from,node=from1,task=sys-stats,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=stream,node=stream0,task=test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=window,node=window6,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=cq,retention_policy=monitor,task_master=main points_received=10 1478791462000000000\nkapacitor_edges,child=http_out3,host=hostname.local,parent=window2,task=sys-stats,type=batch collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=mean4,host=hostname.local,parent=log3,task=deadman-test,type=batch collected=0,emitted=0 1478791462000000000\nkapacitor_ingress,database=_kapacitor,host=hostname.local,measurement=nodes,retention_policy=autogen,task_master=main points_received=207 1478791462000000000\nkapacitor_edges,child=stream0,host=hostname.local,parent=stream,task=sys-stats,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=log6,host=hostname.local,parent=sum5,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=from1,host=hostname.local,parent=stream0,task=sys-stats,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=alert,node=alert2,task=test,type=stream alerts_triggered=0,avg_exec_time_ns=0i,crits_triggered=0,infos_triggered=0,oks_triggered=0,warns_triggered=0 1478791462000000000\nkapacitor_edges,child=log3,host=hostname.local,parent=derivative2,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_ingress,database=_kapacitor,host=hostname.local,measurement=runtime,retention_policy=autogen,task_master=main points_received=9 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=tsm1_filestore,retention_policy=monitor,task_master=main points_received=120 1478791462000000000\nkapacitor_edges,child=derivative2,host=hostname.local,parent=from1,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=stream,node=stream0,task=derivative-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=queryExecutor,retention_policy=monitor,task_master=main points_received=10 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=tsm1_wal,retention_policy=monitor,task_master=main points_received=120 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=log,node=log6,task=derivative-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=stream,host=hostname.local,parent=stats,task=task_master:main,type=stream collected=598,emitted=598 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=write,retention_policy=monitor,task_master=main points_received=10 1478791462000000000\nkapacitor_edges,child=stream0,host=hostname.local,parent=stream,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=log,node=log3,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=from,node=from1,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_ingress,database=_kapacitor,host=hostname.local,measurement=ingress,retention_policy=autogen,task_master=main points_received=148 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=eval,node=eval4,task=derivative-test,type=stream avg_exec_time_ns=0i,eval_errors=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=derivative,node=derivative2,task=derivative-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=runtime,retention_policy=monitor,task_master=main points_received=10 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=httpd,retention_policy=monitor,task_master=main points_received=10 1478791462000000000\nkapacitor_edges,child=sum5,host=hostname.local,parent=eval4,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_ingress,database=_kapacitor,host=hostname.local,measurement=kapacitor,retention_policy=autogen,task_master=main points_received=9 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=from,node=from1,task=test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=tsm1_engine,retention_policy=monitor,task_master=main points_received=120 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=window,node=window2,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=stream,node=stream0,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=influxdb_out4,host=hostname.local,parent=http_out3,task=sys-stats,type=batch collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=window2,host=hostname.local,parent=from1,task=deadman-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=from,node=from1,task=derivative-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=from1,host=hostname.local,parent=stream0,task=deadman-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=database,retention_policy=monitor,task_master=main points_received=40 1478791462000000000\nkapacitor_edges,child=stream,host=hostname.local,parent=write_points,task=task_master:main,type=stream collected=750,emitted=750 1478791462000000000\nkapacitor_edges,child=log7,host=hostname.local,parent=window6,task=deadman-test,type=batch collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=window2,host=hostname.local,parent=from1,task=sys-stats,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=log,node=log7,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_ingress,database=_kapacitor,host=hostname.local,measurement=edges,retention_policy=autogen,task_master=main points_received=225 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=derivative,node=derivative5,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=from1,host=hostname.local,parent=stream0,task=test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=alert2,host=hostname.local,parent=from1,task=test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=log,node=log3,task=derivative-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=influxdb_out,node=influxdb_out4,task=sys-stats,type=stream avg_exec_time_ns=0i,points_written=0,write_errors=0 1478791462000000000\nkapacitor_edges,child=stream0,host=hostname.local,parent=stream,task=test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=log3,host=hostname.local,parent=window2,task=deadman-test,type=batch collected=0,emitted=0 1478791462000000000\nkapacitor_edges,child=derivative5,host=hostname.local,parent=mean4,task=deadman-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=stream,node=stream0,task=sys-stats,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=window,node=window2,task=sys-stats,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_nodes,host=hostname.local,kind=mean,node=mean4,task=deadman-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=from1,host=hostname.local,parent=stream0,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\nkapacitor_ingress,database=_internal,host=hostname.local,measurement=tsm1_cache,retention_policy=monitor,task_master=main points_received=120 1478791462000000000\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Cisco MDT Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Cisco MDT input plugin in Telegraf. It specifies transport type, listener address, message size limits, TLS options, and custom field mappings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cisco_telemetry_mdt/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cisco_telemetry_mdt]]\n transport = \"grpc\"\n service_address = \":57000\"\n max_msg_size = 4000000\n\n # tls_cert = \"/etc/telegraf/cert.pem\"\n # tls_key = \"/etc/telegraf/key.pem\"\n # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n\n # include_delete_field = false\n # source_field_name = \"mdt_source\"\n\n [inputs.cisco_telemetry_mdt.aliases]\n   ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n\n [inputs.cisco_telemetry_mdt.dmes]\n # Global Property Xformation.\n # prop1 = \"uint64 to int\"\n # prop2 = \"uint64 to string\"\n # prop3 = \"string to uint64\"\n # prop4 = \"string to int64\"\n # prop5 = \"string to float64\"\n # auto-prop-xfrom = \"auto-float-xfrom\"\n\n [inputs.cisco_telemetry_mdt.grpc_enforcement_policy]\n  # permit_keepalive_without_calls = false\n  # keepalive_minimum_time = \"5m\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Warp10 Output Plugin in TOML\nDESCRIPTION: Configuration template for the Warp10 output plugin specifying connection details, authentication, and optional TLS settings. Includes settings for timeout, error handling, and metric prefix configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/warp10/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Write metrics to Warp 10\n[[outputs.warp10]]\n  # Prefix to add to the measurement.\n  prefix = \"telegraf.\"\n\n  # URL of the Warp 10 server\n  warp_url = \"http://localhost:8080\"\n\n  # Write token to access your app on warp 10\n  token = \"Token\"\n\n  # Warp 10 query timeout\n  # timeout = \"15s\"\n\n  ## Print Warp 10 error body\n  # print_error_body = false\n\n  ## Max string error size\n  # max_string_error_size = 511\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Internet Speed Monitor Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Internet Speed Monitor input plugin for Telegraf. It includes options for interval, memory saving mode, caching, concurrent connections, test mode, and server ID filtering.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/internet_speed/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Monitors internet speed using speedtest.net service\n[[inputs.internet_speed]]\n  ## This plugin downloads many MB of data each time it is run. As such\n  ## consider setting a higher interval for this plugin to reduce the\n  ## demand on your internet connection.\n  # interval = \"60m\"\n\n  ## Enable to reduce memory usage\n  # memory_saving_mode = false\n\n  ## Caches the closest server location\n  # cache = false\n\n  ## Number of concurrent connections\n  ## By default or set to zero, the number of CPU cores is used. Use this to\n  ## reduce the impact on system performance or to increase the connections on\n  ## faster connections to ensure the fastest speed.\n  # connections = 0\n\n  ## Test mode\n  ## By default, a single sever is used for testing. This may work for most,\n  ## however, setting to \"multi\" will reach out to multiple servers in an\n  ## attempt to get closer to ideal internet speeds.\n  ## And \"multi\" will use all available servers to calculate average packet loss.\n  # test_mode = \"single\"\n\n  ## Server ID exclude filter\n  ## Allows the user to exclude or include specific server IDs received by\n  ## speedtest-go. Values in the exclude option will be skipped over. Values in\n  ## the include option are the only options that will be picked from.\n  ##\n  ## See the list of servers speedtest-go will return at:\n  ##     https://www.speedtest.net/api/js/servers?engine=js&limit=10\n  ##\n  # server_id_exclude = []\n  # server_id_include = []\n```\n\n----------------------------------------\n\nTITLE: Configuring MQTT Output Plugin in TOML\nDESCRIPTION: Complete configuration example for the MQTT output plugin showing all available options including server settings, authentication, TLS configuration, and message formatting options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mqtt/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.mqtt]]\n  servers = [\"localhost:1883\", ]\n  topic = \"telegraf/{{ .Hostname }}/{{ .PluginName }}\"\n  # protocol = \"3.1.1\"\n  # qos = 2\n  # keep_alive = 0\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n  # client_id = \"\"\n  # timeout = \"5s\"\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n  # batch = false\n  # retain = false\n  # client_trace = false\n  # layout = \"non-batch\"\n  # homie_device_name = \"\"\n  # homie_node_id = \"\"\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel DLB Input Plugin in Telegraf\nDESCRIPTION: Configuration file for the Intel DLB input plugin specifying telemetry socket path, eventdev commands, device types, and socket behavior settings. Allows customization of metric collection from DPDK-based applications.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_dlb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_dlb]]\n  ## Path to DPDK telemetry socket.\n  # socket_path = \"/var/run/dpdk/rte/dpdk_telemetry.v2\"\n\n  ## Default eventdev command list, it gathers metrics from socket by given commands.\n  ## Supported options:\n  ##   \"/eventdev/dev_xstats\", \"/eventdev/port_xstats\",\n  ##   \"/eventdev/queue_xstats\", \"/eventdev/queue_links\"\n  # eventdev_commands = [\"/eventdev/dev_xstats\", \"/eventdev/port_xstats\", \"/eventdev/queue_xstats\", \"/eventdev/queue_links\"]\n\n  ## Detect DLB devices based on device id.\n  ## Currently, only supported and tested device id is `0x2710`.\n  ## Configuration added to support forward compatibility.\n  # dlb_device_types = [\"0x2710\"]\n\n  ## Specifies plugin behavior regarding unreachable socket (which might not have been initialized yet).\n  ## Available choices:\n  ##   - error: Telegraf will return an error on startup if socket is unreachable\n  ##   - ignore: Telegraf will ignore error regarding unreachable socket on both startup and gather\n  # unreachable_socket_behavior = \"error\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Namepass and Namedrop Filters for Prometheus Input in Telegraf\nDESCRIPTION: This snippet demonstrates the use of namepass and namedrop filters with the Prometheus input plugin in Telegraf. It shows how to exclude container metrics and only include REST client metrics for Kubelet.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n# Drop all metrics about containers for kubelet\n[[inputs.prometheus]]\n  urls = [\"http://kube-node-1:4194/metrics\"]\n  namedrop = [\"container_*\"]\n\n# Only store rest client related metrics for kubelet\n[[inputs.prometheus]]\n  urls = [\"http://kube-node-1:4194/metrics\"]\n  namepass = [\"rest_client_*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sysstat Input Plugin with Grouped Output in TOML\nDESCRIPTION: This TOML configuration snippet sets up the sysstat input plugin for Telegraf with grouped output. It specifies the sadc path, activities to monitor, and various options for system metrics collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sysstat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.sysstat]]\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n  activities = [\"DISK\", \"SNMP\", \"INT\"]\n  group = true\n  [inputs.sysstat.options]\n -C = \"cpu\"\n -B = \"paging\"\n -b = \"io\"\n -d = \"disk\"             # requires DISK activity\n -H = \"hugepages\"\n \"-I ALL\" = \"interrupts\" # requires INT activity\n \"-n ALL\" = \"network\"\n \"-P ALL\" = \"per_cpu\"\n -q = \"queue\"\n -R = \"mem\"\n \"-r ALL\" = \"mem_util\"\n -S = \"swap_util\"\n -u = \"cpu_util\"\n -v = \"inode\"\n -W = \"swap\"\n -w = \"task\"\n  [[inputs.sysstat.device_tags.sda]]\n    vg = \"rootvg\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Consul Health Checks in Telegraf\nDESCRIPTION: This code snippet shows the example output format for Consul health checks collected by Telegraf. It includes metrics with tags and fields representing the health check states.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/consul/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconsul_health_checks,host=wolfpit,node=consul-server-node,check_id=\"serfHealth\" check_name=\"Serf Health Status\",service_id=\"\",status=\"passing\",passing=1i,critical=0i,warning=0i 1464698464486439902\nconsul_health_checks,host=wolfpit,node=consul-server-node,service_name=www.example.com,check_id=\"service:www-example-com.test01\" check_name=\"Service 'www.example.com' check\",service_id=\"www-example-com.test01\",status=\"critical\",passing=0i,critical=1i,warning=0i 1464698464486519036\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Plus API Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Nginx Plus API input plugin that specifies connection URLs, API version, response timeout, and optional TLS configuration options for connecting to the Nginx Plus API endpoint.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Nginx Plus API advanced status information\n[[inputs.nginx_plus_api]]\n  ## An array of Nginx API URIs to gather stats.\n  urls = [\"http://localhost/api\"]\n  # Nginx API version, default: 3\n  # api_version = 3\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring ClickHouse Data Type Mapping in Telegraf\nDESCRIPTION: TOML configuration for mapping Telegraf metric types to ClickHouse SQL data types. This configuration ensures compatibility with ClickHouse's data type system and sets appropriate conversions for different metric value types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sql/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n  [outputs.sql.convert]\n    conversion_style     = \"literal\"\n    integer              = \"Int64\"\n    text                 = \"String\"\n    timestamp            = \"DateTime\"\n    defaultvalue         = \"String\"\n    unsigned             = \"UInt64\"\n    bool                 = \"UInt8\"\n    real                 = \"Float64\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NATS Output Plugin in Telegraf\nDESCRIPTION: Complete configuration example for the NATS output plugin in Telegraf. Includes server connection settings, authentication options, TLS configuration, data format settings, and optional Jetstream configurations for enhanced messaging capabilities.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/nats/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send telegraf measurements to NATS\n[[outputs.nats]]\n  ## URLs of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## Optional client name\n  # name = \"\"\n\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional NATS 2.0 and NATS NGS compatible user credentials\n  # credentials = \"/etc/telegraf/nats.creds\"\n\n  ## NATS subject for producer messages\n  ## For jetstream this is also the subject where messages will be published\n  subject = \"telegraf\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## Jetstream specific configuration. If not nil, it will assume Jetstream context.\n  ## Since this is a table, it should be present at the end of the plugin section. Else you can use inline table format.\n  # [outputs.nats.jetstream]\n    ## Name of the stream, required when using jetstream. Telegraf will\n    ## use the union of the above subject and below the subjects array.\n    # name = \"\"\n    # subjects = []\n\n    ## Use asynchronous publishing for higher throughput, but note that it does not guarantee order within batches.\n    # async_publish = false\n\n    ## Timeout for wating on acknowledgement on asynchronous publishing\n    ## String with valid units \"ns\", \"us\" (or \"s\"), \"ms\", \"s\", \"m\", \"h\".\n    # async_ack_timeout = \"5s\"\n\n    ## Full jetstream create stream config, refer: https://docs.nats.io/nats-concepts/jetstream/streams\n    # retention = \"limits\"\n    # max_consumers = -1\n    # max_msgs_per_subject = -1\n    # max_msgs = -1\n    # max_bytes = -1\n    # max_age = 0\n    # max_msg_size = -1\n    # storage = \"file\"\n    # discard = \"old\"\n    # num_replicas = 1\n    # duplicate_window = 120000000000\n    # sealed = false\n    # deny_delete = false\n    # deny_purge = false\n    # allow_rollup_hdrs = false\n    # allow_direct = true\n    # mirror_direct = false\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenWeatherMap Input Plugin in TOML\nDESCRIPTION: Configuration settings for the OpenWeatherMap Telegraf plugin including API key, city IDs, language settings, and query parameters. The configuration allows customization of data collection intervals, unit systems, and query styles.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openweathermap/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read current weather and forecasts data from openweathermap.org\n[[inputs.openweathermap]]\n  ## OpenWeatherMap API key.\n  app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n  ## City ID's to collect weather data from.\n  city_id = [\"5391959\"]\n\n  ## Language of the description field. Can be one of \"ar\", \"bg\",\n  ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n  ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n  ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n  # lang = \"en\"\n\n  ## APIs to fetch; can contain \"weather\" or \"forecast\".\n  # fetch = [\"weather\", \"forecast\"]\n\n  ## OpenWeatherMap base URL\n  # base_url = \"https://api.openweathermap.org/\"\n\n  ## Timeout for HTTP response.\n  # response_timeout = \"5s\"\n\n  ## Preferred unit system for temperature and wind speed. Can be one of\n  ## \"metric\", \"imperial\", or \"standard\".\n  # units = \"metric\"\n\n  ## Style to query the current weather; available options\n  ##   batch      -- query multiple cities at once using the \"group\" endpoint\n  ##   individual -- query each city individually using the \"weather\" endpoint\n  ## You should use \"individual\" here as it is documented and provides more\n  ## frequent updates. The default is \"batch\" for backward compatibility.\n  # query_style = \"batch\"\n\n  ## Query interval to fetch data.\n  ## By default the global 'interval' setting is used. You should override the\n  ## interval here if the global setting is shorter than 10 minutes as\n  ## OpenWeatherMap weather data is only updated every 10 minutes.\n  # interval = \"10m\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring DFS Replication Service with Telegraf\nDESCRIPTION: Configuration for tracking DFS Replication service volume metrics when the server hosts a DFS Replication folder or is a Domain Controller. Monitors data lookups and database commits.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    # AD, DFS R, Useful if the server hosts a DFS Replication folder or is a Domain Controller\n    ObjectName = \"DFS Replication Service Volumes\"\n    Instances = [\"*\"]\n    Counters = [\"Data Lookups\",\"Database Commits\"]\n    Measurement = \"win_dfsr\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n    #WarnOnMissing = false # Print out when the performance counter is missing, either of object, counter or instance.\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration with Secrets\nDESCRIPTION: Example Docker Compose configuration demonstrating how to define and use secrets for a Telegraf service, including user permissions and secret mounting\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/docker/README.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  telegraf:\n    image: docker.io/telegraf:latest\n    container_name: dockersecret_telegraf\n    user: \"${USERID}\" # Required to access the /run/secrets directory in container\n    secrets:\n      - secret_for_plugin\n    volumes:\n      - /path/to/telegrafconf/host:/etc/telegraf/telegraf.conf:ro\n\nsecrets:\n  secret_for_plugin:\n    environment: TELEGRAF_PLUGIN_CREDENTIAL\n```\n\n----------------------------------------\n\nTITLE: Flattening Dynamic Columns Using Extend Operator\nDESCRIPTION: Recommended approach for flattening dynamic columns using the extend operator. More robust and faster than bag_unpack, handles schema changes gracefully.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/azure_data_explorer/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nTablenmae\n| extend facility_code=toint(fields.facility_code), message=tostring(fields.message), procid= tolong(fields.procid), severity_code=toint(fields.severity_code),\nSysLogTimestamp=unixtime_nanoseconds_todatetime(tolong(fields.timestamp)), version= todouble(fields.version),\nappname= tostring(tags.appname), facility= tostring(tags.facility),host= tostring(tags.host), hostname=tostring(tags.hostname), severity=tostring(tags.severity)\n| project-away fields, tags\n```\n\n----------------------------------------\n\nTITLE: Configuring Webhooks Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for setting up webhook listeners with service address, timeout settings, and multiple webhook endpoints\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.webhooks]]\n  ## Address and port to host Webhook listener on\n  service_address = \":1619\"\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  [inputs.webhooks.filestack]\n    path = \"/filestack\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.github]\n    path = \"/github\"\n    # secret = \"\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.mandrill]\n    path = \"/mandrill\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.rollbar]\n    path = \"/rollbar\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.papertrail]\n    path = \"/papertrail\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.particle]\n    path = \"/particle\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.artifactory]\n    path = \"/artifactory\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Input Plugin in Telegraf\nDESCRIPTION: This TOML snippet demonstrates the configuration settings for the Zookeeper input plugin in Telegraf. It shows how to specify server addresses, set timeouts, parse floats, and configure TLS settings for secure connections. The configuration uses 'mntr' to gather statistics from Zookeeper servers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zookeeper/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n[[inputs.zookeeper]]\n  servers = [\":2181\"]\n  # timeout = \"5s\"\n  # parse_floats = \"string\"\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud PubSub Output in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the Google Cloud PubSub output plugin for Telegraf. It includes required fields such as project and topic, as well as optional settings for content encoding, data format, credentials, batching, and PubSub-specific parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/cloud_pubsub/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Publish Telegraf metrics to a Google Cloud PubSub topic\n[[outputs.cloud_pubsub]]\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub topic.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub topic to publish metrics to.\n  topic = \"my-topic\"\n\n  ## Content encoding for message payloads, can be set to \"gzip\" or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. If true, will send all metrics per write in one PubSub message.\n  # send_batched = true\n\n  ## The following publish_* parameters specifically configures batching\n  ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n  ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1.\n  # publish_count_threshold = 1000\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1\n  # publish_byte_threshold = 1000000\n\n  ## Optional. Specifically configures requests made to the PubSub API.\n  # publish_num_go_routines = 2\n\n  ## Optional. Specifies a timeout for requests to the PubSub API.\n  # publish_timeout = \"30s\"\n\n  ## Optional. If true, published PubSub message data will be base64-encoded.\n  # base64_data = false\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Optional. PubSub attributes to add to metrics.\n  # [outputs.cloud_pubsub.attributes]\n  #   my_attr = \"tag_value\"\n```\n\n----------------------------------------\n\nTITLE: Dual Instance Configuration for Real-time and Historical Metrics\nDESCRIPTION: TOML configuration showing how to set up separate plugin instances for real-time and historical metric collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n## Realtime instance\n[[inputs.vsphere]]\n  interval = \"60s\"\n  vcenters = [ \"https://someaddress/sdk\" ]\n  username = \"someuser@vsphere.local\"\n  password = \"secret\"\n\n  insecure_skip_verify = true\n  force_discover_on_init = true\n\n  # Exclude all historical metrics\n  datastore_metric_exclude = [\"*\"]\n  cluster_metric_exclude = [\"*\"]\n  datacenter_metric_exclude = [\"*\"]\n  resource_pool_metric_exclude = [\"*\"]\n  vsan_metric_exclude = [\"*\"]\n\n  collect_concurrency = 5\n  discover_concurrency = 5\n\n# Historical instance\n[[inputs.vsphere]]\n\n  interval = \"300s\"\n\n  vcenters = [ \"https://someaddress/sdk\" ]\n  username = \"someuser@vsphere.local\"\n  password = \"secret\"\n\n  insecure_skip_verify = true\n  force_discover_on_init = true\n  host_metric_exclude = [\"*\"] # Exclude realtime metrics\n  vm_metric_exclude = [\"*\"] # Exclude realtime metrics\n\n  max_query_metrics = 256\n  collect_concurrency = 3\n```\n\n----------------------------------------\n\nTITLE: Basic Redis Plugin Configuration Example\nDESCRIPTION: Minimal configuration example showing how to connect to a Redis server using default settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/redis/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.redis]]\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:6379\n  ##    tcp://:password@192.168.99.100\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 6379 is used\n  servers = [\"tcp://localhost:6379\"]\n```\n\n----------------------------------------\n\nTITLE: Output Sample - Plain Text\nDESCRIPTION: This snippet demonstrates the output resulting from processing the sample input using the configured Telegraf Parser Processor Plugin. The output includes parsed metrics, ensuring specific fields are extracted and reformatted based on configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/parser/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsyslog,appname=influxd,facility=daemon,hostname=http://influxdb.example.org\\ (influxdb.example.org),severity=info facility_code=3i,log_id=\"09p7QbOG000\",lvl=\"info\",message=\" ts=2018-08-09T21:01:48.137963Z lvl=info msg=\\\"Executing query\\\" log_id=09p7QbOG000 service=query query=\\\"SHOW DATABASES\\\"\",msg=\"Executing query\",procid=\"6629\",query=\"SHOW DATABASES\",service=\"query\",severity_code=6i,timestamp=1533848508138040000i,ts=\"2018-08-09T21:01:48.137963Z\",version=1i\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Regex Processor Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Regex Processor Plugin showing how to transform tag values, field values, rename fields and tags, and rename metrics using regular expressions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/regex/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Transforms tag and field values as well as measurement, tag and field names with regex pattern\n[[processors.regex]]\n  namepass = [\"nginx_requests\"]\n\n  ## Tag value conversion(s). Multiple instances are allowed.\n  [[processors.regex.tags]]\n    ## Tag(s) to process with optional glob expressions such as '*'.\n    key = \"resp_code\"\n    ## Regular expression to match the tag value. If the value doesn't\n    ## match the tag is ignored.\n    pattern = \"^(\\\\d)\\\\d\\\\d$\"\n    ## Replacement expression defining the value of the target tag. You can\n    ## use regexp groups or named groups e.g. ${1} references the first group.\n    replacement = \"${1}xx\"\n    ## Name of the target tag defaulting to 'key' if not specified.\n    ## In case of wildcards being used in `key` the currently processed\n    ## tag-name is used as target.\n    # result_key = \"method\"\n    ## Appends the replacement to the target tag instead of overwriting it when\n    ## set to true.\n    # append = false\n\n  ## Field value conversion(s). Multiple instances are allowed.\n  [[processors.regex.fields]]\n    ## Field(s) to process with optional glob expressions such as '*'.\n    key = \"request\"\n    ## Regular expression to match the field value. If the value doesn't\n    ## match or the field doesn't contain a string the field is ignored.\n    pattern = \"^/api(?P<method>/[\\\\w/]+)\\\\S*\"\n    ## Replacement expression defining the value of the target field. You can\n    ## use regexp groups or named groups e.g. ${method} references the group\n    ## named \"method\".\n    replacement = \"${method}\"\n    ## Name of the target field defaulting to 'key' if not specified.\n    ## In case of wildcards being used in `key` the currently processed\n    ## field-name is used as target.\n    # result_key = \"method\"\n\n  ## Rename metric fields\n  [[processors.regex.field_rename]]\n    ## Regular expression to match on the field name\n    pattern = \"^search_(\\\\w+)d$\"\n    ## Replacement expression defining the name of the new field\n    replacement = \"${1}\"\n    ## If the new field name already exists, you can either \"overwrite\" the\n    ## existing one with the value of the renamed field OR you can \"keep\"\n    ## both the existing and source field.\n    # result_key = \"keep\"\n\n  ## Rename metric tags\n  [[processors.regex.tag_rename]]\n    ## Regular expression to match on a tag name\n    pattern = \"^search_(\\\\w+)d$\"\n    ## Replacement expression defining the name of the new tag\n    replacement = \"${1}\"\n    ## If the new tag name already exists, you can either \"overwrite\" the\n    ## existing one with the value of the renamed tag OR you can \"keep\"\n    ## both the existing and source tag.\n    # result_key = \"keep\"\n\n  ## Rename metrics\n  [[processors.regex.metric_rename]]\n    ## Regular expression to match on an metric name\n    pattern = \"^search_(\\\\w+)d$\"\n    ## Replacement expression defining the new name of the metric\n    replacement = \"${1}\"\n```\n\n----------------------------------------\n\nTITLE: Generating Host Metrics for VSphere in Text Format\nDESCRIPTION: This snippet captures various metrics from the host machines, detailing CPU, memory, disk, and network statistics. Such data is crucial for monitoring the overall health and performance of VSphere hosts.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_15\n\nLANGUAGE: text\nCODE:\n```\nvsphere_host_cpu,esxhostname=DC0_H0,host=host.example.com,moid=host-19,os=Mac,source=DC0_H0,vcenter=localhost:8989 utilization_average=10.46,usage_average=22.4,readiness_average=0.4,costop_summation=2i,coreUtilization_average=19.61,wait_summation=5148518i,idle_summation=58581i,latency_average=0.6,ready_summation=13370i,used_summation=19219i 1535660299000000000\n```\n\nLANGUAGE: text\nCODE:\n```\nvsphere_host_mem,esxhostname=DC0_H0,host=host.example.com,moid=host-19,os=Mac,source=DC0_H0,vcenter=localhost:8989 usage_average=93.27 1535660299000000000\n```\n\n----------------------------------------\n\nTITLE: Telegraf Configuration with Environment Variables\nDESCRIPTION: Example of a Telegraf configuration file using environment variables for various settings, including global tags and InfluxDB outputs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[global_tags]\n  user = \"${USER}\"\n\n[[inputs.mem]]\n\n# For InfluxDB 1.x:\n[[outputs.influxdb]]\n  urls = [\"${INFLUX_URL}\"]\n  skip_database_creation = ${INFLUX_SKIP_DATABASE_CREATION}\n  password = \"${INFLUX_PASSWORD}\"\n\n# For InfluxDB OSS 2:\n[[outputs.influxdb_v2]]\n  urls = [\"${INFLUX_HOST}\"]\n  token = \"${INFLUX_TOKEN}\"\n  organization = \"${INFLUX_ORG}\"\n  bucket = \"${INFLUX_BUCKET}\"\n\n# For InfluxDB Cloud 2:\n[[outputs.influxdb_v2]]\n  urls = [\"${INFLUX_HOST}\"]\n  token = \"${INFLUX_TOKEN}\"\n  organization = \"${INFLUX_ORG}\"\n  bucket = \"${INFLUX_BUCKET}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Netflow Input Plugin in Telegraf using TOML\nDESCRIPTION: Sample configuration for the Netflow input plugin in Telegraf. Defines settings for service address, buffer size, protocol version, and PEN mapping options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netflow/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Netflow v5, Netflow v9 and IPFIX collector\n[[inputs.netflow]]\n  ## Address to listen for netflow,ipfix or sflow packets.\n  ##   example: service_address = \"udp://:2055\"\n  ##            service_address = \"udp4://:2055\"\n  ##            service_address = \"udp6://:2055\"\n  service_address = \"udp://:2055\"\n\n  ## Set the size of the operating system's receive buffer.\n  ##   example: read_buffer_size = \"64KiB\"\n  ## Uses the system's default if not set.\n  # read_buffer_size = \"\"\n\n  ## Protocol version to use for decoding.\n  ## Available options are\n  ##   \"ipfix\"      -- IPFIX / Netflow v10 protocol (also works for Netflow v9)\n  ##   \"netflow v5\" -- Netflow v5 protocol\n  ##   \"netflow v9\" -- Netflow v9 protocol (also works for IPFIX)\n  ##   \"sflow v5\"   -- sFlow v5 protocol\n  # protocol = \"ipfix\"\n\n  ## Private Enterprise Numbers (PEN) mappings for decoding\n  ## This option allows to specify vendor-specific mapping files to use during\n  ## decoding.\n  # private_enterprise_number_files = []\n\n  ## Log incoming packets for tracing issues\n  # log_level = \"trace\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Parser in Telegraf using TOML\nDESCRIPTION: Configuration example for setting up the Parquet parser in Telegraf. Includes options for file input, data format specification, tag columns, measurement naming, and timestamp handling with timezone support.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/parquet/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"parquet\"\n\n  ## Tag column is an array of columns that should be added as tags.\n  # tag_columns = []\n\n  ## Name column is the column to use as the measurement name.\n  # measurement_column = \"\"\n\n  ## Timestamp column is the column containing the time that should be used to\n  ## create the metric. If not set, then the time of parsing is used.\n  # timestamp_column = \"\"\n\n  ## Timestamp format is the time layout that should be used to interpret the\n  ## timestamp_column. The time must be `unix`, `unix_ms`, `unix_us`, `unix_ns`,\n  ## or a time in the \"reference time\".  To define a different format, arrange\n  ## the values from the \"reference time\" in the example to match the format\n  ## you will be using.  For more information on the \"reference time\", visit\n  ## https://golang.org/pkg/time/#Time.Format\n  ##   ex: timestamp_format = \"Mon Jan 2 15:04:05 -0700 MST 2006\"\n  ##       timestamp_format = \"2006-01-02T15:04:05Z07:00\"\n  ##       timestamp_format = \"01/02/2006 15:04:05\"\n  ##       timestamp_format = \"unix\"\n  ##       timestamp_format = \"unix_ms\"\n  # timestamp_format = \"\"\n\n  ## Timezone allows you to provide an override for timestamps that\n  ## do not already include an offset\n  ## e.g. 04/06/2016 12:41:45\n  ##\n  ## Default: \"\" which renders UTC\n  ## Options are as follows:\n  ##   1. Local               -- interpret based on machine localtime\n  ##   2. \"America/New_York\"  -- Unix TZ values like those found in\n  ##      https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n  ##   3. UTC                 -- or blank/unspecified, will return timestamp in UTC\n  # timestamp_timezone = \"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Telegraf Configuration for Webhooks Plugin\nDESCRIPTION: Command to generate a new configuration file for the webhooks input plugin, filtering for webhooks input and InfluxDB output\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf config -input-filter webhooks -output-filter influxdb > config.conf.new\n```\n\n----------------------------------------\n\nTITLE: Configuring OPCUA Listener Input Plugin for Telegraf\nDESCRIPTION: Comprehensive TOML configuration for the Telegraf OPCUA listener input plugin. Includes settings for endpoint connection, security, authentication, node monitoring, event handling, and device-specific workarounds. Supports both inline and bracketed notation for node configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opcua_listener]]\n  ## Metric name\n  # name = \"opcua_listener\"\n  #\n  ## OPC UA Endpoint URL\n  # endpoint = \"opc.tcp://localhost:4840\"\n  #\n  ## Maximum time allowed to establish a connect to the endpoint.\n  # connect_timeout = \"10s\"\n  #\n  ## Behavior when we fail to connect to the endpoint on initialization. Valid options are:\n  ##     \"error\": throw an error and exits Telegraf\n  ##     \"ignore\": ignore this plugin if errors are encountered\n  #      \"retry\": retry connecting at each interval\n  # connect_fail_behavior = \"error\"\n  #\n  ## Maximum time allowed for a request over the established connection.\n  # request_timeout = \"5s\"\n  #\n  # Maximum time that a session shall remain open without activity.\n  # session_timeout = \"20m\"\n  #\n  ## The interval at which the server should at least update its monitored items.\n  ## Please note that the OPC UA server might reject the specified interval if it cannot meet the required update rate.\n  ## Therefore, always refer to the hardware/software documentation of your server to ensure the specified interval is supported.\n  # subscription_interval = \"100ms\"\n  #\n  ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n  ## \"Basic256Sha256\", or \"auto\"\n  # security_policy = \"auto\"\n  #\n  ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n  # security_mode = \"auto\"\n  #\n  ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n  ## If cert path is not supplied, self-signed cert and key will be generated.\n  # certificate = \"/etc/telegraf/cert.pem\"\n  #\n  ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n  ## If key path is not supplied, self-signed cert and key will be generated.\n  # private_key = \"/etc/telegraf/key.pem\"\n  #\n  ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n  ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n  # auth_method = \"Anonymous\"\n  #\n  ## Username. Required for auth_method = \"UserName\"\n  # username = \"\"\n  #\n  ## Password. Required for auth_method = \"UserName\"\n  # password = \"\"\n  #\n  ## Option to select the metric timestamp to use. Valid options are:\n  ##     \"gather\" -- uses the time of receiving the data in telegraf\n  ##     \"server\" -- uses the timestamp provided by the server\n  ##     \"source\" -- uses the timestamp provided by the source\n  # timestamp = \"gather\"\n  #\n  ## The default timetsamp format is RFC3339Nano\n  # Other timestamp layouts can be configured using the Go language time\n  # layout specification from https://golang.org/pkg/time/#Time.Format\n  # e.g.: json_timestamp_format = \"2006-01-02T15:04:05Z07:00\"\n  #timestamp_format = \"\"\n  #\n  #\n  ## Client trace messages\n  ## When set to true, and debug mode enabled in the agent settings, the OPCUA\n  ## client's messages are included in telegraf logs. These messages are very\n  ## noisey, but essential for debugging issues.\n  # client_trace = false\n  #\n  ## Include additional Fields in each metric\n  ## Available options are:\n  ##   DataType -- OPC-UA Data Type (string)\n  # optional_fields = []\n  #\n  ## Node ID configuration\n  ## name              - field name to use in the output\n  ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\n  ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\n  ## identifier        - OPC UA ID (tag as shown in opcua browser)\n  ## default_tags      - extra tags to be added to the output metric (optional)\n  ## monitoring_params - additional settings for the monitored node (optional)\n  ##\n  ## Monitoring parameters\n  ## sampling_interval  - interval at which the server should check for data\n  ##                      changes (default: 0s)\n  ## queue_size         - size of the notification queue (default: 10)\n  ## discard_oldest     - how notifications should be handled in case of full\n  ##                      notification queues, possible values:\n  ##                      true: oldest value added to queue gets replaced with new\n  ##                            (default)\n  ##                      false: last value added to queue gets replaced with new\n  ## data_change_filter - defines the condition under which a notification should\n  ##                      be reported\n  ##\n  ## Data change filter\n  ## trigger        - specify the conditions under which a data change notification\n  ##                  should be reported, possible values:\n  ##                  \"Status\": only report notifications if the status changes\n  ##                            (default if parameter is omitted)\n  ##                  \"StatusValue\": report notifications if either status or value\n  ##                                 changes\n  ##                  \"StatusValueTimestamp\": report notifications if either status,\n  ##                                          value or timestamp changes\n  ## deadband_type  - type of the deadband filter to be applied, possible values:\n  ##                  \"Absolute\": absolute change in a data value to report a notification\n  ##                  \"Percent\": works only with nodes that have an EURange property set\n  ##                             and is defined as: send notification if\n  ##                             (last value - current value) >\n  ##                             (deadband_value/100.0) * ((highlow) of EURange)\n  ## deadband_value - value to deadband_type, must be a float value, no filter is set\n  ##                  for negative values\n  ##\n  ## Use either the inline notation or the bracketed notation, not both.\n  #\n  ## Inline notation (default_tags and monitoring_params not supported yet)\n  # nodes = [\n  #   {name=\"node1\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #   {name=\"node2\", namespace=\"\", identifier_type=\"\", identifier=\"\"}\n  # ]\n  #\n  ## Bracketed notation\n  # [[inputs.opcua_listener.nodes]]\n  #   name = \"node1\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n  #   default_tags = { tag1 = \"value1\", tag2 = \"value2\" }\n  #\n  # [[inputs.opcua_listener.nodes]]\n  #   name = \"node2\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n  #\n  #   [inputs.opcua_listener.nodes.monitoring_params]\n  #     sampling_interval = \"0s\"\n  #     queue_size = 10\n  #     discard_oldest = true\n  #\n  #     [inputs.opcua_listener.nodes.monitoring_params.data_change_filter]\n  #       trigger = \"Status\"\n  #       deadband_type = \"Absolute\"\n  #       deadband_value = 0.0\n  #\n  ## Node Group\n  ## Sets defaults so they aren't required in every node.\n  ## Default values can be set for:\n  ## * Metric name\n  ## * OPC UA namespace\n  ## * Identifier\n  ## * Default tags\n  ## * Sampling interval\n  ##\n  ## Multiple node groups are allowed\n  #[[inputs.opcua_listener.group]]\n  ## Group Metric name. Overrides the top level name.  If unset, the\n  ## top level name is used.\n  # name =\n  #\n  ## Group default namespace. If a node in the group doesn't set its\n  ## namespace, this is used.\n  # namespace =\n  #\n  ## Group default identifier type. If a node in the group doesn't set its\n  ## namespace, this is used.\n  # identifier_type =\n  #\n  ## Default tags that are applied to every node in this group. Can be\n  ## overwritten in a node by setting a different value for the tag name.\n  ##   example: default_tags = { tag1 = \"value1\" }\n  # default_tags = {}\n  #\n  ## Group default sampling interval. If a node in the group doesn't set its\n  ## sampling interval, this is used.\n  # sampling_interval = \"0s\"\n  #\n  ## Node ID Configuration.  Array of nodes with the same settings as above.\n  ## Use either the inline notation or the bracketed notation, not both.\n  #\n  ## Inline notation (default_tags and monitoring_params not supported yet)\n  # nodes = [\n  #  {name=\"node1\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #  {name=\"node2\", namespace=\"\", identifier_type=\"\", identifier=\"\"}\n  #]\n  #\n  ## Bracketed notation\n  # [[inputs.opcua_listener.group.nodes]]\n  #   name = \"node1\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n  #   default_tags = { tag1 = \"override1\", tag2 = \"value2\" }\n  #\n  # [[inputs.opcua_listener.group.nodes]]\n  #   name = \"node2\"\n  #   namespace = \"\"\n  #   identifier_type = \"\"\n  #   identifier = \"\"\n  #\n  #   [inputs.opcua_listener.group.nodes.monitoring_params]\n  #     sampling_interval = \"0s\"\n  #     queue_size = 10\n  #     discard_oldest = true\n  #\n  #     [inputs.opcua_listener.group.nodes.monitoring_params.data_change_filter]\n  #       trigger = \"Status\"\n  #       deadband_type = \"Absolute\"\n  #       deadband_value = 0.0\n  #\n\n  ## Multiple event groups are allowed.\n  # [[inputs.opcua_listener.events]]\n  #   ## Polling interval for data collection\n  #   # sampling_interval = \"10s\"\n  #   ## Size of the notification queue\n  #   # queue_size = 10\n  #   ## Node parameter defaults for node definitions below\n  #   # namespace = \"\"\n  #   # identifier_type = \"\"\n  #   ## Specifies OPCUA Event sources to filter on\n  #   # source_names = [\"SourceName1\", \"SourceName2\"]\n  #   ## Fields to capture from event notifications\n  #   fields = [\"Severity\", \"Message\", \"Time\"]\n  #\n  #   ## Type or level of events to capture from the monitored nodes.\n  #   [inputs.opcua_listener.events.event_type_node]\n  #     namespace = \"\"\n  #     identifier_type = \"\"\n  #     identifier = \"\"\n  #\n  #   ## Nodes to monitor for event notifications associated with the defined\n  #   ## event type\n  #   [[inputs.opcua_listener.events.node_ids]]\n  #     namespace = \"\"\n  #     identifier_type = \"\"\n  #     identifier = \"\"\n\n  ## Enable workarounds required by some devices to work correctly\n  # [inputs.opcua_listener.workarounds]\n  #  ## Set additional valid status codes, StatusOK (0x0) is always considered valid\n  #  # additional_valid_status_codes = [\"0xC0\"]\n  #  ## Use unregistered reads instead of registered reads\n  #  # use_unregistered_reads = false\n```\n\n----------------------------------------\n\nTITLE: Configuring MQTT Output for Homie v4 Layout in Telegraf\nDESCRIPTION: TOML configuration for the MQTT output plugin in Telegraf to use the Homie v4 layout. It specifies the topic template, layout type, and templates for device name and node ID.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mqtt/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.mqtt]]\n  topic = 'telegraf/{{ .PluginName }}'\n  layout = \"homie-v4\"\n\n  homie_device_name ='{{.PluginName}} plugin'\n  homie_node_id = '{{.Tag \"source\"}}'\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Fireboard Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Fireboard input plugin including authentication token, server URL override, and HTTP timeout settings. The auth_token is required for API access, while URL and timeout are optional configurable parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fireboard/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read real time temps from fireboard.io servers\n[[inputs.fireboard]]\n  ## Specify auth token for your account\n  auth_token = \"invalidAuthToken\"\n  ## You can override the fireboard server URL if necessary\n  # url = https://fireboard.io/api/v1/devices.json\n  ## You can set a different http_timeout if you need to\n  ## You should set a string using an number and time indicator\n  ## for example \"12s\" for 12 seconds.\n  # http_timeout = \"4s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kibana Input Plugin in TOML\nDESCRIPTION: Configuration template for the Kibana input plugin. Includes server specification, timeout settings, authentication options, and TLS configuration parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kibana/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read status information from one or more Kibana servers\n[[inputs.kibana]]\n  ## Specify a list of one or more Kibana servers\n  servers = [\"http://localhost:5601\"]\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## If 'use_system_proxy' is set to true, Telegraf will check env vars such as\n  ## HTTP_PROXY, HTTPS_PROXY, and NO_PROXY (or their lowercase counterparts).\n  ## If 'use_system_proxy' is set to false (default) and 'http_proxy_url' is\n  ## provided, Telegraf will use the specified URL as HTTP proxy.\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n```\n\n----------------------------------------\n\nTITLE: Telegraf Binary Message Parsing Configuration\nDESCRIPTION: TOML configuration for Telegraf's file input plugin to parse three different binary message types. Includes filter definitions and field mappings for each message type.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/binary/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"messageA.bin\", \"messageB.bin\", \"messageC.bin\"]\n  data_format = \"binary\"\n  endianness = \"le\"\n\n  [[inputs.file.binary]]\n    metric_name = \"messageA\"\n\n    entries = [\n      { bits = 32, omit = true },\n      { name = \"address\", type = \"uint16\", assignment = \"tag\" },\n      { name = \"count\",   type = \"int16\" },\n      { name = \"failure\", type = \"bool\", bits = 32, assignment = \"tag\" },\n      { name = \"value\",   type = \"float64\" },\n      { type = \"unix\",    assignment = \"time\" },\n    ]\n\n    [inputs.file.binary.filter]\n      selection = [{ offset = 16, bits = 8, match = \"0x0A\" }]\n\n  [[inputs.file.binary]]\n    metric_name = \"messageB\"\n\n    entries = [\n      { bits = 32, omit = true },\n      { name = \"value\",   type = \"uint32\" },\n    ]\n\n    [inputs.file.binary.filter]\n      selection = [{ offset = 16, bits = 8, match = \"0x0B\" }]\n\n  [[inputs.file.binary]]\n    metric_name = \"messageC\"\n\n    entries = [\n      { bits = 32, omit = true },\n      { name = \"x\",   type = \"float32\" },\n      { name = \"y\",   type = \"float32\" },\n      { type = \"unix\",    assignment = \"time\" },\n    ]\n\n    [inputs.file.binary.filter]\n      selection = [{ offset = 16, bits = 8, match = \"0x0C\" }]\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Processor Plugin in Telegraf\nDESCRIPTION: Complete configuration sample for the timestamp processor plugin showing all available options including field selection, timestamp format specification, and timezone configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/timestamp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Convert a timestamp field to other timestamp format\n[[processors.timestamp]]\n  ## Timestamp key to convert\n  ## Specify the field name that contains the timestamp to convert. The result\n  ## will replace the current field value.\n  field = \"\"\n\n  ## Timestamp Format\n  ## This defines the time layout used to interpret the source timestamp field.\n  ## The time must be `unix`, `unix_ms`, `unix_us`, `unix_ns`, or a time in Go\n  ## \"reference time\". For more information on Go \"reference time\". For more\n  ## see: https://golang.org/pkg/time/#Time.Format\n  source_timestamp_format = \"\"\n\n  ## Timestamp Timezone\n  ## Source timestamp timezone. If not set, assumed to be in UTC.\n  ## Options are as follows:\n  ##   1. UTC                 -- or unspecified will return timestamp in UTC\n  ##   2. Local               -- interpret based on machine localtime\n  ##   3. \"America/New_York\"  -- Unix TZ values like those found in\n  ##        https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n  # source_timestamp_timezone = \"\"\n\n  ## Target timestamp format\n  ## This defines the destination timestamp format. It also can accept either\n  ## `unix`, `unix_ms`, `unix_us`, `unix_ns`, or a time in Go \"reference time\".\n  destination_timestamp_format = \"\"\n\n  ## Target Timestamp Timezone\n  ## Source timestamp timezone. If not set, assumed to be in UTC.\n  ## Options are as follows:\n  ##   1. UTC                 -- or unspecified will return timestamp in UTC\n  ##   2. Local               -- interpret based on machine localtime\n  ##   3. \"America/New_York\"  -- Unix TZ values like those found in\n  ##        https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n  # destination_timestamp_timezone = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenLDAP Plugin in TOML\nDESCRIPTION: Configuration settings for the OpenLDAP Telegraf input plugin, including host, port, TLS settings, authentication credentials, and metric name formatting options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openldap/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# OpenLDAP cn=Monitor plugin\n[[inputs.openldap]]\n  host = \"localhost\"\n  port = 389\n\n  # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n  # note that port will likely need to be changed to 636 for ldaps\n  # valid options: \"\" | \"starttls\" | \"ldaps\"\n  tls = \"\"\n\n  # skip peer certificate verification. Default is false.\n  insecure_skip_verify = false\n\n  # Path to PEM-encoded Root certificate to use to verify server certificate\n  tls_ca = \"/etc/ssl/certs.pem\"\n\n  # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n  bind_dn = \"\"\n  bind_password = \"\"\n\n  # reverse metric names so they sort more naturally\n  # Defaults to false if unset, but is set to true when generating a new config\n  reverse_metric_names = true\n```\n\n----------------------------------------\n\nTITLE: Configuring S.M.A.R.T. Input Plugin for Telegraf\nDESCRIPTION: Sample configuration for the S.M.A.R.T. input plugin in Telegraf's configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from storage devices supporting S.M.A.R.T.\n[[inputs.smart]]\n    ## Optionally specify the path to the smartctl executable\n    # path_smartctl = \"/usr/bin/smartctl\"\n\n    ## Optionally specify the path to the nvme-cli executable\n    # path_nvme = \"/usr/bin/nvme\"\n\n    ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n    ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n    ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n    # enable_extensions = [\"auto-on\"]\n\n    ## On most platforms used cli utilities requires root access.\n    ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.\n    ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n    ## without a password.\n    # use_sudo = false\n\n    ## Adds an extra tag \"device_type\", which can be used to differentiate\n    ## multiple disks behind the same controller (e.g., MegaRAID).\n    # tag_with_device_type = false\n\n    ## Skip checking disks in this power mode. Defaults to\n    ## \"standby\" to not wake up disks that have stopped rotating.\n    ## See --nocheck in the man pages for smartctl.\n    ## smartctl version 5.41 and 5.42 have faulty detection of\n    ## power mode and might require changing this value to\n    ## \"never\" depending on your disks.\n    # nocheck = \"standby\"\n\n    ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n    ## information from each drive into the 'smart_attribute' measurement.\n    # attributes = false\n\n    ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n    # excludes = [ \"/dev/pass6\" ]\n\n    ## Optionally specify devices and device type, if unset\n    ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n    ## and all found will be included except for the excluded in excludes.\n    # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n\n    ## Timeout for the cli command to complete.\n    # timeout = \"30s\"\n\n    ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n    ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n    ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n    ## SMART data - one individual array drive at the time. In such case please set this configuration option\n    ## to \"sequential\" to get readings for all drives.\n    ## valid options: concurrent, sequential\n    # read_method = \"concurrent\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Certificate Source in Telegraf\nDESCRIPTION: The configuration snippet sets up the sources and additional settings for retrieving metrics from SSL certificates. Dependencies include Telegraf and its input plugins. Key parameters are 'sources' for certificate locations, 'timeout' for SSL connection timeout, and optional TLS configuration details. Inputs are certificate sources defined in various protocols, and outputs are metrics regarding certificate verification and attributes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/x509_cert/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Reads metrics from a SSL certificate\n[[inputs.x509_cert]]\n  ## List certificate sources, support wildcard expands for files\n  ## Prefix your entry with 'file://' if you intend to use relative paths\n  sources = [\"tcp://example.org:443\", \"https://influxdata.com:443\",\n            \"smtp://mail.localhost:25\", \"udp://127.0.0.1:4433\",\n            \"/etc/ssl/certs/ssl-cert-snakeoil.pem\",\n            \"/etc/mycerts/*.mydomain.org.pem\", \"file:///path/to/*.pem\",\n            \"jks:///etc/mycerts/keystore.jks\",\n            \"pkcs12:///etc/mycerts/keystore.p12\"]\n\n  ## Timeout for SSL connection\n  # timeout = \"5s\"\n\n  ## Pass a different name into the TLS request (Server Name Indication).\n  ## This is synonymous with tls_server_name, and only one of the two\n  ## options may be specified at one time.\n  ##   example: server_name = \"myhost.example.org\"\n  # server_name = \"myhost.example.org\"\n\n  ## Only output the leaf certificates and omit the root ones.\n  # exclude_root_certs = false\n\n  ## Pad certificate serial number with zeroes to 128-bits.\n  # pad_serial_with_zeroes = false\n\n  ## Password to be used with PKCS#12 or JKS files\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_server_name = \"myhost.example.org\"\n\n  ## Set the proxy URL\n  # use_proxy = true\n  # proxy_url = \"http://localhost:8888\"\n```\n\n----------------------------------------\n\nTITLE: Time Multiplexed PMU Metrics Output Format\nDESCRIPTION: Example output showing time multiplexed PMU metrics with various CPU events. Demonstrates how enabled and running times differ when events are multiplexed, affecting the scaled values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmu/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\npmu_metric,cpu=0,event=CPU_CLK_THREAD_UNHALTED.REF_XCLK,host=xyz raw=2947727i,scaled=4428970i,enabled=2201071844i,running=1464935978i 1621254412000000000\npmu_metric,cpu=0,event=CPU_CLK_UNHALTED.THREAD_P_ANY,host=xyz running=1465155618i,raw=302553190i,scaled=454511623i,enabled=2201035323i 1621254412000000000\npmu_metric,cpu=0,event=CPU_CLK_UNHALTED.REF_XCLK,host=xyz enabled=2200994057i,running=1466812391i,raw=3177535i,scaled=4767982i 1621254412000000000\npmu_metric,cpu=0,event=CPU_CLK_UNHALTED.REF_XCLK_ANY,host=xyz enabled=2200963921i,running=1470523496i,raw=3359272i,scaled=5027894i 1621254412000000000\npmu_metric,cpu=0,event=L1D_PEND_MISS.PENDING_CYCLES_ANY,host=xyz enabled=2200933946i,running=1470322480i,raw=23631950i,scaled=35374798i 1621254412000000000\npmu_metric,cpu=0,event=L1D_PEND_MISS.PENDING_CYCLES,host=xyz raw=18767833i,scaled=28169827i,enabled=2200888514i,running=1466317384i 1621254412000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel Baseband Accelerator Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Intel Baseband Accelerator Input Plugin for Telegraf. It specifies paths for the socket and log file, behavior for unreachable sockets, timeout settings for socket access and telemetry waiting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_baseband/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_baseband]]\n  ## Path to socket exposed by pf-bb-config for CLI interaction (mandatory).\n  ## In version v23.03 of pf-bb-config the path is created according to the schema:\n  ##   \"/tmp/pf_bb_config.0000\\:<b>\\:<d>.<f>.sock\" where 0000\\:<b>\\:<d>.<f> is the PCI device ID.\n  socket_path = \"\"\n\n  ## Path to log file exposed by pf-bb-config with telemetry to read (mandatory).\n  ## In version v23.03 of pf-bb-config the path is created according to the schema:\n  ##   \"/var/log/pf_bb_cfg_0000\\:<b>\\:<d>.<f>.log\" where 0000\\:<b>\\:<d>.<f> is the PCI device ID.\n  log_file_path = \"\"\n\n  ## Specifies plugin behavior regarding unreachable socket (which might not have been initialized yet).\n  ## Available choices:\n  ##   - error: Telegraf will return an error on startup if socket is unreachable\n  ##   - ignore: Telegraf will ignore error regarding unreachable socket on both startup and gather\n  # unreachable_socket_behavior = \"error\"\n\n  ## Duration that defines how long the connected socket client will wait for\n  ## a response before terminating connection.\n  ## Since it's local socket access to a fast packet processing application, the timeout should\n  ## be sufficient for most users.\n  ## Setting the value to 0 disables the timeout (not recommended).\n  # socket_access_timeout = \"1s\"\n\n  ## Duration that defines maximum time plugin will wait for pf-bb-config to write telemetry to the log file.\n  ## Timeout may differ depending on the environment.\n  ## Must be equal or larger than 50ms.\n  # wait_for_telemetry_timeout = \"1s\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Wireguard Metrics - Text\nDESCRIPTION: This text snippet demonstrates the expected output format for metrics collected from Wireguard interfaces and peers. It shows how device and peer data is formatted and timestamps represented in nanoseconds.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/wireguard/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n\"wireguard_device,host=WGVPN,name=wg0,type=linux_kernel firewall_mark=51820i,listen_port=58216i 1582513589000000000\\nwireguard_device,host=WGVPN,name=wg0,type=linux_kernel peers=1i 1582513589000000000\\nwireguard_peer,device=wg0,host=WGVPN,public_key=NZTRIrv/ClTcQoNAnChEot+WL7OH7uEGQmx8oAN9rWE= allowed_ips=2i,persistent_keepalive_interval_ns=60000000000i,protocol_version=1i,allowed_peer_cidr=192.168.1.0/24,10.0.0.0/8 1582513589000000000\\nwireguard_peer,device=wg0,host=WGVPN,public_key=NZTRIrv/ClTcQoNAnChEot+WL7OH7uEGQmx8oAN9rWE= last_handshake_time_ns=1582513584530013376i,rx_bytes=6484i,tx_bytes=13540i 1582513589000000000\"\n```\n\n----------------------------------------\n\nTITLE: K3s Quick Setup Configuration\nDESCRIPTION: TOML configuration specifically for k3s server instances, using existing admin credentials for authentication.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[kube_inventory]\nbearer_token = \"/run/telegraf-kubernetes-token\"\ntls_cert = \"/run/telegraf-kubernetes-cert\"\ntls_key = \"/run/telegraf-kubernetes-key\"\n```\n\n----------------------------------------\n\nTITLE: Ordering Processor Plugins in Telegraf Configuration\nDESCRIPTION: Example demonstrating how to specify the order in which processor plugins execute in Telegraf. Shows two processors (rename and strings) with explicit order parameters to control processing sequence.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_17\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.rename]]\n  order = 1\n  [[processors.rename.replace]]\n    tag = \"path\"\n    dest = \"resource\"\n\n[[processors.strings]]\n  order = 2\n  [[processors.strings.trim_prefix]]\n    tag = \"resource\"\n    prefix = \"/api/\"\n```\n\n----------------------------------------\n\nTITLE: Example JMX Metrics Output\nDESCRIPTION: Sample output showing the format and structure of JMX metrics collected through the Jolokia2 proxy, including memory pool statistics and usage metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jolokia2_proxy/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=Compressed\\ Class\\ Space PeakUsage.max=1073741824,PeakUsage.committed=3145728,PeakUsage.init=0,Usage.committed=3145728,Usage.init=0,PeakUsage.used=3017976,Usage.max=1073741824,Usage.used=3017976 1503764025000000000\njvm_memory_pool,pool_name=Code\\ Cache PeakUsage.init=2555904,PeakUsage.committed=6291456,Usage.committed=6291456,PeakUsage.used=6202752,PeakUsage.max=251658240,Usage.used=6210368,Usage.max=251658240,Usage.init=2555904 1503764025000000000\njvm_memory_pool,pool_name=G1\\ Eden\\ Space CollectionUsage.max=-1,PeakUsage.committed=56623104,PeakUsage.init=56623104,PeakUsage.used=53477376,Usage.max=-1,Usage.committed=49283072,Usage.used=19922944,CollectionUsage.committed=49283072,CollectionUsage.init=56623104,CollectionUsage.used=0,PeakUsage.max=-1,Usage.init=56623104 1503764025000000000\njvm_memory_pool,pool_name=G1\\ Old\\ Gen CollectionUsage.max=1073741824,CollectionUsage.committed=0,PeakUsage.max=1073741824,PeakUsage.committed=1017118720,PeakUsage.init=1017118720,PeakUsage.used=137032208,Usage.max=1073741824,CollectionUsage.init=1017118720,Usage.committed=1017118720,Usage.init=1017118720,Usage.used=134708752,CollectionUsage.used=0 1503764025000000000\njvm_memory_pool,pool_name=G1\\ Survivor\\ Space Usage.max=-1,Usage.init=0,CollectionUsage.max=-1,CollectionUsage.committed=7340032,CollectionUsage.used=7340032,PeakUsage.committed=7340032,Usage.committed=7340032,Usage.used=7340032,CollectionUsage.init=0,PeakUsage.max=-1,PeakUsage.init=0,PeakUsage.used=7340032 1503764025000000000\njvm_memory_pool,pool_name=Metaspace PeakUsage.init=0,PeakUsage.used=21852224,PeakUsage.max=-1,Usage.max=-1,Usage.committed=22282240,Usage.init=0,Usage.used=21852224,PeakUsage.committed=22282240 1503764025000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Graphite Output in Telegraf (TOML)\nDESCRIPTION: This snippet provides a sample configuration for the Graphite output plugin in Telegraf. It includes settings for server endpoints, prefix, templates, timeouts, and TLS options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/graphite/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for Graphite server to send metrics to\n[[outputs.graphite]]\n  ## TCP endpoint for your graphite instance.\n  ## If multiple endpoints are configured, the output will be load balanced.\n  ## Only one of the endpoints will be written to with each iteration.\n  servers = [\"localhost:2003\"]\n\n  ## Local address to bind when connecting to the server\n  ## If empty or not set, the local address is automatically chosen.\n  # local_address = \"\"\n\n  ## Prefix metrics name\n  prefix = \"\"\n\n  ## Graphite output template\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  template = \"host.tags.measurement.field\"\n\n  ## Strict sanitization regex\n  ## This is the default sanitization regex that is used on data passed to the\n  ## graphite serializer. Users can add additional characters here if required.\n  ## Be aware that the characters, '/' '@' '*' are always replaced with '_',\n  ## '..' is replaced with '.', and '\\' is removed even if added to the\n  ## following regex.\n  # graphite_strict_sanitize_regex = '[^a-zA-Z0-9-:._=\\p{L}]'\n\n  ## Enable Graphite tags support\n  # graphite_tag_support = false\n\n  ## Applied sanitization mode when graphite tag support is enabled.\n  ## * strict - uses the regex specified above\n  ## * compatible - allows for greater number of characters\n  # graphite_tag_sanitize_mode = \"strict\"\n\n  ## Character for separating metric name and field for Graphite tags\n  # graphite_separator = \".\"\n\n  ## Graphite templates patterns\n  ## 1. Template for cpu\n  ## 2. Template for disk*\n  ## 3. Default template\n  # templates = [\n  #  \"cpu tags.measurement.host.field\",\n  #  \"disk* measurement.field\",\n  #  \"host.measurement.tags.field\"\n  #]\n\n  ## timeout in seconds for the write connection to graphite\n  # timeout = \"2s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Checking UDP/IP Receive Buffer Limits (Shell)\nDESCRIPTION: This snippet outlines shell commands for checking and adjusting the UDP/IP receive buffer limits on Linux systems, ensuring optimal performance for UDP traffic. It involves querying current settings and modifying the configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socket_listener/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n\"\"\"\nsysctl net.core.rmem_max\nsysctl net.core.rmem_default\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PMT Plugin in Telegraf\nDESCRIPTION: TOML configuration for the Intel PMT Telegraf input plugin. Specifies the path to PMT XML specification file and options for enabling specific metrics by datatype or name.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_pmt]]\n  ## Filepath to PMT XML within local copies of XML files from PMT repository.\n  ## The filepath should be absolute.\n  spec = \"/home/telegraf/Intel-PMT/xml/pmt.xml\"\n  \n  ## Enable metrics by their datatype.\n  ## See the Enabling Metrics section in README for more details.\n  ## If empty, all metrics are enabled.\n  ## When used, the alternative option samples_enabled should NOT be used.\n  # datatypes_enabled = []\n  \n  ## Enable metrics by their name.\n  ## See the Enabling Metrics section in README for more details.\n  ## If empty, all metrics are enabled.\n  ## When used, the alternative option datatypes_enabled should NOT be used.\n  # samples_enabled = []\n```\n\n----------------------------------------\n\nTITLE: Basic AWS EC2 Tag Configuration Example\nDESCRIPTION: Simple configuration example showing how to append accountId and instanceId to metrics tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/aws_ec2/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.aws_ec2]]\n  tags = [ \"accountId\", \"instanceId\"]\n```\n\n----------------------------------------\n\nTITLE: Running fail2ban-client directly to view status\nDESCRIPTION: Shell command showing how to run fail2ban-client directly to view the status of a jail, with sample output displaying the current failed and banned counts.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fail2ban/README.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# fail2ban-client status sshd\nStatus for the jail: sshd\n|- Filter\n|  |- Currently failed: 5\n|  |- Total failed:     20\n|  `- File list:        /var/log/secure\n`- Actions\n   |- Currently banned: 2\n   |- Total banned:     10\n   `- Banned IP list:   192.168.0.1 192.168.0.2\n```\n\n----------------------------------------\n\nTITLE: Configuring Filecount Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Filecount input plugin including directory paths, file matching patterns, recursive options, and file filtering criteria based on size and modification time.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/filecount/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Count files in a directory\n[[inputs.filecount]]\n  ## Directories to gather stats about.\n  ## This accept standard unit glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**    -> recursively find all directories in /var/log and count files in each directories\n  ##   /var/log/*/*   -> find all directories with a parent dir in /var/log and count files in each directories\n  ##   /var/log       -> count all files in /var/log and all of its subdirectories\n  directories = [\"/var/cache/apt\", \"/tmp\"]\n\n  ## Only count files that match the name pattern. Defaults to \"*\".\n  name = \"*\"\n\n  ## Count files in subdirectories. Defaults to true.\n  recursive = true\n\n  ## Only count regular files. Defaults to true.\n  regular_only = true\n\n  ## Follow all symlinks while walking the directory tree. Defaults to false.\n  follow_symlinks = false\n\n  ## Only count files that are at least this size. If size is\n  ## a negative number, only count files that are smaller than the\n  ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n  ## Without quotes and units, interpreted as size in bytes.\n  size = \"0B\"\n\n  ## Only count files that have not been touched for at least this\n  ## duration. If mtime is negative, only count files that have been\n  ## touched in this duration. Defaults to \"0s\".\n  mtime = \"0s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Win Eventlog Input Plugin - TOML\nDESCRIPTION: This snippet configures the win_eventlog input plugin to collect metrics from Windows Event Logs. It defines global configuration options, specifies queries, and outlines expected behaviors for fetching and filtering events.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_eventlog/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_eventlog]]\n  # Telegraf should have Administrator permissions to subscribe for some\n  ## Windows Events channels (e.g. System log)\n\n  # locale = 0\n\n  # eventlog_name = \"\"\n\n  xpath_query = '''\n  <QueryList>\n    <Query Id=\"0\" Path=\"Security\">\n      <Select Path=\"Security\">*</Select>\n      <Suppress Path=\"Security\">*[System[( (EventID &gt;= 5152 and EventID &lt;= 5158) or EventID=5379 or EventID=4672)]]</Suppress>\n    </Query>\n    <Query Id=\"1\" Path=\"Application\">\n      <Select Path=\"Application\">*[System[(Level &lt; 4)]]</Select>\n    </Query>\n    <Query Id=\"2\" Path=\"Windows PowerShell\">\n      <Select Path=\"Windows PowerShell\">*[System[(Level &lt; 4)]]</Select>\n    </Query>\n    <Query Id=\"3\" Path=\"System\">\n      <Select Path=\"System\">*</Select>\n    </Query>\n    <Query Id=\"4\" Path=\"Setup\">\n      <Select Path=\"Setup\">*</Select>\n    </Query>\n  </QueryList>\n  '''\n\n  # from_beginning = false\n\n  # event_batch_size = 5\n\n  # process_userdata = true\n\n  # process_eventdata = true\n\n  # separator = \"_\"\n\n  # only_first_line_of_message = true\n\n  # timestamp_from_event = true\n\n  # event_tags = [\"Source\", \"EventID\", \"Level\", \"LevelText\", \"Task\", \"TaskText\", \"Opcode\", \"OpcodeText\", \"Keywords\", \"Channel\", \"Computer\"]\n\n  # event_fields = [\"*\"]\n\n  # exclude_fields = []\n\n  # exclude_empty = [\"Task\", \"Opcode\", \"*ActivityID\", \"UserID\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Explorer Output Plugin in Telegraf\nDESCRIPTION: TOML configuration for the Azure Data Explorer output plugin in Telegraf. Specifies endpoint URL, database, timeout, metrics grouping type, and other settings for data ingestion.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/azure_data_explorer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Sends metrics to Azure Data Explorer\n[[outputs.azure_data_explorer]]\n  ## The URI property of the Azure Data Explorer resource on Azure\n  ## ex: endpoint_url = https://myadxresource.australiasoutheast.kusto.windows.net\n  endpoint_url = \"\"\n\n  ## The Azure Data Explorer database that the metrics will be ingested into.\n  ## The plugin will NOT generate this database automatically, it's expected that this database already exists before ingestion.\n  ## ex: \"exampledatabase\"\n  database = \"\"\n\n  ## Timeout for Azure Data Explorer operations\n  # timeout = \"20s\"\n\n  ## Type of metrics grouping used when pushing to Azure Data Explorer.\n  ## Default is \"TablePerMetric\" for one table per different metric.\n  ## For more information, please check the plugin README.\n  # metrics_grouping_type = \"TablePerMetric\"\n\n  ## Name of the single table to store all the metrics (Only needed if metrics_grouping_type is \"SingleTable\").\n  # table_name = \"\"\n\n  ## Creates tables and relevant mapping if set to true(default).\n  ## Skips table and mapping creation if set to false, this is useful for running Telegraf with the lowest possible permissions i.e. table ingestor role.\n  # create_tables = true\n\n  ##  Ingestion method to use.\n  ##  Available options are\n  ##    - managed  --  streaming ingestion with fallback to batched ingestion or the \"queued\" method below\n  ##    - queued   --  queue up metrics data and process sequentially\n  # ingestion_type = \"queued\"\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus HTTP Duration Metrics\nDESCRIPTION: Defines Prometheus metrics for measuring HTTP request durations in microseconds. Includes help text, metric type declaration, and measurements across different quantiles (0.5, 0.9, 0.99) along with sum and count values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/valid_summary/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP http_request_duration_microseconds The HTTP request latencies in microseconds.\n# TYPE http_request_duration_microseconds summary\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.5\"} 552048.506\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.9\"} 5.876804288e+06\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.99\"} 5.876804288e+06\nhttp_request_duration_microseconds_sum{handler=\"prometheus\"} 1.8909097205e+07\nhttp_request_duration_microseconds_count{handler=\"prometheus\"} 9\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Nginx input plugin in Telegraf. Specifies the URLs to gather statistics from Nginx's stub_status module, with optional TLS configuration and response timeout settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Nginx's basic status information (ngx_http_stub_status_module)\n[[inputs.nginx]]\n  ## An array of Nginx stub_status URI to gather stats.\n  urls = [\"http://localhost/server_status\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Varnish Input Plugin in TOML\nDESCRIPTION: This code snippet provides a sample configuration for the Varnish Input Plugin in Telegraf. It includes paths for the varnishstat and varnishadm binaries, options for using sudo, and customization of metrics and regex patterns. The snippet also highlights the use of different metric versions, which depend on the version of Varnish used.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin to collect stats from Varnish HTTP Cache\n# This plugin ONLY supports non-Windows\n[[inputs.varnish]]\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the varnishstat binary can be overridden with:\n  binary = \"/usr/bin/varnishstat\"\n\n  ## Additional custom arguments for the varnishstat command\n  # binary_args = [\"-f\", \"MAIN.*\"]\n\n  ## The default location of the varnishadm binary can be overridden with:\n  adm_binary = \"/usr/bin/varnishadm\"\n\n  ## Custom arguments for the varnishadm command\n  # adm_binary_args = [\"\"]\n\n  ## Metric version defaults to metric_version=1, use metric_version=2 for removal of nonactive vcls\n  ## Varnish 6.0.2 and newer is required for metric_version=2.\n  metric_version = 1\n\n  ## Additional regexps to override builtin conversion of varnish metrics into telegraf metrics.\n  ## Regexp group \"_vcl\" is used for extracting the VCL name. Metrics that contain nonactive VCL's are skipped.\n  ## Regexp group \"_field\" overrides the field name. Other named regexp groups are used as tags.\n  # regexps = ['^XCNT\\.(?P<_vcl>[\\w\\-]*)(\\.)*(?P<group>[\\w\\-.+]*)\\.(?P<_field>[\\w\\-.+]*)\\.val']\n\n  ## By default, telegraf gather stats for 3 metric points.\n  ## Setting stats will override the defaults shown below.\n  ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n  ## stats may also be set to [\"*\"], which will collect all stats\n  stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n\n  ## Optional name for the varnish instance (or working directory) to query\n  ## Usually append after -n in varnish cli\n  # instance_name = instanceName\n\n  ## Timeout for varnishstat command\n  # timeout = \"1s\"\n\n```\n\n----------------------------------------\n\nTITLE: Generating Complete Telegraf Configuration\nDESCRIPTION: Command to output a sample configuration with default values for all available plugins. This redirects the output to a file named telegraf.conf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/COMMANDS_AND_FLAGS.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf config > telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: Configuring Systemd-Units Input Plugin in TOML\nDESCRIPTION: Sample configuration for the systemd_units Telegraf plugin that collects information about systemd unit states. The configuration includes options for filtering units by pattern, unit type, and scope, as well as settings for collecting disabled units and detailed information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/systemd_units/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather information about systemd-unit states\n# This plugin ONLY supports Linux\n[[inputs.systemd_units]]\n  ## Pattern of units to collect\n  ## A space-separated list of unit-patterns including wildcards determining\n  ## the units to collect.\n  ##  ex: pattern = \"telegraf* influxdb* user@*\"\n  # pattern = \"*\"\n\n  ## Filter for a specific unit type\n  ## Available settings are: service, socket, target, device, mount,\n  ## automount, swap, timer, path, slice and scope\n  # unittype = \"service\"\n\n  ## Collect system or user scoped units\n  ##  ex: scope = \"user\"\n  # scope = \"system\"\n\n  ## Collect also units not loaded by systemd, i.e. disabled or static units\n  ## Enabling this feature might introduce significant load when used with\n  ## unspecific patterns (such as '*') as systemd will need to load all\n  ## matching unit files.\n  # collect_disabled_units = false\n\n  ## Collect detailed information for the units\n  # details = false\n\n  ## Timeout for state-collection\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Generating Disk Metrics for VSphere in Text Format\nDESCRIPTION: This snippet includes examples of disk metrics collected from virtual machines in a VSphere setup. Metrics such as read/write averages are included to evaluate disk performance effectively.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_virtualDisk,esxhostname=DC0_H0,guest=other,host=host.example.com,moid=vm-35,os=Mac,source=DC0_H0_VM0,vcenter=localhost:8989,vmname=DC0_H0_VM0 write_average=144i,read_average=4i 1535660299000000000\n```\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_virtualDisk,esxhostname=DC0_H0,guest=other,host=host.example.com,moid=vm-38,os=Mac,source=DC0_H0_VM1,vcenter=localhost:8989,vmname=DC0_H0_VM1 write_average=232i,read_average=4i 1535660299000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the OpenSearch Query Input Plugin for Telegraf. It specifies the cluster endpoints, authentication details, and query parameters for aggregating data from OpenSearch indices.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query]]\n  ## OpenSearch cluster endpoint(s). Multiple urls can be specified as part\n  ## of the same cluster.  Only one successful call will be made per interval.\n  urls = [ \"https://node1.os.example.com:9200\" ] # required.\n\n  ## OpenSearch client timeout, defaults to \"5s\".\n  # timeout = \"5s\"\n\n  ## HTTP basic authentication details\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Skip TLS validation.  Useful for local testing and self-signed certs.\n  # insecure_skip_verify = false\n\n  [[inputs.opensearch_query.aggregation]]\n    ## measurement name for the results of the aggregation query\n    measurement_name = \"measurement\"\n\n    ## OpenSearch index or index pattern to search\n    index = \"index-*\"\n\n    ## The date/time field in the OpenSearch index (mandatory).\n    date_field = \"@timestamp\"\n\n    ## If the field used for the date/time field in OpenSearch is also using\n    ## a custom date/time format it may be required to provide the format to\n    ## correctly parse the field.\n    ##\n    ## If using one of the built in OpenSearch formats this is not required.\n    ## https://opensearch.org/docs/2.4/opensearch/supported-field-types/date/#built-in-formats\n    # date_field_custom_format = \"\"\n\n    ## Time window to query (eg. \"1m\" to query documents from last minute).\n    ## Normally should be set to same as collection interval\n    query_period = \"1m\"\n\n    ## Lucene query to filter results\n    # filter_query = \"*\"\n\n    ## Fields to aggregate values (must be numeric fields)\n    # metric_fields = [\"metric\"]\n\n    ## Aggregation function to use on the metric fields\n    ## Must be set if 'metric_fields' is set\n    ## Valid values are: avg, sum, min, max, sum\n    # metric_function = \"avg\"\n\n    ## Fields to be used as tags.  Must be text, non-analyzed fields. Metric\n    ## aggregations are performed per tag\n    # tags = [\"field.keyword\", \"field2.keyword\"]\n\n    ## Set to true to not ignore documents when the tag(s) above are missing\n    # include_missing_tag = false\n\n    ## String value of the tag when the tag does not exist\n    ## Required when include_missing_tag is true\n    # missing_tag_value = \"null\"\n```\n\n----------------------------------------\n\nTITLE: Example SNMP Data Output in Telegraf\nDESCRIPTION: Sample output from Telegraf's SNMP plugin showing collected metrics. This shows the format of data collected from both system-level SNMP metrics and interface table data with various statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nsnmp,agent_host=127.0.0.1,sysName=example.org uptime=113319.74 1575509815000000000\ninterface,agent_host=127.0.0.1,ifDescr=wlan0,ifIndex=3,sysName=example.org ifAdminStatus=1i,ifInDiscards=0i,ifInErrors=0i,ifInNUcastPkts=0i,ifInOctets=3436617431i,ifInUcastPkts=2717778i,ifInUnknownProtos=0i,ifLastChange=0i,ifMtu=1500i,ifOperStatus=1i,ifOutDiscards=0i,ifOutErrors=0i,ifOutNUcastPkts=0i,ifOutOctets=581368041i,ifOutQLen=0i,ifOutUcastPkts=1354338i,ifPhysAddress=\"c8:5b:76:c9:e6:8c\",ifSpecific=\".0.0\",ifSpeed=0i,ifType=6i 1575509815000000000\ninterface,agent_host=127.0.0.1,ifDescr=eth0,ifIndex=2,sysName=example.org ifAdminStatus=1i,ifInDiscards=0i,ifInErrors=0i,ifInNUcastPkts=21i,ifInOctets=3852386380i,ifInUcastPkts=3634004i,ifInUnknownProtos=0i,ifLastChange=9088763i,ifMtu=1500i,ifOperStatus=1i,ifOutDiscards=0i,ifOutErrors=0i,ifOutNUcastPkts=0i,ifOutOctets=434865441i,ifOutQLen=0i,ifOutUcastPkts=2110394i,ifPhysAddress=\"c8:5b:76:c9:e6:8c\",ifSpecific=\".0.0\",ifSpeed=1000000000i,ifType=6i 1575509815000000000\ninterface,agent_host=127.0.0.1,ifDescr=lo,ifIndex=1,sysName=example.org ifAdminStatus=1i,ifInDiscards=0i,ifInErrors=0i,ifInNUcastPkts=0i,ifInOctets=51555569i,ifInUcastPkts=339097i,ifInUnknownProtos=0i,ifLastChange=0i,ifMtu=65536i,ifOperStatus=1i,ifOutDiscards=0i,ifOutErrors=0i,ifOutNUcastPkts=0i,ifOutOctets=51555569i,ifOutQLen=0i,ifOutUcastPkts=339097i,ifSpecific=\".0.0\",ifSpeed=10000000i,ifType=24i 1575509815000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Output Format in Telegraf\nDESCRIPTION: This snippet shows how to configure the CSV serializer in a Telegraf file output. It includes options for timestamp format, separators, headers, column prefixes, and column ordering to customize the CSV output format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/csv/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"csv\"\n\n  ## The default timestamp format is Unix epoch time.\n  # Other timestamp layout can be configured using the Go language time\n  # layout specification from https://golang.org/pkg/time/#Time.Format\n  # e.g.: csv_timestamp_format = \"2006-01-02T15:04:05Z07:00\"\n  # csv_timestamp_format = \"unix\"\n\n  ## The default separator for the CSV format.\n  # csv_separator = \",\"\n\n  ## Output the CSV header in the first line.\n  ## Enable the header when outputting metrics to a new file.\n  ## Disable when appending to a file or when using a stateless\n  ## output to prevent headers appearing between data lines.\n  # csv_header = false\n\n  ## Prefix tag and field columns with \"tag_\" and \"field_\" respectively.\n  ## This can be helpful if you need to know the \"type\" of a column.\n  # csv_column_prefix = false\n\n  ## Use the specified order for the columns.\n  ## This can be helpful if you need a specific output order. To specify tags,\n  ## use a `tag.` prefix, for fields use a `field.` prefix and use `name` and\n  ## `timestamp` to reference the measurement name and timestamp respectively.\n  ## NOTE: The output will only contain the specified tags, fields, etc. All\n  ##       other data will be dropped. In case a tag or field does not exist,\n  ##       the column will be empty.\n  ##  ex. csv_columns = [\"timestamp\", \"tag.host\", \"field.value\"]\n  ##\n  ## By default all metric data will be written in the order:\n  ##   timestamp, name, tags..., fields...\n  ## with tags and fields being ordered alphabetically.\n  # csv_columns = []\n```\n\n----------------------------------------\n\nTITLE: Auth0 Configuration for OAuth2 Secret-store\nDESCRIPTION: Example configuration for using Auth0 as the OAuth2 service provider in Telegraf's secret-store, showing required token_endpoint and audience parameter settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/oauth2/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[secretstores.oauth2]]\n  id = \"secretstore\"\n  service = \"auth0\"\n  token_endpoint = \"https://YOUR_DOMAIN/oauth/token\"\n\n  [[secretstores.oauth2.token]]\n    key = \"mytoken\"\n    client_id = \"YOUR_CLIENT_ID\"\n    client_secret = \"YOUR_CLIENT_SECRET\"\n\n    [secretstores.oauth2.token.parameters]\n        audience = \"YOUR_API_IDENTIFIER\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Graylog Output Plugin in TOML\nDESCRIPTION: Configuration template for the Graylog output plugin in Telegraf. Includes settings for server endpoints, connection timeout, short message field configuration, name field prefixing, connection retry options, and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/graylog/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send telegraf metrics to graylog\n[[outputs.graylog]]\n  ## Endpoints for your graylog instances.\n  servers = [\"udp://127.0.0.1:12201\"]\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n  ## The field to use as the GELF short_message, if unset the static string\n  ## \"telegraf\" will be used.\n  ##   example: short_message_field = \"message\"\n  # short_message_field = \"\"\n\n  ## According to GELF payload specification, additional fields names must be prefixed\n  ## with an underscore. Previous versions did not prefix custom field 'name' with underscore.\n  ## Set to true for backward compatibility.\n  # name_field_no_prefix = false\n\n  ## Connection retry options\n  ## Attempt to connect to the endpoints if the initial connection fails.\n  ## If 'false', Telegraf will give up after 3 connection attempt and will\n  ## exit with an error. If set to 'true', the plugin will retry to connect\n  ## to the unconnected endpoints infinitely.\n  # connection_retry = false\n  ## Time to wait between connection retry attempts.\n  # connection_retry_wait_time = \"15s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Tag Table with View in PostgreSQL for Telegraf\nDESCRIPTION: Complex PostgreSQL configuration that creates tag tables with views for Telegraf data. This setup stores metric and tag tables in a 'telegraf' schema while exposing joined views in the 'public' schema.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/postgresql/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\ntags_as_foreign_keys = true\nschema = \"telegraf\"\ncreate_templates = [\n    '''CREATE TABLE {{ .table }} ({{ .columns }})''',\n    '''CREATE VIEW {{ .table.WithSchema \"public\" }} AS SELECT time, {{ (.tagTable.Columns.Tags.Concat .allColumns.Fields).Identifiers | join \",\" }} FROM {{ .table }} t, {{ .tagTable }} tt WHERE t.tag_id = tt.tag_id''',\n]\nadd_column_templates = [\n    '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join \", ADD COLUMN IF NOT EXISTS \" }}''',\n    '''DROP VIEW IF EXISTS {{ .table.WithSchema \"public\" }}''',\n    '''CREATE VIEW {{ .table.WithSchema \"public\" }} AS SELECT time, {{ (.tagTable.Columns.Tags.Concat .allColumns.Fields).Identifiers | join \",\" }} FROM {{ .table }} t, {{ .tagTable }} tt WHERE t.tag_id = tt.tag_id''',\n]\ntag_table_add_column_templates = [\n    '''ALTER TABLE {{.table}} ADD COLUMN IF NOT EXISTS {{.columns|join \", ADD COLUMN IF NOT EXISTS \"}}''',\n    '''DROP VIEW IF EXISTS {{ .metricTable.WithSchema \"public\" }}''',\n    '''CREATE VIEW {{ .metricTable.WithSchema \"public\" }} AS SELECT time, {{ (.allColumns.Tags.Concat .metricTable.Columns.Fields).Identifiers | join \",\" }} FROM {{ .metricTable }} t, {{ .tagTable }} tt WHERE t.tag_id = tt.tag_id''',\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Juniper Telemetry Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Juniper Telemetry Input Plugin for Telegraf. It specifies server addresses, authentication details, sensor subscriptions, and various optional settings like TLS configuration and retry delays.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jti_openconfig_telemetry/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Subscribe and receive OpenConfig Telemetry data using JTI\n[[inputs.jti_openconfig_telemetry]]\n  ## List of device addresses to collect telemetry from\n  servers = [\"localhost:1883\"]\n\n  ## Authentication details. Username and password are must if device expects\n  ## authentication. Client ID must be unique when connecting from multiple instances\n  ## of telegraf to the same device\n  username = \"user\"\n  password = \"pass\"\n  client_id = \"telegraf\"\n\n  ## Frequency to get data\n  sample_frequency = \"1000ms\"\n\n  ## Sensors to subscribe for\n  ## A identifier for each sensor can be provided in path by separating with space\n  ## Else sensor path will be used as identifier\n  ## When identifier is used, we can provide a list of space separated sensors.\n  ## A single subscription will be created with all these sensors and data will\n  ## be saved to measurement with this identifier name\n  sensors = [\n   \"/interfaces/\",\n   \"collection /components/ /lldp\",\n  ]\n\n  ## We allow specifying sensor group level reporting rate. To do this, specify the\n  ## reporting rate in Duration at the beginning of sensor paths / collection\n  ## name. For entries without reporting rate, we use configured sample frequency\n  sensors = [\n   \"1000ms customReporting /interfaces /lldp\",\n   \"2000ms collection /components\",\n   \"/interfaces\",\n  ]\n\n  ## Timestamp Source\n  ## Set to 'collection' for time of collection, and 'data' for using the time\n  ## provided by the _timestamp field.\n  # timestamp_source = \"collection\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Minimal TLS version to accept by the client\n  # tls_min_version = \"TLS12\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n  ## Failed streams/calls will not be retried if 0 is provided\n  retry_delay = \"1000ms\"\n\n  ## Period for sending keep-alive packets on idle connections\n  ## This is helpful to identify broken connections to the server\n  # keep_alive_period = \"10s\"\n\n  ## To treat all string values as tags, set this to true\n  str_as_tags = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Zipkin Input Plugin in TOML\nDESCRIPTION: This configuration snippet sets up the Zipkin Input Plugin for Telegraf to gather trace and timing data. It includes settings for URL path, port, and timeout durations. Dependencies include an available Zipkin server and Telegraf installation. Inputs are trace data received in JSON or thrift format, and outputs include processed span data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zipkin/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather data from a Zipkin server including trace and timing data\n[[inputs.zipkin]]\n  ## URL path for span data\n  # path = \"/api/v1/spans\"\n\n  ## Port on which Telegraf listens\n  # port = 9411\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Converter Processor Plugin in Telegraf with TOML\nDESCRIPTION: Sample configuration for the Converter processor plugin showing all available options. It includes tag and field conversion settings with options for type conversion, measurement renaming, and timestamp handling.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/converter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Convert values to another metric value type\n[[processors.converter]]\n  ## Tags to convert\n  ##\n  ## The table key determines the target type, and the array of key-values\n  ## select the keys to convert.  The array may contain globs.\n  ##   <target-type> = [<tag-key>...]\n  [processors.converter.tags]\n    measurement = []\n    string = []\n    integer = []\n    unsigned = []\n    boolean = []\n    float = []\n\n    ## Optional tag to use as metric timestamp\n    # timestamp = []\n\n    ## Format of the timestamp determined by the tag above. This can be any of\n    ## \"unix\", \"unix_ms\", \"unix_us\", \"unix_ns\", or a valid Golang time format.\n    ## It is required, when using the timestamp option.\n    # timestamp_format = \"\"\n\n  ## Fields to convert\n  ##\n  ## The table key determines the target type, and the array of key-values\n  ## select the keys to convert.  The array may contain globs.\n  ##   <target-type> = [<field-key>...]\n  [processors.converter.fields]\n    measurement = []\n    tag = []\n    string = []\n    integer = []\n    unsigned = []\n    boolean = []\n    float = []\n\n    ## Optional field to use for converting base64 encoding of IEEE 754 Float32 values \n    ## i.e. data_json_content_state_openconfig-platform-psu:output-power\":\"RKeAAA==\"\n    ## into a float32 value 1340\n    # base64_ieee_float32 = []\n\n    ## Optional field to use as metric timestamp\n    # timestamp = []\n\n    ## Format of the timestamp determined by the field above. This can be any\n    ## of \"unix\", \"unix_ms\", \"unix_us\", \"unix_ns\", or a valid Golang time\n    ## format. It is required, when using the timestamp option.\n    # timestamp_format = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Input in Telegraf\nDESCRIPTION: This configuration shows how to set up the Memcached input plugin in Telegraf to collect metrics from one or more Memcached servers.  The `servers` array specifies the addresses of the Memcached instances to monitor, including optional port numbers. TLS configuration options are also available to secure the connection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/memcached/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Read metrics from one or many memcached servers.\\n[[inputs.memcached]]\\n  # An array of address to gather stats about. Specify an ip on hostname\\n  # with optional port. ie localhost, 10.0.0.1:11211, etc.\\n  servers = [\\\"localhost:11211\\\"]\\n  # An array of unix memcached sockets to gather stats about.\\n  # unix_sockets = [\\\"/var/run/memcached.sock\\\"]\\n\\n  ## Optional TLS Config\\n  # enable_tls = false\\n  # tls_ca = \\\"/etc/telegraf/ca.pem\\\"\\n  # tls_cert = \\\"/etc/telegraf/cert.pem\\\"\\n  # tls_key = \\\"/etc/telegraf/key.pem\\\"\\n  ## If false, skip chain & host verification\\n  # insecure_skip_verify = true\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NFS Client Plugin in TOML\nDESCRIPTION: Configuration template for the NFS client input plugin showing all available options including fullstat mode, mount filtering, and operation selection parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nfsclient/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read per-mount NFS client metrics from /proc/self/mountstats\n[[inputs.nfsclient]]\n  ## Read more low-level metrics (optional, defaults to false)\n  # fullstat = false\n\n  ## List of mounts to explicitly include or exclude (optional)\n  ## The pattern (Go regexp) is matched against the mount point (not the\n  ## device being mounted).  If include_mounts is set, all mounts are ignored\n  ## unless present in the list. If a mount is listed in both include_mounts\n  ## and exclude_mounts, it is excluded.  Go regexp patterns can be used.\n  # include_mounts = []\n  # exclude_mounts = []\n\n  ## List of operations to include or exclude from collecting.  This applies\n  ## only when fullstat=true.  Semantics are similar to {include,exclude}_mounts:\n  ## the default is to collect everything; when include_operations is set, only\n  ## those OPs are collected; when exclude_operations is set, all are collected\n  ## except those listed.  If include and exclude are set, the OP is excluded.\n  ## See /proc/self/mountstats for a list of valid operations; note that\n  ## NFSv3 and NFSv4 have different lists.  While it is not possible to\n  ## have different include/exclude lists for NFSv3/4, unused elements\n  ## in the list should be okay.  It is possible to have different lists\n  ## for different mountpoints:  use multiple [[input.nfsclient]] stanzas,\n  ## with their own lists.  See \"include_mounts\" above, and be careful of\n  ## duplicate metrics.\n  # include_operations = []\n  # exclude_operations = []\n```\n\n----------------------------------------\n\nTITLE: Configuring WHOIS Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration sets up the `inputs.whois` plugin in Telegraf. It specifies a list of domains to query, along with optional settings for a custom WHOIS server, query timeout, and enabling WHOIS referral chain query.  This configuration defines how Telegraf collects WHOIS data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/whois/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Reads whois data and expose as metrics\n[[inputs.whois]]\n  ## List of domains to query\n  domains = [\"example.com\", \"influxdata.com\"]\n\n  ## Use Custom WHOIS server\n  # server = \"whois.iana.org\"\n\n  ## Timeout for WHOIS queries\n  # timeout = \"30s\"\n\n  ## Enable WHOIS referral chain query\n  # referral_chain_query = false\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Output Plugin in Telegraf\nDESCRIPTION: Configuration template for the Syslog output plugin showing all available options including network address settings, TLS configuration, framing options, SD-PARAMs settings, and default message parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/syslog/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.syslog]]\n  ## URL to connect to\n  ## ex: address = \"tcp://127.0.0.1:8094\"\n  ## ex: address = \"tcp4://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n  ## ex: address = \"udp://127.0.0.1:8094\"\n  ## ex: address = \"udp4://127.0.0.1:8094\"\n  ## ex: address = \"udp6://127.0.0.1:8094\"\n  address = \"tcp://127.0.0.1:8094\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## The framing technique with which it is expected that messages are\n  ## transported (default = \"octet-counting\").  Whether the messages come\n  ## using the octet-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n  ## be one of \"octet-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## SD-PARAMs settings\n  ## Syslog messages can contain key/value pairs within zero or more\n  ## structured data sections.  For each unrecognized metric tag/field a\n  ## SD-PARAMS is created.\n  ##\n  ## Example:\n  ##   [[outputs.syslog]]\n  ##     sdparam_separator = \"_\"\n  ##     default_sdid = \"default@32473\"\n  ##     sdids = [\"foo@123\", \"bar@456\"]\n  ##\n  ##   input => xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n  ##   output (structured data only) => [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n\n  ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n  # sdparam_separator = \"_\"\n\n  ## Default sdid used for tags/fields that don't contain a prefix defined in\n  ## the explicit sdids setting below If no default is specified, no SD-PARAMs\n  ## will be used for unrecognized field.\n  # default_sdid = \"default@32473\"\n\n  ## List of explicit prefixes to extract from tag/field keys and use as the\n  ## SDID, if they match (see above example for more details):\n  # sdids = [\"foo@123\", \"bar@456\"]\n\n  ## Default severity value. Severity and Facility are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n  ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n  # default_severity_code = 5\n\n  ## Default facility value. Facility and Severity are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n  ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n  # default_facility_code = 1\n\n  ## Default APP-NAME value (RFC5424#section-6.2.5)\n  ## Used when no metric tag with key \"appname\" is defined.\n  ## If unset, \"Telegraf\" is the default\n  # default_appname = \"Telegraf\"\n```\n\n----------------------------------------\n\nTITLE: Example TopK Processor Configuration for Process CPU Usage\nDESCRIPTION: Configuration to select top 3 processes grouped by process ID and track CPU usage aggregation over 20-second intervals\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/topk/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.topk]]\n  period = 20\n  k = 3\n  group_by = [\"pid\"]\n  fields = [\"cpu_usage\"]\n```\n\n----------------------------------------\n\nTITLE: Array Data Type Configuration Example\nDESCRIPTION: Configuration for handling array data types. Demonstrates subscription setup for boolean and integer arrays.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ctrlx_datalayer/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.ctrlx_datalayer.subscription]]\n   measurement=\"array\"\n   nodes=[\n      { name=\"ar_uint8\", address=\"alldata/dynamic/array-of-uint8\"},\n      { name=\"ar_bool8\", address=\"alldata/dynamic/array-of-bool8\"},\n   ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud PubSub Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the cloud_pubsub input plugin that reads metrics from Google Cloud PubSub. Includes required settings for project and subscription configuration, as well as optional parameters for customizing behavior such as retry delays, message size limits, and subscription settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloud_pubsub/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from Google PubSub\n[[inputs.cloud_pubsub]]\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub subscription.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub subscription to ingest metrics from.\n  subscription = \"my-subscription\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. Number of seconds to wait before attempting to restart the\n  ## PubSub subscription receiver after an unexpected error.\n  ## If the streaming pull for a PubSub Subscription fails (receiver),\n  ## the agent attempts to restart receiving messages after this many seconds.\n  # retry_delay_seconds = 5\n\n  ## Optional. Maximum byte length of a message to consume.\n  ## Larger messages are dropped with an error. If less than 0 or unspecified,\n  ## treated as no limit.\n  # max_message_len = 1000000\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## The following are optional Subscription ReceiveSettings in PubSub.\n  ## Read more about these values:\n  ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n\n  ## Optional. Maximum number of seconds for which a PubSub subscription\n  ## should auto-extend the PubSub ACK deadline for each message. If less than\n  ## 0, auto-extension is disabled.\n  # max_extension = 0\n\n  ## Optional. Maximum number of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_messages = 0\n\n  ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_bytes = 0\n\n  ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n  ## to pull messages from PubSub concurrently. This limit applies to each\n  ## subscription separately and is treated as the PubSub default if less than\n  ## 1. Note this setting does not limit the number of messages that can be\n  ## processed concurrently (use \"max_outstanding_messages\" instead).\n  # max_receiver_go_routines = 0\n\n  ## Optional. If true, Telegraf will attempt to base64 decode the\n  ## PubSub message data before parsing. Many GCP services that\n  ## output JSON to Google PubSub base64-encode the JSON payload.\n  # base64_data = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## If content encoding is not \"identity\", sets the maximum allowed size, \n  ## in bytes, for a message payload when it's decompressed. Can be increased \n  ## for larger payloads or reduced to protect against decompression bombs.\n  ## Acceptable units are B, KiB, KB, MiB, MB...\n  # max_decompression_size = \"500MB\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Telegraf Procstat Plugin\nDESCRIPTION: Shows the formatted output of the Telegraf procstat plugin, including procstat_lookup metrics (with pid count and running status), procstat metrics (with process resource usage details), and procstat_socket metrics (with TCP connection details).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/procstat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nprocstat_lookup,host=prash-laptop,pattern=influxd,pid_finder=pgrep,result=success pid_count=1i,running=1i,result_code=0i 1582089700000000000\nprocstat,host=prash-laptop,pattern=influxd,process_name=influxd,user=root involuntary_context_switches=151496i,child_minor_faults=1061i,child_major_faults=8i,cpu_time_user=2564.81,pid=32025i,major_faults=8609i,created_at=1580107536000000000i,voluntary_context_switches=1058996i,cpu_time_system=616.98,memory_swap=0i,memory_locked=0i,memory_usage=1.7797634601593018,num_threads=18i,cpu_time_iowait=0,memory_rss=148643840i,memory_vms=1435688960i,memory_data=0i,memory_stack=0i,minor_faults=1856550i 1582089700000000000\nprocstat_socket,host=prash-laptop,process_name=browser,protocol=tcp4 bytes_received=826987i,bytes_sent=32869i,dest=\"192.168.0.2\",dest_port=443i,lost=0i,pid=32025i,retransmits=0i,rx_queue=0i,src=\"192.168.0.1\",src_port=52106i,state=\"established\",tx_queue=0i 1582089700000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PowerStat Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to configure the Intel PowerStat plugin in Telegraf. It allows users to specify which package metrics (current power consumption, DRAM power, TDP, etc.) and per-CPU metrics (frequency, C-state residency, temperature, etc.) to collect. It also shows how to include or exclude specific CPUs and configure the timeout for MSR reads, offering fine-grained control over monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n\n# Intel PowerStat plugin enables monitoring of platform metrics (power, TDP)\n# and per-CPU metrics like temperature, power and utilization. Please see the\n# plugin readme for details on software and hardware compatibility.\n# This plugin ONLY supports Linux.\n[[inputs.intel_powerstat]]\n  ## The user can choose which package metrics are monitored by the plugin with\n  ## the package_metrics setting:\n  ## - The default, will collect \"current_power_consumption\",\n  ##   \"current_dram_power_consumption\" and \"thermal_design_power\".\n  ## - Leaving this setting empty means no package metrics will be collected.\n  ## - Finally, a user can specify individual metrics to capture from the\n  ##   supported options list.\n  ## Supported options:\n  ##   \"current_power_consumption\", \"current_dram_power_consumption\",\n  ##   \"thermal_design_power\", \"max_turbo_frequency\", \"uncore_frequency\",\n  ##   \"cpu_base_frequency\"\n  # package_metrics = [\"current_power_consumption\", \"current_dram_power_consumption\", \"thermal_design_power\"]\n\n  ## The user can choose which per-CPU metrics are monitored by the plugin in\n  ## cpu_metrics array.\n  ## Empty or missing array means no per-CPU specific metrics will be collected\n  ## by the plugin.\n  ## Supported options:\n  ##   \"cpu_frequency\", \"cpu_c0_state_residency\", \"cpu_c1_state_residency\",\n  ##   \"cpu_c3_state_residency\", \"cpu_c6_state_residency\", \"cpu_c7_state_residency\",\n  ##   \"cpu_temperature\", \"cpu_busy_frequency\", \"cpu_c0_substate_c01\",\n  ##   \"cpu_c0_substate_c02\", \"cpu_c0_substate_c0_wait\"\n  # cpu_metrics = []\n\n  ## CPUs metrics to include from those configured in cpu_metrics array\n  ## Can't be combined with excluded_cpus. Empty means all CPUs are gathered.\n  ## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n  # included_cpus = []\n\n  ## CPUs metrics to exclude from those configured in cpu_metrics array\n  ## Can't be combined with included_cpus. Empty means all CPUs are gathered.\n  ## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n  # excluded_cpus = []\n\n  ## Filesystem location of JSON file that contains PMU event definitions.\n  ## Mandatory only for perf-related metrics (cpu_c0_substate_c01, cpu_c0_substate_c02, cpu_c0_substate_c0_wait).\n  # event_definitions = \"\"\n\n  ## The user can set the timeout duration for MSR reading.\n  ## Enabling this timeout can be useful in situations where, on heavily loaded systems,\n  ## the code waits too long for a kernel response to MSR read requests.\n  ## 0 disables the timeout (default).\n  # msr_read_timeout = \"0ms\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Input Plugin with Default Settings\nDESCRIPTION: Basic configuration for the ECS input plugin using the default settings. This configuration allows for metric collection with customizable container inclusion/exclusion patterns and timeout settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ecs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about ECS containers\n[[inputs.ecs]]\n  ## ECS metadata url.\n  ## Metadata v2 API is used if set explicitly. Otherwise,\n  ## v3 metadata endpoint API is used if available.\n  # endpoint_url = \"\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"RUNNING\" state will be captured.\n  ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n  ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n  # container_status_include = []\n  # container_status_exclude = []\n\n  ## ecs labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n  ecs_label_exclude = []\n\n  ## Timeout for queries.\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Input File with Protocol-Buffers - TOML\nDESCRIPTION: This snippet illustrates how to configure an input file in Telegraf to use the Protocol-Buffers data format. It specifies mandatory properties for the parser including protocol-buffer definition files and types while allowing for import paths and optional byte skips. Properly handle multiple definitions in data parsing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.dat\"]\n\n  data_format = \"xpath_protobuf\"\n  xpath_protobuf_files = [\"A.proto\"]\n  xpath_protobuf_type = \"foo.Measurement\"\n  xpath_protobuf_import_paths = [\".\", \"/data/my_proto_files\"]\n  \n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Time Series Output in Telegraf\nDESCRIPTION: Configuration settings for connecting Telegraf to a Redis Time Series server. Includes options for server address, authentication credentials, operation timeout, string field conversion, and TLS settings. The plugin enables writing metrics to Redis Time Series with customizable connection and security parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/redistimeseries/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Publishes metrics to a redis timeseries server\n[[outputs.redistimeseries]]\n  ## The address of the RedisTimeSeries server.\n  address = \"127.0.0.1:6379\"\n\n  ## Redis ACL credentials\n  # username = \"\"\n  # password = \"\"\n  # database = 0\n\n  ## Timeout for operations such as ping or sending metrics\n  # timeout = \"10s\"\n\n  ## Enable attempt to convert string fields to numeric values\n  ## If \"false\" or in case the string value cannot be converted the string\n  ## field will be dropped.\n  # convert_string_fields = true\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Batch Field Processing with XPath in Telegraf XML Parsing\nDESCRIPTION: Explains how to automate the field extraction process using XPath's field selectors within a Telegraf configuration. It highlights batch processing for dynamic or previously unknown fields, showcasing flexibility in parsing complex XML structures.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.xml\"]\n  data_format = \"xml\"\n\n  [[inputs.file.xpath]]\n    metric_selection = \"/Bus/child::Sensor\"\n    metric_name = \"string('sensors')\"\n\n    timestamp = \"/Gateway/Timestamp\"\n    timestamp_format = \"2006-01-02T15:04:05Z\"\n\n    field_selection = \"child::Variable\"\n    field_name = \"name(@*[1])\"\n    field_value = \"number(@*[1])\"\n\n    [inputs.file.xpath.tags]\n      name = \"substring-after(@name, ' ')\"\n```\n\n----------------------------------------\n\nTITLE: Event Configuration Pattern Definition\nDESCRIPTION: Regular expression pattern showing the schema for configuring events in the PMU plugin. Defines the format for event names and their modifiers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmu/README.md#2025-04-16_snippet_1\n\nLANGUAGE: regexp\nCODE:\n```\nEVENT_NAME(:(config|config1|config2)=(0x[0-9a-f]{1-16})(p|k|u|h|H|I|G|D))*\n```\n\n----------------------------------------\n\nTITLE: Additional Configuration for Nested Dropwizard Registry\nDESCRIPTION: This TOML configuration snippet shows additional settings required to parse a Dropwizard registry nested within a JSON document. It specifies paths for the metric registry, timestamp, and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/dropwizard/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\ndropwizard_metric_registry_path = \"metrics\"\ndropwizard_time_path = \"time\"\ndropwizard_time_format = \"2006-01-02T15:04:05Z07:00\"\ndropwizard_tags_path = \"tags\"\n## tag paths per tag are supported too, eg.\n#[inputs.yourinput.dropwizard_tag_paths]\n#  tag1 = \"tags.tag1\"\n#  tag2 = \"tags.tag2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring APC UPSD Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the apcupsd Telegraf plugin. It specifies server connections and timeout settings for monitoring APC UPSes connected to apcupsd servers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/apcupsd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Monitor APC UPSes connected to apcupsd\n[[inputs.apcupsd]]\n  # A list of running apcupsd server to connect to.\n  # If not provided will default to tcp://127.0.0.1:3551\n  servers = [\"tcp://127.0.0.1:3551\"]\n\n  ## Timeout for dialing server.\n  timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Recording Goroutines Count in Prometheus Format\nDESCRIPTION: This snippet reports the current number of goroutines in the Go application. It helps in understanding the concurrency status of the application functioning.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/multiple/input.txt#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n# TYPE go_goroutines gauge\n# HELP go_goroutines Number of goroutines that currently exist.\ngo_goroutines 69\n```\n\n----------------------------------------\n\nTITLE: Implementing a Complete Min Aggregator Plugin for Telegraf\nDESCRIPTION: A complete implementation of a 'min' aggregator plugin for Telegraf. It maintains caches of metrics and calculates minimum values for numeric fields. The plugin demonstrates proper initialization, metric processing, result generation, and cleanup.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/AGGREGATORS.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n//go:generate ../../../tools/readme_config_includer/generator\npackage min\n\n// min.go\n\nimport (\n    _ \"embed\"\n\n    \"github.com/influxdata/telegraf\"\n    \"github.com/influxdata/telegraf/plugins/aggregators\"\n)\n\n//go:embed sample.conf\nvar sampleConfig string\n\ntype Min struct {\n    // caches for metric fields, names, and tags\n    fieldCache map[uint64]map[string]float64\n    nameCache  map[uint64]string\n    tagCache   map[uint64]map[string]string\n}\n\nfunc NewMin() telegraf.Aggregator {\n    m := &Min{}\n    m.Reset()\n    return m\n}\n\nfunc (*Min) SampleConfig() string {\n    return sampleConfig\n}\n\nfunc (m *Min) Init() error {\n    return nil\n}\n\nfunc (m *Min) Add(in telegraf.Metric) {\n    id := in.HashID()\n    if _, ok := m.nameCache[id]; !ok {\n        // hit an uncached metric, create caches for first time:\n        m.nameCache[id] = in.Name()\n        m.tagCache[id] = in.Tags()\n        m.fieldCache[id] = make(map[string]float64)\n        for k, v := range in.Fields() {\n            if fv, ok := convert(v); ok {\n                m.fieldCache[id][k] = fv\n            }\n        }\n    } else {\n        for k, v := range in.Fields() {\n            if fv, ok := convert(v); ok {\n                if _, ok := m.fieldCache[id][k]; !ok {\n                    // hit an uncached field of a cached metric\n                    m.fieldCache[id][k] = fv\n                    continue\n                }\n                if fv < m.fieldCache[id][k] {\n                    // set new minimum\n                    m.fieldCache[id][k] = fv\n                }\n            }\n        }\n    }\n}\n\nfunc (m *Min) Push(acc telegraf.Accumulator) {\n    for id, _ := range m.nameCache {\n        fields := map[string]interface{}{}\n        for k, v := range m.fieldCache[id] {\n            fields[k+\"_min\"] = v\n        }\n        acc.AddFields(m.nameCache[id], fields, m.tagCache[id])\n    }\n}\n\nfunc (m *Min) Reset() {\n    m.fieldCache = make(map[uint64]map[string]float64)\n    m.nameCache = make(map[uint64]string)\n    m.tagCache = make(map[uint64]map[string]string)\n}\n\nfunc convert(in interface{}) (float64, bool) {\n    switch v := in.(type) {\n    case float64:\n        return v, true\n    case int64:\n        return float64(v), true\n    default:\n        return 0, false\n    }\n}\n\nfunc init() {\n    aggregators.Add(\"min\", func() telegraf.Aggregator {\n        return NewMin()\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Input Plugin in TOML\nDESCRIPTION: Configuration template for the Kubernetes input plugin showing all available options including URL settings, authentication, metric naming, label filtering, and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kubernetes/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from the kubernetes kubelet api\n[[inputs.kubernetes]]\n  ## URL for the kubelet, if empty read metrics from all nodes in the cluster\n  url = \"http://127.0.0.1:10255\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /var/run/secrets/kubernetes.io/serviceaccount/token\n  ##\n  ## To re-read the token at each interval, please use a file with the\n  ## bearer_token option. If given a string, Telegraf will always use that\n  ## token.\n  # bearer_token = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Kubernetes Node Metric Name\n  ## The default Kubernetes node metric name (i.e. kubernetes_node) is the same\n  ## for the kubernetes and kube_inventory plugins. To avoid conflicts, set this\n  ## option to a different value.\n  # node_metric_name = \"kubernetes_node\"\n\n  ## Pod labels to be added as tags.  An empty array for both include and\n  ## exclude will include all labels.\n  # label_include = []\n  # label_exclude = [\"*\"]\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Input in Telegraf\nDESCRIPTION: This TOML configuration defines how Telegraf collects syslog messages. It specifies the protocol, address, and port for the syslog receiver, along with options for TLS configuration, buffer size, content encoding, framing technique, and RFC standard.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n[[inputs.syslog]]\n  ## Protocol, address and port to host the syslog receiver.\n  ## If no host is specified, then localhost is used.\n  ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n  ##   ex: server = \"tcp://localhost:6514\"\n  ##       server = \"udp://:6514\"\n  ##       server = \"unix:///var/run/telegraf-syslog.sock\"\n  ## When using tcp, consider using 'tcp4' or 'tcp6' to force the usage of IPv4\n  ## or IPV6 respectively. There are cases, where when not specified, a system\n  ## may force an IPv4 mapped IPv6 address.\n  server = \"tcp://127.0.0.1:6514\"\n\n  ## Permission for unix sockets (only available on unix sockets)\n  ## This setting may not be respected by some platforms. To safely restrict\n  ## permissions it is recommended to place the socket into a previously\n  ## created directory with the desired permissions.\n  ##   ex: socket_mode = \"777\"\n  # socket_mode = \"\"\n\n  ## Maximum number of concurrent connections (only available on stream sockets like TCP)\n  ## Zero means unlimited.\n  # max_connections = 0\n\n  ## Read timeout (only available on stream sockets like TCP)\n  ## Zero means unlimited.\n  # read_timeout = \"0s\"\n\n  ## Optional TLS configuration (only available on stream sockets like TCP)\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Maximum socket buffer size (in bytes when no unit specified)\n  ## For stream sockets, once the buffer fills up, the sender will start\n  ## backing up. For datagram sockets, once the buffer fills up, metrics will\n  ## start dropping. Defaults to the OS default.\n  # read_buffer_size = \"64KiB\"\n\n  ## Period between keep alive probes (only applies to TCP sockets)\n  ## Zero disables keep alive probes. Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Content encoding for message payloads\n  ## Can be set to \"gzip\" for compressed payloads or \"identity\" for no encoding.\n  # content_encoding = \"identity\"\n\n  ## Maximum size of decoded packet (in bytes when no unit specified)\n  # max_decompression_size = \"500MB\"\n\n  ## Framing technique used for messages transport\n  ## Available settings are:\n  ##   octet-counting  -- see RFC5425#section-4.3.1 and RFC6587#section-3.4.1\n  ##   non-transparent -- see RFC6587#section-3.4.2\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## Whether to parse in best effort mode or not (default = false).\n  ## By default best effort parsing is off.\n  # best_effort = false\n\n  ## The RFC standard to use for message parsing\n  ## By default RFC5424 is used. RFC3164 only supports UDP transport (no streaming support)\n  ## Must be one of \"RFC5424\", or \"RFC3164\".\n  # syslog_standard = \"RFC5424\"\n\n  ## Character to prepend to SD-PARAMs (default = \"_\").\n  ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n  ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n  ## For each combination a field is created.\n  ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n  # sdparam_separator = \"_\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring SLURM Input Plugin in Telegraf (TOML)\nDESCRIPTION: This TOML configuration snippet shows how to set up the SLURM input plugin for Telegraf. It includes options for specifying the SLURM REST API URL, authentication credentials, enabled endpoints, response timeout, and TLS settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/slurm/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather SLURM metrics\n[[inputs.slurm]]\n  ## Slurmrestd URL. Both http and https can be used as schemas.\n  url = \"http://127.0.0.1:6820\"\n\n  ## Credentials for JWT-based authentication.\n  # username = \"foo\"\n  # token = \"topSecret\"\n\n  ## Enabled endpoints\n  ## List of endpoints a user can acquire data from.\n  ## Available values are: diag, jobs, nodes, partitions, reservations.\n  # enabled_endpoints = [\"diag\", \"jobs\", \"nodes\", \"partitions\", \"reservations\"]\n\n  ## Maximum time to receive a response. If set to 0s, the\n  ## request will not time out.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config. Note these options will only\n  ## be taken into account when the scheme specififed on\n  ## the URL parameter is https. They will be silently\n  ## ignored otherwise.\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable =\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Password for the key file if it is encrypted\n  # tls_key_pwd = \"\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Minimal TLS version to accept by the client\n  # tls_min_version = \"TLS12\"\n  ## List of ciphers to accept, by default all secure ciphers will be accepted\n  ## See https://pkg.go.dev/crypto/tls#pkg-constants for supported values.\n  ## Use \"all\", \"secure\" and \"insecure\" to add all support ciphers, secure\n  ## suites or insecure suites respectively.\n  # tls_cipher_suites = [\"secure\"]\n  ## Renegotiation method, \"never\", \"once\" or \"freely\"\n  # tls_renegotiation_method = \"never\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Generating CPU Metrics for VSphere in Text Format\nDESCRIPTION: This snippet includes examples of CPU metrics from virtual machines in a VSphere deployment. Each metric entry contains details like hostname, VM ID, and various performance statistics required for monitoring and analytics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_cpu,esxhostname=DC0_H0,guest=other,host=host.example.com,moid=vm-35,os=Mac,source=DC0_H0_VM0,vcenter=localhost:8989,vmname=DC0_H0_VM0 run_summation=2608i,ready_summation=129i,usage_average=5.01,used_summation=2134i,demand_average=326i 1535660299000000000\n```\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_cpu,esxhostname=DC0_H0,guest=other,host=host.example.com,moid=vm-38,os=Mac,source=DC0_H0_VM1,vcenter=localhost:8989,vmname=DC0_H0_VM1 usage_average=5.49,used_summation=1804i,demand_average=308i,run_summation=2001i,ready_summation=120i 1535660299000000000\n```\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_cpu,clustername=DC0_C0,esxhostname=DC0_C0_H0,guest=other,host=host.example.com,moid=vm-41,os=Mac,source=DC0_C0_RP0_VM0,vcenter=localhost:8989,vmname=DC0_C0_RP0_VM0 usage_average=4.19,used_summation=2108i,demand_average=285i,run_summation=1793i,ready_summation=93i 1535660299000000000\n```\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_cpu,clustername=DC0_C0,esxhostname=DC0_C0_H0,guest=other,host=host.example.com,moid=vm-44,os=Mac,source=DC0_C0_RP0_VM1,vcenter=localhost:8989,vmname=DC0_C0_RP0_VM1 run_summation=2277i,ready_summation=118i,usage_average=4.67,used_summation=2546i,demand_average=289i 1535660299000000000\n```\n\n----------------------------------------\n\nTITLE: Querying Memcached Metrics in SQL\nDESCRIPTION: This SQL query demonstrates how to calculate the average get hit and miss ratio, as well as the average size of cached items, number of cached items, and average connection counts per Memcached server over the past hour.  The query groups the results by server, allowing for analysis of individual Memcached instance performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/memcached/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n\"SELECT mean(get_hits) / mean(cmd_get) as get_ratio, mean(get_misses) / mean(cmd_get) as get_misses_ratio, mean(bytes), mean(curr_items), mean(curr_connections) FROM memcached WHERE time > now() - 1h GROUP BY server\"\n```\n\n----------------------------------------\n\nTITLE: Example Output for Memory Metrics as Text\nDESCRIPTION: This text output example illustrates the data collected by the Memory Input Plugin after running the configuration. It lists various memory metrics such as 'active', 'available', and 'used_percent', along with their corresponding values. These metrics help in monitoring and analyzing system memory usage, providing a valuable resource for system metrics and performance evaluations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mem/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmem active=9299595264i,available=16818249728i,available_percent=80.41654254645131,buffered=2383761408i,cached=13316689920i,commit_limit=14751920128i,committed_as=11781156864i,dirty=122880i,free=1877688320i,high_free=0i,high_total=0i,huge_page_size=2097152i,huge_pages_free=0i,huge_pages_total=0i,inactive=7549939712i,low_free=0i,low_total=0i,mapped=416763904i,page_tables=19787776i,shared=670679040i,slab=2081071104i,sreclaimable=1923395584i,sunreclaim=157675520i,swap_cached=1302528i,swap_free=4286128128i,swap_total=4294963200i,total=20913917952i,used=3335778304i,used_percent=15.95004011996231,vmalloc_chunk=0i,vmalloc_total=35184372087808i,vmalloc_used=0i,wired=0i,write_back=0i,write_back_tmp=0i 1574712869000000000\n```\n\n----------------------------------------\n\nTITLE: Basic Region Template Configuration in TOML\nDESCRIPTION: Demonstrates a simple template that transforms incoming metrics by mapping regions as tags and remaining elements as measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\ntemplates = [\n    \"region.region.measurement*\"\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon Kinesis Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up the Amazon Kinesis output plugin for Telegraf. It includes options for AWS authentication, stream configuration, data formatting, and partition key selection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/kinesis/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.kinesis]]\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## debug will show upstream aws messages.\n  debug = false\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## The partition key can be calculated using one of several methods:\n  ##\n  ## Use a static value for all writes:\n  #  [outputs.kinesis.partition]\n  #    method = \"static\"\n  #    key = \"howdy\"\n  #\n  ## Use a random partition key on each write:\n  #  [outputs.kinesis.partition]\n  #    method = \"random\"\n  #\n  ## Use the measurement name as the partition key:\n  #  [outputs.kinesis.partition]\n  #    method = \"measurement\"\n  #\n  ## Use the value of a tag for all writes, if the tag is not set the empty\n  ## default option will be used. When no default, defaults to \"telegraf\"\n  #  [outputs.kinesis.partition]\n  #    method = \"tag\"\n  #    key = \"host\"\n  #    default = \"mykey\"\n```\n\n----------------------------------------\n\nTITLE: vSphere Host Performance Metrics\nDESCRIPTION: Time-series metrics data showing host performance across networking, CPU, disk and memory. Includes metrics like CPU utilization, network throughput, disk I/O, and memory usage with various dimensions like host, cluster and interface.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_17\n\nLANGUAGE: metrics\nCODE:\n```\nvsphere_host_net,esxhostname=DC0_H0,host=host.example.com,moid=host-19,os=Mac,source=DC0_H0,vcenter=localhost:8989 usage_average=1887i,bytesRx_average=662i,bytesTx_average=251i 1535660339000000000\nvsphere_host_net,esxhostname=DC0_H0,host=host.example.com,interface=vmnic0,moid=host-19,os=Mac,source=DC0_H0,vcenter=localhost:8989 usage_average=1481i,bytesTx_average=899i,bytesRx_average=992i 1535660339000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Couchbase Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for setting up Couchbase metrics collection in Telegraf. Includes server connection settings, bucket stats filtering, TLS configuration, and additional statistics collection options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/couchbase/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.couchbase]]\n  servers = [\"http://localhost:8091\"]\n\n  # bucket_stats_included = [\"quota_percent_used\", \"ops_per_sec\", \"disk_fetches\", \"item_count\", \"disk_used\", \"data_used\", \"mem_used\"]\n\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n\n  # cluster_bucket_stats = true\n  # node_bucket_stats = false\n\n  # additional_stats = []\n```\n\n----------------------------------------\n\nTITLE: Example KNX Metrics Output\nDESCRIPTION: Sample output showing the Line Protocol format of collected KNX metrics including illumination, temperature, and window state measurements with their respective tags and values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/knx_listener/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nillumination,groupaddress=5/5/4,host=Hugin,source=1.1.12,unit=lux value=17.889999389648438 1582132674999013274\ntemperature,groupaddress=5/5/1,host=Hugin,source=1.1.8,unit=C value=17.799999237060547 1582132663427587361\nwindowopen,groupaddress=1/0/1,host=Hugin,source=1.1.3 value=true 1582132630425581320\n```\n\n----------------------------------------\n\nTITLE: Filtering Prometheus Metrics with namepass and namedrop in Telegraf\nDESCRIPTION: This example demonstrates how to use namepass and namedrop to filter Prometheus metrics. It drops container-related metrics and keeps only rest client metrics for kubelet.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_22\n\nLANGUAGE: toml\nCODE:\n```\n# Drop all metrics about containers for kubelet\n[[inputs.prometheus]]\n  urls = [\"http://kube-node-1:4194/metrics\"]\n  namedrop = [\"container_*\"]\n\n# Only store rest client related metrics for kubelet\n[[inputs.prometheus]]\n  urls = [\"http://kube-node-1:4194/metrics\"]\n  namepass = [\"rest_client_*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Mock Data Input Plugin in TOML\nDESCRIPTION: Configuration template for the mock input plugin, demonstrating how to set metric names, tags, and various data generation algorithms\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mock/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Generate metrics for test and demonstration purposes\n[[inputs.mock]]\n  ## Set the metric name to use for reporting\n  metric_name = \"mock\"\n\n  ## Optional string key-value pairs of tags to add to all metrics\n  # [inputs.mock.tags]\n  # \"key\" = \"value\"\n\n  ## One or more mock data fields *must* be defined.\n  # [[inputs.mock.constant]]\n  #   name = \"constant\"\n  #   value = value_of_any_type\n  # [[inputs.mock.random]]\n  #   name = \"rand\"\n  #   min = 1.0\n  #   max = 6.0\n  # [[inputs.mock.sine_wave]]\n  #   name = \"wave\"\n  #   amplitude = 1.0\n  #   period = 0.5\n  #   phase = 20.0\n  #   base_line = 0.0\n  # [[inputs.mock.step]]\n  #   name = \"plus_one\"\n  #   start = 0.0\n  #   step = 1.0\n  # [[inputs.mock.stock]]\n  #   name = \"abc\"\n  #   price = 50.00\n  #   volatility = 0.2\n```\n\n----------------------------------------\n\nTITLE: Configuring Binary Serializer with Socket Writer in Telegraf\nDESCRIPTION: Configuration example for the binary serializer plugin showing how to set up endianness and define message format entries with various data types and properties.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/binary/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.socket_writer]]\n  address = \"tcp://127.0.0.1:54000\"\n  metric_batch_size = 1\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"binary\"\n  \n  ## Specify the endianness of the data.\n  ## Available values are \"little\" (little-endian), \"big\" (big-endian) and \"host\",\n  ## where \"host\" means the same endianness as the machine running Telegraf.\n  # endianness = \"host\"\n  \n  ## Definition of the message format and the serialized data.\n  ## Please note that you need to define all elements of the data in the\n  ## correct order.\n  ## An entry can have the following properties:\n  ##  read_from         --  Source of the data. \n  ##                        Can be \"field\", \"tag\", \"time\" or \"name\". \n  ##                        If omitted \"field\" is assumed.\n  ##  name              --  Name of the element (e.g. field or tag).\n  ##                        Can be omitted for \"time\" and \"name\".\n  ##  data_format       --  Target data-type of the entry. Can be \"int8/16/32/64\", \"uint8/16/32/64\",\n  ##                        \"float32/64\", \"string\".\n  ##                        In case of time, this can be any of \"unix\" (default), \"unix_ms\", \"unix_us\",\n  ##                        \"unix_ns\".\n  ##                        If original field type is different from the target type, the field will be converted\n  ##                        If loss of precision is possible, warning will be logged. \n  ##  string_length     --  Length of the string in bytes. Only used for \"string\" type.\n  ##  string_terminator --  Terminator for strings. Only used for \"string\" type.\n  ##                        Valid values are \"null\", \"0x00\", \"00\", \"0x01\", etc.\n  ##                        If original string length is greater than \"string_length\" the string will \n  ##                        be truncated to have length of the `string + terminator = string_length`.\n  ##                        If original string length is smaller than \"string_length\" the string\n  ##                        will be padded with terminator to have length of \"string_length\". (e.g. \"abcd\\0\\0\\0\\0\\0\")\n  ##                        Defaults to \"null\" for strings.\n  entries = [\n    { read_from = \"field\", name = \"addr_3\", data_format=\"int16\" },\n    { read_from = \"field\", name = \"addr_2\", data_format=\"int16\" },\n    { read_from = \"field\", name = \"addr_4_5\", data_format=\"int32\" },\n    { read_from = \"field\", name = \"addr_6_7\", data_format=\"float32\" },\n    { read_from = \"field\", name = \"addr_16_20\", data_format=\"string\", string_terminator = \"null\", string_length = 11 },\n    { read_from = \"field\", name = \"addr_3_sc\", data_format=\"float64\" }\n  ]\n```\n\n----------------------------------------\n\nTITLE: Configuring RavenDB Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the RavenDB input plugin in Telegraf. Specifies connection details, TLS settings, timeout, and which statistics to collect from RavenDB servers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ravendb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.ravendb]]\n  ## Node URL and port that RavenDB is listening on. By default,\n  ## attempts to connect securely over HTTPS, however, if the user\n  ## is running a local unsecure development cluster users can use\n  ## HTTP via a URL like \"http://localhost:8080\"\n  url = \"https://localhost:4433\"\n\n  ## RavenDB X509 client certificate setup\n  # tls_cert = \"/etc/telegraf/raven.crt\"\n  # tls_key = \"/etc/telegraf/raven.key\"\n\n  ## Optional request timeout\n  ##\n  ## Timeout, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request and\n  ## time limit for requests made by this client\n  # timeout = \"5s\"\n\n  ## List of statistics which are collected\n  # At least one is required\n  # Allowed values: server, databases, indexes, collections\n  #\n  # stats_include = [\"server\", \"databases\", \"indexes\", \"collections\"]\n\n  ## List of db where database stats are collected\n  ## If empty, all db are concerned\n  # db_stats_dbs = []\n\n  ## List of db where index status are collected\n  ## If empty, all indexes from all db are concerned\n  # index_stats_dbs = []\n\n  ## List of db where collection status are collected\n  ## If empty, all collections from all db are concerned\n  # collection_stats_dbs = []\n```\n\n----------------------------------------\n\nTITLE: OpenTSDB Telnet Mode Mock Server in Go\nDESCRIPTION: A Go implementation of a mock server that simulates the OpenTSDB telnet interface by listening on port 4242 and echoing received data to stdout.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opentsdb/README.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n    \"io\"\n    \"log\"\n    \"net\"\n    \"os\"\n)\n\nfunc main() {\n    l, err := net.Listen(\"tcp\", \"localhost:4242\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer l.Close()\n    for {\n        conn, err := l.Accept()\n        if err != nil {\n            log.Fatal(err)\n        }\n        go func(c net.Conn) {\n            defer c.Close()\n            io.Copy(os.Stdout, c)\n        }(conn)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Namepass and Namedrop with Separators in Telegraf Socket Listener\nDESCRIPTION: This configuration shows how to use namepass and namedrop filters with custom separators in the Telegraf socket listener input. It demonstrates filtering metrics based on their hierarchical structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_17\n\nLANGUAGE: toml\nCODE:\n```\n# Pass all metrics of type 'A.C.B' and drop all others like 'A.C.D.B'\n[[inputs.socket_listener]]\n  data_format = \"graphite\"\n  templates = [\"measurement*\"]\n\n  namepass = [\"A.*.B\"]\n  namepass_separator = \".\"\n\n# Drop all metrics of type 'A.C.B' and pass all others like 'A.C.D.B'\n[[inputs.socket_listener]]\n  data_format = \"graphite\"\n  templates = [\"measurement*\"]\n\n  namedrop = [\"A.*.B\"]\n  namedrop_separator = \".\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Example Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the example input plugin in Telegraf. It includes a sample configuration option 'example_option'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/example/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# This is an example plugin\n[[inputs.example]]\n  example_option = \"example_value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth2 Secret-store in Telegraf\nDESCRIPTION: Sample configuration for the OAuth2 secret-store plugin in Telegraf, including options for service selection, token endpoints, client credentials, and token expiry settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/oauth2/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Secret-store to retrieve and maintain tokens from various OAuth2 services\n[[secretstores.oauth2]]\n  ## Unique identifier for the secret-store.\n  ## This id can later be used in plugins to reference the secrets\n  ## in this secret-store via @{<id>:<secret_key>} (mandatory)\n  id = \"secretstore\"\n\n  ## Service to retrieve the token(s) from\n  ## Currently supported services are \"custom\", \"auth0\" and \"AzureAD\"\n  # service = \"custom\"\n\n  ## Setting to overwrite the queried token-endpoint\n  ## This setting is optional for some services but mandatory for others such\n  ## as \"custom\" or \"auth0\". Please check the documentation at\n  ## https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/oauth2/README.md\n  # token_endpoint = \"\"\n\n  ## Tenant ID for the AzureAD service\n  # tenant_id = \"\"\n\n  ## Minimal remaining time until the token expires\n  ## If a token expires less than the set duration in the future, the token is\n  ## renewed. This is useful to avoid race-condition issues where a token is\n  ## still valid, but isn't when the request reaches the API endpoint of\n  ## your service using the token.\n  # token_expiry_margin = \"1s\"\n\n  ## Section for defining a token secret\n  [[secretstores.oauth2.token]]\n    ## Unique secret-key used for referencing the token via @{<id>:<secret_key>}\n    key = \"\"\n    ## Client-ID and secret for the 2-legged OAuth flow\n    client_id = \"\"\n    client_secret = \"\"\n    ## Scopes to send in the request\n    # scopes = []\n\n    ## Additional (optional) parameters to include in the token request\n    ## This might for example include the \"audience\" parameter required for\n    ## auth0.\n    # [secretstores.oauth2.token.parameters]\n    #     audience = \"\"\n```\n\n----------------------------------------\n\nTITLE: Capturing Suricata Network Metrics in InfluxDB Line Protocol Format\nDESCRIPTION: This snippet shows two lines of Suricata network monitoring data in InfluxDB line protocol format. Each line contains numerous metrics related to network traffic, including protocol-specific counters, packet statistics, and application layer information. The data is timestamped and associated with specific threads on the host.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/suricata/README.md#2025-04-16_snippet_4\n\nLANGUAGE: influxdb\nCODE:\n```\nsuricata,host=myhost,thread=W#05-wlp4s0 tcp_reassembly_gap=0,capture_kernel_drops=0,decoder_ltnull_unsupported_type=0,tcp_sessions=0,tcp_stream_depth_reached=0,tcp_pseudo_failed=0,app_layer_flow_failed_tcp=0,app_layer_tx_dns_tcp=0,decoder_null=0,decoder_dce_pkt_too_small=0,decoder_udp=7,tcp_rst=3,app_layer_flow_dns_tcp=0,decoder_invalid=0,defrag_ipv4_reassembled=0,tcp_synack=0,app_layer_flow_ftp=0,decoder_bytes=3117,decoder_pppoe=0,app_layer_flow_dcerpc_tcp=0,app_layer_flow_smb=0,decoder_ipv6_in_ipv6=0,decoder_ipraw_invalid_ip_version=0,app_layer_flow_imap=0,app_layer_tx_dns_udp=2,decoder_ppp=0,decoder_ipv4=21,decoder_tcp=14,flow_memcap=0,tcp_syn=0,tcp_invalid_checksum=0,decoder_teredo=0,decoder_ltnull_pkt_too_small=0,defrag_max_frag_hits=0,app_layer_tx_tls=0,decoder_pkts=24,decoder_sll=0,defrag_ipv6_fragments=0,app_layer_flow_dcerpc_udp=0,app_layer_flow_smtp=0,decoder_icmpv6=3,defrag_ipv6_timeouts=0,decoder_ipv6=3,decoder_raw=0,defrag_ipv6_reassembled=0,tcp_no_flow=0,detect_alert=0,app_layer_flow_tls=0,decoder_ethernet=24,decoder_vlan=0,decoder_icmpv4=0,decoder_ipv4_in_ipv6=0,app_layer_flow_failed_udp=1,decoder_mpls=0,decoder_max_pkt_size=653,decoder_sctp=0,defrag_ipv4_timeouts=0,tcp_ssn_memcap_drop=0,app_layer_flow_dns_udp=1,app_layer_tx_smtp=0,capture_kernel_packets=24,decoder_vlan_qinq=0,decoder_gre=0,app_layer_flow_ssh=0,app_layer_flow_msn=0,defrag_ipv4_fragments=0,app_layer_flow_http=0,tcp_segment_memcap_drop=0,tcp_pseudo=0,app_layer_tx_http=0,decoder_erspan=0,decoder_avg_pkt_size=129 1568368562545009684\nsuricata,host=myhost,thread=W#03-wlp4s0 app_layer_flow_failed_tcp=0,decoder_teredo=0,decoder_ipv6_in_ipv6=0,tcp_pseudo_failed=0,tcp_stream_depth_reached=0,tcp_syn=0,decoder_gre=0,tcp_segment_memcap_drop=0,tcp_ssn_memcap_drop=0,app_layer_tx_smtp=0,decoder_raw=0,decoder_ltnull_pkt_too_small=0,tcp_sessions=0,tcp_reassembly_gap=0,app_layer_flow_ssh=0,app_layer_flow_imap=0,decoder_ipv4=463,decoder_ethernet=463,capture_kernel_packets=463,decoder_pppoe=0,defrag_ipv4_reassembled=0,app_layer_flow_tls=0,app_layer_flow_dcerpc_udp=0,app_layer_flow_dns_udp=0,decoder_vlan=0,decoder_ipraw_invalid_ip_version=0,decoder_mpls=0,tcp_no_flow=0,decoder_avg_pkt_size=445,decoder_udp=432,flow_memcap=0,app_layer_tx_dns_udp=0,app_layer_flow_msn=0,app_layer_flow_http=0,app_layer_flow_dcerpc_tcp=0,decoder_ipv6=0,decoder_ipv4_in_ipv6=0,defrag_ipv4_timeouts=0,defrag_ipv4_fragments=0,defrag_ipv6_timeouts=0,decoder_sctp=0,defrag_ipv6_fragments=0,app_layer_flow_dns_tcp=0,app_layer_tx_tls=0,defrag_max_frag_hits=0,decoder_bytes=206345,decoder_vlan_qinq=0,decoder_invalid=0,decoder_ppp=0,tcp_rst=0,detect_alert=0,capture_kernel_drops=0,app_layer_flow_failed_udp=4,decoder_null=0,decoder_icmpv4=0,decoder_icmpv6=0,decoder_ltnull_unsupported_type=0,defrag_ipv6_reassembled=0,tcp_invalid_checksum=0,tcp_synack=0,decoder_tcp=31,tcp_pseudo=0,app_layer_flow_smb=0,app_layer_flow_smtp=0,decoder_max_pkt_size=1463,decoder_dce_pkt_too_small=0,app_layer_tx_http=0,decoder_pkts=463,decoder_sll=0,app_layer_flow_ftp=0,app_layer_tx_dns_tcp=0,decoder_erspan=0 1568368562544966078\n```\n\n----------------------------------------\n\nTITLE: Configuring File Input Plugin in Telegraf using TOML\nDESCRIPTION: Sample TOML configuration for the Telegraf File Input Plugin. This configuration specifies which files to parse, character encoding options, data format settings, and optional tagging parameters for file names and paths.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/file/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Parse a complete file each interval\n[[inputs.file]]\n  ## Files to parse each interval.  Accept standard unix glob matching rules,\n  ## as well as ** to match recursive files and directories.\n  files = [\"/tmp/metrics.out\"]\n\n  ## Character encoding to use when interpreting the file contents.  Invalid\n  ## characters are replaced using the unicode replacement character.  When set\n  ## to the empty string the data is not decoded to text.\n  ##   ex: character_encoding = \"utf-8\"\n  ##       character_encoding = \"utf-16le\"\n  ##       character_encoding = \"utf-16be\"\n  ##       character_encoding = \"\"\n  # character_encoding = \"\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Please use caution when using the following options: when file name\n  ## variation is high, this can increase the cardinality significantly. Read\n  ## more about cardinality here:\n  ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n\n  ## Name of tag to store the name of the file. Disabled if not set.\n  # file_tag = \"\"\n\n  ## Name of tag to store the absolute path and name of the file. Disabled if\n  ## not set.\n  # file_path_tag = \"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Metric Timestamp from Tag in Telegraf\nDESCRIPTION: Example showing how to use a tag value ('time') as the metric timestamp using Unix format, with before and after comparison.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/converter/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.converter]]\n  [processors.converter.tags]\n    timestamp = [\"time\"]\n    timestamp_format = \"unix\n```\n\n----------------------------------------\n\nTITLE: Configuring Zabbix Output Plugin in TOML\nDESCRIPTION: Sample configuration for the Zabbix output plugin showing all available options including address configuration, key prefixing, host tagging, and auto-registration settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics to Zabbix\n[[outputs.zabbix]]\n  ## Address and (optional) port of the Zabbix server\n  address = \"zabbix.example.com:10051\"\n\n  ## Send metrics as type \"Zabbix agent (active)\"\n  # agent_active = false\n\n  ## Add prefix to all keys sent to Zabbix.\n  # key_prefix = \"telegraf.\"\n\n  ## Name of the tag that contains the host name. Used to set the host in Zabbix.\n  ## If the tag is not found, use the hostname of the system running Telegraf.\n  # host_tag = \"host\"\n\n  ## Skip measurement prefix to all keys sent to Zabbix.\n  # skip_measurement_prefix = false\n\n  ## This field will be sent as HostMetadata to Zabbix Server to autoregister the host.\n  ## To enable this feature, this option must be set to a value other than \"\".\n  # autoregister = \"\"\n\n  ## Interval to resend auto-registration data to Zabbix.\n  ## Only applies if autoregister feature is enabled.\n  ## This value is a lower limit, the actual resend should be triggered by the next flush interval.\n  # autoregister_resend_interval = \"30m\"\n\n  ## Interval to send LLD data to Zabbix.\n  ## This value is a lower limit, the actual resend should be triggered by the next flush interval.\n  # lld_send_interval = \"10m\"\n\n  ## Interval to delete stored LLD known data and start capturing it again.\n  ## This value is a lower limit, the actual resend should be triggered by the next flush interval.\n  # lld_clear_interval = \"1h\"\n```\n\n----------------------------------------\n\nTITLE: Generating Complete Telegraf Configuration\nDESCRIPTION: Demonstrates how to generate a sample configuration file with default values for all available plugins and redirect it to a file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/cmd/telegraf/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf config > telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: Setting Telegraf Binary Capabilities for All Metrics\nDESCRIPTION: Shell commands to set the required capabilities on the Telegraf binary for collecting all metrics, with and without perf-related metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n#without perf-related metrics:\nsudo setcap cap_sys_rawio,cap_dac_read_search+ep <path_to_telegraf_binary>\n\n#with perf-related metrics:\nsudo setcap cap_sys_rawio,cap_dac_read_search,cap_sys_admin+ep <path_to_telegraf_binary>\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Parquet Output Plugin for Telegraf. It specifies options for the output directory, file rotation interval, and timestamp field name.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/parquet/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin that writes metrics to parquet files\n[[outputs.parquet]]\n  ## Directory to write parquet files in. If a file already exists the output\n  ## will attempt to continue using the existing file.\n  # directory = \".\"\n\n  ## Files are rotated after the time interval specified. When set to 0 no time\n  ## based rotation is performed.\n  # rotation_interval = \"0h\"\n\n  ## Timestamp field name\n  ## Field name to use to store the timestamp. If set to an empty string, then\n  ## the timestamp is omitted.\n  # timestamp_field_name = \"timestamp\"\n```\n\n----------------------------------------\n\nTITLE: Granting PostgreSQL Permissions for Metrics Collection\nDESCRIPTION: SQL command to grant the necessary permissions for the plugin to access required PostgreSQL views for metrics collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT pg_read_all_stats TO user;\n```\n\n----------------------------------------\n\nTITLE: Querying JSON Attributes Directly in Azure Data Explorer\nDESCRIPTION: Direct method to query JSON data in raw format without parsing. Simple but may impact performance with large data volumes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/azure_data_explorer/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nTablename\n| where name == \"sqlserver_azure_db_resource_stats\" and todouble(fields.avg_cpu_percent) > 7\n```\n\nLANGUAGE: text\nCODE:\n```\nTablename\n| distinct tostring(tags.database_name)\n```\n\n----------------------------------------\n\nTITLE: Configuring Rename Processor in Telegraf using TOML\nDESCRIPTION: Sample configuration for the 'rename' processor plugin in Telegraf. It demonstrates how to set up renaming operations for measurements, tags, and fields using sub-tables for each rename operation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/rename/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Rename measurements, tags, and fields that pass through this filter.\n[[processors.rename]]\n  ## Specify one sub-table per rename operation.\n  [[processors.rename.replace]]\n    measurement = \"network_interface_throughput\"\n    dest = \"throughput\"\n\n  [[processors.rename.replace]]\n    tag = \"hostname\"\n    dest = \"host\"\n\n  [[processors.rename.replace]]\n    field = \"lower\"\n    dest = \"min\"\n\n  [[processors.rename.replace]]\n    field = \"upper\"\n    dest = \"max\"\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch Indices Stats Metrics in Telegraf\nDESCRIPTION: Defines various metrics related to Elasticsearch indices, capturing performance details such as indexing rates, document counts, and flush operations. This monitoring is crucial for maintaining indexing efficiency and query performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n  - elasticsearch_indices_stats_(primaries|total)\n  - tags:\n    - index_name\n  - fields:\n    - completion_size_in_bytes (float)\n    - docs_count (float)\n    - docs_deleted (float)\n    - fielddata_evictions (float)\n    - fielddata_memory_size_in_bytes (float)\n    - flush_periodic (float)\n    - flush_total (float)\n    - flush_total_time_in_millis (float)\n    - get_current (float)\n    - get_exists_time_in_millis (float)\n    - get_exists_total (float)\n    - get_missing_time_in_millis (float)\n    - get_missing_total (float)\n    - get_time_in_millis (float)\n    - get_total (float)\n    - indexing_delete_current (float)\n    - indexing_delete_time_in_millis (float)\n    - indexing_delete_total (float)\n    - indexing_index_current (float)\n    - indexing_index_failed (float)\n    - indexing_index_time_in_millis (float)\n    - indexing_index_total (float)\n    - indexing_is_throttled (float)\n    - indexing_noop_update_total (float)\n    - indexing_throttle_time_in_millis (float)\n    - merges_current (float)\n    - merges_current_docs (float)\n    - merges_current_size_in_bytes (float)\n    - merges_total (float)\n    - merges_total_auto_throttle_in_bytes (float)\n    - merges_total_docs (float)\n    - merges_total_size_in_bytes (float)\n    - merges_total_stopped_time_in_millis (float)\n    - merges_total_throttled_time_in_millis (float)\n```\n\n----------------------------------------\n\nTITLE: Configuring sudo for Telegraf PF Plugin\nDESCRIPTION: This snippet shows how to configure sudo to allow Telegraf to run pfctl without a password. This is necessary if you want to run the PF plugin with elevated privileges.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pf/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sudo\nCODE:\n```\ntelegraf ALL=(root) NOPASSWD: /sbin/pfctl -s info\n```\n\n----------------------------------------\n\nTITLE: Configuring New Relic Output Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the New Relic Output Plugin for Telegraf. It includes settings for the Insights API key, metric prefix, timeout, HTTP proxy, and metric URL. Most options are commented out and can be uncommented and customized as needed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/newrelic/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics to New Relic metrics endpoint\n[[outputs.newrelic]]\n  ## The 'insights_key' parameter requires a NR license key.\n  ## New Relic recommends you create one\n  ## with a convenient name such as TELEGRAF_INSERT_KEY.\n  ## reference: https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key\n  # insights_key = \"New Relic License Key Here\"\n\n  ## Prefix to add to add to metric name for easy identification.\n  ## This is very useful if your metric names are ambiguous.\n  # metric_prefix = \"\"\n\n  ## Timeout for writes to the New Relic API.\n  # timeout = \"15s\"\n\n  ## HTTP Proxy override. If unset use values from the standard\n  ## proxy environment variables to determine proxy, if any.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Metric URL override to enable geographic location endpoints.\n  # If not set use values from the standard\n  # metric_url = \"https://metric-api.newrelic.com/metric/v1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Procstat Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the Procstat input plugin in Telegraf. It includes various options for selecting processes to monitor, setting field name prefixes, choosing CPU usage calculation mode, and specifying properties to collect.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/procstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Monitor process cpu and memory usage\n[[inputs.procstat]]\n  ## PID file to monitor process\n  pid_file = \"/var/run/nginx.pid\"\n  ## executable name (ie, pgrep <exe>)\n  # exe = \"nginx\"\n  ## pattern as argument for pgrep (ie, pgrep -f <pattern>)\n  # pattern = \"nginx\"\n  ## user as argument for pgrep (ie, pgrep -u <user>)\n  # user = \"nginx\"\n  ## Systemd unit name, supports globs when include_systemd_children is set to true\n  # systemd_unit = \"nginx.service\"\n  # include_systemd_children = false\n  ## CGroup name or path, supports globs\n  # cgroup = \"systemd/system.slice/nginx.service\"\n  ## Supervisor service names of hypervisorctl management\n  # supervisor_units = [\"webserver\", \"proxy\"]\n\n  ## Windows service name\n  # win_service = \"\"\n\n  ## override for process_name\n  ## This is optional; default is sourced from /proc/<pid>/status\n  # process_name = \"bar\"\n\n  ## Field name prefix\n  # prefix = \"\"\n\n  ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.\n  # mode = \"irix\"\n\n  ## Add the given information tag instead of a field\n  ## This allows to create unique metrics/series when collecting processes with\n  ## otherwise identical tags. However, please be careful as this can easily\n  ## result in a large number of series, especially with short-lived processes,\n  ## creating high cardinality at the output.\n  ## Available options are:\n  ##   cmdline   -- full commandline\n  ##   pid       -- ID of the process\n  ##   ppid      -- ID of the process' parent\n  ##   status    -- state of the process\n  ##   user      -- username owning the process\n  ## socket only options:\n  ##   protocol  -- protocol type of the process socket\n  ##   state     -- state of the process socket\n  ##   src       -- source address of the process socket (non-unix sockets)\n  ##   src_port  -- source port of the process socket (non-unix sockets)\n  ##   dest      -- destination address of the process socket (non-unix sockets)\n  ##   dest_port -- destination port of the process socket (non-unix sockets)\n  ##   name      -- name of the process socket (unix sockets only)\n  ## Available for procstat_lookup:\n  ##   level     -- level of the process filtering\n  # tag_with = []\n\n  ## Properties to collect\n  ## Available options are\n  ##   cpu     -- CPU usage statistics\n  ##   limits  -- set resource limits\n  ##   memory  -- memory usage statistics\n  ##   mmap    -- mapped memory usage statistics (caution: can cause high load)\n  ##   sockets -- socket statistics for protocols in 'socket_protocols'\n  # properties = [\"cpu\", \"limits\", \"memory\", \"mmap\"]\n\n  ## Protocol filter for the sockets property\n  ## Available options are\n  ##   all  -- all of the protocols below\n  ##   tcp4 -- TCP socket statistics for IPv4\n  ##   tcp6 -- TCP socket statistics for IPv6\n  ##   udp4 -- UDP socket statistics for IPv4\n  ##   udp6 -- UDP socket statistics for IPv6\n  ##   unix -- Unix socket statistics\n  # socket_protocols = [\"all\"]\n\n  ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n  ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n  ## the native finder performs the search directly in a manor dependent on the\n  ## platform.  Default is 'pgrep'\n  # pid_finder = \"pgrep\"\n\n  ## New-style filtering configuration (multiple filter sections are allowed)\n  # [[inputs.procstat.filter]]\n  #    ## Name of the filter added as 'filter' tag\n  #    name = \"shell\"\n  #\n  #    ## Service filters, only one is allowed\n  #    ## Systemd unit names (wildcards are supported)\n  #    # systemd_units = []\n  #    ## CGroup name or path (wildcards are supported)\n  #    # cgroups = []\n  #    ## Supervisor service names of hypervisorctl management\n  #    # supervisor_units = []\n  #    ## Windows service names\n  #    # win_service = []\n  #\n  #    ## Process filters, multiple are allowed\n  #    ## Regular expressions to use for matching against the full command\n  #    # patterns = ['.*']\n  #    ## List of users owning the process (wildcards are supported)\n  #    # users = ['*']\n  #    ## List of executable paths of the process (wildcards are supported)\n  #    # executables = ['*']\n  #    ## List of process names (wildcards are supported)\n  #    # process_names = ['*']\n  #    ## Recursion depth for determining children of the matched processes\n  #    ## A negative value means all children with infinite depth\n  #    # recursion_depth = 0\n```\n\n----------------------------------------\n\nTITLE: Configuring LDAP Monitoring Plugin in Telegraf\nDESCRIPTION: This snippet is a configuration file written in TOML for setting up the Telegraf LDAP Input Plugin. It defines server connection details, including options for secure connections and authentication methods. To correctly use this snippet, the monitoring feature must be enabled on the LDAP server. The snippet supports monitoring both OpenLDAP and 389ds servers and includes optional TLS configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ldap/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n# LDAP monitoring plugin\n[[inputs.ldap]]\n  ## Server to monitor\n  ## The scheme determines the mode to use for connection with\n  ##    ldap://...      -- unencrypted (non-TLS) connection\n  ##    ldaps://...     -- TLS connection\n  ##    starttls://...  --  StartTLS connection\n  ## If no port is given, the default ports, 389 for ldap and starttls and\n  ## 636 for ldaps, are used.\n  server = \"ldap://localhost\"\n\n  ## Server dialect, can be \"openldap\" or \"389ds\"\n  # dialect = \"openldap\"\n\n  # DN and password to bind with\n  ## If bind_dn is empty an anonymous bind is performed.\n  bind_dn = \"\"\n  bind_password = \"\"\n\n  ## Reverse the field names constructed from the monitoring DN\n  # reverse_field_names = false\n\n  ## Optional TLS Config\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable =\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Password for the key file if it is encrypted\n  # tls_key_pwd = \"\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Minimal TLS version to accept by the client\n  # tls_min_version = \"TLS12\"\n  ## List of ciphers to accept, by default all secure ciphers will be accepted\n  ## See https://pkg.go.dev/crypto/tls#pkg-constants for supported values.\n  ## Use \"all\", \"secure\" and \"insecure\" to add all support ciphers, secure\n  ## suites or insecure suites respectively.\n  # tls_cipher_suites = [\"secure\"]\n  ## Renegotiation method, \"never\", \"once\" or \"freely\"\n  # tls_renegotiation_method = \"never\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Tacacs Input Plugin in Telegraf with TOML\nDESCRIPTION: Sample configuration for the Tacacs input plugin that collects successful tacacs authentication response times. It specifies server connection parameters, authentication credentials, and timeout settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tacacs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Tacacs plugin collects successful tacacs authentication response times.\n[[inputs.tacacs]]\n  ## An array of Server IPs (or hostnames) and ports to gather from. If none specified, defaults to localhost.\n  # servers = [\"127.0.0.1:49\"]\n\n  ## Request source server IP, normally the server running telegraf.\n  # request_ip = \"127.0.0.1\"\n\n  ## Credentials for tacacs authentication.\n  username = \"myuser\"\n  password = \"mypassword\"\n  secret = \"mysecret\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Schema Update Configuration for SQL Output Plugin\nDESCRIPTION: TOML configuration snippet showing how to enable automatic schema updates when new fields or tags are added to metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.sql]]\n  ## Table update template\n  table_update_template = \"ALTER TABLE {TABLE} ADD COLUMN {COLUMN}\"\n```\n\n----------------------------------------\n\nTITLE: Basic String Processing Example Configuration\nDESCRIPTION: Example configuration showing how to apply lowercase, trim_prefix, and uppercase functions to different metric components. This demonstrates processing both tags and fields with option to store results in a new destination.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/strings/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.strings]]\n  [[processors.strings.lowercase]]\n    tag = \"uri_stem\"\n\n  [[processors.strings.trim_prefix]]\n    tag = \"uri_stem\"\n    prefix = \"/api/\"\n\n  [[processors.strings.uppercase]]\n    field = \"cs-host\"\n    dest = \"cs-host_normalised\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring the Defaults Processor Plugin in Telegraf using TOML\nDESCRIPTION: This configuration example shows how to set up the Defaults processor to ensure certain fields and tags always exist with specified default values on metrics. It demonstrates setting default string, numeric, and boolean field values as well as default tag values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/defaults/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n## Set default fields and tags on your metric(s) when they are nil or empty\n[[processors.defaults]]\n  ## Ensures a set of fields or tags always exists on your metric(s) with their\n  ## respective default value.\n  ## For any given field/tag pair (key = default), if it's not set, a field/tag\n  ## is set on the metric with the specified default.\n  ##\n  ## A field is considered not set if it is nil on the incoming metric;\n  ## or it is not nil but its value is an empty string or is a string\n  ## of one or more spaces.\n  ##   <target-field> = <value>\n  [processors.defaults.fields]\n    field_1 = \"bar\"\n    time_idle = 0\n    is_error = true\n  ## A tag is considered not set if it is nil on the incoming metric;\n  ## or it is not nil but it is empty string or a string of one or\n  ## more spaces.\n  ## <target-tag> = <value>\n  [processors.defaults.tags]\n    tag_1 = \"foo\"\n```\n\n----------------------------------------\n\nTITLE: Example Exec Plugin Configuration\nDESCRIPTION: Configuration example showing how to use the Exec plugin with a shell script, including timeout setting and data format specification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/exec/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.exec]]\n  commands = [\"sh /tmp/test.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Chrony Input Plugin in Telegraf (TOML)\nDESCRIPTION: Sample configuration for the chrony input plugin in Telegraf. Includes options for server address, timeout, DNS lookup settings, metric selection, and unix socket permissions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/chrony/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Get standard chrony metrics.\n[[inputs.chrony]]\n  ## Server address of chronyd with address scheme\n  ## If empty or not set, the plugin will mimic the behavior of chronyc and\n  ## check \"unixgram:///run/chrony/chronyd.sock\", \"udp://127.0.0.1:323\"\n  ## and \"udp://[::1]:323\".\n  # server = \"\"\n\n  ## Timeout for establishing the connection\n  # timeout = \"5s\"\n\n  ## Try to resolve received addresses to host-names via DNS lookups\n  ## Disabled by default to avoid DNS queries especially for slow DNS servers.\n  # dns_lookup = false\n\n  ## Metrics to query named according to chronyc commands\n  ## Available settings are:\n  ##   activity    -- number of peers online or offline\n  ##   tracking    -- information about system's clock performance\n  ##   serverstats -- chronyd server statistics\n  ##   sources     -- extended information about peers\n  ##   sourcestats -- statistics on peers\n  # metrics = [\"tracking\"]\n\n  ## Socket group & permissions\n  ## If the user requests collecting metrics via unix socket, then it is created\n  ## with the following group and permissions.\n  # socket_group = \"chrony\"\n  # socket_perms = \"0660\"\n```\n\n----------------------------------------\n\nTITLE: Example Teamspeak Output in Telegraf\nDESCRIPTION: Sample output showing the metrics collected by the Teamspeak 3 input plugin, formatted in Telegraf's line protocol. Demonstrates various statistics including uptime, client counts, network metrics, and ping data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/teamspeak/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nteamspeak,virtual_server=1,name=LeopoldsServer,host=vm01 bytes_received_total=29638202639i,uptime=13567846i,total_ping=26.89,total_packet_loss=0,packets_sent_total=415821252i,packets_received_total=237069900i,bytes_sent_total=55309568252i,clients_online=11i,query_clients_online=1i 1507406561000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring CrateDB Output Plugin\nDESCRIPTION: TOML configuration for setting up the CrateDB output plugin in Telegraf. Includes connection URL, timeout settings, table name, and auto-creation options. The configuration supports customization of database connection parameters and table management settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/cratedb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.cratedb]]\n  ## Connection parameters for accessing the database see\n  ##   https://pkg.go.dev/github.com/jackc/pgx/v4#ParseConfig\n  ## for available options\n  url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n\n  ## Timeout for all CrateDB queries.\n  # timeout = \"5s\"\n\n  ## Name of the table to store metrics in.\n  # table = \"metrics\"\n\n  ## If true, and the metrics table does not exist, create it automatically.\n  # table_create = false\n\n  ## The character(s) to replace any '.' in an object key with\n  # key_separator = \"_\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Services Input Plugin - TOML\nDESCRIPTION: This TOML snippet configures the Windows services input plugin for Telegraf. It specifies which services to monitor and includes options for excluding specific services. The configuration supports wildcard matching for service names.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_services/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Input plugin to report Windows services info.\n# This plugin ONLY supports Windows\n[[inputs.win_services]]\n  ## Names of the services to monitor. Leave empty to monitor all the available\n  ## services on the host. Globs accepted. Case insensitive.\n  service_names = [\n    \"LanmanServer\",\n    \"TermService\",\n    \"Win*\",\n  ]\n\n  # optional, list of service names to exclude\n  excluded_service_names = ['WinRM']\n```\n\n----------------------------------------\n\nTITLE: Configuring Minecraft Input Plugin for Telegraf (TOML)\nDESCRIPTION: This TOML configuration snippet sets up the Minecraft input plugin to collect scores using RCON. Users must specify the RCON password and optionally the server address and port.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/minecraft/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[inputs.minecraft]]\\n  ## Address of the Minecraft server.\\n  # server = \\\"localhost\\\"\\n\\n  ## Server RCON Port.\\n  # port = \\\"25575\\\"\\n\\n  ## Server RCON Password.\\n  password = \\\"\\\"\\n\\n  ## Uncomment to remove deprecated metric components.\\n  # tagdrop = [\\\"server\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Basic Jolokia2 Agent Configuration\nDESCRIPTION: Basic configuration example showing how to set up the Jolokia2 agent plugin with URLs and metric collection for Java runtime information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jolokia2_agent/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.jolokia2_agent]]\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  # Add agents URLs to query\n  urls = [\"http://localhost:8080/jolokia\"]\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional origin URL to include as a header in the request. Some endpoints\n  ## may reject an empty origin.\n  # origin = \"\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add metrics to read\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the InfluxDB input plugin for Telegraf. It specifies the URLs to collect metrics from, optional authentication, TLS settings, and request timeout. The default URL is set to the local InfluxDB debug endpoint.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.influxdb]]\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n\n  ## Username and password to send using HTTP Basic Authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## http request & header timeout\n  timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring LANZ Input Plugin in TOML\nDESCRIPTION: TOML configuration example for setting up the Arista LANZ input plugin. Specifies the TCP endpoints for LANZ data collection from switches.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lanz/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.lanz]]\n  ## URL to Arista LANZ endpoint\n  servers = [\n    \"tcp://switch1.int.example.com:50001\",\n    \"tcp://switch2.int.example.com:50001\",\n  ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Slab Input Plugin in TOML\nDESCRIPTION: Basic TOML configuration for the Slab input plugin in Telegraf. No specific configuration parameters are required, but sudo access must be properly configured.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/slab/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Get slab statistics from procfs\n# This plugin ONLY supports Linux\n[[inputs.slab]]\n  # no configuration - please see the plugin's README for steps to configure\n  # sudo properly\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf File Input with OpenTSDB Parser\nDESCRIPTION: Configuration example showing how to set up Telegraf's file input plugin to parse OpenTSDB format data. The configuration specifies the input files and sets the data format to OpenTSDB.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/opentsdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"opentsdb\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Clarify Output Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Clarify output plugin in Telegraf. It includes options for setting credentials, timeout, and optional tags for signal ID generation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/clarify/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n## Configuration to publish Telegraf metrics to Clarify\n[[outputs.clarify]]\n  ## Credentials File (Oauth 2.0 from Clarify integration)\n  credentials_file = \"/path/to/clarify/credentials.json\"\n\n  ## Clarify username password (Basic Auth from Clarify integration)\n  username = \"i-am-bob\"\n  password = \"secret-password\"\n\n  ## Timeout for Clarify operations\n  # timeout = \"20s\"\n\n  ## Optional tags to be included when generating the unique ID for a signal in Clarify\n  # id_tags = []\n  # clarify_id_tag = 'clarify_input_id'\n```\n\n----------------------------------------\n\nTITLE: Sample Output Format for NSQ Metrics in Telegraf\nDESCRIPTION: Example output showing the format of metrics collected from NSQ by Telegraf. It demonstrates the structure of server, topic, channel, and client metrics with their associated tags and field values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsq/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnsq_server,server_host=127.0.0.1:35871,server_version=0.3.6 server_count=1i,topic_count=2i 1742836824386224245\nnsq_topic,server_host=127.0.0.1:35871,server_version=0.3.6,topic=t1 backend_depth=13i,channel_count=1i,depth=12i,message_count=14i 1742836824386235365\nnsq_channel,channel=c1,server_host=127.0.0.1:35871,server_version=0.3.6,topic=t1 backend_depth=1i,client_count=1i,deferred_count=3i,depth=0i,inflight_count=2i,message_count=4i,requeue_count=5i,timeout_count=6i 1742836824386241985\nnsq_client,channel=c1,client_address=172.17.0.11:35560,client_deflate=false,client_hostname=373a715cd990,client_id=373a715cd990,client_name=373a715cd990,client_snappy=false,client_tls=false,client_user_agent=nsq_to_nsq/0.3.6\\ go-nsq/1.0.5,client_version=V2,server_host=127.0.0.1:35871,server_version=0.3.6,topic=t1 finish_count=9i,inflight_count=7i,message_count=8i,ready_count=200i,requeue_count=10i 1742836824386252905\nnsq_topic,server_host=127.0.0.1:35871,server_version=0.3.6,topic=t2 backend_depth=29i,channel_count=1i,depth=28i,message_count=30i 1742836824386263806\nnsq_channel,channel=c2,server_host=127.0.0.1:35871,server_version=0.3.6,topic=t2 backend_depth=16i,client_count=1i,deferred_count=18i,depth=15i,inflight_count=17i,message_count=19i,requeue_count=20i,timeout_count=21i 1742836824386270026\nnsq_client,channel=c2,client_address=172.17.0.8:48145,client_deflate=true,client_hostname=377569bd462b,client_id=377569bd462b,client_name=377569bd462b,client_snappy=true,client_tls=true,client_user_agent=go-nsq/1.0.5,client_version=V2,server_host=127.0.0.1:35871,server_version=0.3.6,topic=t2 finish_count=25i,inflight_count=23i,message_count=24i,ready_count=22i,requeue_count=26i 1742836824386277926\n```\n\n----------------------------------------\n\nTITLE: Configuring Riemann Listener Input Plugin in TOML\nDESCRIPTION: Sample configuration for the Riemann Listener input plugin in Telegraf. It includes options for service address, connection limits, timeouts, TLS configuration, buffer size, and keep-alive settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/riemann_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Riemann protobuff listener\n[[inputs.riemann_listener]]\n  ## URL to listen on\n  ## Default is \"tcp://:5555\"\n  #  service_address = \"tcp://:8094\"\n  #  service_address = \"tcp://127.0.0.1:http\"\n  #  service_address = \"tcp4://:8094\"\n  #  service_address = \"tcp6://:8094\"\n  #  service_address = \"tcp6://[2001:db8::1]:8094\"\n\n  ## Maximum number of concurrent connections.\n  ## 0 (default) is unlimited.\n  #  max_connections = 1024\n  ## Read timeout.\n  ## 0 (default) is unlimited.\n  #  read_timeout = \"30s\"\n  ## Optional TLS configuration.\n  #  tls_cert = \"/etc/telegraf/cert.pem\"\n  #  tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  #  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n  ## Maximum socket buffer size (in bytes when no unit specified).\n  #  read_buffer_size = \"64KiB\"\n  ## Period between keep alive probes.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  #  keep_alive_period = \"5m\"\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC Permissions in Kubernetes\nDESCRIPTION: YAML configuration for setting up necessary RBAC permissions in Kubernetes. Defines ClusterRole and aggregated roles for accessing Kubernetes resources.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: influx:cluster:viewer\n  labels:\n    rbac.authorization.k8s.io/aggregate-view-telegraf: \"true\"\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumes\", \"nodes\"]\n    verbs: [\"get\", \"list\"]\n\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: influx:telegraf\naggregationRule:\n  clusterRoleSelectors:\n    - matchLabels:\n        rbac.authorization.k8s.io/aggregate-view-telegraf: \"true\"\n    - matchLabels:\n        rbac.authorization.k8s.io/aggregate-to-view: \"true\"\nrules: []\n```\n\n----------------------------------------\n\nTITLE: Configuring Bond Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Bond input plugin showing available options including host_proc and host_sys directory paths, bond_interfaces filtering, and collection of additional bond details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bond/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collect bond interface status, slaves statuses and failures count\n[[inputs.bond]]\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n  ## Sets 'sys' directory path\n  ## If not specified, then default is /sys\n  # host_sys = \"/sys\"\n\n  ## By default, telegraf gather stats for all bond interfaces\n  ## Setting interfaces will restrict the stats to the specified\n  ## bond interfaces.\n  # bond_interfaces = [\"bond0\"]\n\n  ## Tries to collect additional bond details from /sys/class/net/{bond}\n  ## currently only useful for LACP (mode 4) bonds\n  # collect_sys_details = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple InfluxDB Endpoints in TOML\nDESCRIPTION: This TOML configuration demonstrates how to set up the InfluxDB input plugin to collect metrics from multiple InfluxDB-formatted endpoints. It specifies two different IP addresses to gather data from.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.influxdb]]\n  urls = [\n    \"http://127.0.0.1:8086/debug/vars\",\n    \"http://192.168.2.1:8086/debug/vars\"\n  ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Value Counter Aggregator in Telegraf\nDESCRIPTION: Basic configuration for the valuecounter aggregator plugin, specifying the period and fields to count. Includes settings for dropping original metrics and field selection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/valuecounter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[aggregators.valuecounter]]\n  ## General Aggregator Arguments:\n  ## The period on which to flush & clear the aggregator.\n  # period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  # drop_original = false\n\n  ## The fields for which the values will be counted\n  fields = [\"status\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Instrumental Output Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Instrumental output plugin in Telegraf. It includes settings for API token, metric prefix, output template, connection timeout, and debug mode.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/instrumental/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for sending metrics to an Instrumental project\n[[outputs.instrumental]]\n  ## Project API Token (required)\n  api_token = \"API Token\"  # required\n  ## Prefix the metrics with a given name\n  prefix = \"\"\n  ## Stats output template (Graphite formatting)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  template = \"host.tags.measurement.field\"\n  ## Timeout in seconds to connect\n  timeout = \"2s\"\n  ## Debug true - Print communication to Instrumental\n  debug = false\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamp to Unix Nanoseconds in Telegraf\nDESCRIPTION: Configuration example showing how to convert a timestamp field from ISO format to unix nanosecond timestamp. This preserves the full nanosecond precision of the original timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/timestamp/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.timestamp]]\n  source_timestamp_field = \"timestamp\"\n  source_timestamp_format = \"2006-01-02T15:04:05.999999999Z\"\n  destination_timestamp_format = \"unix_ns\"\n```\n\nLANGUAGE: diff\nCODE:\n```\n- metric value=42i,timestamp=\"2024-03-04T10:10:32.123456789Z\" 1560540094000000000\n+ metric value=42i,timestamp=1709547032123456789 1560540094000000000\n```\n\n----------------------------------------\n\nTITLE: Referencing Docker Secrets in Telegraf Plugin Configuration\nDESCRIPTION: TOML example demonstrating how to reference a Docker secret within a Telegraf plugin configuration using the secret store syntax\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/docker/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.<some_plugin>]]\n  password = \"@{docker_secretstore:secret_for_plugin}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Grok Parser - TOML\nDESCRIPTION: Example configuration for setting up the Grok parser with file input in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/grok/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"/var/log/apache/access.log\"]\n  data_format = \"grok\"\n  grok_patterns = [\"%{COMBINED_LOG_FORMAT}\"]\n  grok_custom_pattern_files = []\n  grok_custom_patterns = '''\n  '''\n  grok_timezone = \"Canada/Eastern\"\n  # grok_unique_timestamp = \"auto\"\n  # grok_multiline = false\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query for Average Response Time\nDESCRIPTION: This TOML configuration snippet sets up an OpenSearch query to calculate the average response time per URI and response status code. It uses the 'avg' metric function and includes missing tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"my-index-*\"\n  filter_query = \"*\"\n  metric_fields = [\"response_time\"]\n  metric_function = \"avg\"\n  tags = [\"URI.keyword\", \"response.keyword\"]\n  include_missing_tag = true\n  missing_tag_value = \"null\"\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf with Config Directory\nDESCRIPTION: Command to install Telegraf as a Windows Service with both a main config file and a config directory.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" --config C:\\\"Program Files\"\\Telegraf\\telegraf.conf --config-directory C:\\\"Program Files\"\\Telegraf\\telegraf.d service install\n```\n\n----------------------------------------\n\nTITLE: AWS CloudWatch List Metrics Commands\nDESCRIPTION: AWS CLI commands to list available metrics and dimensions for EC2 namespace in a specific region.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\naws cloudwatch list-metrics --namespace AWS/EC2 --region us-east-1\naws cloudwatch list-metrics --namespace AWS/EC2 --region us-east-1 --metric-name CPUCreditBalance\n```\n\n----------------------------------------\n\nTITLE: Example Dirname Configuration in Telegraf\nDESCRIPTION: Configuration example showing how to extract the directory portion from a path field and store it in a new field called 'folder'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n  [[processors.filepath.dirname]]\n    field = \"path\"\n    dest = \"folder\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Influx Line Protocol Parser in Telegraf\nDESCRIPTION: Configuration example for the Influx Line Protocol parser showing how to set up file input with the influx data format. Includes options for parser type selection (internal/upstream) and timestamp precision configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/influx/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Influx line protocol parser\n  ## 'internal' is the default. 'upstream' is a newer parser that is faster\n  ## and more memory efficient.\n  # influx_parser_type = \"internal\"\n\n  ## Influx line protocol timestamp precision\n  ## Time duration to specify the precision of the data's timestamp to parse.\n  ## The default assumes nanosecond (1ns) precision, but users can set to\n  ## second (1s), millisecond (1ms), or microsecond (1us) precision as well.\n  # influx_timestamp_precision = \"1ns\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Split Processor Plugin with TOML in Telegraf\nDESCRIPTION: This snippet demonstrates how to configure the Split Processor Plugin in Telegraf using TOML. It provides options to keep or discard the original metric and defines templates specifying metric names, tags, and fields. Dependencies: Telegraf with the split processor plugin must be installed. Required parameters include template definitions with at least a name, tags, or fields specified. The expected output is a set of split metrics based on the defined templates.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/split/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Split a metric into one or more metrics with the specified field(s)/tag(s)\n[[processors.split]]\n  ## Keeps the original metric by default\n  # drop_original = false\n\n  ## Template for an output metric\n  ## Users can define multiple templates to split the original metric into\n  ## multiple, potentially overlapping, metrics.\n  [[processors.split.template]]\n    ## New metric name\n    name = \"\"\n\n    ## List of tag keys for this metric template, accepts globs, e.g. \"*\"\n    tags = []\n\n    ## List of field keys for this metric template, accepts globs, e.g. \"*\"\n    fields = []\n```\n\n----------------------------------------\n\nTITLE: OpenSearch Index Template Example in JSON\nDESCRIPTION: Example of an automatically generated index template showing the mapping structure for telegraf metrics including field types and analysis settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opensearch/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"telegraf-2022.10.02\" : {\n    \"aliases\" : { },\n    \"mappings\" : {\n      \"properties\" : {\n        \"@timestamp\" : {\n          \"type\" : \"date\"\n        },\n        \"disk\" : {\n          \"properties\" : {\n            \"free\" : {\n              \"type\" : \"long\"\n            },\n            \"inodes_free\" : {\n              \"type\" : \"long\"\n            },\n            \"inodes_total\" : {\n              \"type\" : \"long\"\n            },\n            \"inodes_used\" : {\n              \"type\" : \"long\"\n            },\n            \"total\" : {\n              \"type\" : \"long\"\n            },\n            \"used\" : {\n              \"type\" : \"long\"\n            },\n            \"used_percent\" : {\n              \"type\" : \"float\"\n            }\n          }\n        },\n        \"measurement_name\" : {\n          \"type\" : \"text\",\n          \"fields\" : {\n            \"keyword\" : {\n              \"type\" : \"keyword\",\n              \"ignore_above\" : 256\n            }\n          }\n        },\n        \"tag\" : {\n          \"properties\" : {\n            \"cpu\" : {\n              \"type\" : \"text\",\n              \"fields\" : {\n                \"keyword\" : {\n                  \"type\" : \"keyword\",\n                  \"ignore_above\" : 256\n                }\n              }\n            },\n            \"device\" : {\n              \"type\" : \"text\",\n              \"fields\" : {\n                \"keyword\" : {\n                  \"type\" : \"keyword\",\n                  \"ignore_above\" : 256\n                }\n              }\n            },\n            \"host\" : {\n              \"type\" : \"text\",\n              \"fields\" : {\n                \"keyword\" : {\n                  \"type\" : \"keyword\",\n                  \"ignore_above\" : 256\n                }\n              }\n            },\n            \"mode\" : {\n              \"type\" : \"text\",\n              \"fields\" : {\n                \"keyword\" : {\n                  \"type\" : \"keyword\",\n                  \"ignore_above\" : 256\n                }\n              }\n            },\n            \"path\" : {\n              \"type\" : \"text\",\n              \"fields\" : {\n                \"keyword\" : {\n                  \"type\" : \"keyword\",\n                  \"ignore_above\" : 256\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1664693522789\",\n        \"number_of_shards\" : \"1\",\n        \"number_of_replicas\" : \"1\",\n        \"uuid\" : \"TYugdmvsQfmxjzbGRJ8FIw\",\n        \"version\" : {\n          \"created\" : \"136247827\"\n        },\n        \"provided_name\" : \"telegraf-2022.10.02\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Syslog Plugin Example Output\nDESCRIPTION: This code snippet shows example output generated by the Telegraf syslog plugin. It includes fields such as appname, facility, host, hostname, location, severity, source, facility_code, message, severity_code, timestamp, and version.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n```text\nsyslog,appname=docker-compose,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643706396113000i,version=1i 1624643706400667198\nsyslog,appname=tailscaled,facility=daemon,host=bb8,hostname=dev,location=home,severity=info,source=10.0.0.15 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643706403394000i,version=1i 1624643706407850408\nsyslog,appname=docker-compose,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643706675853000i,version=1i 1624643706679251683\nsyslog,appname=telegraf,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643710005006000i,version=1i 1624643710008285426\nsyslog,appname=telegraf,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643710005696000i,version=1i 1624643710010754050\nsyslog,appname=docker-compose,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643715777813000i,version=1i 1624643715782158154\nsyslog,appname=docker-compose,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643716396547000i,version=1i 1624643716400395788\nsyslog,appname=tailscaled,facility=daemon,host=bb8,hostname=dev,location=home,severity=info,source=10.0.0.15 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643716404931000i,version=1i 1624643716416947058\nsyslog,appname=docker-compose,facility=daemon,host=bb8,hostname=droplet,location=home,severity=info,source=10.0.0.12 facility_code=3i,message=\\\"<redacted>\\\",severity_code=6i,timestamp=1624643716676633000i,version=1i 1624643716680157558\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing the StatefulPlugin Interface in Go\nDESCRIPTION: This snippet demonstrates the StatefulPlugin interface that needs to be implemented by plugins to enable state-persistence. It includes two methods: GetState() for retrieving the current state and SetState() for setting the state.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/STATE_PERSISTENCE.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\ntype StatefulPlugin interface {\n    GetState() interface{}\n    SetState(state interface{}) error\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OS Secret-store Plugin in Telegraf\nDESCRIPTION: TOML configuration example for the OS secret-store plugin, showing all available options. The configuration includes store identification, keyring settings, platform-specific options, and dynamic secret support.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/os/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Operating System native secret-store\n[[secretstores.os]]\n  ## Unique identifier for the secret-store.\n  ## This id can later be used in plugins to reference the secrets\n  ## in this secret-store via @{<id>:<secret_key>} (mandatory)\n  id = \"secretstore\"\n\n  ## Keyring Name & Collection\n  ## * Linux: keyring name used for the secrets, collection is unused\n  ## * macOS: keyring specifies the macOS' Keychain name and collection is an\n  ##     optional Keychain service name\n  ## * Windows: keys follow a fixed pattern in the form\n  ##     `<collection>:<keyring>:<key_name>`. Please keep this in mind when\n  ##     creating secrets with the Windows credential tool.\n  # keyring = \"telegraf\"\n  # collection = \"\"\n\n  ## macOS Keychain password\n  ## If no password is specified here, Telegraf will prompt for it at startup\n  ## time.\n  # password = \"\"\n\n  ## Allow dynamic secrets that are updated during runtime of telegraf\n  # dynamic = false\n```\n\n----------------------------------------\n\nTITLE: Configuring AMQP Consumer Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the AMQP Consumer Input Plugin in Telegraf. This configuration defines broker connections, exchange and queue settings, authentication, message handling parameters, and data format specifications.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/amqp_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# AMQP consumer plugin\n[[inputs.amqp_consumer]]\n  ## Brokers to consume from.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Name of the exchange to declare.  If unset, no exchange will be declared.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n\n  ## AMQP queue name.\n  queue = \"telegraf\"\n\n  ## AMQP queue durability can be \"transient\" or \"durable\".\n  queue_durability = \"durable\"\n\n  ## If true, queue will be passively declared.\n  # queue_passive = false\n\n  ## Additional arguments when consuming from Queue\n  # queue_consume_arguments = { }\n  # queue_consume_arguments = {\"x-stream-offset\" = \"first\"}\n\n  ## Additional queue arguments.\n  # queue_arguments = { }\n  # queue_arguments = {\"x-max-length\" = 100}\n\n  ## A binding between the exchange and queue using this binding key is\n  ## created.  If unset, no binding is created.\n  binding_key = \"#\"\n\n  ## Maximum number of messages server should give to the worker.\n  # prefetch_count = 50\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Timeout for establishing the connection to a broker\n  # timeout = \"30s\"\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Content encoding for message payloads, can be set to\n  ## \"gzip\", \"identity\" or \"auto\"\n  ## - Use \"gzip\" to decode gzip\n  ## - Use \"identity\" to apply no encoding\n  ## - Use \"auto\" determine the encoding using the ContentEncoding header\n  # content_encoding = \"identity\"\n\n  ## Maximum size of decoded message.\n  ## Acceptable units are B, KiB, KB, MiB, MB...\n  ## Without quotes and units, interpreted as size in bytes.\n  # max_decompression_size = \"500MB\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: vSphere Inventory Structure Example\nDESCRIPTION: Example showing the hierarchical structure of a vSphere inventory including datacenters, clusters, hosts, and VMs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<root>\n+-DC0 # Virtual datacenter\n   +-datastore # Datastore folder (created by system)\n   | +-Datastore1\n   +-host # Host folder (created by system)\n   | +-Cluster1\n   | | +-Host1\n   | | | +-VM1\n   | | | +-VM2\n   | | | +-hadoop1\n   | | +-ResourcePool1\n   | | | +-VM3\n   | | | +-VM4\n   | +-Host2 # Dummy cluster created for non-clustered host\n   | | +-Host2\n   | | | +-VM5\n   | | | +-VM6\n   +-vm # VM folder (created by system)\n   | +-VM1\n   | +-VM2\n   | +-Folder1\n   | | +-hadoop1\n   | | +-NestedFolder1\n   | | | +-VM3\n   | | | +-VM4\n```\n\n----------------------------------------\n\nTITLE: Configuring Ethtool Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the Ethtool input plugin in Telegraf. It includes options for interface selection, namespace handling, and key normalization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ethtool/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Returns ethtool statistics for given interfaces\n# This plugin ONLY supports Linux\n[[inputs.ethtool]]\n  ## List of interfaces to pull metrics for\n  # interface_include = [\"eth0\"]\n\n  ## List of interfaces to ignore when pulling metrics.\n  # interface_exclude = [\"eth1\"]\n\n  ## Plugin behavior for downed interfaces\n  ## Available choices:\n  ##   - expose: collect & report metrics for down interfaces\n  ##   - skip: ignore interfaces that are marked down\n  # down_interfaces = \"expose\"\n\n  ## Reading statistics from interfaces in additional namespaces is also\n  ## supported, so long as the namespaces are named (have a symlink in\n  ## /var/run/netns). The telegraf process will also need the CAP_SYS_ADMIN\n  ## permission.\n  ## By default, only the current namespace will be used. For additional\n  ## namespace support, at least one of `namespace_include` and\n  ## `namespace_exclude` must be provided.\n  ## To include all namespaces, set `namespace_include` to `[\"*\"]`.\n  ## The initial namespace (if anonymous) can be specified with the empty\n  ## string (\"\").\n\n  ## List of namespaces to pull metrics for\n  # namespace_include = []\n\n  ## List of namespace to ignore when pulling metrics.\n  # namespace_exclude = []\n\n  ## Some drivers declare statistics with extra whitespace, different spacing,\n  ## and mix cases. This list, when enabled, can be used to clean the keys.\n  ## Here are the current possible normalizations:\n  ##  * snakecase: converts fooBarBaz to foo_bar_baz\n  ##  * trim: removes leading and trailing whitespace\n  ##  * lower: changes all capitalized letters to lowercase\n  ##  * underscore: replaces spaces with underscores\n  # normalize_keys = [\"snakecase\", \"trim\", \"lower\", \"underscore\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sumo Logic Output in Telegraf using TOML\nDESCRIPTION: Configuration template for setting up Sumo Logic HTTP metric collector in Telegraf. Includes settings for URL endpoint, data format specification, timeout settings, request body size limits, and additional Sumo Logic specific options like source name, host, category, and custom dimensions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sumologic/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin that can send metrics to Sumo Logic HTTP metric collector.\n[[outputs.sumologic]]\n  ## Unique URL generated for your HTTP Metrics Source.\n  ## This is the address to send metrics to.\n  # url = \"https://events.sumologic.net/receiver/v1/http/<UniqueHTTPCollectorCode>\"\n\n  ## Data format to be used for sending metrics.\n  ## This will set the \"Content-Type\" header accordingly.\n  ## Currently supported formats:\n  ## * graphite - for Content-Type of application/vnd.sumologic.graphite\n  ## * carbon2 - for Content-Type of application/vnd.sumologic.carbon2\n  ## * prometheus - for Content-Type of application/vnd.sumologic.prometheus\n  ##\n  ## More information can be found at:\n  ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#content-type-headers-for-metrics\n  ##\n  ## NOTE:\n  ## When unset, telegraf will by default use the influx serializer which is currently unsupported\n  ## in HTTP Source.\n  data_format = \"carbon2\"\n\n  ## Timeout used for HTTP request\n  # timeout = \"5s\"\n\n  ## Max HTTP request body size in bytes before compression (if applied).\n  ## By default 1MB is recommended.\n  ## NOTE:\n  ## Bear in mind that in some serializer a metric even though serialized to multiple\n  ## lines cannot be split any further so setting this very low might not work\n  ## as expected.\n  # max_request_body_size = 1000000\n\n  ## Additional, Sumo specific options.\n  ## Full list can be found here:\n  ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#supported-http-headers\n\n  ## Desired source name.\n  ## Useful if you want to override the source name configured for the source.\n  # source_name = \"\"\n\n  ## Desired host name.\n  ## Useful if you want to override the source host configured for the source.\n  # source_host = \"\"\n\n  ## Desired source category.\n  ## Useful if you want to override the source category configured for the source.\n  # source_category = \"\"\n\n  ## Comma-separated key=value list of dimensions to apply to every metric.\n  ## Custom dimensions will allow you to query your metrics at a more granular level.\n  # dimensions = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Unpivot Processor in TOML\nDESCRIPTION: This snippet shows how to configure the 'unpivot' processor in a TOML configuration file for Telegraf. It specifies options such as metric mode, tag key, and value key, which determine how fields are transformed during processing. The configuration requires users to understand the implications of the chosen metric mode on the resulting metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/unpivot/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Rotate multi field metric into several single field metrics\n[[processors.unpivot]]\n  ## Metric mode to pivot to\n  ## Set to \"tag\", metrics are pivoted as a tag and the metric is kept as\n  ## the original measurement name. Tag key name is set by tag_key value.\n  ## Set to \"metric\" creates a new metric named the field name. With this\n  ## option the tag_key is ignored. Be aware that this could lead to metric\n  ## name conflicts!\n  # use_fieldname_as = \"tag\"\n\n  ## Tag to use for the name.\n  # tag_key = \"name\"\n\n  ## Field to use for the name of the value.\n  # value_key = \"value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Plus API Input Plugin\nDESCRIPTION: TOML configuration for the Nginx Plus API Telegraf input plugin that specifies the API endpoint URL for gathering statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nginx_plus_api]]\n  ## An array of Nginx Plus API URIs to gather stats.\n  urls = [\"http://localhost/api\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Temp Input Plugin\nDESCRIPTION: This TOML configuration snippet shows how to configure the Telegraf `temp` input plugin. It allows specifying the desired output format for Linux systems (v1 or v2) and adding device tags to distinguish devices with the same name. The commented-out options illustrate the available settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/temp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Read metrics about temperature\n[[inputs.temp]]\n  ## Desired output format (Linux only)\n  ## Available values are\n  ##   v1 -- use pre-v1.22.4 sensor naming, e.g. coretemp_core0_input\n  ##   v2 -- use v1.22.4+ sensor naming, e.g. coretemp_core_0_input\n  # metric_format = \\\"v2\\\"\n\n  ## Add device tag to distinguish devices with the same name (Linux only)\n  # add_device_tag = false\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Event Hubs Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Azure Event Hubs output plugin for Telegraf. It includes settings for the connection string, partition key, maximum message size, timeout, and data format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/event_hubs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for Event Hubs output plugin\n[[outputs.event_hubs]]\n  ## Full connection string to the Event Hub instance. The shared access key\n  ## must have \"Send\" permissions on the target Event Hub.\n  connection_string = \"Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName\"\n\n  ## Partition key to use for the event\n  ## Metric tag or field name to use for the event partition key. The value of\n  ## this tag or field is set as the key for events if it exists. If both, tag\n  ## and field, exist the tag is preferred.\n  # partition_key = \"\"\n\n  ## Set the maximum batch message size in bytes\n  ## The allowable size depends on the Event Hub tier, see\n  ##   https://learn.microsoft.com/azure/event-hubs/event-hubs-quotas#basic-vs-standard-vs-premium-vs-dedicated-tiers\n  ## for details. If unset the default size defined by Azure Event Hubs is\n  ## used (currently 1,000,000 bytes)\n  # max_message_size = \"1MB\"\n\n  ## Timeout for sending the data\n  # timeout = \"30s\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"json\"\n```\n\n----------------------------------------\n\nTITLE: Testing Synproxy Counters via CLI\nDESCRIPTION: This shell command is executed on a Linux system to check the synproxy counters directly from the system's proc filesystem. It helps in troubleshooting and verifying the synproxy metrics functionality.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/synproxy/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncat /proc/net/stat/synproxy\n```\n\n----------------------------------------\n\nTITLE: Logparser Configuration Example - Telegraf (TOML)\nDESCRIPTION: This configuration snippet sets up the logparser input plugin to read and parse log files, specifying parameters such as file paths, grok patterns to use, and timezone settings. This is essential for ensuring that logs are correctly parsed and metrics generated as expected.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/logparser/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n\"```toml @sample.conf\\n# Read metrics off Arista LANZ, via socket\\n[[inputs.logparser]]\\n  ## Log files to parse.\\n  ## These accept standard unix glob matching rules, but with the addition of\\n  ## ** as a \\\"super asterisk\\\". ie:\\n  ##   /var/log/**.log     -> recursively find all .log files in /var/log\\n  ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log\\n  ##   /var/log/apache.log -> only tail the apache log file\\n  files = [\\\"/var/log/apache/access.log\\\"]\\n\\n  ## Read files that currently exist from the beginning. Files that are created\\n  ## while telegraf is running (and that match the \\\"files\\\" globs) will always\\n  ## be read from the beginning.\\n  from_beginning = false\\n\\n  ## Method used to watch for file updates.  Can be either \\\"inotify\\\" or \\\"poll\\\".\\n  # watch_method = \\\"inotify\\\"\\n\\n  ## Parse logstash-style \\\"grok\\\" patterns:\\n  [inputs.logparser.grok]\\n    ## This is a list of patterns to check the given log file(s) for.\\n    ## Note that adding patterns here increases processing time. The most\\n    ## efficient configuration is to have one pattern per logparser.\\n    ## Other common built-in patterns are:\\n    ##   %{COMMON_LOG_FORMAT}   (plain apache & nginx access logs)\\n    ##   %{COMBINED_LOG_FORMAT} (access logs + referrer & agent)\\n    patterns = [\\\"%{COMBINED_LOG_FORMAT}\\\"]\\n\\n    ## Name of the outputted measurement name.\\n    measurement = \\\"apache_access_log\\\"\\n\\n    ## Full path(s) to custom pattern files.\\n    custom_pattern_files = []\\n\\n    ## Custom patterns can also be defined here. Put one pattern per line.\\n    custom_patterns = '''\\n    '''\\n\\n    ## Timezone allows you to provide an override for timestamps that\\n    ## don't already include an offset\\n    ## e.g. 04/06/2016 12:41:45 data one two 5.43s\\n    ##\\n    ## Default: \\\"\\\" which renders UTC\\n    ## Options are as follows:\\n    ##   1. Local             -- interpret based on machine localtime\\n    ##   2. \\\"Canada/Eastern\\\"  -- Unix TZ values like those found in https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\\n    ##   3. UTC               -- or blank/unspecified, will return timestamp in UTC\\n    # timezone = \\\"Canada/Eastern\\\"\\n\\n    ## When set to \\\"disable\\\", timestamp will not incremented if there is a\\n    ## duplicate.\\n    # unique_timestamp = \\\"auto\\\"\\n```\"\n```\n\n----------------------------------------\n\nTITLE: Example LANZ Plugin Output Format\nDESCRIPTION: Sample output showing the format of metrics collected by the LANZ plugin, including global buffer usage and congestion records with various tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lanz/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nlanz_global_buffer_usage_record,entry_type=2,host=telegraf.int.example.com,port=50001,source=switch01.int.example.com timestamp=158334105824919i,buffer_size=505i,duration=0i 1583341058300643815\nlanz_congestion_record,entry_type=2,host=telegraf.int.example.com,intf_name=Ethernet36,port=50001,port_id=61,source=switch01.int.example.com,switch_id=0,traffic_class=1 time_of_max_qlen=0i,tx_latency=564480i,q_drop_count=0i,timestamp=158334105824919i,queue_size=225i 1583341058300636045\nlanz_global_buffer_usage_record,entry_type=2,host=telegraf.int.example.com,port=50001,source=switch01.int.example.com timestamp=158334105824919i,buffer_size=589i,duration=0i 1583341058300457464\nlanz_congestion_record,entry_type=1,host=telegraf.int.example.com,intf_name=Ethernet36,port=50001,port_id=61,source=switch01.int.example.com,switch_id=0,traffic_class=1 q_drop_count=0i,timestamp=158334105824919i,queue_size=232i,time_of_max_qlen=0i,tx_latency=584640i 1583341058300450302\n```\n\n----------------------------------------\n\nTITLE: Monitoring .NET Runtime Metrics with Telegraf\nDESCRIPTION: Configuration for tracking .NET CLR performance counters for IIS worker processes (w3wp). Monitors exceptions, JIT compilation, loading, locks, threads, memory usage, and security checks.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_16\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    # .NET CLR Exceptions, in this case for IIS only\n    ObjectName = \".NET CLR Exceptions\"\n    Counters = [\"# of Exceps Thrown / sec\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_dotnet_exceptions\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # .NET CLR Jit, in this case for IIS only\n    ObjectName = \".NET CLR Jit\"\n    Counters = [\"% Time in Jit\",\"IL Bytes Jitted / sec\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_dotnet_jit\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # .NET CLR Loading, in this case for IIS only\n    ObjectName = \".NET CLR Loading\"\n    Counters = [\"% Time Loading\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_dotnet_loading\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # .NET CLR LocksAndThreads, in this case for IIS only\n    ObjectName = \".NET CLR LocksAndThreads\"\n    Counters = [\"# of current logical Threads\",\"# of current physical Threads\",\"# of current recognized threads\",\"# of total recognized threads\",\"Queue Length / sec\",\"Total # of Contentions\",\"Current Queue Length\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_dotnet_locks\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # .NET CLR Memory, in this case for IIS only\n    ObjectName = \".NET CLR Memory\"\n    Counters = [\"% Time in GC\",\"# Bytes in all Heaps\",\"# Gen 0 Collections\",\"# Gen 1 Collections\",\"# Gen 2 Collections\",\"# Induced GC\",\"Allocated Bytes/sec\",\"Finalization Survivors\",\"Gen 0 heap size\",\"Gen 1 heap size\",\"Gen 2 heap size\",\"Large Object Heap size\",\"# of Pinned Objects\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_dotnet_mem\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n  [[inputs.win_perf_counters.object]]\n    # .NET CLR Security, in this case for IIS only\n    ObjectName = \".NET CLR Security\"\n    Counters = [\"% Time in RT checks\",\"Stack Walk Depth\",\"Total Runtime Checks\"]\n    Instances = [\"w3wp\"]\n    Measurement = \"win_dotnet_security\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n```\n\n----------------------------------------\n\nTITLE: Displaying Kapacitor Node and Edge Metrics in InfluxDB Line Protocol\nDESCRIPTION: Example of Telegraf metrics output in InfluxDB line protocol format for monitoring Kapacitor nodes and edges. The first line shows a node metric with execution time, while the second shows an edge metric with collection and emission counts. Both include host information, task names, and precise timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kapacitor/README.md#2025-04-16_snippet_2\n\nLANGUAGE: influxdb\nCODE:\n```\nkapacitor_nodes,host=hostname.local,kind=sum,node=sum5,task=derivative-test,type=stream avg_exec_time_ns=0i 1478791462000000000\nkapacitor_edges,child=eval4,host=hostname.local,parent=log3,task=derivative-test,type=stream collected=0,emitted=0 1478791462000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Output Plugin in Telegraf\nDESCRIPTION: TOML configuration for file output with JSON data format, specifying output files, timestamp resolution, and optional JSON transformations\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n  data_format = \"json\"\n  json_timestamp_units = \"1s\"\n  #json_timestamp_format = \"\"\n  #json_transformation = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Librato Output Plugin in TOML\nDESCRIPTION: Configuration snippet for setting up the Librato output plugin in Telegraf. Includes required API credentials, timeout settings, and source template configuration. Requires api_user and api_token obtained from Librato account.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/librato/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for Librato API to send metrics to.\n[[outputs.librato]]\n  ## Librato API Docs\n  ## http://dev.librato.com/v1/metrics-authentication\n  ## Librato API user\n  api_user = \"telegraf@influxdb.com\" # required.\n  ## Librato API token\n  api_token = \"my-secret-token\" # required.\n  ## Debug\n  # debug = false\n  ## Connection timeout.\n  # timeout = \"5s\"\n  ## Output source Template (same as graphite buckets)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  ## This template is used in librato's source (not metric's name)\n  template = \"host\"\n```\n\n----------------------------------------\n\nTITLE: Example of Splitting Metrics using TOML Configuration\nDESCRIPTION: This TOML example configures the Split Processor Plugin to create separate metrics for each sensor from a single input metric, demonstrating the split process. Dependencies: Telegraf with the split processor plugin. The snippet contains examples of template configuration using wildcard tags and fields. The expected input is a metric containing multiple sensors, and the output is a distinct metric for each sensor.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/split/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.split]]\n  drop_original = true\n  [[processors.split.template]]\n    name = \"sensor1\"\n    tags = [ \"*\" ]\n    fields = [ \"sensor1*\" ]\n  [[processors.split.template]]\n    name = \"sensor2\"\n    tags = [ \"*\" ]\n    fields = [ \"sensor2*\" ]\n```\n\n----------------------------------------\n\nTITLE: Converter Processor Configuration for Netflow Tagging\nDESCRIPTION: TOML configuration for the converter processor to tag Netflow metrics properly for outputs like InfluxDB. Converts important fields to tags to prevent metric overwriting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netflow/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.converter]]\n  [processors.converter.fields]\n    tag = [\"protocol\", \"src\", \"src_port\", \"dst\", \"dst_port\"]\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Tags to CPU Input Plugin in TOML\nDESCRIPTION: Configuration example showing how to add custom tags to the CPU input plugin using TOML's table syntax. This defines two additional tags: tag1=foo and tag2=bar.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_13\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  [inputs.cpu.tags]\n    tag1 = \"foo\"\n    tag2 = \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Template Filtering by Metric Name\nDESCRIPTION: Demonstrates how to apply different templates based on the name of the bucket using glob matching patterns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\ntemplates = [\n    \"cpu.* measurement.measurement.region\",\n    \"mem.* measurement.measurement.host\"\n]\n```\n\n----------------------------------------\n\nTITLE: Introducing New Plugin Enhancements in Telegraf v1.32.0\nDESCRIPTION: Major changes include a logging system overhaul, json_v2 parser config validation, and experimental disk-backed metric buffer feature. New plugins added include SLURM workload manager input and Parquet/remotefile outputs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n### Important Changes\n\n- Logging overhaul with deprecation of 'logtarget'\n- json_v2 parser now requires explicit configuration\n- New experimental disk-backed metric buffer feature\n\n### New Plugins\n\n- inputs.slurm: SLURM workload manager\n- outputs.parquet: Parquet file writer\n- outputs.remotefile: Output to remote location like S3\n```\n\n----------------------------------------\n\nTITLE: Example Prometheus Remote Write Input Structure\nDESCRIPTION: JSON representation of a Prometheus remote write request structure containing time series data with labels and samples.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheusremotewrite/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nprompb.WriteRequest{\n        Timeseries: []*prompb.TimeSeries{\n            {\n                Labels: []*prompb.Label{\n                    {Name: \"__name__\", Value: \"go_gc_duration_seconds\"},\n                    {Name: \"instance\", Value: \"localhost:9090\"},\n                    {Name: \"job\", Value: \"prometheus\"},\n                    {Name: \"quantile\", Value: \"0.99\"},\n                },\n                Samples: []prompb.Sample{\n                    {Value: 4.63, Timestamp: time.Date(2020, 4, 1, 0, 0, 0, 0, time.UTC).UnixNano()},\n                },\n            },\n        },\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf vSphere Plugin for Realtime vSAN Metrics\nDESCRIPTION: Configuration for collecting realtime vSAN summary metrics with a short 30-second collection interval. Focuses on excluding other metric types and specifically including vSAN summary metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n## Realtime instance\n[[inputs.vsphere]]\n  interval = \"30s\"\n  vcenters = [ \"https://someaddress/sdk\" ]\n  username = \"someuser@vsphere.local\"\n  password = \"secret\"\n\n  insecure_skip_verify = true\n  force_discover_on_init = true\n\n  # Exclude all other metrics\n  vm_metric_exclude = [\"*\"]\n  datastore_metric_exclude = [\"*\"]\n  datacenter_metric_exclude = [\"*\"]\n  host_metric_exclude = [\"*\"]\n  cluster_metric_exclude = [\"*\"]\n\n  vsan_metric_include = [ \"summary.*\" ]\n  vsan_metric_exclude = [ ]\n  vsan_metric_skip_verify = false\n\n  collect_concurrency = 5\n  discover_concurrency = 5\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Template for Telegraf File Output\nDESCRIPTION: This code snippet demonstrates how to configure a batch template for the Telegraf `file` output plugin when using the `template` data format.  When `use_batch_format = true` is set in conjunction with the `file` plugin, this example iterates over a slice of metrics. For each metric, it extracts tags and fields, formatting them in a specific way.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/template/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n  ## When used with output plugins that allow for batch serialisation\n  ## the template for the entire batch can be defined\n  # use_batch_format = true  # The 'file' plugin allows batch mode with this option\n  # batch_template = '''\n{{range $metric := . -}}\n{{$metric.Tag \"host\"}}: {{range $metric.Fields | keys | initial -}}\n{{.}}={{get $metric.Fields .}}, {{end}}\n{{- $metric.Fields|keys|last}}={{$metric.Fields|values|last}}\n{{end -}}\n'''\n```\n\n----------------------------------------\n\nTITLE: Configuring Aerospike Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Aerospike input plugin including server connection details, authentication, TLS configuration, namespace filtering, set level telemetry, and histogram settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/aerospike/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.aerospike]]\n  ## Aerospike servers to connect to (with port)\n  ## This plugin will query all namespaces the aerospike\n  ## server has configured and get stats for them.\n  servers = [\"localhost:3000\"]\n\n  # username = \"telegraf\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_name = \"tlsname\"\n  ## If false, skip chain & host verification\n  # insecure_skip_verify = true\n\n  # Feature Options\n  # Add namespace variable to limit the namespaces executed on\n  # Leave blank to do all\n  # disable_query_namespaces = true # default false\n  # namespaces = [\"namespace1\", \"namespace2\"]\n\n  # Enable set level telemetry\n  # query_sets = true # default: false\n  # Add namespace set combinations to limit sets executed on\n  # Leave blank to do all sets\n  # sets = [\"namespace1/set1\", \"namespace1/set2\", \"namespace3\"]\n\n  # Histograms\n  # enable_ttl_histogram = true # default: false\n  # enable_object_size_linear_histogram = true # default: false\n\n  # by default, aerospike produces a 100 bucket histogram\n  # this is not great for most graphing tools, this will allow\n  # the ability to squash this to a smaller number of buckets\n  # To have a balanced histogram, the number of buckets chosen\n  # should divide evenly into 100.\n  # num_histogram_buckets = 100 # default: 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Alibaba Cloud Monitor Service Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Aliyun CMS input plugin in Telegraf. This configuration specifies authentication methods, regions, collection period, delay interval, and metric specifications for gathering statistics from Alibaba Cloud services.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/aliyuncms/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Pull Metric Statistics from Aliyun CMS\n[[inputs.aliyuncms]]\n  ## Aliyun Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Ram RoleArn credential\n  ## 2) AccessKey STS token credential\n  ## 3) AccessKey credential\n  ## 4) Ecs Ram Role credential\n  ## 5) RSA keypair credential\n  ## 6) Environment variables credential\n  ## 7) Instance metadata credential\n\n  # access_key_id = \"\"\n  # access_key_secret = \"\"\n  # access_key_sts_token = \"\"\n  # role_arn = \"\"\n  # role_session_name = \"\"\n  # private_key = \"\"\n  # public_key_id = \"\"\n  # role_name = \"\"\n\n  ## Specify ali cloud regions to be queried for metric and object discovery\n  ## If not set, all supported regions (see below) would be covered, it can\n  ## provide a significant load on API, so the recommendation here is to\n  ## limit the list as much as possible.\n  ## Allowed values: https://www.alibabacloud.com/help/zh/doc-detail/40654.htm\n  ## Default supported regions are:\n  ##   cn-qingdao,cn-beijing,cn-zhangjiakou,cn-huhehaote,cn-hangzhou,\n  ##   cn-shanghai, cn-shenzhen, cn-heyuan,cn-chengdu,cn-hongkong,\n  ##   ap-southeast-1,ap-southeast-2,ap-southeast-3,ap-southeast-5,\n  ##   ap-south-1,ap-northeast-1, us-west-1,us-east-1,eu-central-1,\n  ##   eu-west-1,me-east-1\n  ##\n  ## From discovery perspective it set the scope for object discovery,\n  ## the discovered info can be used to enrich the metrics with objects\n  ##  attributes/tags. Discovery is not supported for all projects.\n  ## Currently, discovery supported for the following projects:\n  ## - acs_ecs_dashboard\n  ## - acs_rds_dashboard\n  ## - acs_slb_dashboard\n  ## - acs_vpc_eip\n  regions = [\"cn-hongkong\"]\n\n  ## Requested AliyunCMS aggregation Period (required)\n  ## The period must be multiples of 60s and the minimum for AliyunCMS metrics\n  ## is 1 minute (60s). However not all metrics are made available to the\n  ## one minute period. Some are collected at 3 minute, 5 minute, or larger\n  ## intervals.\n  ## See: https://help.aliyun.com/document_detail/51936.html?spm=a2c4g.11186623.2.18.2bc1750eeOw1Pv\n  ## Note that if a period is configured that is smaller than the minimum for\n  ## a particular metric, that metric will not be returned by Aliyun's\n  ## OpenAPI and will not be collected by Telegraf.\n  period = \"5m\"\n\n  ## Collection Delay (required)\n  ## The delay must account for metrics availability via AliyunCMS API.\n  delay = \"1m\"\n\n  ## Recommended: use metric 'interval' that is a multiple of 'period'\n  ## to avoid gaps or overlap in pulled data\n  interval = \"5m\"\n\n  ## Metric Statistic Project (required)\n  project = \"acs_slb_dashboard\"\n\n  ## Maximum requests per second, default value is 200\n  ratelimit = 200\n\n  ## How often the discovery API call executed (default 1m)\n  #discovery_interval = \"1m\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Metrics to Pull\n  ## At least one metrics definition required\n  [[inputs.aliyuncms.metrics]]\n    ## Metrics names to be requested,\n    ## Description can be found here (per project):\n    ## https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n    names = [\"InstanceActiveConnection\", \"InstanceNewConnection\"]\n\n    ## Dimension filters for Metric (optional)\n    ## This allows to get additional metric dimension. If dimension is not\n    ## specified it can be returned or the data can be aggregated - it depends\n    ## on particular metric, you can find details here:\n    ##   https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n    ##\n    ## Note, that by default dimension filter includes the list of discovered\n    ## objects in scope (if discovery is enabled). Values specified here would\n    ## be added into the list of discovered objects. You can specify either\n    ## single dimension:\n    # dimensions = '{\"instanceId\": \"p-example\"}'\n\n    ## Or you can specify several dimensions at once:\n    # dimensions = '[{\"instanceId\": \"p-example\"},{\"instanceId\": \"q-example\"}]'\n\n    ## Tag Query Path\n    ## The following tags added by default:\n    ##   * regionId (if discovery enabled)\n    ##   * userId\n    ##   * instanceId\n    ## Enrichment tags, can be added from discovery (if supported)\n    ## Notation is\n    ##   <measurement_tag_name>:<JMES query path (https://jmespath.org/tutorial.html)>\n    ## To figure out which fields are available, consult the\n    ## Describe<ObjectType> API per project. For example, for SLB see:\n    ##   https://api.aliyun.com/#/?product=Slb&version=2014-05-15&api=DescribeLoadBalancers&params={}&tab=MOCK&lang=GO\n    # tag_query_path = [\n    #    \"address:Address\",\n    #    \"name:LoadBalancerName\",\n    #    \"cluster_owner:Tags.Tag[?TagKey=='cs.cluster.name'].TagValue | [0]\"\n    #    ]\n\n    ## Allow metrics without discovery data, if discovery is enabled.\n    ## If set to true, then metric without discovery data would be emitted, otherwise dropped.\n    ## This cane be of help, in case debugging dimension filters, or partial coverage of\n    ## discovery scope vs monitoring scope\n    # allow_dps_without_discovery = false\n```\n\n----------------------------------------\n\nTITLE: Example of Merge Aggregator Plugin Output in Diff Format\nDESCRIPTION: An example showing the before and after state when using the Merge Aggregator Plugin. It demonstrates how separate metrics with the same timestamp and tags are combined into a single metric with all fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/merge/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- cpu,host=localhost usage_time=42 1567562620000000000\n- cpu,host=localhost idle_time=42 1567562620000000000\n+ cpu,host=localhost idle_time=42,usage_time=42 1567562620000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf for Apache Mesos Metrics Collection (TOML)\nDESCRIPTION: This configuration snippet sets up the Telegraf plugin to collect metrics from Mesos masters. It includes configurable timeout and a list of Mesos masters as well as the groups of metrics to collect. The snippet also shows placeholders for TLS settings if secure connections are needed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mesos/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Telegraf plugin for gathering metrics from N Mesos masters\n[[inputs.mesos]]\n  ## Timeout, in ms.\n  timeout = 100\n\n  ## A list of Mesos masters.\n  masters = [\"http://localhost:5050\"]\n\n  ## Master metrics groups to be collected, by default, all enabled.\n  master_collections = [\n    \"resources\",\n    \"master\",\n    \"system\",\n    \"agents\",\n    \"frameworks\",\n    \"framework_offers\",\n    \"tasks\",\n    \"messages\",\n    \"evqueue\",\n    \"registrar\",\n    \"allocator\",\n  ]\n\n  ## A list of Mesos slaves, default is []\n  # slaves = []\n\n  ## Slave metrics groups to be collected, by default, all enabled.\n  # slave_collections = [\n  #   \"resources\",\n  #   \"agent\",\n  #   \"system\",\n  #   \"executors\",\n  #   \"tasks\",\n  #   \"messages\",\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Binary Parser in Telegraf\nDESCRIPTION: Complete configuration example for the binary parser plugin showing file input setup, endianness configuration, encoding options, and metric parsing definitions. Includes filter configuration for handling different message types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/binary/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.bin\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"binary\"\n\n  ## Do not error-out if none of the filter expressions below matches.\n  # allow_no_match = false\n\n  ## Specify the endianness of the data.\n  ## Available values are \"be\" (big-endian), \"le\" (little-endian) and \"host\",\n  ## where \"host\" means the same endianness as the machine running Telegraf.\n  # endianness = \"host\"\n\n  ## Interpret input using the specified encoding\n  ## Available values are \"none\" (raw bytes), \"hex\" and \"base64\"\n  # binary_encoding = \"none\"\n\n  ## Multiple parsing sections are allowed\n  [[inputs.file.binary]]\n    ## Optional: Metric (measurement) name to use if not extracted from the data.\n    # metric_name = \"my_name\"\n\n    entries = [\n      { type = \"string\", assignment = \"measurement\", terminator = \"null\" },\n      { name = \"address\", type = \"uint16\", assignment = \"tag\" },\n      { name = \"value\",   type = \"float64\" },\n      { type = \"unix\", assignment = \"time\" },\n    ]\n\n    ## Optional: Filter evaluated before applying the configuration.\n    # [inputs.file.binary.filter]\n    #   ## Filter message by the exact length in bytes (default: N/A).\n    #   # length = 0\n    #   ## Filter the message by a minimum length in bytes.\n    #   ## Messages longer of equal length will pass.\n    #   # length_min = 0\n    #   ## List of data parts to match.\n    #   selection = [\n    #     { offset = 0, bits = 8, match = \"0x1F\" },\n    #   ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Monitoring Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Google Cloud Monitoring Input Plugin for Telegraf. It includes parameters such as 'project' for specifying the Google Cloud project, 'metric_type_prefix_include' for selecting specific metric types, and 'interval' for setting data collection frequency. Optional parameters like 'rate_limit' and 'delay' help optimize API usage. Proper authentication is necessary for accessing the APIs, and users should be aware of possible charges when using the API.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/stackdriver/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n# Gather timeseries from Google Cloud Platform v3 monitoring API\n[[inputs.stackdriver]]\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Include timeseries that start with the given metric type.\n  metric_type_prefix_include = [\n    \"compute.googleapis.com/\",\n  ]\n\n  ## Exclude timeseries that start with the given metric type.\n  # metric_type_prefix_exclude = []\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #  \"ALIGN_PERCENTILE_99\",\n  #  \"ALIGN_PERCENTILE_95\",\n  #  \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels> AND <user_labels> AND <system_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##   user_labels ::= <user_labels> {OR <user_label>}\n  ##   system_labels ::= <system_labels> {OR <system_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n  #\n  ## User labels refine the time series selection with the following expression:\n  ##   metadata.user_labels.\"<key>\" = <value>\n  #  [[inputs.stackdriver.filter.user_labels]]\n  #    key = \"environment\"\n  #    value = 'one_of(\"prod\", \"staging\")'\n  #\n  ## System labels refine the time series selection with the following expression:\n  ##   metadata.system_labels.\"<key>\" = <value>\n  #  [[inputs.stackdriver.filter.system_labels]]\n  #    key = \"machine_type\"\n  #    value = 'starts_with(\"e2-\")'\n\n```\n\n----------------------------------------\n\nTITLE: Referencing Secrets from Store in Telegraf Configuration\nDESCRIPTION: The format used to reference secrets defined in a store within Telegraf configuration. This pattern uses a special syntax that includes the store ID and the secret key.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/includes/secret_usage.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n@{<store-id>:<secret_key>}\n```\n\n----------------------------------------\n\nTITLE: Dense CloudWatch Metric Format Example\nDESCRIPTION: Example of a dense metric format for AWS CloudWatch showing usage metrics for Secrets Manager with various statistical fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncloudwatch_aws_usage,class=None,resource=GetSecretValue,service=Secrets\\ Manager,metric_name=call_count,type=API sum=6,sample_count=6,average=1,maximum=1,minimum=1 1715097840000000000\n```\n\n----------------------------------------\n\nTITLE: Defining a Telegraf Plugin Structure in Go\nDESCRIPTION: This snippet shows an example of a Telegraf plugin structure with configuration fields and a state variable. It illustrates how plugin configuration and state can be combined in a single structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/STATE_PERSISTENCE.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\ntype MyPlugin struct {\n    Server  string          `toml:\"server\"`\n    Token   string          `toml:\"token\"`\n    Timeout config.Duration `toml:\"timeout\"`\n\n    offset int\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fritzbox Input Plugin in TOML\nDESCRIPTION: Sample configuration for the Fritzbox input plugin showing URL setup, collection options, timeouts, and TLS configuration. This defines how Telegraf connects to AVM devices and what metrics to collect.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fritzbox/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather fritzbox status\n[[inputs.fritzbox]]\n  ## URLs of the devices to query including login credentials  \n  urls = [ \"http://user:password@fritz.box:49000/\" ]\n\n  ## The information to collect (see README for further details).\n  # collect = [\n  #   \"device\",\n  #   \"wan\",\n  #   \"ppp\",\n  #   \"dsl\",\n  #   \"wlan\",\n  # ]\n\n  ## The http timeout to use.\n  # timeout = \"10s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_key_pwd = \"secret\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Cluster Health Metrics Structure\nDESCRIPTION: Defines the structure and fields for cluster health metrics when cluster_health is enabled. Includes core health indicators like shard status, node counts, and cluster state.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- elasticsearch_cluster_health\n  - tags:\n    - name\n  - fields:\n    - active_primary_shards (integer)\n    - active_shards (integer)\n    - active_shards_percent_as_number (float)\n    - delayed_unassigned_shards (integer)\n    - initializing_shards (integer)\n    - number_of_data_nodes (integer)\n    - number_of_in_flight_fetch (integer)\n    - number_of_nodes (integer)\n    - number_of_pending_tasks (integer)\n    - relocating_shards (integer)\n    - status (string, one of green, yellow or red)\n    - status_code (integer, green = 1, yellow = 2, red = 3)\n    - task_max_waiting_in_queue_millis (integer)\n    - timed_out (boolean)\n    - unassigned_shards (integer)\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PowerStat for Selected CPU Metrics without Package Metrics\nDESCRIPTION: Configuration that disables all package metrics and collects only CPU temperature and frequency metrics for all CPUs except IDs 1-3. This shows how to use excluded_cpus to omit specific processors.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_powerstat]]\n  package_metrics = []\n  cpu_metrics = [\"cpu_frequency\", \"cpu_temperature\"]\n  excluded_cpus = [\"1-3\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring NSQ Consumer Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the NSQ Consumer Input Plugin for Telegraf. It specifies NSQD and NSQLookupd endpoints, topic and channel settings, max in-flight messages, and data format options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsq_consumer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from NSQD topic(s)\n[[inputs.nsq_consumer]]\n  ## An array representing the NSQD TCP HTTP Endpoints\n  nsqd = [\"localhost:4150\"]\n\n  ## An array representing the NSQLookupd HTTP Endpoints\n  nsqlookupd = [\"localhost:4161\"]\n  topic = \"telegraf\"\n  channel = \"consumer\"\n  max_in_flight = 100\n\n  ## Max undelivered messages\n  ## This plugin uses tracking metrics, which ensure messages are read to\n  ## outputs before acknowledging them to the original broker to ensure data\n  ## is not lost. This option sets the maximum messages to read from the\n  ## broker that have not been written by an output.\n  ##\n  ## This value needs to be picked with awareness of the agent's\n  ## metric_batch_size value as well. Setting max undelivered messages too high\n  ## can result in a constant stream of data batches to the output. While\n  ## setting it too low may never flush the broker's messages.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GroundWork Output Plugin in TOML\nDESCRIPTION: Sample configuration for the GroundWork output plugin in Telegraf. Includes settings for URL, authentication, default application type, host display name, service state, and tag mappings. Supports secret-store integration for credentials.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/groundwork/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send telegraf metrics to GroundWork Monitor\n[[outputs.groundwork]]\n  ## URL of your groundwork instance.\n  url = \"https://groundwork.example.com\"\n\n  ## Agent uuid for GroundWork API Server.\n  agent_id = \"\"\n\n  ## Username and password to access GroundWork API.\n  username = \"\"\n  password = \"\"\n\n  ## Default application type to use in GroundWork client\n  # default_app_type = \"TELEGRAF\"\n\n  ## Default display name for the host with services(metrics).\n  # default_host = \"telegraf\"\n\n  ## Default service state.\n  # default_service_state = \"SERVICE_OK\"\n\n  ## The name of the tag that contains the hostname.\n  # resource_tag = \"host\"\n\n  ## The name of the tag that contains the host group name.\n  # group_tag = \"group\"\n```\n\n----------------------------------------\n\nTITLE: Configuring WMI Query for Windows Failover Cluster Metrics in Telegraf\nDESCRIPTION: This snippet sets up a WMI query to collect metrics about Windows Server Failover Clusters, specifically the Dynamic Quorum status. It uses the MSCluster_Cluster class and includes the cluster name and quorum type as tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    namespace = \"root\\\\mscluster\"\n    class_name = \"MSCluster_Cluster\"\n    properties = [\n      \"Name\",\n      \"QuorumType\",\n      \"DynamicQuorumEnabled\"\n    ]\n    tag_properties = [\"Name\",\"QuorumType\"]\n```\n\n----------------------------------------\n\nTITLE: Querying Percentage of Used Connections in PostgreSQL\nDESCRIPTION: This SQL query calculates the percentage of used connections in PostgreSQL by comparing the count of active sessions to the maximum allowed connections. It uses pg_stat_activity and pg_settings tables to retrieve the necessary information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*)*100 / (select cast(nullif(setting, '') AS integer) from pg_settings where name='max_connections') as percentage_of_used_cons from pg_stat_activity\n```\n\n----------------------------------------\n\nTITLE: Configuring MD RAID Statistics Input in Telegraf (TOML)\nDESCRIPTION: This configuration snippet shows how to configure the `mdstat` input plugin in Telegraf.  It sets the file path from which to read MD RAID statistics; if `file_name` is not specified, the default `/proc/mdstat` is used.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mdstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Get kernel statistics from /proc/mdstat\n# This plugin ONLY supports Linux\n[[inputs.mdstat]]\n  ## Sets file path\n  ## If not specified, then default is /proc/mdstat\n  # file_name = \\\"/proc/mdstat\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SocketStat Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the SocketStat input plugin. It allows specifying which socket types to gather information from and optionally setting a custom timeout for ss execution.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socketstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather indicators from established connections, using iproute2's ss command.\n# This plugin ONLY supports non-Windows\n[[inputs.socketstat]]\n  ## ss can display information about tcp, udp, raw, unix, packet, dccp and sctp sockets\n  ## Specify here the types you want to gather\n  protocols = [ \"tcp\", \"udp\" ]\n\n  ## The default timeout of 1s for ss execution can be overridden here:\n  # timeout = \"1s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sensu Go Output Plugin in TOML\nDESCRIPTION: Complete configuration example for the Sensu Go output plugin showing backend/agent API setup, authentication, TLS settings, and event specifications. Includes settings for check metadata, entity details, and metrics handling.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sensu/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send aggregate metrics to Sensu Monitor\n[[outputs.sensu]]\n  ## BACKEND API URL is the Sensu Backend API root URL to send metrics to\n  ## (protocol, host, and port only). The output plugin will automatically\n  ## append the corresponding backend API path\n  ## /api/core/v2/namespaces/:entity_namespace/events/:entity_name/:check_name).\n  ##\n  ## Backend Events API reference:\n  ## https://docs.sensu.io/sensu-go/latest/api/events/\n  ##\n  ## AGENT API URL is the Sensu Agent API root URL to send metrics to\n  ## (protocol, host, and port only). The output plugin will automatically\n  ## append the correspeonding agent API path (/events).\n  ##\n  ## Agent API Events API reference:\n  ## https://docs.sensu.io/sensu-go/latest/api/events/\n  ##\n  ## NOTE: if backend_api_url and agent_api_url and api_key are set, the output\n  ## plugin will use backend_api_url. If backend_api_url and agent_api_url are\n  ## not provided, the output plugin will default to use an agent_api_url of\n  ## http://127.0.0.1:3031\n  ##\n  # backend_api_url = \"http://127.0.0.1:8080\"\n  # agent_api_url = \"http://127.0.0.1:3031\"\n\n  ## API KEY is the Sensu Backend API token\n  ## Generate a new API token via:\n  ##\n  ## $ sensuctl cluster-role create telegraf --verb create --resource events,entities\n  ## $ sensuctl cluster-role-binding create telegraf --cluster-role telegraf --group telegraf\n  ## $ sensuctl user create telegraf --group telegraf --password REDACTED\n  ## $ sensuctl api-key grant telegraf\n  ##\n  ## For more information on Sensu RBAC profiles & API tokens, please visit:\n  ## - https://docs.sensu.io/sensu-go/latest/reference/rbac/\n  ## - https://docs.sensu.io/sensu-go/latest/reference/apikeys/\n  ##\n  # api_key = \"${SENSU_API_KEY}\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Sensu Event details\n  ##\n  ## Below are the event details to be sent to Sensu.  The main portions of the\n  ## event are the check, entity, and metrics specifications. For more information\n  ## on Sensu events and its components, please visit:\n  ## - Events - https://docs.sensu.io/sensu-go/latest/reference/events\n  ## - Checks -  https://docs.sensu.io/sensu-go/latest/reference/checks\n  ## - Entities - https://docs.sensu.io/sensu-go/latest/reference/entities\n  ## - Metrics - https://docs.sensu.io/sensu-go/latest/reference/events#metrics\n  ##\n  ## Check specification\n  ## The check name is the name to give the Sensu check associated with the event\n  ## created. This maps to check.metadata.name in the event.\n  [outputs.sensu.check]\n    name = \"telegraf\"\n\n  ## Entity specification\n  ## Configure the entity name and namespace, if necessary. This will be part of\n  ## the entity.metadata in the event.\n  ##\n  ## NOTE: if the output plugin is configured to send events to a\n  ## backend_api_url and entity_name is not set, the value returned by\n  ## os.Hostname() will be used; if the output plugin is configured to send\n  ## events to an agent_api_url, entity_name and entity_namespace are not used.\n  # [outputs.sensu.entity]\n  #   name = \"server-01\"\n  #   namespace = \"default\"\n\n  ## Metrics specification\n  ## Configure the tags for the metrics that are sent as part of the Sensu event\n  # [outputs.sensu.tags]\n  #   source = \"telegraf\"\n\n  ## Configure the handler(s) for processing the provided metrics\n  # [outputs.sensu.metrics]\n  #   handlers = [\"influxdb\",\"elasticsearch\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring ActiveMQ Input Plugin in Telegraf\nDESCRIPTION: Sample TOML configuration for the ActiveMQ input plugin in Telegraf. Specifies connection parameters including URL, authentication credentials, webadmin path, response timeout, and optional TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/activemq/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.activemq]]\n  ## ActiveMQ WebConsole URL\n  url = \"http://127.0.0.1:8161\"\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Required ActiveMQ webadmin root path\n  # webadmin = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Information in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP query, providing detailed synchronization statistics for a remote NTP server. It includes the server address, reference ID, stratum, poll interval, reachability, delay, offset, and jitter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/failed/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: WMI Query for Physical Memory Metrics\nDESCRIPTION: Example configuration for querying physical memory details using WMI, capturing capacity, speed, and device information with specific tag properties\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    namespace = \"root\\\\cimv2\"\n    class_name = \"Win32_PhysicalMemory\"\n    properties = [\n      \"Name\",\n      \"Capacity\",\n      \"DeviceLocator\",\n      \"Manufacturer\",\n      \"PartNumber\",\n      \"Speed\"\n    ]\n    tag_properties = [\"Name\",\"DeviceLocator\",\"Manufacturer\",\"PartNumber\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Application Insights Output in Telegraf (TOML)\nDESCRIPTION: This configuration snippet sets up the Azure Application Insights output plugin for Telegraf. It includes settings for the instrumentation key, endpoint URL, timeout, and context tag sources. The plugin sends metrics to Azure Application Insights for monitoring and analysis.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/application_insights/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics to Azure Application Insights\n[[outputs.application_insights]]\n  ## Instrumentation key of the Application Insights resource.\n  instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n\n  ## Regions that require endpoint modification https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-endpoints\n  # endpoint_url = \"https://dc.services.visualstudio.com/v2/track\"\n\n  ## Timeout for closing (default: 5s).\n  # timeout = \"5s\"\n\n  ## Enable additional diagnostic logging.\n  # enable_diagnostic_logging = false\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## Context Tag Sources add Application Insights context tags to a tag value.\n  ##\n  ## For list of allowed context tag keys see:\n  ## https://github.com/microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n  # [outputs.application_insights.context_tag_sources]\n  #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n  #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Override Processor Plugin in Telegraf\nDESCRIPTION: Demonstrates configuration options for overriding metric names and adding tags in the Telegraf override processor plugin. Allows modification of metric names through name_override, name_prefix, and name_suffix, as well as adding or overwriting tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/override/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Apply metric modifications using override semantics.\n[[processors.override]]\n  ## All modifications on inputs and aggregators can be overridden:\n  # name_override = \"new_name\"\n  # name_prefix = \"new_name_prefix\"\n  # name_suffix = \"new_name_suffix\"\n\n  ## Tags to be added (all values must be strings)\n  # [processors.override.tags]\n  #   additional_tag = \"tag_value\"\n```\n\n----------------------------------------\n\nTITLE: UPSD Input Plugin Configuration in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to configure the `upsd` input plugin in Telegraf. It shows the available options for specifying the NUT server address, port, authentication credentials, and other parameters such as forcing floating point numbers, enabling additional fields and the log level.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/upsd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Monitor UPSes connected via Network UPS Tools\n[[inputs.upsd]]\n  ## A running NUT server to connect to.\n  ## IPv6 addresses must be enclosed in brackets (e.g. \"[::1]\")\n  # server = \"127.0.0.1\"\n  # port = 3493\n  # username = \"user\"\n  # password = \"password\"\n\n  ## Force parsing numbers as floats\n  ## It is highly recommended to enable this setting to parse numbers\n  ## consistently as floats to avoid database conflicts where some numbers are\n  ## parsed as integers and others as floats.\n  # force_float = false\n\n  ## Collect additional fields if they are available for the UPS\n  ## The fields need to be specified as NUT variable names, see\n  ## https://networkupstools.org/docs/developer-guide.chunked/apas02.html\n  ## Wildcards are accepted.\n  # additional_fields = []\n\n  ## Dump information for debugging\n  ## Allows to print the raw variables (and corresponding types) as received\n  ## from the NUT server ONCE for each UPS.\n  ## Please attach this information when reporting issues!\n  # log_level = \"trace\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Secret-Store Plugin in Telegraf\nDESCRIPTION: TOML configuration for setting up the Docker secret store plugin with options for secret path and dynamic secret support\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/docker/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[secretstores.docker]]\n  ## Unique identifier for the secretstore.\n  ## This id can later be used in plugins to reference the secrets\n  ## in this secret-store via @{<id>:<secret_key>} (mandatory)\n  id = \"docker_secretstore\"\n\n  ## Default Path to directory where docker stores the secrets file\n  ## Current implementation in docker compose v2 only allows the following\n  ## value for the path where the secrets are mounted at runtime\n  # path = \"/run/secrets\"\n\n  ## Allow dynamic secrets that are updated during runtime of telegraf\n  ## Dynamic Secrets work only with `file` or `external` configuration\n  ## in `secrets` section of the `docker-compose.yml` file\n  # dynamic = false\n```\n\n----------------------------------------\n\nTITLE: Configuring AAD Authentication for Telegraf SQL Server Plugin\nDESCRIPTION: TOML configuration example for using Azure Active Directory authentication with the Telegraf sqlserver input plugin. This method uses the system-assigned managed identity of the VM to authenticate with Azure SQL Database without requiring a password.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n  servers = [\n    \"Server=<Azure_SQL_Server_Name>.database.windows.net;Port=1433;Database=<Azure_SQL_Database_Name>;app name=telegraf;log=1;\",\n  ]\n  auth_method = \"AAD\"\n```\n\n----------------------------------------\n\nTITLE: Histogram Bucket Representation in Telegraf\nDESCRIPTION: Example of how to format histogram data in Telegraf using tags with the 'le' (less than or equal) convention inspired by Prometheus. It demonstrates proper bucketing with the special '+Inf' tag for values out of defined ranges.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/REVIEWS.md#2025-04-16_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncpu,le=0.0 usage_idle_bucket=0i 1486998330000000000\ncpu,le=50.0 usage_idle_bucket=2i 1486998330000000000\ncpu,le=100.0 usage_idle_bucket=2i 1486998330000000000\ncpu,le=+Inf usage_idle_bucket=2i 1486998330000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Processor in TOML\nDESCRIPTION: Sample configuration for the batch processor plugin showing basic settings including batch tag name and number of batches. Includes optional skip_existing parameter to control handling of pre-existing batch assignments.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/batch/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.batch]]\n  ## The name of the tag to use for adding the batch index\n  batch_tag = \"my_batch\"\n\n  ## The number of batches to create\n  batches = 16\n\n  ## Do not assign metrics with an existing batch assignment to a\n  ## different batch. \n  # skip_existing = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Output Plugin in TOML\nDESCRIPTION: Sample configuration for the Elasticsearch output plugin in TOML format. This is a placeholder as the actual configuration was not provided in the input.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for Elasticsearch output plugin\n# Actual configuration details were not provided in the input\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query for Document Count per Response Code\nDESCRIPTION: This TOML configuration snippet sets up an OpenSearch query to count documents matching a filter query, returning results grouped by response status code. It uses tags to aggregate by response code.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"*\"\n  filter_query = \"downloads\"\n  tags = [\"response.keyword\"]\n  include_missing_tag = false\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Using Logger in Go Plugin\nDESCRIPTION: Demonstrates how to use the Logger in a Telegraf plugin. The example shows logging an error message.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/LOGGING.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\np.Log.Errorf(\"Unable to write to file: %v\", err)\n```\n\n----------------------------------------\n\nTITLE: Accessing Telegraf Help Documentation\nDESCRIPTION: Demonstrates how to access the full list of Telegraf subcommands and flags through the help command. This provides users with comprehensive documentation of available options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/cmd/telegraf/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf help\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf to Monitor Caddy HTTP Server\nDESCRIPTION: TOML configuration for Telegraf to scrape Prometheus metrics from a Caddy HTTP server. Specifies the default URL where Caddy exposes its metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/prometheus/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.prometheus]]\n#   ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:2019/metrics\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Input Plugin with V2 Metadata Endpoint\nDESCRIPTION: Configuration for the ECS input plugin explicitly using the v2 metadata endpoint. This configuration enforces the use of v2 API by setting a specific endpoint URL.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ecs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about ECS containers\n[[inputs.ecs]]\n  ## ECS metadata url.\n  ## Metadata v2 API is used if set explicitly. Otherwise,\n  ## v3 metadata endpoint API is used if available.\n  endpoint_url = \"http://169.254.170.2\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"RUNNING\" state will be captured.\n  ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n  ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n  # container_status_include = []\n  # container_status_exclude = []\n\n  ## ecs labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n  ecs_label_exclude = []\n\n  ## Timeout for queries.\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Translating SNMP OIDs with snmptranslate\nDESCRIPTION: Shell command showing how to check if a numeric OID can be translated to a textual OID using the snmptranslate utility. This is useful for troubleshooting SNMP configuration in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n$ snmptranslate .1.3.6.1.2.1.1.3.0\nDISMAN-EVENT-MIB::sysUpTimeInstance\n```\n\n----------------------------------------\n\nTITLE: Correct Windows Path String Handling in TOML\nDESCRIPTION: Demonstrates proper handling of Windows file paths using both escaped strings and literal strings in TOML.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TOML.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\npath = \"C:\\\\Program Files\\\\\"\npath = 'C:\\Program Files\\'\n```\n\n----------------------------------------\n\nTITLE: Configuring LVM Input Plugin for Telegraf (TOML)\nDESCRIPTION: This TOML snippet configures the LVM input plugin for Telegraf, allowing it to collect metrics about physical volumes, volume groups, and logical volumes. The configuration optionally specifies the use of sudo for executing LVM commands, along with paths to the necessary binary files.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lvm/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n\"[[inputs.lvm]]\\n  use_sudo = false\\n  #pvs_binary = \"/usr/sbin/pvs\"\\n  #vgs_binary = \"/usr/sbin/vgs\"\\n  #lvs_binary = \"/usr/sbin/lvs\"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Wavefront Output in Telegraf (TOML)\nDESCRIPTION: This TOML snippet demonstrates how to configure Telegraf to output metrics in the Wavefront Data Format. It includes options for outputting to stdout, handling of metric and tag names, and toggling prefix conversion. Key parameters include `wavefront_use_strict` for name sanitation and `data_format` for specifying serialization format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/wavefront/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  files = [\"stdout\"]\n\n  ## Use Strict rules to sanitize metric and tag names from invalid characters\n  ## When enabled forward slash (/) and comma (,) will be accepted\n  # wavefront_use_strict = false\n\n  ## point tags to use as the source name for Wavefront (if none found, host will be used)\n  # wavefront_source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"wavefront\"\n  ## Users who wish their prefix paths to not be converted may set the following:\n  ## default behavior (enabled prefix/path conversion):       prod.prefix.name.metric.name\n  ## configurable behavior (disabled prefix/path conversion): prod.prefix_name.metric_name\n  # wavefront_disable_prefix_conversion = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomad Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Nomad input plugin in Telegraf. Includes URL endpoint configuration, response timeout settings, and optional TLS configuration parameters for secure connections.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nomad/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from the Nomad API\n[[inputs.nomad]]\n  ## URL for the Nomad agent\n  # url = \"http://127.0.0.1:4646\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n```\n\n----------------------------------------\n\nTITLE: Configuring WMI Query for Windows Operating System Metrics in Telegraf\nDESCRIPTION: This snippet configures a WMI query to collect metrics about the Windows operating system, including paging file space, virtual memory, OS SKU, and product type. It uses the Win32_OperatingSystem class and specifies properties to collect and tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    class_name = \"Win32_OperatingSystem\"\n    namespace = \"root\\\\cimv2\"\n    properties = [\n      \"Name\",\n      \"Caption\",\n      \"FreeSpaceInPagingFiles\",\n      \"FreeVirtualMemory\",\n      \"OperatingSystemSKU\",\n      \"OSArchitecture\",\n      \"ProductType\"\n    ]\n    tag_properties = [\"Name\",\"Caption\",\"OSArchitecture\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Upstream Check Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Nginx Upstream Check input plugin for Telegraf. It specifies the URL for the status endpoint, HTTP method, headers, timeout, and optional authentication and TLS settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_upstream_check/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nginx_upstream_check]]\n  ## An URL where Nginx Upstream check module is enabled\n  ## It should be set to return a JSON formatted response\n  url = \"http://127.0.0.1/status?format=json\"\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"check.example.com\"\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugepages Input Plugin in TOML\nDESCRIPTION: TOML configuration for the Hugepages input plugin that specifies which huge page types to collect metrics from. Supports 'root', 'per_node', and 'meminfo' types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/hugepages/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gathers huge pages measurements.\n# This plugin ONLY supports Linux\n[[inputs.hugepages]]\n  ## Supported huge page types:\n  ##   - \"root\"     - based on root huge page control directory:\n  ##                  /sys/kernel/mm/hugepages\n  ##   - \"per_node\" - based on per NUMA node directories:\n  ##                  /sys/devices/system/node/node[0-9]*/hugepages\n  ##   - \"meminfo\"  - based on /proc/meminfo file\n  # types = [\"root\", \"per_node\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring DiskIO Input Plugin in Telegraf\nDESCRIPTION: Sample TOML configuration for the DiskIO input plugin. This configuration allows specifying devices to monitor, controls gathering of serial numbers, and enables device metadata tags and name templates for improved device identification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/diskio/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n  ## Devices to collect stats for\n  ## Wildcards are supported except for disk synonyms like '/dev/disk/by-id'.\n  ## ex. devices = [\"sda\", \"sdb\", \"vd*\", \"/dev/disk/by-id/nvme-eui.00123deadc0de123\"]\n  # devices = [\"*\"]\n\n  ## Skip gathering of the disk's serial numbers.\n  # skip_serial_number = true\n\n  ## Device metadata tags to add on systems supporting it (Linux only)\n  ## Use 'udevadm info -q property -n <device>' to get a list of properties.\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n  ## the device. The template may contain variables in the form of '$PROPERTY' or\n  ## '${PROPERTY}'. The first template which does not contain any variables not\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring InLong Output in Telegraf\nDESCRIPTION: Configuration block for setting up Telegraf to send metrics to Apache InLong. Specifies the manager URL, group ID, and stream ID required for InLong integration. Includes optional data format configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/inlong/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send telegraf metrics to Apache Inlong\n[[outputs.inlong]]\n  ## Manager URL to obtain the Inlong data-proxy IP list for sending the data\n  url = \"http://127.0.0.1:8083\"\n\n  ## Unique identifier for the data-stream group\n  group_id = \"telegraf\"  \n\n  ## Unique identifier for the data stream within its group\n  stream_id = \"telegraf\"  \n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Writing Data to InfluxDB Listener via cURL\nDESCRIPTION: Example command showing how to write metrics to the InfluxDB Listener endpoint using cURL with InfluxDB Line Protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb_listener/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -i -XPOST 'http://localhost:8186/write' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'\n```\n\n----------------------------------------\n\nTITLE: Configuring RAS Plugin in TOML\nDESCRIPTION: Configuration settings for the RAS plugin in Telegraf. Specifies the optional path to RASDaemon sqlite3 database with default path at /var/lib/rasdaemon/ras-mc_event.db.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ras/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.ras]]\n  ## Optional path to RASDaemon sqlite3 database.\n  ## Default: /var/lib/rasdaemon/ras-mc_event.db\n  # db_path = \"\"\n```\n\n----------------------------------------\n\nTITLE: Mapping Enum Values in Telegraf Configuration\nDESCRIPTION: This snippet demonstrates how to configure the Enum Processor for mapping field values. It defines the fields to map, the destination field for mapped values, and default values for any unmapped entries.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/enum/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.enum]]\\n  [[processors.enum.mapping]]\\n    ## Names of the fields to map. Globs accepted.\\n    fields = [\\\"status\\\"]\\n\\n    ## Name of the tags to map. Globs accepted.\\n    # tags = [\\\"status\\\"]\\n\\n    ## Destination tag or field to be used for the mapped value.  By default the\\n    ## source tag or field is used, overwriting the original value.\\n    dest = \\\"status_code\\\"\\n\\n    ## Default value to be used for all values not contained in the mapping\\n    ## table.  When unset and no match is found, the original field will remain\\n    ## unmodified and the destination tag or field will not be created.\\n    # default = 0\\n\\n    ## Table of mappings\\n    [processors.enum.mapping.value_mappings]\\n      green = 1\\n      amber = 2\\n      red = 3\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CouchDB Input Plugin in TOML\nDESCRIPTION: Configuration settings for the CouchDB input plugin including host specification and optional HTTP Basic Authentication parameters. The plugin can monitor multiple CouchDB instances simultaneously.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/couchdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read CouchDB Stats from one or more servers\n[[inputs.couchdb]]\n  ## Works with CouchDB stats endpoints out of the box\n  ## Multiple Hosts from which to read CouchDB stats:\n  hosts = [\"http://localhost:8086/_stats\"]\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"telegraf\"\n  # basic_password = \"p@ssw0rd\"\n```\n\n----------------------------------------\n\nTITLE: Configuring http_listener_v2 with form_urlencoded parser for query parsing\nDESCRIPTION: This configuration example demonstrates how to set up the http_listener_v2 input plugin to parse query parameters using the form_urlencoded format. It overrides the metric name and specifies a tag key.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/form_urlencoded/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.http_listener_v2]]\n  name_override = \"mymetric\"\n  service_address = \":8080\"\n  data_source = \"query\"\n  data_format = \"form_urlencoded\"\n  form_urlencoded_tag_keys = [\"tag1\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring CSGO Server Monitoring in TOML\nDESCRIPTION: Configuration snippet for setting up CSGO server monitoring in Telegraf. Allows specification of multiple CSGO servers with their IP addresses, ports, and RCON passwords.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/csgo/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.csgo]]\n  ## Specify servers using the following format:\n  ##    servers = [\n  ##      [\"ip1:port1\", \"rcon_password1\"],\n  ##      [\"ip2:port2\", \"rcon_password2\"],\n  ##    ]\n  #\n  ## If no servers are specified, no data will be collected\n  servers = []\n```\n\n----------------------------------------\n\nTITLE: Setting GODEBUG Environment Variable for Resolving Hostname Issues\nDESCRIPTION: Commands for setting the GODEBUG environment variable to use the cgo resolver instead of Go's pure resolver for name resolution, which can help resolve hostname issues in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/FAQ.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport GODEBUG=netdns=cgo\n```\n\nLANGUAGE: shell\nCODE:\n```\nGODEBUG=netdns=cgo\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamp to Different Format in Telegraf\nDESCRIPTION: Configuration example showing how to convert a timestamp field from ISO format to a different timestamp format with reduced precision. The example trims the seconds and fractional seconds from the timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/timestamp/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.timestamp]]\n  source_timestamp_field = \"timestamp\"\n  source_timestamp_format = \"2006-01-02T15:04:05.999999999Z\"\n  destination_timestamp_format = \"2006-01-02T15:04\"\n```\n\nLANGUAGE: diff\nCODE:\n```\n- metric value=42i,timestamp=\"2024-03-04T10:10:32.123456Z\" 1560540094000000000\n+ metric value=42i,timestamp=\"2024-03-04T10:10\" 1560540094000000000\n```\n\n----------------------------------------\n\nTITLE: Querying Azure SQL Database Metrics\nDESCRIPTION: Specialized metrics collection for Azure SQL Database including resource governance, wait stats, and performance counters specific to cloud deployment.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nsys.dm_io_virtual_file_stats\\nsys.dm_os_memory_clerks\\nsys.dm_user_db_resource_governance\\nsys.dm_os_performance_counters\\nsys.dm_db_wait_stats\\nsys.dm_os_wait_stats\\nsys.dm_exec_sessions\\nsys.dm_exec_requests\\nsys.dm_os_schedulers\n```\n\n----------------------------------------\n\nTITLE: Sample Metrics Input for Homie v4 MQTT Output in Telegraf\nDESCRIPTION: Example of input metrics that will be transformed and published according to the Homie v4.0 specification. These metrics include various device data with different tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mqtt/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nmodbus,source=device\\ 1,location=main\\ building,type=Machine\\ A,status=ok temperature=21.4,serial\\ number=\"324nlk234r5u9834t\",working\\ hours=123i,supplied=true 1676522982000000000\nmodbus,source=device\\ 2,location=main\\ building,type=Machine\\ B,status=offline supplied=false 1676522982000000000\nmodbus,source=device\\ 2,location=main\\ building,type=Machine\\ B,status=online supplied=true,Throughput=12345i,Load\\ [%]=81.2,account\\ no=\"T3L3GrAf\",Temperature=25.38,Voltage=24.1,Current=100 1676542982000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring PowerDNS Recursor Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the PowerDNS Recursor input plugin in Telegraf. It specifies the path to the Recursor control socket, socket directory, permissions, and control protocol version.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/powerdns_recursor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.powerdns_recursor]]\n  ## Path to the Recursor control socket.\n  unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n\n  ## Directory to create receive socket.  This default is likely not writable,\n  ## please reference the full plugin documentation for a recommended setup.\n  # socket_dir = \"/var/run/\"\n  ## Socket permissions for the receive socket.\n  # socket_mode = \"0666\"\n\n  ## The version of the PowerDNS control protocol to use. You will have to\n  ## change this based on your PowerDNS Recursor version, see below:\n  ## Version 1: PowerDNS <4.5.0\n  ## Version 2: PowerDNS 4.5.0 - 4.5.11\n  ## Version 3: PowerDNS >=4.6.0\n  ## By default this is set to 1.\n  # control_protocol_version = 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Starlark Processor for ctrlX Data Layer Output in TOML\nDESCRIPTION: This snippet demonstrates how to configure a Starlark processor to handle JSON output from the ctrlX Data Layer input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ctrlx_datalayer/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.ctrlx_datalayer.subscription]]\n   measurement = \"osci\"\n   nodes = [\n     {address=\"oscilloscope/instances/Osci_PLC/rec-values/allsignals\"},\n   ]\n   output_json_string = true\n\n[[processors.starlark]]\n   namepass = [\n      'osci',\n   ]\n   script = \"oscilloscope.star\"\n```\n\n----------------------------------------\n\nTITLE: Filter metrics with status 'warning' or 'failure'\nDESCRIPTION: This configuration shows how to filter metrics with a status of either 'warning' or 'failure'. The `namepass` option is used to only process metrics named \"machine\". A rule is defined to pass metrics that have a `status` tag with the value 'warning' or 'failure'.  The `default` action is set to \"drop\", so metrics that don't match the rule are dropped.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.filter]]\\n  namepass = [\\\"machine\\\"]\\n  default = \\\"drop\\\"\\n\\n  [[processors.filter.rule]]\\n    tags = {\\\"status\\\" = [\\\"warning\\\", \\\"failure\\\"]}\\n    action = \\\"pass\\\"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Collectd Parser Plugin in Telegraf\nDESCRIPTION: This snippet demonstrates the output format of the Collectd Parser Plugin in Telegraf. It shows how memory metrics are parsed and stored, including tags for type and type_instance, and the corresponding values and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/collectd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmemory,type=memory,type_instance=buffered value=2520051712 1560455990829955922\nmemory,type=memory,type_instance=used value=3710791680 1560455990829955922\nmemory,type=memory,type_instance=buffered value=2520047616 1560455980830417318\nmemory,type=memory,type_instance=cached value=9472626688 1560455980830417318\nmemory,type=memory,type_instance=slab_recl value=2088894464 1560455980830417318\nmemory,type=memory,type_instance=slab_unrecl value=146984960 1560455980830417318\nmemory,type=memory,type_instance=free value=2978258944 1560455980830417318\nmemory,type=memory,type_instance=used value=3707047936 1560455980830417318\n```\n\n----------------------------------------\n\nTITLE: Configuring Counters Refresh Interval\nDESCRIPTION: Sets the interval for matching and refreshing configured counters, with a default of 1 minute to avoid high CPU load\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_3\n\nLANGUAGE: TOML\nCODE:\n```\nCountersRefreshInterval=1m\n```\n\n----------------------------------------\n\nTITLE: Aggregator Output Format\nDESCRIPTION: Example output showing both original metrics and aggregated counts of response codes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/valuecounter/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\naccess,url=/some/path,path=/tmp/tst.log,host=localhost.localdomain response=\"200\" 1511948755991487011\naccess,url=/some/path,path=/tmp/tst.log,host=localhost.localdomain response=\"401\" 1511948755991522282\naccess,url=/some/path,path=/tmp/tst.log,host=localhost.localdomain response=\"200\" 1511948755991531697\naccess,path=/tmp/tst.log,host=localhost.localdomain,url=/some/path response_200=2i,response_401=1i 1511948761000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Date Processor Plugin in Telegraf\nDESCRIPTION: Configuration sample for the date processor plugin in Telegraf. It shows how to add a month tag to metrics with options for date format, timezone, and offset settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/date/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Dates measurements, tags, and fields that pass through this filter.\n[[processors.date]]\n  ## New tag to create\n  tag_key = \"month\"\n\n  ## New field to create (cannot set both field_key and tag_key)\n  # field_key = \"month\"\n\n  ## Date format string, must be a representation of the Go \"reference time\"\n  ## which is \"Mon Jan 2 15:04:05 -0700 MST 2006\".\n  date_format = \"Jan\"\n\n  ## If destination is a field, date format can also be one of\n  ## \"unix\", \"unix_ms\", \"unix_us\", or \"unix_ns\", which will insert an integer field.\n  # date_format = \"unix\"\n\n  ## Offset duration added to the date string when writing the new tag.\n  # date_offset = \"0s\"\n\n  ## Timezone to use when creating the tag or field using a reference time\n  ## string.  This can be set to one of \"UTC\", \"Local\", or to a location name\n  ## in the IANA Time Zone database.\n  ##   example: timezone = \"America/Los_Angeles\"\n  # timezone = \"UTC\"\n```\n\n----------------------------------------\n\nTITLE: Simple Data Type Configuration Example\nDESCRIPTION: Configuration for monitoring memory metrics with simple numeric values. Shows subscription setup with measurement tags and node definitions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ctrlx_datalayer/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.ctrlx_datalayer.subscription]]\n   measurement=\"memory\"\n   [inputs.ctrlx_datalayer.subscription.tags]\n      sub_tag1=\"memory_tag1\"\n      sub_tag2=\"memory_tag2\"\n\n   [[inputs.ctrlx_datalayer.subscription.nodes]]\n      name   =\"available\"\n      address=\"framework/metrics/system/memavailable-mb\"\n      [inputs.ctrlx_datalayer.subscription.nodes.tags]\n         node_tag1=\"memory_available_tag1\"\n         node_tag2=\"memory_available_tag2\"\n\n   [[inputs.ctrlx_datalayer.subscription.nodes]]\n      name   =\"used\"\n      address=\"framework/metrics/system/memused-mb\"\n      [inputs.ctrlx_datalayer.subscription.nodes.tags]\n         node_tag1=\"memory_used_node_tag1\"\n         node_tag2=\"memory_used_node_tag2\"\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf manually with Go\nDESCRIPTION: Command to run Telegraf manually during development using Go's run command with a configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEBUG.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngo run ./cmd/telegraf --config config.toml\n```\n\n----------------------------------------\n\nTITLE: Libvirt Plugin Configuration in TOML\nDESCRIPTION: This TOML configuration defines the basic structure for the `inputs.libvirt` plugin in Telegraf. It showcases the configurable options such as `domains`, `libvirt_uri`, `statistics_groups`, and `additional_statistics`. These options control which virtual machines and metrics are collected.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/libvirt/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# The libvirt plugin collects statistics from virtualized guests using virtualization libvirt API.\n[[inputs.libvirt]]\n     ## Domain names from which libvirt gather statistics.\n     ## By default (empty or missing array) the plugin gather statistics from each domain registered in the host system.\n     # domains = []\n\n     ## Libvirt connection URI with hypervisor.\n     ## The plugin supports multiple transport protocols and approaches which are configurable via the URI.\n     ## The general URI form: driver[+transport]://[username@][hostname][:port]/[path][?extraparameters]\n     ## Supported transport protocols: ssh, tcp, tls, unix\n     ## URI examples for each type of transport protocol:\n     ## 1. SSH:  qemu+ssh://<USER@IP_OR_HOSTNAME>/system?keyfile=/<PATH_TO_PRIVATE_KEY>&known_hosts=/<PATH_TO_known_hosts>\n     ## 2. TCP:  qemu+tcp://<IP_OR_HOSTNAME>/system\n     ## 3. TLS:  qemu+tls://<HOSTNAME>/system?pkipath=/certs_dir/<COMMON_LOCATION_OF_CACERT_AND_SERVER_CLIENT_CERTS>\n     ## 4. UNIX: qemu+unix:///system?socket=/<PATH_TO_libvirt-sock>\n     ## Default URI is qemu:///system\n     # libvirt_uri = \"qemu:///system\"\n\n     ## Statistics groups for which libvirt plugin will gather statistics.\n     ## Supported statistics groups: state, cpu_total, balloon, vcpu, interface, block, perf, iothread, memory, dirtyrate\n     ## Empty array means no metrics for statistics groups will be exposed by the plugin.\n     ## By default the plugin will gather all available statistics.\n     # statistics_groups = [\"state\", \"cpu_total\", \"balloon\", \"vcpu\", \"interface\", \"block\", \"perf\", \"iothread\", \"memory\", \"dirtyrate\"]\n\n     ## A list containing additional statistics to be exposed by libvirt plugin.\n     ## Supported additional statistics: vcpu_mapping\n     ## By default (empty or missing array) the plugin will not collect additional statistics.\n     # additional_statistics = []\n\n\n```\n\n----------------------------------------\n\nTITLE: Using Fieldinclude and Fieldexclude in Telegraf CPU and Disk Inputs\nDESCRIPTION: This configuration shows how to use fieldinclude and fieldexclude filters with CPU and disk inputs in Telegraf. It excludes guest and steal CPU usage metrics and only includes inode-related metrics for disks.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n# Drop all metrics for guest & steal CPU usage\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  fieldexclude = [\"usage_guest\", \"usage_steal\"]\n\n# Only store inode related metrics for disks\n[[inputs.disk]]\n  fieldinclude = [\"inodes*\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Linux Privileged Port Permissions\nDESCRIPTION: Shell command to enable CAP_NET_BIND_SERVICE capability for Telegraf to listen on privileged ports.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp_trap/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsetcap cap_net_bind_service=+ep /usr/bin/telegraf\n```\n\n----------------------------------------\n\nTITLE: MQTT Consumer Topic Parsing Configuration Example\nDESCRIPTION: Example configuration demonstrating how to extract tag values from MQTT topics using the topic parsing feature. Shows how to map topic elements to measurements, tags, and fields with type conversion.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mqtt_consumer/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.mqtt_consumer]]\n  ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n  ## clusters or standalone servers, use a separate plugin instance.\n  ##   example: servers = [\"tcp://localhost:1883\"]\n  ##            servers = [\"ssl://localhost:1883\"]\n  ##            servers = [\"ws://localhost:1883\"]\n  servers = [\"tcp://127.0.0.1:1883\"]\n\n  ## Topics that will be subscribed to.\n  topics = [\n    \"telegraf/+/cpu/23\",\n  ]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"value\"\n  data_type = \"float\"\n\n  [[inputs.mqtt_consumer.topic_parsing]]\n    topic = \"telegraf/one/cpu/23\"\n    measurement = \"_/_/measurement/_\"\n    tags = \"tag/_/_/_\"\n    fields = \"_/_/_/test\"\n    [inputs.mqtt_consumer.topic_parsing.types]\n      test = \"int\"\n```\n\n----------------------------------------\n\nTITLE: Using Constants in Starlark Scripts\nDESCRIPTION: Reusing Starlark scripts with different parameters using configuration constants\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/starlark/README.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef apply(metric):\n    if metric.fields[somecustomstr] >= somecustomnum:\n        metric.fields.clear()\n    return metric\n```\n\n----------------------------------------\n\nTITLE: Configuring Suricata Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet specifies settings for the Suricata input plugin in Telegraf. It includes options for the data source, delimiter for keys, metric version, and alert settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/suricata/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"\"\"\n# Suricata stats and alerts plugin\n[[inputs.suricata]]\n  ## Source\n  ## Data sink for Suricata stats log. This is expected to be a filename of a\n  ## unix socket to be created for listening.\n  # source = \"/var/run/suricata-stats.sock\"\n\n  ## Delimiter\n  ## Used for flattening field keys, e.g. subitem \"alert\" of \"detect\" becomes\n  ## \"detect_alert\" when delimiter is \"_\".\n  # delimiter = \"_\"\n\n  ## Metric version\n  ## Version 1 only collects stats and optionally will look for alerts if\n  ## the configuration setting alerts is set to true.\n  ## Version 2 parses any event type message by default and produced metrics\n  ## under a single metric name using a tag to differentiate between event\n  ## types. The timestamp for the message is applied to the generated metric.\n  ## Additional tags and fields are included as well.\n  # version = \"1\"\n\n  ## Alerts\n  ## In metric version 1, only status is captured by default, alerts must be\n  ## turned on with this configuration option. This option does not apply for\n  ## metric version 2.\n  # alerts = false\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf vSphere Plugin for Historical vSAN Metrics\nDESCRIPTION: Configuration for collecting historical vSAN performance metrics with a 5-minute (300-second) collection interval. Focuses on excluding other metric types and specifically including vSAN performance metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n# Historical instance\n[[inputs.vsphere]]\n\n  interval = \"300s\"\n  vcenters = [ \"https://someaddress/sdk\" ]\n  username = \"someuser@vsphere.local\"\n  password = \"secret\"\n\n  insecure_skip_verify = true\n  force_discover_on_init = true\n\n  # Exclude all other metrics\n  vm_metric_exclude = [\"*\"]\n  datastore_metric_exclude = [\"*\"]\n  datacenter_metric_exclude = [\"*\"]\n  host_metric_exclude = [\"*\"]\n  cluster_metric_exclude = [\"*\"]\n\n  vsan_metric_include = [ \"performance.*\" ]\n  vsan_metric_exclude = [ ]\n  vsan_metric_skip_verify = false\n\n  collect_concurrency = 5\n  discover_concurrency = 5\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluentd Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Fluentd input plugin. Specifies the endpoint URL for accessing Fluentd metrics and allows excluding specific plugin types from monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fluentd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.fluentd]]\n  ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n  ##\n  ## Endpoint:\n  ## - only one URI is allowed\n  ## - https is not supported\n  endpoint = \"http://localhost:24220/api/plugins.json\"\n\n  ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n  exclude = [\n    \"monitor_agent\",\n    \"dummy\",\n  ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Unbound Input Plugin in Telegraf\nDESCRIPTION: This TOML code snippet demonstrates how to configure the Unbound input plugin in Telegraf.  It includes options for specifying the Unbound server address, enabling sudo access, and customizing the collection of thread metrics and recursive query time histograms. The `server` parameter specifies the address of the Unbound server.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/unbound/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# A plugin to collect stats from the Unbound DNS resolver\n[[inputs.unbound]]\n  ## Address of server to connect to, read from unbound conf default, optionally ':port'\n  ## Will lookup IP if given a hostname\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the unbound-control binary can be overridden with:\n  # binary = \"/usr/sbin/unbound-control\"\n\n  ## The default location of the unbound config file can be overridden with:\n  # config_file = \"/etc/unbound/unbound.conf\"\n\n  ## The default timeout of 1s can be overridden with:\n  # timeout = \"1s\"\n\n  ## When set to true, thread metrics are tagged with the thread id.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  thread_as_tag = false\n\n  ## Collect metrics with the histogram of the recursive query times:\n  # histogram = false\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Neoom Beaam Input Plugin for Telegraf\nDESCRIPTION: Configuration template for the Neoom Beaam Input Plugin that specifies how to connect to a Beaam gateway. It includes settings for the gateway address, authentication token, configuration refresh behavior, and TLS options for secure connections.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/neoom_beaam/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read energy data from the local Neoom Beaam gateway\n[[inputs.neoom_beaam]]\n  ## Address of the gateway\n  # address = \"https://10.10.10.10\"\n\n  ## Bearer token for accessing the data\n  # token = \"\"\n\n  ## Enforce refreshing the site configuration in each gather cycle\n  # refresh_configuration = false\n\n  ## Optional TLS Config\n  ## Set to true/false to enforce TLS being enabled/disabled. If not set,\n  ## enable TLS only if any of the other options are specified.\n  # tls_enable =\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Password for the key file if it is encrypted\n  # tls_key_pwd = \"\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Minimal TLS version to accept by the client\n  # tls_min_version = \"TLS12\"\n  ## List of ciphers to accept, by default all secure ciphers will be accepted\n  ## See https://pkg.go.dev/crypto/tls#pkg-constants for supported values.\n  ## Use \"all\", \"secure\" and \"insecure\" to add all support ciphers, secure\n  ## suites or insecure suites respectively.\n  # tls_cipher_suites = [\"secure\"]\n  ## Renegotiation method, \"never\", \"once\" or \"freely\"\n  # tls_renegotiation_method = \"never\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Accessing Telegraf Help Documentation\nDESCRIPTION: Command to display the full list of Telegraf subcommands and flags. This provides comprehensive information about all available options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/COMMANDS_AND_FLAGS.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf help\n```\n\n----------------------------------------\n\nTITLE: Configuring Disk Input Plugin with TOML in Telegraf\nDESCRIPTION: TOML configuration for the Disk Input Plugin that specifies mount points to monitor and filesystem types to ignore. This configuration allows for customizing which mount points are included and which filesystem types are excluded from monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n  ## Ignore mount points by mount options.\n  ## The 'mount' command reports options of all mounts in parathesis.\n  ## Bind mounts can be ignored with the special 'bind' option.\n  # ignore_mount_opts = []\n```\n\n----------------------------------------\n\nTITLE: Example DNS Query Output Format\nDESCRIPTION: Sample output showing the metric format for a successful DNS query, including tags for domain, result code, record type, and server, along with measurement values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dns_query/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndns_query,domain=google.com,rcode=NOERROR,record_type=A,result=success,server=127.0.0.1 rcode_value=0i,result_code=0i,query_time_ms=0.13746 1550020750001000000\n```\n\n----------------------------------------\n\nTITLE: WMI Query for Computer System Metrics\nDESCRIPTION: Comprehensive configuration for gathering computer system metrics including processors, memory, and system details as tags\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    namespace = \"root\\\\cimv2\"\n    class_name = \"Win32_ComputerSystem\"\n    properties = [\n      \"Name\",\n      \"Domain\",\n      \"Manufacturer\",\n      \"Model\",\n      \"NumberOfLogicalProcessors\",\n      \"NumberOfProcessors\",\n      \"TotalPhysicalMemory\"\n    ]\n    tag_properties = [\"Name\",\"Domain\",\"Manufacturer\",\"Model\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring AMD ROCm SMI Plugin in TOML\nDESCRIPTION: Configuration settings for the AMD ROCm SMI input plugin. Specifies optional settings for binary path and polling timeout.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/amd_rocm_smi/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.amd_rocm_smi]]\n  ## Optional: path to rocm-smi binary, defaults to $PATH via exec.LookPath\n  # bin_path = \"/opt/rocm/bin/rocm-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring InfiniBand Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the InfiniBand input plugin. Allows enabling/disabling RDMA counter collection with minimal configuration required.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/infiniband/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gets counters from all InfiniBand cards and ports installed\n# This plugin ONLY supports Linux\n[[inputs.infiniband]]\n  # no configuration\n\n  ## Collect RDMA counters\n  # gather_rdma = false\n```\n\n----------------------------------------\n\nTITLE: Configuring PowerDNS Input Plugin in TOML\nDESCRIPTION: TOML configuration for the PowerDNS input plugin specifying unix socket paths for metric collection. If no servers are specified, it defaults to '/var/run/pdns.controlsocket'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/powerdns/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or many PowerDNS servers\n[[inputs.powerdns]]\n  # An array of sockets to gather stats about.\n  # Specify a path to unix socket.\n  #\n  # If no servers are specified, then '/var/run/pdns.controlsocket' is used as the path.\n  unix_sockets = [\"/var/run/pdns.controlsocket\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Input Plugin\nDESCRIPTION: This snippet demonstrates how to configure the Logstash Input Plugin for Telegraf using a TOML file. The configuration includes specifying the Logstash API endpoint URL, enabling collection of different metrics components, and setting up optional HTTP authentication, TLS settings, and proxy configurations. It supports monitoring Logstash 5+ instances. Key parameters include the 'url', 'collect', and 'timeout'. The snippet also includes settings for Basic Auth and TLS for secure communication.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/logstash/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics exposed by Logstash\n[[inputs.logstash]]\n  ## The URL of the exposed Logstash API endpoint.\n  url = \"http://127.0.0.1:9600\"\n\n  ## Use Logstash 5 single pipeline API, set to true when monitoring\n  ## Logstash 5.\n  # single_pipeline = false\n\n  ## Enable optional collection components.  Can contain\n  ## \"pipelines\", \"process\", and \"jvm\".\n  # collect = [\"pipelines\", \"process\", \"jvm\"]\n\n  ## Timeout for HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Use TLS but skip chain & host verification.\n  # insecure_skip_verify = false\n\n  ## If 'use_system_proxy' is set to true, Telegraf will check env vars such as\n  ## HTTP_PROXY, HTTPS_PROXY, and NO_PROXY (or their lowercase counterparts).\n  ## If 'use_system_proxy' is set to false (default) and 'http_proxy_url' is\n  ## provided, Telegraf will use the specified URL as HTTP proxy.\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## Optional HTTP headers.\n  # [inputs.logstash.headers]\n  #   \"X-Special-Header\" = \"Special-Value\"\n```\n\n----------------------------------------\n\nTITLE: Mapping Response to Metric Name using Aliases - TOML\nDESCRIPTION: This snippet demonstrates how to configure aliases for the GNMI input plugin in Telegraf to manually map responses to metric names when the default cannot be determined. The `addresses` specify the GNMI endpoints, and the `aliases` section allows for overriding the metric names based on the incoming response paths.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/gnmi/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.gnmi]]\n  addresses     = [\"...\"]\n\n  [inputs.gnmi.aliases]\n    memory = \"/components\"\n\n  [[inputs.gnmi.subscription]]\n    name = \"memory\"\n    origin = \"openconfig\"\n    path = \"/junos/system/linecard/cpu/memory\"\n    subscription_mode = \"sample\"\n    sample_interval = \"60s\"\n```\n\n----------------------------------------\n\nTITLE: Zabbix Trap Format Example - Simple Metric\nDESCRIPTION: Example showing how a simple Telegraf metric is converted into Zabbix trap format\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmeasurement,host=hostname valueA=0,valueB=1\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"host\": \"hostname\", \"key\": \"telegraf.measurement.valueA\", \"value\": \"0\"}\n{\"host\": \"hostname\", \"key\": \"telegraf.measurement.valueB\", \"value\": \"1\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf BIND Input Plugin\nDESCRIPTION: TOML configuration for the BIND input plugin specifying URLs, timeout, and other collection parameters for gathering nameserver statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bind/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.bind]]\n  ## An array of BIND XML statistics URI to gather stats.\n  ## Default is \"http://localhost:8053/xml/v3\".\n  # urls = [\"http://localhost:8053/xml/v3\"]\n  # gather_memory_contexts = false\n  # gather_views = false\n\n  ## Report xml v3 counters as integers instead of unsigned for backward\n  ## compatibility. Set this to false as soon as possible!\n  ## Values are clipped if exceeding the integer range.\n  # report_counters_as_int = true\n\n  ## Timeout for http requests made by bind nameserver\n  # timeout = \"4s\"\n```\n\n----------------------------------------\n\nTITLE: Zabbix Sender JSON for Metrics with Tags\nDESCRIPTION: JSON structure sent to Zabbix server for metrics with tags, showing how tag values are incorporated into the trapper key.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"request\":\"sender data\",\n  \"data\": [\n    {\n      \"host\":\"myHost\",\n      \"key\":\"telegraf.docker_container_net.rx_errors[laughing_babbage]\",\n      \"value\":\"0\",\n      \"clock\":15227640380\n    },\n    {\n      \"host\":\"myHost\",\n      \"key\":\"telegraf.docker_container_net.tx_errors[laughing_babbage]\",\n      \"value\":\"0\",\n      \"clock\":15227640380\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Processes Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Telegraf processes input plugin. Includes options for using sudo with the ps command on BSD systems.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/processes/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.processes]]\n  ## Use sudo to run ps command on *BSD systems. Linux systems will read\n  ## /proc, so this does not apply there.\n  # use_sudo = false\n```\n\n----------------------------------------\n\nTITLE: Basic Jolokia2 Proxy Configuration in TOML\nDESCRIPTION: Basic configuration example for the Jolokia2 proxy input plugin showing required settings for URL, target, and metric collection including optional parameters for authentication and timing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jolokia2_proxy/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.jolokia2_proxy]]\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  ## Proxy agent\n  url = \"http://localhost:8080/jolokia\"\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional origin URL to include as a header in the request. Some endpoints\n  ## may reject an empty origin.\n  # origin = \"\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add proxy targets to query\n  # default_target_username = \"\"\n  # default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  ## Add metrics to read\n  [[inputs.jolokia2_proxy.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\n----------------------------------------\n\nTITLE: Object Data Type Configuration Example\nDESCRIPTION: Configuration for handling JSON object data types. Shows setup for monitoring motion control axes with complex object structures.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ctrlx_datalayer/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.ctrlx_datalayer.subscription]]\n   measurement=\"motion\"\n   nodes=[\n      {name=\"linear\", address=\"motion/axs/Axis_1/state/values/actual\"},\n      {name=\"rotational\", address=\"motion/axs/Axis_2/state/values/actual\"},\n   ]\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamp to Unix Format in Telegraf\nDESCRIPTION: Configuration example showing how to convert a timestamp field from ISO format to unix timestamp format. The input ISO timestamp \"2024-03-04T10:10:32.123456Z\" is converted to a unix timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/timestamp/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.timestamp]]\n  source_timestamp_field = \"timestamp\"\n  source_timestamp_format = \"2006-01-02T15:04:05.999999999Z\"\n  destination_timestamp_format = \"unix\"\n```\n\nLANGUAGE: diff\nCODE:\n```\n- metric value=42i,timestamp=\"2024-03-04T10:10:32.123456Z\" 1560540094000000000\n+ metric value=42i,timestamp=1709547032 1560540094000000000\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Logfmt Data Transformation in Telegraf\nDESCRIPTION: This example shows how the logfmt parser transforms input data. It illustrates the conversion of a logfmt-formatted log line into a structured metric with tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/logfmt/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n- method=GET host=example.org ts=2018-07-24T19:43:40.275Z connect=4ms service=8ms status=200 bytes=1653\n+ logfmt,host=example.org,method=GET ts=\"2018-07-24T19:43:40.275Z\",connect=\"4ms\",service=\"8ms\",status=200i,bytes=1653i\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch OS Metrics in Telegraf\nDESCRIPTION: This snippet defines OS-level metrics from the Elasticsearch node, tracking CPU load, memory usage, and swap statistics to understand system performance within Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n  - elasticsearch_os\n  - tags:\n    - cluster_name\n    - node_attribute_ml.enabled\n    - node_attribute_ml.machine_memory\n    - node_attribute_ml.max_open_jobs\n    - node_attribute_xpack.installed\n    - node_host\n    - node_id\n    - node_name\n  - fields:\n    - cgroup_cpu_cfs_period_micros (float)\n    - cgroup_cpu_cfs_quota_micros (float)\n    - cgroup_cpu_stat_number_of_elapsed_periods (float)\n    - cgroup_cpu_stat_number_of_times_throttled (float)\n    - cgroup_cpu_stat_time_throttled_nanos (float)\n    - cgroup_cpuacct_usage_nanos (float)\n    - cpu_load_average_15m (float)\n    - cpu_load_average_1m (float)\n    - cpu_load_average_5m (float)\n    - cpu_percent (float)\n    - mem_free_in_bytes (float)\n    - mem_free_percent (float)\n    - mem_total_in_bytes (float)\n    - mem_used_in_bytes (float)\n    - mem_used_percent (float)\n    - swap_free_in_bytes (float)\n    - swap_total_in_bytes (float)\n    - swap_used_in_bytes (float)\n    - timestamp (float)\n```\n\n----------------------------------------\n\nTITLE: Multiple Template Transformation Examples\nDESCRIPTION: Demonstrates how different statsd metrics are transformed using the glob pattern templates, applying different transformations based on metric prefixes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/README.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncpu.load.us-west:100|g\n=> cpu_load,region=us-west 100\n\nmem.cached.localhost:256|g\n=> mem_cached,host=localhost 256\n```\n\n----------------------------------------\n\nTITLE: Configuring MessagePack Output in Telegraf\nDESCRIPTION: This snippet illustrates the configuration needed to output metrics in MessagePack format. It includes file paths and specifies the data format, ensuring users have the correct settings for their Telegraf outputs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/msgpack/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"msgpack\"\n```\n\n----------------------------------------\n\nTITLE: Generating Heap Visualization\nDESCRIPTION: Command to generate a PNG visualization of the heap memory usage\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROFILING.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngo tool pprof -png http://localhost:6060/debug/pprof/heap > heap.png\n```\n\n----------------------------------------\n\nTITLE: Configuring Kernel Input Plugin in TOML\nDESCRIPTION: Configuration snippet for the kernel input plugin showing optional collection parameters for KSM and PSI metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kernel/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Plugin to collect various Linux kernel statistics.\n# This plugin ONLY supports Linux\n[[inputs.kernel]]\n  ## Additional gather options\n  ## Possible options include:\n  ## * ksm - kernel same-page merging\n  ## * psi - pressure stall information\n  # collect = []\n```\n\n----------------------------------------\n\nTITLE: Custom Pattern with Built-in Conversion - TOML\nDESCRIPTION: Example showing how to combine built-in timestamp conversion with custom patterns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/grok/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  grok_patterns = [\"%{TS_UNIX:timestamp:ts-unix} value=%{NUMBER:value:int}\"]\n  grok_custom_patterns = '''\n    TS_UNIX %{DAY} %{MONTH} %{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{TZ} %{YEAR}\n  '''\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Tags and Fields in Starlark\nDESCRIPTION: Demonstrating different methods of iterating over metric tags and fields\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/starlark/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef apply(metric):\n    # Fastest iteration (read-only)\n    for k in metric.tags:\n        pass\n\n    # Iteration allowing modification\n    for k, v in metric.tags.items():\n        pass\n    return metric\n```\n\n----------------------------------------\n\nTITLE: Displaying Telegraf SMART Monitoring Output Format\nDESCRIPTION: Sample output showing how Telegraf formats SMART device monitoring data. The output includes three measurements: smart_device with overall health metrics, and two smart_attribute records showing detailed SMART attributes with IDs, values, thresholds and statuses.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nsmart_device,enabled=Enabled,host=mbpro.local,device=rdisk0,model=APPLE\\ SSD\\ SM0512F,serial_no=S1K5NYCD964433,wwn=5002538655584d30,capacity=500277790720 udma_crc_errors=0i,exit_status=0i,health_ok=true,read_error_rate=0i,temp_c=40i 1502536854000000000\nsmart_attribute,capacity=500277790720,device=rdisk0,enabled=Enabled,fail=-,flags=-O-RC-,host=mbpro.local,id=199,model=APPLE\\ SSD\\ SM0512F,name=UDMA_CRC_Error_Count,serial_no=S1K5NYCD964433,wwn=5002538655584d30 exit_status=0i,raw_value=0i,threshold=0i,value=200i,worst=200i 1502536854000000000\nsmart_attribute,capacity=500277790720,device=rdisk0,enabled=Enabled,fail=-,flags=-O---K,host=mbpro.local,id=199,model=APPLE\\ SSD\\ SM0512F,name=Unknown_SSD_Attribute,serial_no=S1K5NYCD964433,wwn=5002538655584d30 exit_status=0i,raw_value=0i,threshold=0i,value=100i,worst=100i 1502536854000000000\n```\n\n----------------------------------------\n\nTITLE: Modifying BSD/Darwin Kernel Limits for UDP (Shell)\nDESCRIPTION: This snippet provides the command to set the kernel limit for socket buffers on BSD/Darwin systems, ensuring that the system can handle large amounts of UDP traffic efficiently.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socket_listener/README.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n\"\"\"\nsysctl -w kern.ipc.maxsockbuf=9646900\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Immutable Data Tables in TimescaleDB for Telegraf\nDESCRIPTION: Configuration for PostgreSQL-compatible databases that don't allow modification of table schema after creation. Creates and manages views to join tables together when schema changes are needed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/postgresql/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\ntags_as_foreign_keys = true\nschema = 'telegraf'\ncreate_templates = [\n    '''CREATE TABLE {{ .table }} ({{ .allColumns }})''',\n    '''SELECT create_hypertable({{ .table|quoteLiteral }}, 'time', chunk_time_interval => INTERVAL '7d')''',\n    '''ALTER TABLE {{ .table }} SET (timescaledb.compress, timescaledb.compress_segmentby = 'tag_id')''',\n    '''SELECT add_compression_policy({{ .table|quoteLiteral }}, INTERVAL '14d')''',\n    '''CREATE VIEW {{ .table.WithSuffix \"_data\" }} AS SELECT {{ .allColumns.Selectors | join \",\" }} FROM {{ .table }}''',\n    '''CREATE VIEW {{ .table.WithSchema \"public\" }} AS SELECT time, {{ (.tagTable.Columns.Tags.Concat .allColumns.Fields).Identifiers | join \",\" }} FROM {{ .table.WithSuffix \"_data\" }} t, {{ .tagTable }} tt WHERE t.tag_id = tt.tag_id''',\n]\nadd_column_templates = [\n    '''ALTER TABLE {{ .table }} RENAME TO {{ (.table.WithSuffix \"_\" .table.Columns.Hash).WithSchema \"\" }}''',\n    '''ALTER VIEW {{ .table.WithSuffix \"_data\" }} RENAME TO {{ (.table.WithSuffix \"_\" .table.Columns.Hash \"_data\").WithSchema \"\" }}''',\n    '''DROP VIEW {{ .table.WithSchema \"public\" }}''',\n\n    '''CREATE TABLE {{ .table }} ({{ .allColumns }})''',\n    '''SELECT create_hypertable({{ .table|quoteLiteral }}, 'time', chunk_time_interval => INTERVAL '7d')''',\n    '''ALTER TABLE {{ .table }} SET (timescaledb.compress, timescaledb.compress_segmentby = 'tag_id')''',\n    '''SELECT add_compression_policy({{ .table|quoteLiteral }}, INTERVAL '14d')''',\n    '''CREATE VIEW {{ .table.WithSuffix \"_data\" }} AS SELECT {{ .allColumns.Selectors | join \",\" }} FROM {{ .table }} UNION ALL SELECT {{ (.allColumns.Union .table.Columns).Selectors | join \",\" }} FROM {{ .table.WithSuffix \"_\" .table.Columns.Hash \"_data\" }}''',\n    '''CREATE VIEW {{ .table.WithSchema \"public\" }} AS SELECT time, {{ (.tagTable.Columns.Tags.Concat .allColumns.Fields).Identifiers | join \",\" }} FROM {{ .table.WithSuffix \"_data\" }} t, {{ .tagTable }} tt WHERE t.tag_id = tt.tag_id''',\n]\ntag_table_add_column_templates = [\n    '''ALTER TABLE {{ .table }} ADD COLUMN IF NOT EXISTS {{ .columns|join \", ADD COLUMN IF NOT EXISTS \" }}''',\n    '''DROP VIEW {{ .metricTable.WithSchema \"public\" }}''',\n    '''CREATE VIEW {{ .metricTable.WithSchema \"public\" }} AS SELECT time, {{ (.allColumns.Tags.Concat .metricTable.Columns.Fields).Identifiers | join \",\" }} FROM {{ .metricTable.WithSuffix \"_data\" }} t, {{ .table }} tt WHERE t.tag_id = tt.tag_id''',\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Radius Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Radius input plugin, including server specifications, authentication credentials, and timeout settings. Defines how to set up server endpoints, authentication details, and request parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/radius/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.radius]]\n  ## An array of Server IPs and ports to gather from. If none specified, defaults to localhost.\n  servers = [\"127.0.0.1:1812\",\"hostname.domain.com:1812\"]\n\n  ## Credentials for radius authentication.\n  username = \"myuser\"\n  password = \"mypassword\"\n  secret = \"mysecret\"\n\n  ## Request source server IP, normally the server running telegraf.\n  ## This corresponds to Radius' NAS-IP-Address.\n  # request_ip = \"127.0.0.1\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Riemann Output Plugin in TOML\nDESCRIPTION: Sample configuration for the Riemann output plugin showing all available settings including URL, TTL, separator, tags, and timeout options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/riemann/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.riemann]]\n  ## The full TCP or UDP URL of the Riemann server\n  url = \"tcp://localhost:5555\"\n\n  ## Riemann event TTL, floating-point time in seconds.\n  ## Defines how long that an event is considered valid for in Riemann\n  # ttl = 30.0\n\n  ## Separator to use between measurement and field name in Riemann service name\n  ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n  separator = \"/\"\n\n  ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n  # measurement_as_attribute = false\n\n  ## Send string metrics as Riemann event states.\n  ## Unless enabled all string metrics will be ignored\n  # string_as_state = false\n\n  ## A list of tag keys whose values get sent as Riemann tags.\n  ## If empty, all Telegraf tag values will be sent as tags\n  # tag_keys = [\"telegraf\",\"custom_tag\"]\n\n  ## Additional Riemann tags to send.\n  # tags = [\"telegraf-output\"]\n\n  ## Description for Riemann event\n  # description_text = \"metrics collected from telegraf\"\n\n  ## Riemann client write timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HueBridge Plugin in TOML\nDESCRIPTION: Basic TOML configuration for the HueBridge input plugin, including bridge URL specification, room assignments, timeout settings, and TLS configuration options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/huebridge/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.huebridge]]\n  ## URL of bridges to query in the form <scheme>://<bridge id>:<user name>@<address>/\n  ## See documentation for available schemes.\n  bridges = [ \"address://<bridge id>:<user name>@<bridge hostname or address>/\" ]\n  \n  ## Manual device to room assignments to apply during status evaluation.\n  ## E.g. for motion sensors which are reported without a room assignment.\n  # room_assignments = { \"Motion sensor 1\" = \"Living room\", \"Motion sensor 2\" = \"Corridor\" }\n  \n  ## Timeout for gathering information\n  # timeout = \"10s\"\n  \n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_key_pwd = \"secret\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Example Telegraf HTTP Response Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the HTTP response input plugin, including response time, content length, and status codes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/http_response/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttp_response,method=GET,result=success,server=http://github.com,status_code=200 content_length=87878i,http_response_code=200i,response_time=0.937655534,result_code=0i,result_type=\"success\" 1565839598000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Google BigQuery Output Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Google BigQuery output plugin in Telegraf. It includes settings for credentials, project, dataset, timeout, and other options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/bigquery/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.bigquery]]\n  ## Credentials File\n  credentials_file = \"/path/to/service/account/key.json\"\n\n  ## Google Cloud Platform Project\n  # project = \"\"\n\n  ## The namespace for the metric descriptor\n  dataset = \"telegraf\"\n\n  ## Timeout for BigQuery operations.\n  # timeout = \"5s\"\n\n  ## Character to replace hyphens on Metric name\n  # replace_hyphen_to = \"_\"\n\n  ## Write all metrics in a single compact table\n  # compact_table = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTSDB Output in TOML\nDESCRIPTION: Configuration settings for the OpenTSDB output plugin including host, port, batch size, and other parameters for both telnet and HTTP modes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opentsdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.opentsdb]]\n  ## prefix for metrics keys\n  prefix = \"my.specific.prefix.\"\n\n  ## DNS name of the OpenTSDB server\n  ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n  ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n  host = \"opentsdb.example.com\"\n\n  ## Port of the OpenTSDB server\n  port = 4242\n\n  ## Number of data points to send to OpenTSDB in Http requests.\n  ## Not used with telnet API.\n  http_batch_size = 50\n\n  ## URI Path for Http requests to OpenTSDB.\n  ## Used in cases where OpenTSDB is located behind a reverse proxy.\n  http_path = \"/api/put\"\n\n  ## Debug true - Prints OpenTSDB communication\n  debug = false\n\n  ## Separator separates measurement name from field\n  separator = \"_\"\n```\n\n----------------------------------------\n\nTITLE: Example Bond Plugin Configuration with Interface Filtering\nDESCRIPTION: A practical configuration example that sets a custom proc directory path and restricts data collection to specific bond interfaces (bond0 and bond1).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bond/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.bond]]\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  host_proc = \"/proc\"\n\n  ## By default, telegraf gather stats for all bond interfaces\n  ## Setting interfaces will restrict the stats to the specified\n  ## bond interfaces.\n  bond_interfaces = [\"bond0\", \"bond1\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring DC/OS Input Plugin for Telegraf with TOML\nDESCRIPTION: Sample configuration for the DC/OS input plugin that collects metrics from a Distributed Cloud OS cluster. It includes options for authentication, filtering, connection parameters, and TLS settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dcos/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Input plugin for DC/OS metrics\n[[inputs.dcos]]\n  ## The DC/OS cluster URL.\n  cluster_url = \"https://dcos-master-1\"\n\n  ## The ID of the service account.\n  service_account_id = \"telegraf\"\n  ## The private key file for the service account.\n  service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n\n  ## Path containing login token.  If set, will read on every gather.\n  # token_file = \"/home/dcos/.dcos/token\"\n\n  ## In all filter options if both include and exclude are empty all items\n  ## will be collected.  Arrays may contain glob patterns.\n  ##\n  ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n  ## be collected for its containers or apps.\n  # node_include = []\n  # node_exclude = []\n  ## Container IDs to collect container metrics from.\n  # container_include = []\n  # container_exclude = []\n  ## Container IDs to collect app metrics from.\n  # app_include = []\n  # app_exclude = []\n\n  ## Maximum concurrent connections to the cluster.\n  # max_connections = 10\n  ## Maximum time to receive a response from cluster.\n  # response_timeout = \"20s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain & host verification\n  # insecure_skip_verify = true\n\n  ## Recommended filtering to reduce series cardinality.\n  # [inputs.dcos.tagdrop]\n  #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Disque Input Plugin in Telegraf\nDESCRIPTION: Configuration example for the Disque input plugin in Telegraf. Specifies how to connect to one or more Disque servers with optional port and password settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disque/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or many disque servers\n[[inputs.disque]]\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port and password.\n  ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring CISCO Power Table Collection in Telegraf\nDESCRIPTION: TOML configuration for collecting data from the CISCO-POWER-ETHERNET-EXT-MIB table before joining with another table. This shows basic table and field configuration for collecting power consumption and entity physical index data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.snmp.table]]\nname = \"ciscoPower\"\nindex_as_tag = true\n\n[[inputs.snmp.table.field]]\nname = \"PortPwrConsumption\"\noid = \"CISCO-POWER-ETHERNET-EXT-MIB::cpeExtPsePortPwrConsumption\"\n\n[[inputs.snmp.table.field]]\nname = \"EntPhyIndex\"\noid = \"CISCO-POWER-ETHERNET-EXT-MIB::cpeExtPsePortEntPhyIndex\"\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Parser in Telegraf\nDESCRIPTION: Basic configuration settings for the JSON parser plugin in Telegraf including file input, data format, and various JSON parsing options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  data_format = \"json\"\n\n  ## When strict is true and a JSON array is being parsed, all objects within the\n  ## array must be valid\n  json_strict = true\n\n  ## Query is a GJSON path that specifies a specific chunk of JSON to be\n  ## parsed, if not specified the whole document will be parsed.\n  json_query = \"\"\n\n  ## Tag keys is an array of keys that should be added as tags.\n  tag_keys = [\n    \"my_tag_1\",\n    \"my_tag_2\",\n    \"tags_*\",\n    \"tag*\"\n  ]\n\n  ## Array of glob pattern strings or booleans keys that should be added as string fields.\n  json_string_fields = []\n\n  ## Name key is the key to use as the measurement name.\n  json_name_key = \"\"\n\n  ## Time key is the key containing the time that should be used to create the\n  ## metric.\n  json_time_key = \"\"\n\n  ## Time format is the time layout that should be used to interpret the json_time_key.\n  json_time_format = \"\"\n\n  ## Timezone allows you to provide an override for timestamps\n  json_timezone = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PuppetAgent Input Plugin in Telegraf\nDESCRIPTION: Shows the TOML configuration for the PuppetAgent input plugin in Telegraf. It specifies the location of the last_run_summary.yaml file to be read by the plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/puppetagent/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n# Reads last_run_summary.yaml file and converts to measurements\n[[inputs.puppetagent]]\n  ## Location of puppet last run summary file\n  location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Amon Output Plugin in Telegraf (TOML)\nDESCRIPTION: This snippet shows the TOML configuration for the Amon output plugin in Telegraf. It includes required settings like server_key and amon_instance, as well as an optional timeout setting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/amon/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for Amon Server to send metrics to.\n[[outputs.amon]]\n  ## Amon Server Key\n  server_key = \"my-server-key\" # required.\n\n  ## Amon Instance URL\n  amon_instance = \"https://youramoninstance\" # required\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Zabbix Sender JSON for Metrics without Tags\nDESCRIPTION: JSON structure sent to Zabbix server for metrics without additional tags, showing how field values are transmitted.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"request\":\"sender data\",\n  \"data\":[\n    {\n      \"host\":\"myHost\",\n      \"key\":\"telegraf.mem.available_percent\",\n      \"value\":\"14.382719\",\n      \"clock\":1522764428\n    },\n    {\n      \"host\":\"myHost\",\n      \"key\":\"telegraf.mem.used\",\n      \"value\":\"14246531072\",\n      \"clock\":1522764428\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Joining Two SNMP Tables in Telegraf Configuration\nDESCRIPTION: TOML configuration demonstrating how to join data from two SNMP tables using the secondary index feature. This example joins the CISCO power table with the entity table to create a combined result with names and power consumption.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.snmp.table]]\nname = \"ciscoPowerEntity\"\nindex_as_tag = true\n\n[[inputs.snmp.table.field]]\nname = \"PortPwrConsumption\"\noid = \"CISCO-POWER-ETHERNET-EXT-MIB::cpeExtPsePortPwrConsumption\"\n\n[[inputs.snmp.table.field]]\nname = \"EntPhyIndex\"\noid = \"CISCO-POWER-ETHERNET-EXT-MIB::cpeExtPsePortEntPhyIndex\"\nsecondary_index_table = true    # enables joining\n\n[[inputs.snmp.table.field]]\nname = \"EntPhysicalName\"\noid = \"ENTITY-MIB::entPhysicalName\"\nsecondary_index_use = true      # this tag is indexed from secondary table\nis_tag = true\n```\n\n----------------------------------------\n\nTITLE: Example IPtables Rule Output\nDESCRIPTION: Sample output from iptables command showing rule statistics and their corresponding Telegraf metrics format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/iptables/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\niptables -nvL INPUT\n```\n\nLANGUAGE: text\nCODE:\n```\nChain INPUT (policy DROP 0 packets, 0 bytes)\npkts bytes target     prot opt in     out     source               destination\n100   1024   ACCEPT     tcp  --  *      *       192.168.0.0/24       0.0.0.0/0            tcp dpt:22 /* ssh */\n 42   2048   ACCEPT     tcp  --  *      *       192.168.0.0/24       0.0.0.0/0            tcp dpt:80 /* httpd */\n```\n\nLANGUAGE: text\nCODE:\n```\niptables,table=filter,chain=INPUT,ruleid=ssh pkts=100i,bytes=1024i 1453831884664956455\niptables,table=filter,chain=INPUT,ruleid=httpd pkts=42i,bytes=2048i 1453831884664956455\n```\n\n----------------------------------------\n\nTITLE: Configuring Nvidia SMI Input Plugin in TOML\nDESCRIPTION: Configuration snippet for the nvidia_smi Telegraf input plugin, showing optional settings for binary path and timeout.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nvidia_smi/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nvidia_smi]]\n  ## Optional: path to nvidia-smi binary, defaults \"/usr/bin/nvidia-smi\"\n  ## We will first try to locate the nvidia-smi binary with the explicitly specified value (or default value),\n  ## if it is not found, we will try to locate it on PATH(exec.LookPath), if it is still not found, an error will be returned\n  # bin_path = \"/usr/bin/nvidia-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Formatting CPU Metrics Event in JSON\nDESCRIPTION: Example of how the plugin formats CPU metrics events for Elasticsearch. It includes timestamp, measurement name, CPU usage metrics, and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"@timestamp\": \"2017-01-01T00:00:00+00:00\",\n  \"measurement_name\": \"cpu\",\n  \"cpu\": {\n    \"usage_guest\": 0,\n    \"usage_guest_nice\": 0,\n    \"usage_idle\": 71.85413456197966,\n    \"usage_iowait\": 0.256805341656516,\n    \"usage_irq\": 0,\n    \"usage_nice\": 0,\n    \"usage_softirq\": 0.2054442732579466,\n    \"usage_steal\": 0,\n    \"usage_system\": 15.04879301548127,\n    \"usage_user\": 12.634822807288275\n  },\n  \"tag\": {\n    \"cpu\": \"cpu-total\",\n    \"host\": \"elastichost\",\n    \"dc\": \"datacenter1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Collectd Parser Plugin in Telegraf using TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Collectd Parser Plugin in Telegraf. It includes settings for the socket listener, data format, authentication file, security level, TypesDB specifications, and multi-value plugin handling.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/collectd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.socket_listener]]\n  service_address = \"udp://:25826\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"collectd\"\n\n  ## Authentication file for cryptographic security levels\n  collectd_auth_file = \"/etc/collectd/auth_file\"\n  ## One of none (default), sign, or encrypt\n  collectd_security_level = \"encrypt\"\n  ## Path of to TypesDB specifications\n  collectd_typesdb = [\"/usr/share/collectd/types.db\"]\n\n  ## Multi-value plugins can be handled two ways.\n  ## \"split\" will parse and store the multi-value plugin data into separate measurements\n  ## \"join\" will parse and store the multi-value plugin as a single multi-value measurement.\n  ## \"split\" is the default behavior for backward compatibility with previous versions of influxdb.\n  collectd_parse_multivalue = \"split\"\n```\n\n----------------------------------------\n\nTITLE: Configuring KNX Listener in TOML\nDESCRIPTION: Configuration template for the KNX listener plugin showing service type settings, interface address configuration, and measurement definitions including datapoint types and group addresses.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/knx_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Listener capable of handling KNX bus messages provided through a KNX-IP Interface.\n[[inputs.knx_listener]]\n  ## Type of KNX-IP interface.\n  ## Can be either \"tunnel_udp\", \"tunnel_tcp\", \"tunnel\" (alias for tunnel_udp) or \"router\".\n  # service_type = \"tunnel\"\n\n  ## Address of the KNX-IP interface.\n  service_address = \"localhost:3671\"\n\n  ## Measurement definition(s)\n  # [[inputs.knx_listener.measurement]]\n  #   ## Name of the measurement\n  #   name = \"temperature\"\n  #   ## Datapoint-Type (DPT) of the KNX messages\n  #   dpt = \"9.001\"\n  #   ## Use the string representation instead of the numerical value for the\n  #   ## datapoint-type and the addresses below\n  #   # as_string = false\n  #   ## List of Group-Addresses (GAs) assigned to the measurement\n  #   addresses = [\"5/5/1\"]\n\n  # [[inputs.knx_listener.measurement]]\n  #   name = \"illumination\"\n  #   dpt = \"9.004\"\n  #   addresses = [\"5/5/3\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Noise Processor Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Noise processor plugin in Telegraf, showing all available options for adding noise to numerical fields including distribution type selection, distribution parameters, and field filtering options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/noise/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Adds noise to numerical fields\n[[processors.noise]]\n  ## Specified the type of the random distribution.\n  ## Can be \"laplacian\", \"gaussian\" or \"uniform\".\n  # type = \"laplacian\n\n  ## Center of the distribution.\n  ## Only used for Laplacian and Gaussian distributions.\n  # mu = 0.0\n\n  ## Scale parameter for the Laplacian or Gaussian distribution\n  # scale = 1.0\n\n  ## Upper and lower bound of the Uniform distribution\n  # min = -1.0\n  # max = 1.0\n\n  ## Apply the noise only to numeric fields matching the filter criteria below.\n  ## Excludes takes precedence over includes.\n  # include_fields = []\n  # exclude_fields = []\n```\n\n----------------------------------------\n\nTITLE: Configuring Serializer with File Output in Telegraf\nDESCRIPTION: Sample configuration showing how to use the serializer plugin with the file output. It demonstrates setting the data_format option and includes an example custom option.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/EXAMPLE_README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"stdout\"]\n\n  ## Describe variables using the standard SampleConfig style.\n  ##   https://github.com/influxdata/telegraf/wiki/SampleConfig\n  example_option = \"example_value\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"example\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Mailchimp Input Plugin with TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to configure the Mailchimp input plugin in Telegraf. It includes settings for the Mailchimp API key, the age of reports to collect, and an optional campaign ID to filter the collected metrics. The `api_key` is a required field, obtained from the Mailchimp account settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mailchimp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gathers metrics from the /3.0/reports MailChimp API\n[[inputs.mailchimp]]\n  ## MailChimp API key\n  ## get from https://admin.mailchimp.com/account/api/\n  api_key = \"\" # required\n\n  ## Reports for campaigns sent more than days_old ago will not be collected.\n  ## 0 means collect all and is the default value.\n  days_old = 0\n\n  ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n  # campaign_id = \"\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring DFS Namespace Service with Telegraf\nDESCRIPTION: Configuration for tracking DFS Namespace service referrals metrics when the server hosts a DFS Namespace or is a Domain Controller. Monitors processed requests, failed requests, and average response time.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_perf_counters]]\n  [[inputs.win_perf_counters.object]]\n    # AD, DFS N, Useful if the server hosts a DFS Namespace or is a Domain Controller\n    ObjectName = \"DFS Namespace Service Referrals\"\n    Instances = [\"*\"]\n    Counters = [\"Requests Processed\",\"Requests Failed\",\"Avg. Response Time\"]\n    Measurement = \"win_dfsn\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n    #WarnOnMissing = false # Print out when the performance counter is missing, either of object, counter or instance.\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Statistics Aggregator Plugin in TOML\nDESCRIPTION: Sample configuration for the basicstats aggregator plugin in Telegraf. It shows how to set the aggregation period, whether to drop original metrics, and which statistics to calculate.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/basicstats/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Keep the aggregate basicstats of each metric passing through.\n[[aggregators.basicstats]]\n  ## The period on which to flush & clear the aggregator.\n  # period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  # drop_original = false\n\n  ## Configures which basic stats to push as fields\n  # stats = [\"count\",\"min\",\"max\",\"mean\",\"variance\",\"stdev\"]\n```\n\n----------------------------------------\n\nTITLE: Running the readme_linter tool\nDESCRIPTION: This snippet shows how to run the `readme_linter` tool after it has been built. The tool takes the path to the readme file as an argument. Multiple files can be passed as arguments to lint them simultaneously.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/readme_linter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n```shell\ntelegraf/tools/readme_linter$ ./readme_linter <path to readme>\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring IPMI Sensor Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows the available options for the IPMI Sensor Input Plugin in Telegraf. It includes settings for ipmitool path, sudo usage, server specifications, privilege levels, timeouts, and sensor types to collect.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipmi_sensor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from the bare metal servers via IPMI\n[[inputs.ipmi_sensor]]\n  ## Specify the path to the ipmitool executable\n  # path = \"/usr/bin/ipmitool\"\n\n  ## Use sudo\n  ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.\n  ## Sudo must be configured to allow the telegraf user to run ipmitool\n  ## without a password.\n  # use_sudo = false\n\n  ## Servers\n  ## Specify one or more servers via a url. If no servers are specified, local\n  ## machine sensor stats will be queried. Uses the format:\n  ##  [username[:password]@][protocol[(address)]]\n  ##  e.g. root:passwd@lan(127.0.0.1)\n  # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n\n  ## Session privilege level\n  ## Choose from: CALLBACK, USER, OPERATOR, ADMINISTRATOR\n  # privilege = \"ADMINISTRATOR\"\n\n  ## Timeout\n  ## Timeout for the ipmitool command to complete.\n  # timeout = \"20s\"\n\n  ## Metric schema version\n  ## See the plugin readme for more information on schema versioning.\n  # metric_version = 1\n\n  ## Sensors to collect\n  ## Choose from:\n  ##   * sdr: default, collects sensor data records\n  ##   * chassis_power_status: collects the power status of the chassis\n  ##   * dcmi_power_reading: collects the power readings from the Data Center Management Interface\n  # sensors = [\"sdr\"]\n\n  ## Hex key\n  ## Optionally provide the hex key for the IMPI connection.\n  # hex_key = \"\"\n\n  ## Cache\n  ## If ipmitool should use a cache\n  ## Using a cache can speed up collection times depending on your device.\n  # use_cache = false\n\n  ## Path to the ipmitools cache file (defaults to OS temp dir)\n  ## The provided path must exist and must be writable\n  # cache_path = \"\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of MD RAID Statistics (text)\nDESCRIPTION: This example shows the output format of the `mdstat` metric collected by the Telegraf plugin. It provides the ActivityState, Devices, Name, BlocksSynced, BlocksSyncedFinishTime, BlocksSyncedPct, BlocksSyncedSpeed, BlocksTotal, DisksActive, DisksFailed, DisksSpare, DisksTotal, and DisksDown as fields with the corresponding values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mdstat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"mdstat,ActivityState=active,Devices=sdm1\\\\,sdn1,Name=md1 BlocksSynced=231299072i,BlocksSyncedFinishTime=0,BlocksSyncedPct=0,BlocksSyncedSpeed=0,BlocksTotal=231299072i,DisksActive=2i,DisksFailed=0i,DisksSpare=0i,DisksTotal=2i,DisksDown=0i 1617814276000000000\nmdstat,ActivityState=active,Devices=sdm5\\\\,sdn5,Name=md2 BlocksSynced=2996224i,BlocksSyncedFinishTime=0,BlocksSyncedPct=0,BlocksSyncedSpeed=0,BlocksTotal=2996224i,DisksActive=2i,DisksFailed=0i,DisksSpare=0i,DisksTotal=2i,DisksDown=0i 1617814276000000000\"\n```\n\n----------------------------------------\n\nTITLE: Scaled Value Output in Diff Format\nDESCRIPTION: This diff output shows the result of applying the scaling configuration to the `cpu` field. It demonstrates the change from an input value of 25 to a scaled output value of 75.0. The temperature field remains unchanged, indicating that the scaling was only applied to the `cpu` field.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/scale/README.md#2025-04-16_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n\"- temperature, cpu=25\\n+ temperature, cpu=75.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Graphite Output Format in Telegraf\nDESCRIPTION: A complete configuration example for the Graphite data format in Telegraf. It shows how to set output files, prefix, template patterns, sanitization options, and tag support features.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/graphite/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"graphite\"\n\n  ## Prefix added to each graphite bucket\n  prefix = \"telegraf\"\n  ## Graphite template pattern\n  template = \"host.tags.measurement.field\"\n\n  ## Graphite templates patterns\n  ## 1. Template for cpu\n  ## 2. Template for disk*\n  ## 3. Default template\n  # templates = [\n  #  \"cpu tags.measurement.host.field\",\n  #  \"disk* measurement.field\",\n  #  \"host.measurement.tags.field\"\n  #]\n\n  ## Strict sanitization regex\n  ## This is the default sanitization regex that is used on data passed to the\n  ## graphite serializer. Users can add additional characters here if required.\n  ## Be aware that the characters, '/' '@' '*' are always replaced with '_',\n  ## '..' is replaced with '.', and '\\' is removed even if added to the\n  ## following regex.\n  # graphite_strict_sanitize_regex = '[^a-zA-Z0-9-:._=\\p{L}]'\n\n  ## Support Graphite tags, recommended to enable when using Graphite 1.1 or later.\n  # graphite_tag_support = false\n\n  ## Applied sanitization mode when graphite tag support is enabled.\n  ## * strict - uses the regex specified above\n  ## * compatible - allows for greater number of characters\n  # graphite_tag_sanitize_mode = \"strict\"\n\n  ## Character for separating metric name and field for Graphite tags\n  # graphite_separator = \".\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Update Policy Transform Function\nDESCRIPTION: Creates a transform function and update policy for converting dynamic data type columns. Recommended approach for querying large volumes of data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/azure_data_explorer/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n// Function to transform data\n.create-or-alter function Transform_TargetTableName() {\n      SourceTableName\n      | mv-apply fields on (extend key = tostring(bag_keys(fields)[0]))\n      | project fieldname=key, value=todouble(fields[key]), name, tags, timestamp\n}\n\n// Create destination table with above query's results schema (if it doesn't exist already)\n.set-or-append TargetTableName <| Transform_TargetTableName() | limit 0\n\n// Apply update policy on destination table\n.alter table TargetTableName policy update\n@'[{\"IsEnabled\": true, \"Source\": \"SourceTableName\", \"Query\": \"Transform_TargetTableName()\", \"IsTransactional\": true, \"PropagateIngestionProperties\": false}]'\n```\n\n----------------------------------------\n\nTITLE: Creating Elasticsearch Index Template in JSON\nDESCRIPTION: Example of an index template created by Telegraf on Elasticsearch 5.x. It defines settings and mappings for indexes, including dynamic templates for tags and metrics fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"order\": 0,\n  \"template\": \"telegraf-*\",\n  \"settings\": {\n    \"index\": {\n      \"mapping\": {\n        \"total_fields\": {\n          \"limit\": \"5000\"\n        }\n      },\n      \"auto_expand_replicas\" : \"0-1\",\n      \"codec\" : \"best_compression\",\n      \"refresh_interval\": \"10s\"\n    }\n  },\n  \"mappings\": {\n    \"_default_\": {\n      \"dynamic_templates\": [\n        {\n          \"tags\": {\n            \"path_match\": \"tag.*\",\n            \"mapping\": {\n              \"ignore_above\": 512,\n              \"type\": \"keyword\"\n            },\n            \"match_mapping_type\": \"string\"\n          }\n        },\n        {\n          \"metrics_long\": {\n            \"mapping\": {\n              \"index\": false,\n              \"type\": \"float\"\n            },\n            \"match_mapping_type\": \"long\"\n          }\n        },\n        {\n          \"metrics_double\": {\n            \"mapping\": {\n              \"index\": false,\n              \"type\": \"float\"\n            },\n            \"match_mapping_type\": \"double\"\n          }\n        },\n        {\n          \"text_fields\": {\n            \"mapping\": {\n              \"norms\": false\n            },\n            \"match\": \"*\"\n          }\n        }\n      ],\n      \"_all\": {\n        \"enabled\": false\n      },\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"measurement_name\": {\n          \"type\": \"keyword\"\n        }\n      }\n    }\n  },\n  \"aliases\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf File Output with Template Format\nDESCRIPTION: This code snippet configures the Telegraf `file` output plugin to use the `template` data format. It specifies the files to write to (stdout and /tmp/metrics.out), sets the `data_format` to \"template\", and defines a Go template for formatting individual metrics. The template extracts the \"host\" tag and the \"available\" field.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/template/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"template\"\n\n  ## Go template which defines output format\n  template = '{{ .Tag \"host\" }} {{ .Field \"available\" }}'\n```\n\n----------------------------------------\n\nTITLE: Splunk Metric HEC JSON Output Example (file)\nDESCRIPTION: This example demonstrates the  JSON format when `hec_routing` is false when using file output with Splunk. The `time` field is at the same level as other metric fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    \"_value\": 0.6,\n    \"cpu\": \"cpu0\",\n    \"dc\": \"mobile\",\n    \"metric_name\": \"cpu.usage_user\",\n    \"user\": \"ronnocol\",\n    \"time\": 1529708430\n}\n```\n\n----------------------------------------\n\nTITLE: Sample InfiniBand Metrics Output\nDESCRIPTION: Example output showing collected metrics including port statistics, packet counts, and error rates from an InfiniBand device, with both standard and RDMA counter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/infiniband/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ninfiniband,device=mlx5_bond_0,host=hop-r640-12,port=1 port_xmit_data=85378896588i,VL15_dropped=0i,port_rcv_packets=34914071i,port_rcv_data=34600185253i,port_xmit_discards=0i,link_downed=0i,local_link_integrity_errors=0i,symbol_error=0i,link_error_recovery=0i,multicast_rcv_packets=0i,multicast_xmit_packets=0i,unicast_xmit_packets=82002535i,excessive_buffer_overrun_errors=0i,port_rcv_switch_relay_errors=0i,unicast_rcv_packets=34914071i,port_xmit_constraint_errors=0i,port_rcv_errors=0i,port_xmit_wait=0i,port_rcv_remote_physical_errors=0i,port_rcv_constraint_errors=0i,port_xmit_packets=82002535i 1737652060000000000\ninfiniband,device=mlx5_bond_0,host=hop-r640-12,port=1 local_ack_timeout_err=0i,lifespan=10i,out_of_buffer=0i,resp_remote_access_errors=0i,resp_local_length_error=0i,np_cnp_sent=0i,roce_slow_restart=0i,rx_read_requests=6000i,duplicate_request=0i,resp_cqe_error=0i,rx_write_requests=19000i,roce_slow_restart_cnps=0i,rx_icrc_encapsulated=0i,rnr_nak_retry_err=0i,roce_adp_retrans=0i,out_of_sequence=0i,req_remote_access_errors=0i,roce_slow_restart_trans=0i,req_remote_invalid_request=0i,req_cqe_error=0i,resp_cqe_flush_error=0i,packet_seq_err=0i,roce_adp_retrans_to=0i,np_ecn_marked_roce_packets=0i,rp_cnp_handled=0i,implied_nak_seq_err=0i,rp_cnp_ignored=0i,req_cqe_flush_error=0i,rx_atomic_requests=0i 1737652060000000000\n```\n\n----------------------------------------\n\nTITLE: Adding Formatted Fields as a Tag in Telegraf Template Processor\nDESCRIPTION: Advanced example that formats all fields with their values into a structured message tag using range iteration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = \"message\"\n  template = '''Message about {{.Name}} fields:\n{{ range $field, $value := .Fields -}}\n{{$field}}:{{$value}}\n{{ end }}'''\n```\n\n----------------------------------------\n\nTITLE: Configuring Solr Input Plugin - TOML\nDESCRIPTION: This TOML configuration snippet specifies how to set up the Solr input plugin for Telegraf. It includes attributes for specifying Solr servers, cores, optional HTTP Basic authentication credentials, and timeout settings. The default value for cores is to collect data from all available cores if not specified.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/solr/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[inputs.solr]]\\n  ## specify a list of one or more Solr servers\\n  servers = [\\\"http://localhost:8983\\\"]\\n\\n  ## specify a list of one or more Solr cores (default - all)\\n  # cores = [\\\"*\\\"]\\n  \\n  ## Optional HTTP Basic Auth Credentials\\n  # username = \\\"username\\\"\\n  # password = \\\"pa$$word\\\"\\n\\n  ## Timeout for HTTP requests\\n  # timeout = \\\"5s\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf as a Windows Service\nDESCRIPTION: This code snippet demonstrates how to install Telegraf as an official Windows service using the command line. This allows Telegraf to run in the background and automatically start with the operating system.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_9\n\nLANGUAGE: batch\nCODE:\n```\n> C:\\Program Files\\Telegraf\\telegraf.exe --service install\n```\n\n----------------------------------------\n\nTITLE: Configuring Dovecot Input Plugin in TOML\nDESCRIPTION: Configuration settings for the Dovecot input plugin specifying server connections and filtering options. Allows configuring server addresses either via address:port or Unix Domain Socket paths, and supports filtering by type (user/domain/ip/global) with wildcard patterns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dovecot/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about dovecot servers\n[[inputs.dovecot]]\n  ## specify dovecot servers via an address:port list\n  ##  e.g.\n  ##    localhost:24242\n  ## or as an UDS socket\n  ##  e.g.\n  ##    /var/run/dovecot/old-stats\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost:24242\"]\n\n  ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n  type = \"global\"\n\n  ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n  ## If type = \"ip\" filters should be <IP/network>\n  filters = [\"\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Linux CPU Plugin in Telegraf (TOML)\nDESCRIPTION: This TOML configuration snippet is used to set up the Linux CPU Input Plugin in Telegraf, specifying options for sysfs filesystem path and the metrics to be collected, such as 'cpufreq' and 'thermal'. It allows for customization of CPU metric collection based on user-defined parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/linux_cpu/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Provides Linux CPU metrics\\n# This plugin ONLY supports Linux\\n[[inputs.linux_cpu]]\\n  ## Path for sysfs filesystem.\\n  ## See https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt\\n  ## Defaults:\\n  # host_sys = \\\"/sys\\\"\\n\\n  ## CPU metrics collected by the plugin.\\n  ## Supported options:\\n  ## \\\"cpufreq\\\", \\\"thermal\\\"\\n  ## Defaults:\\n  # metrics = [\\\"cpufreq\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring File Input with JSON_v2 Parser for Nested Arrays\nDESCRIPTION: This TOML configuration demonstrates how to set up the file input plugin to process a JSON file with the json_v2 parser. It focuses on extracting data from a book object and setting the title as a tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json_v2/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n    files = [\"./testdata/multiple_arrays_in_object/input.json\"]\n    data_format = \"json_v2\"\n    [[inputs.file.json_v2]]\n        [[inputs.file.json_v2.object]]\n            path = \"book\"\n            tags = [\"title\"]\n            disable_prepend_keys = true\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB V2 Listener in TOML\nDESCRIPTION: Configuration settings for the InfluxDB V2 Listener input plugin, including service address, timeouts, TLS settings, and authentication options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb_v2_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.influxdb_v2_listener]]\n  ## Address and port to host InfluxDB listener on\n  ## (Double check the port. Could be 9999 if using OSS Beta)\n  service_address = \":8086\"\n\n  ## Maximum undelivered metrics before rate limit kicks in.\n  ## When the rate limit kicks in, HTTP status 429 will be returned.\n  ## 0 disables rate limiting\n  # max_undelivered_metrics = 0\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed HTTP request body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  # max_body_size = \"32MiB\"\n\n  ## Optional tag to determine the bucket.\n  ## If the write has a bucket in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  # bucket_tag = \"\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional token to accept for HTTP authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # token = \"some-long-shared-secret-token\"\n\n  ## Influx line protocol parser\n  ## 'internal' is the default. 'upstream' is a newer parser that is faster\n  ## and more memory efficient.\n  # parser_type = \"internal\"\n```\n\n----------------------------------------\n\nTITLE: Parsing XML for Gateway and Sensor Status using Telegraf\nDESCRIPTION: Demonstrates basic parsing of the XML file 'example.xml' to extract gateway and sensor information using XPath expressions. The XML configuration uses XPath to identify tags, integer fields, and boolean comparisons against the gateway status. The processed output provides metrics for gateway attributes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<Gateway>\n  <Name>Main Gateway</Name>\n  <Timestamp>2020-08-01T15:04:03Z</Timestamp>\n  <Sequence>12</Sequence>\n  <Status>ok</Status>\n</Gateway>\n\n<Bus>\n  <Sensor name=\"Sensor Facility A\">\n    <Variable temperature=\"20.0\"/>\n    <Variable power=\"123.4\"/>\n    <Variable frequency=\"49.78\"/>\n    <Variable consumers=\"3\"/>\n    <Mode>busy</Mode>\n  </Sensor>\n  <Sensor name=\"Sensor Facility B\">\n    <Variable temperature=\"23.1\"/>\n    <Variable power=\"14.3\"/>\n    <Variable frequency=\"49.78\"/>\n    <Variable consumers=\"1\"/>\n    <Mode>standby</Mode>\n  </Sensor>\n  <Sensor name=\"Sensor Facility C\">\n    <Variable temperature=\"19.7\"/>\n    <Variable power=\"0.02\"/>\n    <Variable frequency=\"49.78\"/>\n    <Variable consumers=\"0\"/>\n    <Mode>error</Mode>\n  </Sensor>\n</Bus>\n```\n\n----------------------------------------\n\nTITLE: Redis Sentinel Metrics Format Example\nDESCRIPTION: Example of Redis Sentinel metrics output showing various performance metrics, sentinel statistics, and system resource usage. The output includes measurements from two sentinel instances on different ports (26379 and 26380) with metrics like client connections, CPU usage, network I/O, and sentinel-specific stats.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/redis_sentinel/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nredis_sentinel,host=somehostname,port=26379,source=localhost active_defrag_hits=0i,active_defrag_key_hits=0i,active_defrag_key_misses=0i,active_defrag_misses=0i,blocked_clients=0i,client_recent_max_input_buffer=2i,client_recent_max_output_buffer=0i,clients=3i,evicted_keys=0i,expired_keys=0i,expired_stale_perc=0,expired_time_cap_reached_count=0i,instantaneous_input_kbps=0.01,instantaneous_ops_per_sec=0i,instantaneous_output_kbps=0,keyspace_hits=0i,keyspace_misses=0i,latest_fork_usec=0i,lru_clock=9926289i,migrate_cached_sockets=0i,pubsub_channels=0i,pubsub_patterns=0i,redis_version=\"5.0.5\",rejected_connections=0i,sentinel_masters=1i,sentinel_running_scripts=0i,sentinel_scripts_queue_length=0i,sentinel_simulate_failure_flags=0i,sentinel_tilt=0i,slave_expires_tracked_keys=0i,sync_full=0i,sync_partial_err=0i,sync_partial_ok=0i,total_commands_processed=459i,total_connections_received=6i,total_net_input_bytes=24517i,total_net_output_bytes=14864i,uptime_ns=303000000000i,used_cpu_sys=0.404,used_cpu_sys_children=0,used_cpu_user=0.436,used_cpu_user_children=0 1570207377000000000\nredis_sentinel,host=somehostname,port=26380,source=localhost active_defrag_hits=0i,active_defrag_key_hits=0i,active_defrag_key_misses=0i,active_defrag_misses=0i,blocked_clients=0i,client_recent_max_input_buffer=2i,client_recent_max_output_buffer=0i,clients=2i,evicted_keys=0i,expired_keys=0i,expired_stale_perc=0,expired_time_cap_reached_count=0i,instantaneous_input_kbps=0.01,instantaneous_ops_per_sec=0i,instantaneous_output_kbps=0,keyspace_hits=0i,keyspace_misses=0i,latest_fork_usec=0i,lru_clock=9926289i,migrate_cached_sockets=0i,pubsub_channels=0i,pubsub_patterns=0i,redis_version=\"5.0.5\",rejected_connections=0i,sentinel_masters=1i,sentinel_running_scripts=0i,sentinel_scripts_queue_length=0i,sentinel_simulate_failure_flags=0i,sentinel_tilt=0i,slave_expires_tracked_keys=0i,sync_full=0i,sync_partial_err=0i,sync_partial_ok=0i,total_commands_processed=442i,total_connections_received=2i,total_net_input_bytes=23861i,total_net_output_bytes=4443i,uptime_ns=312000000000i,used_cpu_sys=0.46,used_cpu_sys_children=0,used_cpu_user=0.416,used_cpu_user_children=0 1570207377000000000\n```\n\n----------------------------------------\n\nTITLE: Verifying Telegraf Access to Mount Information\nDESCRIPTION: Shell commands for troubleshooting Telegraf's ability to read mount information from /proc and stat information from mount points. These commands help diagnose permission issues that might prevent Telegraf from collecting disk metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ sudo -u telegraf cat /proc/self/mounts | grep sda2\n/dev/sda2 /home ext4 rw,relatime,data=ordered 0 0\n$ sudo -u telegraf stat /home\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Parser Plugin for Telegraf in TOML\nDESCRIPTION: This snippet shows how to configure the Avro parser plugin for Telegraf using TOML. It includes settings for Kafka consumer, Avro data format, schema registry, and various Avro-specific options for handling fields, tags, and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/avro/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.kafka_consumer]]\n  ## Kafka brokers.\n  brokers = [\"localhost:9092\"]\n\n  ## Topics to consume.\n  topics = [\"telegraf\"]\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 1000000\n\n  ## Avro data format settings\n  data_format = \"avro\"\n\n  ## Avro message format\n  ## Supported values are \"binary\" (default) and \"json\"\n  # avro_format = \"binary\"\n\n  ## URL of the schema registry which may contain username and password in the\n  ## form http[s]://[username[:password]@]<host>[:port]\n  ## NOTE: Exactly one of schema registry and schema must be set\n  avro_schema_registry = \"http://localhost:8081\"\n\n  ## Path to the schema registry certificate. Should be specified only if\n  ## required for connection to the schema registry.\n  # avro_schema_registry_cert = \"/etc/telegraf/ca_cert.crt\"\n\n  ## Schema string; exactly one of schema registry and schema must be set\n  #avro_schema = '''\n  #        {\n  #          \"type\":\"record\",\n  #          \"name\":\"Value\",\n  #          \"namespace\":\"com.example\",\n  #          \"fields\":[\n  #              {\n  #                  \"name\":\"tag\",\n  #                  \"type\":\"string\"\n  #              },\n  #              {\n  #                  \"name\":\"field\",\n  #                  \"type\":\"long\"\n  #              },\n  #              {\n  #                  \"name\":\"timestamp\",\n  #                  \"type\":\"long\"\n  #              }\n  #          ]\n  #      }\n  #'''\n\n  ## Measurement field name; The meauserment name will be taken \n  ## from this field. If not set, determine measurement name\n  ## from the following 'avro_measurement' option\n  # avro_measurement_field = \"field_name\"\n\n  ## Measurement string; if not set, determine measurement name from\n  ## schema (as \"<namespace>.<name>\")\n  # avro_measurement = \"ratings\"\n\n  ## Avro fields to be used as tags; optional.\n  # avro_tags = [\"CHANNEL\", \"CLUB_STATUS\"]\n\n  ## Avro fields to be used as fields; if empty, any Avro fields\n  ## detected from the schema, not used as tags, will be used as\n  ## measurement fields.\n  # avro_fields = [\"STARS\"]\n\n  ## Avro fields to be used as timestamp; if empty, current time will\n  ## be used for the measurement timestamp.\n  # avro_timestamp = \"\"\n  ## If avro_timestamp is specified, avro_timestamp_format must be set\n  ## to one of 'unix', 'unix_ms', 'unix_us', or 'unix_ns'.  It will\n  ## default to 'unix'.\n  # avro_timestamp_format = \"unix\"\n\n  ## Used to separate parts of array structures.  As above, the default\n  ## is the empty string, so a=[\"a\", \"b\"] becomes a0=\"a\", a1=\"b\".\n  ## If this were set to \"_\", then it would be a_0=\"a\", a_1=\"b\".\n  # avro_field_separator = \"_\"\n\n  ## Define handling of union types. Possible values are:\n  ##   flatten  -- add type suffix to field name (default)\n  ##   nullable -- do not modify field name but discard \"null\" field values\n  ##   any      -- do not modify field name and set field value to the received type\n  # avro_union_mode = \"flatten\"\n\n  ## Default values for given tags: optional\n  # tags = { \"application\": \"hermes\", \"region\": \"central\" }\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch Memory Metrics in Telegraf\nDESCRIPTION: Defines memory-related metrics for Elasticsearch, including segments and translog statistics, to monitor memory usage effectively. Each metric is specified as a float for accurate measurement.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n    - segments_norms_memory_in_bytes (float)\n    - segments_points_memory_in_bytes (float)\n    - segments_stored_fields_memory_in_bytes (float)\n    - segments_term_vectors_memory_in_bytes (float)\n    - segments_terms_memory_in_bytes (float)\n    - segments_version_map_memory_in_bytes (float)\n    - store_size_in_bytes (float)\n    - translog_earliest_last_modified_age (float)\n    - translog_operations (float)\n    - translog_size_in_bytes (float)\n    - translog_uncommitted_operations (float)\n    - translog_uncommitted_size_in_bytes (float)\n    - warmer_current (float)\n    - warmer_total (float)\n    - warmer_total_time_in_millis (float)\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Response Plugin in Telegraf with TOML\nDESCRIPTION: Sample configuration for the Network Response Input Plugin in Telegraf. It defines settings for protocol, address, timeouts, and optional string matching for UDP checks.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/net_response/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collect response time of a TCP or UDP connection\n[[inputs.net_response]]\n  ## Protocol, must be \"tcp\" or \"udp\"\n  ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n  ## a send/expect string pair (see below).\n  protocol = \"tcp\"\n  ## Server address (default localhost)\n  address = \"localhost:80\"\n\n  ## Set timeout\n  # timeout = \"1s\"\n\n  ## Set read timeout (only used if expecting a response)\n  # read_timeout = \"1s\"\n\n  ## The following options are required for UDP checks. For TCP, they are\n  ## optional. The plugin will send the given string to the server and then\n  ## expect to receive the given 'expect' string back.\n  ## string sent to the server\n  # send = \"ssh\"\n  ## expected string in answer\n  # expect = \"ssh\"\n\n  ## Uncomment to remove deprecated fields; recommended for new deploys\n  # fieldexclude = [\"result_type\", \"string_found\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Health Output Plugin in Telegraf\nDESCRIPTION: Configuration example for the Telegraf health output plugin. Shows how to set up HTTP endpoints, timeouts, authentication, TLS certificates, and metric comparison rules. Includes settings for monitoring intervals between metrics and defining health check conditions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/health/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configurable HTTP health check resource based on metrics\n[[outputs.health]]\n  ## Address and port to listen on.\n  ##   ex: service_address = \"http://localhost:8080\"\n  ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n  # service_address = \"http://:8080\"\n\n  ## The maximum duration for reading the entire request.\n  # read_timeout = \"5s\"\n  ## The maximum duration for writing the entire response.\n  # write_timeout = \"5s\"\n\n  ## Username and password to accept for HTTP basic authentication.\n  # basic_username = \"user1\"\n  # basic_password = \"secret\"\n\n  ## Allowed CA certificates for client certificates.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## TLS server certificate and private key.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Maximum expected time between metrics being written\n  ## Enforces an unhealthy state if there was no new metric seen for at least\n  ## the specified time. The check is disabled by default and only used if a\n  ## positive time is specified.\n  # max_time_between_metrics = \"0s\"\n\n  ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n  ## plugin definition, otherwise additional config options are read as part of\n  ## the table\n\n  ## One or more check sub-tables should be defined, it is also recommended to\n  ## use metric filtering to limit the metrics that flow into this output.\n  ##\n  ## When using the default buffer sizes, this example will fail when the\n  ## metric buffer is half full.\n  ##\n  ## namepass = [\"internal_write\"]\n  ## tagpass = { output = [\"influxdb\"] }\n  ##\n  ## [[outputs.health.compares]]\n  ##   field = \"buffer_size\"\n  ##   lt = 5000.0\n  ##\n  ## [[outputs.health.contains]]\n  ##   field = \"buffer_size\"\n```\n\n----------------------------------------\n\nTITLE: Azure Monitor Storage Account Metrics Sample Output\nDESCRIPTION: Sample Telegraf output showing various Azure Storage Account metrics including used capacity, transactions, ingress/egress data, latency measurements, and availability. The output is formatted as line protocol with metrics tagged by resource details and includes various statistical measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/azure_monitor/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nazure_monitor_microsoft_storage_storageaccounts_used_capacity,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Bytes average=9065573,maximum=9065573,minimum=9065573,timeStamp=\"2021-11-08T09:52:00Z\",total=9065573 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_transactions,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Count average=1,count=6,maximum=1,minimum=0,timeStamp=\"2021-11-08T09:52:00Z\",total=6 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_ingress,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Bytes average=5822.333333333333,count=6,maximum=5833,minimum=0,timeStamp=\"2021-11-08T09:52:00Z\",total=34934 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_egress,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Bytes average=840.1666666666666,count=6,maximum=841,minimum=0,timeStamp=\"2021-11-08T09:52:00Z\",total=5041 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_success_server_latency,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=MilliSeconds average=12.833333333333334,count=6,maximum=30,minimum=8,timeStamp=\"2021-11-08T09:52:00Z\",total=77 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_success_e2e_latency,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=MilliSeconds average=12.833333333333334,count=6,maximum=30,minimum=8,timeStamp=\"2021-11-08T09:52:00Z\",total=77 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_availability,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Percent average=100,count=6,maximum=100,minimum=100,timeStamp=\"2021-11-08T09:52:00Z\",total=600 1636368744000000000\nazure_monitor_microsoft_storage_storageaccounts_used_capacity,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Bytes average=9065573,maximum=9065573,minimum=9065573,timeStamp=\"2021-11-08T09:52:00Z\",total=9065573 1636368745000000000\nazure_monitor_microsoft_storage_storageaccounts_transactions,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Count average=1,count=6,maximum=1,minimum=0,timeStamp=\"2021-11-08T09:52:00Z\",total=6 1636368745000000000\nazure_monitor_microsoft_storage_storageaccounts_ingress,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Bytes average=5822.333333333333,count=6,maximum=5833,minimum=0,timeStamp=\"2021-11-08T09:52:00Z\",total=34934 1636368745000000000\nazure_monitor_microsoft_storage_storageaccounts_egress,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Bytes average=840.1666666666666,count=6,maximum=841,minimum=0,timeStamp=\"2021-11-08T09:52:00Z\",total=5041 1636368745000000000\nazure_monitor_microsoft_storage_storageaccounts_success_server_latency,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=MilliSeconds average=12.833333333333334,count=6,maximum=30,minimum=8,timeStamp=\"2021-11-08T09:52:00Z\",total=77 1636368745000000000\nazure_monitor_microsoft_storage_storageaccounts_success_e2e_latency,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=MilliSeconds average=12.833333333333334,count=6,maximum=30,minimum=8,timeStamp=\"2021-11-08T09:52:00Z\",total=77 1636368745000000000\nazure_monitor_microsoft_storage_storageaccounts_availability,host=Azure-MBP,namespace=Microsoft.Storage/storageAccounts,resource_group=azure-rg,resource_name=azuresa,resource_region=eastus,subscription_id=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx,unit=Percent average=100,count=6,maximum=100,minimum=100,timeStamp=\"2021-11-08T09:52:00Z\",total=600 1636368745000000000\n```\n\n----------------------------------------\n\nTITLE: Querying Jenkins Node Metrics in SQL\nDESCRIPTION: This SQL query retrieves average memory and temporary storage availability from jenkins_node measurements over the last 15 minutes, grouped by time intervals.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jenkins/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT mean(\"memory_available\") AS \"mean_memory_available\", mean(\"memory_total\") AS \"mean_memory_total\", mean(\"temp_available\") AS \"mean_temp_available\" FROM \"jenkins_node\" WHERE time > now() - 15m GROUP BY time(:interval:) FILL(null)\n```\n\n----------------------------------------\n\nTITLE: Configuring Localized Wildcard Expansion\nDESCRIPTION: Controls whether object and counter names are localized when using wildcard expansion on a localized Windows installation\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_2\n\nLANGUAGE: TOML\nCODE:\n```\nLocalizeWildcardsExpansion=true\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf as Windows Service\nDESCRIPTION: Command to install Telegraf as a Windows Service with the default configuration path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:Program Files\\Telegraf\\telegraf.exe\" service install\n```\n\n----------------------------------------\n\nTITLE: Using fieldinclude and fieldexclude for CPU and Disk Metrics in Telegraf\nDESCRIPTION: This configuration shows how to use fieldinclude and fieldexclude to filter specific fields from CPU and disk metrics. It excludes guest and steal CPU usage, and includes only inode-related metrics for disks.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_21\n\nLANGUAGE: toml\nCODE:\n```\n# Drop all metrics for guest & steal CPU usage\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  fieldexclude = [\"usage_guest\", \"usage_steal\"]\n\n# Only store inode related metrics for disks\n[[inputs.disk]]\n  fieldinclude = [\"inodes*\"]\n```\n\n----------------------------------------\n\nTITLE: Example HTTP HEAD Request for Config File Status Check\nDESCRIPTION: An example curl command demonstrating the HTTP HEAD request that Telegraf would use to check for updates to a remote configuration file, showing the Last-Modified header that would be used to detect changes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-007-url-config-behavior.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ curl --head http://localhost:8000/config.toml\nHTTP/1.0 200 OK\nServer: SimpleHTTP/0.6 Python/3.12.3\nDate: Mon, 29 Apr 2024 18:18:56 GMT\nContent-type: application/octet-stream\nContent-Length: 1336\nLast-Modified: Mon, 29 Apr 2024 11:44:19 GMT\n```\n\n----------------------------------------\n\nTITLE: Building Customized Telegraf with Make\nDESCRIPTION: Demonstrates how to build a customized version of Telegraf using the project's makefile with the BUILDTAGS environment variable. This example builds Telegraf including all inputs, the InfluxDB v2 output, and the JSON parser.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CUSTOMIZATION.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nBUILDTAGS=\"custom,inputs,outputs.influxdb_v2,parsers.json\" make\n```\n\n----------------------------------------\n\nTITLE: Example Output from Filecount Plugin\nDESCRIPTION: Sample output showing the metrics generated by the Filecount plugin including file counts, total sizes, and timestamp information for different directories.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/filecount/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfilecount,directory=/var/cache/apt count=7i,size_bytes=7438336i,oldest_file_timestamp=1507152973123456789i,newest_file_timestamp=1507152973123456789i 1530034445000000000\nfilecount,directory=/tmp count=17i,size_bytes=28934786i,oldest_file_timestamp=1507152973123456789i,newest_file_timestamp=1507152973123456789i 1530034445000000000\n```\n\n----------------------------------------\n\nTITLE: Checking and Setting BSD/Darwin UDP/IP Buffer Limits (Shell)\nDESCRIPTION: This snippet includes commands for checking and setting the maximum socket buffer size on BSD/Darwin systems, which ensures that sufficient memory is allocated for efficient UDP traffic handling.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socket_listener/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n\"\"\"\nsysctl kern.ipc.maxsockbuf\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Enabling RCON on Minecraft Server (Configuration File)\nDESCRIPTION: This configuration snippet shows necessary settings in the server.properties file required to enable the RCON protocol on a Minecraft server.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/minecraft/README.md#2025-04-16_snippet_1\n\nLANGUAGE: conf\nCODE:\n```\n\"enable-rcon=true\\nrcon.password=<your password>\\nrcon.port=<1-65535>\"\n```\n\n----------------------------------------\n\nTITLE: Basic OneAgent Configuration for Dynatrace Output\nDESCRIPTION: Minimal configuration for running Telegraf with Dynatrace OneAgent, where no additional options are required as metrics are exported via the local OneAgent.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/dynatrace/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.dynatrace]]\n  ## No options are required. By default, metrics will be exported via the OneAgent on the local host.\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes Pod Metrics with Telegraf\nDESCRIPTION: This snippet captures readiness status and other metrics for a specific Kubernetes Pod.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_pod,namespace=default,node_name=ip-172-17-0-2.internal,pod_name=tick1 last_transition_time=1547578322000000000i,ready=\"false\" 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: JSONata Batch Transformation - Metric Grouping\nDESCRIPTION: JSONata expression to restructure and combine metrics by grouping and filtering\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"time\": $min(metrics.timestamp) * 1000 ~> $fromMillis(),\n    \"images\": metrics{\n        tags.host: {\n            name: fields.n_images\n        }\n    },\n    \"capacity alerts\": metrics[fields.n_images < 10].[(tags.host & \" \" & name)]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Printer Processor Plugin in Telegraf using TOML\nDESCRIPTION: Sample configuration for the Printer Processor Plugin in Telegraf. It includes options for controlling line length, field ordering, unsigned integer support, and timestamp handling. These settings are primarily for debugging purposes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/printer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Print all metrics that pass through this filter.\n[[processors.printer]]\n  ## Maximum line length in bytes.  Useful only for debugging.\n  # influx_max_line_bytes = 0\n\n  ## When true, fields will be output in ascending lexical order.  Enabling\n  ## this option will result in decreased performance and is only recommended\n  ## when you need predictable ordering while debugging.\n  # influx_sort_fields = false\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: `42u`.  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n  ## When true, Telegraf will omit the timestamp on data to allow InfluxDB\n  ## to set the timestamp of the data during ingestion. This is generally NOT\n  ## what you want as it can lead to data points captured at different times\n  ## getting omitted due to similar data.\n  # influx_omit_timestamp = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Collection Concurrency in TOML\nDESCRIPTION: Configuration settings for controlling concurrent collection and discovery of objects and metrics in vSphere environments.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n  ## number of go routines to use for collection and discovery of objects and metrics\n  # collect_concurrency = 1\n  # discover_concurrency = 1\n```\n\n----------------------------------------\n\nTITLE: Sample Influx Line Protocol Data Format\nDESCRIPTION: Example showing weather and air quality measurements in Influx line protocol format, demonstrating measurements, tags, and fields structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/timestream/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nweather,location=us-midwest,season=summer temperature=82,humidity=71 1465839830100400200\nairquality,location=us-west no2=5,pm25=16 1465839830100400200\n```\n\n----------------------------------------\n\nTITLE: Example Output of Wireless Plugin\nDESCRIPTION: This is an example of the output produced by the Telegraf Wireless plugin in Line Protocol format.  It shows the various metrics collected, including signal strength (level), noise, link quality, and packet statistics for a specific wireless interface.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/wireless/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nwireless,host=example.localdomain,interface=wlan0 misc=0i,frag=0i,link=60i,level=-50i,noise=-256i,nwid=0i,crypt=0i,retry=1525i,missed_beacon=0i,status=0i 1519843022000000000\n\n```\n\n----------------------------------------\n\nTITLE: Simplified vSAN Metric Collection Configuration\nDESCRIPTION: Example showing a simplified configuration for collecting specific vSAN metrics, including summary statistics and performance metrics for hosts and storage devices.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n  vsan_metric_include = [\"summary.*\", \"performance.host-domclient\", \"performance.cache-disk\", \"performance.disk-group\", \"performance.capacity-disk\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring WMI Input Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Windows Management Instrumentation input plugin with basic settings for querying WMI classes and properties\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  ## Hostname or IP for remote connections, by default the local machine is queried\n  # host = \"\"\n  ## Credentials for the connection, by default no credentials are used\n  # username = \"\"\n  # password = \"\"\n\n  ## WMI query to execute, multiple methods are possible\n  [[inputs.win_wmi.query]]\n    ## Namespace, class and a list of properties to use in the WMI query\n    namespace = \"root\\\\cimv2\"\n    class_name = \"Win32_Volume\"\n    properties = [\"Name\", \"Capacity\", \"FreeSpace\"]\n    ## Optional WHERE clause for the WQL query\n    # filter = 'NOT Name LIKE \"\\\\\\\\?\\\\%\"'\n    ## Returned properties to use as tags instead of fields\n    # tag_properties = [\"Name\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Performance Counter Sources\nDESCRIPTION: Specifies remote or local hosts to collect performance counters from, with authentication requirements\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_6\n\nLANGUAGE: TOML\nCODE:\n```\nSources = [\"localhost\", \"SQL-SERVER-01\", \"SQL-SERVER-02\", \"SQL-SERVER-03\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring NSDP Input Plugin in Telegraf\nDESCRIPTION: This snippet shows the TOML configuration for the Netgear Switch Discovery Protocol input plugin. It includes options for specifying the target address, setting device response limits, and configuring timeout duration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsdp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather Netgear Switch Discovery Protocol status\n[[inputs.nsdp]]\n  ## The target address to use for status gathering. Either Broadcast (default)\n  ## or the address of a single well-known device.\n  # address = \"255.255.255.255:63322\"\n\n  ## The maximum number of device responses to wait for. 0 means no limit.\n  ## NSDP works asynchronously. Without a limit (0) the plugin always waits\n  ## the amount given in timeout for possible responses. By setting this\n  ## option to the known number of devices, the plugin completes\n  ## processing as soon as the last device has answered.\n  # device_limit = 0\n\n  ## The maximum duration to wait for device responses.\n  # timeout = \"2s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Lustre Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the Lustre input plugin with optional proc file path configurations for MGS, OST, and MDS metrics collection\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lustre2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from local Lustre service on OST, MDS\n# This plugin ONLY supports Linux\n[[inputs.lustre2]]\n  ## An array of /proc globs to search for Lustre stats\n  ## If not specified, the default will work on Lustre 2.12.x\n  ##\n  # mgs_procfiles = [\n  #   \"/sys/fs/lustre/mgs/*/eviction_count\",\n  # ]\n  # ost_procfiles = [\n  #   \"/proc/fs/lustre/obdfilter/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/exports/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/brw_stats\",\n  #   \"/proc/fs/lustre/osd-zfs/*/brw_stats\",\n  #   \"/sys/fs/lustre/odbfilter/*/eviction_count\",\n  # ]\n  # mds_procfiles = [\n  #   \"/proc/fs/lustre/mdt/*/md_stats\",\n  #   \"/proc/fs/lustre/mdt/*/job_stats\",\n  #   \"/proc/fs/lustre/mdt/*/exports/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/brw_stats\",\n  #   \"/proc/fs/lustre/osd-zfs/*/brw_stats\",\n  #   \"/sys/fs/lustre/mdt/*/eviction_count\",\n  # ]\n```\n\n----------------------------------------\n\nTITLE: Scaling Example Configuration in TOML\nDESCRIPTION: This TOML configuration defines a scaling for the `cpu` field, mapping an input range of 0.0 to 50.0 to an output range of 50.0 to 100.0. This snippet shows how to implement scaling with minimum and maximum values. The scaling is applied to the field named 'cpu'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/scale/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.scale.scaling]]\\n    input_minimum = 0.0\\n    input_maximum = 50.0\\n    output_minimum = 50.0\\n    output_maximum = 100.0\\n    fields = [\\\"cpu\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Filepath Processor Plugin in Telegraf\nDESCRIPTION: Sample configuration for the filepath processor plugin showing all available options including basename, dirname, stem, clean, rel, and toslash transformations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Performs file path manipulations on tags and fields\n[[processors.filepath]]\n  ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag\n  # [[processors.filepath.basename]]\n  #   tag = \"path\"\n  #   dest = \"basepath\"\n\n  ## Treat the field value as a path and keep all but the last element of path, typically the path's directory\n  # [[processors.filepath.dirname]]\n  #   field = \"path\"\n\n  ## Treat the tag value as a path, converting it to its the last element without its suffix\n  # [[processors.filepath.stem]]\n  #   tag = \"path\"\n\n  ## Treat the tag value as a path, converting it to the shortest path name equivalent\n  ## to path by purely lexical processing\n  # [[processors.filepath.clean]]\n  #   tag = \"path\"\n\n  ## Treat the tag value as a path, converting it to a relative path that is lexically\n  ## equivalent to the source path when joined to 'base_path'\n  # [[processors.filepath.rel]]\n  #   tag = \"path\"\n  #   base_path = \"/var/log\"\n\n  ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only\n  ## effect on Windows\n  # [[processors.filepath.toslash]]\n  #   tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Example Output Format for OpenWeatherMap Data\nDESCRIPTION: Sample output showing the format of collected weather data including measurements for San Francisco. The output includes various weather metrics like pressure, temperature, wind conditions, and cloud coverage.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openweathermap/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nweather,city=San\\ Francisco,city_id=5391959,condition_id=803,condition_main=Clouds,country=US,forecast=114h,host=robot pressure=1027,temperature=10.09,wind_degrees=34,wind_speed=1.24,condition_description=\"broken clouds\",cloudiness=80i,humidity=67i,rain=0,feels_like=8.9,condition_icon=\"04n\" 1645952400000000000\nweather,city=San\\ Francisco,city_id=5391959,condition_id=804,condition_main=Clouds,country=US,forecast=117h,host=robot humidity=65i,rain=0,temperature=10.12,wind_degrees=31,cloudiness=90i,pressure=1026,feels_like=8.88,wind_speed=1.31,condition_description=\"overcast clouds\",condition_icon=\"04n\" 1645963200000000000\nweather,city=San\\ Francisco,city_id=5391959,condition_id=804,condition_main=Clouds,country=US,forecast=120h,host=robot cloudiness=100i,humidity=61i,rain=0,temperature=10.28,wind_speed=1.94,condition_icon=\"04d\",pressure=1027,feels_like=8.96,wind_degrees=16,condition_description=\"overcast clouds\" 1645974000000000000\n```\n\n----------------------------------------\n\nTITLE: Dedup Processor Example Using Diff Format\nDESCRIPTION: Demonstration of how the Dedup processor works, showing the input metrics (negative lines) and the resulting filtered output (positive lines). The processor removes consecutive duplicate field values while preserving the first occurrence of each unique value.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/dedup/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- cpu,cpu=cpu0 time_idle=42i,time_guest=1i\n- cpu,cpu=cpu0 time_idle=42i,time_guest=2i\n- cpu,cpu=cpu0 time_idle=42i,time_guest=2i\n- cpu,cpu=cpu0 time_idle=44i,time_guest=2i\n- cpu,cpu=cpu0 time_idle=44i,time_guest=2i\n+ cpu,cpu=cpu0 time_idle=42i,time_guest=1i\n+ cpu,cpu=cpu0 time_idle=42i,time_guest=2i\n+ cpu,cpu=cpu0 time_idle=44i,time_guest=2i\n```\n\n----------------------------------------\n\nTITLE: Running Custom Builder with Configuration File\nDESCRIPTION: This command runs the custom_builder tool using a specified configuration file to build a customized Telegraf binary. Dependencies include the custom_builder tool and a valid configuration file at the specified path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# ./tools/custom_builder/custom_builder --config /etc/telegraf/telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: Sample PostgreSQL Query Configurations\nDESCRIPTION: Example query configurations showing various monitoring scenarios including database stats, background writer stats, and session information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.postgresql_extensible.query]]\n  sqlquery=\"SELECT * FROM pg_stat_database\"\n  version=901\n  withdbname=false\n  tagvalue=\"\"\n[[inputs.postgresql_extensible.query]]\n  sqlquery=\"SELECT * FROM pg_stat_bgwriter\"\n  version=901\n  withdbname=false\n  tagvalue=\"\"\n[[inputs.postgresql_extensible.query]]\n  sqlquery=\"select * from sessions\"\n  version=901\n  withdbname=false\n  tagvalue=\"db,username,state\"\n```\n\n----------------------------------------\n\nTITLE: CLI Flag Definition for URL Config Retry Attempts in Telegraf\nDESCRIPTION: Defines the '--config-url-retry-attempts' CLI flag for Telegraf that controls how many times to retry obtaining a remote configuration during startup, with a default of 3 attempts and support for unlimited retries with -1.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-007-url-config-behavior.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n--config-url-retry-attempts=3   Number of times to attempt to obtain a remote\n                                configuration via a URL during startup. Set to\n                                -1 for unlimited attempts.\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf File Input with XML and XPath\nDESCRIPTION: This code snippet is a configuration example for the Telegraf input plugin, designed to process XML files. It utilizes XPath queries to specify how fields and tags are extracted from the XML document. Dependencies include Telegraf and its configuration system, as well as familiarity with XML and XPath syntax. The configuration allows for setting the data format to XML, defining metric selection and name, and specifying paths for field and tag extraction. It expects valid XML files as inputs and outputs metrics to be processed by Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.xml\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"xml\"\n\n  ## PROTOCOL-BUFFER definitions\n  ## Protocol-buffer definition file\n  # xpath_protobuf_file = [\"sparkplug_b.proto\"]\n  ## Name of the protocol-buffer message type to use in a fully qualified form.\n  # xpath_protobuf_type = \"org.eclipse.tahu.protobuf.Payload\"\n  ## List of paths to use when looking up imported protocol-buffer definition files.\n  # xpath_protobuf_import_paths = [\".\"]\n\n  ## Print the internal XML document when in debug logging mode.\n  ## This is especially useful when using the parser with non-XML formats like protocol-buffers\n  ## to get an idea on the expression necessary to derive fields etc.\n  # xpath_print_document = false\n\n  ## Allow the results of one of the parsing sections to be empty.\n  ## Useful when not all selected files have the exact same structure.\n  # xpath_allow_empty_selection = false\n\n  ## Get native data-types for all data-format that contain type information.\n  ## Currently, protobuf, msgpack and JSON support native data-types\n  # xpath_native_types = false\n\n  ## Multiple parsing sections are allowed\n  [[inputs.file.xpath]]\n    ## Optional: XPath-query to select a subset of nodes from the XML document.\n    metric_selection = \"/Bus/child::Sensor\"\n\n    ## Optional: XPath-query to set the metric (measurement) name.\n    # metric_name = \"string('example')\"\n\n    ## Optional: Query to extract metric timestamp.\n    ## If not specified the time of execution is used.\n    # timestamp = \"/Gateway/Timestamp\"\n    ## Optional: Format of the timestamp determined by the query above.\n    ## This can be any of \"unix\", \"unix_ms\", \"unix_us\", \"unix_ns\" or a valid Golang\n    ## time format. If not specified, a \"unix\" timestamp (in seconds) is expected.\n    # timestamp_format = \"2006-01-02T15:04:05Z\"\n\n    ## Field specifications using a selector.\n    field_selection = \"child::*\"\n    ## Optional: Queries to specify field name and value.\n    ## These options are only to be used in combination with 'field_selection'!\n    ## By default the node name and node content is used if a field-selection\n    ## is specified.\n    # field_name  = \"name()\"\n    # field_value = \".\"\n\n    ## Optional: Expand field names relative to the selected node\n    ## This allows to flatten out nodes with non-unique names in the subtree\n    # field_name_expansion = false\n\n    ## Tag specifications using a selector.\n    ## tag_selection = \"child::*\"\n    ## Optional: Queries to specify tag name and value.\n    ## These options are only to be used in combination with 'tag_selection'!\n    ## By default the node name and node content is used if a tag-selection\n    ## is specified.\n    # tag_name  = \"name()\"\n    # tag_value = \".\"\n\n    ## Optional: Expand tag names relative to the selected node\n    ## This allows to flatten out nodes with non-unique names in the subtree\n    # tag_name_expansion = false\n\n    ## Tag definitions using the given XPath queries.\n    [inputs.file.xpath.tags]\n      name   = \"substring-after(Sensor/@name, ' ')\"\n      device = \"string('the ultimate sensor')\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Executable Output Plugin in Telegraf (TOML)\nDESCRIPTION: This snippet shows the configuration options for the Executable Output Plugin in Telegraf. It includes settings for the command to execute, environment variables, timeout, batch processing, and data format selection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/exec/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics to command as input over stdin\n[[outputs.exec]]\n  ## Command to ingest metrics via stdin.\n  command = [\"tee\", \"-a\", \"/dev/null\"]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Timeout for command to complete.\n  # timeout = \"5s\"\n\n  ## Whether the command gets executed once per metric, or once per metric batch\n  ## The serializer will also run in batch mode when this is true.\n  # use_batch_format = true\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n```\n\n----------------------------------------\n\nTITLE: Example Output from Apache Input Plugin\nDESCRIPTION: Sample output showing the metrics collected by the Apache input plugin, including worker stats, CPU metrics, connection information, and scoreboard values with corresponding tags for port and server.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/apache/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\napache,port=80,server=debian-stretch-apache BusyWorkers=1,BytesPerReq=0,BytesPerSec=0,CPUChildrenSystem=0,CPUChildrenUser=0,CPULoad=0.00995025,CPUSystem=0.01,CPUUser=0.01,ConnsAsyncClosing=0,ConnsAsyncKeepAlive=0,ConnsAsyncWriting=0,ConnsTotal=0,IdleWorkers=49,Load1=0.01,Load15=0,Load5=0,ParentServerConfigGeneration=3,ParentServerMPMGeneration=2,ReqPerSec=0.00497512,ServerUptimeSeconds=201,TotalAccesses=1,TotalkBytes=0,Uptime=201,scboard_closing=0,scboard_dnslookup=0,scboard_finishing=0,scboard_idle_cleanup=0,scboard_keepalive=0,scboard_logging=0,scboard_open=100,scboard_reading=0,scboard_sending=1,scboard_starting=0,scboard_waiting=49 1502489900000000000\n```\n\n----------------------------------------\n\nTITLE: Example Basename Configuration in Telegraf\nDESCRIPTION: Configuration example showing how to extract the filename from a path and replace the original path tag with it.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n  [[processors.filepath.basename]]\n    tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Stream Server Traffic Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Nginx Stream Server Traffic Input Plugin. It includes settings for URLs, response timeout, and optional TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_sts/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Nginx virtual host traffic status module information (nginx-module-sts)\n[[inputs.nginx_sts]]\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Example Output of Network Connection Statistics in Telegraf\nDESCRIPTION: This example shows the output format of the netstat plugin in Telegraf. It includes various TCP connection states and UDP socket counts, with their respective values and a timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netstat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnetstat tcp_close=0i,tcp_close_wait=0i,tcp_closing=0i,tcp_established=14i,tcp_fin_wait1=0i,tcp_fin_wait2=0i,tcp_last_ack=0i,tcp_listen=1i,tcp_none=46i,tcp_syn_recv=0i,tcp_syn_sent=0i,tcp_time_wait=0i,udp_socket=10i 1668520568000000000\n```\n\n----------------------------------------\n\nTITLE: Adding Field Exclusion for Deprecated Metrics in TOML\nDESCRIPTION: Demonstrates how to add filtering options in the sample configuration to help users remove deprecated metrics. The configuration is commented out by default.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEPRECATION.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.system]]\n  ## Uncomment to remove deprecated metrics.\n  # fieldexclude = [\"uptime_format\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Required Kernel Modules for Intel PowerStat\nDESCRIPTION: Shell commands to load necessary kernel modules including RAPL, MSR, and Intel uncore frequency modules. Different commands are provided based on kernel version compatibility.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# rapl modules:\n## kernel < 4.0\nsudo modprobe intel_rapl\n## kernel >= 4.0\nsudo modprobe rapl\nsudo modprobe intel_rapl_common\nsudo modprobe intel_rapl_msr\n\n# msr module:\nsudo modprobe msr\n\n# cpufreq module:\n### integrated in kernel\n\n# intel-uncore-frequency module:\n## only for kernel >= 5.6.0\nsudo modprobe intel-uncore-frequency\n```\n\n----------------------------------------\n\nTITLE: Example Rel Configuration in Telegraf\nDESCRIPTION: Configuration example showing how to convert a path tag to be relative to a specified base path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n  [[processors.filepath.rel]]\n    tag = \"path\"\n    base_path = \"/var/log\"\n```\n\n----------------------------------------\n\nTITLE: Explicit Specification Example in Regex Processor\nDESCRIPTION: Example configuration showing explicit specification of tag and field transformations with respective result patterns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/regex/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.regex]]\n  namepass = [\"nginx_requests\"]\n\n  [[processors.regex.tags]]\n    key = \"resp_code\"\n    pattern = \"^(\\\\d)\\\\d\\\\d$\"\n    replacement = \"${1}xx\"\n\n  [[processors.regex.fields]]\n    key = \"request\"\n    pattern = \"^/api(?P<method>/[\\\\w/]+)\\\\S*\"\n    replacement = \"${method}\"\n    result_key = \"method\"\n\n  [[processors.regex.fields]]\n    key = \"request\"\n    pattern = \".*category=(\\\\w+).*\"\n    replacement = \"${1}\"\n    result_key = \"search_category\"\n\n  [[processors.regex.field_rename]]\n    pattern = \"^client_(\\\\w+)$\"\n    replacement = \"${1}\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Comment Directive\nDESCRIPTION: A markdown comment that disables markdownlint checks\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/gnmi/testcases/issue_15046/models/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!-- markdownlint-disable -->\n```\n\n----------------------------------------\n\nTITLE: Configuring S2 Geo Processor in Telegraf\nDESCRIPTION: Sample configuration for the S2 Geo processor plugin. It shows how to set up the plugin to add S2 cell ID tags based on latitude and longitude fields, including options for field names, tag key, and cell level.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/s2geo/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Add the S2 Cell ID as a tag based on latitude and longitude fields\n[[processors.s2geo]]\n  ## The name of the lat and lon fields containing WGS-84 latitude and\n  ## longitude in decimal degrees.\n  # lat_field = \"lat\"\n  # lon_field = \"lon\"\n\n  ## New tag to create\n  # tag_key = \"s2_cell_id\"\n\n  ## Cell level (see https://s2geometry.io/resources/s2cell_statistics.html)\n  # cell_level = 9\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes ResourceQuota Metrics with Telegraf\nDESCRIPTION: This snippet captures ResourceQuota metrics, tracking used and hard limits for various resources.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_resourcequota,host=vjain,namespace=default,resource=pods-high hard_cpu=1000i,hard_memory=214748364800i,hard_pods=10i,used_cpu=0i,used_memory=0i,used_pods=0i 1629110393000000000\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_resourcequota,host=vjain,namespace=default,resource=pods-low hard_cpu=5i,hard_memory=10737418240i,hard_pods=10i,used_cpu=0i,used_memory=0i,used_pods=0i 1629110393000000000\n```\n\n----------------------------------------\n\nTITLE: Example Output in Line Protocol Format\nDESCRIPTION: This snippet shows the expected output of the example plugin in Line Protocol format. It includes two measurements with their respective tags and fields, demonstrating the structure of the data collected by the plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/example/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmeasurement1,tag1=foo,tag2=bar field1=1i,field2=2.1 1453831884664956455\nmeasurement2,tag1=foo,tag2=bar,tag3=baz field3=1i 1453831884664956455\n```\n\n----------------------------------------\n\nTITLE: Configuring Icinga2 Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Icinga2 input plugin for Telegraf. It includes settings for server address, objects to collect, status metrics, authentication, timeout, and TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/icinga2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather Icinga2 status\n[[inputs.icinga2]]\n  ## Required Icinga2 server address\n  # server = \"https://localhost:5665\"\n\n  ## Collected Icinga2 objects (\"services\", \"hosts\")\n  ## Specify at least one object to collect from /v1/objects endpoint.\n  # objects = [\"services\"]\n\n  ## Collect metrics from /v1/status endpoint\n  ## Choose from:\n  ##     \"ApiListener\", \"CIB\", \"IdoMysqlConnection\", \"IdoPgsqlConnection\"\n  # status = []\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = true\n```\n\n----------------------------------------\n\nTITLE: JSONata Transformation - Arithmetic and Renaming\nDESCRIPTION: JSONata expression to perform calculations and rename metric properties\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"capacity\": $sum($sift($.fields,function($value,$key){$key~>/^field_/}).*),\n    \"images\": fields.n_images,\n    \"host\": tags.host,\n    \"time\": $fromMillis(timestamp*1000)\n}\n```\n\n----------------------------------------\n\nTITLE: Complete OpenStack Plugin Configuration in TOML\nDESCRIPTION: Detailed configuration example for the OpenStack input plugin including authentication, service selection, security settings, and various optional parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openstack/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.openstack]]\n  ## The recommended interval to poll is '30m'\n\n  ## The identity endpoint to authenticate against and get the service catalog from.\n  authentication_endpoint = \"https://my.openstack.cloud:5000\"\n\n  ## The domain to authenticate against when using a V3 identity endpoint.\n  # domain = \"default\"\n\n  ## The project to authenticate as.\n  # project = \"admin\"\n\n  ## User authentication credentials. Must have admin rights.\n  username = \"admin\"\n  password = \"password\"\n\n  ## Available services are:\n  ## \"agents\", \"aggregates\", \"cinder_services\", \"flavors\", \"hypervisors\",\n  ## \"networks\", \"nova_services\", \"ports\", \"projects\", \"servers\",\n  ## \"serverdiagnostics\", \"services\", \"stacks\", \"storage_pools\", \"subnets\",\n  ## \"volumes\"\n  # enabled_services = [\"services\", \"projects\", \"hypervisors\", \"flavors\", \"networks\", \"volumes\"]\n\n  ## Query all instances of all tenants for the volumes and server services\n  ## NOTE: Usually this is only permitted for administrators!\n  # query_all_tenants = true\n\n  ## output secrets (such as adminPass(for server) and UserID(for volume)).\n  # output_secrets = false\n\n  ## Amount of time allowed to complete the HTTP(s) request.\n  # timeout = \"5s\"\n\n  ## HTTP Proxy support\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Options for tags received from Openstack\n  # tag_prefix = \"openstack_tag_\"\n  # tag_value = \"true\"\n\n  ## Timestamp format for timestamp data received from Openstack.\n  ## If false format is unix nanoseconds.\n  # human_readable_timestamps = false\n\n  ## Measure Openstack call duration\n  # measure_openstack_requests = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Output Settings\nDESCRIPTION: This TOML snippet configures the Prometheus output in Telegraf, specifying parameters like file outputs, usage of batch formats, and adjusting metadata inclusion such as timestamps and sorting. The primary dependency is Telegraf, and the setup aims to format and export data in Prometheus format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/prometheus/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  files = [\"stdout\"]\n  use_batch_format = true\n\n  ## Include the metric timestamp on each sample.\n  prometheus_export_timestamp = false\n\n  ## Sort prometheus metric families and metric samples.  Useful for\n  ## debugging.\n  prometheus_sort_metrics = false\n\n  ## Output string fields as metric labels; when false string fields are\n  ## discarded.\n  prometheus_string_as_label = false\n\n  ## Encode metrics without HELP metadata. This helps reduce the payload\n  ## size.\n  prometheus_compact_encoding = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"prometheus\"\n\n  ## Specify the metric type explicitly.\n  ## This overrides the metric-type of the Telegraf metric. Globbing is allowed.\n  [outputs.file.prometheus_metric_types]\n    counter = []\n    gauge = []\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Parser in Telegraf\nDESCRIPTION: Configuration example showing how to set up the Prometheus text format parser in Telegraf's file input plugin. The configuration specifies the input files and sets the data format to 'prometheus'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"prometheus\"\n\n```\n\n----------------------------------------\n\nTITLE: Example Output from Swap Input Plugin\nDESCRIPTION: This snippet illustrates the expected output format from the Swap Input Plugin when retrieving metrics on swap memory usage. It provides specific values for total swap memory, percentage of used memory, and other related metrics in a suitable text format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/swap/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nswap total=20855394304i,used_percent=45.43883523785713,used=9476448256i,free=1715331072i 1511894782000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring the dmcache Input Plugin in Telegraf\nDESCRIPTION: Configuration sample for the dmcache input plugin in Telegraf. This sets up collection of dm-cache statistics with the option to report per-device stats. The plugin only supports Linux systems and requires superuser permissions to run dmsetup commands.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dmcache/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Provide a native collection for dmsetup based statistics for dm-cache\n# This plugin ONLY supports Linux\n[[inputs.dmcache]]\n  ## Whether to report per-device stats or not\n  per_device = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Quix Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Quix output plugin for Telegraf. It specifies the workspace, topic, and authentication token for sending metrics to Quix. Additional options include the API endpoint URL and timeout settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/quix/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics to a Quix data processing pipeline\n[[outputs.quix]]\n  ## Endpoint for providing the configuration\n  # url = \"https://portal-api.platform.quix.io\"\n\n  ## Workspace and topics to send the metrics to\n  workspace = \"your_workspace\"\n  topic = \"your_topic\"\n\n  ## Authentication token created in Quix\n  token = \"your_auth_token\"\n\n  ## Amount of time allowed to complete the HTTP request for fetching the config\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: Post CouchDB 2.0 Output Format\nDESCRIPTION: Example of metrics output format for CouchDB versions 2.0 and later. Shows various statistics including auth cache, HTTP methods, status codes, and database operations in InfluxDB line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/couchdb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncouchdb,server=http://couchdb22:5984/_node/_local/_stats couchdb_auth_cache_hits_value=0,httpd_request_methods_delete_value=0,couchdb_auth_cache_misses_value=0,httpd_request_methods_get_value=42,httpd_status_codes_304_value=0,httpd_status_codes_400_value=0,httpd_request_methods_head_value=0,httpd_status_codes_201_value=0,couchdb_database_reads_value=0,httpd_request_methods_copy_value=0,couchdb_request_time_max=0,httpd_status_codes_200_value=42,httpd_status_codes_301_value=0,couchdb_open_os_files_value=2,httpd_request_methods_put_value=0,httpd_request_methods_post_value=0,httpd_status_codes_202_value=0,httpd_status_codes_403_value=0,httpd_status_codes_409_value=0,couchdb_database_writes_value=0,couchdb_request_time_min=0,httpd_status_codes_412_value=0,httpd_status_codes_500_value=0,httpd_status_codes_401_value=0,httpd_status_codes_404_value=0,httpd_status_codes_405_value=0,couchdb_open_databases_value=0 1536707179000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx VTS Plugin in Telegraf\nDESCRIPTION: Sample configuration for the Nginx Virtual Host Traffic Status plugin showing URL setup, response timeout settings, and optional TLS configuration options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_vts/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Nginx virtual host traffic status module information (nginx-module-vts)\n[[inputs.nginx_vts]]\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Retrieving Host IP in Kubernetes\nDESCRIPTION: Shell command to retrieve the host IP address using the Kubernetes API with authentication token and namespace information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kubernetes/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -s $API_URL/api/v1/namespaces/$POD_NAMESPACE/pods/$HOSTNAME \\\n  --header \"Authorization: Bearer $TOKEN\" \\\n  --insecure | jq -r '.status.hostIP'\n```\n\n----------------------------------------\n\nTITLE: Configuring CloudWatch Metric Streams Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows the available settings for the CloudWatch Metric Streams input plugin in Telegraf. It includes options for service address, paths, timeouts, security settings, and API compatibility.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch_metric_streams/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cloudwatch_metric_streams]]\n  ## Address and port to host HTTP listener on\n  service_address = \":443\"\n\n  ## Paths to listen to.\n  # paths = [\"/telegraf\"]\n\n  ## maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n\n  ## maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Optional access key for Firehose security.\n  # access_key = \"test-key\"\n\n  ## An optional flag to keep Metric Streams metrics compatible with\n  ## CloudWatch's API naming\n  # api_compatability = false\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom State Structure in Go\nDESCRIPTION: This snippet shows an example of defining a custom state structure that can be used for state-persistence in a Telegraf plugin. The structure includes fields for tokens and filter IDs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/STATE_PERSISTENCE.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype MyState struct {\n    CurrentToken string\n    LastToken    string\n    NextToken    string\n    FilterIDs    []int64\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Network Metrics for VSphere in Text Format\nDESCRIPTION: This snippet includes examples of network metrics from virtual machines in a VSphere environment. Each entry captures the average bytes received and transmitted over the network, essential for assessing network performance and load.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_net,esxhostname=DC0_H0,guest=other,host=host.example.com,moid=vm-35,os=Mac,source=DC0_H0_VM0,vcenter=localhost:8989,vmname=DC0_H0_VM0 bytesRx_average=321i,bytesTx_average=335i 1535660299000000000\n```\n\nLANGUAGE: text\nCODE:\n```\nvsphere_vm_net,esxhostname=DC0_H0,guest=other,host=host.example.com,moid=vm-38,os=Mac,source=DC0_H0_VM1,vcenter=localhost:8989,vmname=DC0_H0_VM1 bytesRx_average=242i,bytesTx_average=308i 1535660299000000000\n```\n\n----------------------------------------\n\nTITLE: Scaling Configuration in TOML\nDESCRIPTION: This TOML configuration demonstrates how to define scaling parameters within the `processors.scale` plugin. It shows the structure for specifying input/output minimum/maximum values, as well as the structure for scaling with a factor and offset. It includes commented-out examples for both scaling methods, and how to specify the fields to which the scaling should be applied.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/scale/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.scale]]\\n    ## It is possible to define multiple different scaling that can be applied\\n    ## do different sets of fields. Each scaling expects the following\\n    ## arguments:\\n    ##   - input_minimum: Minimum expected input value\\n    ##   - input_maximum: Maximum expected input value\\n    ##   - output_minimum: Minimum desired output value\\n    ##   - output_maximum: Maximum desired output value\\n    ## alternatively you can specify a scaling with factor and offset\\n    ##   - factor: factor to scale the input value with\\n    ##   - offset: additive offset for value after scaling\\n    ##   - fields: a list of field names (or filters) to apply this scaling to\\n\\n    ## Example: Scaling with minimum and maximum values\\n    # [[processors.scale.scaling]]\\n    #    input_minimum = 0.0\\n    #    input_maximum = 1.0\\n    #    output_minimum = 0.0\\n    #    output_maximum = 100.0\\n    #    fields = [\\\"temperature1\\\", \\\"temperature2\\\"]\\n\\n    ## Example: Scaling with factor and offset\\n    # [[processors.scale.scaling]]\\n    #    factor = 10.0\\n    #    offset = -5.0\\n    #    fields = [\\\"voltage*\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Creating View for Blocking Sessions in PostgreSQL\nDESCRIPTION: This SQL code creates a view named 'blocking_procs' to identify blocking and blocked sessions in PostgreSQL. It joins pg_locks and pg_stat_activity tables to show detailed information about blocking queries, blocked queries, and their duration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE VIEW public.blocking_procs AS\n SELECT a.datname AS db,\n    kl.pid AS blocking_pid,\n    ka.usename AS blocking_user,\n    ka.query AS blocking_query,\n    bl.pid AS blocked_pid,\n    a.usename AS blocked_user,\n    a.query AS blocked_query,\n    to_char(age(now(), a.query_start), 'HH24h:MIm:SSs'::text) AS age\n   FROM pg_locks bl\n     JOIN pg_stat_activity a ON bl.pid = a.pid\n     JOIN pg_locks kl ON bl.locktype = kl.locktype AND NOT bl.database IS\n     DISTINCT FROM kl.database AND NOT bl.relation IS DISTINCT FROM kl.relation\n     AND NOT bl.page IS DISTINCT FROM kl.page AND NOT bl.tuple IS DISTINCT FROM\n     kl.tuple AND NOT bl.virtualxid IS DISTINCT FROM kl.virtualxid AND NOT\n     bl.transactionid IS DISTINCT FROM kl.transactionid AND NOT bl.classid IS\n     DISTINCT FROM kl.classid AND NOT bl.objid IS DISTINCT FROM kl.objid AND\n      NOT bl.objsubid IS DISTINCT FROM kl.objsubid AND bl.pid <> kl.pid\n     JOIN pg_stat_activity ka ON kl.pid = ka.pid\n  WHERE kl.granted AND NOT bl.granted\n  ORDER BY a.query_start;\n```\n\n----------------------------------------\n\nTITLE: Zabbix Sender JSON Format for LLD Data\nDESCRIPTION: JSON structure sent to Zabbix server for low-level discovery containing the discovery rule key and discovery macros.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"request\":\"sender data\",\n  \"data\":[\n    {\n      \"host\":\"myhost\",\n      \"key\":\"telegraf.lld.net_response.port.protocol.server\",\n      \"value\":\"{\\\"data\\\":[{\\\"{#PORT}\\\":\\\"80\\\",\\\"{#PROTOCOL}\\\":\\\"tcp\\\",\\\"{#SERVER}\\\":\\\"example.com\\\"}]}\",\n      \"clock\":1519043805\n    }\n  ],\n  \"clock\":1519043805\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Sentinel Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up the Redis Sentinel input plugin for Telegraf. It includes options for specifying server addresses, ports, and optional TLS configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/redis_sentinel/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or many redis-sentinel servers\n[[inputs.redis_sentinel]]\n  ## specify servers via a url matching:\n  ##  [protocol://][username:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:26379\n  ##    tcp://username:password@192.168.99.100\n  ##    unix:///var/run/redis-sentinel.sock\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 26379 is used\n  # servers = [\"tcp://localhost:26379\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = true\n```\n\n----------------------------------------\n\nTITLE: Configuring OPC UA Groups with Multiple Nodes in Telegraf\nDESCRIPTION: Complex configuration example showing three OPC UA groups with default settings and multiple nodes. Demonstrates hierarchical configuration with inheritance of settings from group to nodes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n  # Group 1\n  [[inputs.opcua_listener.group]]\n    name = \"group1_metric_name\"\n    namespace = \"3\"\n    identifier_type = \"i\"\n    default_tags = { group1_tag = \"val1\" }\n    [[inputs.opcua.group.nodes]]\n      name = \"name\"\n      identifier = \"1001\"\n      default_tags = { node1_tag = \"val2\" }\n    [[inputs.opcua.group.nodes]]\n      name = \"name\"\n      identifier = \"1002\"\n      default_tags = {node1_tag = \"val3\"}\n\n  # Group 2\n  [[inputs.opcua_listener.group]]\n    name = \"group2_metric_name\"\n    namespace = \"3\"\n    identifier_type = \"i\"\n    default_tags = { group2_tag = \"val3\" }\n    [[inputs.opcua.group.nodes]]\n      name = \"saw\"\n      identifier = \"1003\"\n      default_tags = { node2_tag = \"val4\" }\n    [[inputs.opcua.group.nodes]]\n      name = \"sin\"\n      identifier = \"1004\"\n\n  # Group 3\n  [[inputs.opcua_listener.group]]\n    name = \"group3_metric_name\"\n    namespace = \"3\"\n    identifier_type = \"i\"\n    default_tags = { group3_tag = \"val5\" }\n    nodes = [\n      {name=\"name\", identifier=\"1001\"},\n      {name=\"name\", identifier=\"1002\"},\n    ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Bcache Input Plugin in TOML\nDESCRIPTION: Configuration snippet for the Bcache input plugin showing how to specify the bcache path and devices to monitor. The plugin collects statistics from the stats_total directory and dirty_data file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bcache/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.bcache]]\n  ## Bcache sets path\n  ## If not specified, then default is:\n  bcachePath = \"/sys/fs/bcache\"\n\n  ## By default, Telegraf gather stats for all bcache devices\n  ## Setting devices will restrict the stats to the specified\n  ## bcache devices.\n  bcacheDevs = [\"bcache0\"]\n```\n\n----------------------------------------\n\nTITLE: Sample Tengine Metrics Output in Plain Text\nDESCRIPTION: This output exemplifies the metrics collected from the Tengine input plugin. The data includes various metrics such as bytes in/out, total connections, HTTP status counts, and upstream statistics, formatted in line protocol, suitable for monitoring and analysis.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tengine/README.md#2025-04-16_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\ntengine,host=gcp-thz-api-5,port=80,server=localhost,server_name=localhost bytes_in=9129i,bytes_out=56334i,conn_total=14i,http_200=90i,http_206=0i,http_2xx=90i,http_302=0i,http_304=0i,http_3xx=0i,http_403=0i,http_404=0i,http_416=0i,http_499=0i,http_4xx=0i,http_500=0i,http_502=0i,http_503=0i,http_504=0i,http_508=0i,http_5xx=0i,http_other_detail_status=0i,http_other_status=0i,http_ups_4xx=0i,http_ups_5xx=0i,req_total=90i,rt=0i,ups_req=0i,ups_rt=0i,ups_tries=0i 1526546308000000000\\ntengine,host=gcp-thz-api-5,port=80,server=localhost,server_name=28.79.190.35.bc.googleusercontent.com bytes_in=1500i,bytes_out=3009i,conn_total=4i,http_200=1i,http_206=0i,http_2xx=1i,http_302=0i,http_304=0i,http_3xx=0i,http_403=0i,http_404=1i,http_416=0i,http_499=0i,http_4xx=3i,http_500=0i,http_502=0i,http_503=0i,http_504=0i,http_508=0i,http_5xx=0i,http_other_detail_status=0i,http_other_status=0i,http_ups_4xx=0i,http_ups_5xx=0i,req_total=4i,rt=0i,ups_req=0i,ups_rt=0i,ups_tries=0i 1526546308000000000\\ntengine,host=gcp-thz-api-5,port=80,server=localhost,server_name=www.google.com bytes_in=372i,bytes_out=786i,conn_total=1i,http_200=1i,http_206=0i,http_2xx=1i,http_302=0i,http_304=0i,http_3xx=0i,http_403=0i,http_404=0i,http_416=0i,http_499=0i,http_4xx=0i,http_500=0i,http_502=0i,http_503=0i,http_504=0i,http_508=0i,http_5xx=0i,http_other_detail_status=0i,http_other_status=0i,http_ups_4xx=0i,http_ups_5xx=0i,req_total=1i,rt=0i,ups_req=0i,ups_rt=0i,ups_tries=0i 1526546308000000000\\ntengine,host=gcp-thz-api-5,port=80,server=localhost,server_name=35.190.79.28 bytes_in=4433i,bytes_out=10259i,conn_total=5i,http_200=3i,http_206=0i,http_2xx=3i,http_302=0i,http_304=0i,http_3xx=0i,http_403=0i,http_404=11i,http_416=0i,http_499=0i,http_4xx=11i,http_500=0i,http_502=0i,http_503=0i,http_504=0i,http_508=0i,http_5xx=0i,http_other_detail_status=0i,http_other_status=0i,http_ups_4xx=0i,http_ups_5xx=0i,req_total=14i,rt=0i,ups_req=0i,ups_rt=0i,ups_tries=0i 1526546308000000000\\ntengine,host=gcp-thz-api-5,port=80,server=localhost,server_name=tenka-prod-api.txwy.tw bytes_in=3014397400i,bytes_out=14279992835i,conn_total=36844i,http_200=3177339i,http_206=0i,http_2xx=3177339i,http_302=0i,http_304=0i,http_3xx=0i,http_403=0i,http_404=123i,http_416=0i,http_499=0i,http_4xx=123i,http_500=17214i,http_502=4453i,http_503=80i,http_504=0i,http_508=0i,http_5xx=21747i,http_other_detail_status=0i,http_other_status=0i,http_ups_4xx=123i,http_ups_5xx=21747i,req_total=3199209i,rt=245874536i,ups_req=2685076i,ups_rt=245858217i,ups_tries=2685076i 1526546308000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudo Privileges for OpenSMTPD in TOML\nDESCRIPTION: This TOML configuration enables sudo usage for the OpenSMTPD input plugin in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensmtpd/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensmtpd]]\n  use_sudo = true\n```\n\n----------------------------------------\n\nTITLE: GitHub Webhook Configuration Example\nDESCRIPTION: This snippet illustrates the toml configuration block for setting up Telegraf to receive Github webhook events, specifying tags and fields extracted from the events.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/github/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# TAGS\n* 'tagKey' = `tagValue` type\n# FIELDS\n* 'fieldKey' = `fieldValue` type\"\n```\n\n----------------------------------------\n\nTITLE: Standard JSON Format for Secrets Transformation\nDESCRIPTION: This JSON representation defines the standard flat key-value syntax expected for secrets fetched from an HTTP endpoint using the Telegraf plugin. No dependencies are necessary, and the structure supports straightforward key-value pairs, ensuring compatibility with the Telegraf transformation process.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/http/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"secret name A\": \"secret value A\",\n    ...\n    \"secret name X\": \"secret value X\"\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing Telegraf ClickHouse Metrics Output\nDESCRIPTION: Example of Telegraf output when collecting metrics from ClickHouse database, showing events, asynchronous metrics, system metrics, and table statistics in line protocol format. The output includes various performance counters, resource usage statistics, and table metadata from a ClickHouse cluster.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/clickhouse/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nclickhouse_events,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 read_compressed_bytes=212i,arena_alloc_chunks=35i,function_execute=85i,merge_tree_data_writer_rows=3i,rw_lock_acquired_read_locks=421i,file_open=46i,io_buffer_alloc_bytes=86451985i,inserted_bytes=196i,regexp_created=3i,real_time_microseconds=116832i,query=23i,network_receive_elapsed_microseconds=268i,merge_tree_data_writer_compressed_bytes=1080i,arena_alloc_bytes=212992i,disk_write_elapsed_microseconds=556i,inserted_rows=3i,compressed_read_buffer_bytes=81i,read_buffer_from_file_descriptor_read_bytes=148i,write_buffer_from_file_descriptor_write=47i,merge_tree_data_writer_blocks=3i,soft_page_faults=896i,hard_page_faults=7i,select_query=21i,merge_tree_data_writer_uncompressed_bytes=196i,merge_tree_data_writer_blocks_already_sorted=3i,user_time_microseconds=40196i,compressed_read_buffer_blocks=5i,write_buffer_from_file_descriptor_write_bytes=3246i,io_buffer_allocs=296i,created_write_buffer_ordinary=12i,disk_read_elapsed_microseconds=59347044i,network_send_elapsed_microseconds=1538i,context_lock=1040i,insert_query=1i,system_time_microseconds=14582i,read_buffer_from_file_descriptor_read=3i 1569421000000000000\nclickhouse_asynchronous_metrics,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 jemalloc.metadata_thp=0i,replicas_max_relative_delay=0i,jemalloc.mapped=1803177984i,jemalloc.allocated=1724839256i,jemalloc.background_thread.run_interval=0i,jemalloc.background_thread.num_threads=0i,uncompressed_cache_cells=0i,replicas_max_absolute_delay=0i,mark_cache_bytes=0i,compiled_expression_cache_count=0i,replicas_sum_queue_size=0i,number_of_tables=35i,replicas_max_merges_in_queue=0i,replicas_max_inserts_in_queue=0i,replicas_sum_merges_in_queue=0i,replicas_max_queue_size=0i,mark_cache_files=0i,jemalloc.background_thread.num_runs=0i,jemalloc.active=1726210048i,uptime=158i,jemalloc.retained=380481536i,replicas_sum_inserts_in_queue=0i,uncompressed_cache_bytes=0i,number_of_databases=2i,jemalloc.metadata=9207704i,max_part_count_for_partition=1i,jemalloc.resident=1742442496i 1569421000000000000\nclickhouse_metrics,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 replicated_send=0i,write=0i,ephemeral_node=0i,zoo_keeper_request=0i,distributed_files_to_insert=0i,replicated_fetch=0i,background_schedule_pool_task=0i,interserver_connection=0i,leader_replica=0i,delayed_inserts=0i,global_thread_active=41i,merge=0i,readonly_replica=0i,memory_tracking_in_background_schedule_pool=0i,memory_tracking_for_merges=0i,zoo_keeper_session=0i,context_lock_wait=0i,storage_buffer_bytes=0i,background_pool_task=0i,send_external_tables=0i,zoo_keeper_watch=0i,part_mutation=0i,disk_space_reserved_for_merge=0i,distributed_send=0i,version_integer=19014003i,local_thread=0i,replicated_checks=0i,memory_tracking=0i,memory_tracking_in_background_processing_pool=0i,leader_election=0i,revision=54425i,open_file_for_read=0i,open_file_for_write=0i,storage_buffer_rows=0i,rw_lock_waiting_readers=0i,rw_lock_waiting_writers=0i,rw_lock_active_writers=0i,local_thread_active=0i,query_preempted=0i,tcp_connection=1i,http_connection=1i,read=2i,query_thread=0i,dict_cache_requests=0i,rw_lock_active_readers=1i,global_thread=43i,query=1i 1569421000000000000\nclickhouse_tables,cluster=test_cluster_two_shards_localhost,database=system,host=kshvakov,source=localhost,shard_num=1,table=trace_log bytes=754i,parts=1i,rows=1i 1569421000000000000\nclickhouse_tables,cluster=test_cluster_two_shards_localhost,database=default,host=kshvakov,source=localhost,shard_num=1,table=example bytes=326i,parts=2i,rows=2i 1569421000000000000\n```\n\n----------------------------------------\n\nTITLE: Registering Aggregator Plugin in Telegraf\nDESCRIPTION: Registration code for a 'min' aggregator plugin that should be placed in plugins/aggregators/all/min.go. It includes build tags for conditional compilation and imports the plugin package.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/AGGREGATORS.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n//go:build !custom || aggregators || aggregators.min\n\npackage all\n\nimport _ \"github.com/influxdata/telegraf/plugins/aggregators/min\" // register plugin\n```\n\n----------------------------------------\n\nTITLE: Setting UDP/IP Buffer Limits (Shell)\nDESCRIPTION: This snippet provides shell commands to configure the UDP/IP buffer limits on Linux systems. Adjustments are made directly to the sysctl configuration for immediate effect or persistence across reboots.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socket_listener/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n\"\"\"\nsysctl -w net.core.rmem_max=8388608\nsysctl -w net.core.rmem_default=8388608\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Collecting CPU Profile in Telegraf\nDESCRIPTION: Use curl to collect a 30-second CPU profile from Telegraf. This command also saves the Telegraf version and Go environment information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl 'http://localhost:6060/debug/pprof/profile' > cpu.prof\ntelegraf --version > version.txt\ngo env GOOS GOARCH >> version.txt\n```\n\n----------------------------------------\n\nTITLE: TLS Configuration for Jolokia2 Agent\nDESCRIPTION: Example showing how to configure TLS options for secure communication with Jolokia agents.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jolokia2_agent/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.jolokia2_agent]]\n  urls = [\"https://agent:8080/jolokia\"]\n  tls_ca   = \"/var/private/ca.pem\"\n  tls_cert = \"/var/private/client.pem\"\n  tls_key  = \"/var/private/client-key.pem\"\n  #insecure_skip_verify = false\n\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"jvm_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Query Input Plugin in TOML\nDESCRIPTION: Main configuration block for the Elasticsearch Query plugin defining connection settings, authentication, TLS configuration, and proxy settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch_query/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.elasticsearch_query]]\n  urls = [ \"http://node1.es.example.com:9200\" ]\n  # timeout = \"5s\"\n  # enable_sniffer = false\n  # health_check_interval = \"10s\"\n  # username = \"telegraf\"\n  # password = \"mypassword\"\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n  # use_system_proxy = false\n  # http_proxy_url = \"http://localhost:8888\"\n```\n\n----------------------------------------\n\nTITLE: Using Systemd Secrets in HTTP Input Plugin\nDESCRIPTION: Example of referencing systemd-managed secrets in the Telegraf HTTP input plugin, using the systemd secret-store to provide secure username and password credentials.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.http]]\n  urls = [\"http://localhost/metrics\"]\n  username = \"@{systemd:http_user}\"\n  password = \"@{systemd:http_password}\"\n```\n\n----------------------------------------\n\nTITLE: Nginx Plus API Metrics Output\nDESCRIPTION: Sample metrics output showing various Nginx Plus statistics including processes, connections, SSL, HTTP requests, caches, and upstream server metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nnginx_plus_api_processes,port=80,source=demo.nginx.com respawned=0i 1570696321000000000\nnginx_plus_api_connections,port=80,source=demo.nginx.com accepted=68998606i,active=7i,dropped=0i,idle=57i 1570696322000000000\nnginx_plus_api_slabs_pages,port=80,source=demo.nginx.com,zone=hg.nginx.org used=1i,free=503i 1570696322000000000\nnginx_plus_api_slabs_pages,port=80,source=demo.nginx.com,zone=trac.nginx.org used=3i,free=500i 1570696322000000000\nnginx_plus_api_slabs_slots,port=80,source=demo.nginx.com,zone=hg.nginx.org,slot=8 used=1i,free=503i,reqs=10i,fails=0i 1570696322000000000\nnginx_plus_api_slabs_slots,port=80,source=demo.nginx.com,zone=hg.nginx.org,slot=16 used=3i,free=500i,reqs=1024i,fails=0i 1570696322000000000\nnginx_plus_api_slabs_slots,port=80,source=demo.nginx.com,zone=trac.nginx.org,slot=8 used=1i,free=503i,reqs=10i,fails=0i 1570696322000000000\nnginx_plus_api_slabs_slots,port=80,source=demo.nginx.com,zone=trac.nginx.org,slot=16 used=0i,free=1520i,reqs=0i,fails=1i 1570696322000000000\nnginx_plus_api_ssl,port=80,source=demo.nginx.com handshakes=9398978i,handshakes_failed=289353i,session_reuses=1004389i 1570696322000000000\nnginx_plus_api_http_requests,port=80,source=demo.nginx.com current=51i,total=264649353i 1570696322000000000\nnginx_plus_api_http_server_zones,port=80,source=demo.nginx.com,zone=hg.nginx.org discarded=5i,processing=0i,received=24123604i,requests=60138i,responses_1xx=0i,responses_2xx=59353i,responses_3xx=531i,responses_4xx=249i,responses_5xx=0i,responses_total=60133i,sent=830165221i 1570696322000000000\nnginx_plus_api_http_server_zones,port=80,source=demo.nginx.com,zone=trac.nginx.org discarded=250i,processing=0i,received=2184618i,requests=12404i,responses_1xx=0i,responses_2xx=8579i,responses_3xx=2513i,responses_4xx=583i,responses_5xx=479i,responses_total=12154i,sent=139384159i 1570696322000000000\nnginx_plus_api_http_server_zones,port=80,source=demo.nginx.com,zone=lxr.nginx.org discarded=1i,processing=0i,received=1011701i,requests=4523i,responses_1xx=0i,responses_2xx=4332i,responses_3xx=28i,responses_4xx=39i,responses_5xx=123i,responses_total=4522i,sent=72631354i 1570696322000000000\nnginx_plus_api_http_upstreams,port=80,source=demo.nginx.com,upstream=trac-backend keepalive=0i,zombies=0i 1570696322000000000\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Monit Input Plugin in Telegraf\nDESCRIPTION: Configuration settings for connecting to Monit's HTTP interface, including optional authentication, timeout, and TLS settings\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/monit/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics and status information about processes managed by Monit\n[[inputs.monit]]\n  ## Monit HTTPD address\n  address = \"http://127.0.0.1:2812\"\n\n  ## Username and Password for Monit\n  # username = \"\"\n  # password = \"\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Example Hugepages Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the Hugepages plugin, including measurements from root hugepages, per-node hugepages, and meminfo sources with various tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/hugepages/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhugepages_root,host=ubuntu,size_kb=1048576 free=0i,mempolicy=8i,overcommit=0i,reserved=0i,surplus=0i,total=8i 1646258020000000000\nhugepages_root,host=ubuntu,size_kb=2048 free=883i,mempolicy=2048i,overcommit=0i,reserved=0i,surplus=0i,total=2048i 1646258020000000000\nhugepages_per_node,host=ubuntu,size_kb=1048576,node=0 free=0i,surplus=0i,total=4i 1646258020000000000\nhugepages_per_node,host=ubuntu,size_kb=2048,node=0 free=434i,surplus=0i,total=1024i 1646258020000000000\nhugepages_per_node,host=ubuntu,size_kb=1048576,node=1 free=0i,surplus=0i,total=4i 1646258020000000000\nhugepages_per_node,host=ubuntu,size_kb=2048,node=1 free=449i,surplus=0i,total=1024i 1646258020000000000\nhugepages_meminfo,host=ubuntu anonymous_kb=0i,file_kb=0i,free=883i,reserved=0i,shared_kb=0i,size_kb=2048i,surplus=0i,tlb_kb=12582912i,total=2048i 1646258020000000000\n```\n\n----------------------------------------\n\nTITLE: Defining Logger in Go Plugin Struct\nDESCRIPTION: Shows how to define a Logger field in a Telegraf plugin struct. This Logger is pre-configured with the plugin name and alias.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/LOGGING.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype MyPlugin struct {\n    Log telegraf.Logger `toml:\"-\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LM Sensors Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the LM Sensors input plugin for Telegraf. It includes options to remove numbers from field names and set a timeout for the sensors command.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sensors/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Monitor sensors, requires lm-sensors package\n# This plugin ONLY supports Linux\n[[inputs.sensors]]\n  ## Remove numbers from field names.\n  ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n  # remove_numbers = true\n\n  ## Timeout is the maximum amount of time that the sensors command can run.\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: MongoDB Collection Statistics Structure\nDESCRIPTION: Defines metrics for individual collection statistics including size, object counts, and storage details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n- mongodb_col_stats\n  - tags:\n    - hostname\n    - collection\n    - db_name\n  - fields:\n    - size (integer)\n    - avg_obj_size (integer)\n    [...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Mcrouter Input Plugin (TOML)\nDESCRIPTION: This TOML configuration snippet sets up the Mcrouter input plugin to collect metrics from specified mcrouter servers. The servers parameter should include either IP addresses or Unix socket paths for connections. A minimum timeout for metric collection is established, ensuring reasonable response times.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mcrouter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n\"# Read metrics from one or many mcrouter servers.\\n[[inputs.mcrouter]]\\n  ## An array of address to gather stats about. Specify an ip or hostname\\n  ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\\n  servers = [\\\"tcp://localhost:11211\\\", \\\"unix:///var/run/mcrouter.sock\\\"]\\n\\n  ## Timeout for metric collections from all servers.  Minimum timeout is \\\"1s\\\".\\n  # timeout = \\\"5s\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Querying Disk IO Utilization with InfluxDB\nDESCRIPTION: SQL query to calculate the percentage of I/O utilization per disk and host over the last 30 minutes with a 60-second interval. Uses non_negative_derivative to handle counter values properly.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/diskio/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT non_negative_derivative(last(\"io_time\"),1ms) FROM \"diskio\" WHERE time > now() - 30m GROUP BY \"host\",\"name\",time(60s)\n```\n\n----------------------------------------\n\nTITLE: Querying Jenkins Job Duration in SQL\nDESCRIPTION: This SQL query calculates the average duration of Jenkins jobs over the last 24 hours, grouped by time intervals.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jenkins/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT mean(\"duration\") AS \"mean_duration\" FROM \"jenkins_job\" WHERE time > now() - 24h GROUP BY time(:interval:) FILL(null)\n```\n\n----------------------------------------\n\nTITLE: Registering Secret Store Plugin in Telegraf (Go)\nDESCRIPTION: This code snippet demonstrates how to register a secret store plugin in Telegraf. It uses build tags to allow selective inclusion or exclusion of the plugin when customizing Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/SECRETSTORES.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n//go:build !custom || secretstores || secretstores.printer\n\npackage all\n\nimport _ \"github.com/influxdata/telegraf/plugins/secretstores/printer\" // register plugin\n```\n\n----------------------------------------\n\nTITLE: Sample Telegraf Windows Event Log Plugin Output\nDESCRIPTION: Provides example output from the Telegraf Windows Event Log plugin. It shows how various Windows events are formatted, including system, security, and application-specific events with their respective fields and values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_eventlog/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nwin_eventlog,Channel=System,Computer=PC,EventID=105,Keywords=0x8000000000000000,Level=4,LevelText=Information,Opcode=10,OpcodeText=General,Source=WudfUsbccidDriver,Task=1,TaskText=Driver,host=PC ProcessName=\"WUDFHost.exe\",UserName=\"NT AUTHORITY\\\\LOCAL SERVICE\",Data_dwMaxCCIDMessageLength=\"271\",Data_bPINSupport=\"0x0\",Data_bMaxCCIDBusySlots=\"1\",EventRecordID=1914688i,UserID=\"S-1-5-19\",Version=0i,Data_bClassGetEnvelope=\"0x0\",Data_wLcdLayout=\"0x0\",Data_bClassGetResponse=\"0x0\",TimeCreated=\"2020-08-21T08:43:26.7481077Z\",Message=\"The Smartcard reader reported the following class descriptor (part 2).\" 1597999410000000000\nwin_eventlog,Channel=Security,Computer=PC,EventID=4798,Keywords=Audit\\ Success,Level=0,LevelText=Information,Opcode=0,OpcodeText=Info,Source=Microsoft-Windows-Security-Auditing,Task=13824,TaskText=User\\ Account\\ Management,host=PC Data_TargetDomainName=\"PC\",Data_SubjectUserName=\"User\",Data_CallerProcessId=\"0x3d5c\",Data_SubjectLogonId=\"0x46d14f8d\",Version=0i,EventRecordID=223157i,Message=\"A user's local group membership was enumerated.\",Data_TargetUserName=\"User\",Data_TargetSid=\"S-1-5-21-.-.-.-1001\",Data_SubjectUserSid=\"S-1-5-21-.-.-.-1001\",Data_CallerProcessName=\"C:\\\\Windows\\\\explorer.exe\",ActivityID=\"{0d4cc11d-7099-0002-4dc1-4c0d9970d601}\",UserID=\"\",Data_SubjectDomainName=\"PC\",TimeCreated=\"2020-08-21T08:43:27.3036771Z\",ProcessName=\"lsass.exe\" 1597999410000000000\nwin_eventlog,Channel=Microsoft-Windows-Dhcp-Client/Admin,Computer=PC,EventID=1002,Keywords=0x4000000000000001,Level=2,LevelText=Error,Opcode=76,OpcodeText=IpLeaseDenied,Source=Microsoft-Windows-Dhcp-Client,Task=3,TaskText=Address\\ Configuration\\ State\\ Event,host=PC Version=0i,Message=\"The IP address lease 10.20.30.40 for the Network Card with network address 0xaabbccddeeff has been denied by the DHCP server 10.20.30.1 (The DHCP Server sent a DHCPNACK message).\",UserID=\"S-1-5-19\",Data_HWLength=\"6\",Data_HWAddress=\"545595B7EA01\",TimeCreated=\"2020-08-21T08:43:42.8265853Z\",EventRecordID=34i,ProcessName=\"svchost.exe\",UserName=\"NT AUTHORITY\\\\LOCAL SERVICE\" 1597999430000000000\nwin_eventlog,Channel=System,Computer=PC,EventID=10016,Keywords=Classic,Level=3,LevelText=Warning,Opcode=0,OpcodeText=Info,Source=Microsoft-Windows-DistributedCOM,Task=0,host=PC Data_param3=\"\",Data_param6=\"PC\",Data_param8=\"S-1-5-21-2007059868-50816014-3139024325-1001\",Version=0i,UserName=\"PC\\\\User\",Data_param1=\"   \",Data_param2=\"\",Data_param7=\"User\",Data_param9=\"LocalHost (  LRPC)\",Data_param10=\"Microsoft.Windows.ShellExperienceHost_10.0.19041.423_neutral_neutral_cw5n1h2txyewy\",ActivityID=\"{839cac9e-73a1-4559-a847-62f3a5e73e44}\",ProcessName=\"svchost.exe\",Message=\"The     permission settings do not grant   permission for the COM Server application with CLSID \",Data_param5=\"{316CDED5-E4AE-4B15-9113-7055D84DCC97}\",Data_param11=\"S-1-15-2-.-.-.-.-.-.-2861478708\",TimeCreated=\"2020-08-21T08:43:45.5233759Z\",EventRecordID=1914689i,UserID=\"S-1-5-21-.-.-.-1001\",Data_param4=\"{C2F03A33-21F5-47FA-B4BB-156362A2F239}\" 1597999430000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Input Plugin in TOML\nDESCRIPTION: This TOML snippet configures the Memory Input Plugin in Telegraf to read metrics about memory usage. There are no specific fields to configure in this example, indicating that the default configuration is sufficient. Telegraf uses this setup to gather various memory metrics such as 'active', 'available', and 'cached', among others. These metrics are platform-dependent, and the configuration is a part of the Telegraf plugin's features that allow for extensive monitoring of system memory.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mem/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about memory usage\n[[inputs.mem]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Prometheus v1 Metrics Output Example\nDESCRIPTION: Example output showing metrics in prometheus-v1 format, including gauges, counters, and histograms.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opentelemetry/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncpu_temp,foo=bar gauge=87.332\nhttp_requests_total,method=post,code=200 counter=1027\nhttp_requests_total,method=post,code=400 counter=3\nhttp_request_duration_seconds 0.05=24054,0.1=33444,0.2=100392,0.5=129389,1=133988,sum=53423,count=144320\nrpc_duration_seconds 0.01=3102,0.05=3272,0.5=4773,0.9=9001,0.99=76656,sum=1.7560473e+07,count=2693\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Syslog Message Format Example\nDESCRIPTION: A sample RFC5424 formatted syslog message containing priority, timestamp, hostname, application name, process ID, message ID, structured data, and message content. This example shows a web service log entry for a successful HTTP GET request with various metadata included.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_unixtls/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Generating Filtered Telegraf Configuration\nDESCRIPTION: Command to generate a configuration file with only specific input and output plugins. This example filters for the CPU input plugin and InfluxDB output plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/COMMANDS_AND_FLAGS.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf config --input-filter cpu --output-filter influxdb\n```\n\n----------------------------------------\n\nTITLE: Configuring Riak Input Plugin in TOML\nDESCRIPTION: Configuration snippet for the Riak input plugin that specifies the Riak HTTP servers to monitor. The servers parameter accepts a list of HTTP endpoints.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/riak/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics one or many Riak servers\n[[inputs.riak]]\n  # Specify a list of one or more riak http servers\n  servers = [\"http://localhost:8098\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Agent Input Plugin in Telegraf\nDESCRIPTION: Sample TOML configuration for the Consul Agent input plugin. It includes options for specifying the Consul agent URL, authentication tokens, timeout settings, and TLS configuration for secure connections.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/consul_agent/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from the Consul Agent API\n[[inputs.consul_agent]]\n  ## URL for the Consul agent\n  # url = \"http://127.0.0.1:8500\"\n\n  ## Use auth token for authorization.\n  ## If both are set, an error is thrown.\n  ## If both are empty, no token will be used.\n  # token_file = \"/path/to/auth/token\"\n  ## OR\n  # token = \"a1234567-40c7-9048-7bae-378687048181\"\n\n  ## Set timeout (default 5 seconds)\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n```\n\n----------------------------------------\n\nTITLE: Complete Modbus to Binary Serialization Example in Telegraf\nDESCRIPTION: Example configuration showing a complete flow from Modbus data collection to binary serialization, including register definitions and output formatting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/binary/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Retrieve data from MODBUS slave devices\n[[inputs.modbus]]\n  name = \"device\"\n  slave_id = 1\n  timeout = \"1s\"\n\n  controller = \"tcp://127.0.0.1:5020\"\n\n  configuration_type = \"register\"\n\n  holding_registers = [\n    { name = \"addr_2\",     byte_order = \"AB\",   data_type=\"UINT16\",       scale=1.0, address = [2]   },\n    { name = \"addr_3\",     byte_order = \"AB\",   data_type=\"UINT16\",       scale=1.0, address = [3]   },\n    { name = \"addr_4_5\",   byte_order = \"ABCD\", data_type=\"UINT32\",       scale=1.0, address = [4,5] },\n    { name = \"addr_6_7\",   byte_order = \"ABCD\", data_type=\"FLOAT32-IEEE\", scale=1.0, address = [6,7] },\n    { name = \"addr_16_20\", byte_order = \"ABCD\", data_type=\"STRING\",                  address = [16,17,18,19,20] },\n    { name = \"addr_3_sc\",  byte_order = \"AB\",   data_type=\"UFIXED\",       scale=0.1, address = [3]   }\n  ]\n\n[[outputs.socket_writer]]\n  address = \"tcp://127.0.0.1:54000\"\n  metric_batch_size = 1\n\n  data_format = \"binary\"\n  endianness = \"little\"\n  entries = [\n    { read_from = \"field\", name = \"addr_3\",   data_format=\"int16\" },\n    { read_from = \"field\", name = \"addr_2\",   data_format=\"int16\" },\n    { read_from = \"field\", name = \"addr_4_5\", data_format=\"int32\" },\n    { read_from = \"field\", name = \"addr_6_7\",  data_format=\"float32\" },\n    { read_from = \"field\", name = \"addr_16_20\", data_format=\"string\", string_terminator = \"null\", string_length = 11 },\n    { read_from = \"field\", name = \"addr_3_sc\",  data_format=\"float64\" },\n    { read_from = \"time\", data_format=\"int32\", time_format=\"unix\" },\n    { read_from = \"name\", data_format=\"string\", string_terminator = \"null\", string_length = 20 }\n  ]\n```\n\n----------------------------------------\n\nTITLE: PSI Metrics Output Example\nDESCRIPTION: Example output when PSI (Pressure Stall Information) metrics collection is enabled, showing pressure metrics for CPU, memory, and IO resources.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kernel/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\npressure,resource=cpu,type=some avg10=1.53,avg60=1.87,avg300=1.73 1700000000000000000\npressure,resource=memory,type=some avg10=0.00,avg60=0.00,avg300=0.00 1700000000000000000\npressure,resource=memory,type=full avg10=0.00,avg60=0.00,avg300=0.00 1700000000000000000\npressure,resource=io,type=some avg10=0.0,avg60=0.0,avg300=0.0 1700000000000000000\npressure,resource=io,type=full avg10=0.0,avg60=0.0,avg300=0.0 1700000000000000000\npressure,resource=cpu,type=some total=1088168194i 1700000000000000000\npressure,resource=memory,type=some total=3463792i 1700000000000000000\npressure,resource=memory,type=full total=1429641i 1700000000000000000\npressure,resource=io,type=some total=68568296i 1700000000000000000\npressure,resource=io,type=full total=54982338i 1700000000000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Interrupts Input Plugin in TOML for Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Interrupts Input Plugin for Telegraf. It includes options for CPU tagging and IRQ filtering. The cpu_as_tag option determines how CPU metrics are stored, and tagdrop can be used to filter specific IRQs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/interrupts/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.interrupts]]\n  ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n  ## stored as a field.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  # cpu_as_tag = false\n\n  ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n  # [inputs.interrupts.tagdrop]\n  #   irq = [ \"NET_RX\", \"TASKLET\" ]\n```\n\n----------------------------------------\n\nTITLE: DPDK Sample Output Data\nDESCRIPTION: Complete example output showing ethdev info, stats, and xstats metrics with various performance counters and device information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dpdk/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ndpdk,command=/ethdev/info,dpdk_instance=l3fwd-power,host=dpdk-host,params=0 all_multicast=0,dev_configured=1,dev_flags=74,dev_started=1,ethdev_rss_hf=0,lro=0,mac_addr=\"E4:3D:1A:DD:13:31\",mtu=1500,name=\"0000:ca:00.1\",nb_rx_queues=1,nb_tx_queues=1,numa_node=1,port_id=0,promiscuous=1,rx_mbuf_alloc_fail=0,rx_mbuf_size_min=2176,rx_offloads=0,rxq_state_0=1,scattered_rx=0,state=1,tx_offloads=65536,txq_state_0=1 1659017414000000000\ndpdk,command=/ethdev/stats,dpdk_instance=l3fwd-power,host=dpdk-host,params=0 q_opackets_0=0,q_ipackets_5=0,q_errors_11=0,ierrors=0,q_obytes_5=0,q_obytes_10=0,q_opackets_10=0,q_ipackets_4=0,q_ipackets_7=0,q_ipackets_15=0,q_ibytes_5=0,q_ibytes_6=0,q_ibytes_9=0,obytes=0,q_opackets_1=0,q_opackets_11=0,q_obytes_7=0,q_errors_5=0,q_errors_10=0,q_ibytes_4=0,q_obytes_6=0,q_errors_1=0,q_opackets_5=0,q_errors_3=0,q_errors_12=0,q_ipackets_11=0,q_ipackets_12=0,q_obytes_14=0,q_opackets_15=0,q_obytes_2=0,q_errors_8=0,q_opackets_12=0,q_errors_0=0,q_errors_9=0,q_opackets_14=0,q_ibytes_3=0,q_ibytes_15=0,q_ipackets_13=0,q_ipackets_14=0,q_obytes_3=0,q_errors_13=0,q_opackets_3=0,q_ibytes_0=7092,q_ibytes_2=0,q_ibytes_8=0,q_ipackets_8=0,q_ipackets_10=0,q_obytes_4=0,q_ibytes_10=0,q_ibytes_13=0,q_ibytes_1=0,q_ibytes_12=0,opackets=0,q_obytes_1=0,q_errors_15=0,q_opackets_2=0,oerrors=0,rx_nombuf=0,q_opackets_8=0,q_ibytes_11=0,q_ipackets_3=0,q_obytes_0=0,q_obytes_12=0,q_obytes_11=0,q_obytes_13=0,q_errors_6=0,q_ipackets_1=0,q_ipackets_6=0,q_ipackets_9=0,q_obytes_15=0,q_opackets_7=0,q_ibytes_14=0,ipackets=98,q_ipackets_2=0,q_opackets_6=0,q_ibytes_7=0,imissed=0,q_opackets_4=0,q_opackets_9=0,q_obytes_8=0,q_obytes_9=0,q_errors_4=0,q_errors_14=0,q_opackets_13=0,ibytes=7092,q_ipackets_0=98,q_errors_2=0,q_errors_7=0 1606310780000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Synproxy Input Plugin\nDESCRIPTION: This TOML snippet configures the Synproxy input plugin to collect SYN proxy counter statistics from procfs. It is specifically designed for use on Linux systems, and no additional configuration parameters are required.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/synproxy/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.synproxy]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Configuring Ordered Processor Plugins in TOML\nDESCRIPTION: Example demonstrating how to configure multiple processors with specific execution order. Shows rename and strings processors with ordered processing of metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.rename]]\n  order = 1\n  [[processors.rename.replace]]\n    tag = \"path\"\n    dest = \"resource\"\n\n[[processors.strings]]\n  order = 2\n  [[processors.strings.trim_prefix]]\n    tag = \"resource\"\n    prefix = \"/api/\"\n```\n\n----------------------------------------\n\nTITLE: Querying vSphere Metrics in InfluxDB Line Protocol\nDESCRIPTION: Collection of vSphere performance metrics in InfluxDB line protocol format. Includes measurements for virtual machines, hosts, networking, CPU, memory, and disk metrics with associated tags and timestamp values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_16\n\nLANGUAGE: influxdb\nCODE:\n```\nvsphere_vm_net,clustername=DC0_C0,esxhostname=DC0_C0_H0,guest=other,host=host.example.com,moid=vm-44,os=Mac,source=DC0_C0_RP0_VM1,vcenter=localhost:8989,vmname=DC0_C0_RP0_VM1 bytesRx_average=357i,bytesTx_average=268i 1535660319000000000\nvsphere_vm_virtualDisk,clustername=DC0_C0,esxhostname=DC0_C0_H0,guest=other,host=host.example.com,moid=vm-44,os=Mac,source=DC0_C0_RP0_VM1,vcenter=localhost:8989,vmname=DC0_C0_RP0_VM1 write_average=528i,read_average=1i 1535660319000000000\n```\n\n----------------------------------------\n\nTITLE: Returning Multiple Metrics in Starlark\nDESCRIPTION: Example of creating and returning multiple metrics using deepcopy\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/starlark/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef apply(metric):\n    m2 = deepcopy(metric)\n    return [metric, m2]\n```\n\n----------------------------------------\n\nTITLE: Example Output for OpenLDAP Dialect\nDESCRIPTION: This snippet provides an example output for metrics collected from an OpenLDAP server using the Telegraf LDAP Input Plugin. The metrics include operations completed, operations initiated, and statistics entries. Parameters like server and port are captured as tags in the output. Such metrics help in understanding the performance and load on an OpenLDAP server.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ldap/README.md#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nopenldap,server=localhost,port=389 operations_completed=63i,operations_initiated=98i,operations_bind_initiated=10i,operations_unbind_initiated=6i,operations_modrdn_completed=0i,operations_delete_initiated=0i,operations_add_completed=2i,operations_delete_completed=0i,operations_abandon_completed=0i,statistics_entries=1516i,threads_open=2i,threads_active=1i,waiters_read=1i,operations_modify_completed=0i,operations_extended_initiated=4i,threads_pending=0i,operations_search_initiated=36i,operations_compare_initiated=0i,connections_max_file_descriptors=4096i,operations_modify_initiated=0i,operations_modrdn_initiated=0i,threads_max=16i,time_uptime=6017i,connections_total=1037i,connections_current=1i,operations_add_initiated=2i,statistics_bytes=162071i,operations_unbind_completed=6i,operations_abandon_initiated=0i,statistics_pdu=1566i,threads_max_pending=0i,threads_backload=1i,waiters_write=0i,operations_bind_completed=10i,operations_search_completed=35i,operations_compare_completed=0i,operations_extended_completed=4i,statistics_referrals=0i,threads_starting=0i 1516912070000000000\n```\n\n----------------------------------------\n\nTITLE: Example Ipset Save Output\nDESCRIPTION: This bash snippet demonstrates the output of the 'sudo ipset save' command, showing a sample IP set configuration with packet and byte counters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipset/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo ipset save\ncreate myset hash:net family inet hashsize 1024 maxelem 65536 counters comment\nadd myset 10.69.152.1 packets 8 bytes 672 comment \"machine A\"\n```\n\n----------------------------------------\n\nTITLE: Configuring P4 Runtime Input Plugin in TOML\nDESCRIPTION: TOML configuration for the P4 Runtime input plugin in Telegraf. It specifies connection details, device ID, counter filtering, and optional TLS settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/p4runtime/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# P4Runtime telemetry input plugin\n[[inputs.p4runtime]]\n  ## Define the endpoint of P4Runtime gRPC server to collect metrics.\n  # endpoint = \"127.0.0.1:9559\"\n  ## Set DeviceID required for Client Arbitration.\n  ## https://p4.org/p4-spec/p4runtime/main/P4Runtime-Spec.html#sec-client-arbitration-and-controller-replication\n  # device_id = 1\n  ## Filter counters by their names that should be observed.\n  ## Example: counter_names_include=[\"ingressCounter\", \"egressCounter\"]\n  # counter_names_include = []\n\n  ## Optional TLS Config.\n  ## Enable client-side TLS and define CA to authenticate the device.\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.crt\"\n  ## Set minimal TLS version to accept by the client.\n  # tls_min_version = \"TLS12\"\n  ## Use TLS but skip chain & host verification.\n  # insecure_skip_verify = true\n\n  ## Define client-side TLS certificate & key to authenticate to the device.\n  # tls_cert = \"/etc/telegraf/client.crt\"\n  # tls_key = \"/etc/telegraf/client.key\"\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch Process Metrics in Telegraf\nDESCRIPTION: Defines performance metrics for the Elasticsearch process, including CPU usage, memory usage, and descriptor counts to ensure efficient execution of Elasticsearch processes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n  - elasticsearch_process\n  - tags:\n    - cluster_name\n    - node_attribute_ml.enabled\n    - node_attribute_ml.machine_memory\n    - node_attribute_ml.max_open_jobs\n    - node_attribute_xpack.installed\n    - node_host\n    - node_id\n    - node_name\n  - fields:\n    - cpu_percent (float)\n    - cpu_total_in_millis (float)\n    - max_file_descriptors (float)\n    - mem_total_virtual_in_bytes (float)\n    - open_file_descriptors (float)\n    - timestamp (float)\n```\n\n----------------------------------------\n\nTITLE: Configuring WMI Query for SQL Server Metrics in Telegraf\nDESCRIPTION: This snippet sets up a WMI query to collect SQL Server version and SKU information. It uses the SqlServiceAdvancedProperty class and includes a filter to specifically target MSSQLSERVER instances. The query is useful for creating SQL Server inventory dashboards.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_10\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    namespace = \"Root\\\\Microsoft\\\\SqlServer\\\\ComputerManagement15\"\n    class_name = \"SqlServiceAdvancedProperty\"\n    properties = [\n      \"PropertyName\",\n      \"ServiceName\",\n      \"PropertyStrValue\",\n      \"SqlServiceType\"\n    ]\n    filter = \"ServiceName LIKE 'MSSQLSERVER' AND SqlServiceType = 1 AND (PropertyName LIKE 'FILEVERSION' OR PropertyName LIKE 'SKUNAME')\"\n    tag_properties = [\"PropertyName\",\"ServiceName\",\"PropertyStrValue\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring IPVS Input Plugin in Telegraf\nDESCRIPTION: TOML configuration for the IPVS input plugin in Telegraf. This plugin collects virtual and real server stats from Linux IPVS and requires no additional configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipvs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collect virtual and real server stats from Linux IPVS\n# This plugin ONLY supports Linux\n[[inputs.ipvs]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Configuring http_listener_v2 with form_urlencoded parser in Telegraf\nDESCRIPTION: This snippet shows how to configure the http_listener_v2 input plugin to use the form_urlencoded data format. It specifies the service address, data source, and tag keys to be collected.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/form_urlencoded/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.http_listener_v2]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Part of the request to consume.  Available options are \"body\" and\n  ## \"query\".\n  data_source = \"body\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"form_urlencoded\"\n\n  ## Array of key names which should be collected as tags.\n  ## By default, keys with string value are ignored if not marked as tags.\n  form_urlencoded_tag_keys = [\"tag1\"]\n```\n\n----------------------------------------\n\nTITLE: Example Output from Azure Storage Queue Plugin\nDESCRIPTION: Sample output from the Azure Storage Queue plugin showing metrics collected from two different queues. It displays size and message age information tagged with queue name and storage account.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/azure_storage_queue/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nazure_storage_queues,queue=myqueue,account=mystorageaccount oldest_message_age=799714900i,size=7i 1565970503000000000\nazure_storage_queues,queue=myemptyqueue,account=mystorageaccount size=0i 1565970502000000000\n```\n\n----------------------------------------\n\nTITLE: Showing Transformation of Metrics with Diff\nDESCRIPTION: This diff snippet illustrates the transformation of CPU metrics using the pivot processor. It shows how multiple single-valued metrics can be converted into a more compact multi-field metric representation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/pivot/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n\"- cpu,cpu=cpu0,name=time_idle value=42i\\n- cpu,cpu=cpu0,name=time_user value=43i\\n+ cpu,cpu=cpu0 time_idle=42i\\n+ cpu,cpu=cpu0 time_user=43i\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Neptune Apex Input Plugin in TOML\nDESCRIPTION: TOML configuration for the Neptune Apex input plugin in Telegraf. Specifies server URLs and response timeout for communicating with Apex controllers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/neptune_apex/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Neptune Apex data collector\n[[inputs.neptune_apex]]\n  ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n  ## Measurements will be logged under \"apex\".\n\n  ## The base URL of the local Apex(es). If you specify more than one server, they will\n  ## be differentiated by the \"source\" tag.\n  servers = [\n    \"http://apex.local\",\n  ]\n\n  ## The response_timeout specifies how long to wait for a reply from the Apex.\n  #response_timeout = \"5s\"\n\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Environment Configuration for NFS Client\nDESCRIPTION: Example of setting the MOUNT_PROC environment variable in a Docker compose file to specify an alternative location for the mountstats file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nfsclient/README.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nMOUNT_PROC: /host/proc/self/mountstats\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSMTPD Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the OpenSMTPD input plugin. It includes options for using sudo, specifying the binary location, and setting a timeout.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensmtpd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensmtpd]]\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the smtpctl binary can be overridden with:\n  binary = \"/usr/sbin/smtpctl\"\n\n  # The default timeout of 1s can be overridden with:\n  #timeout = \"1s\"\n```\n\n----------------------------------------\n\nTITLE: HTTP Output Configuration for Splunk HEC in Telegraf\nDESCRIPTION: This TOML configuration shows how to configure the Telegraf HTTP output to send metrics to a Splunk HEC endpoint. It includes setting the URL, data format to \"splunkmetric\", HEC routing options, and custom headers for authorization and request channel.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.http]]\n  ## URL is the address to send metrics to\n  url = \"https://localhost:8088/services/collector\"\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP method, one of: \"POST\" or \"PUT\"\n  # method = \"POST\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"splunkmetric\"\n\n  ## Provides time, index, source overrides for the HEC\n  splunkmetric_hec_routing = true\n  # splunkmetric_multimetric = true\n  # splunkmetric_omit_event_tag = false\n\n  ## Additional HTTP headers\n  [outputs.http.headers]\n    # Should be set manually to \"application/json\" for json data_format\n    Content-Type = \"application/json\"\n    Authorization = \"Splunk xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n    X-Splunk-Request-Channel = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n```\n\n----------------------------------------\n\nTITLE: Processing Metrics in Go Daemon\nDESCRIPTION: This Go program reads metrics from standard input, modifies the 'count' field by doubling it, and writes the modified metric to standard output using influx line protocol. It requires the `influxdata/telegraf` package.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/execd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"os\"\n\n    \"github.com/influxdata/telegraf/metric\"\n    \"github.com/influxdata/telegraf/plugins/parsers/influx\"\n    serializers_influx \"github.com/influxdata/telegraf/plugins/serializers/influx\"\n)\n\nfunc main() {\n    parser := influx.NewStreamParser(os.Stdin)\n    serializer := serializers_influx.Serializer{}\n    if err := serializer.Init(); err != nil {\n        fmt.Fprintf(os.Stderr, \"serializer init failed: %v\\n\", err)\n        os.Exit(1)\n    }\n\n    for {\n        metric, err := parser.Next()\n        if err != nil {\n            if err == influx.EOF {\n                return // stream ended\n            }\n            if parseErr, isParseError := err.(*influx.ParseError); isParseError {\n                fmt.Fprintf(os.Stderr, \"parse ERR %v\\n\", parseErr)\n                os.Exit(1)\n            }\n            fmt.Fprintf(os.Stderr, \"ERR %v\\n\", err)\n            os.Exit(1)\n        }\n\n        c, found := metric.GetField(\"count\")\n        if !found {\n            fmt.Fprintf(os.Stderr, \"metric has no count field\\n\")\n            os.Exit(1)\n        }\n        switch t := c.(type) {\n        case float64:\n            t *= 2\n            metric.AddField(\"count\", t)\n        case int64:\n            t *= 2\n            metric.AddField(\"count\", t)\n        default:\n            fmt.Fprintf(os.Stderr, \"count is not an unknown type, it's a %T\\n\", c)\n            os.Exit(1)\n        }\n        b, err := serializer.Serialize(metric)\n        if err != nil {\n            fmt.Fprintf(os.Stderr, \"ERR %v\\n\", err)\n            os.Exit(1)\n        }\n        fmt.Fprint(os.Stdout, string(b))\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Systemd Units Output - Detailed Mode\nDESCRIPTION: Expanded output format that includes additional metrics like memory usage, PID, restart count, and status error codes. Also adds preset and state information for services where available.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/systemd_units/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsystemd_units,active=active,host=host1.example.com,load=loaded,name=dbus.service,sub=running,preset=disabled,state=static,user=telegraf active_code=0i,load_code=0i,mem_avail=6470856704i,mem_current=2691072i,mem_peak=3895296i,pid=481i,restarts=0i,status_errno=0i,sub_code=0i,swap_current=794624i,swap_peak=884736i 1533730725000000000\nsystemd_units,active=inactive,host=host1.example.com,load=not-found,name=networking.service,sub=dead,user=telegraf active_code=2i,load_code=2i,pid=0i,restarts=0i,status_errno=0i,sub_code=1i 1533730725000000000\nsystemd_units,active=active,host=host1.example.com,load=loaded,name=pcscd.service,sub=running,preset=disabled,state=indirect,user=telegraf active_code=0i,load_code=0i,mem_avail=6370541568i,mem_current=512000i,mem_peak=4399104i,pid=1673i,restarts=0i,status_errno=0i,sub_code=0i,swap_current=3149824i,swap_peak=3149824i 1533730725000000000\n```\n\n----------------------------------------\n\nTITLE: Displaying ZFS Pool Metrics in Telegraf\nDESCRIPTION: This snippet provides an example output of ZFS metrics as produced by Telegraf. It includes details about pool health, allocation, capacity, ARC stats, and other ZFS-specific data points. These metrics are crucial for understanding the health and performance of ZFS storage systems.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zfs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nzfs_pool,health=ONLINE,pool=zroot allocated=1578590208i,capacity=2i,dedupratio=1,fragmentation=1i,free=64456531968i,size=66035122176i 1464473103625653908\nzfs_dataset,dataset=zata avail=10741741326336,used=8564135526400,usedsnap=0,usedds=90112\nzfs,pools=zroot arcstats_allocated=4167764i,arcstats_anon_evictable_data=0i,arcstats_anon_evictable_metadata=0i,arcstats_anon_size=16896i,arcstats_arc_meta_limit=10485760i,arcstats_arc_meta_max=115269568i,arcstats_arc_meta_min=8388608i,arcstats_arc_meta_used=51977456i,arcstats_c=16777216i,arcstats_c_max=41943040i,arcstats_c_min=16777216i,arcstats_data_size=0i,arcstats_deleted=1699340i,arcstats_demand_data_hits=14836131i,arcstats_demand_data_misses=2842945i,arcstats_demand_hit_predictive_prefetch=0i,arcstats_demand_metadata_hits=1655006i,arcstats_demand_metadata_misses=830074i,arcstats_duplicate_buffers=0i,arcstats_duplicate_buffers_size=0i,arcstats_duplicate_reads=123i,arcstats_evict_l2_cached=0i,arcstats_evict_l2_eligible=332172623872i,arcstats_evict_l2_ineligible=6168576i,arcstats_evict_l2_skip=0i,arcstats_evict_not_enough=12189444i,arcstats_evict_skip=195190764i,arcstats_hash_chain_max=2i,arcstats_hash_chains=10i,arcstats_hash_collisions=43134i,arcstats_hash_elements=2268i,arcstats_hash_elements_max=6136i,arcstats_hdr_size=565632i,arcstats_hits=16515778i,arcstats_l2_abort_lowmem=0i,arcstats_l2_asize=0i,arcstats_l2_cdata_free_on_write=0i,arcstats_l2_cksum_bad=0i,arcstats_l2_compress_failures=0i,arcstats_l2_compress_successes=0i,arcstats_l2_compress_zeros=0i,arcstats_l2_evict_l1cached=0i,arcstats_l2_evict_lock_retry=0i,arcstats_l2_evict_reading=0i,arcstats_l2_feeds=0i,arcstats_l2_free_on_write=0i,arcstats_l2_hdr_size=0i,arcstats_l2_hits=0i,arcstats_l2_io_error=0i,arcstats_l2_misses=0i,arcstats_l2_read_bytes=0i,arcstats_l2_rw_clash=0i,arcstats_l2_size=0i,arcstats_l2_write_buffer_bytes_scanned=0i,arcstats_l2_write_buffer_iter=0i,arcstats_l2_write_buffer_list_iter=0i,arcstats_l2_write_buffer_list_null_iter=0i,arcstats_l2_write_bytes=0i,arcstats_l2_write_full=0i,arcstats_l2_write_in_l2=0i,arcstats_l2_write_io_in_progress=0i,arcstats_l2_write_not_cacheable=380i,arcstats_l2_write_passed_headroom=0i,arcstats_l2_write_pios=0i,arcstats_l2_write_spa_mismatch=0i,arcstats_l2_write_trylock_fail=0i,arcstats_l2_writes_done=0i,arcstats_l2_writes_error=0i,arcstats_l2_writes_lock_retry=0i,arcstats_l2_writes_sent=0i,arcstats_memory_throttle_count=0i,arcstats_metadata_size=17014784i,arcstats_mfu_evictable_data=0i,arcstats_mfu_evictable_metadata=16384i,arcstats_mfu_ghost_evictable_data=5723648i,arcstats_mfu_ghost_evictable_metadata=10709504i,arcstats_mfu_ghost_hits=1315619i,arcstats_mfu_ghost_size=16433152i,arcstats_mfu_hits=7646611i,arcstats_mfu_size=305152i,arcstats_misses=3676993i,arcstats_mru_evictable_data=0i,arcstats_mru_evictable_metadata=0i,arcstats_mru_ghost_evictable_data=0i,arcstats_mru_ghost_evictable_metadata=80896i,arcstats_mru_ghost_hits=324250i,arcstats_mru_ghost_size=80896i,arcstats_mru_hits=8844526i,arcstats_mru_size=16693248i,arcstats_mutex_miss=354023i,arcstats_other_size=34397040i,arcstats_p=4172800i,arcstats_prefetch_data_hits=0i,arcstats_prefetch_data_misses=0i,arcstats_prefetch_metadata_hits=24641i,arcstats_prefetch_metadata_misses=3974i,arcstats_size=51977456i,arcstats_sync_wait_for_async=0i,vdev_cache_stats_delegations=779i,vdev_cache_stats_hits=323123i,vdev_cache_stats_misses=59929i,zfetchstats_hits=0i,zfetchstats_max_streams=0i,zfetchstats_misses=0i 1464473103634124908\n```\n\n----------------------------------------\n\nTITLE: File Output Configuration for Splunk Metrics in Telegraf\nDESCRIPTION: This TOML configuration illustrates how to configure the Telegraf file output to write metrics to a file in the Splunk metric format.  It disables HEC routing and enables multimetric to be ingested into Splunk.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n # Send telegraf metrics to file(s)\n[[outputs.file]]\n   ## Files to write to, \"stdout\" is a specially handled file.\n   files = [\"/tmp/metrics.out\"]\n\n   ## Data format to output.\n   ## Each data format has its own unique set of configuration options, read\n   ## more about them here:\n   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n   data_format = \"splunkmetric\"\n   splunkmetric_hec_routing = false\n   splunkmetric_multimetric = true\n   splunkmetric_omit_event_tag = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Statsd Templates in TOML\nDESCRIPTION: Example configuration using templates to transform statsd buckets into InfluxDB measurements and tags. This basic template uses measurement.measurement.region format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\ntemplates = [\n    \"measurement.measurement.region\"\n]\n```\n\n----------------------------------------\n\nTITLE: Process State Mappings Across Operating Systems\nDESCRIPTION: Mapping of process state codes between Linux, FreeBSD, and Darwin operating systems, showing how different state codes correspond to Telegraf metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/processes/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nLinux  FreeBSD  Darwin  meaning\n  R       R       R     running\n  S       S       S     sleeping\n  Z       Z       Z     zombie\n  X      none    none   dead\n  T       T       T     stopped\n  I       I       I     idle (sleeping for longer than about 20 seconds)\n  D      D,L      U     blocked (waiting in uninterruptible sleep, or locked)\n  W       W      none   paging (linux kernel < 2.6 only), wait (freebsd)\n```\n\n----------------------------------------\n\nTITLE: Socket Statistics Header Format\nDESCRIPTION: A text header row showing the column format for displaying socket statistics data. Includes receive queue (Recv-Q), send queue (Send-Q), local address:port, and peer address:port columns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socketstat/testdata/udp_no_sockets.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nRecv-Q Send-Q       Local Address:Port                     Peer Address:Port\n```\n\n----------------------------------------\n\nTITLE: Sample Output for Windows Operating System WMI Query in Telegraf\nDESCRIPTION: This snippet shows an example of the output generated by the Windows operating system WMI query configuration. It includes metrics like free paging file space and virtual memory, along with tags for OS name and architecture.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nwin_wmi_Win32_OperatingSystem,Caption=Microsoft\\ Windows\\ 10\\ Enterprise,InstallationType=Client,Name=Microsoft\\ Windows\\ 10\\ Enterprise|C:\\WINDOWS|\\Device\\Harddisk0\\Partition3,OSArchitecture=64-bit,host=foo FreeSpaceInPagingFiles=5203244i,FreeVirtualMemory=16194496i,OperatingSystemSKU=4i,ProductType=1i 1654269272000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Connection Statistics Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the netstat input plugin for Telegraf to read TCP metrics such as established connections, time wait states, and socket counts. No additional configuration is required for this plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read TCP metrics such as established, time wait and sockets counts.\n[[inputs.netstat]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Example Output of PHP-FPM Metrics with JSON Format\nDESCRIPTION: Sample output of PHP-FPM metrics collected by Telegraf when using JSON format. It includes additional process-specific metrics and more detailed information about each PHP-FPM process.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/phpfpm/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nphpfpm,pool=www,url=http://127.0.0.1:44637?full&json accepted_conn=3879i,active_processes=1i,idle_processes=9i,listen_queue=0i,listen_queue_len=0i,max_active_processes=3i,max_children_reached=0i,max_listen_queue=0i,slow_requests=0i,start_since=4901i,total_processes=10i\nphpfpm_process,pool=www,request_method=GET,request_uri=/fpm-status?json&full,script=-,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=583i,last_request_cpu=0,last_request_memory=0,request_duration=159i,requests=386i,start_time=1702044927i,state=\"Running\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/fpm-status,script=-,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=584i,last_request_cpu=0,last_request_memory=2097152,request_duration=174i,requests=390i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/index.php,script=script.php,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=585i,last_request_cpu=104.93,last_request_memory=2097152,request_duration=9530i,requests=389i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/ping,script=-,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=586i,last_request_cpu=0,last_request_memory=2097152,request_duration=127i,requests=399i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/index.php,script=script.php,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=587i,last_request_cpu=0,last_request_memory=2097152,request_duration=9713i,requests=382i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/ping,script=-,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=588i,last_request_cpu=0,last_request_memory=2097152,request_duration=133i,requests=383i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/fpm-status?json,script=-,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=589i,last_request_cpu=0,last_request_memory=2097152,request_duration=154i,requests=381i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/ping,script=-,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=590i,last_request_cpu=0,last_request_memory=2097152,request_duration=108i,requests=397i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/index.php,script=script.php,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=591i,last_request_cpu=110.28,last_request_memory=2097152,request_duration=9068i,requests=381i,start_time=1702044927i,state=\"Idle\"\nphpfpm_process,pool=www,request_method=GET,request_uri=/index.php,script=script.php,url=http://127.0.0.1:44637?full&json,user=- content_length=0i,pid=592i,last_request_cpu=64.27,last_request_memory=2097152,request_duration=15559i,requests=391i,start_time=1702044927i,state=\"Idle\"\n```\n\n----------------------------------------\n\nTITLE: Blacklist OK status\nDESCRIPTION: This TOML configuration demonstrates how to blacklist metrics with the tag `status` set to `OK`. By default all metrics will be passed. Only the `machine` metrics are processed by the filter, and any machine metrics that have the `status` tag set to `OK` are dropped.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filter/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.filter]]\\n  namepass = [\\\"machine\\\"]\\n\\n  [[processors.filter.rule]]\\n    tags = {\\\"status\\\" = [\\\"OK\\\"]}\\n\"\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Indices Total Shards Statistics Schema\nDESCRIPTION: Defines the metric fields for total shard statistics across all indices, including counts of failed, successful, and total shards.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n- elasticsearch_indices_stats_shards_total\n  - fields:\n    - failed (float)\n    - successful (float)\n    - total (float)\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Line Protocol in Telegraf\nDESCRIPTION: Configuration for parsing JSON Line Protocol data in Telegraf using XPath expressions, specifying metric name, field and tag selections, and timestamp format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PARSING_DATA.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"fields\": {\"temp\": 32.3, \"humidity\": 23, \"alarm\": false},\n  \"name\": \"measurement\",\n  \"tags\": {\"node\": \"node1\"},\n  \"time\": \"2024-03-04T10:10:32.123456Z\"\n}\n```\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\nfiles = [\"test.json\"]\nprecision = \"1us\"\ndata_format = \"xpath_json\"\n\n[[inputs.file.xpath]]\n  metric_name = \"/name\"\n  field_selection = \"fields/*\"\n  tag_selection = \"tags/*\"\n  timestamp = \"/time\"\n  timestamp_format = \"2006-01-02T15:04:05.999999999Z\"\n```\n\n----------------------------------------\n\nTITLE: Example Output for 389ds Dialect\nDESCRIPTION: This snippet provides an example output for metrics collected from a 389ds server using the Telegraf LDAP Input Plugin. It captures metrics such as operations completed and initiated, bytes sent and received, and connection details. The output aids in monitoring server load and connection statistics, facilitating effective management of 389ds servers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ldap/README.md#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n389ds,port=32805,server=localhost add_operations=0i,anonymous_binds=0i,backends=0i,bind_security_errors=0i,bytes_received=0i,bytes_sent=256i,cache_entries=0i,cache_hits=0i,chainings=0i,compare_operations=0i,connections=1i,connections_in_max_threads=0i,connections_max_threads=0i,copy_entries=0i,current_connections=1i,current_connections_at_max_threads=0i,delete_operations=0i,dtablesize=63936i,entries_returned=2i,entries_sent=2i,errors=2i,in_operations=11i,list_operations=0i,maxthreads_per_conn_hits=0i,modify_operations=1i,modrdn_operations=0i,onelevel_search_operations=0i,operations_completed=10i,operations_initiated=11i,read_operations=0i,read_waiters=0i,referrals=0i,referrals_returned=0i,search_operations=3i,security_errors=0i,simpleauth_binds=1i,strongauth_binds=2i,threads=17i,total_connections=4i,unauth_binds=0i,wholesubtree_search_operations=1i 1695637234047087280\n```\n\n----------------------------------------\n\nTITLE: MongoDB Database Statistics Structure\nDESCRIPTION: Specifies metrics related to database-level statistics including collection counts, storage sizes, and filesystem usage.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n- mongodb_db_stats\n  - tags:\n    - db_name\n    - hostname\n  - fields:\n    - avg_obj_size (float)\n    - collections (integer)\n    [...]\n```\n\n----------------------------------------\n\nTITLE: Default Kernel Metrics Output\nDESCRIPTION: Example output showing basic kernel metrics including boot time, context switches, entropy, interrupts, and forked processes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kernel/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nkernel boot_time=1690487872i,context_switches=321398652i,entropy_avail=256i,interrupts=141868628i,processes_forked=946492i 1691339564000000000\n```\n\n----------------------------------------\n\nTITLE: Example Noise Processor Implementation in Telegraf\nDESCRIPTION: Practical example of using the Noise processor with the CPU input plugin. It applies Laplacian noise to CPU metrics while excluding specific fields and metrics using filtering options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/noise/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n\n[[processors.noise]]\n  scale = 1.0\n  mu = 0.0\n  noise_type = \"laplacian\"\n  include_fields = []\n  exclude_fields = [\"usage_steal\", \"usage_user\", \"uptime_format\", \"usage_idle\" ]\n  namedrop = [\"swap\", \"disk\", \"net\"]\n```\n\n----------------------------------------\n\nTITLE: Example Dovecot Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the Dovecot input plugin including system statistics, mail operations, and resource usage measurements. The output is formatted with tags for server identification and metric categorization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dovecot/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndovecot,server=dovecot-1.domain.test,type=global clock_time=101196971074203.94,disk_input=6493168218112i,disk_output=17978638815232i,invol_cs=1198855447i,last_update=\"2016-04-08 11:04:13.000379245 +0200 CEST\",mail_cache_hits=68192209i,mail_lookup_attr=0i,mail_lookup_path=653861i,mail_read_bytes=86705151847i,mail_read_count=566125i,maj_faults=17208i,min_faults=1286179702i,num_cmds=917469i,num_connected_sessions=8896i,num_logins=174827i,read_bytes=30327690466186i,read_count=1772396430i,reset_timestamp=\"2016-04-08 10:28:45 +0200 CEST\",sys_cpu=157965.692,user_cpu=219337.48,vol_cs=2827615787i,write_bytes=17150837661940i,write_count=992653220i 1460106266642153907\n```\n\n----------------------------------------\n\nTITLE: Example Proxmox Metrics Output\nDESCRIPTION: Sample output showing the format and structure of metrics collected by the Proxmox plugin, including various system measurements and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/proxmox/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nproxmox,host=pxnode,node_fqdn=pxnode.example.com,vm_fqdn=vm1.example.com,vm_id=112,vm_name=vm1,vm_type=lxc cpuload=0.147998116735236,disk_free=4461129728i,disk_total=5217320960i,disk_used=756191232i,disk_used_percentage=14,mem_free=1046827008i,mem_total=1073741824i,mem_used=26914816i,mem_used_percentage=2,status=\"running\",swap_free=536698880i,swap_total=536870912i,swap_used=172032i,swap_used_percentage=0,uptime=1643793i 1595457277000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring SmartCTL Input Plugin in TOML\nDESCRIPTION: Configuration settings for the SmartCTL plugin including path specification, sudo usage, device inclusion/exclusion, power mode checks, and timeout settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smartctl/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.smartctl]]\n    ## Optionally specify the path to the smartctl executable\n    # path = \"/usr/sbin/smartctl\"\n\n    ## Use sudo\n    ## On most platforms used, smartctl requires root access. Setting 'use_sudo'\n    ## to true will make use of sudo to run smartctl. Sudo must be configured to\n    ## allow the telegraf user to run smartctl without a password.\n    # use_sudo = false\n\n    ## Devices to include or exclude\n    ## By default, the plugin will use all devices found in the output of\n    ## `smartctl --scan-open`. Only one option is allowed at a time. If set, include\n    ## sets the specific devices to scan, while exclude omits specific devices.\n    # devices_include = []\n    # devices_exclude = []\n\n    ## Skip checking disks in specified power mode\n    ## Defaults to \"standby\" to not wake up disks that have stopped rotating.\n    ## For full details on the options here, see the --nocheck section in the\n    ## smartctl man page. Choose from:\n    ##   * never: always check the device\n    ##   * sleep: check the device unless it is in sleep mode\n    ##   * standby: check the device unless it is in sleep or standby mode\n    ##   * idle: check the device unless it is in sleep, standby, or idle mode\n    # nocheck = \"standby\"\n\n    ## Timeout for the cli command to complete\n    # timeout = \"30s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Port Name Lookup Processor in Telegraf\nDESCRIPTION: Sample configuration for the port_name processor plugin. This configuration shows all available options including source tag/field specification, destination naming, protocol handling, and default protocol settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/port_name/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Given a tag/field of a TCP or UDP port number, add a tag/field of the service name looked up in the system services file\n[[processors.port_name]]\n  ## Name of tag holding the port number\n  # tag = \"port\"\n  ## Or name of the field holding the port number\n  # field = \"port\"\n\n  ## Name of output tag or field (depending on the source) where service name will be added\n  # dest = \"service\"\n\n  ## Default tcp or udp\n  # default_protocol = \"tcp\"\n\n  ## Tag containing the protocol (tcp or udp, case-insensitive)\n  # protocol_tag = \"proto\"\n\n  ## Field containing the protocol (tcp or udp, case-insensitive)\n  # protocol_field = \"proto\"\n```\n\n----------------------------------------\n\nTITLE: Querying Azure SQL Managed Instance Metrics\nDESCRIPTION: Metrics collection specific to Azure SQL Managed Instance including resource governance, performance counters, and system health statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nsys.dm_io_virtual_file_stats\\nsys.dm_os_memory_clerks\\nsys.dm_instance_resource_governance\\nsys.dm_os_performance_counters\\nsys.dm_os_wait_stats\\nsys.dm_exec_sessions\\nsys.dm_exec_requests\\nsys.dm_os_schedulers\n```\n\n----------------------------------------\n\nTITLE: CSV Input Example for Telegraf Parser\nDESCRIPTION: This snippet shows an example CSV input that can be parsed by Telegraf using the CSV parser plugin. It includes a header row and a data row with various fields including a timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/csv/README.md#2025-04-16_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\nmeasurement,cpu,time_user,time_system,time_idle,time\ncpu,cpu0,42,42,42,2018-09-13T13:03:28Z\n```\n\n----------------------------------------\n\nTITLE: Example LeoStorage Metrics Output Format\nDESCRIPTION: Sample output of collected LeoStorage metrics demonstrating system performance, object statistics, and message queue metrics with timestamp and node tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/leofs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nleofs,host=storage_0,node=storage_0@127.0.0.1 allocated_memory=63504384,allocated_memory_5min=0,comp_last_end_datetime=0,comp_last_start_datetime=0,comp_num_of_ongoing_targets=0,comp_num_of_out_of_targets=0,comp_num_of_pending_targets=8,comp_state=0,ets_memory_usage=3877824,ets_memory_usage_5min=0,mq_mdcr_num_of_msg_req_comp_metadata=0,mq_mdcr_num_of_msg_req_sync_obj=0,mq_num_of_msg_async_deletion_dir=0,mq_num_of_msg_deletion_dir=0,mq_num_of_msg_recovery_node=0,mq_num_of_msg_req_deletion_dir=0,num_of_active_objects=70,num_of_deletes=0,num_of_deletes_5min=0,num_of_processes=577,num_of_processes_5min=0,num_of_reads=1,num_of_reads_5min=0,num_of_rebalance_messages=0,num_of_replication_messages=0,num_of_sync-vnode_messages=0,num_of_writes=70,num_of_writes_5min=0,processes_memory_usage=20029464,processes_memory_usage_5min=0,system_memory_usage=25900472,system_memory_usage_5min=0,total_memory_usage=45920987,total_memory_usage_5min=0,total_objects=70,total_size=2,total_size_of_active_objects=2,used_allocated_memory=69,used_allocated_memory_5min=0 1524529826000000000\n```\n\n----------------------------------------\n\nTITLE: Example Output of Port Name Lookup Processor\nDESCRIPTION: A diff showing how the port_name processor transforms metrics. In this example, a metric with port number 80 is enhanced with the service name 'http' as a new tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/port_name/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- measurement,port=80 field=123 1560540094000000000\n+ measurement,port=80,service=http field=123 1560540094000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Dedup Processor in Telegraf with TOML\nDESCRIPTION: Sample configuration for the Dedup Processor Plugin that filters metrics with repeating field values. The 'dedup_interval' parameter sets the maximum time to suppress duplicate output, which defaults to 600 seconds (10 minutes).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/dedup/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Filter metrics with repeating field values\n[[processors.dedup]]\n  ## Maximum time to suppress output\n  dedup_interval = \"600s\"\n```\n\n----------------------------------------\n\nTITLE: Querying NGINX Plus API Resolver Zone Metrics in InfluxDB Format\nDESCRIPTION: This snippet displays metrics for an NGINX Plus API resolver zone. It includes various DNS resolution statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_5\n\nLANGUAGE: influxdb\nCODE:\n```\nnginx_plus_api_resolver_zones,port=80,source=demo.nginx.com,zone=resolver1 addr=0i,formerr=0i,name=0i,noerror=0i,notimp=0i,nxdomain=0i,refused=0i,servfail=0i,srv=0i,timedout=0i,unknown=0i 1570696324000000000\n```\n\n----------------------------------------\n\nTITLE: Example Output of Telegraf Internal Input Plugin\nDESCRIPTION: This snippet demonstrates the output format of the Telegraf Internal Input Plugin. It includes various metrics such as memory stats, agent stats, write stats, and plugin-specific metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/internal/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ninternal_memstats,host=tyrion alloc_bytes=4457408i,sys_bytes=10590456i,pointer_lookups=7i,mallocs=17642i,frees=7473i,heap_sys_bytes=6848512i,heap_idle_bytes=1368064i,heap_in_use_bytes=5480448i,heap_released_bytes=0i,total_alloc_bytes=6875560i,heap_alloc_bytes=4457408i,heap_objects_bytes=10169i,num_gc=2i 1480682800000000000\ninternal_agent,host=tyrion,go_version=1.12.7,version=1.99.0 metrics_written=18i,metrics_dropped=0i,metrics_gathered=19i,gather_errors=0i,gather_timeouts=0i 1480682800000000000\ninternal_write,output=file,host=tyrion,version=1.99.0 buffer_limit=10000i,write_time_ns=636609i,metrics_added=18i,metrics_written=18i,buffer_size=0i 1480682800000000000\ninternal_gather,input=internal,host=tyrion,version=1.99.0 metrics_gathered=19i,gather_time_ns=442114i,gather_timeouts=0i 1480682800000000000\ninternal_gather,input=http_listener,host=tyrion,version=1.99.0 metrics_gathered=0i,gather_time_ns=167285i,gather_timeouts=0i 1480682800000000000\ninternal_http_listener,address=:8186,host=tyrion,version=1.99.0 queries_received=0i,writes_received=0i,requests_received=0i,buffers_created=0i,requests_served=0i,pings_received=0i,bytes_received=0i,not_founds_served=0i,pings_served=0i,queries_served=0i,writes_served=0i 1480682800000000000\ninternal_mqtt_consumer,host=tyrion,version=1.99.0 messages_received=622i,payload_size=37942i 1657282270000000000\n```\n\n----------------------------------------\n\nTITLE: TLS Configuration for Jolokia2 Proxy\nDESCRIPTION: Example configuration showing TLS options for secure communication with Jolokia proxies, including certificate paths and target configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jolokia2_proxy/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.jolokia2_proxy]]\n  url = \"https://proxy:8080/jolokia\"\n\n  tls_ca   = \"/var/private/ca.pem\"\n  tls_cert = \"/var/private/client.pem\"\n  tls_key  = \"/var/private/client-key.pem\"\n  #insecure_skip_verify = false\n\n  #default_target_username = \"\"\n  #default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  [[inputs.jolokia2_proxy.metric]]\n    name  = \"jvm_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Trig Input\nDESCRIPTION: This configuration snippet demonstrates how to configure the 'trig' input plugin in Telegraf's configuration file. It sets the amplitude of the sine and cosine waves generated by the plugin. The amplitude parameter controls the maximum value of the waves.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/trig/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Inserts sine and cosine waves for demonstration purposes\n[[inputs.trig]]\n  ## Set the amplitude\n  amplitude = 10.0\"\n```\n\n----------------------------------------\n\nTITLE: OpenStack Telegraf Metrics Output Format\nDESCRIPTION: Demonstrates the line protocol format for various OpenStack metrics collected by Telegraf, including neutron agents, compute resources, network configurations, storage pools, and server diagnostics. Each line represents a different metric with associated tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openstack/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nopenstack_neutron_agent,agent_host=vim2,agent_type=DHCP\\ agent,availability_zone=nova,binary=neutron-dhcp-agent,host=telegraf_host,topic=dhcp_agent admin_state_up=true,alive=true,created_at=\"2021-01-07T03:40:53Z\",heartbeat_timestamp=\"2021-10-14T07:46:40Z\",id=\"17e1e446-d7da-4656-9e32-67d3690a306f\",resources_synced=false,started_at=\"2021-07-02T21:47:42Z\" 1634197616000000000\nopenstack_aggregate,host=telegraf_host,name=non-dpdk aggregate_host=\"vim3\",aggregate_hosts=2i,created_at=\"2021-02-01T18:28:00Z\",deleted=false,deleted_at=\"0001-01-01T00:00:00Z\",id=3i,updated_at=\"0001-01-01T00:00:00Z\" 1634197617000000000\nopenstack_flavor,host=telegraf_host,is_public=true,name=hwflavor disk=20i,ephemeral=0i,id=\"f89785c0-6b9f-47f5-a02e-f0fcbb223163\",ram=8192i,rxtx_factor=1,swap=0i,vcpus=8i 1634197617000000000\nopenstack_hypervisor,cpu_arch=x86_64,cpu_feature_3dnowprefetch=true,cpu_feature_abm=true,cpu_feature_acpi=true,cpu_feature_adx=true,cpu_feature_aes=true,cpu_feature_apic=true,cpu_feature_xtpr=true,cpu_model=C-Server,cpu_vendor=xyz,host=telegraf_host,hypervisor_hostname=vim3,hypervisor_type=QEMU,hypervisor_version=4002000,service_host=vim3,service_id=192,state=up,status=enabled cpu_topology_cores=28i,cpu_topology_sockets=1i,cpu_topology_threads=2i,current_workload=0i,disk_available_least=2596i,free_disk_gb=2744i,free_ram_mb=374092i,host_ip=\"xx:xx:xx:x::xxx\",id=\"12\",local_gb=3366i,local_gb_used=622i,memory_mb=515404i,memory_mb_used=141312i,running_vms=15i,vcpus=0i,vcpus_used=72i 1634197618000000000\nopenstack_network,host=telegraf_host,name=Network\\ 2,project_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx,status=active,tenant_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx admin_state_up=true,availability_zone_hints=\"\",created_at=\"2021-07-29T15:58:25Z\",id=\"f5af5e71-e890-4245-a377-d4d86273c319\",shared=false,subnet_id=\"2f7341c6-074d-42aa-9abc-71c662d9b336\",subnets=1i,updated_at=\"2021-09-02T16:46:48Z\" 1634197618000000000\nopenstack_nova_service,host=telegraf_host,host_machine=vim3,name=nova-compute,state=up,status=enabled,zone=nova disabled_reason=\"\",forced_down=false,id=\"192\",updated_at=\"2021-10-14T07:46:52Z\" 1634197619000000000\nopenstack_port,device_id=a043b8b3-2831-462a-bba8-19088f3db45a,device_owner=compute:nova,host=telegraf_host,name=offload-port1,network_id=6b40d744-9a48-43f2-a4c8-2e0ccb45ac96,project_id=71f9bc44621234f8af99a3949258fc7b,status=ACTIVE,tenant_id=71f9bc44621234f8af99a3949258fc7b admin_state_up=true,allowed_address_pairs=0i,fixed_ips=1i,id=\"fb64626a-07e1-4d78-a70d-900e989537cc\",ip_address=\"1.1.1.5\",mac_address=\"xx:xx:xx:xx:xx:xx\",security_groups=\"\",subnet_id=\"eafa1eca-b318-4746-a55a-682478466689\" 1634197620000000000\nopenstack_identity,domain_id=default,host=telegraf_host,name=service,parent_id=default enabled=true,id=\"a0877dd2ed1d4b5f952f5689bc04b0cb\",is_domain=false,projects=7i 1634197621000000000\nopenstack_server,flavor=0d438971-56cf-4f86-801f-7b04b29384cb,host=telegraf_host,host_id=c0fe05b14261d35cf8748a3f5aae1234b88c2fd62b69fe24ca4a27e9,host_name=vim1,image=b295f1f3-1w23-470c-8734-197676eedd16,name=test-VM7,project=admin,status=active,tenant_id=80ac889731f540498fb1dc78e4bcd5ed,user_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx accessIPv4=\"\",accessIPv6=\"\",addresses=1i,adminPass=\"\",created=\"2021-09-07T14:40:11Z\",disk_gb=8i,fault_code=0i,fault_created=\"0001-01-01T00:00:00Z\",fault_details=\"\",fault_message=\"\",id=\"db92ee0d-459b-458e-9fe3-2be5ec7c87e1\",progress=0i,ram_mb=16384i,security_groups=1i,updated=\"2021-09-07T14:40:19Z\",vcpus=4i,volumes_attached=0i 1634197656000000000\nopenstack_service,host=telegraf_host,name=identity service_enabled=true,service_id=\"ad605eff92444a158d0f78768f2c4668\" 1634197656000000000\nopenstack_storage_pool,driver_version=1.0.0,host=telegraf_host,name=storage_bloack_1,storage_protocol=nfs,vendor_name=xyz,volume_backend_name=abc free_capacity_gb=4847.54,total_capacity_gb=4864 1634197658000000000\nopenstack_subnet,cidr=10.10.20.10/28,gateway_ip=10.10.20.17,host=telegraf_host,ip_version=4,name=IPv4_Subnet_2,network_id=73c6e1d3-f522-4a3f-8e3c-762a0c06d68b,openstack_tags_lab=True,project_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx,tenant_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx allocation_pools=\"10.10.20.11-10.10.20.30\",dhcp_enabled=true,dns_nameservers=\"\",id=\"db69fbb2-9ca1-4370-8c78-82a27951c94b\" 1634197660000000000\nopenstack_volume,attachment_attachment_id=c83ca0d6-c467-44a0-ac1f-f87d769c0c65,attachment_device=/dev/vda,attachment_host_name=vim1,availability_zone=nova,bootable=true,host=telegraf_host,status=in-use,user_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx,volume_type=storage_bloack_1 attachment_attached_at=\"2021-01-12T21:02:04Z\",attachment_server_id=\"c0c6b4af-0d26-4a0b-a6b4-4ea41fa3bb4a\",created_at=\"2021-01-12T21:01:47Z\",encrypted=false,id=\"d4204f1b-b1ae-1233-b25c-a57d91d2846e\",multiattach=false,size=80i,total_attachments=1i,updated_at=\"2021-01-12T21:02:04Z\" 1634197660000000000\nopenstack_request_duration,host=telegraf_host networks=703214354i 1634197660000000000\nopenstack_server_diagnostics,disk_name=vda,host=telegraf_host,no_of_disks=1,no_of_ports=2,port_name=vhu1234566c-9c,server_id=fdddb58c-bbb9-1234-894b-7ae140178909 cpu0_time=4924220000000,cpu1_time=218809610000000,cpu2_time=218624300000000,cpu3_time=220505700000000,disk_errors=-1,disk_read=619156992,disk_read_req=35423,disk_write=8432728064,disk_write_req=882445,memory=8388608,memory-actual=8388608,memory-rss=37276,memory-swap_in=0,port_rx=410516469288,port_rx_drop=13373626,port_rx_errors=-1,port_rx_packets=52140392,port_tx=417312195654,port_tx_drop=0,port_tx_errors=0,port_tx_packets=321385978 1634197660000000000\n```\n\n----------------------------------------\n\nTITLE: Standard Carbon2 Format Example\nDESCRIPTION: Example of the standard Carbon2 format output when using the default \"field_separate\" configuration. Shows how metrics are structured with separate metric and field tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/carbon2/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmetric=name field=field_1 host=foo  30 1234567890\nmetric=name field=field_2 host=foo  4 1234567890\nmetric=name field=field_N host=foo  59 1234567890\n```\n\n----------------------------------------\n\nTITLE: Unix Timestamp Pattern - TOML\nDESCRIPTION: Configuration example for parsing unix timestamps using Grok patterns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/grok/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  grok_patterns = ['%{NUMBER:timestamp:ts-epoch} value=%{NUMBER:value:int}']\n```\n\n----------------------------------------\n\nTITLE: Decoding CloudWatch Metric Stream JSON Data in Telegraf\nDESCRIPTION: This JSON example shows the structure of a decoded metric from CloudWatch Metric Streams. It includes metadata like stream name, account ID, and region, as well as the actual metric data with timestamp and values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch_metric_streams/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"metric_stream_name\": \"sandbox-dev-cloudwatch-metric-stream\",\n    \"account_id\": \"541737779709\",\n    \"region\": \"us-west-2\",\n    \"namespace\": \"AWS/EC2\",\n    \"metric_name\": \"CPUUtilization\",\n    \"dimensions\": {\n        \"InstanceId\": \"i-0efc7ghy09c123428\"\n    },\n    \"timestamp\": 1651679580000,\n    \"value\": {\n        \"max\": 10.011666666666667,\n        \"min\": 10.011666666666667,\n        \"sum\": 10.011666666666667,\n        \"count\": 1\n    },\n    \"unit\": \"Percent\"\n}\n```\n\n----------------------------------------\n\nTITLE: Telegraf Supervisor Input Configuration\nDESCRIPTION: This snippet shows a sample TOML configuration for the Telegraf Supervisor input plugin.  It includes the `url` parameter for specifying the Supervisor's XML-RPC endpoint and demonstrates optional settings for including or excluding specific process metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/supervisor/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Gathers information about processes that running under supervisor using XML-RPC API\n[[inputs.supervisor]]\n  ## Url of supervisor's XML-RPC endpoint if basic auth enabled in supervisor http server,\n  ## than you have to add credentials to url (ex. http://login:pass@localhost:9001/RPC2)\n  # url=\"http://localhost:9001/RPC2\"\n  ## With settings below you can manage gathering additional information about processes\n  ## If both of them empty, then all additional information will be collected.\n  ## Currently supported supported additional metrics are: pid, rc\n  # metrics_include = []\n  # metrics_exclude = [\"pid\", \"rc\"]\n\n```\n\n----------------------------------------\n\nTITLE: Example Output from APC UPSD Input Plugin\nDESCRIPTION: Sample output demonstrating the metrics collected by the apcupsd plugin. The output includes various UPS metrics such as battery charge, voltage levels, and status flags formatted in Telegraf's line protocol.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/apcupsd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\napcupsd,serial=AS1231515,status=ONLINE,ups_name=name1 time_on_battery=0,load_percent=9.7,time_left_minutes=98,output_voltage=230.4,internal_temp=32.4,battery_voltage=27.4,input_frequency=50.2,input_voltage=230.4,battery_charge_percent=100,status_flags=8i 1490035922000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantile Aggregator Plugin in Telegraf\nDESCRIPTION: Sample configuration for the quantile aggregator plugin in Telegraf. Defines settings for period, quantile values, algorithm selection, and compression factor for t-digest approximation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/quantile/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Keep the aggregate quantiles of each metric passing through.\n[[aggregators.quantile]]\n  ## General Aggregator Arguments:\n  ## The period on which to flush & clear the aggregator.\n  # period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  # drop_original = false\n\n  ## Quantiles to output in the range [0,1]\n  # quantiles = [0.25, 0.5, 0.75]\n\n  ## Type of aggregation algorithm\n  ## Supported are:\n  ##  \"t-digest\" -- approximation using centroids, can cope with large number of samples\n  ##  \"exact R7\" -- exact computation also used by Excel or NumPy (Hyndman & Fan 1996 R7)\n  ##  \"exact R8\" -- exact computation (Hyndman & Fan 1996 R8)\n  ## NOTE: Do not use \"exact\" algorithms with large number of samples\n  ##       to not impair performance or memory consumption!\n  # algorithm = \"t-digest\"\n\n  ## Compression for approximation (t-digest). The value needs to be\n  ## greater or equal to 1.0. Smaller values will result in more\n  ## performance but less accuracy.\n  # compression = 100.0\n```\n\n----------------------------------------\n\nTITLE: Example Telegraf Process Metrics Output\nDESCRIPTION: Sample output showing the format of process metrics collected by the plugin, including counts for different process states and total thread count.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/processes/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nprocesses blocked=8i,running=1i,sleeping=265i,stopped=0i,total=274i,zombie=0i,dead=0i,paging=0i,total_threads=687i 1457478636980905042\n```\n\n----------------------------------------\n\nTITLE: Flattening Dynamic Columns Using Bag Unpack\nDESCRIPTION: Alternative method using bag_unpack plugin to automatically unpack dynamic type columns. Less recommended as it's sensitive to schema changes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/azure_data_explorer/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nTablename\n| evaluate bag_unpack(tags, columnsConflict='replace_source')\n| evaluate bag_unpack(fields, columnsConflict='replace_source')\n```\n\n----------------------------------------\n\nTITLE: Creating and Encrypting Systemd Credentials for Telegraf\nDESCRIPTION: Shell commands to encrypt credentials using systemd-creds, creating secure values that can be accessed by the Telegraf service. Shows examples for both direct input and password prompt methods.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\necho -n \"john-doe-jr\" | sudo systemd-creds encrypt - /etc/credstore.encrypted/telegraf.http_user\n```\n\nLANGUAGE: shell\nCODE:\n```\nsystemd-ask-password -n | sudo systemd-creds encrypt - /etc/credstore.encrypted/telegraf.http_password\n```\n\n----------------------------------------\n\nTITLE: Systemd Units Output - Standard Mode\nDESCRIPTION: Shows the basic output format for systemd unit metrics including host, service name, load state, active state, and sub state. Each metric includes load_code, active_code, and sub_code integer values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/systemd_units/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsystemd_units,host=host1.example.com,name=dbus.service,load=loaded,active=active,sub=running,user=telegraf load_code=0i,active_code=0i,sub_code=0i 1533730725000000000\nsystemd_units,host=host1.example.com,name=networking.service,load=loaded,active=failed,sub=failed,user=telegraf load_code=0i,active_code=3i,sub_code=12i 1533730725000000000\nsystemd_units,host=host1.example.com,name=ssh.service,load=loaded,active=active,sub=running,user=telegraf load_code=0i,active_code=0i,sub_code=0i 1533730725000000000\n```\n\n----------------------------------------\n\nTITLE: Metric Renaming Example in Regex Processor\nDESCRIPTION: Example configuration showing how to rename metrics using regular expression patterns and capture groups.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/regex/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.regex]]\n  [[processors.regex.metric_rename]]\n    pattern = '^(\\w+)_.*$'\n    replacement = \"${1}\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Telegraf with Webhooks Configuration\nDESCRIPTION: Steps to copy the generated configuration to the Telegraf configuration directory and start the Telegraf service\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncp config.conf.new /etc/telegraf/telegraf.conf\nsudo service telegraf start\n```\n\n----------------------------------------\n\nTITLE: Configuring Netfilter Conntrack Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet sets up the Netfilter Conntrack input plugin for Telegraf. It specifies collection methods, directories to search, and files to look for connection statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/conntrack/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collects conntrack stats from the configured directories and files.\n# This plugin ONLY supports Linux\n[[inputs.conntrack]]\n  ## The following defaults would work with multiple versions of conntrack.\n  ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n  ## kernel versions, as are the directory locations.\n\n  ## Look through /proc/net/stat/nf_conntrack for these metrics\n  ## all - aggregated statistics\n  ## percpu - include detailed statistics with cpu tag\n  collect = [\"all\", \"percpu\"]\n\n  ## User-specified directories and files to look through\n  ## Directories to search within for the conntrack files above.\n  ## Missing directories will be ignored.\n  dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n\n  ## Superset of filenames to look for within the conntrack dirs.\n  ## Missing files will be ignored.\n  files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n          \"nf_conntrack_count\",\"nf_conntrack_max\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query for Document Count with Filter\nDESCRIPTION: This TOML configuration snippet sets up an OpenSearch query to count documents matching a specific filter query across all indices. It doesn't use any specific metric fields or aggregations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"*\"\n  filter_query = \"product_1 AND HEAD\"\n  query_period = \"1m\"\n  date_field = \"@timestamp\"\n```\n\n----------------------------------------\n\nTITLE: Example LeoGateway Metrics Output Format\nDESCRIPTION: Sample output of collected LeoGateway metrics showing Erlang VM performance, object cache statistics, and request counts with timestamp and node tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/leofs/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nleofs,host=gateway_0,node=gateway_0@127.0.0.1 allocated_memory=87941120,allocated_memory_5min=88067672,count_of_cache-hit=0,count_of_cache-miss=0,ets_memory_usage=4843497,ets_memory_usage_5min=4841574,num_of_deletes=0,num_of_deletes_5min=0,num_of_processes=555,num_of_processes_5min=555,num_of_reads=0,num_of_reads_5min=0,num_of_writes=0,num_of_writes_5min=0,processes_memory_usage=17388052,processes_memory_usage_5min=17413928,system_memory_usage=49531263,system_memory_usage_5min=49577819,total_cached_size=0,total_memory_usage=66917393,total_memory_usage_5min=66989469,total_of_files=0,used_allocated_memory=69,used_allocated_memory_5min=69 1524105894000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Ping Input Plugin\nDESCRIPTION: TOML configuration for the Telegraf ping input plugin, including host specification, method selection, and various ping parameters\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ping/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Ping given url(s) and return statistics\n[[inputs.ping]]\n  ## Hosts to send ping packets to.\n  urls = [\"example.org\"]\n\n  ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n  ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n  ## the plugin will send pings directly.\n  ##\n  ## While the default is \"exec\" for backwards compatibility, new deployments\n  ## are encouraged to use the \"native\" method for improved compatibility and\n  ## performance.\n  # method = \"exec\"\n\n  ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n  ## option of the ping command.\n  # count = 1\n\n  ## Time to wait between sending ping packets in seconds.  Operates like the\n  ## \"-i\" option of the ping command.\n  # ping_interval = 1.0\n\n  ## If set, the time to wait for a ping response in seconds.  Operates like\n  ## the \"-W\" option of the ping command.\n  # timeout = 1.0\n\n  ## If set, the total ping deadline, in seconds.  Operates like the -w option\n  ## of the ping command.\n  # deadline = 10\n\n  ## Interface or source address to send ping from.  Operates like the -I or -S\n  ## option of the ping command.\n  # interface = \"\"\n\n  ## Percentiles to calculate. This only works with the native method.\n  # percentiles = [50, 95, 99]\n\n  ## Specify the ping executable binary.\n  # binary = \"ping\"\n\n  ## Arguments for ping command. When arguments is not empty, the command from\n  ## the binary option will be used and other options (ping_interval, timeout,\n  ## etc) will be ignored.\n  # arguments = [\"-c\", \"3\"]\n\n  ## Use only IPv4 addresses when resolving a hostname. By default, both IPv4\n  ## and IPv6 can be used.\n  # ipv4 = false\n\n  ## Use only IPv6 addresses when resolving a hostname. By default, both IPv4\n  ## and IPv6 can be used.\n  # ipv6 = false\n\n  ## Number of data bytes to be sent. Corresponds to the \"-s\"\n  ## option of the ping command. This only works with the native method.\n  # size = 56\n```\n\n----------------------------------------\n\nTITLE: Configuring Salesforce Input Plugin in TOML\nDESCRIPTION: TOML configuration block for setting up Salesforce credentials and optional parameters like security token, environment type, and API version in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/salesforce/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read API usage and limits for a Salesforce organisation\n[[inputs.salesforce]]\n  ## specify your credentials\n  ##\n  username = \"your_username\"\n  password = \"your_password\"\n  ##\n  ## (optional) security token\n  # security_token = \"your_security_token\"\n  ##\n  ## (optional) environment type (sandbox or production)\n  ## default is: production\n  ##\n  # environment = \"production\"\n  ##\n  ## (optional) API version (default: \"39.0\")\n  ##\n  # version = \"39.0\"\n```\n\n----------------------------------------\n\nTITLE: Example Curl Command for AWS Data Firehose Input\nDESCRIPTION: Example of sending data to the AWS Data Firehose input plugin using curl. The command sends a base64 encoded 'hello world' string with required AWS Firehose headers and JSON payload structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/firehose/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -i -XPOST 'localhost:8080/telegraf' \\\n--header 'x-amz-firehose-request-id: ed4acda5-034f-9f42-bba1-f29aea6d7d8f' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"requestId\": \"ed4acda5-034f-9f42-bba1-f29aea6d7d8f\",\n    \"timestamp\": 1578090901599,\n    \"records\": [\n        {\n          \"data\": \"aGVsbG8gd29ybGQK\" // \"hello world\"\n        }\n    ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query for Bytes Statistics\nDESCRIPTION: This TOML configuration snippet sets up an OpenSearch query to calculate statistics on the 'bytes' field, grouped by response code. It uses the 'stats' metric function to get multiple statistical measures in one query.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query.aggregation]]\n    measurement_name = \"bytes_stats\"\n    index = \"opensearch_dashboards_sample_data_logs\"\n    date_field = \"timestamp\"\n    query_period = \"10m\"\n    filter_query = \"*\"\n    metric_fields = [\"bytes\"]\n    metric_function = \"stats\"\n    tags = [\"response.keyword\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring TimescaleDB with Tags as Foreign Keys in Telegraf\nDESCRIPTION: Configuration for using TimescaleDB with Telegraf where tags are stored as foreign keys. This setup creates a hypertable with compression enabled and segment by tag_id.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/postgresql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\ntags_as_foreign_keys = true\ncreate_templates = [\n    '''CREATE TABLE {{ .table }} ({{ .columns }})''',\n    '''SELECT create_hypertable({{ .table|quoteLiteral }}, 'time', chunk_time_interval => INTERVAL '7d')''',\n    '''ALTER TABLE {{ .table }} SET (timescaledb.compress, timescaledb.compress_segmentby = 'tag_id')''',\n]\n```\n\n----------------------------------------\n\nTITLE: Core Event Group Metrics Output Format\nDESCRIPTION: Example output showing core PMU metrics with CPU thread unhalted events. Shows metrics format with tags for CPU, event type, events_tag and host with corresponding values for enabled, running, raw and scaled counts.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmu/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\npmu_metric,cpu=0,event=CPU_CLK_THREAD_UNHALTED.REF_XCLK,events_tag=unhalted,host=xyz enabled=2871237051i,running=2871237051i,raw=1171711i,scaled=1171711i 1621254096000000000\npmu_metric,cpu=0,event=CPU_CLK_UNHALTED.THREAD_P_ANY,events_tag=unhalted,host=xyz enabled=2871240713i,running=2871240713i,raw=72340716i,scaled=72340716i 1621254096000000000\npmu_metric,cpu=1,event=CPU_CLK_THREAD_UNHALTED.REF_XCLK,events_tag=unhalted,host=xyz enabled=2871118275i,running=2871118275i,raw=1646752i,scaled=1646752i 1621254096000000000\npmu_metric,cpu=1,event=CPU_CLK_UNHALTED.THREAD_P_ANY,events_tag=unhalted,host=xyz raw=108802421i,scaled=108802421i,enabled=2871120107i,running=2871120107i 1621254096000000000\npmu_metric,cpu=2,event=CPU_CLK_THREAD_UNHALTED.REF_XCLK,events_tag=unhalted,host=xyz enabled=2871143950i,running=2871143950i,raw=1316834i,scaled=1316834i 1621254096000000000\npmu_metric,cpu=2,event=CPU_CLK_UNHALTED.THREAD_P_ANY,events_tag=unhalted,host=xyz enabled=2871074681i,running=2871074681i,raw=68728436i,scaled=68728436i 1621254096000000000\n```\n\n----------------------------------------\n\nTITLE: Supervisor HTTP Server Configuration\nDESCRIPTION: This snippet demonstrates how to configure the `inet_http_server` section in Supervisor's configuration file to enable the HTTP server required by the Telegraf Supervisor input plugin.  It includes settings for the port, username, and password, which are necessary for basic authentication.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/supervisor/README.md#2025-04-16_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n\n[inet_http_server]\nport = 127.0.0.1:9001\nusername = user\npassword = pass\n\n```\n\n----------------------------------------\n\nTITLE: Setting Interval for Libvirt Plugin in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to set a custom interval for the `inputs.libvirt` plugin. This is helpful to adjust data collection frequency when dealing with performance issues or a large number of VMs to monitor. Setting an appropriate interval can prevent `Collection took longer than expected` warnings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/libvirt/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.libvirt]]\n  interval = \"30s\"\n```\n\n----------------------------------------\n\nTITLE: JSONata Batch Transformation - Metric Processing\nDESCRIPTION: JSONata expression to process multiple metrics, extracting and computing values\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\nmetrics.{\n    \"capacity\": $sum($sift($.fields,function($value,$key){$key~>/^field_/}).*),\n    \"images\": fields.n_images,\n    \"service\": (name & \"(\" & tags.host & \")\"),\n    \"time\": $fromMillis(timestamp*1000)\n}\n```\n\n----------------------------------------\n\nTITLE: Example Output of NATS Server Monitoring in Telegraf\nDESCRIPTION: Sample output from the NATS server monitoring plugin showing collected metrics including uptime, memory usage, connection counts, and message statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nats/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnats,server=http://localhost:8222 uptime=117158348682i,mem=6647808i,subscriptions=0i,out_bytes=0i,connections=0i,in_msgs=0i,total_connections=0i,cores=2i,cpu=0,slow_consumers=0i,routes=0i,remotes=0i,out_msgs=0i,in_bytes=0i 1517015107000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Agent Options in Telegraf\nDESCRIPTION: This snippet shows the global agent configuration options for Telegraf. It includes settings for interval, round interval, metric batch size, metric buffer limit, collection jitter, flush interval, flush jitter, precision, debug, quiet mode, and logfile options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/migrations/inputs_jolokia/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s.\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  ## Valid time units are \"ns\", \"us\" (or \"s\"), \"ms\", \"s\".\n  precision = \"\"\n\n  ## Log at debug level.\n  # debug = false\n  ## Log only error level messages.\n  # quiet = false\n\n  ## Log target controls the destination for logs and can be one of \"file\",\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\n  ## is determined by the \"logfile\" setting.\n  # logtarget = \"file\"\n\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\n  ## the empty string then logs are written to stderr.\n  # logfile = \"\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  # logfile_rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # logfile_rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  # log_with_timezone = \"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Alert Notifications Using TICK Script\nDESCRIPTION: This shell snippet presents a TICK script for alerting on changes in the state of monitored Windows services. It triggers HTTP POST notifications when a service transitions away from the running state and vice versa.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_services/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nstream\n    |from()\n        .database('telegraf')\n        .retentionPolicy('autogen')\n        .measurement('win_services')\n        .groupBy('host','service_name')\n    |alert()\n        .crit(lambda: \"state\" != 4)\n        .stateChangesOnly()\n        .message('Service {{ index .Tags \"service_name\" }} on Host {{ index .Tags \"host\" }} is in state {{ index .Fields \"state\" }} ')\n        .post('http://localhost:666/alert/service')\n```\n\n----------------------------------------\n\nTITLE: OpenLDAP Metrics Output Example\nDESCRIPTION: Sample output showing the format and types of metrics collected by the plugin, including operations statistics, thread information, and connection details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openldap/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nopenldap,server=localhost,port=389,host=niska.ait.psu.edu operations_bind_initiated=10i,operations_unbind_initiated=6i,operations_modrdn_completed=0i,operations_delete_initiated=0i,operations_add_completed=2i,operations_delete_completed=0i,operations_abandon_completed=0i,statistics_entries=1516i,threads_open=2i,threads_active=1i,waiters_read=1i,operations_modify_completed=0i,operations_extended_initiated=4i,threads_pending=0i,operations_search_initiated=36i,operations_compare_initiated=0i,connections_max_file_descriptors=4096i,operations_modify_initiated=0i,operations_modrdn_initiated=0i,threads_max=16i,time_uptime=6017i,connections_total=1037i,connections_current=1i,operations_add_initiated=2i,statistics_bytes=162071i,operations_unbind_completed=6i,operations_abandon_initiated=0i,statistics_pdu=1566i,threads_max_pending=0i,threads_backload=1i,waiters_write=0i,operations_bind_completed=10i,operations_search_completed=35i,operations_compare_completed=0i,operations_extended_completed=4i,statistics_referrals=0i,threads_starting=0i 1516912070000000000\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input for Dropwizard Metric Registry\nDESCRIPTION: This JSON snippet represents a typical Dropwizard metric registry, including counters, meters, gauges, histograms, and timers. It demonstrates the structure of metric data that the Dropwizard parser can process.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/dropwizard/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"version\": \"3.0.0\",\n    \"counters\" : {\n        \"measurement,tag1=green\" : {\n            \"count\" : 1\n        }\n    },\n    \"meters\" : {\n        \"measurement\" : {\n            \"count\" : 1,\n            \"m15_rate\" : 1.0,\n            \"m1_rate\" : 1.0,\n            \"m5_rate\" : 1.0,\n            \"mean_rate\" : 1.0,\n            \"units\" : \"events/second\"\n        }\n    },\n    \"gauges\" : {\n        \"measurement\" : {\n            \"value\" : 1\n        }\n    },\n    \"histograms\" : {\n        \"measurement\" : {\n            \"count\" : 1,\n            \"max\" : 1.0,\n            \"mean\" : 1.0,\n            \"min\" : 1.0,\n            \"p50\" : 1.0,\n            \"p75\" : 1.0,\n            \"p95\" : 1.0,\n            \"p98\" : 1.0,\n            \"p99\" : 1.0,\n            \"p999\" : 1.0,\n            \"stddev\" : 1.0\n        }\n    },\n    \"timers\" : {\n        \"measurement\" : {\n            \"count\" : 1,\n            \"max\" : 1.0,\n            \"mean\" : 1.0,\n            \"min\" : 1.0,\n            \"p50\" : 1.0,\n            \"p75\" : 1.0,\n            \"p95\" : 1.0,\n            \"p98\" : 1.0,\n            \"p99\" : 1.0,\n            \"p999\" : 1.0,\n            \"stddev\" : 1.0,\n            \"m15_rate\" : 1.0,\n            \"m1_rate\" : 1.0,\n            \"m5_rate\" : 1.0,\n            \"mean_rate\" : 1.0,\n            \"duration_units\" : \"seconds\",\n            \"rate_units\" : \"calls/second\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes DaemonSet Metrics with Telegraf\nDESCRIPTION: This snippet records metrics for a Kubernetes DaemonSet, including its scheduling state and availability.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_daemonset,daemonset_name=telegraf,selector_select1=s1,namespace=logging number_unavailable=0i,desired_number_scheduled=11i,number_available=11i,number_misscheduled=8i,number_ready=11i,updated_number_scheduled=11i,created=1527758699000000000i,generation=16i,current_number_scheduled=11i 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Custom Timestamp Pattern - TOML\nDESCRIPTION: Configuration example showing how to parse custom timestamp formats using Grok patterns.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/grok/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-\"2006-01-02 15:04:05\"} value=%{NUMBER:value:int}']\n```\n\n----------------------------------------\n\nTITLE: Displaying Telegraf Mesos Master Metrics Output Format\nDESCRIPTION: This example shows the output format when Telegraf collects metrics from a Mesos master node in leader state. The output includes tags such as role, state, host, and server followed by various metrics including CPU, memory, disk usage, and framework statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mesos/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmesos,role=master,state=leader,host=172.17.8.102,server=172.17.8.101\nallocator/event_queue_dispatches=0,master/cpus_percent=0,\nmaster/cpus_revocable_percent=0,master/cpus_revocable_total=0,\nmaster/cpus_revocable_used=0,master/cpus_total=2,\nmaster/cpus_used=0,master/disk_percent=0,master/disk_revocable_percent=0,\nmaster/disk_revocable_total=0,master/disk_revocable_used=0,master/disk_total=10823,\nmaster/disk_used=0,master/dropped_messages=2,master/elected=1,\nmaster/event_queue_dispatches=10,master/event_queue_http_requests=0,\nmaster/event_queue_messages=0,master/frameworks_active=2,master/frameworks_connected=2,\nmaster/frameworks_disconnected=0,master/frameworks_inactive=0,\nmaster/invalid_executor_to_framework_messages=0,\nmaster/invalid_framework_to_executor_messages=0,\nmaster/invalid_status_update_acknowledgements=0,master/invalid_status_updates=0,master/mem_percent=0,\nmaster/mem_revocable_percent=0,master/mem_revocable_total=0,\nmaster/mem_revocable_used=0,master/mem_total=1002,\nmaster/mem_used=0,master/messages_authenticate=0,\nmaster/messages_deactivate_framework=0 ...\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol to Carbon2 Conversion Example\nDESCRIPTION: Complete example showing conversion from InfluxDB Line Protocol to Carbon2 format. Demonstrates how a single InfluxDB metric with multiple fields is split into multiple Carbon2 metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/carbon2/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nweather,location=us-midwest,season=summer temperature=82,wind=100 1234567890\n```\n\nLANGUAGE: text\nCODE:\n```\nmetric=weather field=temperature location=us-midwest season=summer  82 1234567890\nmetric=weather field=wind location=us-midwest season=summer  100 1234567890\n```\n\n----------------------------------------\n\nTITLE: Creating CrateDB Table Schema for Metrics\nDESCRIPTION: SQL schema definition for creating the required table structure in CrateDB. Includes columns for hash_id, timestamp, name, tags, fields, and a generated day column. The table is partitioned by day with a composite primary key.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/cratedb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE IF NOT EXISTS my_metrics (\n  \"hash_id\" LONG INDEX OFF,\n  \"timestamp\" TIMESTAMP,\n  \"name\" STRING,\n  \"tags\" OBJECT(DYNAMIC),\n  \"fields\" OBJECT(DYNAMIC),\n  \"day\" TIMESTAMP GENERATED ALWAYS AS date_trunc('day', \"timestamp\"),\n  PRIMARY KEY (\"timestamp\", \"hash_id\",\"day\")\n) PARTITIONED BY(\"day\");\n```\n\n----------------------------------------\n\nTITLE: Example Bcache Metrics Output\nDESCRIPTION: Sample output showing the format of collected metrics including dirty data, bypass statistics, cache hits/misses, and other performance metrics for a bcache device.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bcache/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nbcache,backing_dev=\"md10\",bcache_dev=\"bcache0\" dirty_data=11639194i,bypassed=5167704440832i,cache_bypass_hits=146270986i,cache_bypass_misses=0i,cache_hit_ratio=90i,cache_hits=511941651i,cache_miss_collisions=157678i,cache_misses=50647396i,cache_readaheads=0i\n```\n\n----------------------------------------\n\nTITLE: Example JSON with Nested Arrays in Object\nDESCRIPTION: This JSON example demonstrates a complex nested structure with arrays inside objects that the json_v2 parser can process. It contains a book object with nested arrays for chapters, characters, and random numbers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json_v2/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"book\": {\n        \"title\": \"The Lord Of The Rings\",\n        \"chapters\": [\n            \"A Long-expected Party\",\n            \"The Shadow of the Past\"\n        ],\n        \"author\": \"Tolkien\",\n        \"characters\": [\n            {\n                \"name\": \"Bilbo\",\n                \"species\": \"hobbit\"\n            },\n            {\n                \"name\": \"Frodo\",\n                \"species\": \"hobbit\"\n            }\n        ],\n        \"random\": [\n            1,\n            2\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Neptune Apex Data Output in Telegraf Format\nDESCRIPTION: This code snippet shows the structured output of Neptune Apex aquarium controller data as it would appear in Telegraf. It includes measurements for controller status, various outputs (e.g., pumps, lights, heaters), and probe readings (e.g., temperature, pH, ORP).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/neptune_apex/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nneptune_apex,hardware=1.0,host=ubuntu,software=5.04_7A18,source=apex,type=controller power_failed=1544814000000000000i,power_restored=1544833875000000000i,serial=\"AC5:12345\" 1545978278000000000\nneptune_apex,device_id=base_Var1,hardware=1.0,host=ubuntu,name=VarSpd1_I1,output_id=0,output_type=variable,software=5.04_7A18,source=apex,type=output state=\"PF1\" 1545978278000000000\nneptune_apex,device_id=base_Var2,hardware=1.0,host=ubuntu,name=VarSpd2_I2,output_id=1,output_type=variable,software=5.04_7A18,source=apex,type=output state=\"PF2\" 1545978278000000000\nneptune_apex,device_id=base_Var3,hardware=1.0,host=ubuntu,name=VarSpd3_I3,output_id=2,output_type=variable,software=5.04_7A18,source=apex,type=output state=\"PF3\" 1545978278000000000\nneptune_apex,device_id=base_Var4,hardware=1.0,host=ubuntu,name=VarSpd4_I4,output_id=3,output_type=variable,software=5.04_7A18,source=apex,type=output state=\"PF4\" 1545978278000000000\nneptune_apex,device_id=base_Alarm,hardware=1.0,host=ubuntu,name=SndAlm_I6,output_id=4,output_type=alert,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=base_Warn,hardware=1.0,host=ubuntu,name=SndWrn_I7,output_id=5,output_type=alert,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=base_email,hardware=1.0,host=ubuntu,name=EmailAlm_I5,output_id=6,output_type=alert,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=base_email2,hardware=1.0,host=ubuntu,name=Email2Alm_I9,output_id=7,output_type=alert,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=2_1,hardware=1.0,host=ubuntu,name=RETURN_2_1,output_id=8,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0.3,state=\"AON\",watt=34 1545978278000000000\nneptune_apex,device_id=2_2,hardware=1.0,host=ubuntu,name=Heater1_2_2,output_id=9,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AOF\",watt=0 1545978278000000000\nneptune_apex,device_id=2_3,hardware=1.0,host=ubuntu,name=FREE_2_3,output_id=10,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"OFF\",watt=1 1545978278000000000\nneptune_apex,device_id=2_4,hardware=1.0,host=ubuntu,name=LIGHT_2_4,output_id=11,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"OFF\",watt=1 1545978278000000000\nneptune_apex,device_id=2_5,hardware=1.0,host=ubuntu,name=LHead_2_5,output_id=12,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=4 1545978278000000000\nneptune_apex,device_id=2_6,hardware=1.0,host=ubuntu,name=SKIMMER_2_6,output_id=13,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0.1,state=\"AON\",watt=12 1545978278000000000\nneptune_apex,device_id=2_7,hardware=1.0,host=ubuntu,name=FREE_2_7,output_id=14,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"OFF\",watt=1 1545978278000000000\nneptune_apex,device_id=2_8,hardware=1.0,host=ubuntu,name=CABLIGHT_2_8,output_id=15,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=1 1545978278000000000\nneptune_apex,device_id=2_9,hardware=1.0,host=ubuntu,name=LinkA_2_9,output_id=16,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=2_10,hardware=1.0,host=ubuntu,name=LinkB_2_10,output_id=17,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=3_1,hardware=1.0,host=ubuntu,name=RVortech_3_1,output_id=18,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"TBL\",xstatus=\"OK\" 1545978278000000000\nneptune_apex,device_id=3_2,hardware=1.0,host=ubuntu,name=LVortech_3_2,output_id=19,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"TBL\",xstatus=\"OK\" 1545978278000000000\nneptune_apex,device_id=4_1,hardware=1.0,host=ubuntu,name=OSMOLATO_4_1,output_id=20,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AOF\",watt=0 1545978278000000000\nneptune_apex,device_id=4_2,hardware=1.0,host=ubuntu,name=HEATER2_4_2,output_id=21,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AOF\",watt=0 1545978278000000000\nneptune_apex,device_id=4_3,hardware=1.0,host=ubuntu,name=NUC_4_3,output_id=22,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0.1,state=\"AON\",watt=8 1545978278000000000\nneptune_apex,device_id=4_4,hardware=1.0,host=ubuntu,name=CABFAN_4_4,output_id=23,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=1 1545978278000000000\nneptune_apex,device_id=4_5,hardware=1.0,host=ubuntu,name=RHEAD_4_5,output_id=24,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=3 1545978278000000000\nneptune_apex,device_id=4_6,hardware=1.0,host=ubuntu,name=FIRE_4_6,output_id=25,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=3 1545978278000000000\nneptune_apex,device_id=4_7,hardware=1.0,host=ubuntu,name=LightGW_4_7,output_id=26,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=1 1545978278000000000\nneptune_apex,device_id=4_8,hardware=1.0,host=ubuntu,name=GBSWITCH_4_8,output_id=27,output_type=outlet,software=5.04_7A18,source=apex,type=output amp=0,state=\"AON\",watt=0 1545978278000000000\nneptune_apex,device_id=4_9,hardware=1.0,host=ubuntu,name=LinkA_4_9,output_id=28,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=4_10,hardware=1.0,host=ubuntu,name=LinkB_4_10,output_id=29,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=5_1,hardware=1.0,host=ubuntu,name=LinkA_5_1,output_id=30,output_type=unknown,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=Cntl_A1,hardware=1.0,host=ubuntu,name=ATO_EMPTY,output_id=31,output_type=virtual,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=Cntl_A2,hardware=1.0,host=ubuntu,name=LEAK,output_id=32,output_type=virtual,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,device_id=Cntl_A3,hardware=1.0,host=ubuntu,name=SKMR_NOPWR,output_id=33,output_type=virtual,software=5.04_7A18,source=apex,type=output state=\"AOF\" 1545978278000000000\nneptune_apex,hardware=1.0,host=ubuntu,name=Tmp,probe_type=Temp,software=5.04_7A18,source=apex,type=probe value=78.1 1545978278000000000\nneptune_apex,hardware=1.0,host=ubuntu,name=pH,probe_type=pH,software=5.04_7A18,source=apex,type=probe value=7.93 1545978278000000000\nneptune_apex,hardware=1.0,host=ubuntu,name=ORP,probe_type=ORP,software=5.04_7A18,source=apex,type=probe value=191 1545978278000000000\nneptune_apex,hardware=1.0,host=ubuntu,name=Salt,probe_type=Cond,software=5.04_7A18,source=apex,type=probe value=29.4 1545978278000000000\nneptune_apex,hardware=1.0,host=ubuntu,name=Volt_2,software=5.04_7A18,source=apex,type=probe value=117 1545978278000000000\nneptune_apex,hardware=1.0,host=ubuntu,name=Volt_4,software=5.04_7A18,source=apex,type=probe value=118 1545978278000000000\n```\n\n----------------------------------------\n\nTITLE: Example Metrics Output\nDESCRIPTION: Sample output showing the format and structure of metrics collected by the nvidia-smi plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nvidia_smi/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nnvidia_smi,compute_mode=Default,host=8218cf,index=0,name=GeForce\\ GTX\\ 1070,pstate=P2,uuid=GPU-823bc202-6279-6f2c-d729-868a30f14d96 fan_speed=100i,memory_free=7563i,memory_total=8112i,memory_used=549i,temperature_gpu=53i,utilization_gpu=100i,utilization_memory=90i 1523991122000000000\nnvidia_smi,compute_mode=Default,host=8218cf,index=1,name=GeForce\\ GTX\\ 1080,pstate=P2,uuid=GPU-f9ba66fc-a7f5-94c5-da19-019ef2f9c665 fan_speed=100i,memory_free=7557i,memory_total=8114i,memory_used=557i,temperature_gpu=50i,utilization_gpu=100i,utilization_memory=85i 1523991122000000000\nnvidia_smi,compute_mode=Default,host=8218cf,index=2,name=GeForce\\ GTX\\ 1080,pstate=P2,uuid=GPU-d4cfc28d-0481-8d07-b81a-ddfc63d74adf fan_speed=100i,memory_free=7557i,memory_total=8114i,memory_used=557i,temperature_gpu=58i,utilization_gpu=100i,utilization_memory=86i 1523991122000000000\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Payload for Webhook\nDESCRIPTION: This snippet shows how to configure a JSON payload that includes measurement data to be sent through the Particle webhook. It specifies the measurement name and formats it correctly for API consumption.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/particle/README.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"measurement\": \"your_measurement_name\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Sudo Permission for SmartCTL\nDESCRIPTION: Configuration example for enabling sudo usage with the SmartCTL plugin and the required sudoers file configuration for proper permission setup.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smartctl/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.smart_json]]\n  use_sudo = true\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# Add the following lines:\nCmnd_Alias SMARTCTL = /usr/sbin/smartctl\ntelegraf  ALL=(ALL) NOPASSWD: SMARTCTL\nDefaults!SMARTCTL !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: DynamoDB Table Key Configuration for Checkpointing\nDESCRIPTION: Shell command showing the required key configuration for creating a DynamoDB table used for checkpointing in the Kinesis Consumer plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kinesis_consumer/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nPartition key: namespace\nSort key: shard_id\n```\n\n----------------------------------------\n\nTITLE: Example Metric Output Format\nDESCRIPTION: Sample output showing the format of metrics collected by the HueBridge plugin, including light status, temperature, light level, motion sensor, and device power metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/huebridge/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhuebridge_light,huebridge_bridge_id=0123456789ABCDEF,huebridge_room=Name#15,huebridge_device=Name#3 on=0 1734880329\nhuebridge_temperature,huebridge_room=Name#15,huebridge_device=Name#7,huebridge_device_enabled=true,huebridge_bridge_id=0123456789ABCDEF temperature=17.63 1734880329\nhuebridge_light_level,huebridge_bridge_id=0123456789ABCDEF,huebridge_room=Name#15,huebridge_device=Name#7,huebridge_device_enabled=true light_level=18948,light_level_lux=78.46934003526889 1734880329\nhuebridge_motion_sensor,huebridge_bridge_id=0123456789ABCDEF,huebridge_room=Name#15,huebridge_device=Name#7,huebridge_device_enabled=true motion=0 1734880329\nhuebridge_device_power,huebridge_bridge_id=0123456789ABCDEF,huebridge_room=Name#15,huebridge_device=Name#7 battery_level=100,battery_state=normal 1734880329\n```\n\n----------------------------------------\n\nTITLE: Converted UserData Fields in Telegraf Windows Event Log Output\nDESCRIPTION: Shows how the XML UserData is converted into flattened key-value pairs in the Telegraf output. Each nested element becomes a field with underscores separating the levels of nesting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_eventlog/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nCbsPackageChangeState_PackageIdentifier = \"KB4566782\"\nCbsPackageChangeState_IntendedPackageState = \"5112\"\nCbsPackageChangeState_IntendedPackageStateTextized = \"Installed\"\nCbsPackageChangeState_ErrorCode = \"0x0\"\nCbsPackageChangeState_Client = \"UpdateAgentLCU\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Cgroup Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the cgroup input plugin. It allows specifying directories to search for cgroup files and the specific stat fields to collect.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cgroup/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read specific statistics per cgroup\n# This plugin ONLY supports Linux\n[[inputs.cgroup]]\n  ## Directories in which to look for files, globs are supported.\n  ## Consider restricting paths to the set of cgroups you really\n  ## want to monitor if you have a large number of cgroups, to avoid\n  ## any cardinality issues.\n  # paths = [\n  #   \"/sys/fs/cgroup/memory\",\n  #   \"/sys/fs/cgroup/memory/child1\",\n  #   \"/sys/fs/cgroup/memory/child2/*\",\n  # ]\n  ## cgroup stat fields, as file names, globs are supported.\n  ## these file names are appended to each path from above.\n  # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query for Maximum Response Time\nDESCRIPTION: This TOML configuration snippet sets up an OpenSearch query to find the maximum response time per method and URI. It uses the 'max' metric function and excludes missing tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"my-index-*\"\n  filter_query = \"*\"\n  metric_fields = [\"response_time\"]\n  metric_function = \"max\"\n  tags = [\"method.keyword\",\"URI.keyword\"]\n  include_missing_tag = false\n  missing_tag_value = \"null\"\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Netflow v9 Sample Data\nDESCRIPTION: This snippet presents sample data output for Netflow v9 protocol. It includes similar network flow information as Netflow v5, but with some differences in the fields and structure of the data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netflow/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"140.82.121.3\",src_port=443u,dst=\"192.168.119.100\",dst_port=55516u,in_bytes=87477u,in_packets=78u,flow_start_ms=1666350478660u,flow_end_ms=1666350481316u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"140.82.121.6\",src_port=443u,dst=\"192.168.119.100\",dst_port=36408u,in_bytes=5009u,in_packets=21u,flow_start_ms=1666350478447u,flow_end_ms=1666350481267u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"140.82.112.22\",src_port=443u,dst=\"192.168.119.100\",dst_port=39638u,in_bytes=925u,in_packets=6u,flow_start_ms=1666350478324u,flow_end_ms=1666350481214u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"140.82.114.26\",src_port=443u,dst=\"192.168.119.100\",dst_port=49398u,in_bytes=250u,in_packets=2u,flow_start_ms=1666350481131u,flow_end_ms=1666350481362u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"192.168.119.100\",src_port=55516u,dst=\"140.82.121.3\",dst_port=443u,in_bytes=4969u,in_packets=37u,flow_start_ms=1666350478652u,flow_end_ms=1666350481269u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"192.168.119.100\",src_port=36408u,dst=\"140.82.121.6\",dst_port=443u,in_bytes=2736u,in_packets=21u,flow_start_ms=1666350478438u,flow_end_ms=1666350481258u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"192.168.119.100\",src_port=39638u,dst=\"140.82.112.22\",dst_port=443u,in_bytes=1560u,in_packets=6u,flow_start_ms=1666350478225u,flow_end_ms=1666350481255u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\nnetflow,source=127.0.0.1,version=NetFlowV9 protocol=\"tcp\",src=\"192.168.119.100\",src_port=49398u,dst=\"140.82.114.26\",dst_port=443u,in_bytes=697u,in_packets=4u,flow_start_ms=1666350481030u,flow_end_ms=1666350481362u,tcp_flags=\"...PA...\",engine_type=\"17\",engine_id=\"0x01\",icmp_type=0u,icmp_code=0u,fwd_status=\"unknown\",fwd_reason=\"unknown\",src_tos=\"0x00\"\n```\n\n----------------------------------------\n\nTITLE: Sample Salesforce Metrics Output\nDESCRIPTION: Example output showing various Salesforce metrics including API limits, storage usage, and workflow metrics with their maximum and remaining values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/salesforce/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsalesforce,organization_id=XXXXXXXXXXXXXXXXXX,host=xxxxx.salesforce.com daily_workflow_emails_max=546000i,hourly_time_based_workflow_max=50i,daily_async_apex_executions_remaining=250000i,daily_durable_streaming_api_events_remaining=1000000i,streaming_api_concurrent_clients_remaining=2000i,daily_bulk_api_requests_remaining=10000i,hourly_sync_report_runs_remaining=500i,daily_api_requests_max=5000000i,data_storage_mb_remaining=1073i,file_storage_mb_remaining=1069i,daily_generic_streaming_api_events_remaining=10000i,hourly_async_report_runs_remaining=1200i,hourly_time_based_workflow_remaining=50i,daily_streaming_api_events_remaining=1000000i,single_email_max=5000i,hourly_dashboard_refreshes_remaining=200i,streaming_api_concurrent_clients_max=2000i,daily_durable_generic_streaming_api_events_remaining=1000000i,daily_api_requests_remaining=4999998i,hourly_dashboard_results_max=5000i,hourly_async_report_runs_max=1200i,daily_durable_generic_streaming_api_events_max=1000000i,hourly_dashboard_results_remaining=5000i,concurrent_sync_report_runs_max=20i,durable_streaming_api_concurrent_clients_remaining=2000i,daily_workflow_emails_remaining=546000i,hourly_dashboard_refreshes_max=200i,daily_streaming_api_events_max=1000000i,hourly_sync_report_runs_max=500i,hourly_o_data_callout_max=10000i,mass_email_max=5000i,mass_email_remaining=5000i,single_email_remaining=5000i,hourly_dashboard_statuses_max=999999999i,concurrent_async_get_report_instances_max=200i,daily_durable_streaming_api_events_max=1000000i,daily_generic_streaming_api_events_max=10000i,hourly_o_data_callout_remaining=10000i,concurrent_sync_report_runs_remaining=20i,daily_bulk_api_requests_max=10000i,data_storage_mb_max=1073i,hourly_dashboard_statuses_remaining=999999999i,concurrent_async_get_report_instances_remaining=200i,daily_async_apex_executions_max=250000i,durable_streaming_api_concurrent_clients_max=2000i,file_storage_mb_max=1073i 1501565661000000000\n```\n\n----------------------------------------\n\nTITLE: Example Chrony Metrics Output for Telegraf\nDESCRIPTION: Sample output format from the chrony input plugin showing collected NTP metrics with tags for reference_id, stratum, and leap_status.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/chrony/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nchrony,leap_status=not\\ synchronized,reference_id=A29FC87B,stratum=3 frequency=-16.000999450683594,last_offset=0.000012651000361074694,residual_freq=0,rms_offset=0.000025576999178156257,root_delay=0.0016550000291317701,root_dispersion=0.00330700003542006,skew=0.006000000052154064,system_time=0.000020389999917824753,update_interval=507.1999816894531 1706271167571675297\n```\n\n----------------------------------------\n\nTITLE: Standard JSON Metric Format - Single Metric\nDESCRIPTION: JSON representation of a single metric with fields, name, tags, and timestamp\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"fields\": {\n        \"field_1\": 30,\n        \"field_2\": 4,\n        \"field_N\": 59,\n        \"n_images\": 660\n    },\n    \"name\": \"docker\",\n    \"tags\": {\n        \"host\": \"raynor\"\n    },\n    \"timestamp\": 1458229140\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Cluster Health Indices Metrics\nDESCRIPTION: Details the metrics structure for index-level health monitoring when cluster_health and cluster_health_level=indices are enabled.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- elasticsearch_cluster_health_indices\n  - tags:\n    - index\n    - name\n  - fields:\n    - active_primary_shards (integer)\n    - active_shards (integer)\n    - initializing_shards (integer)\n    - number_of_replicas (integer)\n    - number_of_shards (integer)\n    - relocating_shards (integer)\n    - status (string, one of green, yellow or red)\n    - status_code (integer, green = 1, yellow = 2, red = 3)\n    - unassigned_shards (integer)\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PMT Inputs with Datatype Filter - TOML\nDESCRIPTION: This snippet demonstrates how to configure Telegraf's Intel PMT input to collect only specific metrics defined by a datatype filter. The configuration specifies the XML file to be used and lists enabled datatypes, which allows for targeted data collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_pmt]]\n  spec = \"/home/telegraf/Intel-PMT/xml/pmt.xml\"\n  datatypes_enabled = [\"tbandwidth_28b\",\"ttemperature\"]\n```\n\n----------------------------------------\n\nTITLE: OpenLDAP Monitoring Query\nDESCRIPTION: LDAP query used to gather monitoring metrics from OpenLDAP, including monitorCounter, monitoredInfo, monitorOpInitiated, and monitorOpCompleted attributes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openldap/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n(|(objectClass=monitorCounterObject)(objectClass=monitorOperation)(objectClass=monitoredObject))\n```\n\n----------------------------------------\n\nTITLE: Configuring JOSE Secret-store Plugin in TOML Format\nDESCRIPTION: Sample configuration for the JOSE secret-store plugin in Telegraf. Defines the secret-store with an ID, path for storing secrets, and optional password configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/jose/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# File based Javascript Object Signing and Encryption based secret-store\n[[secretstores.jose]]\n  ## Unique identifier for the secret-store.\n  ## This id can later be used in plugins to reference the secrets\n  ## in this secret-store via @{<id>:<secret_key>} (mandatory)\n  id = \"secretstore\"\n\n  ## Directory for storing the secrets\n  path = \"/etc/telegraf/secrets\"\n\n  ## Password to access the secrets.\n  ## If no password is specified here, Telegraf will prompt for it at startup time.\n  # password = \"\"\n```\n\n----------------------------------------\n\nTITLE: Example HDDtemp Output Format\nDESCRIPTION: Sample output showing the metrics format produced by the HDDtemp plugin. Demonstrates temperature readings from multiple drives with associated tags like device, model, unit, and status.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/hddtemp/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhddtemp,source=server1,unit=C,status=,device=sdb,model=WDC\\ WD740GD-00FLA1 temperature=43i 1481655647000000000\nhddtemp,device=sdc,model=SAMSUNG\\ HD103UI,unit=C,source=server1,status= temperature=38i 148165564700000000\nhddtemp,device=sdd,model=SAMSUNG\\ HD103UI,unit=C,source=server1,status= temperature=36i 1481655647000000000\n```\n\n----------------------------------------\n\nTITLE: Example MarkLogic Metrics Output\nDESCRIPTION: This text block represents an example output from the Telegraf MarkLogic input plugin. It showcases the format and the types of metrics collected, including CPU stats, memory usage, disk space, and network traffic. The metrics are tagged with hostname and ID.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/marklogic/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmarklogic,host=localhost,id=2592913110757471141,source=ml1.local total_cpu_stat_iowait=0.0125649003311992,memory_process_swap_size=0i,host_size=380i,data_dir_space=28216i,query_read_load=0i,ncpus=1i,log_device_space=28216i,query_read_bytes=13947332i,merge_write_load=0i,http_server_receive_bytes=225893i,online=true,ncores=4i,total_cpu_stat_user=0.150778993964195,total_cpu_stat_system=0.598927974700928,total_cpu_stat_idle=99.2210006713867,memory_system_total=3947i,memory_system_free=2669i,memory_size=4096i,total_rate=14.7697010040283,http_server_send_bytes=0i,memory_process_size=903i,memory_process_rss=486i,merge_read_load=0i,total_load=0.00502600101754069 1566373000000000000\n```\n\n----------------------------------------\n\nTITLE: Rsyslog Advanced Format (RainerScript) Configuration\nDESCRIPTION: This bash script shows an alternative Rsyslog configuration using the advanced format (RainerScript). It forwards messages over TCP with octet framing to Telegraf. The action defines the protocol, TCP framing, target IP, port, and the template to use for formatting syslog messages.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# forward over tcp with octet framing according to RFC 5425\naction(type=\"omfwd\" Protocol=\"tcp\" TCP_Framing=\"octet-counted\" Target=\"127.0.0.1\" Port=\"6514\" Template=\"RSYSLOG_SyslogProtocol23Format\")\n\n# uncomment to use udp according to RFC 5424\n#action(type=\"omfwd\" Protocol=\"udp\" Target=\"127.0.0.1\" Port=\"6514\" Template=\"RSYSLOG_SyslogProtocol23Format\")\n\n```\n\n----------------------------------------\n\nTITLE: Example Monit Metric Output in Telegraf\nDESCRIPTION: Sample output demonstrating various system metrics collected from Monit, including file, process, system, and remote host information\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/monit/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmonit_file,monitoring_mode=active,monitoring_status=monitored,pending_action=none,platform_name=Linux,service=rsyslog_pid,source=xyzzy.local,status=running,version=5.20.0 mode=644i,monitoring_mode_code=0i,monitoring_status_code=1i,pending_action_code=0i,size=3i,status_code=0i 1579735047000000000\nmonit_process,monitoring_mode=active,monitoring_status=monitored,pending_action=none,platform_name=Linux,service=rsyslog,source=xyzzy.local,status=running,version=5.20.0 children=0i,cpu_percent=0,cpu_percent_total=0,mem_kb=3148i,mem_kb_total=3148i,mem_percent=0.2,mem_percent_total=0.2,monitoring_mode_code=0i,monitoring_status_code=1i,parent_pid=1i,pending_action_code=0i,pid=318i,status_code=0i,threads=4i 1579735047000000000\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input with Nested Dropwizard Registry\nDESCRIPTION: This JSON snippet shows a document containing a Dropwizard registry nested within a 'metrics' field, along with additional metadata like timestamp and tags. It demonstrates how to parse Dropwizard metrics from a more complex JSON structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/dropwizard/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"time\" : \"2017-02-22T14:33:03.662+02:00\",\n    \"tags\" : {\n        \"tag1\" : \"green\",\n        \"tag2\" : \"yellow\"\n    },\n    \"metrics\" : {\n        \"counters\" : {\n            \"measurement\" : {\n                \"count\" : 1\n            }\n        },\n        \"meters\" : {},\n        \"gauges\" : {},\n        \"histograms\" : {},\n        \"timers\" : {}\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes ConfigMap Metrics with Telegraf\nDESCRIPTION: This snippet logs metrics for a Kubernetes ConfigMap, tracking its creation and related metadata.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_configmap,configmap_name=envoy-config,namespace=default,resource_version=56593031 created=1544103867000000000i 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Pivot Processor Plugin with TOML\nDESCRIPTION: This code snippet uses TOML syntax to configure the pivot processor in Telegraf. The configuration sets the tag used for the naming of the new field to 'name' and specifies the field 'value' to be used as the new field's value.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/pivot/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"[[processors.pivot]]\\n  ## Tag to use for naming the new field.\\n  tag_key = \\\"name\\\"\\n  ## Field to use as the value of the new field.\\n  value_key = \\\"value\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Output Format for Intel DLB Metrics\nDESCRIPTION: Sample output showing the format of collected metrics including device statistics, queue links, and RAS metrics from both DPDK telemetry and kernel sources.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_dlb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nintel_dlb,command=/eventdev/dev_xstats\\,0,host=controller1 dev_dir_pool_size=0i,dev_inflight_events=8192i,dev_ldb_pool_size=8192i,dev_nb_events_limit=8192i,dev_pool_size=0i,dev_rx_drop=0i,dev_rx_interrupt_wait=0i,dev_rx_ok=463126660i,dev_rx_umonitor_umwait=0i,dev_total_polls=78422946i,dev_tx_nospc_dir_hw_credits=0i,dev_tx_nospc_hw_credits=584614i,dev_tx_nospc_inflight_credits=0i,dev_tx_nospc_inflight_max=0i,dev_tx_nospc_ldb_hw_credits=584614i,dev_tx_nospc_new_event_limit=59331982i,dev_tx_ok=694694059i,dev_zero_polls=29667908i 1641996791000000000\nintel_dlb,command=/eventdev/queue_links\\,0\\,1,host=controller1 qid_0=128i,qid_1=128i 1641996791000000000\nintel_dlb_ras,device=pci0000:6d,host=controller1,metric_file=aer_dev_correctable BadDLLP=0i,BadTLP=0i,CorrIntErr=0i,HeaderOF=0i,NonFatalErr=0i,Rollover=0i,RxErr=0i,TOTAL_ERR_COR=0i,Timeout=0i 1641996791000000000\nintel_dlb_ras,device=pci0000:6d,host=controller1,metric_file=aer_dev_fatal ACSViol=0i,AtomicOpBlocked=0i,BlockedTLP=0i,CmpltAbrt=0i,CmpltTO=0i,DLP=0i,ECRC=0i,FCP=0i,MalfTLP=0i,PoisonTLPBlocked=0i,RxOF=0i,SDES=0i,TLP=0i,TLPBlockedErr=0i,TOTAL_ERR_FATAL=0i,UncorrIntErr=0i,Undefined=0i,UnsupReq=0i,UnxCmplt=0i 1641996791000000000\n```\n\n----------------------------------------\n\nTITLE: Splunk Multimetric HEC JSON Format Example\nDESCRIPTION: This example illustrates the multimetric JSON format for Splunk HEC, allowing multiple metric values to be sent in a single payload.  The metric names are prepended to field names.  Splunk Enterprise and Splunk Cloud 8.0 or later support this format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"time\": 1572469920,\n  \"event\": \"metric\",\n  \"host\": \"mono.local\",\n  \"fields\": {\n    \"class\": \"osx\",\n    \"cpu\": \"cpu0\",\n    \"metric_name:telegraf.cpu.usage_guest\": 0,\n    \"metric_name:telegraf.cpu.usage_guest_nice\": 0,\n    \"metric_name:telegraf.cpu.usage_idle\": 65.1,\n    \"metric_name:telegraf.cpu.usage_iowait\": 0,\n    \"metric_name:telegraf.cpu.usage_irq\": 0,\n    \"metric_name:telegraf.cpu.usage_nice\": 0,\n    \"metric_name:telegraf.cpu.usage_softirq\": 0,\n    \"metric_name:telegraf.cpu.usage_steal\": 0,\n    \"metric_name:telegraf.cpu.usage_system\": 10.2,\n    \"metric_name:telegraf.cpu.usage_user\": 24.7,\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudo Permissions for S.M.A.R.T. Plugin\nDESCRIPTION: Sudoers file configuration to allow Telegraf to execute smartctl and nvme-cli commands with elevated privileges.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# For smartctl add the following lines:\nCmnd_Alias SMARTCTL = /usr/bin/smartctl\ntelegraf  ALL=(ALL) NOPASSWD: SMARTCTL\nDefaults!SMARTCTL !logfile, !syslog, !pam_session\n\n# For nvme-cli add the following lines:\nCmnd_Alias NVME = /path/to/nvme\ntelegraf  ALL=(ALL) NOPASSWD: NVME\nDefaults!NVME !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Batch CSV Output Format Example with Headers in Telegraf\nDESCRIPTION: This example shows how multiple metrics appear in batch format with headers enabled. Each line represents a complete metric with timestamp, measurement name, host tag, fields, and the n_images field.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/csv/README.md#2025-04-16_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\ntimestamp,measurement,host,field_1,field_2,...,field_N,n_images\n1458229140,docker,raynor,30,4,...,59,660\n1458229143,docker,raynor,28,5,...,60,665\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Disk Queue Depth with InfluxDB\nDESCRIPTION: SQL query to calculate the average disk queue depth over time based on weighted I/O time. This provides a more accurate view of queue depth than instantaneous iops_in_progress values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/diskio/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT non_negative_derivative(last(\"weighted_io_time\"),1ms) from \"diskio\" WHERE time > now() - 30m GROUP BY \"host\",\"name\",time(60s)\n```\n\n----------------------------------------\n\nTITLE: Aggregated Uncore Event Metrics Output Format\nDESCRIPTION: Example output showing aggregated uncore PMU metrics with combined XSNP response events across units. Shows aggregated counter values with socket and unit_type tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmu/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit_type=cbox enabled=13199712335i,running=13199712335i,raw=467485i,scaled=467485i 1621254412000000000\n```\n\n----------------------------------------\n\nTITLE: Setting Capabilities for Telegraf Service in Systemd\nDESCRIPTION: Systemd configuration to grant CAP_NET_RAW and CAP_NET_ADMIN capabilities to the Telegraf service. This is necessary for the IPVS plugin to communicate over netlink sockets.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipvs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW CAP_NET_ADMIN\nAmbientCapabilities=CAP_NET_RAW CAP_NET_ADMIN\n```\n\n----------------------------------------\n\nTITLE: Joined SNMP Tables Output in Telegraf\nDESCRIPTION: Sample output showing the result of joining two SNMP tables. This demonstrates how the CISCO power data and entity table data are combined using the secondary index feature to create a unified view with both power consumption and interface names.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n> ciscoPowerEntity,EntPhysicalName=GigabitEthernet1/2,index=1.2 EntPhyIndex=1002i,PortPwrConsumption=6643i 1621461148000000000\n> ciscoPowerEntity,EntPhysicalName=GigabitEthernet1/6,index=1.6 EntPhyIndex=1006i,PortPwrConsumption=10287i 1621461148000000000\n> ciscoPowerEntity,EntPhysicalName=GigabitEthernet1/5,index=1.5 EntPhyIndex=1005i,PortPwrConsumption=8358i 1621461148000000000\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Maximum Queue Size\nDESCRIPTION: InfluxDB SQL query to retrieve maximum queue size for all interfaces over the last hour, grouped by time, hostname, and interface name.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lanz/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT max(\"queue_size\") AS \"max_queue_size\" FROM \"congestion_record\" WHERE time > now() - 1h GROUP BY time(10s), \"hostname\", \"intf_name\"\n```\n\n----------------------------------------\n\nTITLE: Adding Year as Tag in Telegraf Template Processor\nDESCRIPTION: Example showing how to add the current UTC year as a tag, similar to the date processor functionality.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = \"year\"\n  template = '{{.Time.UTC.Year}}'\n```\n\n----------------------------------------\n\nTITLE: Rename Processor Example Output in Diff Format\nDESCRIPTION: A diff example showing the before and after states of a metric processed by the rename processor. It demonstrates how measurement, tag, and field names are transformed according to the configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/rename/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- network_interface_throughput,hostname=backend.example.com lower=10i,upper=1000i,mean=500i 1502489900000000000\n+ throughput,host=backend.example.com min=10i,max=1000i,mean=500i 1502489900000000000\n```\n\n----------------------------------------\n\nTITLE: Performance Counter Object Configuration\nDESCRIPTION: Demonstrates a typical configuration for collecting performance counters for a specific object, instances, and counters\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_7\n\nLANGUAGE: TOML\nCODE:\n```\n[[inputs.win_perf_counters.object]]\nObjectName = \"LogicalDisk\"\nInstances = [\"C:\", \"D:\", \"E:\"]\nCounters = [\"% Idle Time\", \"% Disk Read Time\", \"% Disk Write Time\"]\nMeasurement = \"win_disk\"\nUseRawValues = true\n```\n\n----------------------------------------\n\nTITLE: Example Ping Metric Output\nDESCRIPTION: Sample output showing the format of collected ping metrics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ping/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nping,url=example.org average_response_ms=23.066,ttl=63,maximum_response_ms=24.64,minimum_response_ms=22.451,packets_received=5i,packets_transmitted=5i,percent_packet_loss=0,result_code=0i,standard_deviation_ms=0.809 1535747258000000000\n```\n\n----------------------------------------\n\nTITLE: SNMP Lookup Processor with IfDescr Fallback in TOML\nDESCRIPTION: This TOML configuration for the SNMP Lookup Processor Plugin in Telegraf shows how to specify a fallback to 'ifDescr' if 'ifName' is unavailable. It sets the 'agent_tag' and 'index_tag', then lists the OIDs for 'ifName' resolution. This setup ensures a reliable fallback for network interface identification using SNMP.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/snmp_lookup/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.snmp_lookup]]\n  agent_tag = \"agent\"\n  index_tag = \"ifIndex\"\n\n  [[processors.snmp_lookup.tag]]\n    oid = \".1.3.6.1.2.1.2.2.1.2\"\n    name = \"ifName\"\n\n  [[processors.snmp_lookup.tag]]\n    oid = \".1.3.6.1.2.1.31.1.1.1.1\"\n    name = \"ifName\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OPC UA Event Groups for Event Monitoring\nDESCRIPTION: Configuration example for setting up OPC UA event monitoring with multiple event groups. Shows how to define event type nodes, monitored nodes, and filtering criteria for event capture.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n# Group 1\n[[inputs.opcua_listener.events]]\n   sampling_interval = \"10s\"\n   queue_size = \"100\"\n   source_names = [\"SourceName1\", \"SourceName2\"]\n   fields = [\"Severity\", \"Message\", \"Time\"]\n\n   [inputs.opcua_listener.events.event_type_node]\n     namespace = \"1\"\n     identifier_type = \"i\"\n     identifier = \"1234\"\n\n   [[inputs.opcua_listener.events.node_ids]]\n     namespace = \"2\"\n     identifier_type = \"i\"\n     identifier = \"2345\"\n\n# Group 2\n[[inputs.opcua_listener.events]]\n   sampling_interval = \"10s\"\n   queue_size = \"100\"\n   namespace = \"3\"\n   identifier_type = \"s\"\n   source_names = [\"SourceName1\", \"SourceName2\"]\n   fields = [\"Severity\", \"Message\", \"Time\"]\n\n   [inputs.opcua_listener.events.event_type_node]\n     namespace = \"1\"\n     identifier_type = \"i\"\n     identifier = \"5678\"\n\n    node_ids = [\n      {identifier=\"Sensor1\"}, // default values will be used for namespace and identifier_type\n      {namespace=\"2\", identifier=\"TemperatureSensor\"}, // default values will be used for identifier_type\n      {namespace=\"5\", identifier_type=\"i\", identifier=\"2002\"} // no default values will be used\n    ]\n```\n\n----------------------------------------\n\nTITLE: Example Output of Nginx Stream Server Traffic Metrics\nDESCRIPTION: This snippet demonstrates the format and content of metrics collected by the Nginx Stream Server Traffic Input Plugin. It includes various metrics for upstream servers, connections, and overall server status.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_sts/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnginx_sts_upstream,host=localhost,port=80,source=127.0.0.1,upstream=backend_cluster,upstream_address=1.2.3.4:8080 upstream_connect_msec_counter=0i,out_bytes=0i,down=false,connects=0i,session_msec=0i,upstream_session_msec=0i,upstream_session_msec_counter=0i,upstream_connect_msec=0i,upstream_firstbyte_msec_counter=0i,response_3xx_count=0i,session_msec_counter=0i,weight=1i,max_fails=1i,backup=false,upstream_firstbyte_msec=0i,in_bytes=0i,response_1xx_count=0i,response_2xx_count=0i,response_4xx_count=0i,response_5xx_count=0i,fail_timeout=10i 1584699180000000000\nnginx_sts_upstream,host=localhost,port=80,source=127.0.0.1,upstream=backend_cluster,upstream_address=9.8.7.6:8080 upstream_firstbyte_msec_counter=0i,response_2xx_count=0i,down=false,upstream_session_msec_counter=0i,out_bytes=0i,response_5xx_count=0i,weight=1i,max_fails=1i,fail_timeout=10i,connects=0i,session_msec_counter=0i,upstream_session_msec=0i,in_bytes=0i,response_1xx_count=0i,response_3xx_count=0i,response_4xx_count=0i,session_msec=0i,upstream_connect_msec=0i,upstream_connect_msec_counter=0i,upstream_firstbyte_msec=0i,backup=false 1584699180000000000\nnginx_sts_server,host=localhost,port=80,source=127.0.0.1,zone=* response_2xx_count=0i,response_4xx_count=0i,response_5xx_count=0i,session_msec_counter=0i,in_bytes=0i,out_bytes=0i,session_msec=0i,response_1xx_count=0i,response_3xx_count=0i,connects=0i 1584699180000000000\nnginx_sts_connections,host=localhost,port=80,source=127.0.0.1 waiting=1i,accepted=146i,handled=146i,requests=13421i,active=3i,reading=0i,writing=2i 1584699180000000000\n```\n\n----------------------------------------\n\nTITLE: Batch JSON Metric Format - Multiple Metrics\nDESCRIPTION: JSON representation of multiple metrics with a metrics array containing individual metric objects\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"metrics\": [\n        {\n            \"fields\": {\n                \"field_1\": 30,\n                \"field_2\": 4,\n                \"field_N\": 59,\n                \"n_images\": 660\n            },\n            \"name\": \"docker\",\n            \"tags\": {\n                \"host\": \"raynor\"\n            },\n            \"timestamp\": 1458229140\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Average Response Time Aggregation Configuration\nDESCRIPTION: Configuration example for aggregating average response times per URI and response status code from Elasticsearch logs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch_query/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.elasticsearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"my-index-*\"\n  filter_query = \"*\"\n  metric_fields = [\"response_time\"]\n  metric_function = \"avg\"\n  tags = [\"URI.keyword\", \"response.keyword\"]\n  include_missing_tag = true\n  missing_tag_value = \"null\"\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Example Output Format for XtremIO Metrics\nDESCRIPTION: This snippet showcases the format of the output generated by the XtremIO input plugin, presenting examples of metrics output for different components such as batteries, clusters, and volumes. Each line of output is prefixed with the metric name and includes tags representing the component's properties, followed by key-value pairs for specific metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/xtremio/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nxio,guid=abcdefghifklmnopqrstuvwxyz111111,host=HOSTNAME,model_name=Eaton\\ 5P\\ 1550,name=X2-BBU,power_feed=PWR-B,serial_number=SER1234567890 bbus_average_daily_temp=22i,bbus_enabled=1i,bbus_power=286i,bbus_ups_low_battery_no_input=0i,bbus_ups_need_battery_replacement=0i 1638295340000000000\nxio,guid=abcdefghifklmnopqrstuvwxyz222222,host=HOSTNAME,model_name=Eaton\\ 5P\\ 1550,name=X1-BBU,power_feed=PWR-A,serial_number=SER1234567891 bbus_average_daily_temp=22i,bbus_enabled=1i,bbus_power=246i,bbus_ups_low_battery_no_input=0i,bbus_ups_need_battery_replacement=0i 1638295340000000000\nxio,guid=abcdefghifklmnopqrstuvwxyz333333,hardware_platform=X1,host=HOSTNAME,license_id=LIC123456789,name=SERVER01,sys_psnt_serial_number=FNM01234567890 clusters_compression_factor=1.5160012465000001,clusters_data_reduction_ratio=2.1613617899,clusters_free_ssd_space_in_percent=34i,clusters_number_of_volumes=36i,clusters_percent_memory_in_use=29i,clusters_read_iops=331i,clusters_ssd_num=50i,clusters_write_iops=4649i 1638295341000000000\n```\n\n----------------------------------------\n\nTITLE: Logging Container Metadata in InfluxDB Line Protocol\nDESCRIPTION: This snippet logs metadata for ECS containers in a format suitable for InfluxDB. It includes attributes such as memory and CPU limits, current status, image, and container ID, making it useful for resource monitoring and management.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ecs/README.md#2025-04-16_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\necs_container_meta,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a limit_mem=0,type=\"CNI_PAUSE\",container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",docker_name=\"ecs-nginx-2-internalecspause\",limit_cpu=0,known_status=\"RESOURCES_PROVISIONED\",image=\"amazon/amazon-ecs-pause:0.1.0\",image_id=\"\",desired_status=\"RESOURCES_PROVISIONED\" 1542642001000000000\n```\n\n----------------------------------------\n\nTITLE: AzureAD Configuration for OAuth2 Secret-store\nDESCRIPTION: Example configuration for using AzureAD as the OAuth2 service provider in Telegraf's secret-store, showing required tenant_id and scope settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/oauth2/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[secretstores.oauth2]]\n  id = \"secretstore\"\n  service = \"AzureAD\"\n  tenant_id = \"YOUR_TENANT_ID\"\n\n  [[secretstores.oauth2.token]]\n    key = \"mytoken\"\n    client_id = \"YOUR_CLIENT_ID\"\n    client_secret = \"YOUR_CLIENT_SECRET\"\n    scopes = [\"YOUR_CLIENT_ID/.default\"]\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf with Config and Input Filter\nDESCRIPTION: This shell command runs Telegraf with a specified configuration file and filters the inputs to only include InfluxDB metrics. It uses the '--test' flag to execute the test mode, which outputs metrics to be collected by Telegraf without actually writing them to the database. The command requires Telegraf to be installed and configured on the system.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf --config ~/ws/telegraf.conf --input-filter influxdb --test\n```\n\n----------------------------------------\n\nTITLE: Displaying Network Statistics Output from Telegraf\nDESCRIPTION: This code snippet shows the formatted output of network statistics collected by Telegraf using the nstat plugin. It includes data for netstat, SNMP, and SNMPv6, with various metrics such as IP, TCP, UDP, and ICMP statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nstat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n> nstat,host=Hugin,name=netstat IpExtInBcastOctets=2142i,IpExtInBcastPkts=1i,IpExtInCEPkts=0i,IpExtInCsumErrors=0i,IpExtInECT0Pkts=0i,IpExtInECT1Pkts=0i,IpExtInMcastOctets=2636i,IpExtInMcastPkts=14i,IpExtInNoECTPkts=234065i,IpExtInNoRoutes=2i,IpExtInOctets=135040263i,IpExtInTruncatedPkts=0i,IpExtOutBcastOctets=2162i,IpExtOutBcastPkts=2i,IpExtOutMcastOctets=3196i,IpExtOutMcastPkts=28i,IpExtOutOctets=45962238i,IpExtReasmOverlaps=0i,MPTcpExtAddAddr=0i,MPTcpExtAddAddrDrop=0i,MPTcpExtAddAddrTx=0i,MPTcpExtAddAddrTxDrop=0i,MPTcpExtBlackhole=0i,MPTcpExtDSSCorruptionFallback=0i,MPTcpExtDSSCorruptionReset=0i,MPTcpExtDSSNoMatchTCP=0i,MPTcpExtDSSNotMatching=0i,MPTcpExtDataCsumErr=0i,MPTcpExtDuplicateData=0i,MPTcpExtEchoAdd=0i,MPTcpExtEchoAddTx=0i,MPTcpExtEchoAddTxDrop=0i,MPTcpExtInfiniteMapRx=0i,MPTcpExtInfiniteMapTx=0i,MPTcpExtMPCapableACKRX=0i,MPTcpExtMPCapableEndpAttempt=0i,MPTcpExtMPCapableFallbackACK=0i,MPTcpExtMPCapableFallbackSYNACK=0i,MPTcpExtMPCapableSYNACKRX=0i,MPTcpExtMPCapableSYNRX=0i,MPTcpExtMPCapableSYNTX=0i,MPTcpExtMPCapableSYNTXDisabled=0i,MPTcpExtMPCapableSYNTXDrop=0i,MPTcpExtMPCurrEstab=0i,MPTcpExtMPFailRx=0i,MPTcpExtMPFailTx=0i,MPTcpExtMPFallbackTokenInit=0i,MPTcpExtMPFastcloseRx=0i,MPTcpExtMPFastcloseTx=0i,MPTcpExtMPJoinAckHMacFailure=0i,MPTcpExtMPJoinAckRx=0i,MPTcpExtMPJoinNoTokenFound=0i,MPTcpExtMPJoinPortAckRx=0i,MPTcpExtMPJoinPortSynAckRx=0i,MPTcpExtMPJoinPortSynRx=0i,MPTcpExtMPJoinSynAckBackupRx=0i,MPTcpExtMPJoinSynAckHMacFailure=0i,MPTcpExtMPJoinSynAckRx=0i,MPTcpExtMPJoinSynBackupRx=0i,MPTcpExtMPJoinSynRx=0i,MPTcpExtMPJoinSynTx=0i,MPTcpExtMPJoinSynTxBindErr=0i,MPTcpExtMPJoinSynTxConnectErr=0i,MPTcpExtMPJoinSynTxCreatSkErr=0i,MPTcpExtMPPrioRx=0i,MPTcpExtMPPrioTx=0i,MPTcpExtMPRstRx=0i,MPTcpExtMPRstTx=0i,MPTcpExtMPTCPRetrans=0i,MPTcpExtMismatchPortAckRx=0i,MPTcpExtMismatchPortSynRx=0i,MPTcpExtNoDSSInWindow=0i,MPTcpExtOFOMerge=0i,MPTcpExtOFOQueue=0i,MPTcpExtOFOQueueTail=0i,MPTcpExtPortAdd=0i,MPTcpExtRcvPruned=0i,MPTcpExtRcvWndConflict=0i,MPTcpExtRcvWndConflictUpdate=0i,MPTcpExtRcvWndShared=0i,MPTcpExtRmAddr=0i,MPTcpExtRmAddrDrop=0i,MPTcpExtRmAddrTx=0i,MPTcpExtRmAddrTxDrop=0i,MPTcpExtRmSubflow=0i,MPTcpExtSndWndShared=0i,MPTcpExtSubflowRecover=0i,MPTcpExtSubflowStale=0i,TcpExtArpFilter=0i,TcpExtBusyPollRxPackets=0i,TcpExtDelayedACKLocked=1i,TcpExtDelayedACKLost=123i,TcpExtDelayedACKs=1313i,TcpExtEmbryonicRsts=0i,TcpExtIPReversePathFilter=0i,TcpExtListenDrops=0i,TcpExtListenOverflows=0i,TcpExtLockDroppedIcmps=0i,TcpExtOfoPruned=0i,TcpExtOutOfWindowIcmps=0i,TcpExtPAWSActive=0i,TcpExtPAWSEstab=0i,TcpExtPFMemallocDrop=0i,TcpExtPruneCalled=0i,TcpExtRcvPruned=0i,TcpExtSyncookiesFailed=0i,TcpExtSyncookiesRecv=0i,TcpExtSyncookiesSent=0i,TcpExtTCPACKSkippedChallenge=0i,TcpExtTCPACKSkippedFinWait2=0i,TcpExtTCPACKSkippedPAWS=0i,TcpExtTCPACKSkippedSeq=1i,TcpExtTCPACKSkippedSynRecv=0i,TcpExtTCPACKSkippedTimeWait=0i,TcpExtTCPAOBad=0i,TcpExtTCPAODroppedIcmps=0i,TcpExtTCPAOGood=0i,TcpExtTCPAOKeyNotFound=0i,TcpExtTCPAORequired=0i,TcpExtTCPAbortFailed=0i,TcpExtTCPAbortOnClose=132i,TcpExtTCPAbortOnData=457i,TcpExtTCPAbortOnLinger=0i,TcpExtTCPAbortOnMemory=0i,TcpExtTCPAbortOnTimeout=0i,TcpExtTCPAckCompressed=15i,TcpExtTCPAutoCorking=1471i,TcpExtTCPBacklogCoalesce=113i,TcpExtTCPBacklogDrop=0i,TcpExtTCPChallengeACK=0i,TcpExtTCPDSACKIgnoredDubious=0i,TcpExtTCPDSACKIgnoredNoUndo=65i,TcpExtTCPDSACKIgnoredOld=0i,TcpExtTCPDSACKOfoRecv=0i,TcpExtTCPDSACKOfoSent=0i,TcpExtTCPDSACKOldSent=123i,TcpExtTCPDSACKRecv=78i,TcpExtTCPDSACKRecvSegs=78i,TcpExtTCPDSACKUndo=0i,TcpExtTCPDeferAcceptDrop=0i,TcpExtTCPDelivered=95905i,TcpExtTCPDeliveredCE=0i,TcpExtTCPFastOpenActive=0i,TcpExtTCPFastOpenActiveFail=0i,TcpExtTCPFastOpenBlackhole=0i,TcpExtTCPFastOpenCookieReqd=0i,TcpExtTCPFastOpenListenOverflow=0i,TcpExtTCPFastOpenPassive=0i,TcpExtTCPFastOpenPassiveAltKey=0i,TcpExtTCPFastOpenPassiveFail=0i,TcpExtTCPFastRetrans=3i,TcpExtTCPFromZeroWindowAdv=1i,TcpExtTCPFullUndo=2i,TcpExtTCPHPAcks=40380i,TcpExtTCPHPHits=46243i,TcpExtTCPHystartDelayCwnd=81i,TcpExtTCPHystartDelayDetect=3i,TcpExtTCPHystartTrainCwnd=0i,TcpExtTCPHystartTrainDetect=0i,TcpExtTCPKeepAlive=2816i,TcpExtTCPLossFailures=0i,TcpExtTCPLossProbeRecovery=0i,TcpExtTCPLossProbes=85i,TcpExtTCPLossUndo=0i,TcpExtTCPLostRetransmit=0i,TcpExtTCPMD5Failure=0i,TcpExtTCPMD5NotFound=0i,TcpExtTCPMD5Unexpected=0i,TcpExtTCPMTUPFail=0i,TcpExtTCPMTUPSuccess=0i,TcpExtTCPMemoryPressures=0i,TcpExtTCPMemoryPressuresChrono=0i,TcpExtTCPMigrateReqFailure=0i,TcpExtTCPMigrateReqSuccess=0i,TcpExtTCPMinTTLDrop=0i,TcpExtTCPOFODrop=0i,TcpExtTCPOFOMerge=0i,TcpExtTCPOFOQueue=509i,TcpExtTCPOrigDataSent=89707i,TcpExtTCPPLBRehash=0i,TcpExtTCPPartialUndo=1i,TcpExtTCPPureAcks=35929i,TcpExtTCPRcvCoalesce=9070i,TcpExtTCPRcvCollapsed=0i,TcpExtTCPRcvQDrop=0i,TcpExtTCPRenoFailures=0i,TcpExtTCPRenoRecovery=0i,TcpExtTCPRenoRecoveryFail=0i,TcpExtTCPRenoReorder=0i,TcpExtTCPReqQFullDoCookies=0i,TcpExtTCPReqQFullDrop=0i,TcpExtTCPRetransFail=0i,TcpExtTCPSACKDiscard=0i,TcpExtTCPSACKReneging=0i,TcpExtTCPSACKReorder=79i,TcpExtTCPSYNChallenge=0i,TcpExtTCPSackFailures=0i,TcpExtTCPSackMerged=3i,TcpExtTCPSackRecovery=3i,TcpExtTCPSackRecoveryFail=0i,TcpExtTCPSackShiftFallback=116i,TcpExtTCPSackShifted=2i,TcpExtTCPSlowStartRetrans=0i,TcpExtTCPSpuriousRTOs=0i,TcpExtTCPSpuriousRtxHostQueues=0i,TcpExtTCPSynRetrans=1i,TcpExtTCPTSReorder=1i,TcpExtTCPTimeWaitOverflow=0i,TcpExtTCPTimeouts=1i,TcpExtTCPToZeroWindowAdv=1i,TcpExtTCPWantZeroWindowAdv=4i,TcpExtTCPWinProbe=0i,TcpExtTCPWqueueTooBig=0i,TcpExtTCPZeroWindowDrop=0i,TcpExtTW=7592i,TcpExtTWKilled=0i,TcpExtTWRecycled=0i,TcpExtTcpDuplicateDataRehash=0i,TcpExtTcpTimeoutRehash=1i 1742837823000000000\n2025-03-24T17:37:03Z D! [agent] Stopping service inputs\n> nstat,host=Hugin,name=snmp IcmpInAddrMaskReps=0i,IcmpInAddrMasks=0i,IcmpInCsumErrors=0i,IcmpInDestUnreachs=1i,IcmpInEchoReps=0i,IcmpInEchos=0i,IcmpInErrors=0i,IcmpInMsgs=1i,IcmpInParmProbs=0i,IcmpInRedirects=0i,IcmpInSrcQuenchs=0i,IcmpInTimeExcds=0i,IcmpInTimestampReps=0i,IcmpInTimestamps=0i,IcmpMsgInType3=1i,IcmpMsgOutType3=1i,IcmpOutAddrMaskReps=0i,IcmpOutAddrMasks=0i,IcmpOutDestUnreachs=1i,IcmpOutEchoReps=0i,IcmpOutEchos=0i,IcmpOutErrors=0i,IcmpOutMsgs=1i,IcmpOutParmProbs=0i,IcmpOutRateLimitGlobal=0i,IcmpOutRateLimitHost=0i,IcmpOutRedirects=0i,IcmpOutSrcQuenchs=0i,IcmpOutTimeExcds=0i,IcmpOutTimestampReps=0i,IcmpOutTimestamps=0i,IpDefaultTTL=64i,IpForwDatagrams=12i,IpForwarding=1i,IpFragCreates=2i,IpFragFails=0i,IpFragOKs=1i,IpInAddrErrors=0i,IpInDelivers=227088i,IpInDiscards=0i,IpInHdrErrors=0i,IpInReceives=227102i,IpInUnknownProtos=0i,IpOutDiscards=0i,IpOutNoRoutes=44i,IpOutRequests=194901i,IpOutTransmits=194914i,IpReasmFails=0i,IpReasmOKs=0i,IpReasmReqds=0i,IpReasmTimeout=0i,TcpActiveOpens=12453i,TcpAttemptFails=4005i,TcpCurrEstab=9i,TcpEstabResets=2312i,TcpInCsumErrors=0i,TcpInErrs=0i,TcpInSegs=206125i,TcpMaxConn=-1i,TcpOutRsts=6928i,TcpOutSegs=189170i,TcpPassiveOpens=7628i,TcpRetransSegs=86i,TcpRtoAlgorithm=1i,TcpRtoMax=120000i,TcpRtoMin=200i,UdpIgnoredMulti=0i,UdpInCsumErrors=0i,UdpInDatagrams=27391i,UdpInErrors=0i,UdpLiteIgnoredMulti=0i,UdpLiteInCsumErrors=0i,UdpLiteInDatagrams=0i,UdpLiteInErrors=0i,UdpLiteMemErrors=0i,UdpLiteNoPorts=0i,UdpLiteOutDatagrams=0i,UdpLiteRcvbufErrors=0i,UdpLiteSndbufErrors=0i,UdpMemErrors=0i,UdpNoPorts=1i,UdpOutDatagrams=14308i,UdpRcvbufErrors=0i,UdpSndbufErrors=0i 1742837823000000000\n2025-03-24T17:37:03Z D! [agent] Input channel closed\n2025-03-24T17:37:03Z D! [agent] Stopped Successfully\n> nstat,host=Hugin,name=snmp6 Icmp6InCsumErrors=0i,Icmp6InDestUnreachs=0i,Icmp6InEchoReplies=0i,Icmp6InEchos=0i,Icmp6InErrors=0i,Icmp6InGroupMembQueries=0i,Icmp6InGroupMembReductions=0i,Icmp6InGroupMembResponses=0i,Icmp6InMLDv2Reports=0i,Icmp6InMsgs=0i,Icmp6InNeighborAdvertisements=0i,Icmp6InNeighborSolicits=0i,Icmp6InParmProblems=0i,Icmp6InPktTooBigs=0i,Icmp6InRedirects=0i,Icmp6InRouterAdvertisements=0i,Icmp6InRouterSolicits=0i,Icmp6InTimeExcds=0i,Icmp6OutDestUnreachs=0i,Icmp6OutEchoReplies=0i,Icmp6OutEchos=0i,Icmp6OutErrors=0i,Icmp6OutGroupMembQueries=0i,Icmp6OutGroupMembReductions=0i,Icmp6OutGroupMembResponses=0i,Icmp6OutMLDv2Reports=271i,Icmp6OutMsgs=430i,Icmp6OutNeighborAdvertisements=0i,Icmp6OutNeighborSolicits=62i,Icmp6OutParmProblems=0i,Icmp6OutPktTooBigs=0i,Icmp6OutRateLimitHost=0i,Icmp6OutRedirects=0i,Icmp6OutRouterAdvertisements=0i,Icmp6OutRouterSolicits=97i,Icmp6OutTimeExcds=0i,Icmp6OutType133=97i,Icmp6OutType135=62i,Icmp6OutType143=271i,Ip6FragCreates=0i,Ip6FragFails=0i,Ip6FragOKs=0i,Ip6InAddrErrors=0i,Ip6InBcastOctets=0i,Ip6InCEPkts=0i,Ip6InDelivers=6433i,Ip6InDiscards=0i,Ip6InECT0Pkts=0i,Ip6InECT1Pkts=0i,Ip6InHdrErrors=0i,Ip6InMcastOctets=2652i,Ip6InMcastPkts=11i,Ip6InNoECTPkts=6433i,Ip6InNoRoutes=0i,Ip6InOctets=763395i,Ip6InReceives=6433i,Ip6InTooBigErrors=0i,Ip6InTruncatedPkts=0i,Ip6InUnknownProtos=0i,Ip6OutBcastOctets=0i,Ip6OutDiscards=12i,Ip6OutForwDatagrams=0i,Ip6OutMcastOctets=35016i,Ip6OutMcastPkts=453i,Ip6OutNoRoutes=4652i,Ip6OutOctets=795759i,Ip6OutRequests=6875i,Ip6OutTransmits=6875i,Ip6ReasmFails=0i,Ip6ReasmOKs=0i,Ip6ReasmReqds=0i,Ip6ReasmTimeout=0i,Udp6IgnoredMulti=0i,Udp6InCsumErrors=0i,Udp6InDatagrams=45i,Udp6InErrors=0i,Udp6MemErrors=0i,Udp6NoPorts=0i,Udp6OutDatagrams=24i,Udp6RcvbufErrors=0i,Udp6SndbufErrors=0i,UdpLite6InCsumErrors=0i,UdpLite6InDatagrams=0i,UdpLite6InErrors=0i,UdpLite6MemErrors=0i,UdpLite6NoPorts=0i,UdpLite6OutDatagrams=0i,UdpLite6RcvbufErrors=0i,UdpLite6SndbufErrors=0i 1742837823000000000\n```\n\n----------------------------------------\n\nTITLE: Debugging Zookeeper Output via Netcat\nDESCRIPTION: This shell command uses netcat to send the 'mntr' command to a Zookeeper server running on localhost at port 2181. It helps in debugging by retrieving the current state and metrics of the server directly.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zookeeper/README.md#2025-04-16_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\necho mntr | nc localhost 2181\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes Deployment Metrics with Telegraf\nDESCRIPTION: This snippet captures metrics related to a Kubernetes Deployment's replica availability.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_deployment,deployment_name=deployd,selector_select1=s1,namespace=default replicas_unavailable=0i,created=1544103082000000000i,replicas_available=1i 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Inserting Performance Metrics into InfluxDB\nDESCRIPTION: This snippet provides performance metric entries for various cores, capturing data such as core identifier, data type reference, GUID, NUMA node, PCI BDF, sample group, sample name, and the value of the throttle counter. Each line represents a unique metric entry for insertion into InfluxDB, following the InfluxDB line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_5\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=49,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C49_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=50,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C50_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=51,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C51_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=52,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C52_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1468880i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=53,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C53_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2151919i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=54,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C54_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=55,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C55_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2065994i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=56,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C56_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=57,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C57_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=58,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C58_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1553691i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=59,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C59_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1624177i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=60,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C60_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=61,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C61_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=62,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C62_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=63,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C63_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=0,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C0_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=12977949i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=1,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C1_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=2,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C2_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=3,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C3_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=4,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C4_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=7180524i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=5,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C5_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=6,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C6_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=8667263i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=7,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C7_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=8,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C8_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=5945851i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=9,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C9_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=10,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C10_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=6669829i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=11,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C11_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=12,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C12_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=13,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C13_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=14,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C14_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=6579832i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=15,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C15_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=6101856i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=16,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C16_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=17,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C17_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=18,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C18_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=7796183i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=19,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C19_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=6849098i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=20,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C20_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=21,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C21_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=22,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C22_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=12378942i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=23,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C23_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=24,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C24_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=25,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C25_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=8299231i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=26,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C26_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=27,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C27_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=7986390i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=28,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C28_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=29,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C29_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=30,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C30_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=31,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C31_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=32,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C32_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=9876325i 1693766334000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=33,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C33_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=8547471i 1693766334000000000\n```\n\n----------------------------------------\n\nTITLE: Simple Nginx Monitoring Configuration\nDESCRIPTION: A minimal configuration example for monitoring Nginx using Telegraf. Specifies a single URL to gather statistics from the Nginx stub_status module.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nginx]]\n  ## An array of Nginx stub_status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n```\n\n----------------------------------------\n\nTITLE: Sample OPC UA Node Metric Output\nDESCRIPTION: Example output from a configured OPC UA node showing the temperature value and quality status in the Telegraf metric format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nopcua,id=ns\\=3;s\\=Temperature temp=79.0,Quality=\"OK (0x0)\" 1597820490000000000\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Beanstalkd Telegraf Plugin\nDESCRIPTION: Example output showing the metrics collected by the Beanstalkd input plugin. The output includes two measurement types: 'beanstalkd_overview' with system-wide statistics and 'beanstalkd_tube' with tube-specific statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/beanstalkd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nbeanstalkd_overview,host=server.local,hostname=a2ab22ed12e0,id=232485800aa11b24,server=localhost:11300,version=1.10 cmd_stats_tube=29482i,current_jobs_delayed=0i,current_jobs_urgent=6i,cmd_kick=0i,cmd_stats=7378i,cmd_stats_job=0i,current_waiting=0i,max_job_size=65535i,pid=6i,cmd_bury=0i,cmd_reserve_with_timeout=0i,cmd_touch=0i,current_connections=1i,current_jobs_ready=6i,current_producers=0i,cmd_delete=0i,cmd_list_tubes=7369i,cmd_peek_ready=0i,cmd_put=6i,cmd_use=3i,cmd_watch=0i,current_jobs_reserved=0i,rusage_stime=6.07,cmd_list_tubes_watched=0i,cmd_pause_tube=0i,total_jobs=6i,binlog_records_migrated=0i,cmd_list_tube_used=0i,cmd_peek_delayed=0i,cmd_release=0i,current_jobs_buried=0i,job_timeouts=0i,binlog_current_index=0i,binlog_max_size=10485760i,total_connections=7378i,cmd_peek_buried=0i,cmd_reserve=0i,current_tubes=4i,binlog_records_written=0i,cmd_peek=0i,rusage_utime=1.13,uptime=7099i,binlog_oldest_index=0i,current_workers=0i,cmd_ignore=0i 1528801650000000000\nbeanstalkd_tube,host=server.local,name=notifications,server=localhost:11300 pause_time_left=0i,current_jobs_buried=0i,current_jobs_delayed=0i,current_jobs_reserved=0i,current_using=0i,current_waiting=0i,pause=0i,total_jobs=3i,cmd_delete=0i,cmd_pause_tube=0i,current_jobs_ready=3i,current_jobs_urgent=3i,current_watching=0i 1528801650000000000\n```\n\n----------------------------------------\n\nTITLE: Low-Level Discovery Metric Format for Zabbix\nDESCRIPTION: Example of the low-level discovery metric generated for Zabbix that contains tag values as discovery macros.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nlld.host=myhost net_response.port.protocol.server=\"{\\\"data\\\":[{\\\"{#PORT}\\\":\\\"80\\\",\\\"{#PROTOCOL}\\\":\\\"tcp\\\",\\\"{#SERVER}\\\":\\\"example.com\\\"}]}\"\n```\n\n----------------------------------------\n\nTITLE: Event Log Filtering in Win Eventlog Input Plugin - TOML\nDESCRIPTION: This snippet defines how to filter event logs using event log name, XPath Query, and XML Query formats. It explains the usage of eventlog_name and xpath_query settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_eventlog/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\neventlog_name = \"Application\"\n  xpath_query = '''\n\n```\n\nLANGUAGE: toml\nCODE:\n```\neventlog_name = \"\"\n  xpath_query = \"Event/System[EventID=999]\"\n\n```\n\n----------------------------------------\n\nTITLE: Example Slab Metrics Output\nDESCRIPTION: Sample output showing memory usage metrics collected by the Slab plugin, displaying sizes for different kmalloc caches.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/slab/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nslab kmalloc_1024_size=239927296i,kmalloc_512_size=5582848i 1651049129000000000\n```\n\n----------------------------------------\n\nTITLE: Using namepass and namedrop with Separators for Graphite Data in Telegraf\nDESCRIPTION: This configuration shows how to use namepass and namedrop with custom separators to filter Graphite-formatted metrics based on their structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_23\n\nLANGUAGE: toml\nCODE:\n```\n# Pass all metrics of type 'A.C.B' and drop all others like 'A.C.D.B'\n[[inputs.socket_listener]]\n  data_format = \"graphite\"\n  templates = [\"measurement*\"]\n\n  namepass = [\"A.*.B\"]\n  namepass_separator = \".\"\n\n# Drop all metrics of type 'A.C.B' and pass all others like 'A.C.D.B'\n[[inputs.socket_listener]]\n  data_format = \"graphite\"\n  templates = [\"measurement*\"]\n\n  namedrop = [\"A.*.B\"]\n  namedrop_separator = \".\"\n```\n\n----------------------------------------\n\nTITLE: Sample Output for Grouped Sysstat Configuration in Telegraf\nDESCRIPTION: This text snippet shows the output generated by Telegraf when using the sysstat input plugin with grouped output. It includes various system metrics such as CPU utilization, network statistics, and disk I/O.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sysstat/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncpu_util,device=all pct_idle=98.85,pct_iowait=0,pct_nice=0.38,pct_steal=0,pct_system=0.64,pct_user=0.13 1459255626657883725\nswap pswpin_per_s=0,pswpout_per_s=0 1459255626658387650\nper_cpu,device=cpu1 pct_idle=98.98,pct_iowait=0,pct_nice=0.26,pct_steal=0,pct_system=0.51,pct_user=0.26 1459255626659630437\nper_cpu,device=all pct_idle=98.85,pct_iowait=0,pct_nice=0.38,pct_steal=0,pct_system=0.64,pct_user=0.13 1459255626659670744\nper_cpu,device=cpu0 pct_idle=98.73,pct_iowait=0,pct_nice=0.76,pct_steal=0,pct_system=0.51,pct_user=0 1459255626659697515\nhugepages kbhugfree=0,kbhugused=0,pct_hugused=0 1459255626660057517\nnetwork,device=lo coll_per_s=0,pct_ifutil=0,rxcmp_per_s=0,rxdrop_per_s=0,rxerr_per_s=0,rxfifo_per_s=0,rxfram_per_s=0,rxkB_per_s=0.81,rxmcst_per_s=0,rxpck_per_s=16,txcarr_per_s=0,txcmp_per_s=0,txdrop_per_s=0,txerr_per_s=0,txfifo_per_s=0,txkB_per_s=0.81,txpck_per_s=16 1459255626661197666\nnetwork access_per_s=0,active_per_s=0,asmf_per_s=0,asmok_per_s=0,asmrq_per_s=0,atmptf_per_s=0,badcall_per_s=0,call_per_s=0,estres_per_s=0,fragcrt_per_s=0,fragf_per_s=0,fragok_per_s=0,fwddgm_per_s=0,getatt_per_s=0,hit_per_s=0,iadrerr_per_s=0,iadrmk_per_s=0,iadrmkr_per_s=0,idel_per_s=16,idgm_per_s=0,idgmerr_per_s=0,idisc_per_s=0,idstunr_per_s=0,iech_per_s=0,iechr_per_s=0,ierr_per_s=0,ihdrerr_per_s=0,imsg_per_s=0,ip-frag=0,iparmpb_per_s=0,irec_per_s=16,iredir_per_s=0,iseg_per_s=16,isegerr_per_s=0,isrcq_per_s=0,itm_per_s=0,itmex_per_s=0,itmr_per_s=0,iukwnpr_per_s=0,miss_per_s=0,noport_per_s=0,oadrmk_per_s=0,oadrmkr_per_s=0,odgm_per_s=0,odisc_per_s=0,odstunr_per_s=0,oech_per_s=0,oechr_per_s=0,oerr_per_s=0,omsg_per_s=0,onort_per_s=0,oparmpb_per_s=0,oredir_per_s=0,orq_per_s=16,orsts_per_s=0,oseg_per_s=16,osrcq_per_s=0,otm_per_s=0,otmex_per_s=0,otmr_per_s=0,packet_per_s=0,passive_per_s=0,rawsck=0,read_per_s=0,retrans_per_s=0,saccess_per_s=0,scall_per_s=0,sgetatt_per_s=0,sread_per_s=0,swrite_per_s=0,tcp-tw=7,tcp_per_s=0,tcpsck=1543,totsck=4052,udp_per_s=0,udpsck=2,write_per_s=0 1459255626661381788\nnetwork,device=ens33 coll_per_s=0,pct_ifutil=0,rxcmp_per_s=0,rxdrop_per_s=0,rxerr_per_s=0,rxfifo_per_s=0,rxfram_per_s=0,rxkB_per_s=0,rxmcst_per_s=0,rxpck_per_s=0,txcarr_per_s=0,txcmp_per_s=0,txdrop_per_s=0,txerr_per_s=0,txfifo_per_s=0,txkB_per_s=0,txpck_per_s=0 1459255626661533072\ndisk,device=sda,vg=rootvg avgqu-sz=0.01,avgrq-sz=8.5,await=3.31,pct_util=0.1,rd_sec_per_s=0,svctm=0.25,tps=4,wr_sec_per_s=34 1459255626663974389\nqueue blocked=0,ldavg-1=1.61,ldavg-15=1.34,ldavg-5=1.67,plist-sz=1415,runq-sz=0 1459255626664159054\npaging fault_per_s=0.25,majflt_per_s=0,pct_vmeff=0,pgfree_per_s=19,pgpgin_per_s=0,pgpgout_per_s=17,pgscand_per_s=0,pgscank_per_s=0,pgsteal_per_s=0 1459255626664304249\nmem_util kbactive=2206568,kbanonpg=1472208,kbbuffers=118020,kbcached=1035252,kbcommit=8717200,kbdirty=156,kbinact=418912,kbkstack=24672,kbmemfree=1744868,kbmemused=3610272,kbpgtbl=87116,kbslab=233804,kbvmused=0,pct_commit=136.13,pct_memused=67.42 1459255626664554981\nio bread_per_s=0,bwrtn_per_s=34,rtps=0,tps=4,wtps=4 1459255626664596198\ninode dentunusd=235039,file-nr=17120,inode-nr=94505,pty-nr=14 1459255626664663693\ninterrupts,device=i000 intr_per_s=0 1459255626664800109\ninterrupts,device=i003 intr_per_s=0 1459255626665255145\ninterrupts,device=i004 intr_per_s=0 1459255626665281776\ninterrupts,device=i006 intr_per_s=0 1459255626665297416\ninterrupts,device=i007 intr_per_s=0 1459255626665321008\ninterrupts,device=i010 intr_per_s=0 1459255626665339413\ninterrupts,device=i012 intr_per_s=0 1459255626665361510\ninterrupts,device=i013 intr_per_s=0 1459255626665381327\ninterrupts,device=i015 intr_per_s=1 1459255626665397313\ninterrupts,device=i001 intr_per_s=0.25 1459255626665412985\ninterrupts,device=i002 intr_per_s=0 1459255626665430475\ninterrupts,device=i005 intr_per_s=0 1459255626665453944\ninterrupts,device=i008 intr_per_s=0 1459255626665470650\ninterrupts,device=i011 intr_per_s=0 1459255626665486069\ninterrupts,device=i009 intr_per_s=0 1459255626665502913\ninterrupts,device=i014 intr_per_s=0 1459255626665518152\ntask cswch_per_s=722.25,proc_per_s=0 1459255626665849646\ncpu,device=all pct_idle=98.85,pct_iowait=0,pct_nice=0.38,pct_steal=0,pct_system=0.64,pct_user=0.13 1459255626666639715\nmem bufpg_per_s=0,campg_per_s=1.75,frmpg_per_s=-8.25 1459255626666770205\nswap_util kbswpcad=0,kbswpfree=1048572,kbswpused=0,pct_swpcad=0,pct_swpused=0 1459255626667313276\n```\n\n----------------------------------------\n\nTITLE: Viewing Specific Player Scores Command (Shell Script)\nDESCRIPTION: This shell command retrieves and displays the current scores of a specified player from the Minecraft scoreboard, helping to ensure the plugin is functioning correctly.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/minecraft/README.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n\"/scoreboard players list Etho\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Secret Store Plugin for Telegraf (Go)\nDESCRIPTION: This code snippet shows the implementation of a secret store plugin for Telegraf. It includes the necessary methods to conform to the telegraf.SecretStore interface, such as Get, Set, List, and GetResolver. The plugin also demonstrates how to register itself using the secretstores.Add function.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/SECRETSTORES.md#2025-04-16_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n//go:generate ../../../tools/readme_config_includer/generator\npackage main\n\nimport (\n    _ \"embed\"\n    \"errors\"\n\n    \"github.com/influxdata/telegraf\"\n    \"github.com/influxdata/telegraf/plugins/secretstores\"\n)\n\n//go:embed sample.conf\nvar sampleConfig string\n\ntype Printer struct {\n    Log telegraf.Logger `toml:\"-\"`\n\n    cache map[string]string\n}\n\nfunc (p *Printer) SampleConfig() string {\n    return sampleConfig\n}\n\nfunc (p *Printer) Init() error {\n    return nil\n}\n\n// Get searches for the given key and return the secret\nfunc (p *Printer) Get(key string) ([]byte, error) {\n    v, found := p.cache[key]\n    if !found {\n        return nil, errors.New(\"not found\")\n    }\n\n    return []byte(v), nil\n}\n\n// Set sets the given secret for the given key\nfunc (p *Printer) Set(key, value string) error {\n    p.cache[key] = value\n    return nil\n}\n\n// List lists all known secret keys\nfunc (p *Printer) List() ([]string, error) {\n    keys := make([]string, 0, len(p.cache))\n    for k := range p.cache {\n        keys = append(keys, k)\n    }\n    return keys, nil\n}\n\n// GetResolver returns a function to resolve the given key.\nfunc (p *Printer) GetResolver(key string) (telegraf.ResolveFunc, error) {\n    resolver := func() ([]byte, bool, error) {\n        s, err := p.Get(key)\n        return s, false, err\n    }\n    return resolver, nil\n}\n\n// Register the secret-store on load.\nfunc init() {\n    secretstores.Add(\"printer\", func(string) telegraf.SecretStore {\n        return &Printer{}\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Tags Example in Telegraf Template Processor\nDESCRIPTION: Example showing how to combine multiple tags to create a single tag named \"topic\" using the hostname and level tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = \"topic\"\n  template = '{{ .Tag \"hostname\" }}.{{ .Tag \"level\" }}'\n```\n\n----------------------------------------\n\nTITLE: Documenting Deprecated Metrics in Markdown\nDESCRIPTION: Shows how to mark metrics as deprecated in a README file using Markdown. This example indicates which field is deprecated and what replacement to use.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEPRECATION.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n- system\n  - fields:\n    - uptime_format (string, deprecated in 1.10: use `uptime` field)\n```\n\n----------------------------------------\n\nTITLE: Configuring Nebius Cloud Monitoring Output Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the Nebius Cloud Monitoring output plugin in Telegraf. It includes options for setting the timeout and API endpoint.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/nebius_cloud_monitoring/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send aggregated metrics to Nebius.Cloud Monitoring\n[[outputs.nebius_cloud_monitoring]]\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Nebius.Cloud monitoring API endpoint. Normally should not be changed\n  # endpoint = \"https://monitoring.api.il.nebius.cloud/monitoring/v2/data/write\"\n```\n\n----------------------------------------\n\nTITLE: CSV Key-Name-Value Example for Lookup Tables\nDESCRIPTION: A practical example of the CSV format with key-name-value pattern mapping hosts to location and rack information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\nxyzzy-green,location,eu-central,rack,C12-01\nxyzzy-red,location,us-west,rack,C01-42\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Individual Shard Statistics Schema\nDESCRIPTION: Comprehensive schema defining per-shard metrics including indexing, search, caching, segments, and resource utilization statistics. Includes tags for identification and boolean/numeric fields for various performance indicators.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_10\n\nLANGUAGE: markdown\nCODE:\n```\n- elasticsearch_indices_stats_shards\n  - tags:\n    - index_name\n    - node_name\n    - shard_name\n    - type\n  - fields:\n    - commit_generation (float)\n    - commit_num_docs (float)\n    - completion_size_in_bytes (float)\n    [...additional fields omitted for brevity...]\n    - warmer_total_time_in_millis (float)\n```\n\n----------------------------------------\n\nTITLE: Configuring Wildcard Expansion for Performance Counters\nDESCRIPTION: Enables using wildcards in instance and counter names, with support for partial wildcards on Windows Vista and newer\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_1\n\nLANGUAGE: TOML\nCODE:\n```\nUseWildcardsExpansion=true\n```\n\n----------------------------------------\n\nTITLE: OPC UA Node Metric with Additional DataType Field\nDESCRIPTION: Example output showing how the metric appears when 'DataType' is included in the Additional Metrics configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nopcua,id=ns\\=3;s\\=Temperature temp=79.0,Quality=\"OK (0x0)\",DataType=\"Float\" 1597820490000000000\n```\n\n----------------------------------------\n\nTITLE: Example Fluentd Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the plugin, including buffer statistics, retry counts, and various plugin-specific measurements with their associated tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fluentd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfluentd,host=T440s,plugin_id=object:9f748c,plugin_category=input,plugin_type=dummy buffer_total_queued_size=0,buffer_queue_length=0,retry_count=0 1492006105000000000\nfluentd,plugin_category=input,plugin_type=dummy,host=T440s,plugin_id=object:8da98c buffer_queue_length=0,retry_count=0,buffer_total_queued_size=0 1492006105000000000\nfluentd,plugin_id=object:820190,plugin_category=input,plugin_type=monitor_agent,host=T440s retry_count=0,buffer_total_queued_size=0,buffer_queue_length=0 1492006105000000000\nfluentd,plugin_id=object:c5e054,plugin_category=output,plugin_type=stdout,host=T440s buffer_queue_length=0,retry_count=0,buffer_total_queued_size=0 1492006105000000000\nfluentd,plugin_type=s3,host=T440s,plugin_id=object:bd7a90,plugin_category=output buffer_queue_length=0,retry_count=0,buffer_total_queued_size=0 1492006105000000000\nfluentd,plugin_id=output_td, plugin_category=output,plugin_type=tdlog, host=T440s buffer_available_buffer_space_ratios=100,buffer_queue_byte_size=0,buffer_queue_length=0,buffer_stage_byte_size=0,buffer_stage_length=0,buffer_total_queued_size=0,emit_count=0,emit_records=0,flush_time_count=0,retry_count=0,rollback_count=0,slow_flush_count=0,write_count=0 1651474085000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring HDDtemp Plugin in TOML\nDESCRIPTION: Configuration settings for the HDDtemp Telegraf plugin. Allows specifying the hddtemp daemon address and which devices to monitor. Default behavior monitors all detected disks.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/hddtemp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Monitor disks' temperatures using hddtemp\n[[inputs.hddtemp]]\n  ## By default, telegraf gathers temps data from all disks detected by the\n  ## hddtemp.\n  ##\n  ## Only collect temps from the selected disks.\n  ##\n  ## A * as the device name will return the temperature values of all disks.\n  ##\n  # address = \"127.0.0.1:7634\"\n  # devices = [\"sda\", \"*\"]\n```\n\n----------------------------------------\n\nTITLE: Retrieving SMART Information using smartctl\nDESCRIPTION: Command to retrieve detailed SMART information for a specific device using smartctl.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsmartctl --info --attributes --health -n <nocheck> --format=brief <device>\n```\n\n----------------------------------------\n\nTITLE: Logging IO Statistics in InfluxDB Line Protocol\nDESCRIPTION: This snippet logs recursive IO statistics for ECS containers to InfluxDB in line protocol format. It includes metrics such as bytes read and written, along with total IO operations. The context suggests it is part of a monitoring setup for container performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ecs/README.md#2025-04-16_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\necs_container_blkio,cluster=test,com.amazonaws.ecs.cluster=test,com.amazonaws.ecs.container-name=~internal~ecs~pause,com.amazonaws.ecs.task-arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a,com.amazonaws.ecs.task-definition-family=nginx,com.amazonaws.ecs.task-definition-version=2,device=total,family=nginx,host=c4b301d4a123,id=e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba,name=~internal~ecs~pause,revision=2,task_arn=arn:aws:ecs:aws-region-1:012345678901:task/a1234abc-a0a0-0a01-ab01-0abc012a0a0a io_serviced_recursive_async=0i,io_serviced_recursive_read=40i,io_serviced_recursive_sync=40i,io_serviced_recursive_write=0i,io_serviced_recursive_total=40i,io_service_bytes_recursive_read=3162112i,io_service_bytes_recursive_write=0i,io_service_bytes_recursive_async=0i,container_id=\"e6af031b91deb3136a2b7c42f262ed2ab554e2fe2736998c7d8edf4afe708dba\",io_service_bytes_recursive_sync=3162112i,io_service_bytes_recursive_total=3162112i 1542642001000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring WMI Query for Bitlocker Metrics in Telegraf\nDESCRIPTION: This snippet configures a WMI query to collect Bitlocker encryption status for eligible volumes. It uses the MBAM_Volume class and includes the volume name as a tag. The ExcludeNameKey configuration is mentioned due to the absence of a Name property.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    namespace = \"root\\\\Microsoft\\\\MBAM\"\n    class_name = \"MBAM_Volume\"\n    properties = [\n      \"Compliant\",\n      \"VolumeName\"\n    ]\n    tag_properties = [\"VolumeName\"]\n```\n\n----------------------------------------\n\nTITLE: Example Riak Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the Riak plugin, including CPU usage, memory statistics, node operations, and various timing measurements. The output is in InfluxDB line protocol format with tags for server and nodename.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/riak/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nriak,nodename=riak@127.0.0.1,server=localhost:8098 cpu_avg1=31i,cpu_avg15=69i,cpu_avg5=51i,memory_code=11563738i,memory_ets=5925872i,memory_processes=30236069i,memory_system=93074971i,memory_total=123311040i,node_get_fsm_objsize_100=0i,node_get_fsm_objsize_95=0i,node_get_fsm_objsize_99=0i,node_get_fsm_objsize_mean=0i,node_get_fsm_objsize_median=0i,node_get_fsm_siblings_100=0i,node_get_fsm_siblings_95=0i,node_get_fsm_siblings_99=0i,node_get_fsm_siblings_mean=0i,node_get_fsm_siblings_median=0i,node_get_fsm_time_100=0i,node_get_fsm_time_95=0i,node_get_fsm_time_99=0i,node_get_fsm_time_mean=0i,node_get_fsm_time_median=0i,node_gets=0i,node_gets_total=19i,node_put_fsm_time_100=0i,node_put_fsm_time_95=0i,node_put_fsm_time_99=0i,node_put_fsm_time_mean=0i,node_put_fsm_time_median=0i,node_puts=0i,node_puts_total=0i,pbc_active=0i,pbc_connects=0i,pbc_connects_total=20i,vnode_gets=0i,vnode_gets_total=57i,vnode_index_reads=0i,vnode_index_reads_total=0i,vnode_index_writes=0i,vnode_index_writes_total=0i,vnode_puts=0i,vnode_puts_total=0i,read_repair=0i,read_repairs_total=0i 1455913392622482332\n```\n\n----------------------------------------\n\nTITLE: Example RFC5424 Syslog Message Format for Telegraf Processing\nDESCRIPTION: A sample syslog message in RFC5424 format containing web service request information. The message includes priority value (29), timestamp, hostname (web1), service name (someservice), process ID (2341), message ID (2), structured data, and the actual message content showing HTTP request details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_strict_unixtls/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Example WHOIS Output\nDESCRIPTION: This text shows an example of the output generated by the Telegraf WHOIS input plugin. Each line represents a domain and its corresponding WHOIS information, including tags for domain and status, and fields containing creation timestamp, DNSSEC enabled status, expiration timestamp, registrar, and updated timestamp. The output also includes an error tag when a domain is not found.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/whois/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nwhois,domain=example.com,status=unknown creation_timestamp=694224000i,dnssec_enabled=false,expiration_timestamp=0i,expiry=0i,name_servers=\"\",registrant=\"\",registrar=\"\",updated_timestamp=0i 1741128738000000000\nwhois,domain=influxdata.com,status=clientTransferProhibited creation_timestamp=1403603283i,dnssec_enabled=false,expiration_timestamp=1750758483i,expiry=9629744i,name_servers=\"ns-1200.awsdns-22.org,ns-127.awsdns-15.com,ns-2037.awsdns-62.co.uk,ns-820.awsdns-38.net\",registrant=\"\",registrar=\"NameCheap, Inc.\",updated_timestamp=1716620263i 1741128738000000000\nwhois,domain=influxdata-test.com,status=not\\ found error=\"whoisparser: domain is not found\" 1741128739000000000\n```\n\n----------------------------------------\n\nTITLE: OpenMetrics Input Example Format\nDESCRIPTION: Example of OpenMetrics text format input showing various metric types including summary, gauge, and counter metrics with labels and values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# TYPE acme_http_router_request_seconds summary\n# UNIT acme_http_router_request_seconds seconds\n# HELP acme_http_router_request_seconds Latency though all of ACME's HTTP request router.\nacme_http_router_request_seconds_sum{path=\"/api/v1\",method=\"GET\"} 9036.32\nacme_http_router_request_seconds_count{path=\"/api/v1\",method=\"GET\"} 807283.0\nacme_http_router_request_seconds{path=\"/api/v1\",method=\"GET\",quantile=\"0.5\"} 1.29854\nacme_http_router_request_seconds{path=\"/api/v1\",method=\"GET\",quantile=\"0.9\"} 54.85479\nacme_http_router_request_seconds{path=\"/api/v1\",method=\"GET\",quantile=\"0.99\"} 6884.32324\nacme_http_router_request_seconds_created{path=\"/api/v1\",method=\"GET\"} 1605281325.0\nacme_http_router_request_seconds_sum{path=\"/api/v2\",method=\"POST\"} 479.3\nacme_http_router_request_seconds_count{path=\"/api/v2\",method=\"POST\"} 34.0\nacme_http_router_request_seconds_created{path=\"/api/v2\",method=\"POST\"} 1605281325.0\nacme_http_router_request_seconds{path=\"/api/v2\",method=\"POST\",quantile=\"0.5\"} 0.85412\nacme_http_router_request_seconds{path=\"/api/v2\",method=\"POST\",quantile=\"0.9\"} 1.15429\nacme_http_router_request_seconds{path=\"/api/v2\",method=\"POST\",quantile=\"0.99\"} 3698.48132\n# TYPE go_goroutines gauge\n# HELP go_goroutines Number of goroutines that currently exist.\ngo_goroutines 69\n# TYPE process_cpu_seconds counter\n# UNIT process_cpu_seconds seconds\n# HELP process_cpu_seconds Total user and system CPU time spent in seconds.\nprocess_cpu_seconds_total 4.20072246e+06\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Parsing UserData XML in Windows Event Log for Telegraf\nDESCRIPTION: Demonstrates how UserData XML content is converted into additional fields by the Telegraf Windows Event Log plugin. The XML structure is flattened into key-value pairs, with underscores separating nested levels.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_eventlog/README.md#2025-04-16_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<UserData>\n <CbsPackageChangeState xmlns=\"http://manifests.microsoft.com/win/2004/08/windows/setup_provider\">\n  <PackageIdentifier>KB4566782</PackageIdentifier>\n  <IntendedPackageState>5112</IntendedPackageState>\n  <IntendedPackageStateTextized>Installed</IntendedPackageStateTextized>\n  <ErrorCode>0x0</ErrorCode>\n  <Client>UpdateAgentLCU</Client>\n </CbsPackageChangeState>\n</UserData>\n```\n\n----------------------------------------\n\nTITLE: Defining Compact Table Schema in JSON\nDESCRIPTION: This JSON snippet defines the schema for the compact table option in the BigQuery output plugin. It specifies the structure for storing all metrics in a single table with timestamp, name, tags, and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/bigquery/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"mode\": \"REQUIRED\",\n    \"name\": \"timestamp\",\n    \"type\": \"TIMESTAMP\"\n  },\n  {\n    \"mode\": \"REQUIRED\",\n    \"name\": \"name\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"mode\": \"REQUIRED\",\n    \"name\": \"tags\",\n    \"type\": \"JSON\"\n  },\n  {\n    \"mode\": \"REQUIRED\",\n    \"name\": \"fields\",\n    \"type\": \"JSON\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Maximum TX Latency\nDESCRIPTION: InfluxDB SQL query to retrieve maximum transmission latency for all interfaces over the last hour, grouped by time, hostname, and interface name.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lanz/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT max(\"tx_latency\") AS \"max_tx_latency\" FROM \"congestion_record\" WHERE time > now() - 1h GROUP BY time(10s), \"hostname\", \"intf_name\"\n```\n\n----------------------------------------\n\nTITLE: Example Mock Data Output for Telegraf Input Plugin\nDESCRIPTION: Sample output demonstrating the mock data generation with multiple algorithms and tags, showing time-series metric generation\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mock/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmock_sensors,building=5A,site=FTC random=4.875966794516125,abc=50,wave=0,plus_one=0 1632170840000000000\nmock_sensors,building=5A,site=FTC random=5.738651873834452,abc=45.095549448434774,wave=5.877852522924732,plus_one=1 1632170850000000000\nmock_sensors,building=5A,site=FTC random=1.0429328917205203,abc=51.928560083072924,wave=9.510565162951535,plus_one=2 1632170860000000000\nmock_sensors,building=5A,site=FTC random=5.290188595384418,abc=44.41090520217027,wave=9.510565162951536,plus_one=3 1632170870000000000\nmock_sensors,building=5A,site=FTC random=2.0724967227069135,abc=47.212167806890314,wave=5.877852522924733,plus_one=4 1632170880000000000\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Log Entry Example in Plaintext\nDESCRIPTION: A sample log entry formatted according to RFC5424 standard (structured syslog format). The entry starts with a priority value '<1>' followed by version '3' and then empty fields denoted by hyphens, with a simple 'hello world' message spanning two lines.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcp_1st_newline_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n28 <1>3 - - - - - - hello\nworld\n```\n\n----------------------------------------\n\nTITLE: Executing Integration Tests in Telegraf\nDESCRIPTION: Commands for running integration tests, including options for running only integration tests or the full test suite.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake test-integration\n```\n\n----------------------------------------\n\nTITLE: Sample SQL Query for Synproxy Metrics\nDESCRIPTION: This SQL query retrieves the difference in various synproxy metrics over a specified time period from InfluxDB, allowing users to analyze fluctuations in SYN packet statistics over the last hour at 5-minute intervals.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/synproxy/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT difference(last(\"cookie_invalid\")) AS \"cookie_invalid\", difference(last(\"cookie_retrans\")) AS \"cookie_retrans\", difference(last(\"cookie_valid\")) AS \"cookie_valid\", difference(last(\"entries\")) AS \"entries\", difference(last(\"syn_received\")) AS \"syn_received\", difference(last(\"conn_reopened\")) AS \"conn_reopened\" FROM synproxy WHERE time > NOW() - 1h GROUP BY time(5m) FILL(null);\n```\n\n----------------------------------------\n\nTITLE: CSV Key-Name-Value Format Example for Lookup Tables\nDESCRIPTION: Example of the CSV format with key-name-value pattern. Each line contains a key followed by pairs of tag names and values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\n# Optional comments\nkeyA,tag-name1,tag-value1,...,tag-nameN,tag-valueN\nkeyB,tag-name1,tag-value1\n...\nkeyZ,tag-name1,tag-value1,...,tag-nameM,tag-valueM\n```\n\n----------------------------------------\n\nTITLE: Defining a Batch Template in Telegraf\nDESCRIPTION: This code snippet provides an example of a `batch_template` configuration for Telegraf's `template` data format.  It illustrates how to format multiple metrics within a batch.  The template iterates through the metrics, extracting and displaying the name of each metric.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/template/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nbatch_template = '''My batch metric names: {{range $index, $metric := . -}}\n{{if $index}}, {{ end }}{{ $metric.Name }}\n{{- end }}'''\n```\n\n----------------------------------------\n\nTITLE: Importing External Plugin in main.go\nDESCRIPTION: Example of how to import an external plugin in the main.go file. This is a required step to make the plugin functional when run as a stand-alone program with the shim.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/EXTERNAL_PLUGINS.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n_ \"github.com/me/my-plugin-telegraf/plugins/inputs/cpu\"\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Starlark Using catch\nDESCRIPTION: Handling potential errors in Starlark scripts using the built-in catch function\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/starlark/README.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nload(\"json.star\", \"json\")\n\ndef apply(metric):\n    error = catch(lambda: failing(metric))\n    if error != None:\n        metric.fields[\"error\"] = error\n    return metric\n\ndef failing(metric):\n    json.decode(\"non-json-content\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Top-Level SNMP Field with snmpget\nDESCRIPTION: Shell command demonstrating how to request a top-level SNMP field using the snmpget utility. This can be used to verify SNMP connectivity and access to specific OIDs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nsnmpget -v2c -c public 127.0.0.1 sysUpTime.0\n```\n\n----------------------------------------\n\nTITLE: Adding All Fields as a Single Tag in Telegraf Template Processor\nDESCRIPTION: Simple example showing how to add all fields with their values into a single tag called \"message\".\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = \"message\"\n  template = 'Message about {{.Name}} fields: {{.Fields}}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Input Plugins in Telegraf\nDESCRIPTION: This snippet shows how to configure input plugins in Telegraf. It includes examples for various input plugins such as CPU, disk, diskio, kernel, memory, processes, and swap, demonstrating how to set collection intervals and specify metrics to collect.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/migrations/inputs_jolokia/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n  ## the device. The template may contain variables in the form of '$PROPERTY' or\n  ## '${PROPERTY}'. The first template which does not contain any variables not\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n# Read metrics about memory usage\n[[inputs.mem]]\n  # no configuration\n\n# Read metrics about swap memory usage\n[[inputs.swap]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: JSON Format Example for Lookup Tables\nDESCRIPTION: Example of the JSON format used for lookup tables. Shows the structure with keys mapping to objects containing tag name-value pairs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"keyA\": {\n    \"tag-name1\": \"tag-value1\",\n    ...\n    \"tag-nameN\": \"tag-valueN\",\n  },\n  ...\n  \"keyZ\": {\n    \"tag-name1\": \"tag-value1\",\n    ...\n    \"tag-nameM\": \"tag-valueM\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Graphite Parser in Telegraf using TOML\nDESCRIPTION: Configuration example for the Graphite parser plugin showing how to set up command execution, measurement naming, and template patterns. Includes options for custom separators and various template formats for transforming Graphite data into Telegraf metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/graphite/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.exec]]\n  ## Commands array\n  commands = [\"/tmp/test.sh\", \"/usr/bin/mycollector --foo=bar\"]\n\n  ## measurement name suffix (for separating different commands)\n  name_suffix = \"_mycollector\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"graphite\"\n\n  ## This string will be used to join the matched values.\n  separator = \"_\"\n\n  ## Each template line requires a template pattern. It can have an optional\n  ## filter before the template and separated by spaces. It can also have optional extra\n  ## tags following the template. Multiple tags should be separated by commas and no spaces\n  ## similar to the line protocol format. There can be only one default template.\n  ## Templates support below format:\n  ## 1. filter + template\n  ## 2. filter + template + extra tag(s)\n  ## 3. filter + template with field key\n  ## 4. default template\n  templates = [\n    \"*.app env.service.resource.measurement\",\n    \"stats.* .host.measurement* region=eu-east,agent=sensu\",\n    \"stats2.* .host.measurement.field\",\n    \"measurement*\"\n  ]\n```\n\n----------------------------------------\n\nTITLE: WMI Query for Processor Information\nDESCRIPTION: Configuration for retrieving processor details including number of cores and processor name as a tag\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.win_wmi]]\n  name_prefix = \"win_wmi_\"\n  [[inputs.win_wmi.query]]\n    namespace = \"root\\\\cimv2\"\n    class_name = \"Win32_Processor\"\n    properties = [\"Name\",\"NumberOfCores\"]\n    tag_properties = [\"Name\"]\n```\n\n----------------------------------------\n\nTITLE: Example Output for NSDP Plugin\nDESCRIPTION: This snippet demonstrates the format of metrics collected by the NSDP plugin. It shows a sample line protocol output with various tags identifying the device and port, along with numeric field values for network statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsdp/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnsdp_device_port,device=12:34:56:78:9a:bc,device_ip=10.1.0.4,device_model=GS108Ev3,device_name=switch2,device_port=1 broadcasts_total=0u,bytes_recv=3879427866u,bytes_sent=506548796u,errors_total=0u,multicasts_total=0u,packets_total=0u 1737152505014578000\n```\n\n----------------------------------------\n\nTITLE: Configuring Tag Limit Processor in TOML\nDESCRIPTION: This code snippet demonstrates the configuration of the 'tag_limit' processor in TOML format. It sets a limit on the number of tags preserved and specifies which tags should be preferentially preserved. The 'limit' parameter defines the maximum number of tags, while the 'keep' list specifies tags to prioritize. This configuration is typically included in the Telegraf configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/tag_limit/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n# Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.\\n[[processors.tag_limit]]\\n  ## Maximum number of tags to preserve\\n  limit = 3\\n\\n  ## List of tags to preferentially preserve\\n  keep = [\"environment\", \"region\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring NTP Query Plugin in TOML\nDESCRIPTION: Configuration settings for the NTP query plugin including server specification, DNS lookup options, and reach format settings. The configuration allows customization of ntpq command options and output formatting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Get standard NTP query metrics, requires ntpq executable.\n[[inputs.ntpq]]\n  ## Servers to query with ntpq.\n  ## If no server is given, the local machine is queried.\n  # servers = []\n\n  ## If false, set the -n ntpq flag. Can reduce metric gather time.\n  ## DEPRECATED since 1.24.0: add '-n' to 'options' instead to skip DNS lookup\n  # dns_lookup = true\n\n  ## Options to pass to the ntpq command.\n  # options = \"-p\"\n\n  ## Output format for the 'reach' field.\n  ## Available values are\n  ##   octal   --  output as is in octal representation e.g. 377 (default)\n  ##   decimal --  convert value to decimal representation e.g. 371 -> 249\n  ##   count   --  count the number of bits in the value. This represents\n  ##               the number of successful reaches, e.g. 37 -> 5\n  ##   ratio   --  output the ratio of successful attempts e.g. 37 -> 5/8 = 0.625\n  # reach_format = \"octal\"\n```\n\n----------------------------------------\n\nTITLE: Basic Template Pattern Syntax Example\nDESCRIPTION: Shows the basic form of a template pattern used to map Graphite data to Telegraf metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"host.mytag.mytag.measurement.measurement.field*\"\n```\n\n----------------------------------------\n\nTITLE: Host System Performance Metrics Catalog\nDESCRIPTION: Detailed performance metrics for monitoring host system resources, covering CPU, memory, network, power, and virtual machine operations\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/METRICS.md#2025-04-16_snippet_2\n\nLANGUAGE: metrics\nCODE:\n```\ncpu.corecount.contention.average\\ncpu.usage.average\\ncpu.reservedCapacity.average\n```\n\n----------------------------------------\n\nTITLE: Example Radius Metric Output\nDESCRIPTION: Shows the format of metrics output by the Radius plugin, including tags for response code, source, and source port, along with the response time measurement in milliseconds.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/radius/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nradius,response_code=Access-Accept,source=hostname.com,source_port=1812 responsetime_ms=311i 1677526200000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring BIND Daemon Statistics Channel\nDESCRIPTION: JSON configuration snippet for enabling statistics collection in the BIND daemon by configuring the statistics channel.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bind/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nstatistics-channels {\n    inet 127.0.0.1 port 8053;\n};\n```\n\n----------------------------------------\n\nTITLE: SSH Configuration for Remote Varnish Monitoring\nDESCRIPTION: TOML configuration for monitoring Varnish instances over SSH using custom binary arguments\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.varnish]]\n  binary = \"/usr/bin/ssh\"\n  binary_args = [\"root@10.100.0.112\", \"varnishstat\", \"-n\", \"/var/lib/varnish/ubuntu\", \"-j\"]\n  adm_binary   =  \"/usr/bin/ssh\"\n  adm_binary_args = [\"root@10.100.0.112\", \"varnishadm\", \"-n\", \"/var/lib/varnish/ubuntu\", \"vcl.list\", \"-j\"]\n  metric_version = 2\n  stats = [\"*\"]\n```\n\n----------------------------------------\n\nTITLE: Displaying Netflow v5 Sample Data\nDESCRIPTION: This snippet shows sample data output for Netflow v5 protocol. It includes network flow information such as source and destination IP addresses, ports, protocol, bytes transferred, and various other metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netflow/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"140.82.121.3\",src_port=443u,dst=\"192.168.119.100\",dst_port=55516u,flows=8u,in_bytes=87477u,in_packets=78u,first_switched=86400660u,last_switched=86403316u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"140.82.121.6\",src_port=443u,dst=\"192.168.119.100\",dst_port=36408u,flows=8u,in_bytes=5009u,in_packets=21u,first_switched=86400447u,last_switched=86403267u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"140.82.112.22\",src_port=443u,dst=\"192.168.119.100\",dst_port=39638u,flows=8u,in_bytes=925u,in_packets=6u,first_switched=86400324u,last_switched=86403214u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"140.82.114.26\",src_port=443u,dst=\"192.168.119.100\",dst_port=49398u,flows=8u,in_bytes=250u,in_packets=2u,first_switched=86403131u,last_switched=86403362u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"192.168.119.100\",src_port=55516u,dst=\"140.82.121.3\",dst_port=443u,flows=8u,in_bytes=4969u,in_packets=37u,first_switched=86400652u,last_switched=86403269u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"192.168.119.100\",src_port=36408u,dst=\"140.82.121.6\",dst_port=443u,flows=8u,in_bytes=2736u,in_packets=21u,first_switched=86400438u,last_switched=86403258u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"192.168.119.100\",src_port=39638u,dst=\"140.82.112.22\",dst_port=443u,flows=8u,in_bytes=1560u,in_packets=6u,first_switched=86400225u,last_switched=86403255u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\nnetflow,source=127.0.0.1,version=NetFlowV5 protocol=\"tcp\",src=\"192.168.119.100\",src_port=49398u,dst=\"140.82.114.26\",dst_port=443u,flows=8u,in_bytes=697u,in_packets=4u,first_switched=86403030u,last_switched=86403362u,tcp_flags=\"...PA...\",engine_type=\"19\",engine_id=\"0x56\",sys_uptime=90003000u,src_tos=\"0x00\",bgp_src_as=0u,bgp_dst_as=0u,src_mask=0u,dst_mask=0u,in_snmp=0u,out_snmp=0u,next_hop=\"0.0.0.0\",seq_number=0u,sampling_interval=0u\n```\n\n----------------------------------------\n\nTITLE: Sample Output for SQL Server WMI Query in Telegraf\nDESCRIPTION: This snippet demonstrates the output format for the SQL Server WMI query configuration. It shows two metrics, one for the file version and another for the SKU name, with relevant properties included as tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nwin_wmi_SqlServiceAdvancedProperty,PropertyName=FILEVERSION,PropertyStrValue=2019.150.4178.1,ServiceName=MSSQLSERVER,host=foo,sqlinstance=foo SqlServiceType=1i 1654269272000000000\nwin_wmi_SqlServiceAdvancedProperty,PropertyName=SKUNAME,PropertyStrValue=Developer\\ Edition\\ (64-bit),ServiceName=MSSQLSERVER,host=foo,sqlinstance=foo SqlServiceType=1i 1654269272000000000\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Linux CPU Metrics (Text)\nDESCRIPTION: This snippet provides example output format of the Linux CPU metrics as emitted by the Telegraf plugin, including CPU identification and scaled frequencies, timestamped for monitoring. Each line represents metrics for a specific CPU core with detailed values for scaling frequencies and throttling statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/linux_cpu/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"linux_cpu,cpu=0,host=go scaling_max_freq=4700000i,cpuinfo_min_freq=400000i,cpuinfo_max_freq=4700000i,throttle_count=0i,throttle_max_time=0i,throttle_total_time=0i,scaling_cur_freq=803157i,scaling_min_freq=400000i 1617621150000000000\\nlINUX_CPU,cpu=1,host=go throttle_total_time=0i,scaling_cur_freq=802939i,scaling_min_freq=400000i,scaling_max_freq=4700000i,cpuinfo_min_freq=400000i,cpuinfo_max_freq=4700000i,throttle_count=0i,throttle_max_time=0i 1617621150000000000\\nlINUX_CPU,cpu=10,host=go throttle_max_time=0i,throttle_total_time=0i,scaling_cur_freq=838343i,scaling_min_freq=400000i,scaling_max_freq=4700000i,cpuinfo_min_freq=400000i,cpuinfo_max_freq=4700000i,throttle_count=0i 1617621150000000000\\nlINUX_CPU,cpu=11,host=go cpuinfo_max_freq=4700000i,throttle_count=0i,throttle_max_time=0i,throttle_total_time=0i,scaling_cur_freq=800054i,scaling_min_freq=400000i,scaling_max_freq=4700000i,cpuinfo_min_freq=400000i 1617621150000000000\\nlINUX_CPU,cpu=2,host=go throttle_total_time=0i,scaling_cur_freq=800404i,scaling_min_freq=400000i,scaling_max_freq=4700000i,cpuinfo_min_freq=400000i,cpuinfo_max_freq=4700000i,throttle_count=0i,throttle_max_time=0i 1617621150000000000\\n... (more lines omitted)\"\n```\n\n----------------------------------------\n\nTITLE: Document Count Query Configuration\nDESCRIPTION: Configuration for counting documents matching a filter query across all indices.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch_query/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.elasticsearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"*\"\n  filter_query = \"product_1 AND HEAD\"\n  query_period = \"1m\"\n  date_field = \"@timestamp\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Aerospike Metrics in Telegraf Output Format\nDESCRIPTION: Sample output from Telegraf when collecting Aerospike metrics. The output includes detailed statistics for nodes, namespaces, sets, and TTL histograms with metrics like batch operations, memory usage, object counts, and histogram distributions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/aerospike/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\naerospike_node,aerospike_host=localhost:3000,node_name=\"BB9020011AC4202\" batch_error=0i,batch_index_complete=0i,batch_index_created_buffers=0i,batch_index_destroyed_buffers=0i,batch_index_error=0i,batch_index_huge_buffers=0i,batch_index_initiate=0i,batch_index_queue=\"0:0,0:0,0:0,0:0\",batch_index_timeout=0i,batch_index_unused_buffers=0i,batch_initiate=0i,batch_queue=0i,batch_timeout=0i,client_connections=6i,cluster_integrity=true,cluster_key=\"8AF422E05281249E\",cluster_size=1i,delete_queue=0i,demarshal_error=0i,early_tsvc_batch_sub_error=0i,early_tsvc_client_error=0i,early_tsvc_udf_sub_error=0i,fabric_connections=16i,fabric_msgs_rcvd=0i,fabric_msgs_sent=0i,heartbeat_connections=0i,heartbeat_received_foreign=0i,heartbeat_received_self=0i,info_complete=47i,info_queue=0i,migrate_allowed=true,migrate_partitions_remaining=0i,migrate_progress_recv=0i,migrate_progress_send=0i,objects=0i,paxos_principal=\"BB9020011AC4202\",proxy_in_progress=0i,proxy_retry=0i,query_long_running=0i,query_short_running=0i,reaped_fds=0i,record_refs=0i,rw_in_progress=0i,scans_active=0i,sindex_gc_activity_dur=0i,sindex_gc_garbage_cleaned=0i,sindex_gc_garbage_found=0i,sindex_gc_inactivity_dur=0i,sindex_gc_list_creation_time=0i,sindex_gc_list_deletion_time=0i,sindex_gc_locktimedout=0i,sindex_gc_objects_validated=0i,sindex_ucgarbage_found=0i,sub_objects=0i,system_free_mem_pct=92i,system_swapping=false,tsvc_queue=0i,uptime=1457i 1468923222000000000\naeropike_namespace,aerospike_host=localhost:3000,namespace=test,node_name=\"BB9020011AC4202\" allow_nonxdr_writes=true,allow_xdr_writes=true,available_bin_names=32768i,batch_sub_proxy_complete=0i,batch_sub_proxy_error=0i,batch_sub_proxy_timeout=0i,batch_sub_read_error=0i,batch_sub_read_not_found=0i,batch_sub_read_success=0i,batch_sub_read_timeout=0i,batch_sub_tsvc_error=0i,batch_sub_tsvc_timeout=0i,client_delete_error=0i,client_delete_not_found=0i,client_delete_success=0i,client_delete_timeout=0i,client_lang_delete_success=0i,client_lang_error=0i,client_lang_read_success=0i,client_lang_write_success=0i,client_proxy_complete=0i,client_proxy_error=0i,client_proxy_timeout=0i,client_read_error=0i,client_read_not_found=0i,client_read_success=0i,client_read_timeout=0i,client_tsvc_error=0i,client_tsvc_timeout=0i,client_udf_complete=0i,client_udf_error=0i,client_udf_timeout=0i,client_write_error=0i,client_write_success=0i,client_write_timeout=0i,cold_start_evict_ttl=4294967295i,conflict_resolution_policy=\"generation\",current_time=206619222i,data_in_index=false,default_ttl=432000i,device_available_pct=99i,device_free_pct=100i,device_total_bytes=4294967296i,device_used_bytes=0i,disallow_null_setname=false,enable_benchmarks_batch_sub=false,enable_benchmarks_read=false,enable_benchmarks_storage=false,enable_benchmarks_udf=false,enable_benchmarks_udf_sub=false,enable_benchmarks_write=false,enable_hist_proxy=false,enable_xdr=false,evict_hist_buckets=10000i,evict_tenths_pct=5i,evict_ttl=0i,evicted_objects=0i,expired_objects=0i,fail_generation=0i,fail_key_busy=0i,fail_record_too_big=0i,fail_xdr_forbidden=0i,geo2dsphere_within.earth_radius_meters=6371000i,geo2dsphere_within.level_mod=1i,geo2dsphere_within.max_cells=12i,geo2dsphere_within.max_level=30i,geo2dsphere_within.min_level=1i,geo2dsphere_within.strict=true,geo_region_query_cells=0i,geo_region_query_falsepos=0i,geo_region_query_points=0i,geo_region_query_reqs=0i,high_water_disk_pct=50i,high_water_memory_pct=60i,hwm_breached=false,ldt_enabled=false,ldt_gc_rate=0i,ldt_page_size=8192i,master_objects=0i,master_sub_objects=0i,max_ttl=315360000i,max_void_time=0i,memory_free_pct=100i,memory_size=1073741824i,memory_used_bytes=0i,memory_used_data_bytes=0i,memory_used_index_bytes=0i,memory_used_sindex_bytes=0i,migrate_order=5i,migrate_record_receives=0i,migrate_record_retransmits=0i,migrate_records_skipped=0i,migrate_records_transmitted=0i,migrate_rx_instances=0i,migrate_rx_partitions_active=0i,migrate_rx_partitions_initial=0i,migrate_rx_partitions_remaining=0i,migrate_sleep=1i,migrate_tx_instances=0i,migrate_tx_partitions_active=0i,migrate_tx_partitions_imbalance=0i,migrate_tx_partitions_initial=0i,migrate_tx_partitions_remaining=0i,non_expirable_objects=0i,ns_forward_xdr_writes=false,nsup_cycle_duration=0i,nsup_cycle_sleep_pct=0i,objects=0i,prole_objects=0i,prole_sub_objects=0i,query_agg=0i,query_agg_abort=0i,query_agg_avg_rec_count=0i,query_agg_error=0i,query_agg_success=0i,query_fail=0i,query_long_queue_full=0i,query_long_reqs=0i,query_lookup_abort=0i,query_lookup_avg_rec_count=0i,query_lookup_error=0i,query_lookup_success=0i,query_lookups=0i,query_reqs=0i,query_short_queue_full=0i,query_short_reqs=0i,query_udf_bg_failure=0i,query_udf_bg_success=0i,read_consistency_level_override=\"off\",repl_factor=1i,scan_aggr_abort=0i,scan_aggr_complete=0i,scan_aggr_error=0i,scan_basic_abort=0i,scan_basic_complete=0i,scan_basic_error=0i,scan_udf_bg_abort=0i,scan_udf_bg_complete=0i,scan_udf_bg_error=0i,set_deleted_objects=0i,sets_enable_xdr=true,sindex.data_max_memory=\"ULONG_MAX\",sindex.num_partitions=32i,single_bin=false,stop_writes=false,stop_writes_pct=90i,storage_engine=\"device\",storage_engine.cold_start_empty=false,storage_engine.data_in_memory=true,storage_engine.defrag_lwm_pct=50i,storage_engine.defrag_queue_min=0i,storage_engine.defrag_sleep=1000i,storage_engine.defrag_startup_minimum=10i,storage_engine.disable_odirect=false,storage_engine.enable_osync=false,storage_engine.file=\"/opt/aerospike/data/test.dat\",storage_engine.filesize=4294967296i,storage_engine.flush_max_ms=1000i,storage_engine.fsync_max_sec=0i,storage_engine.max_write_cache=67108864i,storage_engine.min_avail_pct=5i,storage_engine.post_write_queue=0i,storage_engine.scheduler_mode=\"null\",storage_engine.write_block_size=1048576i,storage_engine.write_threads=1i,sub_objects=0i,udf_sub_lang_delete_success=0i,udf_sub_lang_error=0i,udf_sub_lang_read_success=0i,udf_sub_lang_write_success=0i,udf_sub_tsvc_error=0i,udf_sub_tsvc_timeout=0i,udf_sub_udf_complete=0i,udf_sub_udf_error=0i,udf_sub_udf_timeout=0i,write_commit_level_override=\"off\",xdr_write_error=0i,xdr_write_success=0i,xdr_write_timeout=0i,{test}_query_hist_track_back=300i,{test}_query_hist_track_slice=10i,{test}_query_hist_track_thresholds=\"1,8,64\",{test}_read_hist_track_back=300i,{test}_read_hist_track_slice=10i,{test}_read_hist_track_thresholds=\"1,8,64\",{test}_udf_hist_track_back=300i,{test}_udf_hist_track_slice=10i,{test}_udf_hist_track_thresholds=\"1,8,64\",{test}_write_hist_track_back=300i,{test}_write_hist_track_slice=10i,{test}_write_hist_track_thresholds=\"1,8,64\" 1468923222000000000\naeropike_set,aerospike_host=localhost:3000,node_name=BB99458B42826B0,set=test/test disable_eviction=false,memory_data_bytes=0i,objects=0i,set_enable_xdr=\"use-default\",stop_writes_count=0i,tombstones=0i,truncate_lut=0i 1598033805000000000\naeropike_histogram_ttl,aerospike_host=localhost:3000,namespace=test,node_name=BB98EE5B42826B0,set=test 0=0i,1=0i,10=0i,11=0i,12=0i,13=0i,14=0i,15=0i,16=0i,17=0i,18=0i,19=0i,2=0i,20=0i,21=0i,22=0i,23=0i,24=0i,25=0i,26=0i,27=0i,28=0i,29=0i,3=0i,30=0i,31=0i,32=0i,33=0i,34=0i,35=0i,36=0i,37=0i,38=0i,39=0i,4=0i,40=0i,41=0i,42=0i,43=0i,44=0i,45=0i,46=0i,47=0i,48=0i,49=0i,5=0i,50=0i,51=0i,52=0i,53=0i,54=0i,55=0i,56=0i,57=0i,58=0i,59=0i,6=0i,60=0i,61=0i,62=0i,63=0i,64=0i,65=0i,66=0i,67=0i,68=0i,69=0i,7=0i,70=0i,71=0i,72=0i,73=0i,74=0i,75=0i,76=0i,77=0i,78=0i,79=0i,8=0i,80=0i,81=0i,82=0i,83=0i,84=0i,85=0i,86=0i,87=0i,88=0i,89=0i,9=0i,90=0i,91=0i,92=0i,93=0i,94=0i,95=0i,96=0i,97=0i,98=0i,99=0i 1598034191000000000\n```\n\n----------------------------------------\n\nTITLE: Resulting MQTT Topics and Values for Homie v4 Layout in Telegraf\nDESCRIPTION: Example of the MQTT topics and their corresponding values after processing the sample metrics through the Homie v4 layout. This shows the hierarchical structure and metadata topics as per the Homie specification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mqtt/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ntelegraf/modbus/$homie                            4.0\ntelegraf/modbus/$name                             modbus plugin\ntelegraf/modbus/$state                            ready\ntelegraf/modbus/$nodes                            device-1\n\ntelegraf/modbus/device-1/$name                    device 1\ntelegraf/modbus/device-1/$properties              location,serial-number,source,status,supplied,temperature,type,working-hours\n\ntelegraf/modbus/device-1/location                 main building\ntelegraf/modbus/device-1/location/$name           location\ntelegraf/modbus/device-1/location/$datatype       string\ntelegraf/modbus/device-1/status                   ok\ntelegraf/modbus/device-1/status/$name             status\ntelegraf/modbus/device-1/status/$datatype         string\ntelegraf/modbus/device-1/type                     Machine A\ntelegraf/modbus/device-1/type/$name               type\ntelegraf/modbus/device-1/type/$datatype           string\ntelegraf/modbus/device-1/source                   device 1\ntelegraf/modbus/device-1/source/$name             source\ntelegraf/modbus/device-1/source/$datatype         string\ntelegraf/modbus/device-1/temperature              21.4\ntelegraf/modbus/device-1/temperature/$name        temperature\ntelegraf/modbus/device-1/temperature/$datatype    float\ntelegraf/modbus/device-1/serial-number            324nlk234r5u9834t\ntelegraf/modbus/device-1/serial-number/$name      serial number\ntelegraf/modbus/device-1/serial-number/$datatype  string\ntelegraf/modbus/device-1/working-hours            123\ntelegraf/modbus/device-1/working-hours/$name      working hours\ntelegraf/modbus/device-1/working-hours/$datatype  integer\ntelegraf/modbus/device-1/supplied                 true\ntelegraf/modbus/device-1/supplied/$name           supplied\ntelegraf/modbus/device-1/supplied/$datatype       boolean\n\ntelegraf/modbus/$nodes                            device-1,device-2\n\ntelegraf/modbus/device-2/$name                    device 2\ntelegraf/modbus/device-2/$properties              location,source,status,supplied,type\n\ntelegraf/modbus/device-2/location                 main building\ntelegraf/modbus/device-2/location/$name           location\ntelegraf/modbus/device-2/location/$datatype       string\ntelegraf/modbus/device-2/status                   offline\ntelegraf/modbus/device-2/status/$name             status\ntelegraf/modbus/device-2/status/$datatype         string\ntelegraf/modbus/device-2/type                     Machine B\ntelegraf/modbus/device-2/type/$name               type\ntelegraf/modbus/device-2/type/$datatype           string\ntelegraf/modbus/device-2/source                   device 2\ntelegraf/modbus/device-2/source/$name             source\ntelegraf/modbus/device-2/source/$datatype         string\ntelegraf/modbus/device-2/supplied                 false\ntelegraf/modbus/device-2/supplied/$name           supplied\ntelegraf/modbus/device-2/supplied/$datatype       boolean\n\ntelegraf/modbus/device-2/$properties              account-no,current,load,location,source,status,supplied,temperature,throughput,type,voltage\n\ntelegraf/modbus/device-2/location                 main building\ntelegraf/modbus/device-2/location/$name           location\ntelegraf/modbus/device-2/location/$datatype       string\ntelegraf/modbus/device-2/status                   online\ntelegraf/modbus/device-2/status/$name             status\ntelegraf/modbus/device-2/status/$datatype         string\ntelegraf/modbus/device-2/type                     Machine B\ntelegraf/modbus/device-2/type/$name               type\ntelegraf/modbus/device-2/type/$datatype           string\ntelegraf/modbus/device-2/source                   device 2\ntelegraf/modbus/device-2/source/$name             source\ntelegraf/modbus/device-2/source/$datatype         string\ntelegraf/modbus/device-2/temperature              25.38\ntelegraf/modbus/device-2/temperature/$name        Temperature\ntelegraf/modbus/device-2/temperature/$datatype    float\ntelegraf/modbus/device-2/voltage                  24.1\ntelegraf/modbus/device-2/voltage/$name            Voltage\ntelegraf/modbus/device-2/voltage/$datatype        float\ntelegraf/modbus/device-2/current                  100\ntelegraf/modbus/device-2/current/$name            Current\ntelegraf/modbus/device-2/current/$datatype        float\ntelegraf/modbus/device-2/throughput               12345\ntelegraf/modbus/device-2/throughput/$name         Throughput\ntelegraf/modbus/device-2/throughput/$datatype     integer\ntelegraf/modbus/device-2/load                     81.2\ntelegraf/modbus/device-2/load/$name               Load [%]\ntelegraf/modbus/device-2/load/$datatype           float\ntelegraf/modbus/device-2/account-no               T3L3GrAf\ntelegraf/modbus/device-2/account-no/$name         account no\ntelegraf/modbus/device-2/account-no/$datatype     string\ntelegraf/modbus/device-2/supplied                 true\ntelegraf/modbus/device-2/supplied/$name           supplied\ntelegraf/modbus/device-2/supplied/$datatype       boolean\n```\n\n----------------------------------------\n\nTITLE: Overriding HEC Index Tag in Telegraf\nDESCRIPTION: This TOML configuration demonstrates how to override the default HEC index by adding a tag within an input configuration. In this case, the index is overridden specifically for the CPU metric.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  [inputs.cpu.tags]\n    index = \"cpu_metrics\"\n```\n\n----------------------------------------\n\nTITLE: Example output from the filestat input plugin\nDESCRIPTION: Sample output showing the metrics collected by the filestat plugin. It demonstrates both existing and non-existing files with their respective metrics like existence status, size, and modification time.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/filestat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfilestat,file=/tmp/foo/bar,host=tyrion exists=0i 1507218518192154351\nfilestat,file=/Users/sparrc/ws/telegraf.conf,host=tyrion exists=1i,size=47894i,modification_time=1507152973123456789i  1507218518192154351\n```\n\n----------------------------------------\n\nTITLE: Configuring UNIX Socket for Suricata Stats in YAML\nDESCRIPTION: This YAML configuration snippet is for the Suricata engine, enabling it to send stats to a JSON output through a UNIX socket. It demonstrates how to correctly set up Suricata's eve-log output.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/suricata/README.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n\"\"\"\n- eve-log:\n    enabled: yes\n    filetype: unix_stream\n    filename: /tmp/suricata-stats.sock\n    types:\n      - stats:\n         threads: yes\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Logging Socket Information in Go\nDESCRIPTION: Shows how to log a message with the address of a socket that a plugin is listening on.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/LOGGING.md#2025-04-16_snippet_3\n\nLANGUAGE: go\nCODE:\n```\np.log.InfoF(\"Listening on %s://%s\", protocol, l.Addr())\n```\n\n----------------------------------------\n\nTITLE: MongoDB Top Operation Statistics Structure\nDESCRIPTION: Defines metrics for tracking operation time and counts across different MongoDB operations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n- mongodb_top_stats\n  - tags:\n    - collection\n  - fields:\n    - total_time (integer)\n    - total_count (integer)\n    [...]\n```\n\n----------------------------------------\n\nTITLE: Chaining Secret-Stores with Systemd and JOSE\nDESCRIPTION: Configuration example showing how to chain secret-stores, using systemd secrets to unlock a JOSE secret-store that contains additional secrets, enabling unattended starts with many shared secrets.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[secretstores.systemd]]\n  id = \"systemd\"\n\n[[secretstores.jose]]\n  id = \"mysecrets\"\n  path = \"/etc/telegraf/secrets\"\n  password = \"@{systemd:initial}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Field Collection in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up a field for collecting a single SNMP variable by OID. It includes options for naming, tagging, and value conversion.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.snmp]]\n  # ... snip ...\n\n  [[inputs.snmp.field]]\n    ## Object identifier of the variable as a numeric or textual OID.\n    oid = \"RFC1213-MIB::sysName.0\"\n\n    ## Name of the field or tag to create.  If not specified, it defaults to\n    ## the value of 'oid'. If 'oid' is numeric, an attempt to translate the\n    ## numeric OID into a textual OID will be made.\n    # name = \"\"\n\n    ## If true the variable will be added as a tag, otherwise a field will be\n    ## created.\n    # is_tag = false\n\n    ## Apply one of the following conversions to the variable value:\n    ##   float(X):    Convert the input value into a float and divides by the\n    ##                Xth power of 10. Effectively just moves the decimal left\n    ##                X places. For example a value of `123` with `float(2)`\n    ##                will result in `1.23`.\n    ##   float:       Convert the value into a float with no adjustment. Same\n    ##                as `float(0)`.\n    ##   int:         Convert the value into an integer.\n    ##   ipaddr:      Convert the value to an IP address.\n    ##   hex:         Convert bytes to a hex string.\n    ##   hextoint:X:Y Convert bytes to integer, where X is the endian and Y the\n    ##                bit size. For example: hextoint:LittleEndian:uint64 or\n    ##                hextoint:BigEndian:uint32. Valid options for the endian\n    ##                are: BigEndian and LittleEndian. For the bit size: \n    ##                uint16, uint32 and uint64.\n    ##   enum:        Convert the value according to its syntax in the MIB.\n    ##                (Only supported with gosmi translator)\n    ##   displayhint: Format the value according to the textual convention in the MIB.\n    ##                (Only supported with gosmi translator)\n    ##\n    # conversion = \"\"\n```\n\n----------------------------------------\n\nTITLE: Example Memcached Plugin Output\nDESCRIPTION: This is an example output from the memcached plugin. It shows the fields gathered from memcached with sample values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/memcached/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n\"memcached,server=localhost:11211 accepting_conns=1i,auth_cmds=0i,auth_errors=0i,bytes=0i,bytes_read=7i,bytes_written=0i,cas_badval=0i,cas_hits=0i,cas_misses=0i,cmd_flush=0i,cmd_get=0i,cmd_set=0i,cmd_touch=0i,conn_yields=0i,connection_structures=3i,curr_connections=2i,curr_items=0i,decr_hits=0i,decr_misses=0i,delete_hits=0i,delete_misses=0i,evicted_active=0i,evicted_unfetched=0i,evictions=0i,expired_unfetched=0i,get_expired=0i,get_flushed=0i,get_hits=0i,get_misses=0i,hash_bytes=524288i,hash_is_expanding=0i,hash_power_level=16i,incr_hits=0i,incr_misses=0i,limit_maxbytes=67108864i,listen_disabled_num=0i,max_connections=1024i,reclaimed=0i,rejected_connections=0i,store_no_memory=0i,store_too_large=0i,threads=4i,total_connections=3i,total_items=0i,touch_hits=0i,touch_misses=0i,uptime=3i 1644771989000000000\"\n```\n\n----------------------------------------\n\nTITLE: New Feature Implementations\nDESCRIPTION: Details of new features added across various Telegraf plugins including systemd units pattern support, SQL boolean datatype, and enhanced AWS CloudWatch functionality.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n- Add pattern support for systemd_units\n- Add bool datatype to SQL outputs\n- Pull metrics from multiple AWS CloudWatch namespaces\n- Support AWS Web Identity Provider\n```\n\n----------------------------------------\n\nTITLE: Adding Measurement Name as Tag in Telegraf Template Processor\nDESCRIPTION: Example configuration that adds the measurement name as a tag named \"measurement\".\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = \"measurement\"\n  template = '{{ .Name }}'\n```\n\n----------------------------------------\n\nTITLE: Using Field Value as Tag Name in Telegraf Template Processor\nDESCRIPTION: Example demonstrating how to use a field value as a tag name, with the measurement name as the tag value.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = '{{ .Field \"type\" }}'\n  template = '{{ .Name }}'\n```\n\n----------------------------------------\n\nTITLE: Prometheus v2 Metrics Output Example\nDESCRIPTION: Example output showing metrics in prometheus-v2 format, with measurements stored in the 'prometheus' measurement.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opentelemetry/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nprometheus,foo=bar cpu_temp=87.332\nprometheus,method=post,code=200 http_requests_total=1027\nprometheus,method=post,code=400 http_requests_total=3\nprometheus,le=0.05 http_request_duration_seconds_bucket=24054\nprometheus,le=0.1  http_request_duration_seconds_bucket=33444\nprometheus,le=0.2  http_request_duration_seconds_bucket=100392\nprometheus,le=0.5  http_request_duration_seconds_bucket=129389\nprometheus,le=1    http_request_duration_seconds_bucket=133988\nprometheus         http_request_duration_seconds_count=144320,http_request_duration_seconds_sum=53423\n```\n\n----------------------------------------\n\nTITLE: Configuring Message Separator in Socket Listener Input\nDESCRIPTION: Allows specifying a custom message separator for streams in the socket listener input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_8\n\nLANGUAGE: go\nCODE:\n```\n[[inputs.socket_listener]]\n  message_separator = \"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes StatefulSet Metrics with Telegraf\nDESCRIPTION: This snippet captures metrics for a Kubernetes StatefulSet, tracking its replica counts and related information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_statefulset,namespace=default,selector_select1=s1,statefulset_name=etcd replicas_updated=3i,spec_replicas=3i,observed_generation=1i,created=1544101669000000000i,generation=1i,replicas=3i,replicas_current=3i,replicas_ready=3i 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Tengine Input Plugin in TOML\nDESCRIPTION: This snippet configures the Tengine input plugin in Telegraf. It specifies an array of Tengine reqstat module URIs to gather statistics. Optional TLS settings can be configured for secure communication. The primary configuration specifies default HTTP response timeouts along with optional TLS certificate paths for secure data transmission.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tengine/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n# Read Tengine\\'s basic status information (ngx_http_reqstat_module)\\n[[inputs.tengine]]\\n  ## An array of Tengine reqstat module URI to gather stats.\\n  urls = [\\\"http://127.0.0.1/us\\\"]\\n\\n  ## HTTP response timeout (default: 5s)\\n  # response_timeout = \\\"5s\\\"\\n\\n  ## Optional TLS Config\\n  # tls_ca = \\\"/etc/telegraf/ca.pem\\\"\\n  # tls_cert = \\\"/etc/telegraf/cert.pem\\\"\\n  # tls_key = \\\"/etc/telegraf/key.pem\\\"\\n  ## Use TLS but skip chain & host verification\\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Example Output from Fibaro Input Plugin\nDESCRIPTION: Sample output showing collected metrics from various Fibaro devices including windows, lights, sensors, and switches with their corresponding values, energy consumption, power usage, and other device-specific readings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fibaro/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfibaro,deviceId=9,host=vm1,name=Fentre\\ haute,room=Cuisine,section=Cuisine,type=com.fibaro.FGRM222 energy=2.04,power=0.7,value=99,value2=99 1529996807000000000\nfibaro,deviceId=10,host=vm1,name=Escaliers,room=Dgagement,section=Pices\\ communes,type=com.fibaro.binarySwitch value=0 1529996807000000000\nfibaro,deviceId=13,host=vm1,name=Porte\\ fentre,room=Salon,section=Pices\\ communes,type=com.fibaro.FGRM222 energy=4.33,power=0.7,value=99,value2=99 1529996807000000000\nfibaro,deviceId=21,host=vm1,name=LED\\ lot\\ central,room=Cuisine,section=Cuisine,type=com.fibaro.binarySwitch value=0 1529996807000000000\nfibaro,deviceId=90,host=vm1,name=Dtrioration,room=Entre,section=Pices\\ communes,type=com.fibaro.heatDetector value=0 1529996807000000000\nfibaro,deviceId=163,host=vm1,name=Temprature,room=Cave,section=Cave,type=com.fibaro.temperatureSensor value=21.62 1529996807000000000\nfibaro,deviceId=191,host=vm1,name=Prsence,room=Garde-manger,section=Cuisine,type=com.fibaro.FGMS001 value=1 1529996807000000000\nfibaro,deviceId=193,host=vm1,name=Luminosit,room=Garde-manger,section=Cuisine,type=com.fibaro.lightSensor value=195 1529996807000000000\nfibaro,deviceId=200,host=vm1,name=Etat,room=Garage,section=Extrieur,type=com.fibaro.doorSensor value=0 1529996807000000000\nfibaro,deviceId=220,host=vm1,name=CO2\\ (ppm),room=Salon,section=Pices\\ communes,type=com.fibaro.multilevelSensor value=536 1529996807000000000\nfibaro,deviceId=221,host=vm1,name=Humidit\\ (%),room=Salon,section=Pices\\ communes,type=com.fibaro.humiditySensor value=61 1529996807000000000\nfibaro,deviceId=222,host=vm1,name=Pression\\ (mb),room=Salon,section=Pices\\ communes,type=com.fibaro.multilevelSensor value=1013.7 1529996807000000000\nfibaro,deviceId=223,host=vm1,name=Bruit\\ (db),room=Salon,section=Pices\\ communes,type=com.fibaro.multilevelSensor value=44 1529996807000000000\nfibaro,deviceId=248,host=vm1,name=Temprature,room=Garage,section=Extrieur,type=com.fibaro.temperatureSensor batteryLevel=85,value=10.8 1529996807000000000\n```\n\n----------------------------------------\n\nTITLE: DNS Lookup Configuration Example\nDESCRIPTION: Example configuration showing how to skip DNS lookups using the ntpq options parameter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n  options = \"-p -n\"\n```\n\n----------------------------------------\n\nTITLE: Template with Additional Static Tags\nDESCRIPTION: Shows how to add static tags to metrics that don't exist in the received metric data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\ntemplates = [\n    \"measurement.measurement.field.region datacenter=1a\"\n]\n```\n\n----------------------------------------\n\nTITLE: Listing vSphere Metrics with govc Tool\nDESCRIPTION: Command-line method to retrieve available metrics for virtual machines using the govc utility from the govmomi project\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/METRICS.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngovc metric.ls vm\\/*\n```\n\n----------------------------------------\n\nTITLE: Slurm Metrics Example Output\nDESCRIPTION: This example shows the output format and metrics collected by the Telegraf Slurm plugin. It includes data for slurm diagnostics, job details, node resources, and partition information in InfluxDB line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/slurm/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"slurm_diag,host=hoth,source=slurm_primary.example.net bf_active=false,bf_queue_len=1i,bf_queue_len_mean=1i,jobs_canceled=0i,jobs_completed=137i,jobs_failed=0i,jobs_pending=0i,jobs_running=100i,jobs_started=137i,jobs_submitted=137i,schedule_cycle_last=27i,schedule_cycle_mean=86i,server_thread_count=3i 1723466497000000000\nslurm_jobs,host=hoth,job_id=23160,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.11BCgQ\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/7CQODmQ3uw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmG9JKDmILUkln\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo225\\\",partition=\\\"atlas\\\",priority=4294878569i,standard_error=\\\"/home/sessiondir/7CQODmQ3uw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmG9JKDmILUkln.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/7CQODmQ3uw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmG9JKDmILUkln.comment\\\",start_time=1723354525i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723354525i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=2000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23365,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.yRcFYL\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/LgwNDmTLAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm2BKKDm8bFZsm\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo224\\\",partition=\\\"atlas\\\",priority=4294878364i,standard_error=\\\"/home/sessiondir/LgwNDmTLAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm2BKKDm8bFZsm.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/LgwNDmTLAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm2BKKDm8bFZsm.comment\\\",start_time=1723376763i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723376761i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23366,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.5Y9Ngb\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/HFYKDmULAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm3BKKDmiyK3em\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo225\\\",partition=\\\"atlas\\\",priority=4294878363i,standard_error=\\\"/home/sessiondir/HFYKDmULAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm3BKKDmiyK3em.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/HFYKDmULAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm3BKKDmiyK3em.comment\\\",start_time=1723376883i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723376882i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23367,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.NmOqMU\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/nnLLDmULAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm4BKKDmfhjFPn\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo225\\\",partition=\\\"atlas\\\",priority=4294878362i,standard_error=\\\"/home/sessiondir/nnLLDmULAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm4BKKDmfhjFPn.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/nnLLDmULAx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm4BKKDmfhjFPn.comment\\\",start_time=1723376883i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723376882i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23385,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.NNsI08\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/PWvNDmH7tw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmz7JKDmqgKyRo\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo225\\\",partition=\\\"atlas\\\",priority=4294878344i,standard_error=\\\"/home/sessiondir/PWvNDmH7tw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmz7JKDmqgKyRo.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/PWvNDmH7tw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmz7JKDmqgKyRo.comment\\\",start_time=1723378725i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723378725i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23386,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.bcmS4h\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/ZNHMDmI7tw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm27JKDm3Ve66n\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo224\\\",partition=\\\"atlas\\\",priority=4294878343i,standard_error=\\\"/home/sessiondir/ZNHMDmI7tw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm27JKDm3Ve66n.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/ZNHMDmI7tw5nKG01gq4B3BRpm7wtQmABFKDmbnHPDm27JKDm3Ve66n.comment\\\",start_time=1723379206i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723379205i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23387,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.OgpoQZ\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/qohNDmUqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmMCKKDmzM4Yhn\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo222\\\",partition=\\\"atlas\\\",priority=4294878342i,standard_error=\\\"/home/sessiondir/qohNDmUqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmMCKKDmzM4Yhn.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/qohNDmUqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmMCKKDmzM4Yhn.comment\\\",start_time=1723379246i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723379245i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23388,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.xYbxSe\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/u9HODmXqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmWCKKDmRlccYn\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo224\\\",partition=\\\"atlas\\\",priority=4294878341i,standard_error=\\\"/home/sessiondir/u9HODmXqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmWCKKDmRlccYn.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/u9HODmXqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmWCKKDmRlccYn.comment\\\",start_time=1723379326i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723379326i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23389,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.QHtIIm\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/ZLvKDmYqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmXCKKDmjp19km\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo227\\\",partition=\\\"atlas\\\",priority=4294878340i,standard_error=\\\"/home/sessiondir/ZLvKDmYqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmXCKKDmjp19km.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/ZLvKDmYqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmXCKKDmjp19km.comment\\\",start_time=1723379326i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723379326i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_jobs,host=hoth,job_id=23393,name=gridjob,source=slurm_primary.example.net command=\\\"/tmp/SLURM_job_script.IH19bN\\\",cpus=2i,current_working_directory=\\\"/home/sessiondir/YdPODmVqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmSCKKDmrYDOwm\\\",group_id=2005i,nice=50i,node_count=1i,nodes=\\\"naboo224\\\",partition=\\\"atlas\\\",priority=4294878336i,standard_error=\\\"/home/sessiondir/YdPODmVqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmSCKKDmrYDOwm.comment\\\",standard_input=\\\"/dev/null\\\",standard_output=\\\"/home/sessiondir/YdPODmVqBx5nKG01gq4B3BRpm7wtQmABFKDmbnHPDmSCKKDmrYDOwm.comment\\\",start_time=1723379767i,state=\\\"RUNNING\\\",state_reason=\\\"None\\\",submit_time=1723379766i,tasks=1i,time_limit=3600i,tres_billing=1,tres_cpu=1,tres_mem=1000,tres_node=1 1723466497000000000\nslurm_nodes,host=hoth,name=naboo145,source=slurm_primary.example.net alloc_cpu=0i,alloc_memory=0i,architecture=\\\"x86_64\\\",cores=18i,cpu_load=0i,cpus=36i,free_memory=86450i,real_memory=94791i,slurmd_version=\\\"22.05.9\\\",state=\\\"idle\\\",tres_billing=36,tres_cpu=36,tres_mem=94791,weight=1i 1723466497000000000\nslurm_nodes,host=hoth,name=naboo146,source=slurm_primary.example.net alloc_cpu=0i,alloc_memory=0i,architecture=\\\"x86_64\\\",cores=18i,cpu_load=0i,cpus=36i,free_memory=92148i,real_memory=94791i,slurmd_version=\\\"22.05.9\\\",state=\\\"idle\\\",tres_billing=36,tres_cpu=36,tres_mem=94791,weight=1i 1723466497000000000\nslurm_nodes,host=hoth,name=naboo147,source=slurm_primary.example.net alloc_cpu=36i,alloc_memory=45000i,architecture=\\\"x86_64\\\",cores=18i,cpu_load=3826i,cpus=36i,free_memory=1607i,real_memory=94793i,slurmd_version=\\\"22.05.9\\\",state=\\\"allocated\\\",tres_billing=36,tres_cpu=36,tres_mem=94793,tres_used_cpu=36,tres_used_mem=45000,weight=1i 1723466497000000000\nslurm_nodes,host=hoth,name=naboo216,source=slurm_primary.example.net alloc_cpu=8i,alloc_memory=8000i,architecture=\\\"x86_64\\\",cores=4i,cpu_load=891i,cpus=8i,free_memory=17972i,real_memory=31877i,slurmd_version=\\\"22.05.9\\\",state=\\\"allocated\\\",tres_billing=8,tres_cpu=8,tres_mem=31877,tres_used_cpu=8,tres_used_mem=8000,weight=1i 1723466497000000000\nslurm_nodes,host=hoth,name=naboo219,source=slurm_primary.example.net alloc_cpu=16i,alloc_memory=16000i,architecture=\\\"x86_64\\\",cores=4i,cpu_load=1382i,cpus=16i,free_memory=15645i,real_memory=31875i,slurmd_version=\\\"22.05.9\\\",state=\\\"allocated\\\",tres_billing=16,tres_cpu=16,tres_mem=31875,tres_used_cpu=16,tres_used_mem=16000,weight=1i 1723466497000000000\nslurm_partitions,host=hoth,name=atlas,source=slurm_primary.example.net nodes=\\\"naboo145,naboo146,naboo147,naboo216,naboo219,naboo222,naboo224,naboo225,naboo227,naboo228,naboo229,naboo234,naboo235,naboo236,naboo237,naboo238,naboo239,naboo240,naboo241,naboo242,naboo243\\\",state=\\\"UP\\\",total_cpu=632i,total_nodes=21i,tres_billing=632,tres_cpu=632,tres_mem=1415207,tres_node=21 1723466497000000000\"\n```\n\n----------------------------------------\n\nTITLE: Example Metric without Tags and Corresponding Zabbix Trap\nDESCRIPTION: Example of a simple memory metric without additional tags and how it's converted to Zabbix trapper format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nmem,host=myHost available_percent=14.684620843239944,used=14246531072i 152276442800000000\n```\n\n----------------------------------------\n\nTITLE: PEN Mapping Configuration for Netflow in CSV Format\nDESCRIPTION: Example of a Private Enterprise Number (PEN) mapping CSV file for vendor-specific element IDs. Shows the format for mapping custom elements with their data types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netflow/README.md#2025-04-16_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\n# PEN.ID, name, data type\n35632.349,in_src_osi_sap,hex\n35632.471,nprobe_ipv4_address,ip\n35632.1028,protocol_ntop,string\n35632.1036,l4_srv_port,uint\n```\n\n----------------------------------------\n\nTITLE: Displaying Intel PMT Throttle Counter Metrics in Telegraf\nDESCRIPTION: This example shows the output format of Intel PMT telemetry data with tpvp_throttle_counter as a datatype metric filter. It displays throttle counter metrics for 49 CPU cores (0-48) with various attributes including NUMA node, PCI BDF address, sample group, and timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nintel_pmt,core=0,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C0_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1886465i 1693766334000000000\nintel_pmt,core=1,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C1_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=2,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C2_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=3,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C3_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=4,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C4_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1357578i 1693766334000000000\nintel_pmt,core=5,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C5_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=6,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C6_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2024801i 1693766334000000000\nintel_pmt,core=7,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C7_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=8,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C8_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1390741i 1693766334000000000\nintel_pmt,core=9,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C9_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=10,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C10_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1536483i 1693766334000000000\nintel_pmt,core=11,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C11_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=12,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C12_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=13,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C13_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=14,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C14_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1604964i 1693766334000000000\nintel_pmt,core=15,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C15_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1168673i 1693766334000000000\nintel_pmt,core=16,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C16_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=17,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C17_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=18,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C18_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1276588i 1693766334000000000\nintel_pmt,core=19,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C19_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1139005i 1693766334000000000\nintel_pmt,core=20,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C20_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=21,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C21_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=22,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C22_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=970698i 1693766334000000000\nintel_pmt,core=23,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C23_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=24,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C24_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=25,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C25_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1178462i 1693766334000000000\nintel_pmt,core=26,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C26_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=27,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C27_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2093384i 1693766334000000000\nintel_pmt,core=28,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C28_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=29,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C29_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=30,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C30_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=31,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C31_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=32,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C32_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2825174i 1693766334000000000\nintel_pmt,core=33,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C33_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2592279i 1693766334000000000\nintel_pmt,core=34,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C34_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=35,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C35_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=36,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C36_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1960662i 1693766334000000000\nintel_pmt,core=37,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C37_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=1821914i 1693766334000000000\nintel_pmt,core=38,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C38_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=39,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C39_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=40,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C40_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=41,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C41_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2654651i 1693766334000000000\nintel_pmt,core=42,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C42_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2230984i 1693766334000000000\nintel_pmt,core=43,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C43_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=44,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C44_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=45,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C45_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=46,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C46_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=2325520i 1693766334000000000\nintel_pmt,core=47,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C47_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\nintel_pmt,core=48,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C48_PVP_THROTTLE_64,sample_name=PVP_THROTTLE_64 value=0i 1693766334000000000\n```\n\n----------------------------------------\n\nTITLE: Sample Output for Ungrouped Sysstat Configuration in Telegraf\nDESCRIPTION: This text snippet shows the output generated by Telegraf when using the sysstat input plugin with ungrouped output. Each metric is reported individually, providing a more granular view of system statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sysstat/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nio_tps value=0.5 1459255780126025822\nio_rtps value=0 1459255780126025822\nio_wtps value=0.5 1459255780126025822\nio_bread_per_s value=0 1459255780126025822\nio_bwrtn_per_s value=38 1459255780126025822\ncpu_util_pct_user,device=all value=39.07 1459255780126025822\ncpu_util_pct_nice,device=all value=0 1459255780126025822\ncpu_util_pct_system,device=all value=47.94 1459255780126025822\ncpu_util_pct_iowait,device=all value=0 1459255780126025822\ncpu_util_pct_steal,device=all value=0 1459255780126025822\ncpu_util_pct_idle,device=all value=12.98 1459255780126025822\nswap_pswpin_per_s value=0 1459255780126025822\ncpu_pct_user,device=all value=39.07 1459255780126025822\ncpu_pct_nice,device=all value=0 1459255780126025822\ncpu_pct_system,device=all value=47.94 1459255780126025822\ncpu_pct_iowait,device=all value=0 1459255780126025822\ncpu_pct_steal,device=all value=0 1459255780126025822\ncpu_pct_idle,device=all value=12.98 1459255780126025822\nper_cpu_pct_user,device=all value=39.07 1459255780126025822\nper_cpu_pct_nice,device=all value=0 1459255780126025822\nper_cpu_pct_system,device=all value=47.94 1459255780126025822\nper_cpu_pct_iowait,device=all value=0 1459255780126025822\nper_cpu_pct_steal,device=all value=0 1459255780126025822\nper_cpu_pct_idle,device=all value=12.98 1459255780126025822\nper_cpu_pct_user,device=cpu0 value=33.5 1459255780126025822\nper_cpu_pct_nice,device=cpu0 value=0 1459255780126025822\nper_cpu_pct_system,device=cpu0 value=65.25 1459255780126025822\nper_cpu_pct_iowait,device=cpu0 value=0 1459255780126025822\nper_cpu_pct_steal,device=cpu0 value=0 1459255780126025822\nper_cpu_pct_idle,device=cpu0 value=1.25 1459255780126025822\nper_cpu_pct_user,device=cpu1 value=44.85 1459255780126025822\nper_cpu_pct_nice,device=cpu1 value=0 1459255780126025822\nper_cpu_pct_system,device=cpu1 value=29.55 1459255780126025822\nper_cpu_pct_iowait,device=cpu1 value=0 1459255780126025822\nper_cpu_pct_steal,device=cpu1 value=0 1459255780126025822\nper_cpu_pct_idle,device=cpu1 value=25.59 1459255780126025822\nhugepages_kbhugfree value=0 1459255780126025822\nhugepages_kbhugused value=0 1459255780126025822\nhugepages_pct_hugused value=0 1459255780126025822\ninterrupts_intr_per_s,device=i000 value=0 1459255780126025822\ninode_dentunusd value=252876 1459255780126025822\nmem_util_kbmemfree value=1613612 1459255780126025822\ndisk_tps,device=sda,vg=rootvg value=0.5 1459255780126025822\nswap_pswpout_per_s value=0 1459255780126025822\nnetwork_rxpck_per_s,device=ens33 value=0 1459255780126025822\nqueue_runq-sz value=4 1459255780126025822\ntask_proc_per_s value=0 1459255780126025822\ntask_cswch_per_s value=2019 1459255780126025822\nmem_frmpg_per_s value=0 1459255780126025822\nmem_bufpg_per_s value=0.5 1459255780126025822\nmem_campg_per_s value=1.25 1459255780126025822\ninterrupts_intr_per_s,device=i001 value=0 1459255780126025822\ninode_file-nr value=19104 1459255780126025822\nmem_util_kbmemused value=3741528 1459255780126025822\ndisk_rd_sec_per_s,device=sda,vg=rootvg value=0 1459255780126025822\nnetwork_txpck_per_s,device=ens33 value=0 1459255780126025822\nqueue_plist-sz value=1512 1459255780126025822\npaging_pgpgin_per_s value=0 1459255780126025822\npaging_pgpgout_per_s value=19 1459255780126025822\npaging_fault_per_s value=0.25 1459255780126025822\npaging_majflt_per_s value=0 1459255780126025822\npaging_pgfree_per_s value=34.25 1459255780126025822\npaging_pgscank_per_s value=0 1459255780126025822\npaging_pgscand_per_s value=0 1459255780126025822\npaging_pgsteal_per_s value=0 1459255780126025822\npaging_pct_vmeff value=0 1459255780126025822\ninterrupts_intr_per_s,device=i002 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i003 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i004 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i005 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i006 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i007 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i008 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i009 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i010 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i011 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i012 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i013 value=0 1459255780126025822\ninterrupts_intr_per_s,device=i014 value=0 1459255780126025822\n```\n\n----------------------------------------\n\nTITLE: Full NFS Statistics Output Example\nDESCRIPTION: Detailed example of NFS metrics when fullstat=true, showing extended statistics including bytes, events, transport metrics, and per-operation data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nfsclient/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nnfs_bytes,mountpoint=/home,serverexport=nfs01:/vol/home directreadbytes=0i,directwritebytes=0i,normalreadbytes=42648757667i,normalwritebytes=0i,readpages=10404603i,serverreadbytes=42617098139i,serverwritebytes=0i,writepages=0i 1608787697000000000\nnfs_events,mountpoint=/home,serverexport=nfs01:/vol/home attrinvalidates=116i,congestionwait=0i,datainvalidates=65i,delay=0i,dentryrevalidates=5911243i,extendwrite=0i,inoderevalidates=200378i,pnfsreads=0i,pnfswrites=0i,setattrtrunc=0i,shortreads=0i,shortwrites=0i,sillyrenames=0i,vfsaccess=7203852i,vfsflush=117405i,vfsfsync=0i,vfsgetdents=3368i,vfslock=0i,vfslookup=740i,vfsopen=157281i,vfsreadpage=16i,vfsreadpages=86874i,vfsrelease=155526i,vfssetattr=0i,vfsupdatepage=0i,vfswritepage=0i,vfswritepages=215514i 1608787697000000000\nnfs_xprt_tcp,mountpoint=/home,serverexport=nfs01:/vol/home backlogutil=0i,badxids=0i,bind_count=1i,connect_count=1i,connect_time=0i,idle_time=0i,inflightsends=15659826i,rpcreceives=2173896i,rpcsends=2173896i 1608787697000000000\n\nnfs_ops,mountpoint=/NFS,operation=NULL,serverexport=1.2.3.4:/storage/NFS trans=0i,timeouts=0i,bytes_sent=0i,bytes_recv=0i,queue_time=0i,response_time=0i,total_time=0i,ops=0i 1612651512000000000\nnfs_ops,mountpoint=/NFS,operation=READ,serverexport=1.2.3.4:/storage/NFS bytes=1207i,timeouts=602i,total_time=607i,exe=607i,trans=601i,bytes_sent=603i,bytes_recv=604i,queue_time=605i,ops=600i,retrans=1i,rtt=606i,response_time=606i 1612651512000000000\nnfs_ops,mountpoint=/NFS,operation=WRITE,serverexport=1.2.3.4:/storage/NFS ops=700i,bytes=1407i,exe=707i,trans=701i,timeouts=702i,response_time=706i,total_time=707i,retrans=1i,rtt=706i,bytes_sent=703i,bytes_recv=704i,queue_time=705i 1612651512000000000\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch Thread Pool Metrics in Telegraf\nDESCRIPTION: This snippet enumerates metrics related to thread pools in Elasticsearch, such as active tasks and completed tasks for various operations. These metrics help monitor the threading efficiency and capacity of the Elasticsearch node.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n  - elasticsearch_thread_pool\n  - tags:\n    - cluster_name\n    - node_attribute_ml.enabled\n    - node_attribute_ml.machine_memory\n    - node_attribute_ml.max_open_jobs\n    - node_attribute_xpack.installed\n    - node_host\n    - node_id\n    - node_name\n  - fields:\n    - analyze_active (float)\n    - analyze_completed (float)\n    - analyze_largest (float)\n    - analyze_queue (float)\n    - analyze_rejected (float)\n    - analyze_threads (float)\n    - ccr_active (float)\n    - ccr_completed (float)\n    - ccr_largest (float)\n    - ccr_queue (float)\n    - ccr_rejected (float)\n    - ccr_threads (float)\n    - flush_active (float)\n    - flush_completed (float)\n    - flush_largest (float)\n    - flush_queue (float)\n    - flush_rejected (float)\n    - flush_threads (float)\n    - generic_active (float)\n    - generic_completed (float)\n    - generic_largest (float)\n    - generic_queue (float)\n    - generic_rejected (float)\n    - generic_threads (float)\n    - index_active (float)\n    - index_completed (float)\n    - index_largest (float)\n    - index_queue (float)\n    - index_rejected (float)\n    - index_threads (float)\n    - refresh_active (float)\n    - refresh_completed (float)\n    - refresh_largest (float)\n    - refresh_queue (float)\n    - refresh_rejected (float)\n    - refresh_threads (float)\n    - search_active (float)\n    - search_completed (float)\n    - search_largest (float)\n    - search_queue (float)\n    - search_rejected (float)\n    - search_threads (float)\n    - warmer_active (float)\n    - warmer_completed (float)\n    - warmer_largest (float)\n    - warmer_queue (float)\n    - warmer_rejected (float)\n    - warmer_threads (float)\n```\n\n----------------------------------------\n\nTITLE: Configuring Raindrops Input Plugin - TOML\nDESCRIPTION: This code snippet contains the configuration required to use the raindrops input plugin in Telegraf. It specifies an array of URIs to gather real-time statistics from raindrops middleware. The configuration should be placed in the Telegraf configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/raindrops/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read raindrops stats (raindrops - real-time stats for preforking Rack servers)\n[[inputs.raindrops]]\n  ## An array of raindrops middleware URI to gather stats.\n  urls = [\"http://localhost:8080/_raindrops\"]\n```\n\n----------------------------------------\n\nTITLE: Resulting Clarify Signal JSON Structure\nDESCRIPTION: This JSON snippet illustrates how the example Telegraf metric would be stored in Clarify. It shows the signal structure with ID, name, labels, and values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/clarify/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"signal\" {\n  \"id\": \"temperature.value.TC0P\"\n  \"name\": \"temperature.value\"\n  \"labels\": {\n    \"host\": [\"demo.clarifylocal\"],\n    \"sensor\": [\"TC0P\"]\n  }\n}\n\"values\" {\n  \"times\": [\"2023-04-28T08:43:16+00:00\"],\n  \"series\": {\n    \"temperature.value.TC0P\": [49]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying SQL Server Performance Metrics - Version 1\nDESCRIPTION: Original set of SQL Server metrics queries that collect data from various system views including performance counters, wait stats, memory clerks, and database properties. This version is deprecated as of Telegraf 1.16.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nsys.dm_os_performance_counters\\nsys.dm_os_wait_stats\\nsys.dm_os_memory_clerks\\nsys.dm_io_virtual_file_stats\\nsys.databases\\nsys.dm_os_volume_stats\\nsys.dm_os_ring_buffers\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Query for Common Statistics\nDESCRIPTION: This TOML configuration snippet sets up an OpenSearch query to generate common statistics for all documents, returning results grouped by response status code. It doesn't specify metric fields, implying it will use default statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensearch_query/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.opensearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"*\"\n  tags = [\"response.keyword\"]\n  include_missing_tag = false\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Linux Sysctl FS Metrics\nDESCRIPTION: This example shows the format of the output metrics collected by the `linux_sysctl_fs` plugin. It includes various filesystem-related metrics and their corresponding integer values, along with the timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/linux_sysctl_fs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n> linux_sysctl_fs,host=foo dentry-want-pages=0i,file-max=44222i,aio-max-nr=65536i,inode-preshrink-nr=0i,dentry-nr=64340i,dentry-unused-nr=55274i,file-nr=1568i,aio-nr=0i,inode-nr=35952i,inode-free-nr=12957i,dentry-age-limit=45i 1490982022000000000\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenNTPD Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet shows the available settings for the OpenNTPD input plugin. It includes options for using sudo, specifying the ntpctl binary location, and setting a timeout for the command execution.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openntpd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Get standard NTP query metrics from OpenNTPD.\n[[inputs.openntpd]]\n  ## Run ntpctl binary with sudo.\n  # use_sudo = false\n\n  ## Location of the ntpctl binary.\n  # binary = \"/usr/sbin/ntpctl\"\n\n  ## Maximum time the ntpctl binary is allowed to run.\n  # timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: PgBouncer Connection String Examples\nDESCRIPTION: Examples of how to specify the connection string for PgBouncer in both standard PostgreSQL format and URL format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pgbouncer/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhost=/run/postgresql port=6432 user=telegraf database=pgbouncer\n```\n\nLANGUAGE: text\nCODE:\n```\npostgres://[pqgotest[:password]]@host:port[/dbname]?sslmode=[disable|verify-ca|verify-full]\n```\n\n----------------------------------------\n\nTITLE: Configuring Passenger Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the Passenger input plugin for Telegraf. It specifies the command to execute passenger-status and retrieve XML output for metric parsing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/passenger/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics of passenger using passenger-status\n[[inputs.passenger]]\n  ## Path of passenger-status.\n  ##\n  ## Plugin gather metric via parsing XML output of passenger-status\n  ## More information about the tool:\n  ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n  ##\n  ## If no path is specified, then the plugin simply execute passenger-status\n  ## hopefully it can be found in your PATH\n  command = \"passenger-status -v --show=xml\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index on Time and Tag Columns in PostgreSQL for Telegraf\nDESCRIPTION: Configuration for creating database indexes on time and tag columns to improve query performance on Telegraf data stored in PostgreSQL.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/postgresql/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\ncreate_templates = [\n    '''CREATE TABLE {{ .table }} ({{ .columns }})''',\n    '''CREATE INDEX ON {{ .table }} USING btree({{ .columns.Keys.Identifiers | join \",\" }})'''\n  ]\n```\n\n----------------------------------------\n\nTITLE: Allowing Multiline Messages in Grok Parser\nDESCRIPTION: Adds an option to allow multiline messages in the Grok parser.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_12\n\nLANGUAGE: go\nCODE:\n```\n[[parsers.grok]]\n  allow_multiline = true\n```\n\n----------------------------------------\n\nTITLE: JSON Array Parsing with Time Configuration\nDESCRIPTION: Example demonstrating how to parse JSON arrays with time field configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n  data_format = \"json\"\n  json_time_key = \"b_time\"\n  json_time_format = \"02 Jan 06 15:04 MST\"\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"a\": 5,\n        \"b\": {\n            \"c\": 6,\n            \"time\":\"04 Jan 06 15:04 MST\"\n        }\n    },\n    {\n        \"a\": 7,\n        \"b\": {\n            \"c\": 8,\n            \"time\":\"11 Jan 07 15:04 MST\"\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Maximum Buffer Size\nDESCRIPTION: InfluxDB SQL query to retrieve maximum buffer size over the last hour for all switches, grouped by time and hostname.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lanz/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT max(\"buffer_size\") AS \"max_buffer_size\" FROM \"global_buffer_usage_record\" WHERE time > now() - 1h GROUP BY time(10s), \"hostname\"\n```\n\n----------------------------------------\n\nTITLE: Basic Starlark Apply Function\nDESCRIPTION: A simple Starlark apply function that returns the original metric without modifications\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/starlark/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef apply(metric):\n    return metric\n```\n\n----------------------------------------\n\nTITLE: Feature Updates in Markdown\nDESCRIPTION: Documentation of new features added to the project.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n### Features\n\n- [#11228](https://github.com/influxdata/telegraf/pull/11228) `processors.parser` Add option to parse tags\n```\n\n----------------------------------------\n\nTITLE: Defining Alarm System State Metrics in Prometheus\nDESCRIPTION: Prometheus metric configuration that defines stateset type metrics for monitoring an alarm system. Tracks states of doors and windows across different building areas using binary values where 1.0 indicates normal state and 0.0 indicates potential issues.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_stateset/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE alarm_system_state stateset\nalarm_system_state{entity=\"ground floor\",alarm_system_state=\"door ok\"} 1.0\nalarm_system_state{entity=\"ground floor\",alarm_system_state=\"window 1 ok\"} 1.0\nalarm_system_state{entity=\"ground floor\",alarm_system_state=\"window 2 ok\"} 0.0\nalarm_system_state{entity=\"basement\",alarm_system_state=\"window 1 ok\"} 1.0\nalarm_system_state{entity=\"basement\",alarm_system_state=\"window 2 ok\"} 1.0\nalarm_system_state{entity=\"basement\",alarm_system_state=\"window 3 ok\"} 1.0\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Example Batch Processor Configuration\nDESCRIPTION: Specific example configuration showing batch processor setup with 3 batches using 'batch' as the tag key.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/batch/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.batch]]\n  ## The tag key to use for batching\n  batch_tag = \"batch\"\n  \n  ## The number of batches to create\n  batches = 3\n```\n\n----------------------------------------\n\nTITLE: Querying NGINX Plus API HTTP Limit Request Metrics in InfluxDB Format\nDESCRIPTION: These snippets show metrics for NGINX Plus API HTTP limit requests. They include counters for delayed, passed, and rejected requests for two different limits.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_6\n\nLANGUAGE: influxdb\nCODE:\n```\nnginx_plus_api_http_limit_reqs,port=80,source=demo.nginx.com,limit=limit_1 delayed=0i,delayed_dry_run=0i,passed=6i,rejected=9i,rejected_dry_run=0i 1570696322000000000\n```\n\nLANGUAGE: influxdb\nCODE:\n```\nnginx_plus_api_http_limit_reqs,port=80,source=demo.nginx.com,limit=limit_2 delayed=13i,delayed_dry_run=3i,passed=6i,rejected=1i,rejected_dry_run=31i 1570696322000000000\n```\n\n----------------------------------------\n\nTITLE: Tracing Spans Output Example\nDESCRIPTION: Example output showing how tracing spans are formatted in the InfluxDB line protocol, including various span attributes and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opentelemetry/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nspans end_time_unix_nano=\"2021-02-19 20:50:25.6893952 +0000 UTC\",instrumentation_library_name=\"tracegen\",kind=\"SPAN_KIND_INTERNAL\",name=\"okey-dokey\",net.peer.ip=\"1.2.3.4\",parent_span_id=\"d5270e78d85f570f\",peer.service=\"tracegen-client\",service.name=\"tracegen\",span.kind=\"server\",span_id=\"4c28227be6a010e1\",status_code=\"STATUS_CODE_OK\",trace_id=\"7d4854815225332c9834e6dbf85b9380\" 1613767825689169000\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple CPU Input Plugins with Different Settings in TOML\nDESCRIPTION: Configuration example demonstrating how to define multiple instances of the same plugin (CPU) with different configurations, using name_override to avoid measurement collisions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_15\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  name_override = \"percpu_usage\"\n  fieldexclude = [\"cpu_time*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kernel Network Statistics Input in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the nstat input plugin for Telegraf. It specifies file paths for network statistics and enables dumping of zero-value metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collect kernel snmp counters and network interface statistics\n[[inputs.nstat]]\n  ## file paths for proc files. If empty default paths will be used:\n  ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n  ## These can also be overridden with env variables, see README.\n  proc_net_netstat = \"/proc/net/netstat\"\n  proc_net_snmp = \"/proc/net/snmp\"\n  proc_net_snmp6 = \"/proc/net/snmp6\"\n  ## dump metrics with 0 values too\n  dump_zeros       = true\n```\n\n----------------------------------------\n\nTITLE: Diff Example for Enum Processor\nDESCRIPTION: This snippet provides examples of how the Enum Processor modifies data. It shows the expected output after processing an input with known and unknown values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/enum/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n\"- xyzzy status=\\\"green\\\" 1502489900000000000\\n+ xyzzy status=\\\"green\\\",status_code=1i 1502489900000000000\\n\\nWith unknown value and no default set:\\n\\n- xyzzy status=\\\"black\\\" 1502489900000000000\\n+ xyzzy status=\\\"black\\\" 1502489900000000000\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Postfix Input Plugin in TOML\nDESCRIPTION: TOML configuration for the Postfix input plugin in Telegraf. It specifies the queue directory option, which is optional as Telegraf can attempt to determine it automatically.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postfix/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Measure postfix queue statistics\n# This plugin ONLY supports non-Windows\n[[inputs.postfix]]\n  ## Postfix queue directory. If not provided, telegraf will try to use\n  ## 'postconf -h queue_directory' to determine it.\n  # queue_directory = \"/var/spool/postfix\"\n```\n\n----------------------------------------\n\nTITLE: Example Output from Network Response Plugin in Telegraf\nDESCRIPTION: Sample output from the Network Response Input Plugin showing metrics for different connection results including successful connection, connection failure, and read failure scenarios.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/net_response/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnet_response,port=8086,protocol=tcp,result=success,server=localhost response_time=0.000092948,result_code=0i,result_type=\"success\" 1525820185000000000\nnet_response,port=8080,protocol=tcp,result=connection_failed,server=localhost result_code=2i,result_type=\"connection_failed\" 1525820088000000000\nnet_response,port=8080,protocol=udp,result=read_failed,server=localhost result_code=3i,result_type=\"read_failed\",string_found=false 1525820088000000000\n```\n\n----------------------------------------\n\nTITLE: Example Metric with Tags and Corresponding Zabbix Trap\nDESCRIPTION: Example of a Docker container network metric with container_name tag and how it's converted to Zabbix trapper format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_11\n\nLANGUAGE: text\nCODE:\n```\ndocker_container_net,host=myHost,container_name=laughing_babbage rx_errors=0i,tx_errors=0i 1522764038000000000\n```\n\n----------------------------------------\n\nTITLE: Example Output Format (Text)\nDESCRIPTION: This example output shows how metrics are structured when collected from a Mcrouter instance. It includes various performance counters such as uptime, server status, and request counts, all formatted in a line protocol compatible with InfluxDB.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mcrouter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n\"mcrouter,server=localhost:11211 uptime=166,num_servers=1,num_servers_new=1,num_servers_up=0,num_servers_down=0,num_servers_closed=0,num_clients=1,num_suspect_servers=0,destination_batches_sum=0,destination_requests_sum=0,outstanding_route_get_reqs_queued=0,outstanding_route_update_reqs_queued=0,outstanding_route_get_avg_queue_size=0,outstanding_route_update_avg_queue_size=0,outstanding_route_get_avg_wait_time_sec=0,outstanding_route_update_avg_wait_time_sec=0,retrans_closed_connections=0,destination_pending_reqs=0,destination_inflight_reqs=0,destination_batch_size=0,asynclog_requests=0,proxy_reqs_processing=1,proxy_reqs_waiting=0,client_queue_notify_period=0,rusage_system=0.040966,rusage_user=0.020483,ps_num_minor_faults=2490,ps_num_major_faults=11,ps_user_time_sec=0.02,ps_system_time_sec=0.04,ps_vsize=697741312,ps_rss=10563584,fibers_allocated=0,fibers_pool_size=0,fibers_stack_high_watermark=0,successful_client_connections=18,duration_us=0,destination_max_pending_reqs=0,destination_max_inflight_reqs=0,retrans_per_kbyte_max=0,cmd_get_count=0,cmd_delete_out=0,cmd_lease_get=0,cmd_set=0,cmd_get_out_all=0,cmd_get_out=0,cmd_lease_set_count=0,cmd_other_out_all=0,cmd_lease_get_out=0,cmd_set_count=0,cmd_lease_set_out=0,cmd_delete_count=0,cmd_other=0,cmd_delete=0,cmd_get=0,cmd_lease_set=0,cmd_set_out=0,cmd_lease_get_count=0,cmd_other_out=0,cmd_lease_get_out_all=0,cmd_set_out_all=0,cmd_other_count=0,cmd_delete_out_all=0,cmd_lease_set_out_all=0 1453831884664956455\"\n```\n\n----------------------------------------\n\nTITLE: vSAN Performance Metrics\nDESCRIPTION: Time-series metrics showing vSAN cluster and host performance data. Includes metrics for I/O operations, latency, throughput, cache hits, and health status across different components.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_18\n\nLANGUAGE: metrics\nCODE:\n```\nvsphere_vsan_performance_hostdomclient,clustername=Example-VSAN,dcname=Example-DC,host=host.example.com,hostname=DC0_C0_H0,moid=domain-c8,source=Example-VSAN,vcenter=localhost:8898 iops_read=7,write_congestion=0,unmap_congestion=0,read_count=2199,iops=8,latency_max_write=8964,latency_avg_unmap=0,latency_avg_write=1883,write_count=364,num_oio=12623,throughput=564127,client_cache_hits=0,latency_max_read=17821,latency_max_unmap=0,read_congestion=0,latency_avg=1154,congestion=0,throughput_read=554721,latency_avg_read=1033,throughput_write=9406,client_cache_hit_rate=0,iops_unmap=0,throughput_unmap=0,latency_stddev=1315,io_count=2563,oio=4,iops_write=1,unmap_count=0 1578955200000000000\n```\n\n----------------------------------------\n\nTITLE: Implementing the 'push' Function in Starlark Aggregator\nDESCRIPTION: Example of the required 'push' function that computes and returns the aggregated metrics. This function is called at the end of each aggregation period to output results.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/starlark/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef push():\n  return state.get(\"last\")\n```\n\n----------------------------------------\n\nTITLE: Configuring sudoers entries for Fail2ban and Telegraf\nDESCRIPTION: Sudoers configuration that allows the telegraf user to execute fail2ban-client commands without a password while disabling logging of these commands.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fail2ban/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nCmnd_Alias FAIL2BAN = /usr/bin/fail2ban-client status, /usr/bin/fail2ban-client status *\ntelegraf  ALL=(root) NOEXEC: NOPASSWD: FAIL2BAN\nDefaults!FAIL2BAN !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Enabling Profiler in Telegraf Configuration\nDESCRIPTION: Add the --pprof-addr option to the Telegraf configuration to enable the profiler. This sets up a local endpoint for collecting profile data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nTELEGRAF_OPTS=\"--pprof-addr localhost:6060\"\n```\n\n----------------------------------------\n\nTITLE: Example Couchbase Metrics Output\nDESCRIPTION: Sample output showing the format of collected metrics including node statistics and bucket metrics with their respective tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/couchbase/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncouchbase_node,cluster=http://localhost:8091/,hostname=172.17.0.2:8091 memory_free=7705575424,memory_total=16558182400 1547829754000000000\ncouchbase_bucket,bucket=beer-sample,cluster=http://localhost:8091/ quota_percent_used=27.09285736083984,ops_per_sec=0,disk_fetches=0,item_count=7303,disk_used=21662946,data_used=9325087,mem_used=28408920 1547829754000000000\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration for Nvidia SMI\nDESCRIPTION: Docker Compose configuration showing how to set up Telegraf with nvidia-smi access in a container environment.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nvidia_smi/README.md#2025-04-16_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  telegraf:\n    image: telegraf\n    runtime: nvidia\n    devices:\n      - /dev/nvidiactl:/dev/nvidiactl\n      - /dev/nvidia0:/dev/nvidia0\n    volumes:\n      - ./telegraf/etc/telegraf.conf:/etc/telegraf/telegraf.conf:ro\n      - /usr/bin/nvidia-smi:/usr/bin/nvidia-smi:ro\n      - /usr/lib/x86_64-linux-gnu/nvidia:/usr/lib/x86_64-linux-gnu/nvidia:ro\n    environment:\n      - LD_PRELOAD=/usr/lib/x86_64-linux-gnu/nvidia/current/libnvidia-ml.so\n```\n\n----------------------------------------\n\nTITLE: Retrieving NVMe Vendor-Specific SMART Metrics\nDESCRIPTION: Command to retrieve vendor-specific SMART metrics for NVMe disks using nvme-cli.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvme <vendor> smart-log-add <device>\n```\n\n----------------------------------------\n\nTITLE: Example Output from Basic Statistics Aggregator\nDESCRIPTION: Sample output showing how the basicstats aggregator transforms input metrics into statistical summaries. The example demonstrates the calculation of various statistics like count, difference, rate, max, min, mean, and standard deviation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/basicstats/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsystem,host=tars load1=1 1475583980000000000\nsystem,host=tars load1=1 1475583990000000000\nsystem,host=tars load1_count=2,load1_diff=0,load1_rate=0,load1_max=1,load1_min=1,load1_mean=1,load1_sum=2,load1_s2=0,load1_stdev=0,load1_interval=10000000000i,load1_last=1 1475584010000000000\nsystem,host=tars load1=1 1475584020000000000\nsystem,host=tars load1=3 1475584030000000000\nsystem,host=tars load1_count=2,load1_diff=2,load1_rate=0.2,load1_max=3,load1_min=1,load1_mean=2,load1_sum=4,load1_s2=2,load1_stdev=1.414162,load1_interval=10000000000i,load1_last=3,load1_first=3 1475584010000000000\n```\n\n----------------------------------------\n\nTITLE: Example of Unpivoting Metrics in Diff Format\nDESCRIPTION: These examples illustrate how metrics are transformed when applying the unpivot processor in both 'tag' and 'metric' modes. It showcases the differences in output format based on the selected mode, providing insight into how raw data is manipulated for better usability.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/unpivot/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\nMetric mode `tag`:\n\n- cpu,cpu=cpu0 time_idle=42i,time_user=43i\n+ cpu,cpu=cpu0,name=time_idle value=42i\n+ cpu,cpu=cpu0,name=time_user value=43i\n\nMetric mode `metric`:\n\n- cpu,cpu=cpu0 time_idle=42i,time_user=43i\n+ time_idle,cpu=cpu0 value=42i\n+ time_user,cpu=cpu0 value=43i\n```\n\n----------------------------------------\n\nTITLE: Registering a Processor Plugin in Telegraf\nDESCRIPTION: Example of how to register a processor plugin in Telegraf's build system. This code should be placed in the plugins/processors/all directory with appropriate build tags for selective inclusion.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROCESSORS.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n//go:build !custom || processors || processors.printer\n\npackage all\n\nimport _ \"github.com/influxdata/telegraf/plugins/processors/printer\" // register plugin\n```\n\n----------------------------------------\n\nTITLE: Linting all plugin readmes\nDESCRIPTION: This command demonstrates how to use shell globbing to lint all README.md files located within the plugins directory. This example utilizes the relative path to the plugins directory from the location where `readme_linter` is executed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/readme_linter/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n```shell\ntelegraf/tools/readme_linter$ ./readme_linter ../../plugins/*/*/README.md\n```\n```\n\n----------------------------------------\n\nTITLE: Sudo Configuration for Varnish Access\nDESCRIPTION: Bash commands for configuring sudo access for the Telegraf user to execute varnishstat\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# Add the following line:\nCmnd_Alias VARNISHSTAT = /usr/bin/varnishstat\ntelegraf  ALL=(ALL) NOPASSWD: VARNISHSTAT\nDefaults!VARNISHSTAT !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Name Suffix in TOML\nDESCRIPTION: Configuration example showing how to use the name_suffix parameter with the CPU input plugin to emit measurements with the name 'cpu_total'. This example sets percpu to false and totalcpu to true.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  name_suffix = \"_total\"\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: Querying Example Plugin Data in InfluxDB SQL\nDESCRIPTION: This SQL query retrieves the maximum, mean, and minimum values of 'field1' from 'measurement1' for the last hour, grouped by tag. It's designed to work with data collected by the example plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/example/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT max(field1), mean(field1), min(field1) FROM measurement1 WHERE tag1=bar AND time > now() - 1h GROUP BY tag\n```\n\n----------------------------------------\n\nTITLE: Running Local Development Checks in Telegraf\nDESCRIPTION: Commands to run local checks before opening a pull request, including linting, dependency checks, tests and documentation generation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake lint\nmake check\nmake check-deps\nmake test\nmake docs\n```\n\n----------------------------------------\n\nTITLE: API Server Request Latency Metrics\nDESCRIPTION: Prometheus histogram metrics showing response latency distribution in microseconds for API server requests. Includes bucket definitions, sum and count for POST operations on bindings resource.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_histogram/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP apiserver_request_latencies_microseconds Response latency distribution in microseconds for each verb, resource and client.\n# TYPE apiserver_request_latencies_microseconds histogram\n# UNIT apiserver_request_latencies_microseconds microseconds\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"125000\"} 1994\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"250000\"} 1997\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"500000\"} 2000\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"1e+06\"} 2005\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"2e+06\"} 2012\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"4e+06\"} 2017\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"8e+06\"} 2024\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"+Inf\"} 2025\napiserver_request_latencies_microseconds_sum{resource=\"bindings\",verb=\"POST\"} 1.02726334e+08\napiserver_request_latencies_microseconds_count{resource=\"bindings\",verb=\"POST\"} 2025\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMetrics Parser in Telegraf\nDESCRIPTION: Basic TOML configuration for the OpenMetrics parser plugin using the file input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"openmetrics\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Logfmt Parser Plugin in Telegraf using TOML\nDESCRIPTION: This snippet demonstrates how to configure the logfmt parser plugin in Telegraf's configuration file. It specifies the input file, data format, and which keys should be collected as tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/logfmt/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"logfmt\"\n\n  ## Array of key names which should be collected as tags. Globs accepted.\n  logfmt_tag_keys = [\"method\",\"host\"]\n```\n\n----------------------------------------\n\nTITLE: Setting up sudo for Telegraf with Fail2ban\nDESCRIPTION: Bash command to edit the sudoers file for Telegraf using visudo, which is required when enabling sudo access for the plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fail2ban/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo visudo -f /etc/sudoers.d/telegraf\n```\n\n----------------------------------------\n\nTITLE: IMAP Command Statistics\nDESCRIPTION: Detailed metrics for IMAP commands including counts and durations for various operations like LIST, STATUS, SELECT, etc.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/dovecot/input.txt#2025-04-16_snippet_2\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP dovecot_imap_command Total number of all events of this kind\n# TYPE dovecot_imap_command counter\ndovecot_imap_command_total{cmd_name=\"LIST\"} 423\ndovecot_imap_command_total{cmd_name=\"LIST\",tagged_reply_state=\"OK\"} 423\n[...additional IMAP commands...]\n```\n\n----------------------------------------\n\nTITLE: JSON Payload with Replaced Reserved Label 'name'\nDESCRIPTION: This snippet demonstrates how the reserved 'name' label is replaced with '_name' to avoid conflicts with the metric name field in the Nebius Monitoring backend.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/nebius_cloud_monitoring/README.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"systemd_units_load_code\",\n  \"labels\": {\n    \"active\": \"active\",\n    \"host\": \"vm\",\n    \"load\": \"loaded\",\n    \"_name\": \"accounts-daemon.service\",\n    \"sub\": \"running\"\n  },\n  \"ts\": \"2023-06-06T11:10:50Z\",\n  \"value\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Querying NGINX Plus API HTTP Location Zone Metrics in InfluxDB Format\nDESCRIPTION: This snippet shows the metrics for an NGINX Plus API HTTP location zone. It includes various counters like requests, responses, and data transfer amounts.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_4\n\nLANGUAGE: influxdb\nCODE:\n```\nnginx_plus_api_http_location_zones,port=80,source=demo.nginx.com,zone=api-calls discarded=64i,received=337530181i,requests=1726513i,responses_1xx=0i,responses_2xx=1726428i,responses_3xx=0i,responses_4xx=21i,responses_5xx=0i,responses_total=1726449i,sent=1902577668i 1570696323000000000\n```\n\n----------------------------------------\n\nTITLE: Defining Token Failure Counter Metric in Prometheus Format\nDESCRIPTION: Defines a counter metric for tracking token acquisition failures with help text and a sample data point. Includes metric type definition, help text, and an actual counter value with timestamp and labels.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_counter/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE get_token_fail_count counter\n# HELP get_token_fail_count help\nget_token_fail_count_total 8 123 # {a=\"b\"} 0.5\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Basic NFS Metrics Output Example\nDESCRIPTION: Example output showing basic NFS metrics for READ and WRITE operations, including operations count, retransmissions, bytes transferred, and timing data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nfsclient/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nnfsstat,mountpoint=/NFS,operation=READ,serverexport=1.2.3.4:/storage/NFS ops=600i,retrans=1i,bytes=1207i,rtt=606i,exe=607i 1612651512000000000\nnfsstat,mountpoint=/NFS,operation=WRITE,serverexport=1.2.3.4:/storage/NFS bytes=1407i,rtt=706i,exe=707i,ops=700i,retrans=1i 1612651512000000000\n```\n\n----------------------------------------\n\nTITLE: Sample PF Metric Output in Telegraf\nDESCRIPTION: This snippet shows an example of the metric output from the PF plugin in Telegraf. It includes various counters such as entries, searches, inserts, and removals from the PF state table.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pf/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\npf,host=columbia entries=3i,searches=2668i,inserts=12i,removals=9i 1510941775000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Minimum-Maximum Aggregator in Telegraf using TOML\nDESCRIPTION: This snippet shows how to configure the Minimum-Maximum Aggregator plugin in Telegraf. It includes options for setting the aggregation period and whether to drop the original metric.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/minmax/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Keep the aggregate min/max of each metric passing through.\n[[aggregators.minmax]]\n  ## General Aggregator Arguments:\n  ## The period on which to flush & clear the aggregator.\n  # period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  # drop_original = false\n```\n\n----------------------------------------\n\nTITLE: Example Line Protocol Output Format\nDESCRIPTION: Sample output showing the Line Protocol format of temperature data collected by the plugin. Includes tags for channel, scale, title, and UUID, along with the temperature field value and timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fireboard/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfireboard,channel=2,host=patas-mbp,scale=Fahrenheit,title=telegraf-FireBoard,uuid=b55e766c-b308-49b5-93a4-df89fe31efd0 temperature=78.2 1561690040000000000\n```\n\n----------------------------------------\n\nTITLE: Riemann Event with Measurement Attribute\nDESCRIPTION: Example of Riemann event output when measurement_as_attribute is set to true.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/riemann/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n#riemann.codec.Event{ ...\n:measurement \"disk\", :service \"used_percent\", :metric 73.16736001949994,\n... :time 1475605021}\n```\n\n----------------------------------------\n\nTITLE: Example Output for Fritzbox Plugin Metrics\nDESCRIPTION: Sample output showing the format of metrics collected by the Fritzbox plugin. Includes device information, WAN statistics, PPP connection details, DSL line status, WLAN metrics, and host connection details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/fritzbox/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nfritzbox_device,service=DeviceInfo1,source=fritz.box uptime=2058438i,model_name=\"Mock 1234\",serial_number=\"123456789\",hardware_version=\"Mock 1234\",software_version=\"1.02.03\" 1737003520174438000\n\nfritzbox_wan,service=WANCommonInterfaceConfig1,source=fritz.box layer1_upstream_max_bit_rate=48816000i,layer1_downstream_max_bit_rate=253247000i,upstream_current_max_speed=511831i,downstream_current_max_speed=1304268i,total_bytes_sent=129497283207i,total_bytes_received=554484531337i 1737003587690504000\n\nfritzbox_ppp,service=WANPPPConnection1,source=fritz.box uptime=369434i,upstream_max_bit_rate=44213433i,downstream_max_bit_rate=68038668i 1737003622308149000\n\nfritzbox_dsl,service=WANDSLInterfaceConfig1,source=fritz.box,status=Up downstream_curr_rate=249065i,downstream_max_rate=249065i,downstream_power=513i,init_timeouts=0i,atuc_crc_errors=13i,errored_secs=25i,atuc_hec_errors=0i,upstream_noise_margin=80i,downstream_noise_margin=60i,downstream_attenuation=140i,receive_blocks=490282831i,transmit_blocks=254577751i,init_errors=0i,crc_errors=53i,fec_errors=0i,hec_errors=0i,upstream_max_rate=48873i,upstream_attenuation=80i,upstream_power=498i,cell_delin=0i,link_retrain=2i,loss_of_framing=0i,upstream_curr_rate=46719i,severly_errored_secs=0i,atuc_fec_errors=0i 1737003645769642000\n\nfritzbox_wlan,band=2400,channel=13,service=WLANConfiguration1,source=fritz.box,ssid=MOCK1234,status=Up total_associations=11i 1737003673561198000\n\nfritzbox_hosts,node=device#17,node_ap=device#1,node_ap_role=master,node_role=slave,link_name=AP:2G:0,link_type=WLAN,service=Hosts1,source=fritz.box cur_data_rate_tx=216000i,cur_data_rate_rx=216000i,max_data_rate_tx=216000i,max_data_rate_rx=216000i 1737003707257394000\nfritzbox_hosts,node=device#24,node_ap=device#17,node_ap_role=slave,node_role=client,link_name=LAN:1,link_type=LAN,service=Hosts1,source=fritz.box max_data_rate_tx=1000000i,max_data_rate_rx=1000000i,cur_data_rate_tx=0i,cur_data_rate_rx=0i 1737003707257248000\n```\n\n----------------------------------------\n\nTITLE: Querying Azure SQL Elastic Pool Metrics\nDESCRIPTION: Resource usage and performance metrics collection at the Elastic Pool level, including pool-specific resource statistics and governance settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nsys.dm_resource_governor_resource_pools_history_ex\\nsys.dm_user_db_resource_governance\\nsys.dm_io_virtual_file_stats\\nsys.dm_os_wait_stats\\nsys.dm_os_memory_clerks\\nsys.dm_os_performance_counters\\nsys.dm_os_schedulers\n```\n\n----------------------------------------\n\nTITLE: Generating UTF-8 Encoded Configuration in PowerShell\nDESCRIPTION: PowerShell command to generate a Telegraf configuration file with proper UTF-8 encoding to avoid encoding issues in PowerShell v5.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf.exe config | Out-File -Encoding utf8 telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: DPDK Plugin Configuration\nDESCRIPTION: Example TOML configuration for the DPDK plugin showing device types, additional commands, and custom tags setup.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dpdk/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.dpdk]]\n  device_types = [\"ethdev\"]\n  additional_commands = [\"/l3fwd-power/stats\"]\n  metadata_fields = []\n  [inputs.dpdk.tags]\n    dpdk_instance = \"l3fwd-power\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing CPU Profile with Go Tool\nDESCRIPTION: Use the Go pprof tool to analyze the collected CPU profile, showing the top 5 CPU consumers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngo tool pprof cpu.prof\n(pprof) top5\n```\n\n----------------------------------------\n\nTITLE: CLI Flag Definition for URL Config Watch Interval in Telegraf\nDESCRIPTION: Defines the '--config-url-watch-interval' CLI flag for Telegraf that sets the time interval for checking updates to URL-based configuration files, which is disabled by default (0s).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-007-url-config-behavior.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n--config-url-watch-interval=0s  Time duration to check for updates to URL based\n                                configuration files. Disabled by default.\n```\n\n----------------------------------------\n\nTITLE: Converting Multiple Fields to Integer Type in Telegraf\nDESCRIPTION: Example showing how to convert all fields matching the pattern 'scboard_*' to integer type, with before and after comparison.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/converter/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.converter]]\n  [processors.converter.fields]\n    integer = [\"scboard_*\"]\n```\n\n----------------------------------------\n\nTITLE: Scanning for SMART Devices using smartctl\nDESCRIPTION: Command to scan for SMART devices using the smartctl utility.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsmartctl --scan\n```\n\n----------------------------------------\n\nTITLE: MongoDB Shard Statistics Structure\nDESCRIPTION: Specifies metrics related to MongoDB sharding status and health.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n- mongodb_shard_stats\n  - tags:\n    - hostname\n  - fields:\n    - in_use (integer)\n    - available (integer)\n    [...]\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Tag Limit Processor in Diff Format\nDESCRIPTION: This example demonstrates how the 'tag_limit' processor affects the data. It shows the data before and after applying the tag limit, where the initial set of tags is reduced according to the specified configuration. This snippet is used for illustrating the result of applying the processor, highlighting how tags are filtered based on the defined preferences.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/tag_limit/README.md#2025-04-16_snippet_1\n\nLANGUAGE: Diff\nCODE:\n```\n+ throughput month=Jun,environment=qa,region=us-east1,lower=10i,upper=1000i,mean=500i 1560540094000000000\\n+ throughput environment=qa,region=us-east1,lower=10i 1560540094000000000\n```\n\n----------------------------------------\n\nTITLE: Defining powerstat_core Measurement Tags in Markdown\nDESCRIPTION: This snippet defines the tags returned with powerstat_core measurements, including package_id, core_id, and cpu_id.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_10\n\nLANGUAGE: markdown\nCODE:\n```\n| Tag          | Description                    |\n|--------------|--------------------------------|\n| `package_id` | ID of platform package/socket. |\n| `core_id`    | ID of physical processor core. |\n| `cpu_id`     | ID of logical processor core.  |\n```\n\n----------------------------------------\n\nTITLE: Binding RBAC Roles in Kubernetes\nDESCRIPTION: YAML configuration for binding the aggregated ClusterRole to service accounts in Kubernetes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: influx:telegraf:viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: influx:telegraf\nsubjects:\n  - kind: ServiceAccount\n    name: telegraf\n    namespace: default\n```\n\n----------------------------------------\n\nTITLE: Converting String Data to Numeric Values with Enum Processor\nDESCRIPTION: Example configuration for using the enum processor to convert string values from MySQL fields (like 'slave_slave_io_running') into numeric values. This is particularly useful for outputs like Prometheus that require numeric data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mysql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.enum]]\n  namepass = \"mysql\"\n  [[processors.enum.mapping]]\n    field = \"slave_slave_io_running\"\n    dest = \"slave_slave_io_running_int\"\n    default = 4\n    [processors.enum.mapping.value_mappings]\n      Yes = 0\n      No = 1\n      Preparing = 2\n      Connecting = 3\n```\n\n----------------------------------------\n\nTITLE: Field Layout Configuration in TOML\nDESCRIPTION: Configuration example for using the 'field' layout option in the MQTT output plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mqtt/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.mqtt]]\n  topic = 'telegraf/{{ .PluginName }}/{{ .Tag \"source\" }}'\n  layout = \"field\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables and Service Paths for Telegraf\nDESCRIPTION: This configuration defines environment variables with the 'E:' prefix and service paths with the 'S:' prefix. Environment variables MY_PARAM_1 and MY_PARAM_2 are assigned values, while service paths point to 'foo/bar/devlink' locations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/diskio/testdata/udev.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nE:MY_PARAM_1=myval1\nE:MY_PARAM_2=myval2\nS:foo/bar/devlink\nS:foo/bar/devlink1\n```\n\n----------------------------------------\n\nTITLE: Visualizing Telegraf Data Flow with Processors and Aggregators\nDESCRIPTION: ASCII diagram illustrating the flow of data in Telegraf, showing how data moves from various input plugins through processors and aggregators before reaching output plugins.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/AGGREGATORS_AND_PROCESSORS.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\n           \n    CPU    \n              \n   \n                \n                                                 \n                                                                       \n  Memory                                              InfluxDB  \n                                                                      \n                  \n                                      Aggregators     \n       Processors         - mean            \n                   - transform       - quantiles                  \n   MySQL    - decorate   - min/max      File    \n                   - filter          - count                      \n                                            \n                            \n                                                \n                                                                      \n   SNMP                                                 Kafka   \n                                                                       \n                                                 \n                \n   \n              \n  Docker   \n           \n\n```\n\n----------------------------------------\n\nTITLE: Setting Posix ACL Permissions for Telegraf\nDESCRIPTION: Shell commands to set up Posix ACL permissions for the Telegraf user to access Postfix queue directories. This method uses setfacl to grant read and execute permissions recursively.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postfix/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo setfacl -Rm g:telegraf:rX /var/spool/postfix/\nsudo setfacl -dm g:telegraf:rX /var/spool/postfix/\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PowerStat with No Per-CPU Telemetry\nDESCRIPTION: This configuration enables only the default processor package specific metrics without collecting any per-CPU metrics by setting cpu_metrics to an empty array.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_powerstat]]\n  cpu_metrics = []\n```\n\n----------------------------------------\n\nTITLE: Telegraf Supervisor Plugin Example Output\nDESCRIPTION: This snippet presents an example of the output generated by the Telegraf Supervisor input plugin.  It showcases the `supervisor_processes` and `supervisor_instance` metrics, including their tags and fields, demonstrating the data that is collected and reported.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/supervisor/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsupervisor_processes,group=ExampleGroup,id=supervisor,port=9001,process=ExampleProcess,source=localhost state=20i,uptime=75958i 1659786637000000000\nsupervisor_instance,id=supervisor,port=9001,source=localhost state=1i 1659786637000000000\n\n```\n\n----------------------------------------\n\nTITLE: Defining CAdvisor Version Metric in Prometheus Format\nDESCRIPTION: Defines a Prometheus info metric named 'cadvisor_version' that exports version information for CAdvisor, Docker, kernel and OS as labels with a constant value of 1. This metric type is commonly used for tracking version metadata in monitoring systems.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_info/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP cadvisor_version A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\n# TYPE cadvisor_version info\ncadvisor_version{cadvisorRevision=\"\",cadvisorVersion=\"\",dockerVersion=\"1.8.2\",kernelVersion=\"3.10.0-229.20.1.el7.x86_64\",osVersion=\"CentOS Linux 7 (Core)\"} 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf File Input for XML Parsing with XPath\nDESCRIPTION: Shows configuration in TOML for parsing the 'example.xml' file using XPath. It details setting up the gateway name, sequence number, and status using XPath functions and type conversions to extract and format the desired data fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.xml\"]\n  data_format = \"xml\"\n\n  [[inputs.file.xpath]]\n    [inputs.file.xpath.tags]\n      gateway = \"substring-before(/Gateway/Name, ' ')\"\n\n    [inputs.file.xpath.fields_int]\n      seqnr = \"/Gateway/Sequence\"\n\n    [inputs.file.xpath.fields]\n      ok = \"/Gateway/Status = 'ok'\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel RDT Input Plugin in Telegraf\nDESCRIPTION: TOML configuration snippet for setting up the Intel RDT input plugin in Telegraf. It includes options for sampling interval, pqos path, metric selection, CPU core groups, process monitoring, and sudo usage.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_rdt/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Intel RDT metrics\n# This plugin ONLY supports non-Windows\n[[inputs.intel_rdt]]\n  ## Optionally set sampling interval to Nx100ms.\n  ## This value is propagated to pqos tool. Interval format is defined by pqos itself.\n  ## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.\n  # sampling_interval = \"10\"\n\n  ## Optionally specify the path to pqos executable.\n  ## If not provided, auto discovery will be performed.\n  # pqos_path = \"/usr/local/bin/pqos\"\n\n  ## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.\n  ## If not provided, default value is false.\n  # shortened_metrics = false\n\n  ## Specify the list of groups of CPU core(s) to be provided as pqos input.\n  ## Mandatory if processes aren't set and forbidden if processes are specified.\n  ## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n  # cores = [\"0-3\"]\n\n  ## Specify the list of processes for which Metrics will be collected.\n  ## Mandatory if cores aren't set and forbidden if cores are specified.\n  ## e.g. [\"qemu\", \"pmd\"]\n  # processes = [\"process\"]\n\n  ## Specify if the pqos process should be called with sudo.\n  ## Mandatory if the telegraf process does not run as root.\n  # use_sudo = false\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf with Nginx Input Plugin\nDESCRIPTION: Command to run Telegraf with the Nginx input plugin in test mode. This allows for verification of the configuration and visualization of the output format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./telegraf --config telegraf.conf --input-filter nginx --test\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Aggregator Plugin in Telegraf using TOML\nDESCRIPTION: Sample TOML configuration for the Merge Aggregator Plugin. It includes settings for the flush period, timestamp rounding precision, and an option to drop original metrics after merging.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/merge/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Merge metrics into multifield metrics by series key\n[[aggregators.merge]]\n  ## General Aggregator Arguments:\n  ## The period on which to flush & clear the aggregator.\n  # period = \"30s\"\n\n  ## Precision to round the metric timestamp to\n  ## This is useful for cases where metrics to merge arrive within a small\n  ## interval and thus vary in timestamp. The timestamp of the resulting metric\n  ## is also rounded.\n  # round_timestamp_to = \"1ns\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  drop_original = true\n```\n\n----------------------------------------\n\nTITLE: Example Output of Synproxy Metrics in Line Protocol\nDESCRIPTION: This section provides an example of the output format in Line Protocol for the synproxy metrics collected. It showcases how metric values are represented along with relevant tags and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/synproxy/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nsynproxy,host=Filter-GW01,rack=filter-node1 conn_reopened=0i,cookie_invalid=235i,cookie_retrans=0i,cookie_valid=8814i,entries=0i,syn_received=8742i 1549550634000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Templates with Glob Patterns in TOML\nDESCRIPTION: Configuration example using multiple templates with glob patterns to apply different transformations based on the metric name prefix.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\ntemplates = [\n    \"cpu.* measurement.measurement.region\",\n    \"mem.* measurement.measurement.host\"\n]\n```\n\n----------------------------------------\n\nTITLE: DPDK Plugin Output Format\nDESCRIPTION: Shows the basic structure of the DPDK plugin output with tags for host, instance, command, and parameters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dpdk/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndpdk,host=dpdk-host,dpdk_instance=l3fwd-power,command=/ethdev/stats,params=0 [fields] [timestamp]\n```\n\n----------------------------------------\n\nTITLE: Example Lustre Metrics Output in Telegraf\nDESCRIPTION: Sample output demonstrating the format of Lustre metrics collected by the Telegraf plugin, showing job statistics for OST and MDS systems\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lustre2/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nlustre2,host=oss2,jobid=42990218,name=wrk-OST0041 jobstats_ost_setattr=0i,jobstats_ost_sync=0i,jobstats_punch=0i,jobstats_read_bytes=4096i,jobstats_read_calls=1i,jobstats_read_max_size=4096i,jobstats_read_min_size=4096i,jobstats_write_bytes=310206488i,jobstats_write_calls=7423i,jobstats_write_max_size=53048i,jobstats_write_min_size=8820i 1556525847000000000\nlustre2,host=mds1,jobid=42992017,name=wrk-MDT0000 jobstats_close=31798i,jobstats_crossdir_rename=0i,jobstats_getattr=34146i,jobstats_getxattr=15i,jobstats_link=0i,jobstats_mkdir=658i,jobstats_mknod=0i,jobstats_open=31797i,jobstats_rename=0i,jobstats_rmdir=0i,jobstats_samedir_rename=0i,jobstats_setattr=1788i,jobstats_setxattr=0i,jobstats_statfs=0i,jobstats_sync=0i,jobstats_unlink=0i 1556525828000000000\n```\n\n----------------------------------------\n\nTITLE: Testing PuppetAgent Input Plugin with Telegraf\nDESCRIPTION: Demonstrates running Telegraf with the PuppetAgent input plugin and displays the collected metrics. This command executes Telegraf in test mode, filtering for the PuppetAgent plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/puppetagent/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\njcross@pit-devops-02 ~ >sudo ./telegraf_linux_amd64 --input-filter puppetagent --config tele.conf --test\n* Plugin: puppetagent, Collection 1\n> [] puppetagent_events_failure value=0\n> [] puppetagent_events_total value=0\n> [] puppetagent_events_success value=0\n> [] puppetagent_resources_failed value=0\n> [] puppetagent_resources_scheduled value=0\n> [] puppetagent_resources_changed value=0\n> [] puppetagent_resources_skipped value=0\n> [] puppetagent_resources_total value=109\n> [] puppetagent_resources_failedtorestart value=0\n> [] puppetagent_resources_restarted value=0\n> [] puppetagent_resources_outofsync value=0\n> [] puppetagent_changes_total value=0\n> [] puppetagent_time_user value=0.00393\n> [] puppetagent_time_schedule value=0.001234\n> [] puppetagent_time_filebucket value=0.000244\n> [] puppetagent_time_file value=0.587734\n> [] puppetagent_time_exec value=0.389584\n> [] puppetagent_time_anchor value=0.000399\n> [] puppetagent_time_sshauthorizedkey value=0.000655\n> [] puppetagent_time_service value=0\n> [] puppetagent_time_package value=1.297537\n> [] puppetagent_time_total value=9.45297606225586\n> [] puppetagent_time_configretrieval value=5.89822006225586\n> [] puppetagent_time_lastrun value=1444940131\n> [] puppetagent_time_cron value=0.000646\n> [] puppetagent_version_config value=1444940121\n> [] puppetagent_version_puppet value=3.7.5\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes Pod Container Metrics with Telegraf\nDESCRIPTION: This snippet logs metrics about a specific container within a Kubernetes Pod, including its resource requests and limits.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_pod_container,condition=Ready,host=vjain,pod_name=uefi-5997f76f69-xzljt,status=True status_condition=1i 1629177981000000000\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_pod_container,container_name=telegraf,namespace=default,node_name=ip-172-17-0-2.internal,node_selector_node-role.kubernetes.io/compute=true,pod_name=tick1,phase=Running,state=running,readiness=ready resource_requests_cpu_units=0.1,resource_limits_memory_bytes=524288000,resource_limits_cpu_units=0.5,restarts_total=0i,state_code=0i,state_reason=\"\",phase_reason=\"\",resource_requests_memory_bytes=524288000 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Displaying Deprecation Warning to Users (Text Example)\nDESCRIPTION: Example of how deprecation warnings are displayed to users at Telegraf startup, providing information about which plugin is deprecated, when it was deprecated, when it will be removed, and what to use instead.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-001-deprecation.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPlugin \"inputs.logparser\" deprecated since version 1.15.0 and will be removed in 1.40.0: use 'inputs.tail' with 'grok' data format instead\n```\n\n----------------------------------------\n\nTITLE: Configuring Twemproxy Input Plugin in Telegraf\nDESCRIPTION: Configuration snippet for the Twemproxy input plugin that specifies the server address and pools to monitor. Requires specifying the stats address and list of pool names to collect metrics from.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/twemproxy/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read Twemproxy stats data\n[[inputs.twemproxy]]\n  ## Twemproxy stats address and port (no scheme)\n  addr = \"localhost:22222\"\n  ## Monitor pool name\n  pools = [\"redis_pool\", \"mc_pool\"]\n```\n\n----------------------------------------\n\nTITLE: Sample Output for Windows Failover Cluster WMI Query in Telegraf\nDESCRIPTION: This snippet demonstrates the output format for the Windows Failover Cluster WMI query configuration. It shows the Dynamic Quorum status along with tags for the cluster name and quorum type.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nwin_wmi_MSCluster_Cluster,Name=testcluster1,QuorumType=Node\\ and\\ File\\ Share\\ Majority,host=testnode1 DynamicQuorumEnabled=1i 1671553260000000000\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Tacacs Input Plugin\nDESCRIPTION: Example of the metrics output format from the Tacacs plugin. Shows the response time in milliseconds and the authentication status from a TACACS+ server at 127.0.0.1:49.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tacacs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntacacs,source=127.0.0.1:49 responsetime_ms=311i,response_status=\"AuthenStatusPass\" 1677526200000000000\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Server Metric Output in Telegraf\nDESCRIPTION: Shows example output for SQL Server metrics collected by Telegraf. Includes CPU usage and database file size metrics with various tags for identification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nsqlserver_cpu_other_process_cpu{host=\"servername\",measurement_db_type=\"SQLServer\",sql_instance=\"SERVERNAME:INST\"} 9\nsqlserver_performance{counter=\"Log File(s) Size (KB)\",counter_type=\"65792\",host=\"servername\",instance=\"instance_name\",measurement_db_type=\"SQLServer\",object=\"MSSQL$INSTANCE_NAME:Databases\",sql_instance=\"SERVERNAME:INSTANCE_NAME\"} 1.048568e+06\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Output Example\nDESCRIPTION: Diff showing the transformation of metrics before and after batch processing, demonstrating how the processor adds batch tags in a round-robin pattern across three batches.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/batch/README.md#2025-04-16_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- temperature cpu=25\n- temperature cpu=50\n- temperature cpu=75\n- temperature cpu=25\n- temperature cpu=50\n- temperature cpu=75\n+ temperature,batch=0 cpu=25\n+ temperature,batch=1 cpu=50\n+ temperature,batch=2 cpu=75\n+ temperature,batch=0 cpu=25\n+ temperature,batch=1 cpu=50\n+ temperature,batch=2 cpu=75\n```\n\n----------------------------------------\n\nTITLE: Configuring Starlark Aggregator Plugin in Telegraf\nDESCRIPTION: Configuration example in TOML format showing how to set up the Starlark aggregator plugin with an inline script that tracks the last metric seen. The configuration demonstrates script definition, file referencing options, and constant declaration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/starlark/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Aggregate metrics using a Starlark script\n[[aggregators.starlark]]\n  ## The Starlark source can be set as a string in this configuration file, or\n  ## by referencing a file containing the script.  Only one source or script\n  ## should be set at once.\n  ##\n  ## Source of the Starlark script.\n  source = '''\nstate = {}\n\ndef add(metric):\n  state[\"last\"] = metric\n\ndef push():\n  return state.get(\"last\")\n\ndef reset():\n  state.clear()\n'''\n\n  ## File containing a Starlark script.\n  # script = \"/usr/local/bin/myscript.star\"\n\n  ## The constants of the Starlark script.\n  # [aggregators.starlark.constants]\n  #   max_size = 10\n  #   threshold = 0.75\n  #   default_name = \"Julia\"\n  #   debug_mode = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Linux Sysctl FS Input Plugin in TOML\nDESCRIPTION: This snippet shows the basic configuration of the `linux_sysctl_fs` input plugin in a Telegraf configuration file. It does not require any specific configuration parameters to function.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/linux_sysctl_fs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Provides Linux sysctl fs metrics\n[[inputs.linux_sysctl_fs]]\n  # no configuration\n\n```\n\n----------------------------------------\n\nTITLE: Granting MongoDB User Permissions\nDESCRIPTION: This snippet shows how to grant the necessary permissions to a MongoDB user for the Telegraf plugin to collect metrics. It grants the \"read\" role with the \"find\" action on the \"local\" database. This is needed when MongoDB has access control enabled.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mongodb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n> db.grantRolesToUser(\"user\", [{role: \"read\", actions: \"find\", db: \"local\"}])\n\n```\n\n----------------------------------------\n\nTITLE: Sample Metrics Format for Field Layout\nDESCRIPTION: Example showing the input metric format and resulting MQTT topics when using the 'field' layout option.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/mqtt/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmodbus,location=main\\ building,source=device\\ 1,status=ok,type=Machine\\ A temperature=21.4,serial\\ number=\"324nlk234r5u9834t\",working\\ hours=123i,supplied=true 1676522982000000000\nmodbus,location=main\\ building,source=device\\ 2,status=offline,type=Machine\\ B temperature=25.0,supplied=true 1676522982000000000\n```\n\n----------------------------------------\n\nTITLE: Enum Processor Configuration in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates how to use the `enum` processor in Telegraf to map boolean values to strings. Specifically, it shows how to convert the `ups_beeper_status` field's `true` and `false` values to `enabled` and `disabled` strings, respectively.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/upsd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.enum]]\n  [[processors.enum.mapping]]\n    field = \"ups_beeper_status\"\n    [processors.enum.mapping.value_mappings]\n      true = \"enabled\"\n      false = \"disabled\"\n\n```\n\n----------------------------------------\n\nTITLE: CSV Key-Values Example for Lookup Tables\nDESCRIPTION: A practical example of the CSV format with keys and values, showing how hosts map to location and rack information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_6\n\nLANGUAGE: csv\nCODE:\n```\n-,location,rack\nxyzzy-green,eu-central,C12-01\nxyzzy-red,us-west,C01-42\n```\n\n----------------------------------------\n\nTITLE: Preserving Newlines in Tail Input Plugin\nDESCRIPTION: Adds an option to preserve newlines for multiline data in the tail input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_9\n\nLANGUAGE: go\nCODE:\n```\n[[inputs.tail]]\n  preserve_newlines = true\n```\n\n----------------------------------------\n\nTITLE: Example Kibana Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the Kibana input plugin, including heap usage, connection stats, and response times.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kibana/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nkibana,host=myhost,name=my-kibana,source=localhost:5601,status=green,version=6.5.4 concurrent_connections=8i,heap_max_bytes=447778816i,heap_total_bytes=447778816i,heap_used_bytes=380603352i,requests_per_sec=1,response_time_avg_ms=57.6,response_time_max_ms=220i,status_code=1i,uptime_ms=6717489805i 1534864502000000000\n```\n\n----------------------------------------\n\nTITLE: Enabling Sudo for Unbound Plugin\nDESCRIPTION: This TOML configuration snippet demonstrates how to enable sudo for the Unbound input plugin in Telegraf.  This is necessary if the telegraf user does not have direct permissions to execute `unbound-control`. Setting `use_sudo` to `true` will prefix the unbound-control command with sudo.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/unbound/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.unbound]]\n  use_sudo = true\n```\n\n----------------------------------------\n\nTITLE: Accessing Heap Profile\nDESCRIPTION: Command to analyze heap memory usage using Go's pprof tool\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROFILING.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngo tool pprof http://localhost:6060/debug/pprof/heap\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Metric Format for Google Cloud Storage Input\nDESCRIPTION: Example of JSON-formatted metrics that reside on Google Cloud Storage. Shows the structure with fields, name, tags, and timestamp that the plugin will process.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/google_cloud_storage/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metrics\": [\n    {\n      \"fields\": {\n        \"cosine\": 10,\n        \"sine\": -1.0975806427415925e-12\n      },\n      \"name\": \"cpu\",\n      \"tags\": {\n        \"datacenter\": \"us-east-1\",\n        \"host\": \"localhost\"\n      },\n      \"timestamp\": 1604148850990\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example SNMP Trap Output Format\nDESCRIPTION: Sample output showing the format of SNMP trap metrics including tags and fields for different trap types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp_trap/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsnmp_trap,mib=SNMPv2-MIB,name=coldStart,oid=.1.3.6.1.6.3.1.1.5.1,source=192.168.122.102,version=2c,community=public snmpTrapEnterprise.0=\"linux\",sysUpTimeInstance=1i 1574109187723429814\nsnmp_trap,mib=NET-SNMP-AGENT-MIB,name=nsNotifyShutdown,oid=.1.3.6.1.4.1.8072.4.0.2,source=192.168.122.102,version=2c,community=public sysUpTimeInstance=5803i,snmpTrapEnterprise.0=\"netSnmpNotificationPrefix\" 1574109186555115459\n```\n\n----------------------------------------\n\nTITLE: Querying SQL Server Performance Metrics - Version 2\nDESCRIPTION: Enhanced version of SQL Server metrics collection that includes more detailed performance counters, availability groups metrics, and resource governor statistics. This version is deprecated as of Telegraf 1.16.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nsys.dm_io_virtual_file_stats\\nsys.dm_os_memory_clerks\\nsys.dm_os_performance_counters\\nsys.dm_os_schedulers\\nsys.dm_exec_requests\\nsys.dm_exec_sessions\\nsys.dm_os_volume_stats\\nsys.dm_os_ring_buffers\n```\n\n----------------------------------------\n\nTITLE: Carbon2 Format with Included Field Names\nDESCRIPTION: Example of Carbon2 format when using the \"metric_includes_field\" configuration option. Demonstrates how metric names incorporate the field name after an underscore.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/carbon2/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmetric=name_field_1 host=foo  30 1234567890\nmetric=name_field_2 host=foo  4 1234567890\nmetric=name_field_N host=foo  59 1234567890\n```\n\n----------------------------------------\n\nTITLE: Intel CPU Support Matrix in Markdown Table\nDESCRIPTION: Detailed markdown table showing Intel CPU models (from Nehalem to AlderLake) and their supported monitoring features including C-state residency monitoring, CPU temperature monitoring, base frequency monitoring, and uncore frequency monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Model number | Processor name                  | `cpu_c1_state_residency`<br/>`cpu_c6_state_residency`<br/>`cpu_temperature`<br/>`cpu_base_frequency` | `cpu_c3_state_residency` | `cpu_c7_state_residency` | `uncore_frequency` |\n|--------------|---------------------------------|:----------------------------------------------------------------------------------------------------:|:------------------------:|:------------------------:|:------------------:|\n| 0x1E         | Intel Nehalem                   |                                                                                                     |                         |                          |                    |\n```\n\n----------------------------------------\n\nTITLE: Implementing the 'add' Function in Starlark Aggregator\nDESCRIPTION: Example of the required 'add' function that stores each incoming metric in the state dictionary. This function is called for each metric that needs to be aggregated.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/starlark/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add(metric):\n  state[\"last\"] = metric\n```\n\n----------------------------------------\n\nTITLE: AWS EC2 Tag Modification Example\nDESCRIPTION: Diff showing the before and after effect of applying AWS EC2 metadata tags to a metric.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/aws_ec2/README.md#2025-04-16_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- cpu,hostname=localhost time_idle=42\n+ cpu,hostname=localhost,accountId=123456789,instanceId=i-123456789123 time_idle=42\n```\n\n----------------------------------------\n\nTITLE: Creating ClickHouse Table with Spaces in Names\nDESCRIPTION: Creates a ClickHouse table 'metric three' with DateTime and String columns, using backticks to escape spaces in identifiers. Uses MergeTree engine ordered by timestamp with an index granularity of 8192.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sql/testdata/clickhouse/expected.txt#2025-04-16_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE foo.`metric three`\n(\n    `timestamp` DateTime,\n    `tag four` String,\n    `string two` String\n)\nENGINE = MergeTree\nORDER BY timestamp\nSETTINGS index_granularity = 8192\n```\n\n----------------------------------------\n\nTITLE: Nvidia SMI Troubleshooting Commands\nDESCRIPTION: Shell commands for troubleshooting the nvidia-smi plugin on Linux and Windows platforms.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nvidia_smi/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo -u telegraf -- /usr/bin/nvidia-smi -q -x\n```\n\nLANGUAGE: sh\nCODE:\n```\n\"C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe\" -q -x\n```\n\n----------------------------------------\n\nTITLE: Testing RPM or DEB with Multiple Images using LXD\nDESCRIPTION: This snippet shows the command to test an RPM or DEB package against a set of images without specifying individual images. This is useful for running broader tests across various distributions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/package_incus_test/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./package-test-lxd --package telegraf_1.21.4-1_amd64.deb\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Peer Status Information in Plaintext\nDESCRIPTION: This output shows the status of NTP time sources with their synchronization state, reference IDs, polling information, network delays, time offsets, and jitter measurements. The output includes both local hardware clock sources (SHM) and remote NTP servers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/issue_2386/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*SHM(0)          .PPS.                          1 u   60  64   377    0.000    0.045   1.012\n+37.58.57.238 (d 192.53.103.103\t\t\t2 u   10 1024  377    1.748    0.373   0.101\n+37.58.57.238 (domain) 192.53.103.103   2 u   10 1024  377    1.748    0.373   0.101\n+37.58.57.238 ( 192.53.103.103\t\t\t2 u   10 1024  377    1.748    0.373   0.101\n-SHM(1)          .GPS.                          1 u   121 128  377    0.000   10.105   2.012\n```\n\n----------------------------------------\n\nTITLE: Publishing Event with JSON Data in Particle Device\nDESCRIPTION: This C++ code snippet illustrates how to format and publish an event containing JSON-formatted data from a Particle device. It uses `String::format` to create the JSON structure before sending it via the Particle.publish method.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/particle/README.md#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nString data = String::format(\"{ \\\"tags\\\" : {\\n     \\\"tag_name\\\": \\\"tag_value\\\" ,\\n     \\\"other_tag\\\": \\\"other_value\\\"\\n    },\\n \\\"values\\\": {\\n     \\\"value_name\\\": %f,\\n  \\\"other_value\\\": %f,\\n    }\\n    }\",  value_value, other_value);\n    Particle.publish(\"event_name\", data, PRIVATE);\n```\n\n----------------------------------------\n\nTITLE: Binary Message Parsing Output Example\nDESCRIPTION: Example output showing the parsed metrics for all three message types, including timestamps, tags, and field values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/binary/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nmetricA,address=383,failure=false count=42i,value=3.1415 1658835984000000000\nmetricB value=3737169374i 1658847037000000000\nmetricC x=2.718280076980591,y=0.0000000000000000000000000000000006626070178575745 1658835984000000000\n```\n\n----------------------------------------\n\nTITLE: Defining powerstat_package Measurement Metrics in Markdown\nDESCRIPTION: This snippet lists the available metrics for the powerstat_package measurement, including power consumption, turbo frequency, and uncore frequency limits.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric name (field)                    | Description                                                                                                                                                                                                                                                                                                                                                                  | Units |\n|----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|\n| `thermal_design_power_watts`           | Maximum Thermal Design Power (TDP) available for processor package.                                                                                                                                                                                                                                                                                                          | Watts |\n| `current_power_consumption_watts`      | Current power consumption of processor package.                                                                                                                                                                                                                                                                                                                              | Watts |\n| `current_dram_power_consumption_watts` | Current power consumption of processor package DRAM subsystem.                                                                                                                                                                                                                                                                                                               | Watts |\n| `max_turbo_frequency_mhz`              | Maximum reachable turbo frequency for number of cores active.                                                                                                                                                                                                                                                                                                                | MHz   |\n| `uncore_frequency_limit_mhz_min`       | Minimum uncore frequency limit for die in processor package.                                                                                                                                                                                                                                                                                                                 | MHz   |\n| `uncore_frequency_limit_mhz_max`       | Maximum uncore frequency limit for die in processor package.                                                                                                                                                                                                                                                                                                                 | MHz   |\n```\n\n----------------------------------------\n\nTITLE: Configuring Wireless Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet demonstrates how to configure the wireless plugin in Telegraf to monitor Wi-Fi signal strength and quality on Linux systems. It shows how to optionally specify the path to the `/proc` directory.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/wireless/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Monitor wifi signal strength and quality\n# This plugin ONLY supports Linux\n[[inputs.wireless]]\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n```\n\n----------------------------------------\n\nTITLE: Executing License Checker with Whitelist\nDESCRIPTION: Runs the license verification tool using a version-controlled whitelist. The command checks against the whitelist file for exceptions on license classifications, identifying discrepancies with an 'ERR:' prefix.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/license_checker/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./tools/license_checker/license_checker \\\n              -whitelist ./tools/license_checker/data/whitelist\n```\n\n----------------------------------------\n\nTITLE: Example Trig Plugin Output\nDESCRIPTION: This example output showcases the data produced by the 'trig' input plugin. It includes 'sine' and 'cosine' fields with their corresponding float values, along with the 'host' tag and timestamp. This data can be used for visualization or further processing in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/trig/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"trig,host=MBP15-SWANG.local cosine=10,sine=0 1632338680000000000\ntrig,host=MBP15-SWANG.local sine=5.877852522924732,cosine=8.090169943749473 1632338690000000000\ntrig,host=MBP15-SWANG.local sine=9.510565162951535,cosine=3.0901699437494745 1632338700000000000\"\n```\n\n----------------------------------------\n\nTITLE: Systemd Capabilities Configuration for Telegraf\nDESCRIPTION: Systemd service configuration to grant necessary capabilities (CAP_NET_RAW and CAP_NET_ADMIN) to Telegraf for iptables access.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/iptables/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW CAP_NET_ADMIN\nAmbientCapabilities=CAP_NET_RAW CAP_NET_ADMIN\n```\n\n----------------------------------------\n\nTITLE: Retrieving an SNMP Table with snmptable\nDESCRIPTION: Shell command showing how to request an entire SNMP table using the snmptable utility. This is useful for exploring available table data before configuring Telegraf collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nsnmptable -v2c -c public 127.0.0.1 ifTable\n```\n\n----------------------------------------\n\nTITLE: Collecting Memory Profile in Telegraf\nDESCRIPTION: Use curl to collect a heap memory profile from Telegraf. This command also saves the Telegraf version and Go environment information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl 'http://localhost:6060/debug/pprof/heap' > mem.prof\ntelegraf --version > version.txt\ngo env GOOS GOARCH >> version.txt\n```\n\n----------------------------------------\n\nTITLE: Invalid Windows Path String in TOML\nDESCRIPTION: Example of invalid TOML configuration showing unescaped backslashes in a Windows file path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TOML.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\npath = \"C:\\Program Files\\\"  # this is invalid TOML\n```\n\n----------------------------------------\n\nTITLE: Displaying Puppet Agent Last Run Summary in YAML\nDESCRIPTION: Shows the content of the 'last_run_summary.yaml' file, which contains metrics from the last Puppet agent run. This file is typically located in /var/lib/puppet/state/.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/puppetagent/README.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\n  events:\n    failure: 0\n    total: 0\n    success: 0\n  resources:\n    failed: 0\n    scheduled: 0\n    changed: 0\n    skipped: 0\n    total: 109\n    failed_to_restart: 0\n    restarted: 0\n    out_of_sync: 0\n  changes:\n    total: 0\n  time:\n    user: 0.004331\n    schedule: 0.001123\n    filebucket: 0.000353\n    file: 0.441472\n    exec: 0.508123\n    anchor: 0.000555\n    yumrepo: 0.006989\n    ssh_authorized_key: 0.000764\n    service: 1.807795\n    package: 1.325788\n    total: 8.85354707064819\n    config_retrieval: 4.75567007064819\n    last_run: 1444936531\n    cron: 0.000584\n  version:\n    config: 1444936521\n    puppet: \"3.7.5\"\n```\n\n----------------------------------------\n\nTITLE: Rsyslog Configuration for Telegraf Integration\nDESCRIPTION: This shell script configures Rsyslog to forward logging messages to Telegraf. It sets up asynchronous processing, enables disk mode for queueing, and forwards messages over TCP with octet framing according to RFC 5425.  It utilizes the `RSYSLOG_SyslogProtocol23Format` template for message formatting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ActionQueueType LinkedList # use asynchronous processing\n$ActionQueueFileName srvrfwd # set file name, also enables disk mode\n$ActionResumeRetryCount -1 # infinite retries on insert failure\n$ActionQueueSaveOnShutdown on # save in-memory data if rsyslog shuts down\n\n# forward over tcp with octet framing according to RFC 5425\n*.* @@(o)127.0.0.1:6514;RSYSLOG_SyslogProtocol23Format\n\n# uncomment to use udp according to RFC 5424\n#*.* @127.0.0.1:6514;RSYSLOG_SyslogProtocol23Format\n\n```\n\n----------------------------------------\n\nTITLE: Important Change Notice for procstat Input Plugin\nDESCRIPTION: Documentation of breaking changes in the procstat input plugin regarding I/O measurement fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n[PR #15186] changes the meaning of `inputs.procstat` fields `read_bytes` and `write_bytes` on Linux to now contain _all_ I/O operations for consistency with other operating-systems. The previous values are output as `disk_read_bytes` and `disk_write_bytes` measuring _only_ the I/O on the storage layer.\n```\n\n----------------------------------------\n\nTITLE: Analyzing Trace Profile with Go Tool\nDESCRIPTION: Use the Go trace tool to analyze the collected trace profile.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngo tool trace trace.bin\n```\n\n----------------------------------------\n\nTITLE: Displaying DC/OS Node and Container Metrics in InfluxDB Line Protocol Format\nDESCRIPTION: This code snippet shows the output of DC/OS node and container metrics in InfluxDB line protocol format. It includes various measurements such as filesystem capacity, network statistics, CPU usage, memory usage, and container-specific metrics. Each line represents a different metric or set of metrics for a specific node or container.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dcos/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndcos_node,cluster=enterprise,hostname=192.168.122.18,path=/boot filesystem_capacity_free_bytes=918188032i,filesystem_capacity_total_bytes=1063256064i,filesystem_capacity_used_bytes=145068032i,filesystem_inode_free=523958,filesystem_inode_total=524288,filesystem_inode_used=330 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=dummy0 network_in_bytes=0i,network_in_dropped=0,network_in_errors=0,network_in_packets=0,network_out_bytes=0i,network_out_dropped=0,network_out_errors=0,network_out_packets=0 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=docker0 network_in_bytes=0i,network_in_dropped=0,network_in_errors=0,network_in_packets=0,network_out_bytes=0i,network_out_dropped=0,network_out_errors=0,network_out_packets=0 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18 cpu_cores=2,cpu_idle=81.62,cpu_system=4.19,cpu_total=13.670000000000002,cpu_user=9.48,cpu_wait=0,load_15min=0.7,load_1min=0.22,load_5min=0.6,memory_buffers_bytes=970752i,memory_cached_bytes=1830473728i,memory_free_bytes=1178636288i,memory_total_bytes=3975073792i,process_count=198,swap_free_bytes=859828224i,swap_total_bytes=859828224i,swap_used_bytes=0i,system_uptime=18874 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=lo network_in_bytes=1090992450i,network_in_dropped=0,network_in_errors=0,network_in_packets=1546938,network_out_bytes=1090992450i,network_out_dropped=0,network_out_errors=0,network_out_packets=1546938 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,path=/ filesystem_capacity_free_bytes=1668378624i,filesystem_capacity_total_bytes=6641680384i,filesystem_capacity_used_bytes=4973301760i,filesystem_inode_free=3107856,filesystem_inode_total=3248128,filesystem_inode_used=140272 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=minuteman network_in_bytes=0i,network_in_dropped=0,network_in_errors=0,network_in_packets=0,network_out_bytes=210i,network_out_dropped=0,network_out_errors=0,network_out_packets=3 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=eth0 network_in_bytes=539886216i,network_in_dropped=1,network_in_errors=0,network_in_packets=979808,network_out_bytes=112395836i,network_out_dropped=0,network_out_errors=0,network_out_packets=891239 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=spartan network_in_bytes=0i,network_in_dropped=0,network_in_errors=0,network_in_packets=0,network_out_bytes=210i,network_out_dropped=0,network_out_errors=0,network_out_packets=3 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,path=/var/lib/docker/overlay filesystem_capacity_free_bytes=1668378624i,filesystem_capacity_total_bytes=6641680384i,filesystem_capacity_used_bytes=4973301760i,filesystem_inode_free=3107856,filesystem_inode_total=3248128,filesystem_inode_used=140272 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=vtep1024 network_in_bytes=0i,network_in_dropped=0,network_in_errors=0,network_in_packets=0,network_out_bytes=0i,network_out_dropped=0,network_out_errors=0,network_out_packets=0 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,path=/var/lib/docker/plugins filesystem_capacity_free_bytes=1668378624i,filesystem_capacity_total_bytes=6641680384i,filesystem_capacity_used_bytes=4973301760i,filesystem_inode_free=3107856,filesystem_inode_total=3248128,filesystem_inode_used=140272 1511859222000000000\ndcos_node,cluster=enterprise,hostname=192.168.122.18,interface=d-dcos network_in_bytes=0i,network_in_dropped=0,network_in_errors=0,network_in_packets=0,network_out_bytes=0i,network_out_dropped=0,network_out_errors=0,network_out_packets=0 1511859222000000000\ndcos_app,cluster=enterprise,container_id=9a78d34a-3bbf-467e-81cf-a57737f154ee,hostname=192.168.122.18 container_received_bytes_per_sec=0,container_throttled_bytes_per_sec=0 1511859222000000000\ndcos_container,cluster=enterprise,container_id=cbf19b77-3b8d-4bcf-b81f-824b67279629,hostname=192.168.122.18 cpus_limit=0.3,cpus_system_time=307.31,cpus_throttled_time=102.029930607,cpus_user_time=268.57,disk_limit_bytes=268435456i,disk_used_bytes=30953472i,mem_limit_bytes=570425344i,mem_total_bytes=13316096i,net_rx_bytes=0i,net_rx_dropped=0,net_rx_errors=0,net_rx_packets=0,net_tx_bytes=0i,net_tx_dropped=0,net_tx_errors=0,net_tx_packets=0 1511859222000000000\ndcos_app,cluster=enterprise,container_id=cbf19b77-3b8d-4bcf-b81f-824b67279629,hostname=192.168.122.18 container_received_bytes_per_sec=0,container_throttled_bytes_per_sec=0 1511859222000000000\ndcos_container,cluster=enterprise,container_id=5725e219-f66e-40a8-b3ab-519d85f4c4dc,hostname=192.168.122.18,task_name=hello-world cpus_limit=0.6,cpus_system_time=25.6,cpus_throttled_time=327.977109217,cpus_user_time=566.54,disk_limit_bytes=0i,disk_used_bytes=0i,mem_limit_bytes=1107296256i,mem_total_bytes=335941632i,net_rx_bytes=0i,net_rx_dropped=0,net_rx_errors=0,net_rx_packets=0,net_tx_bytes=0i,net_tx_dropped=0,net_tx_errors=0,net_tx_packets=0 1511859222000000000\ndcos_app,cluster=enterprise,container_id=5725e219-f66e-40a8-b3ab-519d85f4c4dc,hostname=192.168.122.18 container_received_bytes_per_sec=0,container_throttled_bytes_per_sec=0 1511859222000000000\ndcos_app,cluster=enterprise,container_id=c76e1488-4fb7-4010-a4cf-25725f8173f9,hostname=192.168.122.18 container_received_bytes_per_sec=0,container_throttled_bytes_per_sec=0 1511859222000000000\ndcos_container,cluster=enterprise,container_id=cbe0b2f9-061f-44ac-8f15-4844229e8231,hostname=192.168.122.18,task_name=telegraf cpus_limit=0.2,cpus_system_time=8.109999999,cpus_throttled_time=93.183916045,cpus_user_time=17.97,disk_limit_bytes=0i,disk_used_bytes=0i,mem_limit_bytes=167772160i,mem_total_bytes=0i,net_rx_bytes=0i,net_rx_dropped=0,net_rx_errors=0,net_rx_packets=0,net_tx_bytes=0i,net_tx_dropped=0,net_tx_errors=0,net_tx_packets=0 1511859222000000000\ndcos_container,cluster=enterprise,container_id=b64115de-3d2a-431d-a805-76e7c46453f1,hostname=192.168.122.18 cpus_limit=0.2,cpus_system_time=2.69,cpus_throttled_time=20.064861214,cpus_user_time=6.56,disk_limit_bytes=268435456i,disk_used_bytes=29360128i,mem_limit_bytes=297795584i,mem_total_bytes=13733888i,net_rx_bytes=0i,net_rx_dropped=0,net_rx_errors=0,net_rx_packets=0,net_tx_bytes=0i,net_tx_dropped=0,net_tx_errors=0,net_tx_packets=0 1511859222000000000\ndcos_app,cluster=enterprise,container_id=b64115de-3d2a-431d-a805-76e7c46453f1,hostname=192.168.122.18 container_received_bytes_per_sec=0,container_throttled_bytes_per_sec=0 1511859222000000000\n```\n\n----------------------------------------\n\nTITLE: Diff Example for IfDescr Fallback\nDESCRIPTION: This diff snippet demonstrates the change in metrics when using the SNMP Lookup Processor Plugin with an 'ifDescr' fallback. It shows how the 'ifName' tag is added to the metrics, ensuring comprehensive network device information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/snmp_lookup/README.md#2025-04-16_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- foo,agent=127.0.0.1,ifIndex=2 field=123\n+ foo,agent=127.0.0.1,ifIndex=2,ifName=eth0 field=123\n```\n\n----------------------------------------\n\nTITLE: OpenTSDB Telnet Mode Data Format\nDESCRIPTION: Example of the OpenTSDB telnet mode data format showing various metric entries with timestamps and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opentsdb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nput nine.telegraf.system_load1 1441910356 0.430000 dc=homeoffice host=irimame scope=green\nput nine.telegraf.system_load5 1441910356 0.580000 dc=homeoffice host=irimame scope=green\nput nine.telegraf.system_load15 1441910356 0.730000 dc=homeoffice host=irimame scope=green\nput nine.telegraf.system_uptime 1441910356 3655970.000000 dc=homeoffice host=irimame scope=green\nput nine.telegraf.system_uptime_format 1441910356  dc=homeoffice host=irimame scope=green\nput nine.telegraf.mem_total 1441910356 4145426432 dc=homeoffice host=irimame scope=green\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Carbon2 Serializer in Telegraf\nDESCRIPTION: TOML configuration for setting up the Carbon2 serializer in a Telegraf file output. Includes options for specifying output files, data format, metrics format style, and character replacement for sanitization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/carbon2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"carbon2\"\n\n  ## Optionally configure metrics format, whether to merge metric name and field name.\n  ## Possible options:\n  ## * \"field_separate\"\n  ## * \"metric_includes_field\"\n  ## * \"\" - defaults to \"field_separate\"\n  # carbon2_format = \"field_separate\"\n\n  ## Character used for replacing sanitized characters. By default \":\" is used.\n  ## The following character set is being replaced with sanitize replace char:\n  ## !@#$%^&*()+`'\\\"[]{};<>,?/\\\\|=\n  # carbon2_sanitize_replace_char = \":\"\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf in Docker with Host Disk Monitoring\nDESCRIPTION: Shell command for running Telegraf in a Docker container with access to the host's filesystem. This setup mounts the host filesystem as read-only and configures environment variables to properly monitor disk usage from within the container.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -v /:/hostfs:ro -e HOST_MOUNT_PREFIX=/hostfs -e HOST_PROC=/hostfs/proc telegraf\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status query, providing information about the synchronization state with a remote NTP server. It includes details such as the remote server address, reference ID, stratum, poll interval, reach, delay, offset, and jitter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/bad_header/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nremote      refid   foobar t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Example Output for Collected Minecraft Metrics (Text)\nDESCRIPTION: This text snippet demonstrates the expected output format of the metrics collected from the Minecraft server by the Telegraf plugin, showing various fields and their corresponding values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/minecraft/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n\"minecraft,player=notch,source=127.0.0.1,port=25575 jumps=178i 1498261397000000000\\nminecraft,player=dinnerbone,source=127.0.0.1,port=25575 deaths=1i,jumps=1999i,cow_kills=1i 1498261397000000000\\nminecraft,player=jeb,source=127.0.0.1,port=25575 d_pickaxe=1i,damage_dealt=80i,d_sword=2i,hunger=20i,health=20i,kills=1i,level=33i,jumps=264i,armor=15i 1498261397000000000\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Solr Metrics - Text\nDESCRIPTION: This text snippet showcases the expected output metrics from the Solr input plugin for various handlers and cores. It demonstrates how document counts and performance metrics are reported as time-series data points, including specific attributes for deleted documents, request times, and error counts.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/solr/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"solr_core,core=main,handler=searcher,host=testhost deleted_docs=17616645i,max_docs=261848363i,num_docs=244231718i 1478214949000000000\\nsolr_core,core=main,handler=core,host=testhost deleted_docs=0i,max_docs=0i,num_docs=0i 1478214949000000000\\nsolr_queryhandler,core=main,handler=/replication,host=testhost 15min_rate_reqs_per_second=0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000444659081257,5min_rate_reqs_per_second=0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000014821969375,75th_pc_request_time=16.484211,95th_pc_request_time=16.484211,999th_pc_request_time=16.484211,99th_pc_request_time=16.484211,avg_requests_per_second=0.0000008443809966322143,avg_time_per_request=12.984811,errors=0i,handler_start=1474662050865i,median_request_time=11.352427,requests=3i,timeouts=0i,total_time=38.954433 1478214949000000000\\nsolr_queryhandler,core=main,handler=/update/extract,host=testhost 15min_rate_reqs_per_second=0,5min_rate_reqs_per_second=0,75th_pc_request_time=0,95th_pc_request_time=0,999th_pc_request_time=0,99th_pc_request_time=0,avg_requests_per_second=0,avg_time_per_request=0,errors=0i,handler_start=0i,median_request_time=0,requests=0i,timeouts=0i,total_time=0 1478214949000000000\\nsolr_queryhandler,core=main,handler=org.apache.solr.handler.component.SearchHandler,host=testhost 15min_rate_reqs_per_second=0,5min_rate_reqs_per_second=0,75th_pc_request_time=0,95th_pc_request_time=0,999th_pc_request_time=0,99th_pc_request_time=0,avg_requests_per_second=0,avg_time_per_request=0,errors=0i,handler_start=1474662050861i,median_request_time=0,requests=0i,timeouts=0i,total_time=0 1478214949000000000\\nsolr_queryhandler,core=main,handler=/tvrh,host=testhost 15min_rate_reqs_per_second=0,5min_rate_reqs_per_second=0,75th_pc_request_time=0,95th_pc_request_time=0,999th_pc_request_time=0,99th_pc_request_time=0,avg_requests_per_second=0,avg_time_per_request=0,errors=0i,handler_start=0i,median_request_time=0,requests=0i,timeouts=0i,total_time=0 1478214949000000000\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Sudoers Configuration for NSD\nDESCRIPTION: Bash commands and sudoers configuration to grant Telegraf privileged access to nsd-control without requiring password.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsd/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# Add the following line:\nCmnd_Alias NSDCONTROLCTL = /usr/sbin/nsd-control\ntelegraf  ALL=(ALL) NOPASSWD: NSDCONTROLCTL\nDefaults!NSDCONTROLCTL !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Running Custom Builder with Configuration Directory\nDESCRIPTION: This command executes the custom_builder tool using both individual configuration files and a directory to tailor the Telegraf binary. Ensure each specified path exists and contains valid configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# ./tools/custom_builder/custom_builder                      \\\\n    --config     /etc/telegraf/telegraf.conf \\\\n    --config-dir /etc/telegraf/telegraf.d\n```\n\n----------------------------------------\n\nTITLE: Configuring Value Parser Plugin in Telegraf\nDESCRIPTION: Configuration example for the Value Parser plugin showing how to set up command execution, measurement name override, and data format settings. The example demonstrates reading system entropy values with specific data type configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/value/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.exec]]\n  ## Commands array\n  commands = [\"cat /proc/sys/kernel/random/entropy_avail\"]\n\n  ## override the default metric name of \"exec\"\n  name_override = \"entropy_available\"\n\n  ## override the field name of \"value\"\n  # value_field_name = \"value\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"value\"\n  data_type = \"integer\" # required\n```\n\n----------------------------------------\n\nTITLE: Generated Metric Output from Net Response Input\nDESCRIPTION: Example output when running Telegraf in test mode showing the metric generated by the net_response input with its tags and fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n$ telegraf -config example.conf -test\n* Plugin: inputs.net_response, Collection 1\n> net_response,server=example.com,port=80,protocol=tcp,host=myhost result_type=\"success\",response_time=0.091026869 1522741063000000000\n```\n\n----------------------------------------\n\nTITLE: Example serialization of Telegraf metrics to Wavefront format (Text)\nDESCRIPTION: This example text demonstrates how Telegraf metrics are serialized into Wavefront metrics. The input metric includes various fields such as `user`, `idle`, and `system`, and their corresponding Wavefront metrics show the structured output format. The conversion reflects metric names, values, timestamp, and source tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/wavefront/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncpu,cpu=cpu0,host=testHost user=12,idle=88,system=0 1234567890\n```\n\nLANGUAGE: text\nCODE:\n```\n\"cpu.user\" 12.000000 1234567890 source=\"testHost\" \"cpu\"=\"cpu0\"\n\"cpu.idle\" 88.000000 1234567890 source=\"testHost\" \"cpu\"=\"cpu0\"\n\"cpu.system\" 0.000000 1234567890 source=\"testHost\" \"cpu\"=\"cpu0\"\n```\n\n----------------------------------------\n\nTITLE: Virtual Machine Performance Metrics Catalog\nDESCRIPTION: Comprehensive list of performance metrics for monitoring virtual machine resource usage, including CPU, memory, network, and power metrics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/METRICS.md#2025-04-16_snippet_1\n\nLANGUAGE: metrics\nCODE:\n```\ncpu.demandEntitlementRatio.latest\\ncpu.usage.average\\ncpu.ready.summation\n```\n\n----------------------------------------\n\nTITLE: Defining powerstat_core Measurement Metrics in Markdown\nDESCRIPTION: This snippet lists the available metrics for the powerstat_core measurement, including various CPU states, frequencies, and temperatures.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_11\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric name (field)               | Description                                                                                                                                                               | Units           |\n|-----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|\n| `cpu_frequency_mhz`               | Current operational frequency of CPU Core.                                                                                                                                | MHz             |\n| `cpu_busy_frequency_mhz`          | CPU Core Busy Frequency measured as frequency adjusted to CPU Core busy cycles.                                                                                           | MHz             |\n| `cpu_temperature_celsius`         | Current temperature of CPU Core.                                                                                                                                          | Celsius degrees |\n| `cpu_c0_state_residency_percent`  | Percentage of time that CPU Core spent in C0 Core residency state.                                                                                                        | %               |\n| `cpu_c1_state_residency_percent`  | Percentage of time that CPU Core spent in C1 Core residency state.                                                                                                        | %               |\n| `cpu_c3_state_residency_percent`  | Percentage of time that CPU Core spent in C3 Core residency state.                                                                                                        | %               |\n| `cpu_c6_state_residency_percent`  | Percentage of time that CPU Core spent in C6 Core residency state.                                                                                                        | %               |\n| `cpu_c7_state_residency_percent`  | Percentage of time that CPU Core spent in C7 Core residency state.                                                                                                        | %               |\n| `cpu_c0_substate_c01_percent`     | Percentage of time that CPU Core spent in C0.1 substate out of the total time in the C0 state.                                                                            | %               |\n| `cpu_c0_substate_c02_percent`     | Percentage of time that CPU Core spent in C0.2 substate out of the total time in the C0 state.                                                                            | %               |\n| `cpu_c0_substate_c0_wait_percent` | Percentage of time that CPU Core spent in C0_Wait substate out of the total time in the C0 state.                                                                         | %               |\n| `cpu_busy_cycles_percent`         | (**DEPRECATED** - superseded by cpu_c0_state_residency_percent) CPU Core Busy cycles as a ratio of Cycles spent in C0 state residency to all cycles executed by CPU Core. | %               |\n```\n\n----------------------------------------\n\nTITLE: Displaying Suricata Monitoring Data in InfluxDB Line Protocol Format\nDESCRIPTION: This example shows the output format for Suricata monitoring data as captured by Telegraf. The data includes metrics from various threads and components like flow management, packet decoding, protocol statistics, and memory usage, formatted in InfluxDB line protocol with measurements, tags, fields, and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/suricata/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nsuricata,host=myhost,thread=FM#01 flow_mgr_rows_empty=0,flow_mgr_rows_checked=65536,flow_mgr_closed_pruned=0,flow_emerg_mode_over=0,flow_mgr_flows_timeout_inuse=0,flow_mgr_rows_skipped=65535,flow_mgr_bypassed_pruned=0,flow_mgr_flows_removed=0,flow_mgr_est_pruned=0,flow_mgr_flows_notimeout=1,flow_mgr_flows_checked=1,flow_mgr_rows_busy=0,flow_spare=10000,flow_mgr_rows_maxlen=1,flow_mgr_new_pruned=0,flow_emerg_mode_entered=0,flow_tcp_reuse=0,flow_mgr_flows_timeout=0 1568368562545197545\nsuricata,host=myhost,thread=W#04-wlp4s0 decoder_ltnull_pkt_too_small=0,decoder_ipraw_invalid_ip_version=0,defrag_ipv4_reassembled=0,tcp_no_flow=0,app_layer_flow_tls=1,decoder_udp=25,defrag_ipv6_fragments=0,defrag_ipv4_fragments=0,decoder_tcp=59,decoder_vlan=0,decoder_pkts=84,decoder_vlan_qinq=0,decoder_avg_pkt_size=574,flow_memcap=0,defrag_max_frag_hits=0,tcp_ssn_memcap_drop=0,capture_kernel_packets=84,app_layer_flow_dcerpc_udp=0,app_layer_tx_dns_tcp=0,tcp_rst=0,decoder_icmpv4=0,app_layer_tx_tls=0,decoder_ipv4=84,decoder_erspan=0,decoder_ltnull_unsupported_type=0,decoder_invalid=0,app_layer_flow_ssh=0,capture_kernel_drops=0,app_layer_flow_ftp=0,app_layer_tx_http=0,tcp_pseudo_failed=0,defrag_ipv6_reassembled=0,defrag_ipv6_timeouts=0,tcp_pseudo=0,tcp_sessions=1,decoder_ethernet=84,decoder_raw=0,decoder_sctp=0,app_layer_flow_dns_udp=1,decoder_gre=0,app_layer_flow_http=0,app_layer_flow_imap=0,tcp_segment_memcap_drop=0,detect_alert=0,app_layer_flow_failed_tcp=0,decoder_teredo=0,decoder_mpls=0,decoder_ppp=0,decoder_max_pkt_size=1422,decoder_ipv6=0,tcp_reassembly_gap=0,app_layer_flow_dcerpc_tcp=0,decoder_ipv4_in_ipv6=0,tcp_stream_depth_reached=0,app_layer_flow_dns_tcp=0,app_layer_flow_smtp=0,tcp_syn=1,decoder_sll=0,tcp_invalid_checksum=0,app_layer_tx_dns_udp=1,decoder_bytes=48258,defrag_ipv4_timeouts=0,app_layer_flow_msn=0,decoder_pppoe=0,decoder_null=0,app_layer_flow_failed_udp=3,app_layer_tx_smtp=0,decoder_icmpv6=0,decoder_ipv6_in_ipv6=0,tcp_synack=1,app_layer_flow_smb=0,decoder_dce_pkt_too_small=0 1568368562545174807\nsuricata,host=myhost,thread=W#01-wlp4s0 tcp_synack=0,app_layer_flow_imap=0,decoder_ipv4_in_ipv6=0,decoder_max_pkt_size=684,decoder_gre=0,defrag_ipv4_timeouts=0,tcp_invalid_checksum=0,decoder_ipv4=53,flow_memcap=0,app_layer_tx_http=0,app_layer_tx_smtp=0,decoder_null=0,tcp_no_flow=0,app_layer_tx_tls=0,app_layer_flow_ssh=0,app_layer_flow_smtp=0,decoder_pppoe=0,decoder_teredo=0,decoder_ipraw_invalid_ip_version=0,decoder_ltnull_pkt_too_small=0,tcp_rst=0,decoder_ppp=0,decoder_ipv6=29,app_layer_flow_dns_udp=3,decoder_vlan=0,app_layer_flow_dcerpc_tcp=0,tcp_syn=0,defrag_ipv4_fragments=0,defrag_ipv6_timeouts=0,decoder_raw=0,defrag_ipv6_reassembled=0,tcp_reassembly_gap=0,tcp_sessions=0,decoder_udp=44,tcp_segment_memcap_drop=0,app_layer_tx_dns_udp=3,app_layer_flow_tls=0,decoder_tcp=37,defrag_ipv4_reassembled=0,app_layer_flow_failed_udp=6,app_layer_flow_ftp=0,decoder_icmpv6=1,tcp_stream_depth_reached=0,capture_kernel_drops=0,decoder_sll=0,decoder_bytes=15883,decoder_ethernet=91,tcp_pseudo=0,app_layer_flow_http=0,decoder_sctp=0,decoder_pkts=91,decoder_avg_pkt_size=174,decoder_erspan=0,app_layer_flow_msn=0,app_layer_flow_smb=0,capture_kernel_packets=91,decoder_icmpv4=0,decoder_ipv6_in_ipv6=0,tcp_ssn_memcap_drop=0,decoder_vlan_qinq=0,decoder_ltnull_unsupported_type=0,decoder_invalid=0,defrag_max_frag_hits=0,tcp_pseudo_failed=0,detect_alert=0,app_layer_tx_dns_tcp=0,app_layer_flow_failed_tcp=0,app_layer_flow_dcerpc_udp=0,app_layer_flow_dns_tcp=0,defrag_ipv6_fragments=0,decoder_mpls=0,decoder_dce_pkt_too_small=0 1568368562545148438\nsuricata,host=myhost flow_memuse=7094464,tcp_memuse=3276800,tcp_reassembly_memuse=12332832,dns_memuse=0,dns_memcap_state=0,dns_memcap_global=0,http_memuse=0,http_memcap=0 1568368562545144569\nsuricata,host=myhost,thread=W#07-wlp4s0 app_layer_tx_http=0,app_layer_tx_dns_tcp=0,decoder_vlan=0,decoder_pppoe=0,decoder_sll=0,decoder_tcp=0,flow_memcap=0,app_layer_flow_msn=0,tcp_no_flow=0,tcp_rst=0,tcp_segment_memcap_drop=0,tcp_sessions=0,detect_alert=0,defrag_ipv6_reassembled=0,decoder_ipraw_invalid_ip_version=0,decoder_erspan=0,decoder_icmpv4=0,app_layer_tx_dns_udp=2,decoder_ltnull_pkt_too_small=0,decoder_bytes=1998,decoder_ipv6=1,defrag_ipv4_fragments=0,defrag_ipv6_fragments=0,app_layer_tx_smtp=0,decoder_ltnull_unsupported_type=0,decoder_max_pkt_size=342,app_layer_flow_ftp=0,decoder_ipv6_in_ipv6=0,defrag_ipv4_reassembled=0,defrag_ipv6_timeouts=0,app_layer_flow_dns_tcp=0,decoder_avg_pkt_size=181,defrag_ipv4_timeouts=0,tcp_stream_depth_reached=0,decoder_mpls=0,app_layer_flow_dns_udp=2,tcp_ssn_memcap_drop=0,app_layer_flow_dcerpc_tcp=0,app_layer_flow_failed_udp=2,app_layer_flow_smb=0,app_layer_flow_failed_tcp=0,decoder_invalid=0,decoder_null=0,decoder_gre=0,decoder_ethernet=11,app_layer_flow_ssh=0,defrag_max_frag_hits=0,capture_kernel_drops=0,tcp_pseudo_failed=0,app_layer_flow_smtp=0,decoder_udp=10,decoder_sctp=0,decoder_teredo=0,decoder_icmpv6=1,tcp_pseudo=0,tcp_synack=0,app_layer_tx_tls=0,app_layer_flow_imap=0,capture_kernel_packets=11,decoder_pkts=11,decoder_raw=0,decoder_ppp=0,tcp_syn=0,tcp_invalid_checksum=0,app_layer_flow_tls=0,decoder_ipv4_in_ipv6=0,app_layer_flow_http=0,decoder_dce_pkt_too_small=0,decoder_ipv4=10,decoder_vlan_qinq=0,tcp_reassembly_gap=0,app_layer_flow_dcerpc_udp=0 1568368562545110847\nsuricata,host=myhost,thread=W#06-wlp4s0 app_layer_tx_smtp=0,decoder_ipv6_in_ipv6=0,decoder_dce_pkt_too_small=0,tcp_segment_memcap_drop=0,tcp_sessions=1,decoder_ppp=0,tcp_pseudo_failed=0,app_layer_tx_dns_tcp=0,decoder_invalid=0,defrag_ipv4_timeouts=0,app_layer_flow_smb=0,app_layer_flow_ssh=0,decoder_bytes=19407,decoder_null=0,app_layer_flow_tls=1,decoder_avg_pkt_size=473,decoder_pkts=41,decoder_pppoe=0,decoder_tcp=32,defrag_ipv4_reassembled=0,tcp_reassembly_gap=0,decoder_raw=0,flow_memcap=0,defrag_ipv6_timeouts=0,app_layer_flow_smtp=0,app_layer_tx_http=0,decoder_sll=0,decoder_udp=8,decoder_ltnull_pkt_too_small=0,decoder_ltnull_unsupported_type=0,decoder_ipv4_in_ipv6=0,decoder_vlan=0,decoder_max_pkt_size=1422,tcp_no_flow=0,app_layer_flow_failed_tcp=0,app_layer_flow_dns_tcp=0,app_layer_flow_ftp=0,decoder_icmpv4=0,defrag_max_frag_hits=0,tcp_rst=0,app_layer_flow_msn=0,app_layer_flow_failed_udp=2,app_layer_flow_dns_udp=0,app_layer_flow_dcerpc_udp=0,decoder_ipv4=39,decoder_ethernet=41,defrag_ipv6_reassembled=0,tcp_ssn_memcap_drop=0,app_layer_tx_tls=0,decoder_gre=0,decoder_vlan_qinq=0,tcp_pseudo=0,app_layer_flow_imap=0,app_layer_flow_dcerpc_tcp=0,defrag_ipv4_fragments=0,defrag_ipv6_fragments=0,tcp_synack=1,app_layer_flow_http=0,app_layer_tx_dns_udp=0,capture_kernel_packets=41,decoder_ipv6=2,tcp_invalid_checksum=0,tcp_stream_depth_reached=0,decoder_ipraw_invalid_ip_version=0,decoder_icmpv6=1,tcp_syn=1,detect_alert=0,capture_kernel_drops=0,decoder_teredo=0,decoder_erspan=0,decoder_sctp=0,decoder_mpls=0 1568368562545084670\nsuricata,host=myhost,thread=W#02-wlp4s0 decoder_tcp=53,tcp_rst=3,tcp_reassembly_gap=0,defrag_ipv6_timeouts=0,tcp_ssn_memcap_drop=0,app_layer_flow_dcerpc_tcp=0,decoder_max_pkt_size=1422,decoder_ipv6_in_ipv6=0,tcp_no_flow=0,app_layer_flow_ftp=0,app_layer_flow_ssh=0,decoder_pkts=82,decoder_sctp=0,tcp_invalid_checksum=0,app_layer_flow_dns_tcp=0,decoder_ipraw_invalid_ip_version=0,decoder_bytes=26441,decoder_erspan=0,tcp_pseudo_failed=0,tcp_syn=1,app_layer_tx_http=0,app_layer_tx_smtp=0,decoder_teredo=0,decoder_ipv4=80,defrag_ipv4_fragments=0,tcp_stream_depth_reached=0,app_layer_flow_smb=0,capture_kernel_packets=82,decoder_null=0,decoder_ltnull_pkt_too_small=0,decoder_ppp=0,decoder_icmpv6=1,app_layer_flow_dns_udp=2,app_layer_flow_http=0,app_layer_tx_dns_udp=3,decoder_mpls=0,decoder_sll=0,defrag_ipv4_reassembled=0,tcp_segment_memcap_drop=0,app_layer_flow_imap=0,decoder_ltnull_unsupported_type=0,decoder_icmpv4=0,decoder_raw=0,defrag_ipv4_timeouts=0,app_layer_flow_failed_udp=8,decoder_gre=0,capture_kernel_drops=0,defrag_ipv6_reassembled=0,tcp_pseudo=0,app_layer_flow_tls=1,decoder_avg_pkt_size=322,decoder_dce_pkt_too_small=0,decoder_ethernet=82,defrag_ipv6_fragments=0,tcp_sessions=1,tcp_synack=1,app_layer_tx_dns_tcp=0,decoder_vlan=0,flow_memcap=0,decoder_vlan_qinq=0,decoder_udp=28,decoder_invalid=0,detect_alert=0,app_layer_flow_failed_tcp=0,app_layer_tx_tls=0,decoder_pppoe=0,decoder_ipv6=2,decoder_ipv4_in_ipv6=0,defrag_max_frag_hits=0,app_layer_flow_dcerpc_udp=0,app_layer_flow_smtp=0,app_layer_flow_msn=0 1568368562545061864\nsuricata,host=myhost,thread=W#08-wlp4s0 decoder_dce_pkt_too_small=0,app_layer_tx_dns_tcp=0,decoder_pkts=58,decoder_ppp=0,decoder_raw=0,decoder_ipv4_in_ipv6=0,decoder_max_pkt_size=1392,tcp_invalid_checksum=0,tcp_syn=0,decoder_ipv4=51,decoder_ipv6_in_ipv6=0,decoder_tcp=0,decoder_ltnull_pkt_too_small=0,flow_memcap=0,decoder_udp=58,tcp_ssn_memcap_drop=0,tcp_pseudo=0,app_layer_flow_dcerpc_udp=0,app_layer_flow_dns_udp=5,app_layer_tx_http=0,capture_kernel_drops=0,decoder_vlan=0,tcp_segment_memcap_drop=0,app_layer_flow_ftp=0,app_layer_flow_imap=0,app_layer_flow_http=0,app_layer_flow_tls=0,decoder_icmpv4=0,decoder_sctp=0,defrag_ipv4_timeouts=0,tcp_reassembly_gap=0,detect_alert=0,decoder_ethernet=58,tcp_pseudo_failed=0,decoder_teredo=0,defrag_ipv4_reassembled=0,tcp_sessions=0,app_layer_flow_msn=0,decoder_ipraw_invalid_ip_version=0,tcp_no_flow=0,app_layer_flow_dns_tcp=0,decoder_null=0,defrag_ipv4_fragments=0,app_layer_flow_dcerpc_tcp=0,app_layer_flow_failed_udp=8,app_layer_tx_tls=0,decoder_bytes=15800,decoder_ipv6=7,tcp_stream_depth_reached=0,decoder_invalid=0,decoder_ltnull_unsupported_type=0,app_layer_tx_dns_udp=6,decoder_pppoe=0,decoder_avg_pkt_size=272,decoder_erspan=0,defrag_ipv6_timeouts=0,app_layer_flow_failed_tcp=0,decoder_gre=0,decoder_sll=0,defrag_max_frag_hits=0,app_layer_flow_ssh=0,capture_kernel_packets=58,decoder_mpls=0,decoder_vlan_qinq=0,tcp_rst=0,app_layer_flow_smb=0,app_layer_tx_smtp=0,decoder_icmpv6=0,defrag_ipv6_fragments=0,defrag_ipv6_reassembled=0,tcp_synack=0,app_layer_flow_smtp=0 1568368562545035575\n```\n\n----------------------------------------\n\nTITLE: Rsyslog Configuration for RFC3164 UDP\nDESCRIPTION: This `s` code configures rsyslog to listen for RFC3164 UDP messages on 127.0.0.1:514 and forward them as RFC5424 to Telegraf.  It loads the UDP module, sets the UDP server address, and enables the UDP server to run on port 514, which helps translate RFC3164 messages for Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/README.md#2025-04-16_snippet_4\n\nLANGUAGE: s\nCODE:\n```\n# This makes rsyslog listen on 127.0.0.1:514 to receive RFC3164 udp\n# messages which can them be forwarded to telegraf as RFC5424\n$ModLoad imudp #loads the udp module\n$UDPServerAddress 127.0.0.1\n$UDPServerRun 514\n\n```\n\n----------------------------------------\n\nTITLE: Implementing the 'reset' Function in Starlark Aggregator\nDESCRIPTION: Example of the required 'reset' function that clears the aggregation state. This function is called after pushing metrics to prepare for the next aggregation period.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/starlark/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef reset():\n  state.clear()\n```\n\n----------------------------------------\n\nTITLE: Sending Data to InfluxDB V2 Listener using cURL\nDESCRIPTION: Example of sending metrics to the InfluxDB V2 Listener endpoint using cURL command.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb_v2_listener/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -i -XPOST 'http://localhost:8186/api/v2/write' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'\n```\n\n----------------------------------------\n\nTITLE: Executing Telegraf Secrets Help Command in Shell\nDESCRIPTION: This shell command provides guidance on how to manage secrets using Telegraf by accessing help information. No additional dependencies or parameters are required. Expected output includes help documentation related to secret management in Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/http/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf secrets help\n```\n\n----------------------------------------\n\nTITLE: CloudWatch ELB Metric Output Example\nDESCRIPTION: Example output showing CloudWatch metrics for an Elastic Load Balancer, including latency statistics with various aggregations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cloudwatch/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ncloudwatch_aws_elb,load_balancer_name=p-example,region=us-east-1 latency_average=0.004810798017284538,latency_maximum=0.1100282669067383,latency_minimum=0.0006084442138671875,latency_sample_count=4029,latency_sum=19.382705211639404 1459542420000000000\n```\n\n----------------------------------------\n\nTITLE: Adding Deprecation Notice in Plugin README\nDESCRIPTION: Example of how to update a plugin's README.md file to indicate that it's deprecated, specifying the version when it was deprecated, when it will be removed, and what alternatives to use.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-001-deprecation.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n**Deprecated in version v1.15.0 and scheduled for removal in v1.40.0**:\nPlease use the [tail][] plugin with the [`grok` data format][grok parser]\ninstead!\n```\n\n----------------------------------------\n\nTITLE: Configuring Yandex Cloud Monitoring Output in Telegraf (TOML)\nDESCRIPTION: This TOML configuration snippet sets up the Yandex Cloud Monitoring output plugin for Telegraf. It includes options for timeout, API endpoint, and service name. The plugin uses YC.Compute metadata-based authentication when running inside a Yandex Cloud Compute instance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/yandex_cloud_monitoring/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send aggregated metrics to Yandex.Cloud Monitoring\n[[outputs.yandex_cloud_monitoring]]\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Yandex.Cloud monitoring API endpoint. Normally should not be changed\n  # endpoint_url = \"https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write\"\n\n  ## All user metrics should be sent with \"custom\" service specified. Normally should not be changed\n  # service = \"custom\"\n```\n\n----------------------------------------\n\nTITLE: Capturing SNMP Traffic with tcpdump\nDESCRIPTION: Shell command for capturing SNMP network traffic using tcpdump. This is useful for troubleshooting SNMP connectivity issues by analyzing the actual packets being exchanged between Telegraf and the SNMP device.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nsudo tcpdump -s 0 -i eth0 -w telegraf-snmp.pcap host 127.0.0.1 and port 161\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Renegotiation in Telegraf Agent\nDESCRIPTION: Allows setting the TLS renegotiation method in the Telegraf agent.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_6\n\nLANGUAGE: go\nCODE:\n```\nagent.tls.renegotiation = \"never\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ctrlX Data Layer Input Plugin in TOML\nDESCRIPTION: This snippet shows the TOML configuration for the ctrlX Data Layer input plugin. It includes settings for server connection, authentication, and subscription details for data collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ctrlx_datalayer/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A ctrlX Data Layer server sent event input plugin\n[[inputs.ctrlx_datalayer]]\n   ## Hostname or IP address of the ctrlX CORE Data Layer server\n   ##  example: server = \"localhost\"        # Telegraf is running directly on the device\n   ##           server = \"192.168.1.1\"      # Connect to ctrlX CORE remote via IP\n   ##           server = \"host.example.com\" # Connect to ctrlX CORE remote via hostname\n   ##           server = \"10.0.2.2:8443\"    # Connect to ctrlX CORE Virtual from development environment\n   server = \"localhost\"\n\n   ## Authentication credentials\n   username = \"boschrexroth\"\n   password = \"boschrexroth\"\n\n   ## Use TLS but skip chain & host verification\n   # insecure_skip_verify = false\n\n   ## Timeout for HTTP requests. (default: \"10s\")\n   # timeout = \"10s\"\n\n\n   ## Create a ctrlX Data Layer subscription.\n   ## It is possible to define multiple subscriptions per host. Each subscription can have its own\n   ## sampling properties and a list of nodes to subscribe to.\n   ## All subscriptions share the same credentials.\n   [[inputs.ctrlx_datalayer.subscription]]\n      ## The name of the measurement. (default: \"ctrlx\")\n      measurement = \"memory\"\n\n      ## Configure the ctrlX Data Layer nodes which should be subscribed.\n      ## address - node address in ctrlX Data Layer (mandatory)\n      ## name    - field name to use in the output (optional, default: base name of address)\n      ## tags    - extra node tags to be added to the output metric (optional)\n      ## Note: \n      ## Use either the inline notation or the bracketed notation, not both.\n      ## The tags property is only supported in bracketed notation due to toml parser restrictions\n      ## Examples:\n      ## Inline notation \n      nodes=[\n         {name=\"available\", address=\"framework/metrics/system/memavailable-mb\"},\n         {name=\"used\", address=\"framework/metrics/system/memused-mb\"},\n      ]\n      ## Bracketed notation\n      # [[inputs.ctrlx_datalayer.subscription.nodes]]\n      #    name   =\"available\"\n      #    address=\"framework/metrics/system/memavailable-mb\"\n      #    ## Define extra tags related to node to be added to the output metric (optional)\n      #    [inputs.ctrlx_datalayer.subscription.nodes.tags]\n      #       node_tag1=\"node_tag1\"\n      #       node_tag2=\"node_tag2\"\n      # [[inputs.ctrlx_datalayer.subscription.nodes]]\n      #    name   =\"used\"\n      #    address=\"framework/metrics/system/memused-mb\"\n\n      ## The switch \"output_json_string\" enables output of the measurement as json. \n      ## That way it can be used in in a subsequent processor plugin, e.g. \"Starlark Processor Plugin\".\n      # output_json_string = false\n\n      ## Define extra tags related to subscription to be added to the output metric (optional)\n      # [inputs.ctrlx_datalayer.subscription.tags]\n      #    subscription_tag1 = \"subscription_tag1\"\n      #    subscription_tag2 = \"subscription_tag2\"\n\n      ## The interval in which messages shall be sent by the ctrlX Data Layer to this plugin. (default: 1s)\n      ## Higher values reduce load on network by queuing samples on server side and sending as a single TCP packet.\n      # publish_interval = \"1s\"\n\n      ## The interval a \"keepalive\" message is sent if no change of data occurs. (default: 60s)\n      ## Only used internally to detect broken network connections.\n      # keep_alive_interval = \"60s\"\n\n      ## The interval an \"error\" message is sent if an error was received from a node. (default: 10s)\n      ## Higher values reduce load on output target and network in case of errors by limiting frequency of error messages.\n      # error_interval = \"10s\"\n\n      ## The interval that defines the fastest rate at which the node values should be sampled and values captured. (default: 1s)\n      ## The sampling frequency should be adjusted to the dynamics of the signal to be sampled.\n      ## Higher sampling frequencies increases load on ctrlX Data Layer.\n      ## The sampling frequency can be higher, than the publish interval. Captured samples are put in a queue and sent in publish interval.\n      ## Note: The minimum sampling interval can be overruled by a global setting in the ctrlX Data Layer configuration ('datalayer/subscriptions/settings').\n      # sampling_interval = \"1s\"\n\n      ## The requested size of the node value queue. (default: 10)\n      ## Relevant if more values are captured than can be sent.\n      # queue_size = 10\n\n      ## The behaviour of the queue if it is full. (default: \"DiscardOldest\")\n      ## Possible values: \n      ## - \"DiscardOldest\"\n      ##   The oldest value gets deleted from the queue when it is full.\n      ## - \"DiscardNewest\"\n      ##   The newest value gets deleted from the queue when it is full.\n      # queue_behaviour = \"DiscardOldest\"\n\n      ## The filter when a new value will be sampled. (default: 0.0)\n      ## Calculation rule: If (abs(lastCapturedValue - newValue) > dead_band_value) capture(newValue).\n      # dead_band_value = 0.0\n\n      ## The conditions on which a sample should be captured and thus will be sent as a message. (default: \"StatusValue\")\n      ## Possible values:\n      ## - \"Status\"\n      ##   Capture the value only, when the state of the node changes from or to error state. Value changes are ignored.\n      ## - \"StatusValue\" \n      ##   Capture when the value changes or the node changes from or to error state.\n      ##   See also 'dead_band_value' for what is considered as a value change.\n      ## - \"StatusValueTimestamp\": \n      ##   Capture even if the value is the same, but the timestamp of the value is newer.\n      ##   Note: This might lead to high load on the network because every sample will be sent as a message\n      ##   even if the value of the node did not change.\n      # value_change = \"StatusValue\"\n      \n```\n\n----------------------------------------\n\nTITLE: Using Diff for SNMP Lookup Processor Examples\nDESCRIPTION: This diff snippet illustrates the transformation of metrics by the SNMP Lookup Processor Plugin in Telegraf. The plugin adds the 'ifName' tag to the metrics, enhancing the data with information fetched via SNMP. No additional dependencies are required beyond the configuration of the SNMP lookup tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/snmp_lookup/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- foo,index=2,source=127.0.0.1 field=123\n+ foo,ifName=eth0,index=2,source=127.0.0.1 field=123\n```\n\n----------------------------------------\n\nTITLE: Multiple Templates with Filters in TOML\nDESCRIPTION: Shows how to configure multiple templates with different patterns for 3-part and 4-part measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\ntemplates = [\n    \"*.*.* region.region.measurement\", # <- all 3-part measurements will match this one.\n    \"*.*.*.* region.region.host.measurement\", # <- all 4-part measurements will match this one.\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring RethinkDB Servers with TOML\nDESCRIPTION: This snippet illustrates how to configure the RethinkDB input plugin in a TOML format. It provides examples of server connection URIs and explains the required format depending on RethinkDB versions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/rethinkdb/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\"# Read metrics from one or many RethinkDB servers\\n[[inputs.rethinkdb]]\\n  ## An array of URI to gather stats about. Specify an ip or hostname\\n  ## with optional port add password. ie,\\n  ##   rethinkdb://user:auth_key@10.10.3.30:28105,\\n  ##   rethinkdb://10.10.3.33:18832,\\n  ##   10.0.0.1:10000, etc.\\n  servers = [\\\"127.0.0.1:28015\\\"]\\n\\n  ## If you use actual rethinkdb of > 2.3.0 with username/password authorization,\\n  ## protocol have to be named \\\"rethinkdb2\\\" - it will use 1_0 H.\\n  # servers = [\\\"rethinkdb2://username:password@127.0.0.1:28015\\\"]\\n\\n  ## If you use older versions of rethinkdb (<2.2) with auth_key, protocol\\n  ## have to be named \\\"rethinkdb\\\".\\n  # servers = [\\\"rethinkdb://username:auth_key@127.0.0.1:28015\\\"]\"\n```\n\n----------------------------------------\n\nTITLE: Setting Telegraf Binary Capabilities for Perf Metrics\nDESCRIPTION: Shell command to set the required capabilities on the Telegraf binary for collecting performance-related metrics when not running as root.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo setcap cap_sys_admin+ep <path_to_telegraf_binary>\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plaintext\nDESCRIPTION: Output from an NTP client showing time synchronization status with a remote NTP server. The output displays the remote server address, reference ID, stratum level, polling interval, reach, delay, offset, and jitter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/bad_float_parse/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  2  256   37   51.016  foobar  17.462\n```\n\n----------------------------------------\n\nTITLE: Setting POSIX ACLs for Telegraf Access\nDESCRIPTION: Shell command for granting Telegraf user access to Docker volume directories using POSIX ACLs. This command provides execute permissions to the telegraf user on Docker volume directories to allow proper monitoring of these volumes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo setfacl -R -m u:telegraf:X /var/lib/docker/volumes/\n```\n\n----------------------------------------\n\nTITLE: Testing RPM or DEB with Specific Image using LXD\nDESCRIPTION: This snippet demonstrates how to execute the package test for a specified RPM or DEB package against a designated LXD image. It requires the package name and the image to be specified as arguments.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/package_incus_test/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./package-test-lxd --package telegraf_1.21.4-1_amd64.deb --image debian/bullseye\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Neptune Apex Communication with cURL\nDESCRIPTION: Shell command using cURL to test communication with a local Apex controller, useful for troubleshooting connection issues.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/neptune_apex/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl apex.local/cgi-bin/status.xml\n```\n\n----------------------------------------\n\nTITLE: Configuring Systemd Capabilities for Telegraf\nDESCRIPTION: This text snippet shows how to configure systemd to grant CAP_NET_RAW and CAP_NET_ADMIN capabilities to the Telegraf service, allowing it to run ipset without root privileges.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipset/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW CAP_NET_ADMIN\nAmbientCapabilities=CAP_NET_RAW CAP_NET_ADMIN\n```\n\n----------------------------------------\n\nTITLE: Modifying File Permissions with chmod\nDESCRIPTION: This Bash snippet is used to grant read and execute permissions to all users for files in the '/sys/devices/virtual/powercap/intel-rapl/' directory. This is necessary due to security changes in kernel version v5.4.77 that limit access to root users. The command recursively alters the permissions, allowing non-root users to read the energy metric files. It requires 'sudo' access to execute.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsudo chmod -R a+rx /sys/devices/virtual/powercap/intel-rapl/\n```\n\n----------------------------------------\n\nTITLE: Defining Counter Metric in Prometheus Format\nDESCRIPTION: Example showing how to define a counter metric named 'test_counter' with a label in Prometheus exposition format. The metric has a TYPE declaration and a sample value of 1 with a timestamp of 1601830800000.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/metric_with_timestamp/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE test_counter counter\ntest_counter{label=\"test\"} 1 1601830800000\n```\n\n----------------------------------------\n\nTITLE: Prometheus Remote Write Output Format (v1)\nDESCRIPTION: Example of how the input data is formatted when using metric version 1, showing the line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheusremotewrite/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ngo_gc_duration_seconds,instance=localhost:9090,job=prometheus,quantile=0.99 value=4.63 1614889298859000000\n```\n\n----------------------------------------\n\nTITLE: Count-Based Webhook Point Example\nDESCRIPTION: Illustrates the structure of a count-based Papertrail webhook point in Telegraf, with simplified tagging and counting mechanism\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/papertrail/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npapertrail,host=myserver.example.com,event=saved_search_name count=3i 1453248892000000000\n```\n\n----------------------------------------\n\nTITLE: Enumeration Data Example with Network Response\nDESCRIPTION: Example of how to encode enumeration data in Telegraf metrics, showing proper tag and field usage. The 'result' is stored as a tag while the numeric code is stored as an integer field.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/REVIEWS.md#2025-04-16_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnet_response,result=success result_code=0i\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf File Input with Example Parser\nDESCRIPTION: This snippet demonstrates how to configure the Telegraf file input plugin to use the 'example' parser. It includes options for specifying input files and parser-specific configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/EXAMPLE_README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"example\"\n\n  ## Describe variables using the standard SampleConfig style.\n  ##   https://github.com/influxdata/telegraf/wiki/SampleConfig\n  example_option = \"example_value\"\n```\n\n----------------------------------------\n\nTITLE: Checking Available Credentials in Systemd\nDESCRIPTION: Commands for troubleshooting credential availability, showing how to list all credentials and decrypt specific values to verify configuration is working properly.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nCREDENTIALS_DIRECTORY=/etc/credstore.encrypted sudo systemd-creds list\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemd-creds decrypt /etc/credstore.encrypted/telegraf.http_password -\n```\n\n----------------------------------------\n\nTITLE: Minimal Reverse DNS Configuration Example\nDESCRIPTION: A simplified configuration example showing the minimal setup required to perform reverse DNS lookups on an 'ip' tag and store the result in a 'domain' tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/reverse_dns/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.reverse_dns]]\n  [[processors.reverse_dns.lookup]]\n    tag = \"ip\"\n    dest = \"domain\"\n```\n\n----------------------------------------\n\nTITLE: Example Zookeeper Metrics Output\nDESCRIPTION: This text output snippet exemplifies the format of metrics collected from a Zookeeper server. Each metric is displayed with tags such as server, port, and state, including various fields capturing the server's operational metrics like latencies, packet counts, and connection details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zookeeper/README.md#2025-04-16_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nzookeeper,server=localhost,port=2181,state=standalone ephemerals_count=0i,approximate_data_size=10044i,open_file_descriptor_count=44i,max_latency=0i,packets_received=7i,outstanding_requests=0i,znode_count=129i,max_file_descriptor_count=4096i,version=\"3.4.9-3--1\",avg_latency=0i,packets_sent=6i,num_alive_connections=1i,watch_count=0i,min_latency=0i 1522351112000000000\n```\n\n----------------------------------------\n\nTITLE: Adding Deprecation Warning to Plugin README in Markdown\nDESCRIPTION: Demonstrates how to add a deprecation notice to a plugin's README file using Markdown. This informs users about when the plugin was deprecated and what alternative to use.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEPRECATION.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# Logparser Input Plugin\n\n### **Deprecated in 1.10**: Please use the [tail][] plugin along with the\n`grok` [data format][].\n\n[tail]: /plugins/inputs/tail/README.md\n[data formats]: /docs/DATA_FORMATS_INPUT.md\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Minimum-Maximum Aggregator Output in Telegraf\nDESCRIPTION: This example shows the output of the Minimum-Maximum Aggregator plugin. It displays how the plugin aggregates the minimum and maximum values of the 'load1' field over time, appending '_min' and '_max' to the field names.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/minmax/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsystem,host=tars load1=1.72 1475583980000000000\nsystem,host=tars load1=1.6 1475583990000000000\nsystem,host=tars load1=1.66 1475584000000000000\nsystem,host=tars load1=1.63 1475584010000000000\nsystem,host=tars load1_max=1.72,load1_min=1.6 1475584010000000000\nsystem,host=tars load1=1.46 1475584020000000000\nsystem,host=tars load1=1.39 1475584030000000000\nsystem,host=tars load1=1.41 1475584040000000000\nsystem,host=tars load1_max=1.46,load1_min=1.39 1475584040000000000\n```\n\n----------------------------------------\n\nTITLE: Dynamic Backends Regex Pattern\nDESCRIPTION: Regular expression pattern for parsing dynamic backend (goto) VCL configurations in Varnish metrics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_2\n\nLANGUAGE: regex\nCODE:\n```\n^VBE\\.(?P<_vcl>[\\w\\-]*)\\.goto\\.[[:alnum:]]+\\.\\((?P<backend>\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\\)\\.\\((?P<server>.*)\\)\\.\\(ttl:\\d*\\.\\d*.*\\)\n```\n\n----------------------------------------\n\nTITLE: Configuring Streaming Ingestion in Azure Data Explorer Output\nDESCRIPTION: Adds support for streaming ingestion in the Azure Data Explorer output plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_10\n\nLANGUAGE: go\nCODE:\n```\n[[outputs.azure_data_explorer]]\n  use_streaming_ingest = true\n```\n\n----------------------------------------\n\nTITLE: Example Clean Configuration in Telegraf\nDESCRIPTION: Configuration example showing how to clean a path tag by removing redundant separators and navigation elements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n  [[processors.filepath.clean]]\n    tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ActiveMQ STOMP Output Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the STOMP output plugin for Telegraf. It specifies the host, queue name, optional authentication credentials, TLS configuration, and data format for sending metrics to an Active MQ Broker.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/stomp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for active mq with stomp protocol to send metrics to\n[[outputs.stomp]]\n  host = \"localhost:61613\"\n\n  ## Queue name for producer messages\n  queueName = \"telegraf\"\n\n  ## Username and password if required by the Active MQ server.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Data format to output.\n  data_format = \"json\"\n```\n\n----------------------------------------\n\nTITLE: Building Telegraf Package in Docker Container\nDESCRIPTION: A series of commands to be executed inside the Docker container to clone the Telegraf repository, checkout a specific version, and build a package. These steps obtain the source code and create the desired package format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PACKAGING.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngo get -d github.com/influxdata/telegraf\ncd /go/src/github.com/influxdata/telegraf\ngit checkout release-1.10\ngit reset --hard 1.10.2\nmake deps\nmake package include_packages=\"amd64.deb\"\n```\n\n----------------------------------------\n\nTITLE: Standard CSV Output Format Example in Telegraf\nDESCRIPTION: This example shows the standard CSV output format for a single metric in Telegraf. The values represent a timestamp followed by the measurement name, tags, and field values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/csv/README.md#2025-04-16_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\n1458229140,docker,raynor,30,4,...,59,660\n```\n\n----------------------------------------\n\nTITLE: Standalone Dynatrace Configuration\nDESCRIPTION: Configuration for running Telegraf without OneAgent, requiring explicit URL and API token settings for direct metric ingestion to Dynatrace.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/dynatrace/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.dynatrace]]\n  ## If no OneAgent is running on the host, url and api_token need to be set\n\n  ## Dynatrace Metrics Ingest v2 endpoint to receive metrics\n  url = \"https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest\"\n\n  ## API token is required if a URL is specified and should be restricted to the 'Ingest metrics' scope\n  api_token = \"your API token here\" // hard-coded for illustration only, should be read from environment\n```\n\n----------------------------------------\n\nTITLE: Configuring LeoFS SNMP Metrics Collection in Telegraf\nDESCRIPTION: Sample configuration for collecting metrics from LeoFS servers using Telegraf's SNMP input plugin. Allows specifying server URLs and ports for metric collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/leofs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from a LeoFS Server via SNMP\n[[inputs.leofs]]\n  ## An array of URLs of the form:\n  ##   host [ \\\":\\\" port]\n  servers = [\"127.0.0.1:4010\"]\n```\n\n----------------------------------------\n\nTITLE: Example Output of Date Processor Plugin\nDESCRIPTION: Demonstrates how the date processor plugin transforms metric data by adding a month tag (June) to a throughput measurement, based on the timestamp of the metric.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/date/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- throughput lower=10i,upper=1000i,mean=500i 1560540094000000000\n+ throughput,month=Jun lower=10i,upper=1000i,mean=500i 1560540094000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudo for Ipset Plugin\nDESCRIPTION: This bash snippet demonstrates how to configure sudo permissions for the Telegraf user to run the ipset save command without a password. It includes the necessary sudoers file configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipset/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# Add the following line:\nCmnd_Alias IPSETSAVE = /sbin/ipset save\ntelegraf  ALL=(root) NOPASSWD: IPSETSAVE\nDefaults!IPSETSAVE !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Building Custom Builder with Make\nDESCRIPTION: This command builds the custom_builder tool using the Make build system. Ensure that Make and Golang are installed and available in your system path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# make build_tools\n```\n\n----------------------------------------\n\nTITLE: Listing Players with Recorded Scores Command (Shell Script)\nDESCRIPTION: This shell command lists all players with recorded scores in the Minecraft scoreboard. Useful for verifying that the plugin is capturing metrics accurately.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/minecraft/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n\"/scoreboard players list\"\n```\n\n----------------------------------------\n\nTITLE: Message C Binary Structure\nDESCRIPTION: Definition of Message C format showing field layout including ID, type, length, value x, value y, and timestamp fields in little-endian format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/binary/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n+--------+------+------+------------+------------+--------------------+\n| ID     | type | len  | value x    | value y    | timestamp          |\n+--------+------+------+------------+------------+--------------------+\n| 0x0201 | 0x0C | 0x10 | 0x4DF82D40 | 0x5F305C08 | 0x10D4DF6200000000 |\n+--------+------+------+------------+------------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic OPC UA Node in Telegraf\nDESCRIPTION: Example of a simple OPC UA node configuration that defines a temperature node with namespace 3, string identifier type, and 'Temperature' as the identifier value.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n{name=\"temp\", namespace=\"3\", identifier_type=\"s\", identifier=\"Temperature\"},\n```\n\n----------------------------------------\n\nTITLE: Setting Unix Permissions for Telegraf\nDESCRIPTION: Shell commands to set up Unix permissions for the Telegraf user to access Postfix queue directories. This includes changing group ownership, modifying permissions, and adding Telegraf to the postdrop group.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postfix/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo chgrp -R telegraf /var/spool/postfix/{active,hold,incoming,deferred}\nsudo chmod -R g+rXs /var/spool/postfix/{active,hold,incoming,deferred}\nsudo usermod -a -G postdrop telegraf\nsudo chmod g+r /var/spool/postfix/maildrop\n```\n\n----------------------------------------\n\nTITLE: Sample Syslog Message with Non-ASCII Character\nDESCRIPTION: A sample syslog message in RFC5424 format with severity level 1 (alert). The message contains a non-ASCII character () which might be used to test Unicode handling in syslog parsers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcp_1st_utf8_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n23 <1>1 - - - - - - hell\n```\n\n----------------------------------------\n\nTITLE: Declaring Telegraf Logger in Go Plugins\nDESCRIPTION: Example of how to properly declare the Telegraf logger in a plugin struct instead of using the log package directly. The toml tag \"-\" ensures the logger is not editable from the config.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/REVIEWS.md#2025-04-16_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\n  Log telegraf.Logger `toml:\"-\"`\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes Service Metrics with Telegraf\nDESCRIPTION: This snippet logs a Kubernetes Service's creation and associated configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_service,cluster_ip=172.29.61.80,namespace=redis-cache-0001,port_name=redis,port_protocol=TCP,selector_app=myapp,selector_io.kompose.service=redis,selector_role=slave,service_name=redis-slave created=1588690034000000000i,generation=0i,port=6379i,target_port=0i 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Setting Linux Capabilities for Telegraf\nDESCRIPTION: Command to set necessary Linux capabilities for Telegraf to read PMT telemetry files when not running as root. Adds the cap_dac_read_search capability to the Telegraf executable.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsudo setcap cap_dac_read_search+ep /usr/bin/telegraf\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters with Sufficient Defaults in TOML\nDESCRIPTION: Example showing how to format parameters with defaults that are usually sufficient - these should be commented out.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n  ## What an exchange type is.\n  # exchange_type = \"topic\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudo Permissions for PS Command\nDESCRIPTION: Example sudoers configuration to allow passwordless execution of the ps command for collecting system-wide process statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/processes/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nCmnd_Alias PS = /bin/ps\n<username> ALL=(root) NOPASSWD: PS\nDefaults!PS !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: NTP Peer Status Output\nDESCRIPTION: Standard NTP peer status display showing synchronization metrics for a remote NTP server. Contains fields for remote server name, reference ID, stratum level, type, poll interval, reach, delay, offset and jitter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/single_reach_count/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Riemann Event with Status and Description\nDESCRIPTION: Example of Riemann event with string_as_state enabled and custom description text.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/riemann/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n#riemann.codec.Event{\n:host \"postgresql-1e612b44-e92f-4d27-9f30-5e2f53947870\", :state \"Running\", :ttl 30.0,\n:description \"PostgreSQL master node is up and running\",\n:service \"status\", :time 1475605021}\n```\n\n----------------------------------------\n\nTITLE: Updating jwt-go Module in Go\nDESCRIPTION: Updates the jwt-go module to address CVE-2020-26160 in the DCOS input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_22\n\nLANGUAGE: go\nCODE:\n```\nUpdate jwt-go module to address CVE-2020-26160\n```\n\n----------------------------------------\n\nTITLE: Example Output of Device (Thing) Metrics from Neoom Beaam\nDESCRIPTION: Sample output of device-specific metrics collected by the Neoom Beaam Input Plugin. It shows various measurements from an electricity meter including voltage, frequency, and energy readings with their respective units and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/neoom_beaam/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nneoom_beaam_thing,datapoint=CONNECTION,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=None value=true 1723890960060000000\nneoom_beaam_thing,datapoint=VOLTAGE_P1,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=V value=245 1723905135056000000\nneoom_beaam_thing,datapoint=VOLTAGE_P2,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=V value=242.3 1723905135056000000\nneoom_beaam_thing,datapoint=VOLTAGE_P3,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=V value=245.5 1723905135056000000\nneoom_beaam_thing,datapoint=FREQUENCY,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=Hz value=49.98 1723905135056000000\nneoom_beaam_thing,datapoint=ACTIVE_POWER,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=W value=-826 1723905135056000000\nneoom_beaam_thing,datapoint=OUTPUT_ENERGY,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=Wh value=241826.44736444377 1723905135075000064\nneoom_beaam_thing,datapoint=INPUT_ENERGY,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=Wh value=22988.396324999278 1723904090096000000\nneoom_beaam_thing,datapoint=POWER_P1,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=W value=-305 1723905135056000000\nneoom_beaam_thing,datapoint=POWER_P2,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=W value=-268 1723905135056000000\nneoom_beaam_thing,datapoint=POWER_P3,source=127.0.0.1,thing=ELECTRICITY_METER_AC,unit=W value=-214 1723905135056000000\n```\n\n----------------------------------------\n\nTITLE: Influx Line Protocol Data for Hue Bridge Sensors\nDESCRIPTION: Line protocol formatted data showing measurements from various Hue Bridge sensors. Includes light states, temperature readings, light levels, motion detection, and battery levels with device identifiers, room assignments, and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/huebridge/testdata/metrics/huebridge.txt#2025-04-16_snippet_0\n\nLANGUAGE: influx\nCODE:\n```\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#3,room=Name#15 on=0i 1737181537879611000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#8,room=Name#14 on=0i 1737181537879628000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#12,room=Name#16 on=0i 1737181537879632000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#6,room=Name#13 on=0i 1737181537879634000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#1,room=Name#13 on=0i 1737181537879635000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#2,room=Name#13 on=0i 1737181537879637000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#5,room=Name#15 on=0i 1737181537879639000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#9,room=Name#13 on=0i 1737181537879640000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#11,room=Name#15 on=0i 1737181537879642000\nhuebridge_light,bridge_id=0123456789ABCDEF,device=Name#4,room=Name#14 on=0i 1737181537879646000\nhuebridge_temperature,bridge_id=0123456789ABCDEF,device=Name#7,enabled=true,room=Name#15 temperature=17.6299991607666 1737181537879828000\nhuebridge_light_level,bridge_id=0123456789ABCDEF,device=Name#7,enabled=true,room=Name#15 light_level=18948i,light_level_lux=78.46934003526889 1737181537880034000\nhuebridge_motion_sensor,bridge_id=0123456789ABCDEF,device=Name#7,enabled=true,room=Name#15 motion=0i 1737181537880213000\nhuebridge_device_power,bridge_id=0123456789ABCDEF,device=Name#7,room=Name#15 battery_level=100i 1737181537880360000\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf Service with Full Config Path\nDESCRIPTION: Correct way to install Telegraf service with full path to the configuration file to avoid service startup errors.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" --config \"C:\\MyConfigs\\telegraf.conf\" service install\n```\n\n----------------------------------------\n\nTITLE: Displaying Profile Data in Text Format for Telegraf\nDESCRIPTION: This snippet demonstrates the format of profile data used in the Telegraf project. Each line represents a single profile sample with various attributes including address, host name, profile ID, sample details, build information, and timing data. The data is structured for easy parsing and analysis of performance metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opentelemetry/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nprofiles,address=95210353,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=0,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"fab9b8c848218405738c11a7ec4982e9\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=18694144u,filename=\"chromium\",frame_type=\"native\",location=\"\",memory_limit=250413056u,memory_start=18698240u,stack_trace_id=\"hYmAzQVF8vy8MWbzsKpQNw\",start_time_unix_nano=1721306050081621681u,value=1i 1721306048731622020\nprofiles,address=15945263,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=1,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"7dab4a2e0005d025e75cc72191f8d6bf\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=15638528u,filename=\"dockerd\",frame_type=\"native\",location=\"\",memory_limit=47255552u,memory_start=15638528u,stack_trace_id=\"4N3KEcGylb5Qoi2905c1ZA\",start_time_unix_nano=1721306050081621681u,value=1i 1721306049831718725\nprofiles,address=15952400,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=1,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"7dab4a2e0005d025e75cc72191f8d6bf\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=15638528u,filename=\"dockerd\",frame_type=\"native\",location=\"\",memory_limit=47255552u,memory_start=15638528u,stack_trace_id=\"4N3KEcGylb5Qoi2905c1ZA\",start_time_unix_nano=1721306050081621681u,value=1i 1721306049831718725\nprofiles,address=15953899,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=1,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"7dab4a2e0005d025e75cc72191f8d6bf\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=15638528u,filename=\"dockerd\",frame_type=\"native\",location=\"\",memory_limit=47255552u,memory_start=15638528u,stack_trace_id=\"4N3KEcGylb5Qoi2905c1ZA\",start_time_unix_nano=1721306050081621681u,value=1i 1721306049831718725\nprofiles,address=16148175,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=1,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"7dab4a2e0005d025e75cc72191f8d6bf\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=15638528u,filename=\"dockerd\",frame_type=\"native\",location=\"\",memory_limit=47255552u,memory_start=15638528u,stack_trace_id=\"4N3KEcGylb5Qoi2905c1ZA\",start_time_unix_nano=1721306050081621681u,value=1i 1721306049831718725\nprofiles,address=4770577,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=2,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"cfc3dc7d1638c1284a6b62d4b5c0d74e\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=0u,filename=\"\",frame_type=\"kernel\",location=\"do_epoll_wait\",memory_limit=0u,memory_start=0u,stack_trace_id=\"UaO9bysJnAYXFYobSdHXqg\",start_time_unix_nano=1721306050081621681u,value=1i 1721306050081621681\nprofiles,address=4773632,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=2,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"cfc3dc7d1638c1284a6b62d4b5c0d74e\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=0u,filename=\"\",frame_type=\"kernel\",location=\"__x64_sys_epoll_wait\",memory_limit=0u,memory_start=0u,stack_trace_id=\"UaO9bysJnAYXFYobSdHXqg\",start_time_unix_nano=1721306050081621681u,value=1i 1721306050081621681\nprofiles,address=14783666,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=2,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"cfc3dc7d1638c1284a6b62d4b5c0d74e\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=0u,filename=\"\",frame_type=\"kernel\",location=\"do_syscall_64\",memory_limit=0u,memory_start=0u,stack_trace_id=\"UaO9bysJnAYXFYobSdHXqg\",start_time_unix_nano=1721306050081621681u,value=1i 1721306050081621681\nprofiles,address=16777518,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=2,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"cfc3dc7d1638c1284a6b62d4b5c0d74e\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=0u,filename=\"\",frame_type=\"kernel\",location=\"entry_SYSCALL_64_after_hwframe\",memory_limit=0u,memory_start=0u,stack_trace_id=\"UaO9bysJnAYXFYobSdHXqg\",start_time_unix_nano=1721306050081621681u,value=1i 1721306050081621681\nprofiles,address=1139937,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=2,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"982ed6c7a77f99f0ae746be0187953bf\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=147456u,filename=\"libc.so.6\",frame_type=\"native\",location=\"\",memory_limit=1638400u,memory_start=147456u,stack_trace_id=\"UaO9bysJnAYXFYobSdHXqg\",start_time_unix_nano=1721306050081621681u,value=1i 1721306050081621681\nprofiles,address=117834912,host.name=testbox,profile_id=618098d29a6cefd6a4c0ea806880c2a8,sample=2,sample_name=cpu,sample_type=samples,sample_type_unit=count,sample_unit=nanoseconds build_id=\"fab9b8c848218405738c11a7ec4982e9\",build_id_type=\"BUILD_ID_BINARY_HASH\",end_time_unix_nano=1721306050081621681u,file_offset=18694144u,filename=\"chromium\",frame_type=\"native\",location=\"\",memory_limit=250413056u,memory_start=18698240u,stack_trace_id=\"UaO9bysJnAYXFYobSdHXqg\",start_time_unix_nano=1721306050081621681u,value=1i 1721306050081621681\n```\n\n----------------------------------------\n\nTITLE: Ceph PGMap State Format - Pre v1.3\nDESCRIPTION: Example showing the old format of ceph_pgmap_state metric content before Telegraf v1.3, where states were expressed as direct field names.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n# field_name             value\nactive+clean             123\nactive+clean+scrubbing   3\n```\n\n----------------------------------------\n\nTITLE: Riemann Event with Additional Tags\nDESCRIPTION: Example showing Riemann event output with custom tags configured.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/riemann/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n#riemann.codec.Event{\n:host \"postgresql-1e612b44-e92f-4d27-9f30-5e2f53947870\", :state nil, :description nil, :ttl 30.0,\n:service \"disk/used_percent\", :metric 73.16736001949994, :path \"/boot\", :fstype \"ext4\", :time 1475605021,\n:tags [\"telegraf\" \"postgres_cluster\"]}\n```\n\n----------------------------------------\n\nTITLE: Non-Aggregated Uncore Event Metrics Output Format\nDESCRIPTION: Example output showing non-aggregated uncore PMU metrics with XSNP response events. Includes tags for event type, host, socket, unit and unit_type with corresponding counter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmu/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit=uncore_cbox_0,unit_type=cbox enabled=2870630747i,running=2870630747i,raw=183996i,scaled=183996i 1621254096000000000\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit=uncore_cbox_1,unit_type=cbox enabled=2870608194i,running=2870608194i,raw=185703i,scaled=185703i 1621254096000000000\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit=uncore_cbox_2,unit_type=cbox enabled=2870600211i,running=2870600211i,raw=187331i,scaled=187331i 1621254096000000000\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit=uncore_cbox_3,unit_type=cbox enabled=2870593914i,running=2870593914i,raw=184228i,scaled=184228i 1621254096000000000\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit=uncore_cbox_4,unit_type=cbox scaled=195355i,enabled=2870558952i,running=2870558952i,raw=195355i 1621254096000000000\npmu_metric,event=UNC_CBO_XSNP_RESPONSE.MISS_XCORE,host=xyz,socket=0,unit=uncore_cbox_5,unit_type=cbox enabled=2870554131i,running=2870554131i,raw=197756i,scaled=197756i 1621254096000000000\n```\n\n----------------------------------------\n\nTITLE: CSV Input with Metadata for Telegraf Parser\nDESCRIPTION: This snippet demonstrates a CSV input file with metadata rows, a header row, and a data row. It includes version information and file creation timestamp as metadata.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/csv/README.md#2025-04-16_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\n# Version=1.1\n# File Created: 2021-11-17T07:02:45+10:00\nVersion,measurement,cpu,time_user,time_system,time_idle,time\n1.2,cpu,cpu0,42,42,42,2018-09-13T13:03:28Z\n```\n\n----------------------------------------\n\nTITLE: Example Output of LVM Metrics (Text)\nDESCRIPTION: This text snippet shows example output metrics generated by the LVM input plugin for different types of logical volumes, volume groups, and physical volumes. It demonstrates the expected format and included data points.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lvm/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n\"lvm_physical_vol,path=/dev/sda2,vol_group=vgroot free=0i,size=249510756352i,used=249510756352i,used_percent=100 1631823026000000000\\nlvm_physical_vol,path=/dev/sdb,vol_group=docker free=3858759680i,size=128316342272i,used=124457582592i,used_percent=96.99277612525741 1631823026000000000\\nlvm_vol_group,name=vgroot free=0i,logical_volume_count=1i,physical_volume_count=1i,size=249510756352i,snapshot_count=0i,used_percent=100 1631823026000000000\\nlvm_vol_group,name=docker free=3858759680i,logical_volume_count=1i,physical_volume_count=1i,size=128316342272i,snapshot_count=0i,used_percent=96.99277612525741 1631823026000000000\\nlvm_logical_vol,name=lvroot,vol_group=vgroot data_percent=0,metadata_percent=0,size=249510756352i 1631823026000000000\\nlvm_logical_vol,name=thinpool,vol_group=docker data_percent=0.36000001430511475,metadata_percent=1.3300000429153442,size=121899057152i 1631823026000000000\\n\"\n```\n\n----------------------------------------\n\nTITLE: Testing Bond Plugin Configuration with Telegraf\nDESCRIPTION: Command-line example showing how to test the bond input plugin configuration with Telegraf in test mode.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bond/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf --config telegraf.conf --input-filter bond --test\n```\n\n----------------------------------------\n\nTITLE: Enabling Telegraf Profiling\nDESCRIPTION: Command to start Telegraf with profiling enabled by specifying the pprof address parameter\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROFILING.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf --config telegraf.conf --pprof-addr localhost:6060\n```\n\n----------------------------------------\n\nTITLE: Maximum Response Time Aggregation Configuration\nDESCRIPTION: Configuration example for aggregating maximum response times per method and URI from Elasticsearch logs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch_query/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.elasticsearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"my-index-*\"\n  filter_query = \"*\"\n  metric_fields = [\"response_time\"]\n  metric_function = \"max\"\n  tags = [\"method.keyword\",\"URI.keyword\"]\n  include_missing_tag = false\n  missing_tag_value = \"null\"\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Logging in Go Agent Code\nDESCRIPTION: Illustrates how to log messages in non-plugin sections of Telegraf. It includes the log level and module manually.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/LOGGING.md#2025-04-16_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nlog.Printf(\"E! [agent] Error writing to %s: %v\", output.LogName(), err)\n```\n\n----------------------------------------\n\nTITLE: Marking Deprecated Options in TOML Configuration\nDESCRIPTION: Shows how to indicate deprecated options in the sample TOML configuration file. The example includes a comment specifying when the option was deprecated and what replacement to use.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEPRECATION.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n  ## Broker to publish to.\n  ##   deprecated in 1.7; use the brokers option\n  # url = \"amqp://localhost:5672/influxdb\"\n```\n\n----------------------------------------\n\nTITLE: Generating Plugin-Specific Telegraf Configuration\nDESCRIPTION: Command to generate a configuration for a specific plugin (influxdb in this example) using the --usage flag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf --usage influxdb\n```\n\n----------------------------------------\n\nTITLE: Example Output of Quantile Aggregator Plugin\nDESCRIPTION: Sample output showing the raw input metrics followed by the quantile aggregator results. Demonstrates how original numeric fields are transformed into quantile fields (25th, 50th, 75th percentiles) with the field name format <fieldname>_<quantile*100>.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/quantile/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncpu,cpu=cpu-total,host=Hugin usage_user=10.814851731872487,usage_system=2.1679541490155687,usage_irq=1.046598554697342,usage_steal=0,usage_guest_nice=0,usage_idle=85.79616247197244,usage_nice=0,usage_iowait=0,usage_softirq=0.1744330924495688,usage_guest=0 1608288360000000000\ncpu,cpu=cpu-total,host=Hugin usage_guest=0,usage_system=2.1601016518428664,usage_iowait=0.02541296060990694,usage_irq=1.0165184243964942,usage_softirq=0.1778907242693666,usage_steal=0,usage_guest_nice=0,usage_user=9.275730622616953,usage_idle=87.34434561626493,usage_nice=0 1608288370000000000\ncpu,cpu=cpu-total,host=Hugin usage_idle=85.78199052131747,usage_nice=0,usage_irq=1.0476428036915637,usage_guest=0,usage_guest_nice=0,usage_system=1.995510102269591,usage_iowait=0,usage_softirq=0.1995510102269662,usage_steal=0,usage_user=10.975305562484735 1608288380000000000\ncpu,cpu=cpu-total,host=Hugin usage_guest_nice_075=0,usage_user_050=10.814851731872487,usage_guest_075=0,usage_steal_025=0,usage_irq_025=1.031558489546918,usage_irq_075=1.0471206791944527,usage_iowait_025=0,usage_guest_050=0,usage_guest_nice_050=0,usage_nice_075=0,usage_iowait_050=0,usage_system_050=2.1601016518428664,usage_irq_050=1.046598554697342,usage_guest_nice_025=0,usage_idle_050=85.79616247197244,usage_softirq_075=0.1887208672481664,usage_steal_075=0,usage_system_025=2.0778058770562287,usage_system_075=2.1640279004292173,usage_softirq_050=0.1778907242693666,usage_nice_050=0,usage_iowait_075=0.01270648030495347,usage_user_075=10.895078647178611,usage_nice_025=0,usage_steal_050=0,usage_user_025=10.04529117724472,usage_idle_025=85.78907649664495,usage_idle_075=86.57025404411868,usage_softirq_025=0.1761619083594677,usage_guest_025=0 1608288390000000000\n```\n\n----------------------------------------\n\nTITLE: Running Custom Builder with Remote Configuration\nDESCRIPTION: This command runs the custom_builder tool, downloading the configuration from a specified remote URL, which should be accessible and return a valid configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# ./tools/custom_builder/custom_builder --config http://myserver/telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: Item Prototype Key Format in Zabbix\nDESCRIPTION: Example of the key format used for item prototypes in Zabbix discovery rules to receive Telegraf metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ntelegraf.net_response.response_time[{#PORT},{#PROTOCOL},{#SERVER}]\ntelegraf.net_response.result_type[{#PORT},{#PROTOCOL},{#SERVER}]\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes Node Metrics with Telegraf\nDESCRIPTION: This snippet logs various metrics related to a Kubernetes Node, such as node count and conditions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_node,host=vjain node_count=8i 1628918652000000000\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf for Secret Access\nDESCRIPTION: Command to access Telegraf's help documentation for working with secrets.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/oauth2/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf secrets help\n```\n\n----------------------------------------\n\nTITLE: Example Output of Power Statistics\nDESCRIPTION: The text provides example output format of power statistics collected from a Linux system. It shows metrics like thermal design power, current power consumption, CPU base frequency, and various power states. These outputs are logged in a specific format that includes time stamps and system identifiers, and is typically used in monitoring and performance analysis.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_15\n\nLANGUAGE: text\nCODE:\n```\npowerstat_package,host=ubuntu,package_id=0 thermal_design_power_watts=160 1606494744000000000\npowerstat_package,host=ubuntu,package_id=0 current_power_consumption_watts=35 1606494744000000000\npowerstat_package,host=ubuntu,package_id=0 cpu_base_frequency_mhz=2400i 1669118424000000000\npowerstat_package,host=ubuntu,package_id=0 current_dram_power_consumption_watts=13.94 1606494744000000000\npowerstat_package,host=ubuntu,package_id=0,active_cores=0 max_turbo_frequency_mhz=3000i 1606494744000000000\npowerstat_package,host=ubuntu,package_id=0,active_cores=1 max_turbo_frequency_mhz=2800i 1606494744000000000\npowerstat_package,die=0,host=ubuntu,package_id=0,type=initial uncore_frequency_limit_mhz_min=800,uncore_frequency_limit_mhz_max=2400 1606494744000000000\npowerstat_package,die=0,host=ubuntu,package_id=0,type=current uncore_frequency_mhz_cur=800i,uncore_frequency_limit_mhz_min=800,uncore_frequency_limit_mhz_max=2400 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_frequency_mhz=1200.29 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_temperature_celsius=34i 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c0_state_residency_percent=0.8 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c1_state_residency_percent=6.68 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c3_state_residency_percent=0 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c6_state_residency_percent=92.52 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c7_state_residency_percent=0 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_busy_frequency_mhz=1213.24 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c0_substate_c01_percent=0 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c0_substate_c02_percent=5.68 1606494744000000000\npowerstat_core,core_id=0,cpu_id=0,host=ubuntu,package_id=0 cpu_c0_substate_c0_wait_percent=43.74 1606494744000000000\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plain Text\nDESCRIPTION: This snippet shows the output of an NTP server status command. It displays detailed synchronization information for a remote NTP server, including its name, reference ID, stratum, type, polling interval, reachability, delay, offset, and jitter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/single/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Example Riemann Event Output\nDESCRIPTION: Sample Riemann event output showing default configuration with disk metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/riemann/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n#riemann.codec.Event{\n:host \"postgresql-1e612b44-e92f-4d27-9f30-5e2f53947870\", :state nil, :description nil, :ttl 30.0,\n:service \"disk/used_percent\", :metric 73.16736001949994, :path \"/boot\", :fstype \"ext4\", :time 1475605021}\n```\n\n----------------------------------------\n\nTITLE: Sample Output Format for S7 Protocol Metrics\nDESCRIPTION: Example output showing the format of metrics collected from a Siemens PLC, including timestamp, tags, and various field values with different data types.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/s7comm/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ns7comm,host=Hugin rpm=712i,status_ok=true,last_error=\"empty slot\",last_error_time=1611319681000000000i 1611332164000000000\n```\n\n----------------------------------------\n\nTITLE: Starting Telegraf Service with net command\nDESCRIPTION: Command to start the Telegraf Windows service using the net start command.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n> net start telegraf\n```\n\n----------------------------------------\n\nTITLE: Logs Output Example\nDESCRIPTION: Example output showing how logs are formatted in the InfluxDB line protocol, including various log attributes and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opentelemetry/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nlogs fluent.tag=\"fluent.info\",pid=18i,ppid=9i,worker=0i 1613769568895331700\nlogs fluent.tag=\"fluent.debug\",instance=1720i,queue_size=0i,stage_size=0i 1613769568895697200\nlogs fluent.tag=\"fluent.info\",worker=0i 1613769568896515100\n```\n\n----------------------------------------\n\nTITLE: Setting HOST_MOUNT_PREFIX Environment Variable for Disk Plugin\nDESCRIPTION: The disk input plugin can now be configured with the HOST_MOUNT_PREFIX environment variable. This value is prepended to any mountpaths discovered before retrieving stats, but is not included in the report path. This allows reporting host disk stats when running from within a container.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_10\n\nLANGUAGE: markdown\nCODE:\n```\nThe `disk` input plugin can now be configured with the `HOST_MOUNT_PREFIX` environment variable.\nThis value is prepended to any mountpaths discovered before retrieving stats.\nIt is not included on the report path. This is necessary for reporting host disk stats when running from within a container.\n```\n\n----------------------------------------\n\nTITLE: Example Nginx VTS Metrics Output\nDESCRIPTION: Sample output showing the metrics collected by the Nginx VTS plugin, including connection stats, server performance, filter data, upstream server info, and cache statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_vts/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnginx_vts_connections,source=localhost,port=80,host=localhost waiting=30i,accepted=295333i,handled=295333i,requests=6833487i,active=33i,reading=0i,writing=3i 1518341521000000000\nnginx_vts_server,zone=example.com,port=80,host=localhost,source=localhost cache_hit=158915i,in_bytes=1935528964i,out_bytes=6531366419i,response_2xx_count=809994i,response_4xx_count=16664i,cache_bypass=0i,cache_stale=0i,cache_revalidated=0i,requests=2187977i,response_1xx_count=0i,response_3xx_count=1360390i,cache_miss=2249i,cache_updating=0i,cache_scarce=0i,request_time=13i,response_5xx_count=929i,cache_expired=0i 1518341521000000000\nnginx_vts_server,host=localhost,source=localhost,port=80,zone=* requests=6775284i,in_bytes=5003242389i,out_bytes=36858233827i,cache_expired=318881i,cache_updating=0i,request_time=51i,response_1xx_count=0i,response_2xx_count=4385916i,response_4xx_count=83680i,response_5xx_count=1186i,cache_bypass=0i,cache_revalidated=0i,cache_hit=1972222i,cache_scarce=0i,response_3xx_count=2304502i,cache_miss=408251i,cache_stale=0i 1518341521000000000\nnginx_vts_filter,filter_key=FI,filter_name=country,port=80,host=localhost,source=localhost request_time=0i,in_bytes=139701i,response_3xx_count=0i,out_bytes=2644495i,response_1xx_count=0i,cache_expired=0i,cache_scarce=0i,requests=179i,cache_miss=0i,cache_bypass=0i,cache_stale=0i,cache_updating=0i,cache_revalidated=0i,cache_hit=0i,response_2xx_count=177i,response_4xx_count=2i,response_5xx_count=0i 1518341521000000000\nnginx_vts_upstream,port=80,host=localhost,upstream=backend_cluster,upstream_address=127.0.0.1:6000,source=localhost fail_timeout=10i,backup=false,request_time=31i,response_5xx_count=1081i,response_2xx_count=1877498i,max_fails=1i,in_bytes=2763336289i,out_bytes=19470265071i,weight=1i,down=false,response_time=31i,response_1xx_count=0i,response_4xx_count=76125i,requests=3379232i,response_3xx_count=1424528i 1518341521000000000\nnginx_vts_cache,source=localhost,port=80,host=localhost,zone=example stale=0i,used_bytes=64334336i,miss=394573i,bypass=0i,expired=318788i,updating=0i,revalidated=0i,hit=689883i,scarce=0i,max_bytes=9223372036854775296i,in_bytes=1111161581i,out_bytes=19175548290i 1518341521000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Discard Output Plugin in Telegraf\nDESCRIPTION: Basic TOML configuration for the Discard output plugin. This plugin requires no specific configuration parameters as it simply discards all incoming metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/discard/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Send metrics to nowhere at all\n[[outputs.discard]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Example OPC UA Metrics Output from Configured Groups\nDESCRIPTION: Sample output showing the metrics generated by the OPC UA listener plugin based on the configured groups and nodes. Demonstrates how tags from both groups and nodes are included in the output.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ngroup1_metric_name,group1_tag=val1,id=ns\\=3;i\\=1001,node1_tag=val2 name=0,Quality=\"OK (0x0)\" 1606893246000000000\ngroup1_metric_name,group1_tag=val1,id=ns\\=3;i\\=1002,node1_tag=val3 name=-1.389117,Quality=\"OK (0x0)\" 1606893246000000000\ngroup2_metric_name,group2_tag=val3,id=ns\\=3;i\\=1003,node2_tag=val4 Quality=\"OK (0x0)\",saw=-1.6 1606893246000000000\ngroup2_metric_name,group2_tag=val3,id=ns\\=3;i\\=1004 sin=1.902113,Quality=\"OK (0x0)\" 1606893246000000000\n```\n\n----------------------------------------\n\nTITLE: Key Value Storage Regex Pattern\nDESCRIPTION: Regular expression for parsing key-value storage (kvstore) metrics in Varnish\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_3\n\nLANGUAGE: regex\nCODE:\n```\n^KVSTORE\\.(?P<id>[\\w\\-]*)\\.(?P<_vcl>[\\w\\-]*)\\.([\\w\\-]*)\n```\n\n----------------------------------------\n\nTITLE: Utilizing Time and Metric Names from XML in Telegraf\nDESCRIPTION: Illustrates how to derive metric names and timestamps directly from the XML document using the XPath configuration in TOML. The settings demonstrate extracting these dynamic values from the XML structure instead of relying on fixed configuration data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example.xml\"]\n  data_format = \"xml\"\n\n  [[inputs.file.xpath]]\n    metric_name = \"name(/Gateway/Status)\"\n\n    timestamp = \"/Gateway/Timestamp\"\n    timestamp_format = \"2006-01-02T15:04:05Z\"\n\n    [inputs.file.xpath.tags]\n      gateway = \"substring-before(/Gateway/Name, ' ')\"\n\n    [inputs.file.xpath.fields]\n      ok = \"/Gateway/Status = 'ok'\"\n```\n\n----------------------------------------\n\nTITLE: Showing Alternative Parameter Examples in TOML\nDESCRIPTION: Example showing how to provide examples of possible settings that differ from the default value.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.\n  ##   example: routing_key = \"telegraf\"\n  # routing_key = \"\"\n```\n\n----------------------------------------\n\nTITLE: Interpreting InfluxDB Metrics Output\nDESCRIPTION: The text provides example metrics output from the InfluxDB input plugin, showing data on various measurements such as databases, HTTP requests, and TSM caches. Each line represents a different data point with associated tags, fields, and timestamps. The output is the result of the Telegraf test command which uses these metrics to provide insights into system and application performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ninfluxdb_database,database=_internal,...\n```\n\n----------------------------------------\n\nTITLE: Querying Temperature on Windows using WMIC\nDESCRIPTION: This shell command uses the Windows Management Instrumentation Command-line (WMIC) to query the `MSAcpi_ThermalZoneTemperature` class in the `root\\wmi` namespace. It's used to check if the system supports querying temperature values and can be used for troubleshooting on Windows systems.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/temp/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n\"wmic /namespace:\\\\\\\\root\\\\wmi PATH MSAcpi_ThermalZoneTemperature\"\n```\n\n----------------------------------------\n\nTITLE: Enabling SMART on a Storage Device\nDESCRIPTION: Command to enable SMART functionality on a storage device.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsmartctl -s on <device>\n```\n\n----------------------------------------\n\nTITLE: Filtered Document Count by Response Status\nDESCRIPTION: Configuration for counting documents matching a filter query, aggregated by response status code.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch_query/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.elasticsearch_query.aggregation]]\n  measurement_name = \"http_logs\"\n  index = \"*\"\n  filter_query = \"downloads\"\n  tags = [\"response.keyword\"]\n  include_missing_tag = false\n  date_field = \"@timestamp\"\n  query_period = \"1m\"\n```\n\n----------------------------------------\n\nTITLE: Defining Required Parameters in TOML Configuration\nDESCRIPTION: Example showing how to format parameters that must be set by users or don't have default values - these should be uncommented.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n  ## Brokers are the AMQP brokers to connect to.\n  brokers = [\"amqp://localhost:5672\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring OPC UA Client Listener in TOML\nDESCRIPTION: Sample configuration file showing the configuration structure for the OPC UA client listener input plugin. This is indicated by the @sample.conf annotation but no actual configuration is provided in the input text.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opcua_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n```toml\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Integer Value Encoding\nDESCRIPTION: This snippet provides an example of the error message returned by OpenSearch when it fails to parse an integer value encoded in JSON by Golang's JSON encoder. It highlights the limitations in handling large integer values and outlines the conditions leading to dropped metrics due to unsupported field mappings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opensearch/README.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": {\n    \"root_cause\": [\n      {\"type\": \"mapper_parsing_exception\", \"reason\": \"failed to parse\"}\n    ],\n    \"type\": \"mapper_parsing_exception\",\n    \"reason\": \"failed to parse\",\n    \"caused_by\": {\n      \"type\": \"illegal_state_exception\",\n      \"reason\": \"No matching token for number_type [BIG_INTEGER]\"\n    }\n  },\n  \"status\": 400\n}\n```\n\n----------------------------------------\n\nTITLE: Migrating from Logparser to Tail Plugin - Telegraf (Diff)\nDESCRIPTION: This code snippet demonstrates the differences in configuration when migrating from the deprecated logparser input plugin to the tail input plugin. The changes involve renaming parameters and adjusting their configurations to align with the tail plugin specifications.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/logparser/README.md#2025-04-16_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n\"```diff\\n- [[inputs.logparser]]\\n-   files = [\\\"/var/log/apache/access.log\\\"]\\n-   from_beginning = false\\n-   [inputs.logparser.grok]\\n-     patterns = [\\\"%{COMBINED_LOG_FORMAT}\\\"]\\n-     measurement = \\\"apache_access_log\\\"\\n-     custom_pattern_files = []\\n-     custom_patterns = '''\\n-     '''\\n-     timezone = \\\"Canada/Eastern\\\"\\n\\n+ [[inputs.tail]]\\n+   files = [\\\"/var/log/apache/access.log\\\"]\\n+   from_beginning = false\\n+   grok_patterns = [\\\"%{COMBINED_LOG_FORMAT}\\\"]\\n+   name_override = \\\"apache_access_log\\\"\\n+   grok_custom_pattern_files = []\\n+   grok_custom_patterns = '''\\n+   '''\\n+   grok_timezone = \\\"Canada/Eastern\\\"\\n+   data_format = \\\"grok\\\"\\n```\"\n```\n\n----------------------------------------\n\nTITLE: Example ToSlash Configuration in Telegraf\nDESCRIPTION: Configuration example showing how to convert backslash path separators to forward slashes (primarily useful on Windows).\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n  [[processors.filepath.rel]]\n    tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Error Handling for TLS Handshake Failures - Text\nDESCRIPTION: This snippet provides an example error message that occurs due to TLS handshake failures when the GNMI server's configuration is insecure. It suggests checking the minimum TLS version and cipher suites as potential solutions to the issue without compromising security.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/gnmi/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n2024-01-01T00:00:00Z E! [inputs.gnmi] Error in plugin: failed to setup subscription: rpc error: code = Unavailable desc = connection error: desc = \"transport: authentication handshake failed: remote error: tls: handshake failure\"\n```\n\n----------------------------------------\n\nTITLE: Named Groups Example in Regex Processor\nDESCRIPTION: Example configuration demonstrating how to use named capture groups to extract multiple fields from a single pattern match.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/regex/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.regex]]\n  namepass = [\"nginx_requests\"]\n\n  [[processors.regex.fields]]\n    key = \"request\"\n    pattern = '^/api/(?P<method>\\w+)[/?].*category=(?P<category>\\w+)&(?:.*)'\n```\n\n----------------------------------------\n\nTITLE: PowerShell Buffer Size Configuration\nDESCRIPTION: PowerShell command to increase the output buffer size to prevent truncation when running scripts through Telegraf on Windows.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/exec/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$host.UI.RawUI.BufferSize = new-object System.Management.Automation.Host.Size(1024,50)\n```\n\n----------------------------------------\n\nTITLE: Formatting CPU Metrics as JSON\nDESCRIPTION: This snippet shows how CPU-related metrics are represented in JSON format for an event in Telegraf. It includes details such as the timestamp and various CPU usage statistics. The snippet is useful for understanding how to structure CPU data for ingestion by OpenSearch.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/opensearch/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"@timestamp\": \"2017-01-01T00:00:00+00:00\",\n  \"measurement_name\": \"cpu\",\n  \"cpu\": {\n    \"usage_guest\": 0,\n    \"usage_guest_nice\": 0,\n    \"usage_idle\": 71.85413456197966,\n    \"usage_iowait\": 0.256805341656516,\n    \"usage_irq\": 0,\n    \"usage_nice\": 0,\n    \"usage_softirq\": 0.2054442732579466,\n    \"usage_steal\": 0,\n    \"usage_system\": 15.04879301548127,\n    \"usage_user\": 12.634822807288275\n  },\n  \"tag\": {\n    \"cpu\": \"cpu-total\",\n    \"host\": \"opensearhhost\",\n    \"dc\": \"datacenter1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting System Metrics Event in JSON\nDESCRIPTION: Example of how the plugin formats system metrics events for Elasticsearch. It includes timestamp, measurement name, system load metrics, and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"@timestamp\": \"2017-01-01T00:00:00+00:00\",\n  \"measurement_name\": \"system\",\n  \"system\": {\n    \"load1\": 0.78,\n    \"load15\": 0.8,\n    \"load5\": 0.8,\n    \"n_cpus\": 2,\n    \"n_users\": 2\n  },\n  \"tag\": {\n    \"host\": \"elastichost\",\n    \"dc\": \"datacenter1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Pulling Telegraf CI Docker Image\nDESCRIPTION: Commands to pull the Telegraf CI Docker image from the quay.io repository. This image contains all necessary dependencies for building Telegraf packages.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PACKAGING.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull quay.io/influxdb/telegraf-ci:1.9.7\n```\n\n----------------------------------------\n\nTITLE: Example uWSGI Metrics Output\nDESCRIPTION: This is an example of the output generated by the Telegraf uWSGI input plugin.  It shows the format and types of metrics that are collected and reported, including overview, worker, app, and core metrics. Each line represents a different set of metrics with associated tags and field values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/uwsgi/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nuwsgi_overview,gid=0,uid=0,source=172.17.0.2,version=2.0.18 listen_queue=0i,listen_queue_errors=0i,load=0i,pid=1i,signal_queue=0i 1564441407000000000\nuwsgi_workers,source=172.17.0.2,worker_id=1 accepting=1i,avg_rt=0i,delta_request=0i,exceptions=0i,harakiri_count=0i,last_spawn=1564441202i,pid=6i,requests=0i,respawn_count=1i,rss=0i,running_time=0i,signal_queue=0i,signals=0i,status=\"idle\",tx=0i,vsz=0i 1564441407000000000\nuwsgi_apps,app_id=0,worker_id=1,source=172.17.0.2 exceptions=0i,modifier1=0i,requests=0i,startup_time=0i 1564441407000000000\nuwsgi_cores,core_id=0,worker_id=1,source=172.17.0.2 in_request=0i,offloaded_requests=0i,read_errors=0i,requests=0i,routed_requests=0i,static_requests=0i,write_errors=0i 1564441407000000000\n\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status command, presenting synchronization information for multiple remote time servers. It includes details such as remote server addresses, reference IDs, stratum levels, poll intervals, reach values, and time-related measurements like delay, offset, and jitter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/multi/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n 83.137.98.96    10.177.80.37     2 u  740 1024  377   54.033  243.426 449514.\n 81.7.16.52      10.177.80.37     2 u  739 1024  377   60.785  232.597 449539.\n 131.188.3.221   10.177.80.37     2 u  783 1024  377  111.820  261.921 449528.\n 5.9.29.107      10.177.80.37     2 u  703 1024  377  205.704  160.406 449602.\n 91.189.94.4     10.177.80.37     2 u  673 1024  377  143.047  274.726 449445.\n```\n\n----------------------------------------\n\nTITLE: Sample Output for Bitlocker WMI Query in Telegraf\nDESCRIPTION: This snippet shows an example of the output generated by the Bitlocker WMI query configuration. It includes the compliance status of a volume with the volume name as a tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_wmi/README.md#2025-04-16_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nwin_wmi_MBAM_Volume,VolumeName=C:,host=foo Compliant=1i 1654269272000000000\n```\n\n----------------------------------------\n\nTITLE: Repairing Windows Performance Counters with lodctr\nDESCRIPTION: Command for repairing Windows performance counters when metrics aren't emitted even with the default configuration. This rebuilds counter values in the system registry.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_17\n\nLANGUAGE: batchfile\nCODE:\n```\nlodctr /r\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudo Access for NSD-Control\nDESCRIPTION: TOML configuration enabling sudo access for the NSD plugin in Telegraf configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsd/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nsd]]\n  use_sudo = true\n```\n\n----------------------------------------\n\nTITLE: Displaying sFlow v5 Sample Data\nDESCRIPTION: This snippet shows sample data output for sFlow v5 protocol. It includes network interface statistics and flow sampling data, with different metrics compared to Netflow protocols.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/netflow/README.md#2025-04-16_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nnetflow,source=127.0.0.1,version=sFlowV5 out_errors=0i,out_bytes=3946i,status=\"up\",in_unknown_protocol=4294967295i,out_unicast_packets_total=29i,agent_subid=100000i,interface_type=6i,in_unicast_packets_total=28i,out_dropped_packets=0i,in_bytes=3910i,in_broadcast_packets_total=4294967295i,ip_version=\"IPv4\",agent_ip=\"192.168.119.184\",in_snmp=3i,in_errors=0i,promiscuous=0i,interface=3i,in_mcast_packets_total=4294967295i,in_dropped_packets=0i,sys_uptime=12414i,seq_number=2i,speed=1000000000i,out_mcast_packets_total=4294967295i,out_broadcast_packets_total=4294967295i 12414000000\nnetflow,source=127.0.0.1,version=sFlowV5 sys_uptime=17214i,agent_ip=\"192.168.119.184\",agent_subid=100000i,seq_number=2i,in_phy_interface=1i,ip_version=\"IPv4\" 17214000000\nnetflow,source=127.0.0.1,version=sFlowV5 in_errors=0i,out_unicast_packets_total=36i,interface=3i,in_broadcast_packets_total=4294967295i,ip_version=\"IPv4\",speed=1000000000i,out_bytes=4408i,out_mcast_packets_total=4294967295i,status=\"up\",in_snmp=3i,in_mcast_packets_total=4294967295i,out_broadcast_packets_total=4294967295i,promiscuous=0i,in_bytes=5568i,out_dropped_packets=0i,sys_uptime=22014i,agent_subid=100000i,in_unknown_protocol=4294967295i,interface_type=6i,in_dropped_packets=0i,in_unicast_packets_total=37i,out_errors=0i,agent_ip=\"192.168.119.184\",seq_number=3i 22014000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PowerStat with Default Settings\nDESCRIPTION: This minimal configuration allows getting default processor package specific metrics without any per-CPU metrics, equivalent to explicitly setting cpu_metrics to an empty array.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_powerstat]]\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Tags in TOML\nDESCRIPTION: Demonstrates the correct way to define tags for a CPU input plugin using table syntax. Shows how tag definitions must be placed at the end of the plugin configuration to avoid parsing issues.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TOML.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n  [inputs.cpu.tags]\n    tag1 = \"foo\"\n    tag2 = \"bar\"\n```\n\n----------------------------------------\n\nTITLE: FreeBSD Buffer Tuning for Suricata\nDESCRIPTION: This text snippet provides commands to increase the localhost buffer space in FreeBSD, ensuring that messages from Suricata are not truncated due to buffer limitations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/suricata/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n\"\"\"\nsysctl -w net.local.stream.recvspace=16384\nsysctl -w net.local.stream.sendspace=16384\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Help for License Checker\nDESCRIPTION: Displays an overview of available options for customizing the license verification process. This helps in understanding different flags and configurations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/license_checker/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./tools/license_checker/license_checker --help\n```\n\n----------------------------------------\n\nTITLE: Pre CouchDB 2.0 Output Format\nDESCRIPTION: Example of metrics output format for CouchDB versions prior to 2.0. Demonstrates detailed statistics with additional calculated values like mean, min, max, and standard deviation in InfluxDB line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/couchdb/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncouchdb,server=http://couchdb16:5984/_stats couchdb_request_time_sum=96,httpd_status_codes_200_sum=37,httpd_status_codes_200_min=0,httpd_requests_mean=0.005,httpd_requests_min=0,couchdb_request_time_stddev=3.833,couchdb_request_time_min=1,httpd_request_methods_get_stddev=0.073,httpd_request_methods_get_min=0,httpd_status_codes_200_mean=0.005,httpd_status_codes_200_max=1,httpd_requests_sum=37,couchdb_request_time_current=96,httpd_request_methods_get_sum=37,httpd_request_methods_get_mean=0.005,httpd_request_methods_get_max=1,httpd_status_codes_200_stddev=0.073,couchdb_request_time_mean=2.595,couchdb_request_time_max=25,httpd_request_methods_get_current=37,httpd_status_codes_200_current=37,httpd_requests_current=37,httpd_requests_stddev=0.073,httpd_requests_max=1 1536707179000000000\n```\n\n----------------------------------------\n\nTITLE: CISCO Power Table Output in Telegraf\nDESCRIPTION: Sample output from the CISCO Power Ethernet Extension MIB table showing EntPhyIndex and power consumption data. This represents the data collected from the first table before joining with the entity table.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n> ciscoPower,index=1.2 EntPhyIndex=1002i,PortPwrConsumption=6643i 1621460628000000000\n> ciscoPower,index=1.6 EntPhyIndex=1006i,PortPwrConsumption=10287i 1621460628000000000\n> ciscoPower,index=1.5 EntPhyIndex=1005i,PortPwrConsumption=8358i 1621460628000000000\n```\n\n----------------------------------------\n\nTITLE: Creating View for Session Statistics in PostgreSQL\nDESCRIPTION: This SQL code creates a view named 'sessions' to provide detailed statistics about active sessions in PostgreSQL. It combines information from pg_proctab() function and pg_stat_activity view, showing process states, memory usage, I/O statistics, and query details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE VIEW public.sessions AS\n WITH proctab AS (\n         SELECT pg_proctab.pid,\n                CASE\n                    WHEN pg_proctab.state::text = 'R'::bpchar::text\n                      THEN 'running'::text\n                    WHEN pg_proctab.state::text = 'D'::bpchar::text\n                      THEN 'sleep-io'::text\n                    WHEN pg_proctab.state::text = 'S'::bpchar::text\n                      THEN 'sleep-waiting'::text\n                    WHEN pg_proctab.state::text = 'Z'::bpchar::text\n                      THEN 'zombie'::text\n                    WHEN pg_proctab.state::text = 'T'::bpchar::text\n                      THEN 'stopped'::text\n                    ELSE NULL::text\n                END AS proc_state,\n            pg_proctab.ppid,\n            pg_proctab.utime,\n            pg_proctab.stime,\n            pg_proctab.vsize,\n            pg_proctab.rss,\n            pg_proctab.processor,\n            pg_proctab.rchar,\n            pg_proctab.wchar,\n            pg_proctab.syscr,\n            pg_proctab.syscw,\n            pg_proctab.reads,\n            pg_proctab.writes,\n            pg_proctab.cwrites\n           FROM pg_proctab() pg_proctab(pid, comm, fullcomm, state, ppid, pgrp,\n             session, tty_nr, tpgid, flags, minflt, cminflt, majflt, cmajflt,\n             utime, stime, cutime, cstime, priority, nice, num_threads,\n             itrealvalue, starttime, vsize, rss, exit_signal, processor,\n             rt_priority, policy, delayacct_blkio_ticks, uid, username, rchar,\n             wchar, syscr, syscw, reads, writes, cwrites)\n        ), stat_activity AS (\n         SELECT pg_stat_activity.datname,\n            pg_stat_activity.pid,\n            pg_stat_activity.usename,\n                CASE\n                    WHEN pg_stat_activity.query IS NULL THEN 'no query'::text\n                    WHEN pg_stat_activity.query IS NOT NULL AND\n                    pg_stat_activity.state = 'idle'::text THEN 'no query'::text\n                    ELSE regexp_replace(pg_stat_activity.query, '[\\n\\r]+'::text,\n                       ' '::text, 'g'::text)\n                END AS query\n           FROM pg_stat_activity\n        )\n SELECT stat.datname::name AS db,\n    stat.usename::name AS username,\n    stat.pid,\n    proc.proc_state::text AS state,\n('\"'::text || stat.query) || '\"'::text AS query,\n    (proc.utime/1000)::bigint AS session_usertime,\n    (proc.stime/1000)::bigint AS session_systemtime,\n    proc.vsize AS session_virtual_memory_size,\n    proc.rss AS session_resident_memory_size,\n    proc.processor AS session_processor_number,\n    proc.rchar AS session_bytes_read,\n    proc.rchar-proc.reads AS session_logical_bytes_read,\n    proc.wchar AS session_bytes_written,\n    proc.wchar-proc.writes AS session_logical_bytes_writes,\n    proc.syscr AS session_read_io,\n    proc.syscw AS session_write_io,\n    proc.reads AS session_physical_reads,\n    proc.writes AS session_physical_writes,\n    proc.cwrites AS session_cancel_writes\n   FROM proctab proc,\n    stat_activity stat\n  WHERE proc.pid = stat.pid;\n```\n\n----------------------------------------\n\nTITLE: Sample Timestamp Format for Parsing Data in Telegraf\nDESCRIPTION: Example of a timestamp format that needs to be converted to Telegraf's timestamp_format using Go's reference time format. This shows the input format and the required format specification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/FAQ.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n2023-03-01T00:00:42.586+0800\n```\n\nLANGUAGE: shell\nCODE:\n```\n2006-01-02T15:04:05.000-0700\n```\n\n----------------------------------------\n\nTITLE: Telegraf Configuration: New webhooks plugin\nDESCRIPTION: This code snippet shows the new `webhooks` plugin configuration. The path is moved inside the `[inputs.webhooks.github]` section.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n\n# A Webhooks Event collector\n[[inputs.webhooks]]\n  ## Address and port to host Webhook listener on\n  service_address = \":1618\"\n\n  [inputs.webhooks.github]\n    path = \"/\"\n\n```\n\n----------------------------------------\n\nTITLE: Syslog Message RFC5424 Format Example\nDESCRIPTION: A sample syslog message following the RFC5424 standard format. It includes priority value (29), version (1), timestamp, hostname (web1), application name (someservice), process ID (2341), message ID (2), structured data with origin and meta fields, and the actual HTTP log message content.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_strict_tcp_1st_avg_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Configuring the filestat input plugin in Telegraf using TOML\nDESCRIPTION: Configuration example for the filestat plugin that specifies which files to monitor and whether to calculate MD5 checksums. It supports glob pattern matching for file selection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/filestat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read stats about given file(s)\n[[inputs.filestat]]\n  ## Files to gather stats about.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". See https://github.com/gobwas/glob.\n  files = [\"/etc/telegraf/telegraf.conf\", \"/var/log/**.log\"]\n\n  ## If true, read the entire file and calculate an md5 checksum.\n  md5 = false\n```\n\n----------------------------------------\n\nTITLE: Formatting Unrelated Parameters with Spacing in TOML\nDESCRIPTION: Example showing proper spacing between unrelated parameters in the configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n  ## If true, queue will be declared as an exclusive queue.\n  # queue_exclusive = false\n\n  ## If true, queue will be declared as an auto deleted queue.\n  # queue_auto_delete = false\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n```\n\n----------------------------------------\n\nTITLE: OpenTSDB Data Format Example\nDESCRIPTION: Example of an OpenTSDB Telnet style put format metric. Shows the structure of a CPU usage metric with timestamp, value, and tags for host and CPU core.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/opentsdb/README.md#2025-04-16_snippet_1\n\nLANGUAGE: opentsdb\nCODE:\n```\nput sys.cpu.user 1356998400 42.5 host=webserver01 cpu=0\n```\n\n----------------------------------------\n\nTITLE: Simplified Equivalent Configuration in Telegraf\nDESCRIPTION: The simplified equivalent of the previous configuration, avoiding redundant clean operation since it's automatically applied with the dir function.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n   [[processors.filepath.dir]]\n     tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenSearch Domain with Compatibility Mode in JSON\nDESCRIPTION: JSON request to upgrade an OpenSearch domain and enable compatibility mode for existing Elasticsearch clients.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"DomainName\": \"domain-name\",\n  \"TargetVersion\": \"OpenSearch_1.0\",\n  \"AdvancedOptions\": {\n    \"override_main_response_version\": \"true\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Example NTP Query Output\nDESCRIPTION: Sample output showing the format of metrics collected by the NTP query plugin, including delay, jitter, offset, and other measurements with associated tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nntpq,refid=.GPSs.,remote=*time.apple.com,stratum=1,type=u delay=91.797,jitter=3.735,offset=12.841,poll=64i,reach=377i,when=35i 1457960478909556134\n```\n\n----------------------------------------\n\nTITLE: Configuring Ignored PDH Errors\nDESCRIPTION: Allows specifying a list of PDH error codes to ignore during performance counter collection\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_5\n\nLANGUAGE: TOML\nCODE:\n```\nIgnoredErrors=[\"PDH_NO_DATA\"]\n```\n\n----------------------------------------\n\nTITLE: JSON String Output Configuration\nDESCRIPTION: Optional configuration to output object data as JSON strings instead of individual fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ctrlx_datalayer/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n  output_json_string = true\n```\n\n----------------------------------------\n\nTITLE: Basic Nginx VTS Configuration Example\nDESCRIPTION: Simplified configuration example for the Nginx VTS plugin where only the URL parameter is set, pointing to the Nginx status endpoint.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_vts/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.nginx_vts]]\n  ## An array of Nginx status URIs to gather stats.\n  urls = [\"http://localhost/status\"]\n```\n\n----------------------------------------\n\nTITLE: Parsing Varnish JSON Output in Telegraf\nDESCRIPTION: This snippet demonstrates how Telegraf parses the JSON output from the 'varnishstat -j' command when metric_version=2 is enabled. It shows the structure of a Varnish counter and how it's transformed into an InfluxDB metric.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"MAIN.cache_hit\": {\n    \"description\": \"Cache hits\",\n    \"flag\": \"c\",\n    \"format\": \"i\",\n    \"value\": 51\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Option Value Deprecation Notice\nDESCRIPTION: Code example showing how to implement deprecation for specific option values within a plugin's Init method. This prints a warning when a deprecated value is detected in the configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-001-deprecation.md#2025-04-16_snippet_5\n\nLANGUAGE: golang\nCODE:\n```\nfunc (e *Example) Init() error {\n    ...\n    if e.Mode == \"old\" {\n        models.PrintOptionDeprecationNotice(telegraf.Warn, \"inputs.example\", \"mode\", telegraf.DeprecationInfo{\n            Since:     \"1.23.1\",\n            RemovalIn: \"1.40.0\",\n            Notice:    \"use 'v1' instead\",\n        })\n    }\n    ...\n    return nil\n}\n```\n\n----------------------------------------\n\nTITLE: Sudo Permissions Configuration for LVM Commands (Text)\nDESCRIPTION: This text snippet provides an example configuration for the sudoers file to allow a specific user to execute LVM commands without a password. The instructions include a command to edit the sudoers file and the necessary entry format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/lvm/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"Cmnd_Alias LVM = /usr/sbin/pvs *, /usr/sbin/vgs *, /usr/sbin/lvs *\\n<username>  ALL=(root) NOPASSWD: LVM\\nDefaults!LVM !logfile, !syslog, !pam_session\\n\"\n```\n\n----------------------------------------\n\nTITLE: Grok Pattern Format - Text\nDESCRIPTION: Shows the basic syntax format for Grok patterns used to parse input lines.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/grok/README.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n%{<capture_syntax>[:<semantic_name>][:<modifier>]}\n```\n\n----------------------------------------\n\nTITLE: Configuring System File Limits\nDESCRIPTION: SystemD service configuration to increase file limits for Telegraf\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ping/README.md#2025-04-16_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nLimitNOFILE=8192\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status Information in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status query. It displays various metrics for a single remote NTP server, including its address, reference ID, stratum, poll interval, reachability, delay, offset, and jitter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/minutes/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  2m  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Field Template Configuration with Custom Separator\nDESCRIPTION: Configures a template that specifies how field names are derived from the Graphite metric path with an underscore separator.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\nseparator = \"_\"\ntemplates = [\n    \"measurement.measurement.field.field.region\"\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Aggregator Plugins in Telegraf\nDESCRIPTION: This snippet demonstrates how to configure aggregator plugins in Telegraf. It includes an example of the 'basicstats' aggregator, which can be used to calculate basic statistics like min, max, mean, and count for specified fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/migrations/inputs_jolokia/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n###############################################################################\n#                            AGGREGATOR PLUGINS                              #\n###############################################################################\n\n# # Keep the aggregate basicstats of each metric passing through.\n# [[aggregators.basicstats]]\n#   ## The period on which to flush & clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Configures which basic stats to push as fields\n#   # stats = [\"count\", \"min\", \"max\", \"mean\", \"stdev\", \"s2\", \"sum\"]\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NATS Server Monitoring Plugin in Telegraf with TOML\nDESCRIPTION: Sample TOML configuration for the NATS server monitoring input plugin. It defines the server endpoint URL and optional response timeout parameter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nats/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Provides metrics about the state of a NATS server\n# This plugin does NOT support FreeBSD\n[[inputs.nats]]\n  ## The address of the monitoring endpoint of the NATS server\n  server = \"http://localhost:8222\"\n\n  ## Maximum time to receive response\n  # response_timeout = \"5s\"\n```\n\n----------------------------------------\n\nTITLE: JSON Parsing with Name, Tags, and String Fields\nDESCRIPTION: Example showing how to configure name key, tag keys, and string fields for JSON parsing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n  json_name_key = \"name\"\n  tag_keys = [\"my_tag_1\"]\n  json_string_fields = [\"b_my_field\"]\n  data_format = \"json\"\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"a\": 5,\n    \"b\": {\n        \"c\": 6,\n        \"my_field\": \"description\"\n    },\n    \"my_tag_1\": \"foo\",\n    \"name\": \"my_json\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building Telegraf from Source\nDESCRIPTION: Shell commands to clone the Telegraf repository and build the binary from source. Requires Go and GNU make to be installed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INSTALL_GUIDE.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/influxdata/telegraf.git\ncd telegraf\nmake build\n```\n\n----------------------------------------\n\nTITLE: Setting up InfluxData Repository and Installing Telegraf on RPM-based Systems\nDESCRIPTION: Shell commands to create a repository file for InfluxData and install Telegraf on RPM-based systems like RHEL and CentOS. Includes the GPG key fingerprint for verification.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/INSTALL_GUIDE.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# influxdata-archive_compat.key GPG fingerprint:\n#     9D53 9D90 D332 8DC7 D6C8 D3B9 D8FF 8E1F 7DF8 B07E\ncat <<EOF | sudo tee /etc/yum.repos.d/influxdata.repo\n[influxdata]\nname = InfluxData Repository - Stable\nbaseurl = https://repos.influxdata.com/stable/\\$basearch/main\nenabled = 1\ngpgcheck = 1\ngpgkey = https://repos.influxdata.com/influxdata-archive_compat.key\nEOF\nsudo yum install telegraf\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Server Database Type in Telegraf\nDESCRIPTION: Sets the database type to SQLServer for Telegraf configuration. This determines which metrics and data sources will be used for monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\ndatabase_type = \"SQLServer\"\n```\n\n----------------------------------------\n\nTITLE: Defining Elasticsearch JVM Metrics in Telegraf\nDESCRIPTION: Defines various JVM metrics related to the Elasticsearch cluster, including memory usage, garbage collection stats, and class loading metrics. These are essential for understanding JVM performance in a Java-based environment.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/elasticsearch/README.md#2025-04-16_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  - elasticsearch_jvm\n  - tags:\n    - cluster_name\n    - node_attribute_ml.enabled\n    - node_attribute_ml.machine_memory\n    - node_attribute_ml.max_open_jobs\n    - node_attribute_xpack.installed\n    - node_host\n    - node_id\n    - node_name\n  - fields:\n    - buffer_pools_direct_count (float)\n    - buffer_pools_direct_total_capacity_in_bytes (float)\n    - buffer_pools_direct_used_in_bytes (float)\n    - buffer_pools_mapped_count (float)\n    - buffer_pools_mapped_total_capacity_in_bytes (float)\n    - buffer_pools_mapped_used_in_bytes (float)\n    - classes_current_loaded_count (float)\n    - classes_total_loaded_count (float)\n    - classes_total_unloaded_count (float)\n    - gc_collectors_old_collection_count (float)\n    - gc_collectors_old_collection_time_in_millis (float)\n    - gc_collectors_young_collection_count (float)\n    - gc_collectors_young_collection_time_in_millis (float)\n    - mem_heap_committed_in_bytes (float)\n    - mem_heap_max_in_bytes (float)\n    - mem_heap_used_in_bytes (float)\n    - mem_heap_used_percent (float)\n    - mem_non_heap_committed_in_bytes (float)\n    - mem_non_heap_used_in_bytes (float)\n    - mem_pools_old_max_in_bytes (float)\n    - mem_pools_old_peak_max_in_bytes (float)\n    - mem_pools_old_peak_used_in_bytes (float)\n    - mem_pools_old_used_in_bytes (float)\n    - mem_pools_survivor_max_in_bytes (float)\n    - mem_pools_survivor_peak_max_in_bytes (float)\n    - mem_pools_survivor_peak_used_in_bytes (float)\n    - mem_pools_survivor_used_in_bytes (float)\n    - mem_pools_young_max_in_bytes (float)\n    - mem_pools_young_peak_max_in_bytes (float)\n    - mem_pools_young_peak_used_in_bytes (float)\n    - mem_pools_young_used_in_bytes (float)\n    - threads_count (float)\n    - threads_peak_count (float)\n    - timestamp (float)\n    - uptime_in_millis (float)\n```\n\n----------------------------------------\n\nTITLE: Installing Multiple Telegraf Services\nDESCRIPTION: Commands to install multiple Telegraf instances as separate Windows services with unique names.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" --service-name telegraf-1 service install --display-name \"Telegraf 1\"\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" --service-name telegraf-2 service install --display-name \"Telegraf 2\"\n```\n\n----------------------------------------\n\nTITLE: JSON Payload with Reserved Label 'name'\nDESCRIPTION: This snippet shows a JSON payload where the 'name' label is used, which is reserved for the metric name. This will be replaced in the actual implementation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/nebius_cloud_monitoring/README.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"systemd_units_load_code\",\n  \"labels\": {\n    \"active\": \"active\",\n    \"host\": \"vm\",\n    \"load\": \"loaded\",\n    \"name\": \"accounts-daemon.service\",\n    \"sub\": \"running\"\n  },\n  \"ts\": \"2023-06-06T11:10:50Z\",\n  \"value\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes PersistentVolume Metrics with Telegraf\nDESCRIPTION: This snippet logs metrics related to the state of a Kubernetes PersistentVolume, specifically its phase type.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_persistentvolume,phase=Released,pv_name=pvc-aaaaaaaa-bbbb-cccc-1111-222222222222,storageclass=ebs-1-retain phase_type=3i 1547597616000000000\n```\n\n----------------------------------------\n\nTITLE: Example Output for Raindrops Metrics - TEXT\nDESCRIPTION: This snippet presents example output that the raindrops input plugin would generate. The output displays the collected statistics for middleware processes in a time-series format, suitable for ingestion into InfluxDB.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/raindrops/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nraindrops,port=8080,server=localhost calling=0i,writing=0i 1455479896806238204\nraindrops_listen,ip=0.0.0.0,port=8080 active=0i,queued=0i 1455479896806561938\nraindrops_listen,ip=0.0.0.0,port=8081 active=1i,queued=0i 1455479896806605749\nraindrops_listen,ip=127.0.0.1,port=8082 active=0i,queued=0i 1455479896806646315\nraindrops_listen,ip=0.0.0.0,port=8083 active=0i,queued=0i 1455479896806683252\nraindrops_listen,ip=0.0.0.0,port=8084 active=0i,queued=0i 1455479896806712025\nraindrops_listen,ip=0.0.0.0,port=3000 active=0i,queued=0i 1455479896806779197\nraindrops_listen,socket=/tmp/listen.me active=0i,queued=0i 1455479896806813907\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel PMT Inputs with Sample Filter - TOML\nDESCRIPTION: This snippet illustrates how to filter metrics based on specific samples in Telegraf's Intel PMT input configuration. Using a sample filter, only metrics related to the specified samples will be collected, enhancing data relevance and efficiency.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.intel_pmt]]\n  spec = \"/home/telegraf/Intel-PMT/xml/pmt.xml\"\n  samples_enabled = [\"C0Residency\",\"C1Residency\", \"Cx_TEMP\"]\n```\n\n----------------------------------------\n\nTITLE: Props.conf for Telegraf Metrics in Splunk\nDESCRIPTION: This INI configuration is an example `props.conf` file for Splunk used to properly ingest metrics data sent by Telegraf. It defines the category, description, and extraction settings to handle the JSON data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[telegraf]\ncategory = Metrics\ndescription = Telegraf Metrics\npulldown_type = 1\nDATETIME_CONFIG =\nNO_BINARY_CHECK = true\nSHOULD_LINEMERGE = true\ndisabled = false\nINDEXED_EXTRACTIONS = json\nKV_MODE = none\nTIMESTAMP_FIELDS = time\n```\n\n----------------------------------------\n\nTITLE: Configuring Tomcat Input Plugin in Telegraf\nDESCRIPTION: TOML configuration template for Tomcat metrics collection, including URL, authentication, timeout, and optional TLS settings for secure connection\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tomcat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather metrics from the Tomcat server status page.\n[[inputs.tomcat]]\n  ## URL of the Tomcat server status\n  # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n  ## HTTP Basic Auth Credentials\n  # username = \"tomcat\"\n  # password = \"s3cret\"\n\n  ## Request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Generated Metrics from Zipkin Trace Data\nDESCRIPTION: This example demonstrates the metrics generated from the Zipkin trace data by displaying the computed duration in nanoseconds for each span. These metrics are crucial for diagnosing and monitoring service performance. Understanding of Zipkin annotations and the metric generation process is necessary.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zipkin/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nzipkin,id=7047c59776af8a1b,name=child,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=53106000i 1498688360851331000\nzipkin,annotation=trivial,annotation_key=lc,endpoint_host=127.0.0.1,id=7047c59776af8a1b,name=child,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=53106000i 1498688360851331000\nzipkin,id=17020eb55a8bfe5,name=child,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=50410000i 1498688360904552000\nzipkin,annotation=trivial,annotation_key=lc,endpoint_host=127.0.0.1,id=17020eb55a8bfe5,name=child,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=50410000i 1498688360904552000\nzipkin,id=5195e96239641e,name=parent,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=103680000i 1498688360851318000\nzipkin,annotation=Starting\\ child\\ #0,endpoint_host=127.0.0.1,id=5195e96239641e,name=parent,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=103680000i 1498688360851318000\nzipkin,annotation=Starting\\ child\\ #1,endpoint_host=127.0.0.1,id=5195e96239641e,name=parent,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=103680000i 1498688360851318000\nzipkin,annotation=A\\ Log,endpoint_host=127.0.0.1,id=5195e96239641e,name=parent,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=103680000i 1498688360851318000\nzipkin,annotation=trivial,annotation_key=lc,endpoint_host=127.0.0.1,id=5195e96239641e,name=parent,parent_id=5195e96239641e,service_name=trivial,trace_id=22c4fc8ab3669045 duration_ns=103680000i 1498688360851318000\n```\n\n----------------------------------------\n\nTITLE: Adding Telegraf User to OpenSMTPD Group in Bash\nDESCRIPTION: This bash snippet demonstrates how to add the telegraf user to the opensmtpd group for proper permissions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensmtpd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ groups telegraf\ntelegraf : telegraf\n\n$ usermod -a -G opensmtpd telegraf\n\n$ groups telegraf\ntelegraf : telegraf opensmtpd\n```\n\n----------------------------------------\n\nTITLE: Displaying TCP Connection States and Metrics\nDESCRIPTION: This snippet shows the output of a command that displays TCP connection states and detailed metrics. It includes information such as connection state, local and peer addresses, ports, and various TCP performance parameters like window scaling, round-trip times, and congestion control metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socketstat/testdata/tcp_traffic.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nState      Recv-Q Send-Q       Local Address:Port                     Peer Address:Port\nESTAB      0      0          192.168.1.21:6514                192.168.1.21:443\n        cubic wscale:7,7 rto:204 rtt:0.057/0.033 ato:40 mss:22976 cwnd:10 bytes_acked:1126 bytes_received:532644751 segs_out:211249 segs_in:211254 data_segs_out:2 data_segs_in:211251 send 32247.0Mbps lastsnd:299082764 lastrcv:5248 lastack:5252 rcv_rtt:3.532 rcv_space:186557 minrtt:0.047\nESTAB      0      0         192.168.122.1:55194             192.168.122.1:6514\n        cubic wscale:7,7 rto:204 rtt:0.034/0.01 ato:40 mss:65483 cwnd:10 bytes_acked:790782896 bytes_received:1126 segs_out:333361 segs_in:333361 data_segs_out:333358 data_segs_in:2 send 154077.6Mbps lastsnd:5248 lastrcv:443892492 lastack:5248 rcv_rtt:250 rcv_space:43690 minrtt:0.009\nESTAB      0      0             127.0.0.1:7778                   127.0.0.1:50378\n        cubic wscale:7,7 rto:220 rtt:16.009/21.064 ato:44 mss:65483 cwnd:10 bytes_acked:19983121 bytes_received:266383 segs_out:15431 segs_in:17633 data_segs_out:15119 data_segs_in:5098 send 327.2Mbps lastsnd:9792 lastrcv:9840 lastack:9748 pacing_rate 654.4Mbps retrans:0/1 rcv_rtt:129800 rcv_space:44057 minrtt:0.043\n```\n\n----------------------------------------\n\nTITLE: Example Output Format\nDESCRIPTION: Sample output showing the format of metrics produced by the InfluxDB Listener plugin after processing the input data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb_listener/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000\n```\n\n----------------------------------------\n\nTITLE: Executing Telegraf Secrets Help Command\nDESCRIPTION: Command to display help information about managing secrets with Telegraf's command-line interface.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/os/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf secrets help\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status\nDESCRIPTION: Shows a formatted table output from an NTP client displaying synchronization status with a remote NTP server. The output includes metrics like delay, offset, and jitter, with the asterisk (*) indicating the currently selected time source.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/single_reach_ratio/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Field Wildcard Template with Custom Separator\nDESCRIPTION: Shows how to use field* to capture all remaining elements as the field name with an underscore separator.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\nseparator = \"_\"\ntemplates = [\n    \"measurement.measurement.region.field*\"\n]\n```\n\n----------------------------------------\n\nTITLE: Linting readmes for inputs starting a-d\nDESCRIPTION: This command uses shell globbing to lint README.md files for input plugins whose names start with the letters 'a' through 'd'. It specifies a relative path to the plugins directory, filtering for subdirectories matching the specified pattern.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/readme_linter/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n```shell\ntelegraf/tools/readme_linter$ ./readme_linter ../../plugins/inputs/[a-d]*/README.md\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Listener for Prometheus Remote Write\nDESCRIPTION: TOML configuration for setting up the HTTP listener v2 input plugin to receive Prometheus remote write data. Specifies the service address, paths, and data format settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheusremotewrite/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.http_listener_v2]]\n  ## Address and port to host HTTP listener on\n  service_address = \":1234\"\n\n  ## Paths to listen to.\n  paths = [\"/receive\"]\n\n  ## Data format to consume.\n  data_format = \"prometheusremotewrite\"\n\n  ## Metric version to use, either 1 or 2\n  # metric_version = 2\n```\n\n----------------------------------------\n\nTITLE: Message A Binary Structure\nDESCRIPTION: Definition of Message A format showing field layout including ID, type, length, address, count, failure, value, and timestamp fields in little-endian format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/binary/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n+--------+------+------+--------+--------+------------+--------------------+--------------------+\n| ID     | type | len  | addr   | count  | failure    | value              | timestamp          |\n+--------+------+------+--------+--------+------------+--------------------+--------------------+\n| 0x0201 | 0x0A | 0x18 | 0x7F01 | 0x2A00 | 0x00000000 | 0x6F1283C0CA210940 | 0x10D4DF6200000000 |\n+--------+------+------+--------+--------+------------+--------------------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Configuration Settings\nDESCRIPTION: Required PostgreSQL server configuration settings for enabling monitoring extensions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql_extensible/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nshared_preload_libraries = 'pg_stat_statements,pg_stat_kcache'\n```\n\n----------------------------------------\n\nTITLE: Example Error Log for Collection Interval Issues\nDESCRIPTION: Sample error message showing what happens when metric collection doesn't complete within the configured interval.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n2019-01-16T13:41:10Z W! [agent] input \"inputs.vsphere\" did not complete within its interval\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceNow Operational Intelligence Format Event - JSON\nDESCRIPTION: This snippet illustrates the structure of a metrics event in the ServiceNow Operational Intelligence format. It shows how to represent metric data such as type, resource, and value with timestamps. The output should be in JSON array format for compatibility with the ServiceNow API.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/nowmetric/README.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"metric_type\": \"Disk C: % Free Space\",\n    \"resource\": \"C:\\\\\",\n    \"node\": \"lnux100\",\n    \"value\": 50,\n    \"timestamp\": 1473183012000,\n    \"ci2metric_id\": {\n        \"node\": \"lnux100\"\n    },\n    \"source\": \"Telegraf\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Comparing Input Format to InfluxDB Line Protocol\nDESCRIPTION: This example shows a diff between the input format and the corresponding InfluxDB Line Protocol output. It demonstrates how the parser transforms the input data into the Telegraf metric format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/EXAMPLE_README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- cpu|host=localhost|source=example.org|value=42\n+ cpu,host=localhost,source=example.org value=42\n```\n\n----------------------------------------\n\nTITLE: Setting Up Systemd Credentials System\nDESCRIPTION: Command to initialize the systemd credentials system by creating the root encryption key, which is a prerequisite for using encrypted credentials with Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemd-creds setup\n```\n\n----------------------------------------\n\nTITLE: Adding Scoreboard Objective Command (Shell Script)\nDESCRIPTION: This shell command adds a scoreboard objective in a Minecraft game to track player jumps. This is an essential step for the plugin to start collecting jump metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/minecraft/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n\"/scoreboard objectives add jumps minecraft.custom:minecraft.jump\"\n```\n\n----------------------------------------\n\nTITLE: Example RAS Plugin Output Format\nDESCRIPTION: Sample output showing the metrics format produced by the RAS plugin, including various error counters tagged by host and socket_id.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ras/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nras,host=ubuntu,socket_id=0 external_mce_base_errors=1i,frc_errors=1i,instruction_tlb_errors=5i,internal_parity_errors=1i,internal_timer_errors=1i,l0_and_l1_cache_errors=7i,memory_read_corrected_errors=25i,memory_read_uncorrectable_errors=0i,memory_write_corrected_errors=5i,memory_write_uncorrectable_errors=0i,microcode_rom_parity_errors=1i,processor_base_errors=7i,processor_bus_errors=1i,smm_handler_code_access_violation_errors=1i,unclassified_mce_base_errors=1i 1598867393000000000\nras,host=ubuntu level_2_cache_errors=0i,upi_errors=0i 1598867393000000000\n```\n\n----------------------------------------\n\nTITLE: Entity Table Output in Telegraf\nDESCRIPTION: Sample output from the ENTITY-MIB table showing physical entity names. This represents the second table data that will be joined with the CISCO power table.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/snmp/README.md#2025-04-16_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n> entityTable,index=1006 EntPhysicalName=\"GigabitEthernet1/6\" 1621460809000000000\n> entityTable,index=1002 EntPhysicalName=\"GigabitEthernet1/2\" 1621460809000000000\n> entityTable,index=1005 EntPhysicalName=\"GigabitEthernet1/5\" 1621460809000000000\n```\n\n----------------------------------------\n\nTITLE: JSONata Transformation - Flattening Metric\nDESCRIPTION: JSONata expression to flatten a metric by merging name, timestamp, tags, and fields\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/json/README.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n$merge([{\"name\": name, \"timestamp\": timestamp}, tags, fields])\n```\n\n----------------------------------------\n\nTITLE: Querying Intel PMT Throttle Counter Data in InfluxDB Format\nDESCRIPTION: Time-series data points for Intel PMT throttle counters across multiple CPU cores. Each measurement includes core identification, throttle counter values, PCI device location, and precise timestamps in nanoseconds. Data is structured in InfluxDB line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_pmt/README.md#2025-04-16_snippet_6\n\nLANGUAGE: influxdb\nCODE:\n```\nintel_pmt,core=34,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C34_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\nintel_pmt,core=35,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C35_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=0i 1693766334000000000\nintel_pmt,core=36,datatype_idref=tpvp_throttle_counter,guid=0x87b6fef1,pmt,numa_node=0,pci_bdf=0000:e7:03.1,sample_group=C36_PVP_THROTTLE_1024,sample_name=PVP_THROTTLE_1024 value=9231744i 1693766334000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Teamspeak Input Plugin in TOML\nDESCRIPTION: Sample configuration for the Teamspeak 3 input plugin in Telegraf. Specifies server connection details, authentication credentials, and virtual server settings for data collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/teamspeak/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Reads metrics from a Teamspeak 3 Server via ServerQuery\n[[inputs.teamspeak]]\n  ## Server address for Teamspeak 3 ServerQuery\n  # server = \"127.0.0.1:10011\"\n  ## Username for ServerQuery\n  username = \"serverqueryuser\"\n  ## Password for ServerQuery\n  password = \"secret\"\n  ## Nickname of the ServerQuery client\n  nickname = \"telegraf\"\n  ## Array of virtual servers\n  # virtual_servers = [1]\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite in Telegraf\nDESCRIPTION: Command to execute the complete test suite including all tests.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmake test-all\n```\n\n----------------------------------------\n\nTITLE: Sample Log Input Format\nDESCRIPTION: Example log file content showing HTTP paths and response codes.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/valuecounter/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n/some/path 200\n/some/path 401\n/some/path 200\n```\n\n----------------------------------------\n\nTITLE: Parsing Flat JSON Data in Telegraf\nDESCRIPTION: Configuration for parsing flat JSON data in Telegraf, specifying tag keys, timestamp key, and timestamp format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PARSING_DATA.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"node\": \"node\", \"temp\": 32.3, \"humidity\": 23, \"alarm\": false, \"time\": \"1709572232123456789\"}\n```\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\nfiles = [\"test.json\"]\nprecision = \"1ns\"\ndata_format = \"json\"\n\ntag_keys = [\"node\"]\njson_time_key = \"time\"\njson_time_format = \"unix_ns\"\n\n```\n\n----------------------------------------\n\nTITLE: Example Output Format\nDESCRIPTION: Sample output showing the format of metrics processed by the InfluxDB V2 Listener.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/influxdb_v2_listener/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000\n```\n\n----------------------------------------\n\nTITLE: Mail Delivery Metrics\nDESCRIPTION: Histogram metrics for mail delivery operations showing latency distribution across different time buckets.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/dovecot/input.txt#2025-04-16_snippet_4\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP dovecot_mail_delivery Histogram\n# TYPE dovecot_mail_delivery histogram\ndovecot_mail_delivery_bucket{le=\"10\"} 0\ndovecot_mail_delivery_bucket{le=\"100\"} 0\ndovecot_mail_delivery_bucket{le=\"1000\"} 1\ndovecot_mail_delivery_bucket{le=\"10000\"} 1\ndovecot_mail_delivery_bucket{le=\"100000\"} 1\ndovecot_mail_delivery_bucket{le=\"+Inf\"} 1\ndovecot_mail_delivery_sum 0.000656\ndovecot_mail_delivery_count 1\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status Information in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status command. It displays information about a remote NTP server, including its address, refid, status flags, poll interval, reach, offset, and jitter values. This type of output is commonly used for monitoring and troubleshooting NTP synchronization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/missing_delay_column/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nremote      refid   foobar t when poll reach   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status Output in Plaintext Format\nDESCRIPTION: This snippet shows the standard output format of an NTP server status command, displaying remote NTP servers and their synchronization statistics. The output includes remote server addresses, reference IDs, status flags, polling information, and timing metrics such as delay, offset, and jitter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/no_ref_id/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n 83.137.98.96    10.177.80.37     2 u  740 1024  377   54.033  243.426 449514.\n 91.189.94.4                      2 u  673 1024  377  143.047  274.726 449445.\n 131.188.3.221   10.177.80.37     2 u  783 1024  377  111.820  261.921 449528.\n```\n\n----------------------------------------\n\nTITLE: Splunk Metric HEC JSON Format Example\nDESCRIPTION: This example demonstrates the standard JSON format expected by Splunk's HEC (HTTP Event Collector) when sending metrics. The 'time' field represents the timestamp, 'event' is set to 'metric', 'host' identifies the source, and 'fields' contain the metric values and dimensions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"time\": 1529708430,\n  \"event\": \"metric\",\n  \"host\": \"patas-mbp\",\n  \"fields\": {\n    \"_value\": 0.6,\n    \"cpu\": \"cpu0\",\n    \"dc\": \"mobile\",\n    \"metric_name\": \"cpu.usage_user\",\n    \"user\": \"ronnocol\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ipset Input Plugin in TOML\nDESCRIPTION: This TOML configuration snippet shows the available settings for the Ipset input plugin in Telegraf. It includes options for including unmatched sets, using sudo, counting per IP entries, and setting a timeout.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipset/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather packets and bytes counters from Linux ipsets\n  [[inputs.ipset]]\n    ## By default, we only show sets which have already matched at least 1 packet.\n    ## set include_unmatched_sets = true to gather them all.\n    # include_unmatched_sets = false\n\n    ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n    ## You can avoid using sudo or root, by setting appropriate privileges for\n    ## the telegraf.service systemd service.\n    # use_sudo = false\n\n    ## Add number of entries and number of individual IPs (resolve CIDR syntax) for each ipset\n    # count_per_ip_entries = false\n\n    ## The default timeout of 1s for ipset execution can be overridden here:\n    # timeout = \"1s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudoers File for Unbound Control\nDESCRIPTION: These bash commands and sudoers configuration show how to configure the sudoers file to allow the `telegraf` user to execute `unbound-control` without a password. The `visudo` command opens the sudoers file in a text editor, and the provided lines are added to grant the necessary privileges.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/unbound/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# Add the following line:\nCmnd_Alias UNBOUNDCTL = /usr/sbin/unbound-control\ntelegraf  ALL=(ALL) NOPASSWD: UNBOUNDCTL\nDefaults!UNBOUNDCTL !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS for PostgreSQL Connection in Telegraf\nDESCRIPTION: Example of how to add TLS configuration options to the PostgreSQL connection string for secure connections.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/postgresql/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhost=localhost user=pgotest dbname=app_production sslmode=require sslkey=/etc/telegraf/key.pem sslcert=/etc/telegraf/cert.pem sslrootcert=/etc/telegraf/ca.pem\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Log Line Example\nDESCRIPTION: An example of a RFC5424 formatted log line with severity level 1 (alert) and version 1. The format uses hyphens as placeholders for various fields including timestamp, hostname, app-name, procid, and msgid, followed by structured data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_strict_tcp_1st_min_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<1>1 - - - - - -\n```\n\n----------------------------------------\n\nTITLE: Configuring Swap Input Plugin\nDESCRIPTION: This snippet shows the TOML configuration for the Swap Input Plugin in Telegraf to collect metrics related to swap memory usage on Linux systems. The configuration indicates that no additional parameters are required. The section highlights the use of the '[[inputs.swap]]' table to define the plugin settings.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/swap/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics about swap memory usage\n[[inputs.swap]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Adding Telegraf User to Unbound Group\nDESCRIPTION: These bash commands show how to add the `telegraf` user to the `unbound` group, allowing Telegraf to access Unbound statistics without requiring sudo.  The first command displays the current groups of the telegraf user. The second command modifies the group membership.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/unbound/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ groups telegraf\ntelegraf : telegraf\n\n$ usermod -a -G unbound telegraf\n\n$ groups telegraf\ntelegraf : telegraf unbound\n```\n\n----------------------------------------\n\nTITLE: Benchmark Metrics Definition in OpenMetrics Format\nDESCRIPTION: Defines two test metrics (benchmark_a and benchmark_b) of type gauge with labels for source, platform and SDK version. Each metric includes a value and timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/benchmark/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: openmetrics\nCODE:\n```\n# HELP benchmark_a Test metric for benchmarking\n# TYPE benchmark_a gauge\nbenchmark_a{source=\"myhost\",tags_platform=\"python\",tags_sdkver=\"3.11.5\"} 5 1653643420000\n\n# HELP benchmark_b Test metric for benchmarking\n# TYPE benchmark_b gauge\nbenchmark_b{source=\"myhost\",tags_platform=\"python\",tags_sdkver=\"3.11.4\"} 4 1653643420000\n```\n\n----------------------------------------\n\nTITLE: Adding Plugin Deprecation Entry in Golang\nDESCRIPTION: Shows how to add an entry to the plugins deprecation list in a Golang file. This includes specifying when the plugin was deprecated and suggesting an alternative.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEPRECATION.md#2025-04-16_snippet_0\n\nLANGUAGE: golang\nCODE:\n```\n  \"logparser\": {\n    Since:  \"1.15.0\",\n    Notice: \"use 'inputs.tail' with 'grok' data format instead\",\n  },\n```\n\n----------------------------------------\n\nTITLE: Adding Plugin Deprecation Entry in Go Code\nDESCRIPTION: Shows how to add a deprecation entry for a plugin in the deprecation.go file. This code defines when the plugin was deprecated, when it will be removed, and provides a user-facing hint about alternatives.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-001-deprecation.md#2025-04-16_snippet_1\n\nLANGUAGE: golang\nCODE:\n```\n    \"<plugin name>\": {\n        Since:     \"<x.y.z format version of the next minor release>\",\n        RemovalIn: \"<x.y.z format version of the plugin removal>\",\n        Notice:    \"<user-facing hint e.g. on replacements>\",\n    },\n```\n\n----------------------------------------\n\nTITLE: Updating Sudoers File for OpenSMTPD Access in Bash\nDESCRIPTION: This bash snippet shows how to update the sudoers file to grant necessary permissions for the OpenSMTPD plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/opensmtpd/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ visudo\n# Add the following line:\nCmnd_Alias SMTPCTL = /usr/sbin/smtpctl\ntelegraf  ALL=(ALL) NOPASSWD: SMTPCTL\nDefaults!SMTPCTL !logfile, !syslog, !pam_session\n```\n\n----------------------------------------\n\nTITLE: Resource Pool Performance Metrics Catalog\nDESCRIPTION: Performance metrics for monitoring resource allocation and usage in vSphere resource pools, including CPU, memory, disk, and power metrics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/METRICS.md#2025-04-16_snippet_3\n\nLANGUAGE: metrics\nCODE:\n```\ncpu.usagemhz.average\\ncpu.cpuentitlement.latest\\ncpu.usagemhz.minimum\n```\n\n----------------------------------------\n\nTITLE: Adding Current Metric as Tag in Telegraf Template Processor\nDESCRIPTION: Example showing how to add the current metric object as a tag named \"metric\" using the template string representation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/template/README.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.template]]\n  tag = \"metric\"\n  template = '{{.}}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Input File with XML Data Format - TOML\nDESCRIPTION: This snippet demonstrates how to set up the Telegraf input file for XML data format. Users can specify the XML file to read, while configuring additional parameters for parsing metrics and handling empty node selections. Comments guide through optional configurations for debugging and metric extraction.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/xpath/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file.xpath]]\n    # metric_selection = \"/Bus/child::Sensor\"\n    # metric_name = \"string('example')\"\n    # timestamp = \"/Gateway/Timestamp\"\n    # timestamp_format = \"2006-01-02T15:04:05Z\"\n    # timezone = \"UTC\"\n    # fields_bytes_as_hex = []\n    # fields_bytes_as_base64 = []\n    [inputs.file.xpath.tags]\n      name   = \"substring-after(Sensor/@name, ' ')\"\n      device = \"string('the ultimate sensor')\"\n    [inputs.file.xpath.fields_int]\n      consumers = \"Variable/@consumers\"\n    [inputs.file.xpath.fields]\n      temperature = \"number(Variable/@temperature)\"\n      power       = \"number(Variable/@power)\"\n      frequency   = \"number(Variable/@frequency)\"\n      ok          = \"Mode != 'ok'\"\n  \n```\n\n----------------------------------------\n\nTITLE: Configuring Sysstat Input Plugin with Ungrouped Output in TOML\nDESCRIPTION: This TOML configuration snippet sets up the sysstat input plugin for Telegraf with ungrouped output. It's similar to the grouped configuration but sets the 'group' option to false for individual metric output.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sysstat/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.sysstat]]\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n  activities = [\"DISK\", \"SNMP\", \"INT\"]\n  group = false\n  [inputs.sysstat.options]\n -C = \"cpu\"\n -B = \"paging\"\n -b = \"io\"\n -d = \"disk\"             # requires DISK activity\n -H = \"hugepages\"\n \"-I ALL\" = \"interrupts\" # requires INT activity\n \"-n ALL\" = \"network\"\n \"-P ALL\" = \"per_cpu\"\n -q = \"queue\"\n -R = \"mem\"\n \"-r ALL\" = \"mem_util\"\n -S = \"swap_util\"\n -u = \"cpu_util\"\n -v = \"inode\"\n -W = \"swap\"\n -w = \"task\"\n  [[inputs.sysstat.device_tags.sda]]\n    vg = \"rootvg\"\n```\n\n----------------------------------------\n\nTITLE: Downloading OpenMetrics Protocol Buffer Definition\nDESCRIPTION: Command to download the latest version of the OpenMetrics protocol buffer definition file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nwget https://raw.githubusercontent.com/OpenObservability/OpenMetrics/main/proto/openmetrics_data_model.proto\n```\n\n----------------------------------------\n\nTITLE: Zabbix Trap Format Example - Tagged Metric\nDESCRIPTION: Example showing how a Telegraf metric with tags is converted into Zabbix trap format\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmeasurement,host=hostname,tagA=keyA,tagB=keyB valueA=0,valueB=1\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"host\": \"hostname\", \"key\": \"telegraf.measurement.valueA[keyA,keyB]\", \"value\": \"0\"}\n{\"host\": \"hostname\", \"key\": \"telegraf.measurement.valueB[keyA,keyB]\", \"value\": \"1\"}\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Syslog Message for Telegraf\nDESCRIPTION: An example of a syslog message in the RFC5424 format that would be processed by Telegraf's syslog input plugin. The message includes priority, timestamp, hostname, application name, process ID, message ID, structured data, and message content with HTTP request details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcptls/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Setting PowerDNS User Permissions\nDESCRIPTION: Shell command to add the Telegraf user to the PowerDNS group for access to the control socket.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/powerdns/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nusermod telegraf -a -G pdns\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Synchronization Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP client, presenting synchronization details with a remote NTP server. It includes various metrics such as delay, offset, and jitter, which are crucial for assessing the quality of time synchronization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/bad_when/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  2q  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status check. It includes columns for the remote server, reference ID, stratum, type, last poll time, polling interval, reachability, delay, offset, and jitter. The asterisk (*) indicates that this is the currently selected time source.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/servers_one_fail/input_serverA.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Logging Kubernetes Node Condition Metrics with Telegraf\nDESCRIPTION: This snippet records the condition status of a Kubernetes Node.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kube_inventory/README.md#2025-04-16_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes_node,condition=Ready,host=vjain,node_name=ip-172-17-0-2.internal,status=True status_condition=1i 1629177980000000000\n```\n\n----------------------------------------\n\nTITLE: Prometheus Remote Write Output Format (v2)\nDESCRIPTION: Example of how the input data is formatted when using metric version 2, showing the modified line protocol format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheusremotewrite/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nprometheus_remote_write,instance=localhost:9090,job=prometheus,quantile=0.99 go_gc_duration_seconds=4.63 1614889298859000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Tag Inclusion in Telegraf Inputs\nDESCRIPTION: TOML configuration example showing how to control which tags are included in the metrics to ensure compatibility with Zabbix trapper keys.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\ntaginclude = [\"host\", \"container_name\"]\n```\n\n----------------------------------------\n\nTITLE: Authentication Success Metrics\nDESCRIPTION: Metrics tracking authentication success counts and duration in seconds.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/dovecot/input.txt#2025-04-16_snippet_1\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP dovecot_auth_success Total number of all events of this kind\n# TYPE dovecot_auth_success counter\ndovecot_auth_success_total 892\n# HELP dovecot_auth_success_duration_seconds Total duration of all events of this kind\n# TYPE dovecot_auth_success_duration_seconds counter\ndovecot_auth_success_duration_seconds_total 0.085479\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plaintext\nDESCRIPTION: Output from the NTP query command showing the status of NTP synchronization. The table displays remote server addresses, reference IDs, stratum levels (st), type (t), polling information, network delay, time offset, and jitter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/pool_when_minus/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n 0.eu.pool.ntp.o .POOL.          16 p    -   64    0    0.000   +0.000   0.000\n 195.201.19.162  187.182.182.166  3 u    -   64    1   12.692   +1.283   0.000\n```\n\n----------------------------------------\n\nTITLE: Example Output Data from Prometheus Metrics\nDESCRIPTION: This text snippet shows the processed Prometheus metrics output derived from the example input data. It includes help descriptions and type indications for each metric, showcasing the conversion from input values to Prometheus-compatible format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/prometheus/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# HELP cpu_time_guest Telegraf collected metric\n# TYPE cpu_time_guest counter\ncpu_time_guest{cpu=\"cpu0\"} 9582.54\ncpu_time_guest{cpu=\"cpu1\"} 9660.88\ncpu_time_guest{cpu=\"cpu2\"} 8946.45\ncpu_time_guest{cpu=\"cpu3\"} 9002.31\n# HELP cpu_time_system Telegraf collected metric\n# TYPE cpu_time_system counter\ncpu_time_system{cpu=\"cpu0\"} 28675.47\ncpu_time_system{cpu=\"cpu1\"} 27779.34\ncpu_time_system{cpu=\"cpu2\"} 27406.18\ncpu_time_system{cpu=\"cpu3\"} 27404.97\n# HELP cpu_time_user Telegraf collected metric\n# TYPE cpu_time_user counter\ncpu_time_user{cpu=\"cpu0\"} 99551.84\ncpu_time_user{cpu=\"cpu1\"} 103468.52\ncpu_time_user{cpu=\"cpu2\"} 102591.45\ncpu_time_user{cpu=\"cpu3\"} 100717.05\n```\n\n----------------------------------------\n\nTITLE: Telegraf Configuration: Migrating from github_webhooks\nDESCRIPTION: This code snippet demonstrates how to migrate from the deprecated `github_webhooks` plugin to the new `webhooks` plugin in Telegraf. It showcases the old configuration and the new equivalent configuration, highlighting the necessary changes in the configuration file.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n\n# A Github Webhook Event collector\n[[inputs.github_webhooks]]\n  ## Address and port to host Webhook listener on\n  service_address = \":1618\"\n\n```\n\n----------------------------------------\n\nTITLE: Incorrect CPU Plugin Configuration with Tag Placement\nDESCRIPTION: Shows an incorrect configuration where a plugin option is mistakenly treated as a tag due to placement after the tags table definition.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TOML.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  totalcpu = true\n  [inputs.cpu.tags]\n    tag1 = \"foo\"\n    tag2 = \"bar\"\n  percpu = false  # this is treated as a tag to add, not a config option\n```\n\n----------------------------------------\n\nTITLE: Configuring XtremIO Plugin Settings in TOML\nDESCRIPTION: This snippet demonstrates how to configure the XtremIO input plugin for Telegraf. It specifies the necessary endpoint URL, user credentials, and optional TLS settings for secure communication. Required fields include URL, username, and password, while collectors and TLS configuration remain optional.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/xtremio/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gathers Metrics From a Dell EMC XtremIO Storage Array's V3 API\n[[inputs.xtremio]]\n  ## XtremIO User Interface Endpoint\n  url = \"https://xtremio.example.com/\" # required\n\n  ## Credentials\n  username = \"user1\"\n  password = \"pass123\"\n\n  ## Metrics to collect from the XtremIO\n  # collectors = [\"bbus\",\"clusters\",\"ssds\",\"volumes\",\"xms\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use SSL but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Sample SQL Query for BIND Metrics\nDESCRIPTION: SQL query example for analyzing BIND counter metrics, specifically focusing on A and PTR record queries over time.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/bind/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT non_negative_derivative(mean(/^A$|^PTR$/), 5m) FROM bind_counter \\\nWHERE \"url\" = 'localhost:8053' AND \"type\" = 'qtype' AND time > now() - 1h \\\nGROUP BY time(5m), \"type\"\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Peer Status Output\nDESCRIPTION: Shows the status output from an NTP server displaying peer synchronization information. The output includes remote server name, reference ID, stratum, type, poll interval, reach, delay, offset and jitter statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/single_reach_octal/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Configuring Wavefront Parser in Telegraf\nDESCRIPTION: Configuration example showing how to set up a file input plugin in Telegraf to parse Wavefront-formatted metrics data. The configuration specifies the input file path and sets the data format to 'wavefront'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/wavefront/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"wavefront\"\n```\n\n----------------------------------------\n\nTITLE: Mailchimp Metrics Example Output\nDESCRIPTION: This is an example of the output generated by the Mailchimp input plugin. It shows the metrics collected for a specific campaign, including emails sent, abuse reports, click rates, open rates, and other relevant data.  The metrics are formatted as Telegraf line protocol.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mailchimp/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmailchimp,campaign_title=Freddie's\\ Jokes\\ Vol.\\ 1,id=42694e9e57 abuse_reports=0i,click_rate=42,clicks_total=42i,emails_sent=200i,facebook_likes=42i,facebook_recipient_likes=5i,facebook_unique_likes=8i,forwards_count=0i,forwards_opens=0i,hard_bounces=0i,industry_abuse_rate=0.00021111996110887,industry_bounce_rate=0.0063767751251474,industry_click_rate=0.027431311866951,industry_open_rate=0.17076777144396,industry_type=\"Social Networks and Online Communities\",industry_unopen_rate=0.82285545343089,industry_unsub_rate=0.001436957032815,list_stats_click_rate=42,list_stats_open_rate=42,list_stats_sub_rate=10,list_stats_unsub_rate=20,open_rate=42,opens_total=186i,soft_bounces=2i,syntax_errors=0i,unique_clicks=400i,unique_opens=100i,unique_subscriber_clicks=42i,unsubscribed=2i 1741188555526302348\n```\n\n----------------------------------------\n\nTITLE: Creating User for AAD Authentication in SQL Server\nDESCRIPTION: SQL script to create a user in SQL Server for Azure Active Directory authentication using Managed Identity. This script creates a user from an external provider (Azure AD) and grants it the VIEW DATABASE STATE permission.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/sqlserver/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nEXECUTE ('IF EXISTS(SELECT * FROM sys.database_principals WHERE name = ''<Monitoring_VM_Name>'')\n    BEGIN\n        DROP USER [<Monitoring_VM_Name>]\n    END')\nEXECUTE ('CREATE USER [<Monitoring_VM_Name>] FROM EXTERNAL PROVIDER')\nEXECUTE ('GRANT VIEW DATABASE STATE TO [<Monitoring_VM_Name>]')\n```\n\n----------------------------------------\n\nTITLE: Defining HTTP Request Duration Metrics in Prometheus\nDESCRIPTION: Prometheus metric configuration that defines HTTP request duration measurements in microseconds. Includes type definition, unit specification, help text, and various metric values including quantiles (0.5, 0.9, 0.99), created timestamp, sum, and count.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_summary/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE http_request_duration_microseconds summary\n# UNIT http_request_duration_microseconds microseconds\n# HELP http_request_duration_microseconds The HTTP request latencies in microseconds.\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.5\"} 552048.506\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.9\"} 5.876804288e+06\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.99\"} 5.876804288e+06\nhttp_request_duration_microseconds_created{handler=\"prometheus\"} 1705509488.3\nhttp_request_duration_microseconds_sum{handler=\"prometheus\"} 1.8909097205e+07\nhttp_request_duration_microseconds_count{handler=\"prometheus\"} 9\n# EOF\n```\n\n----------------------------------------\n\nTITLE: C++ Structure for Binary Message Format\nDESCRIPTION: C++ struct definition showing the expected binary message structure on the receiving side with appropriate memory alignment for binary serialization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/binary/README.md#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#pragma pack(push, 1)\nstruct test_struct\n{\n  short addr_3;\n  short addr_2;\n  int addr_4_5;\n  float addr_6_7;\n  char addr_16_20[11];\n  double addr_3_sc;\n  int time;\n  char metric_name[20];\n};\n#pragma pack(pop)\n```\n\n----------------------------------------\n\nTITLE: Parsing Structured Log Entry\nDESCRIPTION: Represents a detailed log message with timestamp, service name, HTTP request, and metadata information. Contains details about a web service request including status code, request path, and client details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_strict_tcptls/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: log\nCODE:\n```\n188 <29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Example Telegraf Output for Ipset Plugin\nDESCRIPTION: This text snippet shows an example of the Telegraf output format for the Ipset input plugin, including the measurement name, tags, fields, and timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ipset/README.md#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nipset,rule=10.69.152.1,host=trashme,set=myset bytes_total=8i,packets_total=672i 1507615028000000000\n```\n\n----------------------------------------\n\nTITLE: Defining cAdvisor Version Info Metric\nDESCRIPTION: Defines a Prometheus metric 'cadvisor_version_info' that provides version information for various system components. The metric is a gauge type with constant value 1 and includes labels for kernel version, OS version, Docker version, and cAdvisor details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/valid_gauge/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP cadvisor_version_info A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\n# TYPE cadvisor_version_info gauge\ncadvisor_version_info{cadvisorRevision=\"\",cadvisorVersion=\"\",dockerVersion=\"1.8.2\",kernelVersion=\"3.10.0-229.20.1.el7.x86_64\",osVersion=\"CentOS Linux 7 (Core)\"} 1\n```\n\n----------------------------------------\n\nTITLE: Telegraf Version 1.14 Release Notes Documentation\nDESCRIPTION: Comprehensive changelog documenting bug fixes, new features, and changes in Telegraf version 1.14 including new input plugins, processors, outputs and various improvements across multiple plugins.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_23\n\nLANGUAGE: markdown\nCODE:\n```\n### Bug Fixes\n\n- [#7236](https://github.com/influxdata/telegraf/issues/7236): Fix PerformanceCounter query performance degradation in sqlserver input.\n- [#7257](https://github.com/influxdata/telegraf/issues/7257): Fix error when using the Name field in template processor.\n- [#7289](https://github.com/influxdata/telegraf/pull/7289): Fix export timestamp not working for prometheus on v2.\n- [#7310](https://github.com/influxdata/telegraf/issues/7310): Fix exclude database and retention policy tags is shared.\n- [#7262](https://github.com/influxdata/telegraf/issues/7262): Fix status path when using globs in phpfpm.\n\n## v1.14 [2020-03-26]\n\n### Release Notes\n\n- In the `sqlserver` input, the `sqlserver_azurestats` measurement has been\n  renamed to `sqlserver_azure_db_resource_stats` due to an issue where numeric\n  metrics were previously being reported incorrectly as strings.\n\n- The `date` processor now uses the UTC timezone when creating its tag.  In\n  previous versions the local time was used.\n```\n\n----------------------------------------\n\nTITLE: Go Memory Stats Metrics in Prometheus Format\nDESCRIPTION: Prometheus metric exports for Go memory statistics including garbage collection CPU fraction, GC system bytes usage, and heap allocation bytes. Each metric includes help text and type definitions following Prometheus conventions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_gauge/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.\n# TYPE go_memstats_gc_cpu_fraction gauge\ngo_memstats_gc_cpu_fraction -0.00014404354379774563\n# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.\n# TYPE go_memstats_gc_sys_bytes gauge\ngo_memstats_gc_sys_bytes 6.0936192e+07\n# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.\n# TYPE go_memstats_heap_alloc_bytes gauge\ngo_memstats_heap_alloc_bytes 1.581062048e+09\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Input Plugin in Telegraf using TOML\nDESCRIPTION: Configuration sample for the network input plugin in Telegraf. This configuration allows for specifying interfaces to monitor using glob patterns and controlling whether protocol statistics should be collected.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/net/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Gather metrics about network interfaces\n[[inputs.net]]\n  ## By default, telegraf gathers stats from any up interface (excluding loopback)\n  ## Setting interfaces will tell it to gather these explicit interfaces,\n  ## regardless of status. When specifying an interface, glob-style\n  ## patterns are also supported.\n  # interfaces = [\"eth*\", \"enp0s[0-1]\", \"lo\"]\n\n  ## On linux systems telegraf also collects protocol stats.\n  ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n  ##\n  ## DEPRECATION NOTICE: A value of 'false' is deprecated and discouraged!\n  ##                     Please set this to `true` and use the 'inputs.nstat'\n  ##                     plugin instead.\n  # ignore_protocol_stats = false\n```\n\n----------------------------------------\n\nTITLE: ZFS Pool Status Listing\nDESCRIPTION: Outputs storage pool names, status, total capacity, used space, and performance metrics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zfs/testcases/freebsd/cache_poolmetrics/zpool.txt#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nfreenas-boot\tONLINE\t30601641984\t2022177280\t28579464704\t-\t6\t1.00x\n```\n\nLANGUAGE: bash\nCODE:\n```\nred1\tONLINE\t8933531975680\t1126164848640\t7807367127040\t8%\t12\t1.83x\n```\n\nLANGUAGE: bash\nCODE:\n```\ntemp1\tONLINE\t2989297238016\t1626309320704\t1362987917312\t38%\t54\t1.28x\n```\n\nLANGUAGE: bash\nCODE:\n```\ntemp2\tONLINE\t2989297238016\t626958278656\t2362338959360\t12%\t20\t1.00x\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf with Nginx VTS Input Plugin\nDESCRIPTION: Shell command demonstrating how to run Telegraf with the Nginx VTS plugin in test mode to verify metrics collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_vts/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./telegraf -config telegraf.conf -input-filter nginx_vts -test\n```\n\n----------------------------------------\n\nTITLE: Documenting Configuration Parameters in TOML\nDESCRIPTION: Example showing proper documentation format for Telegraf configuration parameters using double comments, full sentences, and periods.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n  ## This text describes what an the exchange_type option does.\n  # exchange_type = \"topic\"\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Synchronization Status in Plaintext\nDESCRIPTION: Output from an NTP client command showing synchronization status with a remote NTP server. The table displays the remote server hostname, reference ID, stratum, type, polling interval, reach register, delay, offset, and jitter measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/long_poll/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n-uschi5-ntp-002. 10.177.80.46     3 u  617 68m 377 9.145 +2.849 1.192\n```\n\n----------------------------------------\n\nTITLE: Max Query Metrics Warning Message\nDESCRIPTION: Example log message showing automatic adjustment of max_query_metrics based on server limits.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n2019-01-21T03:24:18Z W! [input.vsphere] Configured max_query_metrics is 256, but server limits it to 64. Reducing.\n```\n\n----------------------------------------\n\nTITLE: Collecting Trace Profile in Telegraf\nDESCRIPTION: Use curl to collect a 10-second trace profile from Telegraf. This command also saves the Telegraf version and Go environment information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl 'http://localhost:6060/debug/pprof/trace?seconds=10' > trace.bin\ntelegraf --version > version.txt\ngo env GOOS GOARCH >> version.txt\n```\n\n----------------------------------------\n\nTITLE: Generating Go Code from Protocol Buffer\nDESCRIPTION: Command to generate Go code from the OpenMetrics protocol buffer definition.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/README.md#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nprotoc --proto_path=. \\\n       --go_out=. \\\n       --go_opt=paths=source_relative \\\n       --go_opt=Mopenmetrics_data_model.proto=github.com/influxdata/telegraf/plugins/parsers/openmetrics \\\n       openmetrics_data_model.proto\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple CPU Input Plugins in TOML\nDESCRIPTION: Example showing how to configure multiple instances of the CPU input plugin with different settings for CPU time collection.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_11\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  percpu = false\n  totalcpu = true\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = false\n  drop = [\"cpu_time\"]\n```\n\n----------------------------------------\n\nTITLE: Defining CAdvisor Version Info Metric in Prometheus Format\nDESCRIPTION: Defines a Prometheus gauge metric named 'cadvisor_version_info' that exposes version information for various system components. The metric includes labels for CAdvisor revision/version, Docker version, kernel version, and OS version, with a constant value of 1.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/default_tags/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP cadvisor_version_info A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\n# TYPE cadvisor_version_info gauge\ncadvisor_version_info{cadvisorRevision=\"\",cadvisorVersion=\"\",dockerVersion=\"1.8.2\",kernelVersion=\"3.10.0-229.20.1.el7.x86_64\",osVersion=\"CentOS Linux 7 (Core)\"} 1\n```\n\n----------------------------------------\n\nTITLE: New Plugin Additions\nDESCRIPTION: Documentation of newly added input and output plugins including rocm_smi for AMD GPUs, mdstat for RAID monitoring, and OpenTelemetry output plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\n- Add rocm_smi input to monitor AMD GPUs\n- Add mdstat input to gather from /proc/mdstat collection\n- Add Elasticsearch query input\n- Add internet Speed Monitor Input Plugin\n- Add OpenTelemetry output\n- Add Azure Data Explorer(ADX) output\n```\n\n----------------------------------------\n\nTITLE: Recommended CPU Plugin Configuration with Inline Tags\nDESCRIPTION: Shows the recommended approach using inline table syntax for tags, which allows for more flexible placement and clearer configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/TOML.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  tags = {tag1 = \"foo\", tag2 = \"bar\"}\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: Querying Message Rates in SQL\nDESCRIPTION: SQL query to calculate the rate of messages published per minute from the total message counts in the RabbitMQ overview metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/rabbitmq/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT NON_NEGATIVE_DERIVATIVE(LAST(\"messages_published\"), 1m) AS messages_published_rate FROM rabbitmq_overview WHERE time > now() - 10m GROUP BY time(1m)\n```\n\n----------------------------------------\n\nTITLE: API Server Request Latency Metrics in Prometheus Format\nDESCRIPTION: Prometheus histogram metrics showing the distribution of API server request latencies in microseconds. The metrics track POST requests to the bindings resource across different latency buckets, including the sum and count of all measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_gaugehistogram/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP apiserver_request_latencies_microseconds Response latency distribution in microseconds for each verb, resource and client.\n# TYPE apiserver_request_latencies_microseconds histogram\n# UNIT apiserver_request_latencies_microseconds microseconds\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"125000\"} 1994\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"250000\"} 1997\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"500000\"} 2000\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"1e+06\"} 2005\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"2e+06\"} 2012\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"4e+06\"} 2017\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"8e+06\"} 2024\napiserver_request_latencies_microseconds_bucket{resource=\"bindings\",verb=\"POST\",le=\"+Inf\"} 2025\napiserver_request_latencies_microseconds_sum{resource=\"bindings\",verb=\"POST\"} 1.02726334e+08\napiserver_request_latencies_microseconds_count{resource=\"bindings\",verb=\"POST\"} 2025\n```\n\n----------------------------------------\n\nTITLE: GoLand Run Configuration for Telegraf\nDESCRIPTION: JetBrains GoLand XML run configuration for debugging Telegraf. This setup enables Telegraf to be built and run with a specified configuration file in the GoLand IDE.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEBUG.md#2025-04-16_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<component name=\"ProjectRunConfigurationManager\">\n  <configuration default=\"false\" name=\"build &amp; run\" type=\"GoApplicationRunConfiguration\" factoryName=\"Go Application\">\n    <module name=\"telegraf\" />\n    <working_directory value=\"$PROJECT_DIR$\" />\n    <parameters value=\"--config telegraf.conf\" />\n    <kind value=\"DIRECTORY\" />\n    <package value=\"github.com/influxdata/telegraf\" />\n    <directory value=\"$PROJECT_DIR$/cmd/telegraf\" />\n    <filePath value=\"$PROJECT_DIR$\" />\n    <method v=\"2\" />\n  </configuration>\n</component>\n```\n\n----------------------------------------\n\nTITLE: Creating ClickHouse Table for Multiple Data Types\nDESCRIPTION: Creates a ClickHouse table 'metric_one' with DateTime, String, Int64, UInt8, UInt64, and Float64 columns. Uses MergeTree engine ordered by timestamp with an index granularity of 8192.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sql/testdata/clickhouse/expected.txt#2025-04-16_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE foo.metric_one\n(\n    `timestamp` DateTime,\n    `tag_one` String,\n    `tag_two` String,\n    `int64_one` Int64,\n    `int64_two` Int64,\n    `bool_one` UInt8,\n    `bool_two` UInt8,\n    `uint64_one` UInt64,\n    `float64_one` Float64\n)\nENGINE = MergeTree\nORDER BY timestamp\nSETTINGS index_granularity = 8192\n```\n\n----------------------------------------\n\nTITLE: Original Input Data Example\nDESCRIPTION: Sample input data showing the original metrics before final aggregation is applied.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/final/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncounter,host=bar i=1,j=4 1554281633101153300\ncounter,host=foo i=1,j=4 1554281633099323601\ncounter,host=bar i=2,j=5 1554281634107980073\ncounter,host=foo i=2,j=5 1554281634105931116\ncounter,host=bar i=3,j=6 1554281635115090133\ncounter,host=foo i=3,j=6 1554281635112992012\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Syslog Message Example for Telegraf\nDESCRIPTION: An example of an RFC5424 formatted syslog message with priority value, timestamp, hostname, application name, and structured data. The message contains HTTP request details including method, path, status code, and response size.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_best_effort_udp_average/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: JSON Query Example with Complex Data\nDESCRIPTION: Shows how to use json_query to parse specific portions of a JSON document with nested structures.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/json/README.md#2025-04-16_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.file]]\n  files = [\"example\"]\n  data_format = \"json\"\n  tag_keys = [\"first\"]\n  json_string_fields = [\"last\"]\n  json_query = \"obj.friends\"\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"obj\": {\n        \"name\": {\"first\": \"Tom\", \"last\": \"Anderson\"},\n        \"age\":37,\n        \"children\": [\"Sara\",\"Alex\",\"Jack\"],\n        \"fav.movie\": \"Deer Hunter\",\n        \"friends\": [\n            {\"first\": \"Dale\", \"last\": \"Murphy\", \"age\": 44},\n            {\"first\": \"Roger\", \"last\": \"Craig\", \"age\": 68},\n            {\"first\": \"Jane\", \"last\": \"Murphy\", \"age\": 47}\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Go Code with gofmt\nDESCRIPTION: The project requires all Go code to be formatted using gofmt. This tool automatically handles most code style requirements, ensuring consistency across the codebase.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/CODE_STYLE.md#2025-04-16_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngofmt\n```\n\n----------------------------------------\n\nTITLE: Sample RFC5424 Syslog Log Entry\nDESCRIPTION: A sample syslog log entry in RFC5424 format that includes priority, timestamp, hostname, application name, process ID, message ID, structured data, and message. This example represents a successful HTTP request with response code 200.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_unix/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Testing Telegraf Configuration\nDESCRIPTION: Command to test if the Telegraf configuration works correctly before starting the service.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" --config \"C:\\Program Files\\Telegraf\\telegraf.conf\" --test\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf with Nginx Plus API Plugin\nDESCRIPTION: Shell command to test the Nginx Plus API plugin configuration with Telegraf in test mode.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nginx_plus_api/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./telegraf -config telegraf.conf -input-filter nginx_plus_api -test\n```\n\n----------------------------------------\n\nTITLE: Example of Plugin Deprecation Entry for logparser\nDESCRIPTION: Concrete example of adding a deprecation entry for the inputs.logparser plugin in the plugins/inputs/deprecations.go file, showing the specific versions and replacement suggestion.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-001-deprecation.md#2025-04-16_snippet_2\n\nLANGUAGE: golang\nCODE:\n```\n    \"logparser\": {\n        Since:     \"1.15.0\",\n        RemovalIn: \"1.40.0\"\n        Notice:    \"use 'inputs.tail' with 'grok' data format instead\",\n    },\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf Secrets Help Command\nDESCRIPTION: Command to get help information about managing secrets with Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/jose/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf secrets help\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Output for ServiceNow - TOML\nDESCRIPTION: This snippet configures the HTTP output for sending metrics to a ServiceNow MID Server. It includes parameters such as the URL, HTTP method (POST), and authentication details. Make sure to customize the credentials and URL for your specific environment.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/nowmetric/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.http]]\n  ## URL is the address to send metrics to\n  url = \"http://<mid server fqdn or ip address>:9082/api/mid/sa/metrics\"\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP method, one of: \"POST\" or \"PUT\"\n  method = \"POST\"\n\n  ## HTTP Basic Auth credentials\n  username = 'evt.integration'\n  password = 'P@$$w0rd!'\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"nowmetric\"\n\n  ## Format Type\n  ## By default, the serializer returns an array of metrics matching the\n  ## Now Metric Operational Intelligence format or with the option set to 'oi'.\n  ## Optionally, if set to 'jsonv2' the output format will involve the newer\n  ## JSON object based format.\n  # nowmetric_format = \"oi\"\n\n  ## Additional HTTP headers\n  [outputs.http.headers]\n  #   # Should be set manually to \"application/json\" for json data_format\n  Content-Type = \"application/json\"\n  Accept = \"application/json\"\n```\n\n----------------------------------------\n\nTITLE: Setting Linux Capabilities\nDESCRIPTION: SystemD configuration to set necessary capabilities for native ping method\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ping/README.md#2025-04-16_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW\nAmbientCapabilities=CAP_NET_RAW\n```\n\n----------------------------------------\n\nTITLE: Capturing CPU Profile\nDESCRIPTION: Command to capture a 30-second CPU profile using pprof\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/PROFILING.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngo tool pprof http://localhost:6060/debug/pprof/profile?seconds=30\n```\n\n----------------------------------------\n\nTITLE: Ceph PGMap State Format - v1.3+\nDESCRIPTION: Example showing the new format of ceph_pgmap_state metric content in Telegraf v1.3+, using a unified 'count' field with state information moved to tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n# field_name    value       tag\ncount           123         state=active+clean\ncount           3           state=active+clean+scrubbing\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status Output in Plaintext Format\nDESCRIPTION: This snippet shows the formatted output from an NTP server query, displaying synchronization statistics. It includes columns for remote server, reference ID, stratum, type, polling information, reach, delay, offset, and jitter measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/days/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  2d  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Loki Output Plugin in TOML\nDESCRIPTION: Configuration example for setting up the Loki output plugin in Telegraf. Includes settings for connection details, authentication, TLS, HTTP headers, and label customization. The configuration allows for specifying the Loki domain, endpoint, timeout, credentials, and various optional parameters for data formatting and transmission.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/loki/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# A plugin that can transmit logs to Loki\n[[outputs.loki]]\n  ## The domain of Loki\n  domain = \"https://loki.domain.tld\"\n\n  ## Endpoint to write api\n  # endpoint = \"/loki/api/v1/push\"\n\n  ## Connection timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n\n  ## Basic auth credential\n  # username = \"loki\"\n  # password = \"pass\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Scope-OrgID\" = \"1\"}\n\n  ## If the request must be gzip encoded\n  # gzip_request = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Sanitize Tag Names\n  ## If true, all tag names will have invalid characters replaced with\n  ## underscores that do not match the regex: ^[a-zA-Z_:][a-zA-Z0-9_:]*.\n  # sanitize_label_names = false\n\n  ## Metric Name Label\n  ## Label to use for the metric name to when sending metrics. If set to an\n  ## empty string, this will not add the label. This is NOT suggested as there\n  ## is no way to differentiate between multiple metrics.\n  # metric_name_label = \"__name\"\n```\n\n----------------------------------------\n\nTITLE: Updating Dependencies in Go Project\nDESCRIPTION: Several dependency updates are listed, including bumping versions of various Go modules used in the Telegraf project. This keeps the project up-to-date with the latest versions of its dependencies.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_13\n\nLANGUAGE: Go\nCODE:\n```\ngo get github.com/aliyun/alibaba-cloud-sdk-go@v1.61.1836\ngo get github.com/prometheus/client_golang@v1.13.1\ngo get github.com/aws/aws-sdk-go-v2/service/timestreamwrite@v1.14.5\ngo get github.com/aws/aws-sdk-go-v2/feature/ec2/imds@v1.12.19\ngo get github.com/gofrs/uuid@v4.3.1\ngo get github.com/aws/aws-sdk-go-v2/service/sts@v1.17.2\ngo get github.com/urfave/cli/v2@v2.23.5\ngo get github.com/Azure/azure-event-hubs-go/v3@v3.3.20\ngo get github.com/showwin/speedtest-go@v1.2.1\ngo get github.com/aws/aws-sdk-go-v2/credentials@v1.13.2\ngo get github.com/yuin/goldmark@v1.5.3\ngo get cloud.google.com/go/pubsub@v1.26.0\ngo get go.mongodb.org/mongo-driver@v1.11.0\n```\n\n----------------------------------------\n\nTITLE: Pulling Telegraf Docker Images\nDESCRIPTION: Commands to pull the latest Telegraf Docker images for both Debian and Alpine-based versions from DockerHub\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/DOCKER.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# latest Debian-based image\ndocker pull telegraf\n# latest Alpine-based image\ndocker pull telegraf:alpine\n```\n\n----------------------------------------\n\nTITLE: Example Zipkin JSON Output\nDESCRIPTION: Displays an example JSON output from the Zipkin server, showing traces, spans, annotations, and binary annotations. This data is used to generate metrics for analysis and debugging of service performance. Prerequisites include properly configured Zipkin Input Plugin and understanding of JSON structure for trace data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zipkin/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"trace_id\": 2505404965370368069,\n        \"name\": \"Child\",\n        \"id\": 8090652509916334619,\n        \"parent_id\": 22964302721410078,\n        \"annotations\": [],\n        \"binary_annotations\": [\n            {\n                \"key\": \"lc\",\n                \"value\": \"dHJpdmlhbA==\",\n                \"annotation_type\": \"STRING\",\n                \"host\": {\n                    \"ipv4\": 2130706433,\n                    \"port\": 0,\n                    \"service_name\": \"trivial\"\n                }\n            }\n        ],\n        \"timestamp\": 1498688360851331,\n        \"duration\": 53106\n    },\n    {\n        \"trace_id\": 2505404965370368069,\n        \"name\": \"Child\",\n        \"id\": 103618986556047333,\n        \"parent_id\": 22964302721410078,\n        \"annotations\": [],\n        \"binary_annotations\": [\n            {\n                \"key\": \"lc\",\n                \"value\": \"dHJpdmlhbA==\",\n                \"annotation_type\": \"STRING\",\n                \"host\": {\n                    \"ipv4\": 2130706433,\n                    \"port\": 0,\n                    \"service_name\": \"trivial\"\n                }\n            }\n        ],\n        \"timestamp\": 1498688360904552,\n        \"duration\": 50410\n    },\n    {\n        \"trace_id\": 2505404965370368069,\n        \"name\": \"Parent\",\n        \"id\": 22964302721410078,\n        \"annotations\": [\n            {\n                \"timestamp\": 1498688360851325,\n                \"value\": \"Starting child #0\",\n                \"host\": {\n                    \"ipv4\": 2130706433,\n                    \"port\": 0,\n                    \"service_name\": \"trivial\"\n                }\n            },\n            {\n                \"timestamp\": 1498688360904545,\n                \"value\": \"Starting child #1\",\n                \"host\": {\n                    \"ipv4\": 2130706433,\n                    \"port\": 0,\n                    \"service_name\": \"trivial\"\n                }\n            },\n            {\n                \"timestamp\": 1498688360954992,\n                \"value\": \"A Log\",\n                \"host\": {\n                    \"ipv4\": 2130706433,\n                    \"port\": 0,\n                    \"service_name\": \"trivial\"\n                }\n            }\n        ],\n        \"binary_annotations\": [\n            {\n                \"key\": \"lc\",\n                \"value\": \"dHJpdmlhbA==\",\n                \"annotation_type\": \"STRING\",\n                \"host\": {\n                    \"ipv4\": 2130706433,\n                    \"port\": 0,\n                    \"service_name\": \"trivial\"\n                }\n            }\n        ],\n        \"timestamp\": 1498688360851318,\n        \"duration\": 103680\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring NSQ Input Plugin in Telegraf with TOML\nDESCRIPTION: Configuration sample for the NSQ input plugin in Telegraf. It specifies the NSQD HTTP API endpoints to connect to and includes optional TLS configuration options for secure connections.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsq/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Read NSQ topic and channel statistics.\n[[inputs.nsq]]\n  ## An array of NSQD HTTP API endpoints\n  endpoints  = [\"http://localhost:4151\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n----------------------------------------\n\nTITLE: Recording CPU Time Usage in Prometheus Format\nDESCRIPTION: This snippet tracks the total CPU time spent by the process in seconds. It's essential for monitoring resource usage and system performance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/multiple/input.txt#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n# TYPE process_cpu_seconds counter\n# UNIT process_cpu_seconds seconds\n# HELP process_cpu_seconds Total user and system CPU time spent in seconds.\nprocess_cpu_seconds_total 4.20072246e+06\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Format Conversion in Telegraf\nDESCRIPTION: Example showing a diff between InfluxDB Line Protocol and the custom format. This illustrates how the serializer transforms data, with pipe characters replacing commas as separators.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/EXAMPLE_README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- cpu,host=localhost,source=example.org value=42\n+ cpu|host=localhost|source=example.org|value=42\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceNow JSONv2 Format Event - JSON\nDESCRIPTION: This snippet shows the structure of a metrics event using the ServiceNow JSONv2 format, detailing how to convey metric data structured as a JSON object. It is specifically formatted to be compatible with the newer JSON-based services offered by ServiceNow.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/nowmetric/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"records\": [\n    {\n      \"metric_type\": \"Disk C: % Free Space\",\n      \"resource\": \"C:\\\\\",\n      \"node\": \"lnux100\",\n      \"value\": 50,\n      \"timestamp\": 1473183012000,\n      \"ci2metric_id\": {\n        \"node\": \"lnux100\"\n      },\n      \"source\": \"Telegraf\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Final Aggregator Output Example\nDESCRIPTION: Example showing the final aggregated output format with _final suffix added to field names.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/final/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncounter,host=bar i_final=3,j_final=6 1554281635115090133\ncounter,host=foo i_final=3,j_final=6 1554281635112992012\n```\n\n----------------------------------------\n\nTITLE: Formatting a Syslog Message in Plaintext\nDESCRIPTION: An example of a syslog message with priority level 13, timestamp, hostname, application name, and message content. This follows standard syslog format with the priority enclosed in angle brackets.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc3164_best_effort_udp/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<13>Dec  2 16:31:03 host app: Test\n```\n\n----------------------------------------\n\nTITLE: Reverse DNS Output Example Comparison\nDESCRIPTION: A diff showing the transformation of a metric after processing by the reverse_dns processor, where an IP address is looked up and a new domain tag is added.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/reverse_dns/README.md#2025-04-16_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- ping,ip=8.8.8.8 elapsed=300i 1502489900000000000\n+ ping,ip=8.8.8.8,domain=dns.google. elapsed=300i 1502489900000000000\n```\n\n----------------------------------------\n\nTITLE: Syslog Priority Value Examples in Telegraf\nDESCRIPTION: Examples of syslog priority values using the format '<PRI>VERSION'. The priority (PRI) value is calculated as Facility*8+Severity and is enclosed in angle brackets. These examples show different priority levels that might be used in Telegraf's logging system.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcp_1st_min_ok_2nd_min_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n16 <1>2 - - - - - -\n17 <4>11 - - - - - -\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios Parser Plugin in Telegraf (TOML)\nDESCRIPTION: This TOML configuration snippet demonstrates how to set up the exec input plugin in Telegraf to execute a Nagios plugin command and parse its output using the Nagios data format. It includes the command to run and specifies the data format as 'nagios'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/nagios/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.exec]]\n  ## Commands array\n  commands = [\"/usr/lib/nagios/plugins/check_load -w 5,6,7 -c 7,8,9\"]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ##   https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"nagios\"\n```\n\n----------------------------------------\n\nTITLE: Setting Capabilities for Wireguard - Bash\nDESCRIPTION: This bash command grants the Telegraf binary the CAP_NET_ADMIN capability, necessary for accessing the kernel space implementation of Wireguard. This is essential for proper functioning when Telegraf is not running as root.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/wireguard/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"sudo setcap CAP_NET_ADMIN+epi $(which telegraf)\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Group Permissions for NSD\nDESCRIPTION: Bash commands for adding the Telegraf user to the NSD group to enable access to NSD control functionality.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ groups telegraf\ntelegraf : telegraf\n\n$ usermod -a -G nsd telegraf\n\n$ groups telegraf\ntelegraf : telegraf nsd\n```\n\n----------------------------------------\n\nTITLE: Building the readme_linter tool\nDESCRIPTION: This snippet shows how to build the readme_linter tool using the go build command.  The command should be executed from within the `telegraf/tools/readme_linter` directory. This creates an executable named `readme_linter`.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/readme_linter/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n```shell\ntelegraf/tools/readme_linter$ go build .\n```\n```\n\n----------------------------------------\n\nTITLE: Starting Telegraf Service with service command\nDESCRIPTION: Alternative command to start the Telegraf Windows service using the service start option.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" service start\n```\n\n----------------------------------------\n\nTITLE: Configuring Print Valid Performance Objects\nDESCRIPTION: Enables printing of all matching performance objects for debugging and configuration verification\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\nPrintValid=true\n```\n\n----------------------------------------\n\nTITLE: Generating UTF-8 Encoded Config in Windows PowerShell\nDESCRIPTION: Command to generate a UTF-8 encoded Telegraf configuration file in Windows PowerShell v5, addressing encoding issues.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ntelegraf.exe config | Out-File -Encoding utf8 telegraf.conf\n```\n\n----------------------------------------\n\nTITLE: Boolean Value Configuration\nDESCRIPTION: This snippet represents a boolean value, specifically 'true', which is commonly used in configuration files to enable features or options. In the context of Telegraf, this would likely be a setting that is either active or inactive. No specific dependencies are required, as it is a fundamental data type.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/multifile/testdata/bool.txt#2025-04-16_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n\"true\"\n```\n\n----------------------------------------\n\nTITLE: Pulling Telegraf Nightly Docker Images from quay.io\nDESCRIPTION: Commands for pulling Telegraf nightly build Docker images. Includes options for both Debian-based (latest) and Alpine-based images from the quay.io repository.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/NIGHTLIES.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Debian-based image\ndocker pull quay.io/influxdb/telegraf-nightly:latest\n# Alpine-based image\ndocker pull quay.io/influxdb/telegraf-nightly:alpine\n```\n\n----------------------------------------\n\nTITLE: File Output Configuration - TOML\nDESCRIPTION: Configuration for testing Grok patterns by outputting to stdout.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/grok/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  files = [\"stdout\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Output Plugin in Telegraf\nDESCRIPTION: Minimal configuration for sending metrics to InfluxDB. The example shows how to specify the URL and database for an InfluxDB instance.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_best_effort_udp_min_incomplete/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n<1>2\n```\n\n----------------------------------------\n\nTITLE: Adding Deprecation Tags to Struct Fields in Golang\nDESCRIPTION: Demonstrates how to add a 'deprecated' struct tag to configuration options in Golang. The tag includes version information and instructions for alternatives.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/DEPRECATION.md#2025-04-16_snippet_3\n\nLANGUAGE: golang\nCODE:\n```\ntype AMQP struct {\n    URL       string `toml:\"url\" deprecated:\"1.7.0;use 'brokers' instead\"`\n    Precision string `toml:\"precision\" deprecated:\"1.2.0;option is ignored\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP client command, displaying detailed status information for a remote NTP server. It includes the server name, IP address, stratum level, type, poll interval, reachability, delay, offset, and jitter measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/servers/input_serverA.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Defining Counter Metric in Prometheus Format\nDESCRIPTION: Demonstrates how to define a counter metric type with labels in Prometheus exposition format. The metric named 'test_counter' has a label 'test' and value of 1 with a timestamp of 1601830800000.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/ignore_timestamp/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# TYPE test_counter counter\ntest_counter{label=\"test\"} 1 1601830800000\n```\n\n----------------------------------------\n\nTITLE: Specifying Multiple Configurations in Custom Builder\nDESCRIPTION: Configure custom_builder with a series of configuration files and directories for systems with differing configurations. Each configuration path should be accessible.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# ./tools/custom_builder/custom_builder             \\\\n    --config system1/telegraf.conf  \\\\n    --config system2/telegraf.conf  \\\\n    --config ...                    \\\\n    --config systemN/telegraf.conf  \\\\n    --config-dir system1/telegraf.d \\\\n    --config-dir system2/telegraf.d \\\\n    --config-dir ...                \\\\n    --config-dir systemN/telegraf.d\n```\n\n----------------------------------------\n\nTITLE: SMTP Command Histograms\nDESCRIPTION: Histogram metrics for SMTP commands showing latency distributions for commands like LHLO, MAIL, RCPT, DATA, and QUIT.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/dovecot/input.txt#2025-04-16_snippet_3\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP dovecot_smtp_command Histogram\n# TYPE dovecot_smtp_command histogram\ndovecot_smtp_command_bucket{cmd_name=\"LHLO\",status_code=\"250\",le=\"10\"} 0\n[...additional histogram buckets...]\n```\n\n----------------------------------------\n\nTITLE: Executing License Checker Tool\nDESCRIPTION: Runs the license verification tool using the current directory as the Telegraf root directory. It reports only the errors by default. This requires that the compiled tool is located at the specified path.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/license_checker/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./tools/license_checker/license_checker\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Secret-store in Telegraf with TOML\nDESCRIPTION: This TOML configuration snippet sets up an HTTP secret-store in Telegraf, specifying the unique store identifier, HTTP endpoint URL, authentication methods, and security features. Dependencies include having a valid HTTP endpoint for secrets retrieval. Parameters involve `id`, `url`, and optional authentication and security settings. The expected output is a successful connection and retrieval of secrets.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/http/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[secretstores.http]]\n  id = \"secretstore\"\n  url = \"http://localhost/secrets\"\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n  # token = \"your-token\"\n  # username = \"username\"\n  # password = \"pa$$word\"\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n  # use_system_proxy = false\n  # http_proxy_url = \"\"\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_min_version = \"TLS12\"\n  # insecure_skip_verify = false\n  # cookie_auth_url = \"https://localhost/authMe\"\n  # cookie_auth_method = \"POST\"\n  # cookie_auth_username = \"username\"\n  # cookie_auth_password = \"pa$$word\"\n  # cookie_auth_headers = { Content-Type = \"application/json\", X-MY-HEADER = \"hello\" }\n  # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n  # cookie_auth_renewal = \"0s\"\n  # timeout = \"5s\"\n  # success_status_codes = [200]\n  # transformation = ''\n  # cipher = \"none\"\n  # [secretstores.http.aes]\n  #   key = \"\"\n  #   init_vector = \"\"\n  #   # kdf_algorithm = \"PBKDF2-HMAC-SHA256\"\n  #   # password = \"\"\n  #   # salt = \"\"\n  #   # iterations = 0\n```\n\n----------------------------------------\n\nTITLE: Message B Binary Structure\nDESCRIPTION: Definition of Message B format showing field layout including ID, type, length, and value fields in little-endian format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/binary/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n+--------+------+------+------------+\n| ID     | type | len  | value      |\n+--------+------+------+------------+\n| 0x0201 | 0x0B | 0x04 | 0xDEADC0DE |\n+--------+------+------+------------+\n```\n\n----------------------------------------\n\nTITLE: Defining a Minimal Syslog Message in RFC5424 Format\nDESCRIPTION: Shows the format of a minimal syslog message with priority 1 (facility=0, severity=1). The message uses placeholder hyphens for the standard syslog fields and contains only the letter 'A' as the message content.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_best_effort_udp_trim_message/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<1>1 - - - - - - \tA\n```\n\n----------------------------------------\n\nTITLE: Example AppArmor Denial Message for Telegraf\nDESCRIPTION: This example shows an AppArmor denial message when Telegraf attempts to use ptrace with read permissions. This occurs when Telegraf is running under a docker-default profile that restricts the ptrace operation.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/APPARMOR.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntype=AVC msg=audit(1588901740.036:2457789): apparmor=\"DENIED\" operation=\"ptrace\" profile=\"docker-default\" pid=9030 comm=\"telegraf\" requested_mask=\"read\" denied_mask=\"read\" peer=\"unconfined\"\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Container for Building Telegraf\nDESCRIPTION: Command to start a shell in the Telegraf CI Docker container, which provides the environment for building Telegraf packages.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PACKAGING.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -ti quay.io/influxdb/telegraf-ci:1.9.7 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Configuring Beanstalkd Input Plugin for Telegraf in TOML\nDESCRIPTION: Configuration file snippet for the Beanstalkd input plugin. Specifies the server address to collect data from and optional list of tubes to gather statistics about. If no tubes are specified, data will be gathered for each tube on the server.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/beanstalkd/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Collects Beanstalkd server and tubes stats\n[[inputs.beanstalkd]]\n  ## Server to collect data from\n  server = \"localhost:11300\"\n\n  ## List of tubes to gather stats about.\n  ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n  tubes = [\"notifications\"]\n```\n\n----------------------------------------\n\nTITLE: Querying NSDP Device Port Metrics in InfluxDB\nDESCRIPTION: This snippet shows the structure of NSDP device port metrics in InfluxDB line protocol format. It includes device information, port number, and various network statistics for each port.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/nsdp/testdata/metrics/nsdp_device_port.txt#2025-04-16_snippet_0\n\nLANGUAGE: influxdb\nCODE:\n```\nnsdp_device_port,device=12:34:56:78:9a:bc,device_ip=10.1.0.4,device_model=GS108Ev3,device_name=switch2,device_port=1 broadcasts_total=0u,bytes_recv=3879427866u,bytes_sent=506548796u,errors_total=0u,multicasts_total=0u,packets_total=0u 1737152505014578000\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry - v1.13.4\nDESCRIPTION: Changelog entry documenting bug fixes and release notes for Telegraf version 1.13.4\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## v1.13.4 [2020-02-25]\n\n### Release Notes\n\n- Official packages now built with Go 1.13.8.\n\n### Bug Fixes\n\n- [#6988]: Parse NaN values from summary types in prometheus input.\n- [#6820]: Fix pgbouncer input when used with newer pgbouncer versions.\n- [#6913]: Support up to 8192 stats in the ethtool input.\n- [#7060]: Fix perf counters collection on named instances in sqlserver input.\n- [#6926]: Use add time for prometheus expiration calculation.\n- [#7057]: Fix inconsistency with input error counting in internal input.\n- [#7063]: Use the same timestamp per call if no time is provided in prometheus input.\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Syslog Message Example in Telegraf\nDESCRIPTION: An example of a properly formatted RFC5424 syslog message. It includes priority value (29), timestamp, hostname (web1), application name (someservice), process ID (2341), message ID (2), structured data with origin namespace containing meta fields, and HTTP request information in the message part.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_strict_udp_average/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Adding OData-Version Header in Go HTTP Request\nDESCRIPTION: Adds the OData-Version header to HTTP requests in the Redfish input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_21\n\nLANGUAGE: go\nCODE:\n```\nAdd OData-Version header to requests\n```\n\n----------------------------------------\n\nTITLE: Example Output from HTTP Input Plugin\nDESCRIPTION: This code snippet shows example output from the HTTP input plugin for Telegraf. It contains metrics for a Citibike station, including availability and status information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/http/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncitibike,station_id=4703 eightd_has_available_keys=false,is_installed=1,is_renting=1,is_returning=1,legacy_id=\"4703\",num_bikes_available=6,num_bikes_disabled=2,num_docks_available=26,num_docks_disabled=0,num_ebikes_available=0,station_status=\"active\" 1641505084000000000\ncitibike,station_id=4704 eightd_has_available_keys=false,is_installed=1,is_renting=1,is_returning=1,legacy_id=\"4704\",num_bikes_available=10,num_bikes_disabled=2,num_docks_available=36,num_docks_disabled=0,num_ebikes_available=0,station_status=\"active\" 1641505084000000000\ncitibike,station_id=4711 eightd_has_available_keys=false,is_installed=1,is_renting=1,is_returning=1,legacy_id=\"4711\",num_bikes_available=9,num_bikes_disabled=0,num_docks_available=36,num_docks_disabled=0,num_ebikes_available=1,station_status=\"active\" 1641505084000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Name Suffix\nDESCRIPTION: Example showing how to configure the CPU input plugin with a name suffix to emit measurements with the name 'cpu_total'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  name_suffix = \"_total\"\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Systemd Secret-Store in Telegraf\nDESCRIPTION: Simple TOML configuration for adding a systemd secret-store to Telegraf configuration, setting up the minimum required parameters to establish the secret-store.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[secretstores.systemd]]\n  id = \"systemd\"\n```\n\n----------------------------------------\n\nTITLE: Defining powerstat_package Measurement Tags in Markdown\nDESCRIPTION: This snippet defines the tags returned with powerstat_package measurements, including package_id, active_cores, hybrid, die, and type.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/intel_powerstat/README.md#2025-04-16_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n| Tag            | Description                                                                                                                                                                                                                    |\n|----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `package_id`   | ID of platform package/socket.                                                                                                                                                                                                 |\n| `active_cores` | Specific tag for `max_turbo_frequency_mhz` metric. The maximum number of activated cores for reachable turbo frequency.                                                                                                        |\n| `hybrid`       | Specific tag for `max_turbo_frequency_mhz` metric. Available only for hybrid processors. Will be set to `primary` for primary cores of a hybrid architecture, and to `secondary` for secondary cores of a hybrid architecture. |\n| `die`          | Specific tag for all `uncore_frequency` metrics. Id of die.                                                                                                                                                                    |\n| `type`         | Specific tag for all `uncore_frequency` metrics. Type of uncore frequency (`current` or `initial`).                                                                                                                            |\n```\n\n----------------------------------------\n\nTITLE: Configuring Processor Plugins in Telegraf\nDESCRIPTION: This snippet shows how to configure processor plugins in Telegraf. It includes an example of the 'enum' processor, which can be used to map string values to numeric values for a specified field.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/migrations/inputs_jolokia/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n###############################################################################\n#                            PROCESSOR PLUGINS                               #\n###############################################################################\n\n# # Convert values to another metric value type\n# [[processors.enum]]\n#   ## The name of the field to map\n#   field = \"status\"\n#\n#   ## Name of the tag to map (when no field is specified)\n#   # tag = \"status\"\n#\n#   ## Destination tag or field to be used for the mapped value.  By default the\n#   ## source tag or field is used, overwriting the original value.\n#   # dest = \"status_code\"\n#\n#   ## Default value to be used for all values not contained in the mapping\n#   ## table.  When unset, the unmodified value for the field will be used if no\n#   ## match is found.\n#   # default = 0\n#\n#   ## Table of mappings\n#   [processors.enum.mapping]\n#     green = 1\n#     amber = 2\n#     red = 3\n```\n\n----------------------------------------\n\nTITLE: Deprecating a Plugin Option with Struct Tags\nDESCRIPTION: Code example showing how to deprecate a specific plugin option by adding a 'deprecated' struct tag to the field. The tag includes information about when it was deprecated, when it will be removed, and an alternative.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-001-deprecation.md#2025-04-16_snippet_4\n\nLANGUAGE: golang\nCODE:\n```\ntype Example struct {\n    ...\n    SSLEnabled bool `toml:\"ssl_enabled\" deprecated:\"1.3.0;1.40.0;use 'tls_*' options instead\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Inline Tags\nDESCRIPTION: Example showing how to configure the CPU input plugin with additional tags using inline table syntax.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  tags = {tag1 = \"foo\", tag2 = \"bar\"}\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: Date Specifiers for Elasticsearch Index Names\nDESCRIPTION: Reference for date format specifiers that can be used in the index_name parameter to create time-based indices.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n%Y - year (2017)\n%y - last two digits of year (00..99)\n%m - month (01..12)\n%d - day of month (e.g., 01)\n%H - hour (00..23)\n%V - week of the year (ISO week) (01..53)\n```\n\n----------------------------------------\n\nTITLE: Configuring Clone Processor in Telegraf\nDESCRIPTION: Sample configuration for the clone processor plugin, demonstrating optional name overrides, prefixes, suffixes, and tag modifications\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/clone/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Apply metric modifications using override semantics.\n[[processors.clone]]\n  ## All modifications on inputs and aggregators can be overridden:\n  # name_override = \"new_name\"\n  # name_prefix = \"new_name_prefix\"\n  # name_suffix = \"new_name_suffix\"\n\n  ## Tags to be added (all values must be strings)\n  # [processors.clone.tags]\n  #   additional_tag = \"tag_value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Name Override\nDESCRIPTION: Example showing how to configure the CPU input plugin with a name override to emit measurements with the name 'foobar'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/config/README.md#2025-04-16_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  name_override = \"foobar\"\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Syslog Format Example in Plaintext\nDESCRIPTION: This snippet shows a basic example of a RFC5424 formatted syslog message. It includes the priority value (<1>), version number (1), and dash characters as placeholders for timestamp, hostname, app name, process ID, and message ID fields, followed by the message content 'A'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_strict_udp_trim_message/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<1>1 - - - - - - \tA\n```\n\n----------------------------------------\n\nTITLE: KSM Metrics Output Example\nDESCRIPTION: Example output when KSM (Kernel Samepage Merging) metrics collection is enabled.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kernel/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nkernel boot_time=1690487872i,context_switches=321252729i,entropy_avail=256i,interrupts=141783427i,ksm_full_scans=0i,ksm_max_page_sharing=256i,ksm_merge_across_nodes=1i,ksm_pages_shared=0i,ksm_pages_sharing=0i,ksm_pages_to_scan=100i,ksm_pages_unshared=0i,ksm_pages_volatile=0i,ksm_run=0i,ksm_sleep_millisecs=20i,ksm_stable_node_chains=0i,ksm_stable_node_chains_prune_millisecs=2000i,ksm_stable_node_dups=0i,ksm_use_zero_pages=0i,processes_forked=946467i 1691339522000000000\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf using Docker\nDESCRIPTION: Command to pull the Telegraf Docker image from Docker Hub.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/QUICK_START.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull telegraf\n```\n\n----------------------------------------\n\nTITLE: Module Version Updates and Bug Fixes\nDESCRIPTION: List of bug fixes including module version updates and plugin-specific fixes to improve functionality and security of Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_17\n\nLANGUAGE: markdown\nCODE:\n```\n- Update thrift module to 0.14.2 and zipkin-go-opentracing to 0.4.5\n- Update runc module to v1.0.0-rc95 to address CVE-2021-30465\n- Migrate dgrijalva/jwt-go to golang-jwt/jwt/v4\n- Update cloud.google.com/go/pubsub module from 1.2.0 to 1.15.0\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status command, presenting synchronization information for multiple remote NTP servers. It includes details such as remote server addresses, reference IDs, status flags, polling intervals, reachability, delay, offset, and jitter values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/servers/input_serverB.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n 83.137.98.96    10.177.80.37     2 u  740 1024  377   54.033  243.426 449514.\n 81.7.16.52      10.177.80.37     2 u  739 1024  377   60.785  232.597 449539.\n 131.188.3.221   10.177.80.37     2 u  783 1024  377  111.820  261.921 449528.\n 5.9.29.107      10.177.80.37     2 u  703 1024  377  205.704  160.406 449602.\n 91.189.94.4     10.177.80.37     2 u  673 1024  377  143.047  274.726 449445.\n```\n\n----------------------------------------\n\nTITLE: Configuring File Output for ServiceNow - TOML\nDESCRIPTION: This snippet shows how to configure the file output in Telegraf to write metrics to a file. It specifies the file path and the data format to be used. The 'nowmetric' format is utilized to ensure compatibility with the expected ServiceNow data structure.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/nowmetric/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"C:/Telegraf/metrics.out\"]\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"nowmetric\"\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Syslog Message Format Example\nDESCRIPTION: An example of a complete syslog message following the RFC5424 format. It includes priority value (163), version (1), timestamp, hostname, application name (truncated due to length), process ID (ID47), and the actual message about a failed 'su root' attempt.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_best_effort_toolong_appname/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<163>1 2003-10-11T22:14:15.003Z mymachine.example.com ALongApplicationNameContainingMoreThanTheAllowed48Character - ID47 - BOM'su root' failed for lonvick on /dev/pts/8\n```\n\n----------------------------------------\n\nTITLE: Querying Icinga2 Service Status in SQL\nDESCRIPTION: These SQL queries demonstrate how to retrieve Icinga2 service status information from the collected metrics. They show examples for querying services with OK, WARNING, CRITICAL, and UNKNOWN statuses within the last 24 hours.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/icinga2/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM \"icinga2_services\" WHERE state_code = 0 AND time > now() - 24h // Service with OK status\nSELECT * FROM \"icinga2_services\" WHERE state_code = 1 AND time > now() - 24h // Service with WARNING status\nSELECT * FROM \"icinga2_services\" WHERE state_code = 2 AND time > now() - 24h // Service with CRITICAL status\nSELECT * FROM \"icinga2_services\" WHERE state_code = 3 AND time > now() - 24h // Service with UNKNOWN status\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Syslog Message with Maximum Field Lengths\nDESCRIPTION: This is an example of a syslog message formatted according to RFC5424 specification with fields populated to their maximum allowed length. The message has priority 191, severity 999, and timestamp in ISO format with millisecond precision and timezone offset.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_best_effort_udp_max/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<191>999 2017-12-31T23:59:59.999999+00:00 abcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabc abcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdef abcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzab abcdefghilmnopqrstuvzabcdefghilm - lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n```\n\n----------------------------------------\n\nTITLE: Checking Telegraf Version\nDESCRIPTION: Command to display the current version of the Telegraf binary. Useful for verifying which version is installed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/COMMANDS_AND_FLAGS.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf version\n```\n\n----------------------------------------\n\nTITLE: Updating Go Version in Telegraf Project\nDESCRIPTION: Command to run Go version update tool with version arguments, supporting full and minor version formats\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/update_goversion/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngo run tools/update_goversion/main.go 1.19.2\n```\n\nLANGUAGE: bash\nCODE:\n```\ngo run tools/update_goversion/main.go 1.19\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Error Response Example\nDESCRIPTION: Example of an error response from Elasticsearch when encountering unsupported integer values in the JSON encoding.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/elasticsearch/README.md#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"error\":{\"root_cause\":[{\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\"}],\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\",\"caused_by\":{\"type\":\"illegal_state_exception\",\"reason\":\"No matching token for number_type [BIG_INTEGER]\"}},\"status\":400}\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry - v1.13.2\nDESCRIPTION: Changelog entry documenting bug fixes for Telegraf version 1.13.2\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## v1.13.2 [2020-01-21]\n\n### Bug Fixes\n\n- [#2652]: Warn without error when processes input is started on Windows.\n- [#6890]: Only parse certificate blocks in x509_cert input.\n- [#6883]: Add custom attributes for all resource types in vsphere input.\n- [#6899]: Fix URL agent address form with udp in snmp input.\n- [#6619]: Change logic to allow recording of device fields when attributes is false.\n- [#6903]: Do not add invalid timestamps to kafka messages.\n- [#6906]: Fix json_strict option and set default of true.\n```\n\n----------------------------------------\n\nTITLE: Environment File for Docker Secrets\nDESCRIPTION: Environment configuration file showing how to define secret values and user ID for Docker Compose\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/docker/README.md#2025-04-16_snippet_2\n\nLANGUAGE: env\nCODE:\n```\nTELEGRAF_PLUGIN_CREDENTIAL=superSecretStuff\n# determine this value by executing `id -u` in terminal\nUSERID=1000\n```\n\n----------------------------------------\n\nTITLE: Syslog Message Format Example with Priority and Version Tags\nDESCRIPTION: This is an example of a Syslog message in a structured format. It includes priority value <1>, version 217, and priority again <11> followed by the number 1 and several dash placeholders. This format is commonly used for system logging in Unix/Linux environments.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcp_1st_underflow_malfunction/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n16 <1>217 <11>1 - - - - - -\n```\n\n----------------------------------------\n\nTITLE: Creating ClickHouse Table for String Data\nDESCRIPTION: Creates a ClickHouse table 'metric_two' with DateTime and String columns. Uses MergeTree engine ordered by timestamp with an index granularity of 8192.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/sql/testdata/clickhouse/expected.txt#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE foo.metric_two\n(\n    `timestamp` DateTime,\n    `tag_three` String,\n    `string_one` String\n)\nENGINE = MergeTree\nORDER BY timestamp\nSETTINGS index_granularity = 8192\n```\n\n----------------------------------------\n\nTITLE: Installing iputils-ping Package\nDESCRIPTION: Command to install the iputils-ping package as an alternative to GNU Inetutils ping\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ping/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\napt-get install iputils-ping\n```\n\n----------------------------------------\n\nTITLE: Dependency Updates in Markdown\nDESCRIPTION: List of dependency version updates and their corresponding pull requests.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n### Dependency Updates\n\n- [#11788](https://github.com/influxdata/telegraf/pull/11788) `deps` Bump cloud.google.com/go/pubsub from 1.24.0 to 1.25.1\n- [#11794](https://github.com/influxdata/telegraf/pull/11794) `deps` Bump github.com/urfave/cli/v2 from 2.14.1 to 2.16.3\n- [#11789](https://github.com/influxdata/telegraf/pull/11789) `deps` Bump github.com/aws/aws-sdk-go-v2/service/ec2\n- [#11799](https://github.com/influxdata/telegraf/pull/11799) `deps` Bump github.com/wavefronthq/wavefront-sdk-go\n- [#11796](https://github.com/influxdata/telegraf/pull/11796) `deps` Bump cloud.google.com/go/bigquery from 1.33.0 to 1.40.0\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Syslog Message Example\nDESCRIPTION: A sample log message following the RFC5424 format with severity level 1 (alert). The message contains placeholder fields marked with hyphens, which would typically contain values like timestamp, hostname, app name, process ID, message ID, and structured data.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcp_1st_min_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n16 <1>1 - - - - - -\n```\n\n----------------------------------------\n\nTITLE: Viewing Help for Custom Builder\nDESCRIPTION: This command provides access to the help documentation for the custom_builder tool, listing available options and usage details.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# ./tools/custom_builder/custom_builder --help\n```\n\n----------------------------------------\n\nTITLE: Converting Port Tag to String Field in Telegraf\nDESCRIPTION: Example showing how to convert a 'port' tag to a string field in Telegraf metrics, with before and after comparison.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/converter/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.converter]]\n  [processors.converter.tags]\n    string = [\"port\"]\n```\n\n----------------------------------------\n\nTITLE: Example Shell Script for Static Metrics\nDESCRIPTION: A simple shell script that outputs static metric values in InfluxDB line protocol format. Values are tagged with tag1 and tag2, and include integer measurements i, j, and k.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/exec/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n#!/bin/sh\necho 'example,tag1=a,tag2=b i=42i,j=43i,k=44i'\n```\n\n----------------------------------------\n\nTITLE: Generating Complete Telegraf Configuration via CLI\nDESCRIPTION: Command to generate a full sample configuration file for Telegraf using the CLI.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/SAMPLE_CONFIG.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelegraf config\n```\n\n----------------------------------------\n\nTITLE: Cloning Telegraf Repository Shell Command\nDESCRIPTION: This shell command clones a specific branch of the Telegraf repository from GitHub. Ensure that Git is installed and network access to GitHub is available.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/custom_builder/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# git clone --branch v1.29.5 --single-branch https://github.com/influxdata/telegraf.git\\n...\\n# cd telegraf\n```\n\n----------------------------------------\n\nTITLE: Building Build Tools Shell Command\nDESCRIPTION: This command builds the necessary tools for the license verification. It should be executed in the command line within the project's directory.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/tools/license_checker/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake build_tools\n```\n\n----------------------------------------\n\nTITLE: Repository Link Reference in Markdown\nDESCRIPTION: GitHub repository links used throughout the changelog to reference issues and pull requests\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n[#143](https://github.com/influxdata/telegraf/issues/143)\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes NodeIP Environment Variable in Pod YAML\nDESCRIPTION: YAML configuration to set the NODE_IP environment variable in a Kubernetes pod using the pod's host IP. This is needed when using node-level pod scrape scope.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/prometheus/README.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: NODE_IP\n    valueFrom:\n      fieldRef:\n        fieldPath: status.hostIP\n```\n\n----------------------------------------\n\nTITLE: Example Output from dmcache Input Plugin\nDESCRIPTION: Sample output from the dmcache input plugin showing the metrics collected from a dm-cache device named 'example'. The output is in InfluxDB line protocol format with various cache statistics as fields and the device name as a tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/dmcache/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndmcache,device=example cache_blocksize=0i,read_hits=995134034411520i,read_misses=916807089127424i,write_hits=195107267543040i,metadata_used=12861440i,write_misses=563725346013184i,promotions=3265223720960i,dirty=0i,metadata_blocksize=0i,cache_used=1099511627776ii,cache_total=0i,length=0i,metadata_total=1073741824i,demotions=3265223720960i 1491482035000000000\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry - v1.13.3\nDESCRIPTION: Changelog entry documenting bug fixes for Telegraf version 1.13.3\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## v1.13.3 [2020-02-04]\n\n### Bug Fixes\n\n- [#5744]: Fix kibana input with Kibana versions greater than 6.4.\n- [#6960]: Fix duplicate TrackingIDs can be returned in queue consumer plugins.\n- [#6913]: Support up to 4096 stats in the ethtool input.\n- [#6973]: Expire metrics on query in addition to on add.\n```\n\n----------------------------------------\n\nTITLE: Telegraf Version Update Notes\nDESCRIPTION: Lists of dependency updates and bugfixes for Telegraf project across versions v1.30.2 and v1.30.3.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n### Dependency Updates\n\n- [#15385] `deps` Bump cloud.google.com/go/storage from 1.40.0 to 1.41.0\n- [#15446] `deps` Bump github.com/awnumar/memguard from 0.22.4 to 0.22.5\n[...additional dependency updates...]\n\n## v1.30.3 [2024-05-20]\n\n### Bugfixes\n\n- [#15213] `http` Stop plugins from leaking file descriptors on telegraf reload\n- [#15312] `input.redis` Discard invalid errorstat lines\n[...additional bugfixes...]\n\n## v1.30.2 [2024-04-22]\n\n### Important Changes\n\n- [PR #15108] reverts the behavior of `inputs.systemd_units`\n[...additional changes...]\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Syslog Message Format Example\nDESCRIPTION: Example of an RFC5424 formatted syslog message with priority value <163>, timestamp, hostname, application name (which exceeds the 48 character limit), process ID, message ID, and the message content about a failed 'su root' attempt.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_strict_toolong_appname/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<163>1 2003-10-11T22:14:15.003Z mymachine.example.com ALongApplicationNameContainingMoreThanTheAllowed48Character - ID47 - BOM'su root' failed for lonvick on /dev/pts/8\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status in Plaintext\nDESCRIPTION: This snippet shows the output of an NTP server status check. It includes information about the remote server, reference ID, stratum, type, poll interval, reach, delay, offset, and jitter.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/hours/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  2h  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Modification in Metrics using Telegraf Split Plugin\nDESCRIPTION: This diff example illustrates the transformation of a single metric containing multiple data points into separate sensor-specific metrics, showing the output of the split process. Dependencies: Correct configuration of the Split Processor Plugin in Telegraf. The input is a combined metric line, and the output consists of individual metrics per sensor, with timestamps preserved.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/split/README.md#2025-04-16_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n-metric,status=active sensor1_channel1=4i,sensor1_channel2=2i,sensor2_channel1=1i,sensor2_channel2=2i 1684784689000000000\n+sensor1,status=active sensor1_channel1=4i,sensor1_channel2=2i 1684784689000000000\n+sensor2,status=active sensor2_channel1=1i,sensor2_channel2=2i 1684784689000000000\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Formatted Syslog Message Example for Telegraf\nDESCRIPTION: An example of a syslog message in RFC5424 format with priority value, timestamp, hostname, application name, process ID, message ID, structured data, and message. This format is commonly used for log collection and processing in monitoring systems like Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_strict_unix/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<29>1 2016-02-21T04:32:57+00:00 web1 someservice 2341 2 [origin][meta sequence=\"14125553\" service=\"someservice\"] \"GET /v1/ok HTTP/1.1\" 200 145 \"-\" \"hacheck 0.9.0\" 24306 127.0.0.1:40124 575\n```\n\n----------------------------------------\n\nTITLE: Configuring Net Response Input in Telegraf (TOML)\nDESCRIPTION: Example configuration for the net_response input plugin in Telegraf that monitors TCP connectivity to example.com:80.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/zabbix/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.net_response]]\n  protocol = \"tcp\"\n  address = \"example.com:80\"\n```\n\n----------------------------------------\n\nTITLE: Redundant Clean Function Example in Telegraf\nDESCRIPTION: Shows how clean is automatically invoked with certain other functions and demonstrates equivalent configurations to avoid redundant operations.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n   [[processors.filepath.dir]]\n     tag = \"path\"\n   [[processors.filepath.clean]]\n     tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Email Contact Information in Markdown\nDESCRIPTION: Contact email address for reporting code of conduct violations to community leaders.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CODE_OF_CONDUCT.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`community@influxdata.com`\n```\n\n----------------------------------------\n\nTITLE: Checking Telegraf Version\nDESCRIPTION: Shows how to verify the version of the Telegraf binary using the version subcommand. This is useful when troubleshooting or ensuring compatibility.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/cmd/telegraf/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntelegraf version\n```\n\n----------------------------------------\n\nTITLE: Socket State Table Format\nDESCRIPTION: Display format for socket connection states showing State, Recv-Q, Send-Q, Local Address:Port and Peer Address:Port columns. Used for displaying active network socket information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/socketstat/testdata/tcp_no_sockets.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nState      Recv-Q Send-Q       Local Address:Port                  Peer Address:Port\n```\n\n----------------------------------------\n\nTITLE: Testing Timestream Plugin\nDESCRIPTION: Command to execute unit tests for the Timestream output plugin\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/timestream/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngo test -v ./plugins/outputs/timestream/...\n```\n\n----------------------------------------\n\nTITLE: Memory Lock Error Message\nDESCRIPTION: Error message shown when Telegraf fails to acquire memory lock due to insufficient resources\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/DOCKER.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\npanic: could not acquire lock on 0x7f7a8890f000, limit reached? [Err: cannot allocate memory]\n```\n\n----------------------------------------\n\nTITLE: Example LeoManager Metrics Output Format\nDESCRIPTION: Sample output of collected LeoManager metrics showing Erlang VM and system performance statistics with timestamp and multiple metric tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/leofs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nleofs,host=manager_0,node=manager_0@127.0.0.1 allocated_memory=78255445,allocated_memory_5min=78159025,ets_memory_usage=4611900,ets_memory_usage_5min=4632599,num_of_processes=223,num_of_processes_5min=223,processes_memory_usage=20201316,processes_memory_usage_5min=20186559,system_memory_usage=37172701,system_memory_usage_5min=37189213,total_memory_usage=57373373,total_memory_usage_5min=57374653,used_allocated_memory=67,used_allocated_memory_5min=67 1524105758000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Lookup Processor Plugin in TOML\nDESCRIPTION: This TOML configuration snippet demonstrates setting up the SNMP Lookup Processor Plugin in Telegraf. It provides options for SNMP version, authentication, and caching tags. Key parameters include 'agent_tag' for the SNMP agent, 'index_tag' for the table row index, and 'timeout' for request handling. Dependencies include SNMP agents and network access, with SNMPv3 support for secure communication. Outputs are additional tags added to the metrics, which can be cached for a predefined TTL.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/snmp_lookup/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Lookup extra tags via SNMP based on the table index\n[[processors.snmp_lookup]]\n  ## Name of tag of the SNMP agent to do the lookup on\n  # agent_tag = \"source\"\n\n  ## Name of tag holding the table row index\n  # index_tag = \"index\"\n\n  ## Timeout for each request.\n  # timeout = \"5s\"\n\n  ## SNMP version; can be 1, 2, or 3.\n  # version = 2\n\n  ## SNMP community string.\n  # community = \"public\"\n\n  ## Number of retries to attempt.\n  # retries = 3\n\n  ## The GETBULK max-repetitions parameter.\n  # max_repetitions = 10\n\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\", or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Context Name.\n  # context_name = \"\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n\n  ## The maximum number of SNMP requests to make at the same time.\n  # max_parallel_lookups = 16\n\n  ## The amount of agents to cache entries for. If limit is reached, \n  ## oldest will be removed first. 0 means no limit.\n  # max_cache_entries = 100\n\n  ## Control whether the metrics need to stay in the same order this plugin\n  ## received them in. If false, this plugin may change the order when data is\n  ## cached. If you need metrics to stay in order set this to true. Keeping the\n  ## metrics ordered may be slightly slower.\n  # ordered = false\n\n  ## The amount of time entries are cached for a given agent. After this period\n  ## elapses if tags are needed they will be retrieved again.\n  # cache_ttl = \"8h\"\n\n  ## Minimum time between requests to an agent in case an index could not be\n  ## resolved. If set to zero no request on missing indices will be triggered.\n  # min_time_between_updates = \"5m\"\n\n  ## List of tags to be looked up.\n  [[processors.snmp_lookup.tag]]\n    ## Object identifier of the variable as a numeric or textual OID.\n    oid = \"IF-MIB::ifName\"\n\n    ## Name of the tag to create.  If not specified, it defaults to the value of 'oid'.\n    ## If 'oid' is numeric, an attempt to translate the numeric OID into a textual OID\n    ## will be made.\n    # name = \"\"\n\n    ## Apply one of the following conversions to the variable value:\n    ##   ipaddr:      Convert the value to an IP address.\n    ##   enum:        Convert the value according to its syntax in the MIB.\n    ##   displayhint: Format the value according to the textual convention in the MIB.\n    ##\n    # conversion = \"\"\n```\n\n----------------------------------------\n\nTITLE: Verifying TPM2 Availability for Systemd Credentials\nDESCRIPTION: Shell command to check if TPM2 is available on the system for enhanced credential protection via hardware-based security, along with example output showing TPM2 status.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/secretstores/systemd/README.md#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemd-creds has-tpm2\n```\n\n----------------------------------------\n\nTITLE: Organizing Imports with goimports\nDESCRIPTION: While not mandatory, the use of goimports is highly recommended for automatically ordering imports in Go files. This tool helps maintain a consistent import structure throughout the project.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/CODE_STYLE.md#2025-04-16_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ngoimports\n```\n\n----------------------------------------\n\nTITLE: JSON Format for Sending Metrics to Nebius Monitoring\nDESCRIPTION: This snippet demonstrates the JSON format used to send metrics to the Nebius Monitoring backend. It includes the metric name, labels, timestamp, and value.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/nebius_cloud_monitoring/README.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"metric_name\",\n  \"labels\": {\n    \"key\": \"value\",\n    \"foo\": \"bar\"\n  },\n  \"ts\": \"2023-06-06T11:10:50Z\",\n  \"value\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Practical JSON Lookup Table Example\nDESCRIPTION: A complete JSON lookup table example showing how host-based keys map to location and rack information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"xyzzy-green\": {\n    \"location\": \"eu-central\",\n    \"rack\": \"C12-01\"\n  },\n  \"xyzzy-red\": {\n    \"location\": \"us-west\",\n    \"rack\": \"C01-42\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Insufficient Memory Lock Warning Message\nDESCRIPTION: Warning message displayed when Docker container has insufficient lockable memory for Telegraf operations\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/DOCKER.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nW! Insufficient lockable memory 64kb when 72kb is required. Please increase the limit for Telegraf in your Operating System!\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Telegraf Specification File Naming in Markdown\nDESCRIPTION: This snippet shows examples of how to name Telegraf specification files using the 'tsd' prefix followed by a number and descriptive title.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/specs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* tsd-001-agent-write-ahead-log.md\n* tsd-002-inputs-apache-increase-timeout.md\n* tsd-003-serializers-parquet.md\n```\n\n----------------------------------------\n\nTITLE: Configuring uWSGI Input Plugin in Telegraf\nDESCRIPTION: This code snippet demonstrates the basic configuration for the uWSGI input plugin in Telegraf.  It defines the server URLs to connect to and retrieve metrics from the uWSGI Stats Server.  It also includes a general connection timeout setting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/uwsgi/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n\n# Read uWSGI metrics.\n[[inputs.uwsgi]]\n  ## List with urls of uWSGI Stats servers. Url must match pattern:\n  ## scheme://address[:port]\n  ##\n  ## For example:\n  ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n  servers = [\"tcp://127.0.0.1:1717\"]\n\n  ## General connection timeout\n  # timeout = \"5s\"\n\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf with DiskIO Plugin in Docker\nDESCRIPTION: Shell command for running Telegraf in a Docker container with access to the host's disk I/O metrics. This requires mounting the host filesystem and using privileged mode for device access.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/diskio/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --privileged -v /:/hostfs:ro -v /run/udev:/run/udev:ro -e HOST_PROC=/hostfs/proc telegraf\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Structure for MessagePack Metrics\nDESCRIPTION: This snippet defines the JSON structure for metrics that will be serialized using MessagePack. It includes example fields and their expected types, emphasizing the importance of following the specified schema for proper serialization.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/msgpack/README.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"name\":\"cpu\",\n   \"time\": <TIMESTAMP>, // https://github.com/msgpack/msgpack/blob/master/spec.md#timestamp-extension-type\n   \"tags\":{\n      \"tag_1\":\"host01\",\n      ...\n   },\n   \"fields\":{\n      \"field_1\":30,\n      \"field_2\":true,\n      \"field_3\":\"field_value\",\n      \"field_4\":30.1,\n      ...\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Hex-encoded Inputs in Binary Parser\nDESCRIPTION: Allows the binary parser to handle hex-encoded inputs.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_11\n\nLANGUAGE: go\nCODE:\n```\n[[parsers.binary]]\n  hex_encoding = true\n```\n\n----------------------------------------\n\nTITLE: Example Output Format for Windows Services Monitoring\nDESCRIPTION: This text snippet shows the expected output format for the metrics collected by the Windows services input plugin. Each entry includes the service name, the display name, and service state along with the host information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_services/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nwin_services,host=WIN2008R2H401,display_name=Server,service_name=LanmanServer state=4i,startup_mode=2i 1500040669000000000\nwin_services,display_name=Remote\\ Desktop\\ Services,service_name=TermService,host=WIN2008R2H401 state=1i,startup_mode=3i 1500040669000000000\n```\n\n----------------------------------------\n\nTITLE: Analyzing Memory Profile with Go Tool\nDESCRIPTION: Use the Go pprof tool to analyze the collected memory profile, showing the top 5 memory consumers.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PROFILING.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ go tool pprof mem.prof\n(pprof) top5\n```\n\n----------------------------------------\n\nTITLE: Configuring Kernel VM Statistics Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet sets up the kernel_vmstat input plugin for Telegraf. The plugin doesn't require any specific configuration and is only supported on Linux systems.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/kernel_vmstat/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Get kernel statistics from /proc/vmstat\n# This plugin ONLY supports Linux\n[[inputs.kernel_vmstat]]\n  # no configuration\n```\n\n----------------------------------------\n\nTITLE: Incorrect Telegraf Service Installation\nDESCRIPTION: Incorrect way to specify the configuration file that will likely cause service startup failure with error #1067.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/WINDOWS_SERVICE.md#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n> \"C:\\Program Files\\Telegraf\\telegraf.exe\" --config \"telegraf.conf\" service install\n```\n\n----------------------------------------\n\nTITLE: Configuring Ruby Daemon in Toml\nDESCRIPTION: This TOML snippet configures the execd processor plugin to run a Ruby script as a daemon, specified by the command array in the configuration.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/execd/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.execd]]\n  command = [\"ruby\", \"plugins/processors/execd/examples/multiplier_line_protocol/multiplier_line_protocol.rb\"]\n```\n\n----------------------------------------\n\nTITLE: Change Category Headers in Markdown\nDESCRIPTION: Section headers used to organize different types of changes\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n### Features\n### Bug Fixes\n### Release Notes\n```\n\n----------------------------------------\n\nTITLE: Maximum Length RFC5424 Syslog Message Format Example\nDESCRIPTION: This is an example of a RFC5424 formatted syslog message with all fields at or near their maximum allowed lengths. The message contains a priority value, timestamp, hostname, application name, process ID, message ID, and a very long message content filled with repeated 'l' characters.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc5424_strict_udp_max/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<191>999 2017-12-31T23:59:59.999999+00:00 abcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabc abcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdef abcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzabcdefghilmnopqrstuvzab abcdefghilmnopqrstuvzabcdefghilm - lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n```\n\n----------------------------------------\n\nTITLE: Documenting Bug Fixes and New Features in Markdown\nDESCRIPTION: Changelog entries documenting bug fixes and new features in Telegraf project, organized by version numbers and categories like inputs, processors, aggregators etc.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n### Bug Fixes\n\n- [#4873](https://github.com/influxdata/telegraf/pull/4873): Add DN attributes as tags in x509_cert input to avoid series overwrite.\n- [#4921](https://github.com/influxdata/telegraf/issues/4921): Prevent connection leak by closing unused connections in amqp output.\n- [#4904](https://github.com/influxdata/telegraf/issues/4904): Use default partition key when tag does not exist in kinesis output.\n- [#4901](https://github.com/influxdata/telegraf/pull/4901): Log the correct error in jti_openconfig.\n- [#4937](https://github.com/influxdata/telegraf/pull/4937): Handle panic when ipmi_sensor input gets bad input.\n- [#4930](https://github.com/influxdata/telegraf/pull/4930): Don't add unserializable fields to jolokia2 input.\n- [#4866](https://github.com/influxdata/telegraf/pull/4866): Fix version check in postgresql_extensible.\n```\n\n----------------------------------------\n\nTITLE: CSV Key-Values Format Example for Lookup Tables\nDESCRIPTION: Example of the CSV format with keys and values. The first line contains the header with tag names, and subsequent lines contain the key followed by values for each tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/lookup/README.md#2025-04-16_snippet_3\n\nLANGUAGE: csv\nCODE:\n```\n# Optional comments\nignored,tag-name1,...,tag-valueN\nkeyA,tag-value1,...,,,,\nkeyB,tag-value1,,,,...,\n...\nkeyZ,tag-value1,...,tag-valueM,...,\n```\n\n----------------------------------------\n\nTITLE: PgBouncer Metrics Output Example\nDESCRIPTION: Example of the metrics output format showing various PgBouncer statistics, pool states, and database information.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pgbouncer/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\npgbouncer,db=pgbouncer,server=host\\=debian-buster-postgres\\ user\\=dbn\\ port\\=6432\\ dbname\\=pgbouncer\\  avg_query_count=0i,avg_query_time=0i,avg_wait_time=0i,avg_xact_count=0i,avg_xact_time=0i,total_query_count=26i,total_query_time=0i,total_received=0i,total_sent=0i,total_wait_time=0i,total_xact_count=26i,total_xact_time=0i 1581569936000000000\npgbouncer_pools,db=pgbouncer,pool_mode=statement,server=host\\=debian-buster-postgres\\ user\\=dbn\\ port\\=6432\\ dbname\\=pgbouncer\\ ,user=pgbouncer cl_active=1i,cl_waiting=0i,maxwait=0i,maxwait_us=0i,sv_active=0i,sv_idle=0i,sv_login=0i,sv_tested=0i,sv_used=0i 1581569936000000000\npgbouncer_lists,db=pgbouncer,server=host\\=debian-buster-postgres\\ user\\=dbn\\ port\\=6432\\ dbname\\=pgbouncer\\ ,user=pgbouncer databases=1i,dns_names=0i,dns_queries=0i,dns_zones=0i,free_clients=47i,free_servers=0i,login_clients=0i,pools=1i,used_clients=3i,used_servers=0i,users=4i 1581569936000000000\npgbouncer_databases,db=pgbouncer,pg_dbname=pgbouncer,server=host\\=debian-buster-postgres\\ user\\=dbn\\ port\\=6432\\ dbname\\=pgbouncer\\ name=pgbouncer disabled=0i,pool_size=2i,current_connections=0i,min_pool_size=0i,reserve_pool=0i,max_connections=0i,paused=0i 1581569936000000000\npgbouncer_databases,db=postgres,pg_dbname=postgres,server=host\\=debian-buster-postgres\\ user\\=dbn\\ port\\=6432\\ dbname\\=pgbouncer\\ name=postgres current_connections=0i,disabled=0i,pool_size=20i,min_pool_size=0i,reserve_pool=0i,paused=0i,max_connections=0i 1581569936000000000\n```\n\n----------------------------------------\n\nTITLE: Formatting a Syslog Entry with Priority 23 in Plaintext\nDESCRIPTION: This is an example of a Syslog format log entry with priority value 23 (facility 2, severity 7). The entry uses the Syslog protocol format with version 1, and includes a message 'hell' with a non-ASCII character.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_strict_tcp_1st_utf8_ok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n23 <1>1 - - - - - - hell\n```\n\n----------------------------------------\n\nTITLE: Example Output of PHP-FPM Metrics in Line Protocol Format\nDESCRIPTION: Sample output of PHP-FPM metrics collected by Telegraf, shown in InfluxDB line protocol format. It includes metrics for different PHP-FPM pools with various statistics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/phpfpm/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nphpfpm,pool=www accepted_conn=13i,active_processes=2i,idle_processes=1i,listen_queue=0i,listen_queue_len=0i,max_active_processes=2i,max_children_reached=0i,max_listen_queue=0i,slow_requests=0i,total_processes=3i 1453011293083331187\nphpfpm,pool=www2 accepted_conn=12i,active_processes=1i,idle_processes=2i,listen_queue=0i,listen_queue_len=0i,max_active_processes=2i,max_children_reached=0i,max_listen_queue=0i,slow_requests=0i,total_processes=3i 1453011293083691422\nphpfpm,pool=www3 accepted_conn=11i,active_processes=1i,idle_processes=2i,listen_queue=0i,listen_queue_len=0i,max_active_processes=2i,max_children_reached=0i,max_listen_queue=0i,slow_requests=0i,total_processes=3i 1453011293083691658\n```\n\n----------------------------------------\n\nTITLE: Executing Integration Tests in Telegraf\nDESCRIPTION: Commands for running integration tests either standalone or as part of the full test suite.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake test-integration\n```\n\nLANGUAGE: shell\nCODE:\n```\nmake test-all\n```\n\n----------------------------------------\n\nTITLE: Example Input Data for Prometheus Metrics\nDESCRIPTION: This text snippet represents a sample input dataset for CPU metrics. It includes fields and tags formatted in a way that Telegraf can process into Prometheus metrics. The primary goal is to provide an example data structure that aligns with Prometheus conventions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/prometheus/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncpu,cpu=cpu0 time_guest=8022.6,time_system=26145.98,time_user=92512.89 1574317740000000000\ncpu,cpu=cpu1 time_guest=8097.88,time_system=25223.35,time_user=96519.58 1574317740000000000\ncpu,cpu=cpu2 time_guest=7386.28,time_system=24870.37,time_user=95631.59 1574317740000000000\ncpu,cpu=cpu3 time_guest=7434.19,time_system=24843.71,time_user=93753.88 1574317740000000000\n```\n\n----------------------------------------\n\nTITLE: Syslog Message Format with Priority and Components\nDESCRIPTION: A standard syslog message with priority value 13 (notice facility, user-level messages), timestamp 'Dec 2 16:31:03', host identifier 'host', application name 'app', and the message content 'Test'.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/rfc3164_strict_udp/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: syslog\nCODE:\n```\n<13>Dec  2 16:31:03 host app: Test\n```\n\n----------------------------------------\n\nTITLE: Recording HTTP Router Latency in Prometheus Format\nDESCRIPTION: This snippet records latency metrics for two API versions (v1 and v2) in a Prometheus-compatible format, providing the sum, count, and quantiles for GET and POST methods. Each metric captures key performance indicators for HTTP requests to the specified paths.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/multiple/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# TYPE acme_http_router_request_seconds summary\n# UNIT acme_http_router_request_seconds seconds\n# HELP acme_http_router_request_seconds Latency though all of ACME's HTTP request router.\nacme_http_router_request_seconds_sum{path=\"/api/v1\",method=\"GET\"} 9036.32\nacme_http_router_request_seconds_count{path=\"/api/v1\",method=\"GET\"} 807283.0\nacme_http_router_request_seconds{path=\"/api/v1\",method=\"GET\",quantile=\"0.5\"} 1.29854\nacme_http_router_request_seconds{path=\"/api/v1\",method=\"GET\",quantile=\"0.9\"} 54.85479\nacme_http_router_request_seconds{path=\"/api/v1\",method=\"GET\",quantile=\"0.99\"} 6884.32324\nacme_http_router_request_seconds_created{path=\"/api/v1\",method=\"GET\"} 1605281325.0\nacme_http_router_request_seconds_sum{path=\"/api/v2\",method=\"POST\"} 479.3\nacme_http_router_request_seconds_count{path=\"/api/v2\",method=\"POST\"} 34.0\nacme_http_router_request_seconds_created{path=\"/api/v2\",method=\"POST\"} 1605281325.0\nacme_http_router_request_seconds{path=\"/api/v2\",method=\"POST\",quantile=\"0.5\"} 0.85412\nacme_http_router_request_seconds{path=\"/api/v2\",method=\"POST\",quantile=\"0.9\"} 1.15429\nacme_http_router_request_seconds{path=\"/api/v2\",method=\"POST\",quantile=\"0.99\"} 3698.48132\n```\n\n----------------------------------------\n\nTITLE: Syslog Priority Value Format Example\nDESCRIPTION: A simple example showing a Syslog priority value format. In Syslog protocol, priority values are often enclosed in angle brackets at the beginning of a message, indicating facility and severity levels.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/testcases/octet_counting_best_effort_tcp_1st_underflow_nok/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n16 <1>2\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Tags with Inline Table Syntax in TOML\nDESCRIPTION: Alternative configuration example showing how to add custom tags to the CPU input plugin using TOML's inline table syntax, which allows for more compact tag definitions.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  tags = {tag1 = \"foo\", tag2 = \"bar\"}\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: Generating JVM Memory Pool Metrics Output - Text Format\nDESCRIPTION: This snippet shows example output data for JVM memory pools in a specific text format. Each line represents metrics for different memory pools, including attributes like PeakUsage and Usage along with their values and timestamps. The data is formatted to be easily parsable by monitoring tools.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/jolokia2_agent/README.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=Compressed\\ Class\\ Space PeakUsage.max=1073741824,PeakUsage.committed=3145728,PeakUsage.init=0,Usage.committed=3145728,Usage.init=0,PeakUsage.used=3017976,Usage.max=1073741824,Usage.used=3017976 1503764025000000000\n```\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=Code\\ Cache PeakUsage.init=2555904,PeakUsage.committed=6291456,Usage.committed=6291456,PeakUsage.used=6202752,PeakUsage.max=251658240,Usage.used=6210368,Usage.max=251658240,Usage.init=2555904 1503764025000000000\n```\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=G1\\ Eden\\ Space CollectionUsage.max=-1,PeakUsage.committed=56623104,PeakUsage.init=56623104,PeakUsage.used=53477376,Usage.max=-1,Usage.committed=49283072,Usage.used=19922944,CollectionUsage.committed=49283072,CollectionUsage.init=56623104,CollectionUsage.used=0,PeakUsage.max=-1,Usage.init=56623104 1503764025000000000\n```\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=G1\\ Old\\ Gen CollectionUsage.max=1073741824,CollectionUsage.committed=0,PeakUsage.max=1073741824,PeakUsage.committed=1017118720,PeakUsage.init=1017118720,PeakUsage.used=137032208,Usage.max=1073741824,CollectionUsage.init=1017118720,Usage.committed=1017118720,Usage.init=1017118720,Usage.used=134708752,CollectionUsage.used=0 1503764025000000000\n```\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=G1\\ Survivor\\ Space Usage.max=-1,Usage.init=0,CollectionUsage.max=-1,CollectionUsage.committed=7340032,CollectionUsage.used=7340032,PeakUsage.committed=7340032,Usage.committed=7340032,Usage.used=7340032,CollectionUsage.init=0,PeakUsage.max=-1,PeakUsage.init=0,PeakUsage.used=7340032 1503764025000000000\n```\n\nLANGUAGE: text\nCODE:\n```\njvm_memory_pool,pool_name=Metaspace PeakUsage.init=0,PeakUsage.used=21852224,PeakUsage.max=-1,Usage.max=-1,Usage.committed=22282240,Usage.init=0,Usage.used=21852224,PeakUsage.committed=22282240 1503764025000000000\n```\n\n----------------------------------------\n\nTITLE: Template Transformation Example\nDESCRIPTION: Shows how a statsd metric is transformed using the configured template, converting from statsd format to InfluxDB line protocol.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncpu.load.us-west:100|g\n=> cpu_load,region=us-west 100\n```\n\n----------------------------------------\n\nTITLE: Redis Connected Clients Metrics Format\nDESCRIPTION: Prometheus metrics format showing the number of connected clients across different Redis instances. The metrics include instance name and port labels with corresponding client count values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/valid_unknown/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP redis_connected_clients Redis connected clients\n# TYPE redis_connected_clients unknown\nredis_connected_clients{instance=\"rough-snowflake-web\",port=\"6380\"} 10.0\nredis_connected_clients{instance=\"rough-snowflake-web\",port=\"6381\"} 12.0\n# EOF\n```\n\n----------------------------------------\n\nTITLE: Input Sample - Plain Text\nDESCRIPTION: This snippet provides a sample input text that simulates a syslog entry with various fields such as 'facility', 'hostname', 'severity', etc. It is used to demonstrate the parsing capabilities of the Telegraf Parser Processor Plugin when configured correctly.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/parser/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsyslog,appname=influxd,facility=daemon,hostname=http://influxdb.example.org\\ (influxdb.example.org),severity=info facility_code=3i,message=\" ts=2018-08-09T21:01:48.137963Z lvl=info msg=\\\"Executing query\\\" log_id=09p7QbOG000 service=query query=\\\"SHOW DATABASES\\\"\",procid=\"6629\",severity_code=6i,timestamp=1533848508138040000i,version=1i\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Sudo Privileges for OpenNTPD Plugin in TOML\nDESCRIPTION: This TOML configuration snippet shows how to enable sudo usage for the OpenNTPD input plugin. This is necessary if the plugin requires elevated permissions to execute the ntpctl command.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/openntpd/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.openntpd]]\n  use_sudo = true\n```\n\n----------------------------------------\n\nTITLE: Process and Dovecot Build Metrics\nDESCRIPTION: Basic metrics showing process start time and Dovecot build information including version and revision.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/openmetrics/testcases/dovecot/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP process_start_time_seconds Timestamp of service start\n# TYPE process_start_time_seconds gauge\nprocess_start_time_seconds 1606393397\n# HELP dovecot_build Dovecot build information\n# TYPE dovecot_build info\ndovecot_build_info{version=\"2.4.devel\",revision=\"38ecc424a\"} 1\n```\n\n----------------------------------------\n\nTITLE: Running Telegraf in Docker with Host Monitoring Capabilities\nDESCRIPTION: Docker command for running Telegraf with the necessary volume mounts and environment variables to monitor the Docker Engine Host from within a container. This setup provides access to host filesystems for proper monitoring.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/FAQ.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --name telegraf \\\n    -v /:/hostfs:ro \\\n    -e HOST_ETC=/hostfs/etc \\\n    -e HOST_PROC=/hostfs/proc \\\n    -e HOST_SYS=/hostfs/sys \\\n    -e HOST_VAR=/hostfs/var \\\n    -e HOST_RUN=/hostfs/run \\\n    -e HOST_MOUNT_PREFIX=/hostfs \\\n    telegraf\n```\n\n----------------------------------------\n\nTITLE: Version Header in Markdown\nDESCRIPTION: Version number and release date formatting used in the changelog\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n## v0.1.8 [2015-09-04]\n```\n\n----------------------------------------\n\nTITLE: Configuring TopK Processor Plugin in TOML\nDESCRIPTION: Sample configuration for TopK processor with options to set aggregation period, top K buckets, grouping tags, fields, and aggregation function\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/topk/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.topk]]\n  ## How many seconds between aggregations\n  # period = 10\n\n  ## How many top buckets to return per field\n  # k = 10\n\n  ## Over which tags should the aggregation be done\n  # group_by = ['*']\n\n  ## The field(s) to aggregate\n  # fields = [\"value\"]\n\n  ## What aggregation function to use\n  # aggregation = \"mean\"\n\n  ## Instead of the top k largest metrics, return the bottom k lowest metrics\n  # bottomk = false\n\n  ## Add a tag with the calculated GroupBy tag\n  # add_groupby_tag = \"\"\n\n  ## Add rank fields for specified fields\n  # add_rank_fields = []\n\n  ## Add aggregate fields for specified fields\n  # add_aggregate_fields = []\n```\n\n----------------------------------------\n\nTITLE: Example Output of Energy Flow Metrics from Neoom Beaam\nDESCRIPTION: Sample output of energy flow metrics collected by the Neoom Beaam Input Plugin. It shows various datapoints like self-sufficiency, power production, grid status, and battery state with their respective units and timestamps.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/neoom_beaam/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nneoom_beaam_energy_flow,datapoint=SELF_SUFFICIENCY,source=127.0.0.1,unit=% value=100 1723883145024999936\nneoom_beaam_energy_flow,datapoint=POWER_PRODUCTION,source=127.0.0.1,unit=W value=1184 1723883150001999872\nneoom_beaam_energy_flow,datapoint=POWER_GRID,source=127.0.0.1,unit=W value=15 1723883150001999872\nneoom_beaam_energy_flow,datapoint=POWER_STORAGE,source=127.0.0.1,unit=W value=3504 1723883150001999872\nneoom_beaam_energy_flow,datapoint=ENERGY_PRODUCED,source=127.0.0.1,unit=Wh value=639300 1723882905007000064\nneoom_beaam_energy_flow,datapoint=ENERGY_IMPORTED,source=127.0.0.1,unit=Wh value=22696.926152777138 1723883150001999872\nneoom_beaam_energy_flow,datapoint=ENERGY_EXPORTED,source=127.0.0.1,unit=Wh value=237421.33112499933 1723883135003000064\nneoom_beaam_energy_flow,datapoint=ENERGY_CHARGED,source=127.0.0.1,unit=Wh value=215200 1723882765001999872\nneoom_beaam_energy_flow,datapoint=ENERGY_DISCHARGED,source=127.0.0.1,unit=Wh value=144800 1723880490004999936\nneoom_beaam_energy_flow,datapoint=STATE_OF_CHARGE,source=127.0.0.1,unit=% value=59 1723883090001999872\nneoom_beaam_energy_flow,datapoint=POWER_CONSUMPTION_CALC,source=127.0.0.1,unit=W value=4703 1723883150001999872\nneoom_beaam_energy_flow,datapoint=ENERGY_CONSUMED_CALC,source=127.0.0.1,unit=Wh value=354175.59502777783 1723883150001999872\n```\n\n----------------------------------------\n\nTITLE: Cluster Metrics Error Message\nDESCRIPTION: Sample error message when cluster metric queries exceed the configured maxQueryMetrics limit.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/README.md#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n2018-11-02T13:37:11Z E! Error in plugin [inputs.vsphere]: ServerFaultCode: This operation is restricted by the administrator - 'vpxd.stats.maxQueryMetrics'. Contact your system administrator\n```\n\n----------------------------------------\n\nTITLE: Configuring Final Aggregator Plugin in TOML\nDESCRIPTION: Configuration settings for the Final Aggregator plugin including period, drop_original, keep_original_field_names, series_timeout, and output_strategy options.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/final/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[[aggregators.final]]\n  ## The period on which to flush & clear the aggregator.\n  # period = \"30s\"\n\n  ## If true, the original metric will be dropped by the\n  ## aggregator and will not get sent to the output plugins.\n  # drop_original = false\n\n  ## If false, _final is added to every field name\n  # keep_original_field_names = false\n\n  ## The time that a series is not updated until considering it final. Ignored\n  ## when output_strategy is \"periodic\".\n  # series_timeout = \"5m\"\n\n  ## Output strategy, supported values:\n  ##   timeout  -- output a metric if no new input arrived for `series_timeout`\n  ##   periodic -- output the last received metric every `period`\n  # output_strategy = \"timeout\"\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Command for ROCm SMI\nDESCRIPTION: Linux shell command to get detailed GPU information using rocm-smi for troubleshooting purposes. Returns comprehensive GPU metrics in JSON format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/amd_rocm_smi/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nrocm-smi rocm-smi -o -l -m -M  -g -c -t -u -i -f -p -P -s -S -v --showreplaycount --showpids --showdriverversion --showmemvendor --showfwinfo --showproductname --showserial --showuniqueid --showbus --showpendingpages --showpagesinfo --showretiredpages --showunreservablepages --showmemuse --showvoltage --showtopo --showtopoweight --showtopohops --showtopotype --showtoponuma --showmeminfo all --json\n```\n\n----------------------------------------\n\nTITLE: Practical Example with Tail Input and Filepath Processor in Telegraf\nDESCRIPTION: A complete configuration example showing how to use the filepath processor with the tail input plugin to process log execution times and extract the stem of the filename.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n# Performs file path manipulations on tags and fields\n[[inputs.tail]]\n  files = [\"/var/log/myjobs/**.log\"]\n  data_format = \"grok\"\n  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-\"2006-01-02 15:04:05\"} total time execution: %{NUMBER:duration_seconds:int}']\n  name_override = \"myjobs\"\n\n[[processors.filepath]]\n   [[processors.filepath.stem]]\n     tag = \"path\"\n     dest = \"stempath\"\n```\n\n----------------------------------------\n\nTITLE: Cluster Performance Metrics Catalog\nDESCRIPTION: Comprehensive performance metrics for monitoring vSphere cluster resources, including CPU, memory, network, power, and virtual machine operation statistics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/METRICS.md#2025-04-16_snippet_4\n\nLANGUAGE: metrics\nCODE:\n```\ncpu.corecount.contention.average\\ncpu.usage.average\\ncpu.reservedCapacity.average\n```\n\n----------------------------------------\n\nTITLE: Datastore Performance Metrics Catalog\nDESCRIPTION: Performance metrics for monitoring datastore resource usage, including read/write operations, throughput, capacity, and disk utilization\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/vsphere/METRICS.md#2025-04-16_snippet_5\n\nLANGUAGE: metrics\nCODE:\n```\ndatastore.numberReadAveraged.average\\ndatastore.throughput.contention.average\\ndatastore.throughput.usage.average\n```\n\n----------------------------------------\n\nTITLE: ENUM Processor Configuration for String to Numeric Mapping\nDESCRIPTION: This TOML configuration defines an ENUM processor in Telegraf to convert string values of the `health_status` field in the `docker_container_health` metric to numeric values. This is useful because Splunk only supports numeric metric values.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/serializers/splunkmetric/README.md#2025-04-16_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n# splunkmetric does not support string values\n[[processors.enum]]\n  namepass = [\"docker_container_health\"]\n  [[processors.enum.mapping]]\n    ## Name of the field to map\n    field = \"health_status\"\n    [processors.enum.mapping.value_mappings]\n    starting = 0\n    healthy = 1\n    unhealthy = 2\n    none = 3\n```\n\n----------------------------------------\n\nTITLE: Sending a request to Telegraf http_listener_v2 with form-urlencoded data\nDESCRIPTION: This curl command demonstrates how to send a GET request to the configured http_listener_v2 endpoint with form-urlencoded data in the query string. It includes a tag and two fields.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/form_urlencoded/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -i -XGET 'http://localhost:8080/telegraf?tag1=foo&field1=0.42&field2=42'\n```\n\n----------------------------------------\n\nTITLE: Setting a Default Status Code Field in Telegraf using TOML\nDESCRIPTION: This configuration snippet demonstrates how to ensure a status_code field with a default value of \"N/A\" is always present in metrics. This is useful for ensuring consistent field presence in metrics for downstream processing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/defaults/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.defaults]]\n  [processors.defaults.fields]\n    status_code = \"N/A\"\n```\n\n----------------------------------------\n\nTITLE: Adding User-Agent Header in HTTP Response Input\nDESCRIPTION: Adds a User-Agent header to requests in the HTTP response input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_7\n\nLANGUAGE: go\nCODE:\n```\n[[inputs.http_response]]\n  headers = { User-Agent = \"Telegraf\" }\n```\n\n----------------------------------------\n\nTITLE: Renaming Measurement from Tag Value in Telegraf\nDESCRIPTION: Example showing how to use a tag value ('topic') as the measurement name, effectively renaming the measurement, with before and after comparison.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/converter/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.converter]]\n  [processors.converter.tags]\n    measurement = [\"topic\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Proxmox User and Token Permissions\nDESCRIPTION: Shell commands for creating a Proxmox user and API token with appropriate PVEAuditor role permissions required for the Telegraf plugin to access metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/proxmox/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n## Create a influx user with PVEAuditor role\npveum user add influx@pve\npveum acl modify / -role PVEAuditor -user influx@pve\n## Create a token with the PVEAuditor role\npveum user token add influx@pve monitoring -privsep 1\npveum acl modify / -role PVEAuditor -token 'influx@pve!monitoring'\n```\n\n----------------------------------------\n\nTITLE: Event-Based Webhook Point Example\nDESCRIPTION: Demonstrates the structure of an event-based Papertrail webhook point in Telegraf, with multiple tags and fields representing a log event\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/papertrail/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npapertrail,host=myserver.example.com,event=saved_search_name count=1i,source_name=\"abc\",program=\"CROND\",severity=\"Info\",source_id=2i,message=\"message body\",source_ip=\"208.75.57.121\",id=7711561783320576i,facility=\"Cron\",url=\"https://papertrailapp.com/searches/42?centered_on_id=7711561783320576\",search_id=42i 1453248892000000000\n```\n\n----------------------------------------\n\nTITLE: Tomcat Metrics Example Output\nDESCRIPTION: Sample output demonstrating the collected Tomcat metrics including JVM memory, memory pools, and connector statistics with timestamp and tags\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/tomcat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntomcat_jvm_memory,host=N8-MBP free=20014352i,max=127729664i,total=41459712i 1474663361000000000\ntomcat_jvm_memorypool,host=N8-MBP,name=Eden\\ Space,type=Heap\\ memory committed=11534336i,init=2228224i,max=35258368i,used=1941200i 1474663361000000000\ntomcat_jvm_memorypool,host=N8-MBP,name=Survivor\\ Space,type=Heap\\ memory committed=1376256i,init=262144i,max=4390912i,used=1376248i 1474663361000000000\ntomcat_jvm_memorypool,host=N8-MBP,name=Tenured\\ Gen,type=Heap\\ memory committed=28549120i,init=5636096i,max=88080384i,used=18127912i 1474663361000000000\ntomcat_jvm_memorypool,host=N8-MBP,name=Code\\ Cache,type=Non-heap\\ memory committed=6946816i,init=2555904i,max=251658240i,used=6406528i 1474663361000000000\ntomcat_jvm_memorypool,host=N8-MBP,name=Compressed\\ Class\\ Space,type=Non-heap\\ memory committed=1966080i,init=0i,max=1073741824i,used=1816120i 1474663361000000000\ntomcat_jvm_memorypool,host=N8-MBP,name=Metaspace,type=Non-heap\\ memory committed=18219008i,init=0i,max=-1i,used=17559376i 1474663361000000000\ntomcat_connector,host=N8-MBP,name=ajp-bio-8009 bytes_received=0i,bytes_sent=0i,current_thread_count=0i,current_threads_busy=0i,error_count=0i,max_threads=200i,max_time=0i,processing_time=0i,request_count=0i 1474663361000000000\ntomcat_connector,host=N8-MBP,name=http-bio-8080 bytes_received=0i,bytes_sent=86435i,current_thread_count=10i,current_threads_busy=1i,error_count=2i,max_threads=200i,max_time=167i,processing_time=245i,request_count=15i 1474663361000000000\n```\n\n----------------------------------------\n\nTITLE: Docker Configuration for Varnish Monitoring\nDESCRIPTION: TOML configuration for monitoring Varnish instances running in Docker containers\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/varnish/README.md#2025-04-16_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.varnish]]\n  binary = \"/usr/local/bin/docker\"\n  binary_args = [\"exec\", \"-t\", \"container_name\", \"varnishstat\",  \"-j\"]\n  adm_binary   =  \"/usr/local/bin/docker\"\n  adm_binary_args =  [\"exec\", \"-t\", \"container_name\", \"varnishadm\", \"vcl.list\", \"-j\"]\n  metric_version = 2\n  stats = [\"*\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Performance Counter Time Source\nDESCRIPTION: Enables using the performance counter's timestamp instead of the current system time\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/README.md#2025-04-16_snippet_4\n\nLANGUAGE: TOML\nCODE:\n```\nUsePerfCounterTime=true\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Status and Metrics\nDESCRIPTION: Shows NTP server synchronization status including remote server details, poll interval, delay, offset and jitter values. The output is formatted in a tabular structure with alignment for easy reading.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/single_reach_decimal/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  256   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Input Plugin with Name Override in TOML\nDESCRIPTION: Configuration example demonstrating how to use the name_override parameter with the CPU input plugin to emit measurements with the custom name 'foobar' instead of the default name.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md#2025-04-16_snippet_12\n\nLANGUAGE: toml\nCODE:\n```\n[[inputs.cpu]]\n  name_override = \"foobar\"\n  percpu = false\n  totalcpu = true\n```\n\n----------------------------------------\n\nTITLE: Version Release Information\nDESCRIPTION: Version tag and release date information for Telegraf releases.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## v1.31.2 [2024-07-22]\n## v1.31.1 [2024-07-01]\n## v1.31.0 [2024-06-10]\n```\n\n----------------------------------------\n\nTITLE: Syslog Troubleshooting Commands using Netcat\nDESCRIPTION: These shell commands use Netcat to send syslog messages to the Telegraf input plugin. The first command sends a message over TCP with octet framing, while the second sends a message over UDP. These commands are used for testing and troubleshooting the syslog input plugin.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/syslog/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# TCP with octet framing\necho \"57 <13>1 2018-10-01T12:00:00.0Z example.org root - - - test\" | nc 127.0.0.1 6514\n\n# UDP\necho \"<13>1 2018-10-01T12:00:00.0Z example.org root - - - test\" | nc -u 127.0.0.1 6514\n\n```\n\n----------------------------------------\n\nTITLE: Displaying NTP Server Synchronization Status in Plain Text\nDESCRIPTION: Sample output from an NTP server showing synchronization information for a remote time source. The output displays the remote server address, reference ID, status, poll interval, reach, delay, offset, and jitter measurements.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ntpq/testcases/bad_int_parse/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*uschi5-ntp-002. 10.177.80.46     2 u  101  foobar   37   51.016  233.010  17.462\n```\n\n----------------------------------------\n\nTITLE: Example Telegraf Input Metric\nDESCRIPTION: This snippet demonstrates a sample Telegraf input metric that would be processed by the Clarify output plugin. It includes tags and a timestamp.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/outputs/clarify/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntemperature,host=demo.clarifylocal,sensor=TC0P value=49 1682670910000000000\n```\n\n----------------------------------------\n\nTITLE: Appending Tag Values Example in Regex Processor\nDESCRIPTION: Example configuration showing how to append transformed values to existing tags rather than replacing them.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/regex/README.md#2025-04-16_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.regex]]\n  namepass = [\"nginx_requests\"]\n\n  [[processors.regex.tags]]\n    key = \"resp_code\"\n    pattern = '^2\\d\\d$'\n    replacement = \" OK\"\n    result_key = \"verb\"\n    append = true\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Settings for Telegraf in TOML\nDESCRIPTION: This snippet defines global settings for Telegraf, including the agent configuration, output plugins, and input plugins. It specifies intervals, round intervals, metric batches, and various other parameters for data collection and processing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/migrations/inputs_udp_listener/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s.\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  ## Valid time units are \"ns\", \"us\" (or \"s\"), \"ms\", \"s\".\n  precision = \"\"\n\n  ## Log at debug level.\n  # debug = false\n  ## Log only error level messages.\n  # quiet = false\n\n  ## Log target controls the destination for logs and can be one of \"file\",\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\n  ## is determined by the \"logfile\" setting.\n  # logtarget = \"file\"\n\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\n  ## the empty string then logs are written to stderr.\n  # logfile = \"\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  # logfile_rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # logfile_rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  # log_with_timezone = \"\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  omit_hostname = false\n\n  ## Method of translating SNMP objects. Can be \"netsnmp\" which\n  ## translates by calling external programs snmptranslate and snmptable,\n  ## or \"gosmi\" which translates using the built-in gosmi library.\n  # snmp_translator = \"netsnmp\"\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n# Configuration for sending metrics to InfluxDB\n[[outputs.influxdb_v2]]\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://127.0.0.1:8086\"]\n\n  ## Token for authentication.\n  token = \"$INFLUX_TOKEN\"\n\n  ## Organization is the name of the organization you wish to write to; must exist.\n  organization = \"org\"\n\n  ## Destination bucket to write into.\n  bucket = \"bucket\"\n\n  ## The value of this tag will be used to determine the bucket.  If this\n  ## tag is not set the 'bucket' option is used as the default.\n  # bucket_tag = \"\"\n\n  ## If true, the bucket tag will not be added to the metric.\n  # exclude_bucket_tag = false\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## Enable or disable uint support for writing uints influxdb 2.0.\n  # influx_uint_support = false\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP application_type\n  # application_type = \"metrics\"\n\n  ## Optional prefix to add to all measurements.\n  # prefix = \"\"\n\n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n  ## the device. The template may contain variables in the form of '$PROPERTY' or\n  ## '${PROPERTY}'. The first template which does not contain any variables not\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n# Get kernel statistics from /proc/stat\n[[inputs.kernel]]\n  # no configuration\n\n# Read metrics about memory usage\n[[inputs.mem]]\n  # no configuration\n\n# Get the number of processes and group them by status\n[[inputs.processes]]\n  # no configuration\n\n# Read metrics about swap memory usage\n[[inputs.swap]]\n  # no configuration\n\n# Read metrics about system load & uptime\n[[inputs.system]]\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"uptime_format\"]\n\n```\n\n----------------------------------------\n\nTITLE: Example Stem Configuration in Telegraf\nDESCRIPTION: Configuration example showing how to extract the filename without extension from a path tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/filepath/README.md#2025-04-16_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[[processors.filepath]]\n  [[processors.filepath.stem]]\n    tag = \"path\"\n```\n\n----------------------------------------\n\nTITLE: Creating New Hue Bridge User\nDESCRIPTION: Bash command for creating a new user on the Hue Bridge by sending a POST request after pressing the bridge's link button.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/huebridge/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -k -X POST http://<bridge address>/api \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"devicetype\":\"huebridge-telegraf-plugin\"}'\n```\n\n----------------------------------------\n\nTITLE: Defining Token Failure Counter Metric for Prometheus\nDESCRIPTION: Defines a Prometheus counter metric that tracks the number of failed token retrieval attempts from an alternate token source. The metric is initialized to 0 and uses standard Prometheus metric naming and documentation format.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/valid_counter/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP get_token_fail_count Counter of failed Token() requests to the alternate token source\n# TYPE get_token_fail_count counter\nget_token_fail_count 0\n```\n\n----------------------------------------\n\nTITLE: Setting GoDEBUG for X.509 Serial Number Handling in Go\nDESCRIPTION: Sets the GoDEBUG environment variable to handle negative X.509 serial numbers in Go, addressing a potential issue with certificate parsing.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\ngodebug x509negativeserial=1\n```\n\n----------------------------------------\n\nTITLE: ZFS Pool Status Output Parsing\nDESCRIPTION: Single line representation of ZFS storage pool status with key performance and capacity metrics\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/zfs/testcases/freebsd/freebsd14/zpool.txt#2025-04-16_snippet_0\n\nLANGUAGE: zfs\nCODE:\n```\nzroot\tONLINE\t67108864000\t11959578624\t55149285376\t4\t17\t1.00\n```\n\n----------------------------------------\n\nTITLE: Example Output from Beat Input Plugin\nDESCRIPTION: Sample text output showing the collected metrics from Beat input plugin. The output includes metrics for beat, beat_libbeat, beat_system, and beat_filebeat with their respective fields and tags.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/beat/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nbeat,beat_beat=filebeat,beat_host=node-6,beat_id=9c1c8697-acb4-4df0-987d-28197814f788,beat_name=node-6-test,beat_version=6.4.2,host=node-6 cpu_system_ticks=656750,cpu_system_time_ms=656750,cpu_total_ticks=5461190,cpu_total_time_ms=5461198,cpu_total_value=5461190,cpu_user_ticks=4804440,cpu_user_time_ms=4804448,info_uptime_ms=342634196,memstats_gc_next=20199584,memstats_memory_alloc=12547424,memstats_memory_total=486296424792,memstats_rss=72552448 1540316047000000000\nbeat_libbeat,beat_beat=filebeat,beat_host=node-6,beat_id=9c1c8697-acb4-4df0-987d-28197814f788,beat_name=node-6-test,beat_version=6.4.2,host=node-6 config_module_running=0,config_module_starts=0,config_module_stops=0,config_reloads=0,output_events_acked=192404,output_events_active=0,output_events_batches=1607,output_events_dropped=0,output_events_duplicates=0,output_events_failed=0,output_events_total=192404,output_read_bytes=0,output_read_errors=0,output_write_bytes=0,output_write_errors=0,outputs_kafka_bytes_read=1118528,outputs_kafka_bytes_write=48002014,pipeline_clients=1,pipeline_events_active=0,pipeline_events_dropped=0,pipeline_events_failed=0,pipeline_events_filtered=11496,pipeline_events_published=192404,pipeline_events_retry=14,pipeline_events_total=203900,pipeline_queue_acked=192404 1540316047000000000\nbeat_system,beat_beat=filebeat,beat_host=node-6,beat_id=9c1c8697-acb4-4df0-987d-28197814f788,beat_name=node-6-test,beat_version=6.4.2,host=node-6 cpu_cores=32,load_1=46.08,load_15=49.82,load_5=47.88,load_norm_1=1.44,load_norm_15=1.5569,load_norm_5=1.4963 1540316047000000000\nbeat_filebeat,beat_beat=filebeat,beat_host=node-6,beat_id=9c1c8697-acb4-4df0-987d-28197814f788,beat_name=node-6-test,beat_version=6.4.2,host=node-6 events_active=0,events_added=3223,events_done=3223,harvester_closed=0,harvester_open_files=0,harvester_running=0,harvester_skipped=0,harvester_started=0,input_log_files_renamed=0,input_log_files_truncated=0 1540320286000000000\n```\n\n----------------------------------------\n\nTITLE: Copying Built Telegraf Package from Docker Container\nDESCRIPTION: Command to copy the built Telegraf package artifact from the Docker container to the host system. This allows you to retrieve the built package after the build process.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/developers/PACKAGING.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker cp romantic_ptolemy:/go/src/github.com/influxdata/telegraf/build/telegraf-1.10.2-1.x86_64.rpm .\n```\n\n----------------------------------------\n\nTITLE: Updating godirwalk Dependency in Go\nDESCRIPTION: Updates the godirwalk package to version 1.16.1.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_20\n\nLANGUAGE: go\nCODE:\n```\nupdate godirwalk to v1.16.1\n```\n\n----------------------------------------\n\nTITLE: Executing pfctl for PF Status Information\nDESCRIPTION: This shell command demonstrates how to run pfctl to retrieve status information from PF. This is the command that the Telegraf plugin will execute to gather metrics.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pf/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n> pfctl -s info\nStatus: Enabled for 0 days 00:26:05           Debug: Urgent\n\nState Table                          Total             Rate\n  current entries                        2\n  searches                           11325            7.2/s\n  inserts                                5            0.0/s\n  removals                               3            0.0/s\nCounters\n  match                              11226            7.2/s\n  bad-offset                             0            0.0/s\n  fragment                               0            0.0/s\n  short                                  0            0.0/s\n  normalize                              0            0.0/s\n  memory                                 0            0.0/s\n  bad-timestamp                          0            0.0/s\n  congestion                             0            0.0/s\n  ip-option                              0            0.0/s\n  proto-cksum                            0            0.0/s\n  state-mismatch                         0            0.0/s\n  state-insert                           0            0.0/s\n  state-limit                            0            0.0/s\n  src-limit                              0            0.0/s\n  synproxy                               0            0.0/s\n```\n\n----------------------------------------\n\nTITLE: Mapping Rollbar Events to Telegraf Fields in TOML\nDESCRIPTION: This snippet provides the TOML format used to map Rollbar webhook event data to Telegraf fields and tags. It demonstrates how to specify tag keys and field keys for different event types such as 'new_item', 'occurrence', and 'deploy'. Users need to ensure the webhook is pointed to the correct Telegraf service for data to be accurately processed.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/webhooks/rollbar/README.md#2025-04-16_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# TAGS\n* 'tagKey' = `tagValue` type\n# FIELDS\n* 'fieldKey' = `fieldValue` type\n```\n\n----------------------------------------\n\nTITLE: Configuring PF Input Plugin in Telegraf\nDESCRIPTION: This TOML configuration snippet shows how to set up the PF input plugin in Telegraf. It includes an option to use sudo for running pfctl, which is required for accessing PF information on most systems.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/pf/README.md#2025-04-16_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Gather counters from PF\n[[inputs.pf]]\n  ## PF require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n  ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n  ## pfctl can be restricted to only list command \"pfctl -s info\".\n  use_sudo = false\n```\n\n----------------------------------------\n\nTITLE: Example Output of S2 Geo Processor in Telegraf\nDESCRIPTION: A diff showing the transformation produced by the S2 Geo processor. It demonstrates how a metric with latitude and longitude fields is enriched with an S2 cell ID tag.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/processors/s2geo/README.md#2025-04-16_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- mta,area=llir,id=GO505_20_2704,status=1 lat=40.878738,lon=-72.517572 1560540094\n+ mta,area=llir,id=GO505_20_2704,status=1,s2_cell_id=89e8ed4 lat=40.878738,lon=-72.517572 1560540094\n```\n\n----------------------------------------\n\nTITLE: Registering Output Plugin in Telegraf Build System\nDESCRIPTION: Shows how to register a new output plugin in the Telegraf build system using build tags and imports. This code should be placed in plugins/outputs/all directory to make the plugin available within Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/docs/OUTPUTS.md#2025-04-16_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n//go:build !custom || outputs || outputs.simpleoutput\n\npackage all\n\nimport _ \"github.com/influxdata/telegraf/plugins/outputs/simpleoutput\" // register plugin\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Left-Padding for X.509 Certificate Serial Numbers\nDESCRIPTION: Adds configuration option to left-pad X.509 certificate serial numbers to 128 bits in the x509_cert input plugin, ensuring consistent formatting.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\n// Configuration to left-pad serial number to 128-bits\n// Implementation details not provided in the snippet\n```\n\n----------------------------------------\n\nTITLE: Defining Histogram for API Server Request Latencies\nDESCRIPTION: This metric snippet defines a histogram to track response latency distributions for different API requests, categorized by the HTTP verb (e.g., POST), the resource (e.g., bindings), and client. It requires a Prometheus server to scrape and process the data. The snippet includes various latency buckets, the sum, and the count of the requests, allowing detailed performance analysis.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/parsers/prometheus/testcases/valid_histogram/input.txt#2025-04-16_snippet_0\n\nLANGUAGE: Prometheus\nCODE:\n```\n# HELP apiserver_request_latencies Response latency distribution in microseconds for each verb, resource and client.\n# TYPE apiserver_request_latencies histogram\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"125000\"} 1994\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"250000\"} 1997\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"500000\"} 2000\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"1e+06\"} 2005\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"2e+06\"} 2012\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"4e+06\"} 2017\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"8e+06\"} 2024\napiserver_request_latencies_bucket{resource=\"bindings\",verb=\"POST\",le=\"+Inf\"} 2025\napiserver_request_latencies_sum{resource=\"bindings\",verb=\"POST\"} 1.02726334e+08\napiserver_request_latencies_count{resource=\"bindings\",verb=\"POST\"} 2025\n```\n\n----------------------------------------\n\nTITLE: Project References in Markdown\nDESCRIPTION: A formatted list of project references and pull request links documenting changes in the project.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG.md#2025-04-16_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n### Bugfixes\n\n- [#11787](https://github.com/influxdata/telegraf/pull/11787) Clear error message when provided config is not a text file\n- [#11835](https://github.com/influxdata/telegraf/pull/11835) Enable global confirmation for installing mingw\n- [#10797](https://github.com/influxdata/telegraf/pull/10797) `inputs.ceph` Modernize Ceph input plugin metrics\n- [#11785](https://github.com/influxdata/telegraf/pull/11785) `inputs.modbus` Do not fail if a single slave reports errors\n- [#11827](https://github.com/influxdata/telegraf/pull/11827) `inputs.ntpq` Handle pools with \"-\" when\n- [#11825](https://github.com/influxdata/telegraf/pull/11825) `parsers.csv` Remove direct checks for the parser type\n- [#11781](https://github.com/influxdata/telegraf/pull/11781) `parsers.xpath` Add array index when expanding names.\n- [#11815](https://github.com/influxdata/telegraf/pull/11815) `parsers` Memory leak for plugins using ParserFunc.\n- [#11826](https://github.com/influxdata/telegraf/pull/11826) `parsers` Unwrap parser and remove some special handling\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Entry - v1.13.1\nDESCRIPTION: Changelog entry documenting bug fixes for Telegraf version 1.13.1\nSOURCE: https://github.com/influxdata/telegraf/blob/master/CHANGELOG-1.13.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## v1.13.1 [2020-01-08]\n\n### Bug Fixes\n\n- [#6788]: Fix ServerProperty query stops working on Azure after failover.\n- [#6803]: Add leading period to OID in SNMP v1 generic traps.\n- [#6823]: Fix missing config fields in prometheus serializer.\n- [#6694]: Fix panic on connection loss with undelivered messages in mqtt_consumer.\n- [#6679]: Encode query hash fields as hex strings in sqlserver input.\n- [#6345]: Invalidate diskio cache if the metadata mtime has changed.\n- [#6800]: Show platform not supported warning only on plugin creation.\n- [#6814]: Fix rabbitmq cannot complete gather after request error.\n- [#6846]: Fix /sbin/init --version executed on Telegraf startup.\n- [#6847]: Use last path element as field key if path fully specified in cisco_telemetry_gnmi input.\n```\n\n----------------------------------------\n\nTITLE: Example Output from ActiveMQ Telegraf Plugin\nDESCRIPTION: Sample output showing the metrics collected by the ActiveMQ input plugin. Includes queue metrics, topic metrics, and subscriber metrics with their respective tags and fields as formatted by Telegraf.\nSOURCE: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/activemq/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nactivemq_queues,name=sandra,host=88284b2fe51b,source=localhost,port=8161 consumer_count=0i,enqueue_count=0i,dequeue_count=0i,size=0i 1492610703000000000\nactivemq_queues,name=Test,host=88284b2fe51b,source=localhost,port=8161 dequeue_count=0i,size=0i,consumer_count=0i,enqueue_count=0i 1492610703000000000\nactivemq_topics,name=ActiveMQ.Advisory.MasterBroker\\ ,host=88284b2fe51b,source=localhost,port=8161 size=0i,consumer_count=0i,enqueue_count=1i,dequeue_count=0i 1492610703000000000\nactivemq_topics,host=88284b2fe51b,name=AAA\\,source=localhost,port=8161  size=0i,consumer_count=1i,enqueue_count=0i,dequeue_count=0i 1492610703000000000\nactivemq_topics,name=ActiveMQ.Advisory.Topic\\,source=localhost,port=8161 ,host=88284b2fe51b enqueue_count=1i,dequeue_count=0i,size=0i,consumer_count=0i 1492610703000000000\nactivemq_topics,name=ActiveMQ.Advisory.Queue\\,source=localhost,port=8161 ,host=88284b2fe51b size=0i,consumer_count=0i,enqueue_count=2i,dequeue_count=0i 1492610703000000000\nactivemq_topics,name=AAAA\\ ,host=88284b2fe51b,source=localhost,port=8161 consumer_count=0i,enqueue_count=0i,dequeue_count=0i,size=0i 1492610703000000000\nactivemq_subscribers,connection_id=NOTSET,destination_name=AAA,,source=localhost,port=8161,selector=AA,active=no,host=88284b2fe51b,client_id=AAA,subscription_name=AAA pending_queue_size=0i,dispatched_queue_size=0i,dispatched_counter=0i,enqueue_counter=0i,dequeue_counter=0i 1492610703000000000\n```"
  }
]
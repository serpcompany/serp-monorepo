[
  {
    "owner": "openai",
    "repo": "openai-cookbook",
    "content": "TITLE: Creating and Orchestrating an Azure Function for Secure Document Search (JavaScript)\nDESCRIPTION: This code defines an Azure Function that processes HTTP requests to perform secure document search and summarization. It checks the Authorization header, retrieves an OBO authentication token via Azure AD, initializes a Microsoft Graph client, and sends the user-provided searchTerm to the Graph Search API. For each retrieved drive item, it downloads the content, tokenizes it, chunks into 10K-token windows, and summarizes each window using an external helper with OpenAI GPT (getRelevantParts). Results with metadata are ranked and returned in the HTTP response. The function depends on previously defined helpers: getOboToken, initGraphClient, getDriveItemContent, and getRelevantParts. Inputs include HTTP headers (Authorization) and body/query (query and searchTerm). Outputs are ranked, summarized document results, or relevant error messages. Only text documents are supported; file type and output length constraints exist due to Azure and OpenAI API limits.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nmodule.exports = async function (context, req) {\n    const query = req.query.query || (req.body && req.body.query);\n    const searchTerm = req.query.searchTerm || (req.body && req.body.searchTerm);\n    if (!req.headers.authorization) {\n        context.res = {\n            status: 400,\n            body: 'Authorization header is missing'\n        };\n        return;\n    }\n    /// The below takes the token passed to the function, to use to get an OBO token.\n    const bearerToken = req.headers.authorization.split(' ')[1];\n    let accessToken;\n    try {\n        accessToken = await getOboToken(bearerToken);\n    } catch (error) {\n        context.res = {\n            status: 500,\n            body: `Failed to obtain OBO token: ${error.message}`\n        };\n        return;\n    }\n    // Initialize the Graph Client using the initGraphClient function defined above\n    let client = initGraphClient(accessToken);\n    // this is the search body to be used in the Microsft Graph Search API: https://learn.microsoft.com/en-us/graph/search-concept-files\n    const requestBody = {\n        requests: [\n            {\n                entityTypes: ['driveItem'],\n                query: {\n                    queryString: searchTerm\n                },\n                from: 0,\n                // the below is set to summarize the top 10 search results from the Graph API, but can configure based on your documents. \n                size: 10\n            }\n        ]\n    };\n\n    try { \n        // Function to tokenize content (e.g., based on words). \n        const tokenizeContent = (content) => {\n            return content.split(/\\s+/);\n        };\n\n        // Function to break tokens into 10k token windows for gpt-4o-mini\n        const breakIntoTokenWindows = (tokens) => {\n            const tokenWindows = []\n            const maxWindowTokens = 10000; // 10k tokens\n            let startIndex = 0;\n\n            while (startIndex < tokens.length) {\n                const window = tokens.slice(startIndex, startIndex + maxWindowTokens);\n                tokenWindows.push(window);\n                startIndex += maxWindowTokens;\n            }\n\n            return tokenWindows;\n        };\n        // This is where we are doing the search\n        const list = await client.api('/search/query').post(requestBody);\n\n        const processList = async () => {\n            // This will go through and for each search response, grab the contents of the file and summarize with gpt-4o-mini\n            const results = [];\n\n            await Promise.all(list.value[0].hitsContainers.map(async (container) => {\n                for (const hit of container.hits) {\n                    if (hit.resource[\"@odata.type\"] === \"#microsoft.graph.driveItem\") {\n                        const { name, id } = hit.resource;\n                        // We use the below to grab the URL of the file to include in the response\n                        const webUrl = hit.resource.webUrl.replace(/\\s/g, \"%20\");\n                        // The Microsoft Graph API ranks the reponses, so we use this to order it\n                        const rank = hit.rank;\n                        // The below is where the file lives\n                        const driveId = hit.resource.parentReference.driveId;\n                        const contents = await getDriveItemContent(client, driveId, id, name);\n                        if (contents !== 'Unsupported File Type') {\n                            // Tokenize content using function defined previously\n                            const tokens = tokenizeContent(contents);\n\n                            // Break tokens into 10k token windows\n                            const tokenWindows = breakIntoTokenWindows(tokens);\n\n                            // Process each token window and combine results\n                            const relevantPartsPromises = tokenWindows.map(window => getRelevantParts(window.join(' '), query));\n                            const relevantParts = await Promise.all(relevantPartsPromises);\n                            const combinedResults = relevantParts.join('\\n'); // Combine results\n\n                            results.push({ name, webUrl, rank, contents: combinedResults });\n                        } \n                        else {\n                            results.push({ name, webUrl, rank, contents: 'Unsupported File Type' });\n                        }\n                    }\n                }\n            }));\n\n            return results;\n        };\n        let results;\n        if (list.value[0].hitsContainers[0].total == 0) {\n            // Return no results found to the API if the Microsoft Graph API returns no results\n            results = 'No results found';\n        } else {\n            // If the Microsoft Graph API does return results, then run processList to iterate through.\n            results = await processList();\n            results.sort((a, b) => a.rank - b.rank);\n        }\n        context.res = {\n            status: 200,\n            body: results\n        };\n    } catch (error) {\n        context.res = {\n            status: 500,\n            body: `Error performing search or processing results: ${error.message}`,\n        };\n    }\n};\n\n```\n\n----------------------------------------\n\nTITLE: Query Zilliz with Vector Search and Filter\nDESCRIPTION: Defines a `query` function that performs a filtered search in Zilliz. It takes a text query and a boolean filter expression. It first gets the embedding for the query text using the `embed` function. It then performs a vector similarity search on the 'embedding' field, applying the provided filter expression (`expr`) to narrow down results based on metadata. The results, including specified output fields (title, type, etc.), are retrieved and printed in a formatted manner. A sample query combining vector search and a filter on release year and rating is demonstrated.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport textwrap\n\ndef query(query, top_k = 5):\n    text, expr = query\n    res = collection.search(embed(text), anns_field='embedding', expr = expr, param=QUERY_PARAM, limit = top_k, output_fields=['title', 'type', 'release_year', 'rating', 'description'])\n    for i, hit in enumerate(res):\n        print('Description:', text, 'Expression:', expr)\n        print('Results:')\n        for ii, hits in enumerate(hit):\n            print('\\t' + 'Rank:', ii + 1, 'Score:', hits.score, 'Title:', hits.entity.get('title'))\n            print('\\t\\t' + 'Type:', hits.entity.get('type'), 'Release Year:', hits.entity.get('release_year'), 'Rating:', hits.entity.get('rating'))\n            print(textwrap.fill(hits.entity.get('description'), 88))\n            print()\n\nmy_query = ('movie about a fluffly animal', 'release_year < 2019 and rating like \\\"PG%\\\"')\n\nquery(my_query)\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Fetch Functionality in Python for OpenAI Chat API\nDESCRIPTION: This Python code demonstrates integrating a custom weather-fetch function into a chat conversation with the OpenAI API. It defines a function to return weather data based on location input, initializes a conversation, specifies available functions, sends messages to the API, handles potential function call requests, invokes the function, and processes the response. It highlights dynamic function invocation and multi-turn conversation management.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to the model\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                    },\n                    \"required\": [\"location\"],\n                },\n            },\n        }\n    ]\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\"  # auto is default, but we'll be explicit\n    )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n    # Step 2: check if the model wanted to call a function\n    if tool_calls:\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        messages.append(response_message)  # extend conversation with assistant's reply\n        # Step 4: send the info for each function call and function response to the model\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(tool_call.function.arguments)\n            function_response = function_to_call(\n                location=function_args.get(\"location\"),\n                unit=function_args.get(\"unit\")\n            )\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": function_response,\n                }\n            )  # extend conversation with function response\n        second_response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n        )  # get a new response from the model where it can see the function response\n        return second_response\nprint(run_conversation())\n```\n\n----------------------------------------\n\nTITLE: Describing Functions for OpenAI in JavaScript\nDESCRIPTION: This code defines an array of `tools`, each describing a function that the OpenAI agent can use.  Each function definition includes its name, description, and parameters schema.  The `getCurrentWeather` function requires latitude and longitude parameters, while `getLocation` requires none.  The `type` is set to \"function\".\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst tools = [\n  {\n    type: \"function\",\n    function: {\n      name: \"getCurrentWeather\",\n      description: \"Get the current weather in a given location\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          latitude: {\n            type: \"string\",\n          },\n          longitude: {\n            type: \"string\",\n          },\n        },\n        required: [\"longitude\", \"latitude\"],\n      },\n    }\n  },\n  {\n    type: \"function\",\n    function: {\n      name: \"getLocation\",\n      description: \"Get the user's location based on their IP address\",\n      parameters: {\n        type: \"object\",\n        properties: {},\n      },\n    }\n  },\n];\n```\n\n----------------------------------------\n\nTITLE: Chunking Tokens from Text for Embedding in Python\nDESCRIPTION: Converts input text into tokens using a specified encoding and yields successive chunks of tokens based on a defined chunk size, enabling partial processing of long texts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef chunked_tokens(text, encoding_name, chunk_length):\n    encoding = tiktoken.get_encoding(encoding_name)\n    tokens = encoding.encode(text)\n    chunks_iterator = batched(tokens, chunk_length)\n    yield from chunks_iterator\n```\n\n----------------------------------------\n\nTITLE: Using Pydantic Models to Parse Structured Chat Output in Python\nDESCRIPTION: Illustrates a method to define a Pydantic model representing the math reasoning schema, including nested 'Step' objects and top-level fields, then uses the OpenAI SDK's beta 'parse' helper method to request a completion that is automatically parsed into strongly typed Pydantic models. This approach replaces manual JSON schema definitions with Python-native data validation and parsing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nclass MathReasoning(BaseModel):\n    class Step(BaseModel):\n        explanation: str\n        output: str\n\n    steps: list[Step]\n    final_answer: str\n\ndef get_math_solution(question: str):\n    completion = client.beta.chat.completions.parse(\n        model=MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": dedent(math_tutor_prompt)},\n            {\"role\": \"user\", \"content\": question},\n        ],\n        response_format=MathReasoning,\n    )\n\n    return completion.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: Custom GPT Instructions for Google Ads Reporting Data Retrieval - Python\nDESCRIPTION: This Python snippet defines the context and detailed instructions for a GPT custom action that audits Google Ads accounts and retrieves real-time reporting data via Adzviser. It requires integration with a Python Code Interpreter to calculate dynamic date ranges based on user inputs such as 'last week' or 'last month'. The snippet outlines stepwise workflows to fetch workspace information, gather applicable metrics and breakdowns, and query Google Ads data segmented by time granularity. It also covers auditing methods by selectively fetching account settings through API calls. Expected inputs include natural language queries specifying date ranges and data granularity, while outputs are structured reporting data files (CSV) ready for analysis within ChatGPT. Limitations include ensuring time granularity is only used if explicitly requested, and handling API syntax errors gracefully.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_googleads_adzviser.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n***Context***:\nYou are a Google Ads specialist who audits account health, retrieves real-time reporting data, and optimizes performances for marketers. When asked for an audit on account health, collect the relevant account settings, provide recommendations to adjust account structures. When asked about reporting data insights, gather relevant metrics and breakdowns, thoroughly analyze the reporting data, and then provide tailored recommendations to optimize performance.\n\n***Instructions for Retrieval of Reporting Data***:\n- Workflow to fetch real-time reporting data\nStep 1. Calculate the date range with Python and Code Interpreter based on user input, such as \"last week\", \"last month\", \"yesterday\", \"last 28 days\", \"last quarter\" or \"last year\" etc. If no specific timeframe is provided, ask the user to clarify. Adjust for calendar variations. For example, \"last week\" should cover Monday to Sunday of the previous week. \nStep 2. Retrieve workspace information using the 'getWorkspace' function. \nStep 3. Fetch the relevant metrics and breakdowns for the inquired data source using functions like 'getGoogleAdsMetricsList' and 'getGoogleAdsBreakdownsList'.\nStep 4. Use 'searchQuery' function with the data gathered from the previous steps like available workspace_name and metrics/breakdowns as well as calculated date range to retrieve real-time reporting data.\n- Time Granularity: If the user asks for daily/weekly/quarterly/monthly data, please reflect such info in the field time_granularity in searchQueryRequest. No need to add time_granularity if the user did not ask for it explicitly.\n- Returned Files: If multiple files are returned, make sure to read all of them. Each file contains data from a segment in a data source or a data source. \n- Necessary Breakdowns Only: Add important breakdowns only. Less is more. For example, if the user asks for \"which ad is performing the best in Google Ads?\", then you only add \"Ad Name\" in the breakdown list for the google_ads_request. No need to add breakdowns such as \"Device\" or \"Campaign Name\". \n\n***Instruction for Auditing****:\n- Workflow to audit Google Ads account\nStep 1. Retrieve workspace information using the 'getWorkspace' function.\nStep 2. Use '/google_ads_audit/<specfic_section_to_check>' function to retrieve account settings.\n- Comprehensive Audit: When asked for an comprehensive audit, don't call all the /google_ads_audit/<specfic_section_to_check> all at once. Show the users what you're planning to do next first. Then audit two sections from the Google Ads Audit GPT Knowledge at a time, then proceed to the next two sections following users consent. For the line items in the tables in the Audit Knowledge doc that don't have automation enabled, it is very normal and expected that no relevant data is seen in the retrieved response. Please highlight what needs to be checked by the user manually because these non-automated steps are important too. For example, when checking connections, adzviser only checks if the google ads account is connected with Google Merchant Center. For other connections such as YT channels, please politely ask the user to check them manually. \n\n***Additional Notes***:\n- Always calculate the date range please with Code Interpreter and Python. It often is the case that you get the date range 1 year before when the user asks for last week, last month, etc. \n- If there is an ApiSyntaxError: Could not parse API call kwargs as JSON, please politely tell the user that this is due to the recent update in OpenAI models and it can be solved by starting a new conversation on ChatGPT.\n- If the users asks for Google Ads data, for example, and there is only one workspace that has connected to Google Ads, then use this workspace name in the searchQueryRequest or googleAdsAuditRequest.\n- During auditing, part of the process is to retrieve the performance metrics at account, campaign, ad group, keyword, and product levels, remember to also run Python to calculate the date range for last month and the previous period. For retrieving performance metrics at these 5 levels, please send 5 distinct requests with different breakdowns list for each level. More can be found in the audit knowledge doc.\n```\n\n----------------------------------------\n\nTITLE: Basic ChatGPT API call with conversation history\nDESCRIPTION: Demonstrates a basic chat completion API call with a conversation history including system, user, and assistant messages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example OpenAI Python library request\nMODEL = \"gpt-3.5-turbo\"\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Orange.\"},\n    ],\n    temperature=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Specialized Agent Handlers in Python\nDESCRIPTION: Defines handler functions for each type of specialized agent (data processing, analysis, and visualization). Each handler creates a specific context, gets responses from the model, and executes the appropriate tools.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define the functions to handle each agent's processing\ndef handle_data_processing_agent(query, conversation_messages):\n    messages = [{\"role\": \"system\", \"content\": processing_system_prompt}]\n    messages.append({\"role\": \"user\", \"content\": query})\n\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        temperature=0,\n        tools=preprocess_tools,\n    )\n\n    conversation_messages.append([tool_call.function for tool_call in response.choices[0].message.tool_calls])\n    execute_tool(response.choices[0].message.tool_calls, conversation_messages)\n\ndef handle_analysis_agent(query, conversation_messages):\n    messages = [{\"role\": \"system\", \"content\": analysis_system_prompt}]\n    messages.append({\"role\": \"user\", \"content\": query})\n\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        temperature=0,\n        tools=analysis_tools,\n    )\n\n    conversation_messages.append([tool_call.function for tool_call in response.choices[0].message.tool_calls])\n    execute_tool(response.choices[0].message.tool_calls, conversation_messages)\n\ndef handle_visualization_agent(query, conversation_messages):\n    messages = [{\"role\": \"system\", \"content\": visualization_system_prompt}]\n    messages.append({\"role\": \"user\", \"content\": query})\n\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        temperature=0,\n        tools=visualization_tools,\n    )\n\n    conversation_messages.append([tool_call.function for tool_call in response.choices[0].message.tool_calls])\n    execute_tool(response.choices[0].message.tool_calls, conversation_messages)\n```\n\n----------------------------------------\n\nTITLE: Handling Model Tool Call Responses and Continuing API Conversation in Python (Steps 2-4)\nDESCRIPTION: This snippet detects and extracts tool call information from a model response, executes the corresponding backend database function, appends the tool result to the message history, and invokes the API again to continue the conversation. It handles both successful tool invocations ('ask_database') and cases where the tool is not present or not identified by the model. Requires a response_message object with a 'tool_calls' field, a working 'ask_database' function, 'conn', and an OpenAI client. Expects valid tool call structure and serializable arguments; outputs model and function results accordingly. Errors are printed for invalid tool names.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n# Step 2: determine if the response from the model includes a tool call.   \ntool_calls = response_message.tool_calls\nif tool_calls:\n    # If true the model will return the name of the tool / function to call and the argument(s)  \n    tool_call_id = tool_calls[0].id\n    tool_function_name = tool_calls[0].function.name\n    tool_query_string = json.loads(tool_calls[0].function.arguments)['query']\n\n    # Step 3: Call the function and retrieve results. Append the results to the messages list.      \n    if tool_function_name == 'ask_database':\n        results = ask_database(conn, tool_query_string)\n        \n        messages.append({\n            \"role\":\"tool\", \n            \"tool_call_id\":tool_call_id, \n            \"name\": tool_function_name, \n            \"content\":results\n        })\n        \n        # Step 4: Invoke the chat completions API with the function response appended to the messages list\n        # Note that messages with role 'tool' must be a response to a preceding message with 'tool_calls'\n        model_response_with_function_call = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n        )  # get a new response from the model where it can see the function response\n        print(model_response_with_function_call.choices[0].message.content)\n    else: \n        print(f\"Error: function {tool_function_name} does not exist\")\nelse: \n    # Model did not identify a function to call, result can be returned to the user \n    print(response_message.content) \n```\n\n----------------------------------------\n\nTITLE: Defining get_chat_completion function - Python\nDESCRIPTION: Defines a function `get_chat_completion` to interact with the OpenAI Chat Completions API.  It takes a list of messages, a model name, and optional parameters like `max_tokens`, `temperature`, `stop`, `tools`, `seed`, `functions`, and `tool_choice`.  It constructs the API request and returns the model's response and its usage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_chat_completion(\n    messages: list[dict[str, str]],\n    model: str = \"gpt-3.5-turbo\",\n    max_tokens=500,\n    temperature=0.0,\n    stop=None,\n    tools=None,\n    seed=42,\n    functions=None,\n    tool_choice=None,\n) -> str:\n    params = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"stop\": stop,\n        \"tools\": tools,\n        \"seed\": seed,\n        \"tool_choice\": tool_choice,\n    }\n    if functions:\n        params[\"functions\"] = functions\n\n    completion = client.chat.completions.create(**params)\n    return completion.choices[0].message, completion.usage\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens for Chat Messages Using tiktoken - Python\nDESCRIPTION: Defines a Python function to estimate the number of tokens used by a list of chat messages formatted for the gpt-3.5-turbo-0613 model. It leverages the tiktoken library to encode message content and accounts for token contributions from role/name fields and message formatting. The function adapts to different models by attempting to retrieve correct encodings but currently supports only gpt-3.5-turbo-0613. Inputs are lists of message dicts, outputs are integer token counts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n  try:\n      encoding = tiktoken.encoding_for_model(model)\n  except KeyError:\n      encoding = tiktoken.get_encoding(\"cl100k_base\")\n  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n      num_tokens = 0\n      for message in messages:\n          num_tokens += 4  # every message follows {role/name}\\n{content}\\n\n          for key, value in message.items():\n              num_tokens += len(encoding.encode(value))\n              if key == \"name\":  # if there's a name, the role is omitted\n                  num_tokens += -1  # role is always required and always 1 token\n      num_tokens += 2  # every reply is primed with assistant\n      return num_tokens\n  else:\n      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Printing Page Content Python\nDESCRIPTION: Prints the content of a specific document. This is done to inspect the loaded documents and examine their content. This confirms the proper loading of document content from the HTML files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(docs[0].page_content)\n```\n\n----------------------------------------\n\nTITLE: Testing Math Solution Function in Python\nDESCRIPTION: Demonstrates how to call the 'get_math_solution' function with a sample math problem and print the structured JSON-like response content. This verifies the function's ability to produce well-structured outputs conforming to the schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Testing with an example question\nquestion = \"how can I solve 8x + 7 = -23\"\n\nresult = get_math_solution(question) \n\nprint(result.content)\n```\n\n----------------------------------------\n\nTITLE: Updating Assistant with Vector Store Resource in Node.js\nDESCRIPTION: Shows how to update an assistant so that its file_search tool uses the specified vector store by modifying the tool_resources property. Requires the OpenAI Node.js beta assistants API and existing assistant and vector store IDs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_6\n\nLANGUAGE: node.js\nCODE:\n```\nawait openai.beta.assistants.update(assistant.id, {\n  tool_resources: { file_search: { vector_store_ids: [vectorStore.id] } },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Function App - Python\nDESCRIPTION: This Python snippet uses `subprocess.run` to execute the `az functionapp create` command via the Azure CLI. It provisions an Azure Function App resource with specified parameters including resource group, consumption plan location, runtime (python), name, storage account, and OS type (Linux).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Replace this with your own values. This name will appear in the URL of the API call https://<app_name>.azurewebsites.net\napp_name = \"<app-name>\"\n\nsubprocess.run([\n    \"az\", \"functionapp\", \"create\",\n    \"--resource-group\", resource_group,\n    \"--consumption-plan-location\", region,\n    \"--runtime\", \"python\",\n    \"--name\", app_name,\n    \"--storage-account\", storage_account_name,\n    \"--os-type\", \"Linux\",\n], check=True)\n```\n\n----------------------------------------\n\nTITLE: Answering Questions with Citations (Prompt)\nDESCRIPTION: Illustrates instructing the model to answer a question based solely on a provided document (delimited by triple quotes) and to include citations in a specific JSON format ({\"citation\": ...}). If the information isn't in the document, it should respond: \"Insufficient information.\"\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_9\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": â€¦}).\n\nUSER: \"\"\"\"\"\"\n\nQuestion: \n```\n\n----------------------------------------\n\nTITLE: Executing Hybrid Search (Vector + Text Field) and Processing Results - Python\nDESCRIPTION: This snippet executes a hybrid search using `search_redis`. It searches semantically for 'Art' using the `title_vector` but filters results using `create_hybrid_field` to ensure the 'text' field contains the phrase \"Leonardo da Vinci\". It then processes the returned documents, specifically examining the first result's text to find and print the sentence containing the required phrase.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# run a hybrid query for articles about Art in the title vector and only include results with the phrase \"Leonardo da Vinci\" in the text\nresults = search_redis(redis_client,\n                       \"Art\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"text\", \"Leonardo da Vinci\")\n                       )\n\n# find specific mention of Leonardo da Vinci in the text that our full-text-search query returned\nmention = [sentence for sentence in results[0].text.split(\"\\n\") if \"Leonardo da Vinci\" in sentence][0]\nmention\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Response with Tools (Python)\nDESCRIPTION: Demonstrates calling the `client.responses.create` method in Python to generate a response using the `gpt-4.1-2025-04-14` model. It passes system instructions (`SYS_PROMPT_CUSTOMER_SERVICE`), includes the previously defined `get_policy_doc` and `get_user_acct` tools, and provides an example user input query. The result's output is accessed via `.to_dict()[\"output\"]`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nresponse = client.responses.create(\n    instructions=SYS_PROMPT_CUSTOMER_SERVICE,\n    model=\"gpt-4.1-2025-04-14\",\n    tools=[get_policy_doc, get_user_acct],\n    input=\"How much will it cost for international service? I'm traveling to France.\",\n    # input=\"Why was my last bill so high?\"\n)\n\nresponse.to_dict()[\"output\"]\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Classification with OpenAI Embeddings and Cosine Similarity in Python\nDESCRIPTION: This snippet implements zero-shot text classification by embedding label descriptions and a review, computing cosine similarity between them, and predicting sentiment. It uses utility functions from openai.embeddings_utils and assumes precomputed label embeddings. Dependencies: openai.embeddings_utils, pandas; inputs: embedding model, review text, labels; outputs: predicted string label; limitation: requires class names or descriptions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\n\ndf= df[df.Score!=3]\ndf['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n\nlabels = ['negative', 'positive']\nlabel_embeddings = [get_embedding(label, model=model) for label in labels]\n\ndef label_score(review_embedding, label_embeddings):\n   return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\nprediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: Sets up the OpenAI client with an API key for generating embeddings. This initialization is required before making any requests to OpenAI's API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"OPENAI_API_KEY\"\n)  # get API key from platform.openai.com\n```\n\n----------------------------------------\n\nTITLE: Loading Book Dataset from Hugging Face Hub in Python\nDESCRIPTION: Imports the `datasets` library and uses the `load_dataset` function to download and load the 'Skelebor/book_titles_and_descriptions_en_clean' dataset. It specifically loads only the 'train' split of the dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\n# Download the dataset and only use the `train` portion (file is around 800Mb)\ndataset = datasets.load_dataset('Skelebor/book_titles_and_descriptions_en_clean', split='train')\n```\n\n----------------------------------------\n\nTITLE: Handling HTTP Document Search and File Extraction in Azure Function - JavaScript\nDESCRIPTION: This Azure Function enables document search and content retrieval workflows for authenticated O365/SharePoint users by orchestrating token exchange, Graph search, file extraction, and result composition. It handles HTTP requests, validates bearer tokens, fetches OBO tokens, initializes the Microsoft Graph client, and invokes search and file retrieval routines. The response is formatted for OpenAI integrations, supporting batch retrieval and error cases. Dependencies include the Azure Functions runtime and helper functions like 'getOboToken', 'initGraphClient', and 'getDriveItemContent'. Ensure environment configuration and Microsoft Graph API access are correctly set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nmodule.exports = async function (context, req) {\n   // const query = req.query.query || (req.body && req.body.query);\n   const searchTerm = req.query.searchTerm || (req.body && req.body.searchTerm);\n   if (!req.headers.authorization) {\n       context.res = {\n           status: 400,\n           body: 'Authorization header is missing'\n       };\n       return;\n   }\n   /// The below takes the token passed to the function, to use to get an OBO token.\n   const bearerToken = req.headers.authorization.split(' ')[1];\n   let accessToken;\n   try {\n       accessToken = await getOboToken(bearerToken);\n   } catch (error) {\n       context.res = {\n           status: 500,\n           body: `Failed to obtain OBO token: ${error.message}`\n       };\n       return;\n   }\n   // Initialize the Graph Client using the initGraphClient function defined above\n   let client = initGraphClient(accessToken);\n   // this is the search body to be used in the Microsft Graph Search API: https://learn.microsoft.com/en-us/graph/search-concept-files\n   const requestBody = {\n       requests: [\n           {\n               entityTypes: ['driveItem'],\n               query: {\n                   queryString: searchTerm\n               },\n               from: 0,\n               // the below is set to summarize the top 10 search results from the Graph API, but can configure based on your documents.\n               size: 10\n           }\n       ]\n   };\n\n\n   try {\n       // This is where we are doing the search\n       const list = await client.api('/search/query').post(requestBody);\n       const processList = async () => {\n           // This will go through and for each search response, grab the contents of the file and summarize with gpt-3.5-turbo\n           const results = [];\n           await Promise.all(list.value[0].hitsContainers.map(async (container) => {\n               for (const hit of container.hits) {\n                   if (hit.resource[\"@odata.type\"] === \"#microsoft.graph.driveItem\") {\n                       const { name, id } = hit.resource;\n                       // The below is where the file lives\n                       const driveId = hit.resource.parentReference.driveId;\n                       // we use the helper function we defined above to get the contents, convert to base64, and restructure it\n                       const contents = await getDriveItemContent(client, driveId, id, name);\n                       results.push(contents)\n               }\n           }));\n           return results;\n       };\n       let results;\n       if (list.value[0].hitsContainers[0].total == 0) {\n           // Return no results found to the API if the Microsoft Graph API returns no results\n           results = 'No results found';\n       } else {\n           // If the Microsoft Graph API does return results, then run processList to iterate through.\n           results = await processList();\n           // this is where we structure the response so ChatGPT knows they are files\n           results = {'openaiFileResponse': results}\n       }\n       context.res = {\n           status: 200,\n           body: results\n       };\n   } catch (error) {\n       context.res = {\n           status: 500,\n           body: `Error performing search or processing results: ${error.message}`,\n       };\n   }\n};\n```\n\n----------------------------------------\n\nTITLE: Evaluating Fact Inclusion with Checklists - example-chat\nDESCRIPTION: This snippet defines a detailed system prompt for evaluating if an answer contains a required set of facts. It instructs the model to restate each fact, cite relevant answer sections, assess inference clarity, and produce a tally of satisfied points in JSON. Dependencies: a conversational LLM interface capable of following system instructions, with candidate answers supplied by the user. Key inputs are the set of facts and the candidate answer; output is a count of satisfied facts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_21\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: You will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": }.\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with OpenAI API in Python\nDESCRIPTION: Creates embeddings for text data using OpenAI's text-embedding-3-small model. This function takes a text string, replaces newlines with spaces, and returns the embedding vector from the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Uploading File to Assistant (Python)\nDESCRIPTION: This snippet uploads a file, creates a vector store, adds the file to the vector store, and updates the assistant using the OpenAI Assistants API. It requires the OpenAI client library to be installed and configured, and a data file for uploading. It creates a vector store with a name and a file ID. It sets the tools for code interpreter and file search and tool_resources. After file upload it checks the status using a while loop until the status is complete or failed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Upload the file\nfile = client.files.create(\n    file=open(\n        \"data/language_models_are_unsupervised_multitask_learners.pdf\",\n        \"rb\",\n    ),\n    purpose=\"assistants\",\n)\n\n# Create a vector store\nvector_store = client.beta.vector_stores.create(\n    name=\"language_models_are_unsupervised_multitask_learners\",\n)\n\n# Add the file to the vector store\nvector_store_file = client.beta.vector_stores.files.create_and_poll(\n    vector_store_id=vector_store.id,\n    file_id=file.id,\n)\n\n# Confirm the file was added\nwhile vector_store_file.status == \"in_progress\":\n    time.sleep(1)\nif vector_store_file.status == \"completed\":\n    print(\"File added to vector store\")\nelif vector_store_file.status == \"failed\":\n    raise Exception(\"Failed to add file to vector store\")\n\n# Update Assistant\nassistant = client.beta.assistants.update(\n    MATH_ASSISTANT_ID,\n    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}],\n    tool_resources={\n        \"file_search\":{\n            \"vector_store_ids\": [vector_store.id]\n        },\n        \"code_interpreter\": {\n            \"file_ids\": [file.id]\n        }\n    },\n)\nshow_json(assistant)\n```\n\n----------------------------------------\n\nTITLE: Installing and importing OpenAI Python library\nDESCRIPTION: Sets up the OpenAI Python library by installing the latest version and importing the necessary modules for API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade openai\n```\n\nLANGUAGE: python\nCODE:\n```\n# import the OpenAI Python library for calling the OpenAI API\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Calling the Chat Completions API in Python\nDESCRIPTION: Shows how to make a call to the OpenAI Chat Completions API using the official Python library. It involves initializing the `OpenAI` client and calling `client.chat.completions.create` with the model name (`gpt-3.5-turbo`) and a list of message objects representing the conversation history (system, user, assistant roles). Requires the `openai` Python package and an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Chained Function Calling with OpenAI GPT in Python\nDESCRIPTION: Sets up and demonstrates a multi-turn function calling process using the OpenAI Chat Completions API. It defines a system message, a maximum call limit (`MAX_CALLS`), a helper function `get_openai_response` to interact with the API, and the main logic in `process_user_instruction`. This function manages a conversation loop: it sends the user's instruction and conversation history (including previous tool calls and simulated results) to the model, processes the model's response (either a message or a tool call request), appends relevant messages back to the history, and simulates tool execution by adding a 'success' message. The loop continues until the model generates a final message or the maximum number of calls is reached. Finally, it executes this process with a sample multi-step user instruction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nSYSTEM_MESSAGE = \"\"\"\nYou are a helpful assistant.\nRespond to the following prompt by using function_call and then summarize actions.\nAsk for clarification if a user request is ambiguous.\n\"\"\"\n\n# Maximum number of function calls allowed to prevent infinite or lengthy loops\nMAX_CALLS = 5\n\n\ndef get_openai_response(functions, messages):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-16k\",\n        tools=functions,\n        tool_choice=\"auto\",  # \"auto\" means the model can pick between generating a message or calling a function.\n        temperature=0,\n        messages=messages,\n    )\n\n\ndef process_user_instruction(functions, instruction):\n    num_calls = 0\n    messages = [\n        {\"content\": SYSTEM_MESSAGE, \"role\": \"system\"},\n        {\"content\": instruction, \"role\": \"user\"},\n    ]\n\n    while num_calls < MAX_CALLS:\n        response = get_openai_response(functions, messages)\n        message = response.choices[0].message\n        print(message)\n        try:\n            print(f\"\\n>> Function call #: {num_calls + 1}\\n\")\n            pp(message.tool_calls)\n            messages.append(message)\n\n            # For the sake of this example, we'll simply add a message to simulate success.\n            # Normally, you'd want to call the function here, and append the results to messages.\n            messages.append(\n                {\n                    \"role\": \"tool\",\n                    \"content\": \"success\",\n                    \"tool_call_id\": message.tool_calls[0].id,\n                }\n            )\n\n            num_calls += 1\n        except:\n            print(\"\\n>> Message:\\n\")\n            print(message.content)\n            break\n\n    if num_calls >= MAX_CALLS:\n        print(f\"Reached max chained function calls: {MAX_CALLS}\")\n\n\nUSER_INSTRUCTION = \"\"\"\nInstruction: Get all the events.\nThen create a new event named AGI Party.\nThen delete event with id 2456.\n\"\"\"\n\nprocess_user_instruction(functions, USER_INSTRUCTION)\n```\n\n----------------------------------------\n\nTITLE: Defining an OpenAI API Client in Python\nDESCRIPTION: Initializes an OpenAI client instance and sets a constant for the GPT model to be used in chat completions. This setup is necessary before performing moderation checks and generating responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\nGPT_MODEL = 'gpt-4o-mini'\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client using openai Python SDK\nDESCRIPTION: This snippet imports necessary modules and initializes an OpenAI client instance to interact with the API. It serves as a foundation for sending requests for model evaluation and hallucination detection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom concurrent.futures import ThreadPoolExecutor\nfrom IPython.display import display, HTML\nimport json\nimport pandas as pd\nfrom sklearn.metrics import precision_score, recall_score\nfrom typing import List\nfrom openai import OpenAI\n\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Summarizing Push Notifications Using OpenAI Chat Completion - Python\nDESCRIPTION: Defines a developer prompt for summarization and a function to call OpenAI's ChatCompletion API using the provided prompt and notifications string. Shows example use with a PushNotifications instance and prints the summary result. Dependencies: openai, pydantic. Inputs: push_notifications string. Outputs: chat completion response. The summarizer expects concise output and requires a valid OpenAI model (e.g., gpt-4o-mini).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nDEVELOPER_PROMPT = \"\"\"\nYou are a helpful assistant that summarizes push notifications.\nYou are given a list of push notifications and you need to collapse them into a single one.\nOutput only the final summary, nothing else.\n\"\"\"\n\ndef summarize_push_notification(push_notifications: str) -> ChatCompletion:\n    result = openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"developer\", \"content\": DEVELOPER_PROMPT},\n            {\"role\": \"user\", \"content\": push_notifications},\n        ],\n    )\n    return result\n\nexample_push_notifications_list = PushNotifications(notifications=\"\"\"\n- Alert: Unauthorized login attempt detected.\n- New comment on your blog post: \"Great insights!\"\n- Tonight's dinner recipe: Pasta Primavera.\n\"\"\")\nresult = summarize_push_notification(example_push_notifications_list.notifications)\nprint(result.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Tool Configuration for Responses API\nDESCRIPTION: Configures tools for the OpenAI Responses API, including a web search tool and a custom Pinecone search function. This setup enables the model to dynamically choose between searching the web or querying the internal knowledge base.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Tools definition: The list of tools includes:\n# - A web search preview tool.\n# - A Pinecone search tool for retrieving medical documents.\n\n# Define available tools.\ntools = [   \n    {\"type\": \"web_search_preview\",\n      \"user_location\": {\n        \"type\": \"approximate\",\n        \"country\": \"US\",\n        \"region\": \"California\",\n        \"city\": \"SF\"\n      },\n      \"search_context_size\": \"medium\"},\n    {\n        \"type\": \"function\",\n        \"name\": \"PineconeSearchDocuments\",\n        \"description\": \"Search for relevant documents based on the medical question asked by the user that is stored within the vector database using a semantic query.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The natural language query to search the vector database.\"\n                },\n                \"top_k\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Number of top results to return.\",\n                    \"default\": 3\n                }\n            },\n            \"required\": [\"query\"],\n            \"additionalProperties\": False\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings Using OpenAI Embedding API in Python\nDESCRIPTION: Loads the OpenAI API key from environment variables and defines a function to convert input text into vector embeddings using the 'text-embedding-3-small' model. It cleans the input text by replacing newlines, and calls the OpenAI embedding creation endpoint. The output is the embedding vector extracted from the response. This vector is used to represent textual data numerically for similarity searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef get_vector(text, model=\"text-embedding-3-small\"):\n    text = text.replace(\"\\n\", \" \")\n    return openai.Embedding.create(input = [text], model = model)['data'][0]['embedding']\n\ntext_1 = \"\"\"Japan narrowly escapes recession\n\nJapan's economy teetered on the brink of a technical recession in the three months to September, figures show.\n\nRevised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. On an annual basis, the data suggests annual growth of just 0.2%, suggesting a much more hesitant recovery than had previously been thought. A common technical definition of a recession is two successive quarters of negative growth.\nThe government was keen to play down the worrying implications of the data. \"I maintain the view that Japan's economy remains in a minor adjustment phase in an upward climb, and we will monitor developments carefully,\" said economy minister Heizo Takenaka. But in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead, observers were less sanguine. \"It's painting a picture of a recovery... much patchier than previously thought,\" said Paul Sheard, economist at Lehman Brothers in Tokyo. Improvements in the job market apparently have yet to feed through to domestic demand, with private consumption up just 0.2% in the third quarter.\n\"\"\"\n\ntext_2 = \"\"\"Dibaba breaks 5,000m world record\n\nEthiopia's Tirunesh Dibaba set a new world record in winning the women's 5,000m at the Boston Indoor Games.\n\nDibaba won in 14 minutes 32.93 seconds to erase the previous world indoor mark of 14:39.29 set by another Ethiopian, Berhane Adera, in Stuttgart last year. But compatriot Kenenisa Bekele's record hopes were dashed when he miscounted his laps in the men's 3,000m and staged his sprint finish a lap too soon. Ireland's Alistair Cragg won in 7:39.89 as Bekele battled to second in 7:41.42. \"I didn't want to sit back and get out-kicked,\" said Cragg. \"So I kept on the pace. The plan was to go with 500m to go no matter what, but when Bekele made the mistake that was it. The race was mine.\" Sweden's Carolina Kluft, the Olympic heptathlon champion, and Slovenia's Jolanda Ceplak had winning performances, too. Kluft took the long jump at 6.63m, while Ceplak easily won the women's 800m in 2:01.52. \n\"\"\"\n\n\ntext_3 = \"\"\"Google's toolbar sparks concern\n\nSearch engine firm Google has released a trial tool which is concerning some net users because it directs people to pre-selected commercial websites.\n\nThe AutoLink feature comes with Google's latest toolbar and provides links in a webpage to Amazon.com if it finds a book's ISBN number on the site. It also links to Google's map service, if there is an address, or to car firm Carfax, if there is a licence plate. Google said the feature, available only in the US, \"adds useful links\". But some users are concerned that Google's dominant position in the search engine market place could mean it would be giving a competitive edge to firms like Amazon.\n\nAutoLink works by creating a link to a website based on information contained in a webpage - even if there is no link specified and whether or not the publisher of the page has given permission.\n\nIf a user clicks the AutoLink feature in the Google toolbar then a webpage with a book's unique ISBN number would link directly to Amazon's website. It could mean online libraries that list ISBN book numbers find they are directing users to Amazon.com whether they like it or not. Websites which have paid for advertising on their pages may also be directing people to rival services. Dan Gillmor, founder of Grassroots Media, which supports citizen-based media, said the tool was a \"bad idea, and an unfortunate move by a company that is looking to continue its hypergrowth\". In a statement Google said the feature was still only in beta, ie trial, stage and that the company welcomed feedback from users. It said: \"The user can choose never to click on the AutoLink button, and web pages she views will never be modified. \"In addition, the user can choose to disable the AutoLink feature entirely at any time.\"\n\nThe new tool has been compared to the Smart Tags feature from Microsoft by some users. It was widely criticised by net users and later dropped by Microsoft after concerns over trademark use were raised. Smart Tags allowed Microsoft to link any word on a web page to another site chosen by the company. Google said none of the companies which received AutoLinks had paid for the service. Some users said AutoLink would only be fair if websites had to sign up to allow the feature to work on their pages or if they received revenue for any \"click through\" to a commercial site. Cory Doctorow, European outreach coordinator for digital civil liberties group Electronic Fronter Foundation, said that Google should not be penalised for its market dominance. \"Of course Google should be allowed to direct people to whatever proxies it chooses. \"But as an end user I would want to know - 'Can I choose to use this service?, 'How much is Google being paid?', 'Can I substitute my own companies for the ones chosen by Google?'\". Mr Doctorow said the only objection would be if users were forced into using AutoLink or \"tricked into using the service\".\n\"\"\"\n\ndoc_1 = {\"content\": text_1, \"vector\": get_vector(text_1)}\ndoc_2 = {\"content\": text_2, \"vector\": get_vector(text_2)}\ndoc_3 = {\"content\": text_3, \"vector\": get_vector(text_3)}\n```\n\n----------------------------------------\n\nTITLE: Parallel Batch Generation of Embeddings with OpenAI API in Python\nDESCRIPTION: Provides a suite of functions for efficient batch generation of embeddings via the OpenAI API, parallelized for scalability. Includes (1) retry-decorated get_embeddings for fetching OpenAI vector embeddings for a list of inputs; (2) batchify to split an iterable into size-n batches; (3) embed_corpus that encodes text, estimates API cost, manages parallel execution, and aggregates results; and (4) generate_embeddings that applies batching to add embeddings for a DataFrame column. Requires OpenAI credentials, access to tiktoken and concurrent.futures, and expects reasonably pre-cleaned text data. Outputs embedding vectors assigned to DataFrame columns and logs processing progress and cost estimates. Limits include rate limits from the OpenAI API and memory requirements for large datasets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n## Batch Embedding Logic\n\n# Simple function to take in a list of text objects and return them as a list of embeddings\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(10))\ndef get_embeddings(input: List):\n    response = client.embeddings.create(\n        input=input,\n        model=EMBEDDING_MODEL\n    ).data\n    return [data.embedding for data in response]\n\n\n# Splits an iterable into batches of size n.\ndef batchify(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx : min(ndx + n, l)]\n     \n\n# Function for batching and parallel processing the embeddings\ndef embed_corpus(\n    corpus: List[str],\n    batch_size=64,\n    num_workers=8,\n    max_context_len=8191,\n):\n    # Encode the corpus, truncating to max_context_len\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    encoded_corpus = [\n        encoded_article[:max_context_len] for encoded_article in encoding.encode_batch(corpus)\n    ]\n\n    # Calculate corpus statistics: the number of inputs, the total number of tokens, and the estimated cost to embed\n    num_tokens = sum(len(article) for article in encoded_corpus)\n    cost_to_embed_tokens = num_tokens / 1000 * EMBEDDING_COST_PER_1K_TOKENS\n    print(\n        f\"num_articles={len(encoded_corpus)}, num_tokens={num_tokens}, est_embedding_cost={cost_to_embed_tokens:.2f} USD\"\n    )\n\n    # Embed the corpus\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        \n        futures = [\n            executor.submit(get_embeddings, text_batch)\n            for text_batch in batchify(encoded_corpus, batch_size)\n        ]\n\n        with tqdm(total=len(encoded_corpus)) as pbar:\n            for _ in concurrent.futures.as_completed(futures):\n                pbar.update(batch_size)\n\n        embeddings = []\n        for future in futures:\n            data = future.result()\n            embeddings.extend(data)\n\n        return embeddings\n    \n\n# Function to generate embeddings for a given column in a DataFrame\ndef generate_embeddings(df, column_name):\n    # Initialize an empty list to store embeddings\n    descriptions = df[column_name].astype(str).tolist()\n    embeddings = embed_corpus(descriptions)\n\n    # Add the embeddings as a new column to the DataFrame\n    df['embeddings'] = embeddings\n    print(\"Embeddings created successfully.\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API\nDESCRIPTION: This code snippet initializes the OpenAI client, which is used to interact with the OpenAI API. It retrieves the API key from the environment variables and uses it for authentication. If the API key is not set in the environment variables, it uses a placeholder.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI  # for making OpenAI API calls\nimport urllib  # for downloading example audio files\nimport os  # for accessing environment variables\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Getting OpenAI Embeddings in Python (v1.0+)\nDESCRIPTION: Initializes the OpenAI client and demonstrates how to compute embedding vectors for a list of input texts using the `client.embeddings.create` method.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI(api_key=OPENAI_API_KEY)\nembedding_model_name = \"text-embedding-3-small\"\n\nresult = client.embeddings.create(\n    input=[\n        \"This is a sentence\",\n        \"A second sentence\"\n    ],\n    model=embedding_model_name,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure AI Search vector and semantic index in Python\nDESCRIPTION: Defines and creates a search index on Azure AI Search service incorporating both vector search (using HNSW algorithm with cosine metric) and semantic search configurations. The index schema includes fields for document IDs, URL, titles, text content, and two vector fields (title_vector and content_vector) with 1536 dimensions each linked to a vector search profile. Index is created or updated via SearchIndexClient.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the SearchIndexClient\nindex_client = SearchIndexClient(\n    endpoint=search_service_endpoint, credential=credential\n)\n\n# Define the fields for the index\nfields = [\n    SimpleField(name=\"id\", type=SearchFieldDataType.String),\n    SimpleField(name=\"vector_id\", type=SearchFieldDataType.String, key=True),\n    SimpleField(name=\"url\", type=SearchFieldDataType.String),\n    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n    SearchableField(name=\"text\", type=SearchFieldDataType.String),\n    SearchField(\n        name=\"title_vector\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n        vector_search_dimensions=1536,\n        vector_search_profile_name=\"my-vector-config\",\n    ),\n    SearchField(\n        name=\"content_vector\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n        vector_search_dimensions=1536,\n        vector_search_profile_name=\"my-vector-config\",\n    ),\n]\n\n# Configure the vector search configuration\nvector_search = VectorSearch(\n    algorithms=[\n        HnswAlgorithmConfiguration(\n            name=\"my-hnsw\",\n            kind=VectorSearchAlgorithmKind.HNSW,\n            parameters=HnswParameters(\n                m=4,\n                ef_construction=400,\n                ef_search=500,\n                metric=VectorSearchAlgorithmMetric.COSINE,\n            ),\n        )\n    ],\n    profiles=[\n        VectorSearchProfile(\n            name=\"my-vector-config\",\n            algorithm_configuration_name=\"my-hnsw\",\n        )\n    ],\n)\n\n# Configure the semantic search configuration\nsemantic_search = SemanticSearch(\n    configurations=[\n        SemanticConfiguration(\n            name=\"my-semantic-config\",\n            prioritized_fields=SemanticPrioritizedFields(\n                title_field=SemanticField(field_name=\"title\"),\n                keywords_fields=[SemanticField(field_name=\"url\")],\n                content_fields=[SemanticField(field_name=\"text\")],\n            ),\n        )\n    ]\n)\n\n# Create the search index with the vector search and semantic search configurations\nindex = SearchIndex(\n    name=index_name,\n    fields=fields,\n    vector_search=vector_search,\n    semantic_search=semantic_search,\n)\n\n# Create or update the index\nresult = index_client.create_or_update_index(index)\nprint(f\"{result.name} created\")\n```\n\n----------------------------------------\n\nTITLE: Uploading a training file using Python OpenAI SDK\nDESCRIPTION: This code demonstrates how to upload a training dataset file ('mydata.jsonl') to OpenAI via the Python SDK, specifying the purpose as 'fine-tune'. It depends on the 'openai' Python package and requires an API key configuration. The uploaded file is used for subsequent fine-tuning jobs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\nclient.files.create(\n  file=open(\"mydata.jsonl\", \"rb\"),\n  purpose=\"fine-tune\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Query Embedding Vector and Preparing for Pinecone Retrieval in Python\nDESCRIPTION: Computes the embedding vector representation of the user query string using the OpenAI embedding API and stores the vector as 'xq' for performing semantic similarity search within the Pinecone index. This vector is used as the search key to retrieve relevant documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nres = openai.Embedding.create(\n    input=[query],\n    engine=embed_model\n)\n\n# retrieve from Pinecone\nxq = res['data'][0]['embedding']\n```\n\n----------------------------------------\n\nTITLE: Connecting to Snowflake Using OAuth Token in Python\nDESCRIPTION: This snippet demonstrates extracting the Azure Entra ID token from an HTTP request, decoding it to retrieve user information, and establishing a connection to Snowflake using OAuth authentication method with the token. Dependencies include azure.functions, jwt, snowflake-connector, and cloud environment variables for account and warehouse setup.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nauth_header = req.headers.get('Authorization')\ntoken_type, token = auth_header.split()\n\ntry:\n    decoded_token = jwt.decode(token, options={\"verify_signature\": False})\n    email = decoded_token.get('upn') \n    \n    conn = snowflake.connector.connect(\n        user=email, # Snowflake username, i.e., user's email in my example\n        account=SNOWFLAKE_ACCOUNT, # Snowflake account, i.e., ab12345.eastus2.azure\n        authenticator=\"oauth\",\n        token=token,\n        warehouse=SNOWFLAKE_WAREHOUSE # Replace with Snowflake warehouse\n    )\n    logging.info(\"Successfully connected to Snowflake.\")\nexcept Exception as e:\n    logging.error(f\"Failed to connect to Snowflake: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Continuing Conversation to Provide Missing Details for Weather Query in Python\nDESCRIPTION: Shows appending additional user input specifying location details to the conversation, then calling `chat_completion_request` again with the updated messages and function specifications. The assistant response after this reflects the new data, generating appropriate function arguments based on the user's provided location.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\nchat_response = chat_completion_request(\n    messages, tools=tools\n)\nassistant_message = chat_response.choices[0].message\nmessages.append(assistant_message)\nassistant_message\n```\n\n----------------------------------------\n\nTITLE: Defining a Retry-Enabled Chat Completion Request Function in Python\nDESCRIPTION: Defines a robust helper function `chat_completion_request` for sending chat message requests to the OpenAI Chat Completions API with optional tools and tool_choice parameters. It uses a retry decorator from tenacity to handle transient failures by retrying with exponential backoff. The function returns the API response or the exception if an error occurs. This facilitates resilient calls to the API using configurable GPT models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\ndef chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n        return response\n    except Exception as e:\n        print(\"Unable to generate ChatCompletion response\")\n        print(f\"Exception: {e}\")\n        return e\n```\n\n----------------------------------------\n\nTITLE: Fallback to Another Model\nDESCRIPTION: This code snippet demonstrates how to handle rate limit errors by falling back to another model. The `completions_with_fallback` function first attempts to use the primary model. If a `openai.RateLimitError` is raised, it switches to the specified `fallback_model` and retries the request. This allows the application to continue functioning even when the primary model is rate-limited.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef completions_with_fallback(fallback_model, **kwargs):\n    try:\n        return client.chat.completions.create(**kwargs)\n    except openai.RateLimitError:\n        kwargs['model'] = fallback_model\n        return client.chat.completions.create(**kwargs)\n    \n    \ncompletions_with_fallback(fallback_model=\"gpt-4o\", model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}])\n```\n\n----------------------------------------\n\nTITLE: Defining PushNotifications Data Model with Pydantic - Python\nDESCRIPTION: Defines a PushNotifications model using pydantic to structure the input data for push notification summaries. The model has a single field: notifications (string). It then prints a JSON schema for integration with evals. Dependency: pydantic. Input: notifications string. Output: JSON schema representing the data structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass PushNotifications(pydantic.BaseModel):\n    notifications: str\n\nprint(PushNotifications.model_json_schema())\n```\n\n----------------------------------------\n\nTITLE: Initializing Microsoft Graph Client\nDESCRIPTION: This code initializes the Microsoft Graph client using the provided access token. It's a crucial step for authenticating with the Microsoft Graph API.  The `Client.init` method is used to configure the client with an authentication provider, which in this case, uses a simple function that provides the access token. This is a prerequisite for making API calls to retrieve data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Client } = require('@microsoft/microsoft-graph-client');\n\nfunction initGraphClient(accessToken) {\n    return Client.init({\n        authProvider: (done) => {\n            done(null, accessToken);\n        }\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Agent with Custom Prompt and Multiple Tools in Python\nDESCRIPTION: This snippet sets up a conversational agent using a custom prompt template that includes historical dialogue, and limits the agent's actions to the expanded toolset. It creates an LLMChain for generating responses, prepares tool names for agent configuration, and initializes an LLMSingleActionAgent with output parsing and stop criteria. Required dependencies include the LLMChain, CustomPromptTemplate, and LLMSingleActionAgent classes, the expanded_tools list, and prompt template variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n# Re-initialize the agent with our new list of tools\nprompt_with_history = CustomPromptTemplate(\n    template=template_with_history,\n    tools=expanded_tools,\n    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n)\nllm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\nmulti_tool_names = [tool.name for tool in expanded_tools]\nmulti_tool_agent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"], \n    allowed_tools=multi_tool_names\n)\n```\n\n----------------------------------------\n\nTITLE: Clustering Embeddings with KMeans using scikit-learn in Python\nDESCRIPTION: Runs KMeans clustering with a specified number of clusters (default set to 4) using the scikit-learn library. Cluster assignments are added as a new column to the dataframe, and the mean review score per cluster is calculated and displayed. Dependencies: scikit-learn, pandas, numpy. 'matrix' must be a 2D numpy array of embeddings and 'df' a pandas DataFrame. Outputs labeled clusters and their average scores. The KMeans fitting will use a fixed random state for reproducibility.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.cluster import KMeans\n\nn_clusters = 4\n\nkmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\nkmeans.fit(matrix)\nlabels = kmeans.labels_\ndf[\"Cluster\"] = labels\n\ndf.groupby(\"Cluster\").Score.mean().sort_values()\n\n```\n\n----------------------------------------\n\nTITLE: Constructing a Function Tool Specification for OpenAI API in Python\nDESCRIPTION: This snippet builds a 'tools' list that specifies an API-invokable function called 'ask_database', which is intended to answer music-related queries by executing provided SQL queries. The schema string is embedded into the tool's parameter description, allowing the LLM to formulate tailored queries. Required dependencies are a properly formatted database schema and a conforming OpenAI interface. Input is none (except variable dependencies); output is a variable 'tools' structured for use in an OpenAI API call. Key parameters include 'query' (the SQL statement).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"ask_database\",\n            \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": f\"\"\"\n                                SQL query extracting info to answer the user's question.\n                                SQL should be written using this database schema:\n                                {database_schema_string}\n                                The query should be returned in plain text, not in JSON.\n                                \"\"\",\n                    }\n                },\n                \"required\": [\"query\"],\n            },\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Function for Generating Image Captions Using GPT-4 Vision Model\nDESCRIPTION: Defines a function that sends an image URL and item title to the GPT-4 model with vision capabilities, requesting a concise caption describing the image, implementing the message payload with text and image URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef get_caption(img_url, title):\n    response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    temperature=0.2,\n    max_tokens=300,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": caption_system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": title\n                },\n                # The content type should be \"image_url\" to use gpt-4-turbo's vision capabilities\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": img_url\n                    }\n                },\n            ],\n        }\n    ]\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Initiating LangChain Agent Executor for Query Handling in Python\nDESCRIPTION: Configures the AgentExecutor to handle queries using the constructed agent, tools list, and verbose mode enabled for showing chain-of-thought reasoning. Requires a valid agent and corresponding tool objects. The executor orchestrates the flow between the LLM, tools, and output parser to generate answers or execute actions for user queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Initiate the agent that will respond to our queries\n# Set verbose=True to share the CoT reasoning the LLM goes through\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Thread and Adding Messages in Python\nDESCRIPTION: This code snippet shows how to create a thread for a conversation and add a message to that thread. It uses `client.beta.threads.create()` to create a thread and then `client.beta.threads.messages.create()` to add a user's message to the created thread.  Requires a valid OpenAI API key and the openai Python library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-function-calling.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n  thread_id=thread.id,\n  role=\"user\",\n  content=\"What's the weather in San Francisco today and the likelihood it'll rain?\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Interaction Logic - Sample GPT Instructions\nDESCRIPTION: This text snippet provides sample instructions intended for a GPT model on how to interact with the documented Azure Function action. It defines the expected behavior for passing parameters (`query`, `searchTerm`), handling successful search results (summarizing with the URL), and managing scenarios where no results are found (retrying with different terms, notifying the user, and suggesting alternatives like checking SharePoint or using Advanced Data Analysis for structured data).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nYou are a Q&A helper that helps answer users questions. You have access to a documents repository through your API action. When a user asks a question, you pass in that question exactly as stated to the \"query\" parameter, and for the \"searchTerm\" you use a single keyword or term you think you should use for the search.\n\n****\n\nScenario 1: There are answers\n\nIf your action returns results, then you take the results from the action and summarize concisely with the webUrl returned from the action. You answer the users question to the best of your knowledge from the action\n\n****\n\nScenario 2: No results found\n\nIf the response you get from the action is \"No results found\", stop there and let the user know there were no results and that you are going to try a different search term, and explain why. You must always let the user know before conducting another search.\n\nExample:\n\n****\n\nI found no results for \"DEI\". I am now going to try [insert term] because [insert explanation]\n\n****\n\nThen, try a different searchTerm that is similar to the one you tried before, with a single word. \n\nTry this three times. After the third time, then let the user know you did not find any relevant documents to answer the question, and to check SharePoint. Be sure to be explicit about what you are searching for at each step.\n\n****\n\nIn either scenario, try to answer the user's question. If you cannot answer the user's question based on the knowledge you find, let the user know and ask them to go check the HR Docs in SharePoint. If the file is a CSV, XLSX, or XLS, you can tell the user to download the file using the link and re-upload to use Advanced Data Analysis.\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Agent System Tools for Data Analysis and Visualization in Python\nDESCRIPTION: Defines the tool configurations for a multi-agent system that handles data preprocessing, analysis, and visualization. Each tool is defined as a function with specific parameters and descriptions, organized into categories based on functionality. The strict parameter ensures parameter validation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntriage_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"send_query_to_agents\",\n            \"description\": \"Sends the user query to relevant agents based on their capabilities.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"agents\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"An array of agent names to send the query to.\"\n                    },\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The user query to send.\"\n                    }\n                },\n                \"required\": [\"agents\", \"query\"]\n            }\n        },\n        \"strict\": True\n    }\n]\n\npreprocess_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"clean_data\",\n            \"description\": \"Cleans the provided data by removing duplicates and handling missing values.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The dataset to clean. Should be in a suitable format such as JSON or CSV.\"\n                    }\n                },\n                \"required\": [\"data\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"transform_data\",\n            \"description\": \"Transforms data based on specified rules.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The data to transform. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"rules\": {\n                        \"type\": \"string\",\n                        \"description\": \"Transformation rules to apply, specified in a structured format.\"\n                    }\n                },\n                \"required\": [\"data\", \"rules\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"aggregate_data\",\n            \"description\": \"Aggregates data by specified columns and operations.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The data to aggregate. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"group_by\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"Columns to group by.\"\n                    },\n                    \"operations\": {\n                        \"type\": \"string\",\n                        \"description\": \"Aggregation operations to perform, specified in a structured format.\"\n                    }\n                },\n                \"required\": [\"data\", \"group_by\", \"operations\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    }\n]\n\n\nanalysis_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"stat_analysis\",\n            \"description\": \"Performs statistical analysis on the given dataset.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The dataset to analyze. Should be in a suitable format such as JSON or CSV.\"\n                    }\n                },\n                \"required\": [\"data\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"correlation_analysis\",\n            \"description\": \"Calculates correlation coefficients between variables in the dataset.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The dataset to analyze. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"variables\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of variables to calculate correlations for.\"\n                    }\n                },\n                \"required\": [\"data\", \"variables\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"regression_analysis\",\n            \"description\": \"Performs regression analysis on the dataset.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The dataset to analyze. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"dependent_var\": {\n                        \"type\": \"string\",\n                        \"description\": \"The dependent variable for regression.\"\n                    },\n                    \"independent_vars\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of independent variables.\"\n                    }\n                },\n                \"required\": [\"data\", \"dependent_var\", \"independent_vars\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    }\n]\n\nvisualization_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"create_bar_chart\",\n            \"description\": \"Creates a bar chart from the provided data.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The data for the bar chart. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"x\": {\n                        \"type\": \"string\",\n                        \"description\": \"Column for the x-axis.\"\n                    },\n                    \"y\": {\n                        \"type\": \"string\",\n                        \"description\": \"Column for the y-axis.\"\n                    }\n                },\n                \"required\": [\"data\", \"x\", \"y\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"create_line_chart\",\n            \"description\": \"Creates a line chart from the provided data.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The data for the line chart. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"x\": {\n                        \"type\": \"string\",\n                        \"description\": \"Column for the x-axis.\"\n                    },\n                    \"y\": {\n                        \"type\": \"string\",\n                        \"description\": \"Column for the y-axis.\"\n                    }\n                },\n                \"required\": [\"data\", \"x\", \"y\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"create_pie_chart\",\n            \"description\": \"Creates a pie chart from the provided data.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"data\": {\n                        \"type\": \"string\",\n                        \"description\": \"The data for the pie chart. Should be in a suitable format such as JSON or CSV.\"\n                    },\n                    \"labels\": {\n                        \"type\": \"string\",\n                        \"description\": \"Column for the labels.\"\n                    },\n                    \"values\": {\n                        \"type\": \"string\",\n                        \"description\": \"Column for the values.\"\n                    }\n                },\n                \"required\": [\"data\", \"labels\", \"values\"],\n                \"additionalProperties\": False\n            }\n        },\n        \"strict\": True\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Completion and Retrieving Token Usage with OpenAI Python Client\nDESCRIPTION: Demonstrates how to instantiate the OpenAI Python client and create a chat completion request for the specified model and messages while setting temperature to zero. It then prints the number of prompt tokens used as reported by the API's response usage field. This snippet confirms that token counts computed locally match the API's calculation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# example token count from the OpenAI API\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=model,\n  messages=messages,\n  temperature=0,\n)\n\nprint(f'{response.usage.prompt_tokens} prompt tokens used.')\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Hybrid Search with Reranking in Azure AI Search in Python\nDESCRIPTION: Enhances hybrid search with semantic reranking powered by Bing. This approach uses language understanding to improve search relevance and adds extractive captions and answers to the results. The code demonstrates how to extract and display semantic answers, reranker scores, and captions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Semantic Hybrid Search\nquery = \"What were the key technological advancements during the Industrial Revolution?\"\n\nsearch_client = SearchClient(search_service_endpoint, index_name, credential)\nvector_query = VectorizedQuery(\n    vector=generate_embeddings(query, deployment),\n    k_nearest_neighbors=3,\n    fields=\"content_vector\",\n)\n\nresults = search_client.search(\n    search_text=query,\n    vector_queries=[vector_query],\n    select=[\"title\", \"text\", \"url\"],\n    query_type=QueryType.SEMANTIC,\n    semantic_configuration_name=\"my-semantic-config\",\n    query_caption=QueryCaptionType.EXTRACTIVE,\n    query_answer=QueryAnswerType.EXTRACTIVE,\n    top=3,\n)\n\nsemantic_answers = results.get_answers()\nfor answer in semantic_answers:\n    if answer.highlights:\n        print(f\"Semantic Answer: {answer.highlights}\")\n    else:\n        print(f\"Semantic Answer: {answer.text}\")\n    print(f\"Semantic Answer Score: {answer.score}\\n\")\n\nfor result in results:\n    print(f\"Title: {result['title']}\")\n    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n    print(f\"URL: {result['url']}\")\n    captions = result[\"@search.captions\"]\n    if captions:\n        caption = captions[0]\n        if caption.highlights:\n            print(f\"Caption: {caption.highlights}\\n\")\n        else:\n            print(f\"Caption: {caption.text}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Calling Chat Completions Endpoint - OpenAI API - Shell\nDESCRIPTION: This shell snippet uses curl to send a POST request to OpenAI's 'chat/completions' endpoint, utilizing the 'gpt-3.5-turbo' model for generating a poetic explanation of recursion. It requires the OPENAI_API_KEY environment variable to be set, and curl as dependency. The request sends message objects as a system instruction and user query within a JSON payload. Input parameters include the model and a history of chat messages. The output will be a JSON response containing the model's completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Compose a poem that explains the concept of recursion in programming.\"\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI API with Function and Tool Choice\nDESCRIPTION: This code snippet invokes the OpenAI Chat Completions API using the `gpt-3.5-turbo-0613` model. It sets up messages, includes the function schema generated by `generate_functions`, and uses `tool_choice` to force the API to use the `enrich_entities` function. The response is then processed to extract the function name, arguments, and the corresponding function response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(min=1, max=10), stop=stop_after_attempt(5))\ndef run_openai_task(labels, text):\n    messages = [\n          {\"role\": \"system\", \"content\": system_message(labels=labels)},\n          {\"role\": \"assistant\", \"content\": assisstant_message()},\n          {\"role\": \"user\", \"content\": user_message(text=text)}\n      ]\n\n    # TODO: functions and function_call are deprecated, need to be updated\n    # See: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools\n    response = openai.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        tools=generate_functions(labels),\n        tool_choice={\"type\": \"function\", \"function\" : {\"name\": \"enrich_entities\"}}, \n        temperature=0,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n\n    response_message = response.choices[0].message\n    \n    available_functions = {\"enrich_entities\": enrich_entities}  \n    function_name = response_message.tool_calls[0].function.name\n    \n    function_to_call = available_functions[function_name]\n    logging.info(f\"function_to_call: {function_to_call}\")\n\n    function_args = json.loads(response_message.tool_calls[0].function.arguments)\n    logging.info(f\"function_args: {function_args}\")\n\n    function_response = function_to_call(text, function_args)\n\n    return {\"model_response\": response, \n            \"function_response\": function_response}\n```\n\n----------------------------------------\n\nTITLE: Performing Filtered Vector Search in Milvus (Python)\nDESCRIPTION: Defines a function `query` that takes a text query and a Milvus filter expression. It embeds the text query using `embed` and then calls `collection.search` with the embedding vector, specified filter expression (`expr`), and search parameters (`param`). The function prints the query details and formatted results including score, title, and other metadata for the top `top_k` matches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport textwrap\n\ndef query(query, top_k = 5):\n    text, expr = query\n    res = collection.search(embed(text), anns_field='embedding', expr = expr, param=QUERY_PARAM, limit = top_k, output_fields=['title', 'type', 'release_year', 'rating', 'description'])\n    for i, hit in enumerate(res):\n        print('Description:', text, 'Expression:', expr)\n        print('Results:')\n        for ii, hits in enumerate(hit):\n            print('\\t' + 'Rank:', ii + 1, 'Score:', hits.score, 'Title:', hits.entity.get('title'))\n            print('\\t\\t' + 'Type:', hits.entity.get('type'), 'Release Year:', hits.entity.get('release_year'), 'Rating:', hits.entity.get('rating'))\n            print(textwrap.fill(hits.entity.get('description'), 88))\n            print()\n\nmy_query = ('movie about a fluffly animal', 'release_year < 2019 and rating like \\\"PG%\\\"')\n\nquery(my_query)\n```\n\n----------------------------------------\n\nTITLE: Querying GPT-4 Vision with an image and text prompt in Python\nDESCRIPTION: Constructs a prompt containing a user query, image description, and embedded image as a data URL, then sends it via OpenAI's API to GPT-4 Vision. Returns the model's interpreted response, enabling multimodal question answering for images.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef image_query(query, image_path):\n    response = client.chat.completions.create(\n        model='gpt-4-vision-preview',\n        messages=[\n            {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                \"type\": \"text\",\n                \"text\": query,\n                },\n                {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{encode_image(image_path)}\",\n                },\n                }\n            ],\n            }\n        ],\n        max_tokens=300,\n    )\n    return response.choices[0].message.content\n\nimage_query('Write a short label of what is show in this image?', image_path)\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Whisper via OpenAI API - Python\nDESCRIPTION: This snippet defines the 'transcribe_audio' function, which uses OpenAI's Whisper ('whisper-1') ASR model to convert spoken audio into text. It requires the 'openai' and 'python-docx' libraries. The 'audio_file_path' parameter must be a valid path to a local audio file; the audio file is opened in binary mode and passed to the OpenAI API. The function returns the transcript as a string. The OpenAI API key should be configured via environment variables or explicitly provided.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    # api_key=\"My API Key\",\n)\nfrom docx import Document\n\ndef transcribe_audio(audio_file_path):\n    with open(audio_file_path, 'rb') as audio_file:\n        transcription = client.audio.transcriptions.create(\"whisper-1\", audio_file)\n    return transcription['text']\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Cross-Encoder Relevance Function with GPT and Logit Bias - Python\nDESCRIPTION: Defines a retry-wrapped function to classify a document as relevant to a query using OpenAI's completions endpoint. The function sends a formatted prompt with the specified 'query' and 'document', enforces logit bias to favor ' Yes' or ' No' tokens, and disables randomness. Dependencies include openai, tenacity, and prior prompt/token ID preparation. The function returns the query, document, model prediction, and log probability of the chosen token. Input: query string and document string. Output: tuple as described. Maximum three attempts on API errors; tune retry parameters as needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\ndef document_relevance(query, document):\n    response = openai.chat.completions.create(\n        model=\"text-davinci-003\",\n        message=prompt.format(query=query, document=document),\n        temperature=0,\n        logprobs=True,\n        logit_bias={3363: 1, 1400: 1},\n    )\n\n    return (\n        query,\n        document,\n        response.choices[0].message.content,\n        response.choices[0].logprobs.token_logprobs[0],\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Azure OpenAI\nDESCRIPTION: Installs the OpenAI Python SDK and python-dotenv package needed for Azure OpenAI service integration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"openai>=1.0.0,<2.0.0\"\n! pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client\nDESCRIPTION: Initializes the OpenAI client using the API key loaded from environment variables. Defines a helper function get_completion for making chat completion requests to the OpenAI API, using the gpt-3.5-turbo model by default.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\noai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = oai_client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0,\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from OpenAI Embeddings API\nDESCRIPTION: Shows the typical JSON structure returned by the OpenAI Embeddings API upon successful creation of an embedding. It includes a list (`data`) containing the embedding object(s), each with the embedding vector (a list of floats), its index, and object type. The response also contains metadata like the model used (`text-embedding-3-small`) and token usage information (`prompt_tokens`, `total_tokens`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ... (omitted for spacing)\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Printing Run Step Details (Python)\nDESCRIPTION: This snippet iterates through the run steps and prints the details of each step using the OpenAI Assistants API. It requires the run_steps data retrieved by listing steps. It iterates through each step and uses json.dumps and show_json to pretty print the step details.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfor step in run_steps.data:\n    step_details = step.step_details\n    print(json.dumps(show_json(step_details), indent=4))\n```\n\n----------------------------------------\n\nTITLE: Creating HNSW Vector Index on Embeddings in Postgres SQL\nDESCRIPTION: Creates a vector index on the embedding column of the documents table using the HNSW (Hierarchical Navigable Small World) index type and the inner product operator for efficient semantic search queries. This SQL snippet should be executed after the documents table is created, and relies on pgvector support. The presence of this index dramatically improves search speed for high-dimensional vector queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncreate index on documents using hnsw (embedding vector_ip_ops);\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings Using OpenAI Embedding Models in Python\nDESCRIPTION: Creates dense vector embeddings of input text data by invoking OpenAI's embedding endpoint with the 'text-embedding-ada-002' model. The snippet shows how to pass a list of sentences and receive back their multidimensional vector representations in the 'data' field, which are essential for semantic similarity computations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nembed_model = \"text-embedding-ada-002\"\n\nres = openai.Embedding.create(\n    input=[\n        \"Sample document text goes here\",\n        \"there will be several phrases in each batch\"\n    ], engine=embed_model\n)\n```\n\n----------------------------------------\n\nTITLE: Calling Chat Completions API with Functions (Python)\nDESCRIPTION: Constructs the initial message history (system and user) and sends it to the `client.chat.completions.create` endpoint. It passes the defined `functions` list, allowing the model to determine if it should call one of the functions based on the user's query. The API response is stored for subsequent processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"},\n    {\"role\": \"user\", \"content\": \"What's the weather like today in Seattle?\"}\n]\n\nchat_completion = client.chat.completions.create(\n    model=deployment,\n    messages=messages,\n    tools=functions,\n)\nprint(chat_completion)\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embedding Using OpenAI API in JavaScript\nDESCRIPTION: Uses the OpenAI SDK to create a text embedding for a supplied string using the text-embedding-3-small model. Requires a valid OpenAI API key (set via environment variable or code), and OpenAI SDK properly imported/initialized. Input is a text string; output is the embedding vector. Usage is asynchronous and returns a result object from which the embedding is destructured.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = new OpenAI();\n\nconst input = \"The cat chases the mouse\";\n\nconst result = await openai.embeddings.create({\n  input,\n  model: \"text-embedding-3-small\",\n});\n\nconst [{ embedding }] = result.data;\n```\n\n----------------------------------------\n\nTITLE: Defining Placeholder Variables for Synthetic Data Generation (Python)\nDESCRIPTION: These Python variables define placeholders ('fill_in_int', 'fill_in_string') used during the generation of synthetic function call data. They represent integer and string parameter values that will be filled in later, potentially by another model like gpt-4o, to create realistic function invocations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplaceholder_int = \"fill_in_int\"\nplaceholder_string = \"fill_in_string\"\n```\n\n----------------------------------------\n\nTITLE: Batch Processing and Upserting Text Embeddings to Pinecone Index in Python\nDESCRIPTION: Performs batched embedding of the merged dataset texts and uploads vectors to the Pinecone index. It handles batches of 100 samples, generates embeddings using OpenAI API with retry-on-failure for rate limits, cleans and prepares metadata, then upserts vectors with their metadata to enable fast vector similarity search later.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\nfrom time import sleep\n\nbatch_size = 100  # how many embeddings we create and insert at once\n\nfor i in tqdm(range(0, len(new_data), batch_size)):\n    # find end of batch\n    i_end = min(len(new_data), i+batch_size)\n    meta_batch = new_data[i:i_end]\n    # get ids\n    ids_batch = [x['id'] for x in meta_batch]\n    # get texts to encode\n    texts = [x['text'] for x in meta_batch]\n    # create embeddings (try-except added to avoid RateLimitError)\n    done = False\n    while not done:\n        try:\n            res = openai.Embedding.create(input=texts, engine=embed_model)\n            done = True\n        except:\n            sleep(5)\n    embeds = [record['embedding'] for record in res['data']]\n    # cleanup metadata\n    meta_batch = [{\n        'start': x['start'],\n        'end': x['end'],\n        'title': x['title'],\n        'text': x['text'],\n        'url': x['url'],\n        'published': x['published'],\n        'channel_id': x['channel_id']\n    } for x in meta_batch]\n    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n    # upsert to Pinecone\n    index.upsert(vectors=to_upsert)\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment\nDESCRIPTION: This command uses the `venv` module to create a new virtual environment named 'openai-env' in the current directory. A virtual environment helps isolate project dependencies.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython -m venv openai-env\n```\n\n----------------------------------------\n\nTITLE: Defining Moderation Prompts and Criteria for Guardrails in Python\nDESCRIPTION: This snippet sets up the domain, moderation criteria, assessment steps, and system prompt for the moderation assistant as formatted multiline Python strings. Dependencies include Python 3+ and access to the OpenAI API for further code. Key parameters specify the content domain and criteria for scoring user content, assuming subsequent formatting with provided data. The templates guide the scoring process in downstream moderation routines.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndomain = \"animal breed recommendation\"\n\nanimal_advice_criteria = \"\"\"\nAssess the presence of explicit recommendation of cat or dog breeds in the content.\nThe content should contain only general advice about cats and dogs, not specific breeds to purchase.\"\"\"\n\nanimal_advice_steps = \"\"\"\n1. Read the content and the criteria carefully.\n2. Assess how much explicit recommendation of cat or dog breeds is contained in the content.\n3. Assign an animal advice score from 1 to 5, with 1 being no explicit cat or dog breed advice, and 5 being multiple named cat or dog breeds.\n\"\"\"\n\nmoderation_system_prompt = \"\"\"\nYou are a moderation assistant. Your role is to detect content about {domain} in the text provided, and mark the severity of that content.\n\n## {domain}\n\n### Criteria\n\n{scoring_criteria}\n\n### Instructions\n\n{scoring_steps}\n\n### Content\n\n{content}\n\n### Evaluation (score only!)\n\"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Client in Python\nDESCRIPTION: Sets up the Pinecone client with an API key to establish a connection to the vector database service. This is required before creating or accessing any index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pinecone import Pinecone\n\npc = Pinecone(api_key=\"...\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Customer Service Routine with Tools in Python\nDESCRIPTION: Sets up a customer service routine by defining a `system_message` containing multi-step instructions for an LLM agent. It also defines two Python functions, `look_up_item` (simulating an item lookup by returning a hardcoded ID) and `execute_refund` (simulating a refund process by printing details), which will serve as tools for the agent.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Customer Service Routine\n\nsystem_message = (\n    \"You are a customer support agent for ACME Inc.\"\n    \"Always answer in a sentence or less.\"\n    \"Follow the following routine with the user:\"\n    \"1. First, ask probing questions and understand the user's problem deeper.\\n\"\n    \" - unless the user has already provided a reason.\\n\"\n    \"2. Propose a fix (make one up).\\n\"\n    \"3. ONLY if not satisfied, offer a refund.\\n\"\n    \"4. If accepted, search for the ID and then execute refund.\"\n    \"\"\n)\n\ndef look_up_item(search_query):\n    \"\"\"Use to find item ID.\n    Search query can be a description or keywords.\"\"\"\n\n    # return hard-coded item ID - in reality would be a lookup\n    return \"item_132612938\"\n\n\ndef execute_refund(item_id, reason=\"not provided\"):\n\n    print(\"Summary:\", item_id, reason) # lazy summary\n    return \"success\"\n```\n\n----------------------------------------\n\nTITLE: Defining Custom GPT Q&A Search-Aware Behavior - Python\nDESCRIPTION: Specifies custom instructions for a GPT Q&A assistant to use an API action that retrieves documents based on a searchTerm keyword. The instructions guide the agent on how to interpret action responses (including errors or empty results), how to perform iterative searches, and when to escalate to the user if no information is found. No Python code execution occurs; instead, it is assumed to be copied into a configuration/settings panel for a custom GPT agent. No dependencies. Inputs: action result (text or file response). Outputs: conversational guidance in user replies. Limitations: meant for configuration in a GPT assistant, not for direct script execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nYou are a Q&A helper that helps answer users questions. You have access to a documents repository through your API action. When a user asks a question, you pass in the \"searchTerm\" a single keyword or term you think you should use for the search.\n\n****\n\nScenario 1: There are answers\n\nIf your action returns results, then you take the results from the action and try to answer the users question. \n\n****\n\nScenario 2: No results found\n\nIf the response you get from the action is \"No results found\", stop there and let the user know there were no results and that you are going to try a different search term, and explain why. You must always let the user know before conducting another search.\n\nExample:\n\n****\n\nI found no results for \"DEI\". I am now going to try [insert term] because [insert explanation]\n\n****\n\nThen, try a different searchTerm that is similar to the one you tried before, with a single word. \n\nTry this three times. After the third time, then let the user know you did not find any relevant documents to answer the question, and to check SharePoint. \nBe sure to be explicit about what you are searching for at each step.\n\n****\n\nIn either scenario, try to answer the user's question. If you cannot answer the user's question based on the knowledge you find, let the user know and ask them to go check the HR Docs in SharePoint. \n```\n\n----------------------------------------\n\nTITLE: Waiting for Run Completion\nDESCRIPTION: This code executes the `wait_on_run` function to wait for the completion of a Run. It calls `wait_on_run` with the Run and Thread objects, which continuously checks the status until it changes from 'queued' or 'in_progress'. Finally, it displays the Run's details using `show_json` after the completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrun = wait_on_run(run, thread)\nshow_json(run)\n```\n\n----------------------------------------\n\nTITLE: Batch Embedding with Retry and Exponential Backoff using Tenacity and OpenAI API in Python\nDESCRIPTION: This snippet introduces a robust method for generating embeddings with rate limit resilience by wrapping the embedding request in a function decorated with tenacity's retry operator, using random exponential backoff and a limited number of retries. It requires both tenacity and openai packages, and handles transient failures automatically. The function accepts a text string and returns its embedding vector, printing its length as an example; limitations include retry attempts capped to six, with wait times capped at 20 seconds.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_embeddings.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Best practice\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nfrom openai import OpenAI\nclient = OpenAI()\n\n# Retry up to 6 times with exponential backoff, starting at 1 second and maxing out at 20 seconds delay\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\ndef get_embedding(text: str, model=\"text-embedding-3-small\") -> list[float]:\n    return client.embeddings.create(input=[text], model=model).data[0].embedding\n\nembedding = get_embedding(\"Your text goes here\", model=\"text-embedding-3-small\")\nprint(len(embedding))\n\n```\n\n----------------------------------------\n\nTITLE: Upserting Data Points into Qdrant Collection\nDESCRIPTION: This Python code ingests the loaded data from the Pandas DataFrame into the 'Articles' collection in Qdrant using the `upsert` method. Each row in the DataFrame becomes a `PointStruct` with a unique ID, the corresponding 'title' and 'content' vectors, and the original row data as payload.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclient.upsert(\n    collection_name=\"Articles\",\n    points=[\n        rest.PointStruct(\n            id=k,\n            vector={\n                \"title\": v[\"title_vector\"],\n                \"content\": v[\"content_vector\"],\n            },\n            payload=v.to_dict(),\n        )\n        for k, v in article_df.iterrows()\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text Chunk using OpenAI Chat API - Python\nDESCRIPTION: This function applies a template prompt to a text chunk and sends it to the OpenAI Chat Completions API to generate a summary or extract information from the chunk.\n\nRequired Dependencies: Requires an initialized OpenAI client (`client`) and a constant for the model name (`GPT_MODEL`).\n\nParameters:\n- `content` (str): The text chunk to be processed.\n- `template_prompt` (str): The prompt string to prepend to the content.\n\nInputs: A text chunk and a prompt string.\nOutputs: The content of the generated message from the OpenAI API response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef extract_chunk(content, template_prompt):\n    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n    prompt = template_prompt + content\n    response = client.chat.completions.create(\n        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Preparing Tool Calling with Function Response and Assistant Interpretation (JSON Format)\nDESCRIPTION: This snippet demonstrates how to fine-tune a model to handle both tool invocation and response interpretation. The example contains an extended interaction: the user asks a question, the assistant makes a tool call, a tool role provides the numeric result, and then the assistant interprets the output. The 'tools' array must be consistent with the chosen tool schema. This supports custom model behavior when processing tool/function outputs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"},\n        {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_id\", \"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"}}]},\n        {\"role\": \"tool\", \"tool_call_id\": \"call_id\", \"content\": \"21.0\"},\n        {\"role\": \"assistant\", \"content\": \"It is 21 degrees celsius in San Francisco, CA\"}\n    ],\n    \"tools\": [...] // same as before\n}\n```\n\n----------------------------------------\n\nTITLE: Truncating Text to Max Tokens Using Tiktoken in Python\nDESCRIPTION: Provides a function to encode a string into tokens and truncate it to a maximum token length, ensuring compatibility with model input restrictions. Uses the tiktoken library for encoding and truncation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport tiktoken\n\ndef truncate_text_tokens(text, encoding_name=EMBEDDING_ENCODING, max_tokens=EMBEDDING_CTX_LENGTH):\n    \"\"\"Truncate a string to have `max_tokens` according to the given encoding.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    return encoding.encode(text)[:max_tokens]\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI API - Python\nDESCRIPTION: This code interacts with the OpenAI API using the defined messages to generate responses. It specifies the model to use, along with various parameters like temperature and the number of responses to generate. The result is a series of generated question-answer pairings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=messages,\n    temperature=0.7,\n    n=5\n)\n\nfor choice in completion.choices:\n    print(choice.message.content + \"\\n\")\n\n```\n\n----------------------------------------\n\nTITLE: Orchestrating LLM Agent Execution with Langchain in Python\nDESCRIPTION: Assembles an LLM-powered agent using Langchain's ChatOpenAI interface, LLM chains, and a custom output parser. The code initializes the LLM, creates a prompt-based LLMChain, and configures an LLMSingleActionAgent with allowed tool names and a stop sequence. Dependencies include Langchain, an OpenAI-compatible API key, and previously defined prompt, tools, and output_parser objects. Expected inputs are user queries, and outputs are agent-completed actions via AgentExecutor. Ensure all objects (llm, tools, prompt, output_parser) are properly instantiated before execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n\n\nllm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n\n# LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\n# Using tools, the LLM chain and output_parser to make an agent\ntool_names = [tool.name for tool in tools]\n\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=output_parser,\n    stop=[\"\\Observation:\"], \n    allowed_tools=tool_names\n)\n\n\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Improving GPT-3.5 Reliability with 'Step-by-Step' Prompting\nDESCRIPTION: This snippet demonstrates how adding the simple instruction 'Let's think step by step.' to the prompt enables gpt-3.5-turbo-instruct to correctly solve the math word problem previously answered incorrectly. The following snippet shows the model's detailed reasoning process and the correct final answer, highlighting the effectiveness of this prompting technique for improving reliability on complex or multi-step tasks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_1\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nQ: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA: Let's think step by step.\n```\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nThere are 16 balls in total.\nHalf of the balls are golf balls.\nThat means that there are 8 golf balls.\nHalf of the golf balls are blue.\nThat means that there are 4 blue golf balls.\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Fetch Functionality in Node.js for OpenAI Chat API\nDESCRIPTION: This Node.js code illustrates integrating a custom weather-fetch function into a chat conversation with the OpenAI API. It defines a function to return weather data based on location input, manages conversation messages, specifies available functions, initiates API requests, handles function call requests from the model, calls the function dynamically, and updates conversation state. It demonstrates asynchronous API handling and dynamic function invocation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling.txt#_snippet_2\n\nLANGUAGE: node.js\nCODE:\n```\nconst openai = new OpenAI();\n\n// Example dummy function hard coded to return the same weather\n// In production, this could be your backend API or an external API\nfunction getCurrentWeather(location, unit = \"fahrenheit\") {\n  if (location.toLowerCase().includes(\"tokyo\")) {\n    return JSON.stringify({ location: \"Tokyo\", temperature: \"10\", unit: \"celsius\" });\n  } else if (location.toLowerCase().includes(\"san francisco\")) {\n    return JSON.stringify({ location: \"San Francisco\", temperature: \"72\", unit: \"fahrenheit\" });\n  } else if (location.toLowerCase().includes(\"paris\")) {\n    return JSON.stringify({ location: \"Paris\", temperature: \"22\", unit: \"fahrenheit\" });\n  } else {\n    return JSON.stringify({ location, temperature: \"unknown\" });\n  }\n}\n\nasync function runConversation() {\n  // Step 1: send the conversation and available functions to the model\n  const messages = [\n    { role: \"user\", content: \"What's the weather like in San Francisco, Tokyo, and Paris?\" },\n  ];\n  const tools = [\n    {\n      type: \"function\",\n      function: {\n        name: \"get_current_weather\",\n        description: \"Get the current weather in a given location\",\n        parameters: {\n          type: \"object\",\n          properties: {\n            location: {\n              type: \"string\",\n              description: \"The city and state, e.g. San Francisco, CA\",\n            },\n            unit: { type: \"string\", enum: [\"celsius\", \"fahrenheit\"] },\n          },\n          required: [\"location\"],\n        },\n      },\n    },\n  ];\n\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: messages,\n    tools: tools,\n    tool_choice: \"auto\" // auto is default, but we'll be explicit\n  });\n  const responseMessage = response.choices[0].message;\n  // Step 2: check if the model wanted to call a function\n  const toolCalls = responseMessage.tool_calls;\n  if (responseMessage.tool_calls) {\n    // Step 3: call the function\n    // Note: the JSON response may not always be valid; be sure to handle errors\n    const availableFunctions = {\n      get_current_weather: getCurrentWeather,\n    }; // only one function in this example, but you can have multiple\n    messages.push(responseMessage); // extend conversation with assistant's reply\n    for (const toolCall of toolCalls) {\n      const functionName = toolCall.function.name;\n      const functionToCall = availableFunctions[functionName];\n      const functionArgs = JSON.parse(toolCall.function.arguments);\n      const functionResponse = functionToCall(\n        functionArgs.location,\n        functionArgs.unit\n      );\n      messages.push({\n        tool_call_id: toolCall.id,\n        role: \"tool\",\n        name: functionName,\n        content: functionResponse,\n      }); // extend conversation with function response\n    }\n    const secondResponse = await openai.chat.completions.create({\n      model: \"gpt-4o\",\n      messages: messages,\n    }); // get a new response from the model where it can see the function response\n    return secondResponse.choices;\n  }\n}\n\nrunConversation().then(console.log).catch(console.error);\n```\n\n----------------------------------------\n\nTITLE: Initial Retrieval Check Prompt (example-chat)\nDESCRIPTION: An example LLM prompt that determines if a given user query requires performing a real-time lookup (retrieval) to be answered accurately. It outputs 'true' if retrieval is needed and 'false' otherwise.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_1\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: Given a user query, determine whether it requires doing a realtime lookup to\nrespond to.\n\n# Examples\nUser Query: \"How can I return this item after 30 days?\"\nResponse: \"true\"\n\nUser Query: \"Thank you!\"\nResponse: \"false\"\n\nUSER: [input user query here]\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Moderating LLM Input and Output using OpenAI Moderation API in Python\nDESCRIPTION: This asynchronous Python function `execute_all_moderations` coordinates checks for both user input and LLM-generated output using a `check_moderation_flag` function (assumed to call the OpenAI Moderation API). It uses `asyncio` to run input moderation and LLM response generation concurrently. If input is flagged, the LLM task is cancelled. If output is flagged, an alternative message is returned. Otherwise, the original LLM response is returned. Requires `asyncio`, `check_moderation_flag`, and `get_chat_response` functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def execute_all_moderations(user_request):\n    # Create tasks for moderation and chat response\n    input_moderation_task = asyncio.create_task(check_moderation_flag(user_request))\n    chat_task = asyncio.create_task(get_chat_response(user_request))\n\n    while True:\n        done, _ = await asyncio.wait(\n            [input_moderation_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n        )\n\n        # If input moderation is not completed, wait and continue to the next iteration\n        if input_moderation_task not in done:\n            await asyncio.sleep(0.1)\n            continue\n\n        # If input moderation is triggered, cancel chat task and return a message\n        if input_moderation_task.result() == True:\n            chat_task.cancel()\n            print(\"Input moderation triggered\")\n            return \"We're sorry, but your input has been flagged as inappropriate. Please rephrase your input and try again.\"\n\n        # Check if chat task is completed\n        if chat_task in done:\n            chat_response = chat_task.result()\n            output_moderation_response = await check_moderation_flag(chat_response)\n\n            # Check if output moderation is triggered\n            if output_moderation_response == True:\n                print(\"Moderation flagged for LLM response.\")\n                return \"Sorry, we're not permitted to give this answer. I can help you with any general queries you might have.\"\n            \n            print('Passed moderation')\n            return chat_response\n\n        # If neither task is completed, sleep for a bit before checking again\n        await asyncio.sleep(0.1)\n```\n\n----------------------------------------\n\nTITLE: Transcribing Hindi Audio to English with GPT-4o\nDESCRIPTION: This snippet demonstrates how to transcribe Hindi audio to English text using GPT-4o. It processes a base64-encoded audio file with a specific prompt instructing the model to transcribe Hindi audio to English text while ignoring background noises.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodalities = [\"text\"]\nprompt = \"The user will provide an audio file in Hindi. Transcribe the audio to English text word for word. Only provide the language transcription, do not include background noises such as applause. \"\n\nresponse_json = process_audio_with_gpt_4o(hindi_audio_data_base64, modalities, prompt)\n\nre_translated_english_text = response_json['choices'][0]['message']['content']\n\nprint(re_translated_english_text)\n```\n\n----------------------------------------\n\nTITLE: Invoking a Function Call Using OpenAI Chat Completions API in Python (Step 1)\nDESCRIPTION: This snippet builds an initial user prompt and invokes the OpenAI chat completions API to obtain a model response that may involve a function/tool call. It passes the set of available tools and sets the 'tool_choice' to 'auto' for model-directed tool selection. Requires the 'client', 'tools', and an initialized 'messages' list. Input is the initial user message; outputs are the API response and the model's tool-call message appended to messages. Response format depends on OpenAI's response structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n# Step #1: Prompt with content that may result in function call. In this case the model can identify the information requested by the user is potentially available in the database schema passed to the model in Tools description. \nmessages = [{\n    \"role\":\"user\", \n    \"content\": \"What is the name of the album with the most tracks?\"\n}]\n\nresponse = client.chat.completions.create(\n    model='gpt-4o', \n    messages=messages, \n    tools= tools, \n    tool_choice=\"auto\"\n)\n\n# Append the message to messages list\nresponse_message = response.choices[0].message \nmessages.append(response_message)\n\nprint(response_message)\n```\n\n----------------------------------------\n\nTITLE: Install Required Python Libraries\nDESCRIPTION: Installs the necessary Python packages using pip. This includes `openai` for generating embeddings, `pymilvus` for interacting with Zilliz, `datasets` for downloading the movie data, and `tqdm` for displaying progress bars during data processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! pip install openai pymilvus datasets tqdm\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Chat Completion with Azure OpenAI\nDESCRIPTION: Demonstrates how to create a basic chat completion using Azure OpenAI. Provides system, user, and assistant messages to establish context for the conversation, and prints the response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# For all possible arguments see https://platform.openai.com/docs/api-reference/chat-completions/create\nresponse = client.chat.completions.create(\n    model=deployment,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Orange.\"},\n    ],\n    temperature=0,\n)\n\nprint(f\"{response.choices[0].message.role}: {response.choices[0].message.content}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Storage Account in Python\nDESCRIPTION: This code snippet demonstrates how to create an Azure Storage Account using the Azure Resource Management and Storage Management SDKs for Python. It first creates a resource group if one doesn't exist. Then, it creates a new storage account within the specified resource group, using the provided SKU, kind, and location. The operation is asynchronous, so it waits for the result before printing the name of the created storage account.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n## Update below with a different name\nstorage_account_name = \"<enter-storage-account-name>\"\n\n## Use below SKU or any other SKU as per your requirement\nsku = \"Standard_LRS\"\nresource_client = ResourceManagementClient(credential, subscription_id)\nstorage_client = StorageManagementClient(credential, subscription_id)\n\n# Create resource group if it doesn't exist\nrg_result = resource_client.resource_groups.create_or_update(resource_group, {\"location\": region})\n\n# Create storage account\nstorage_async_operation = storage_client.storage_accounts.begin_create(\n    resource_group,\n    storage_account_name,\n    {\n        \"sku\": {\"name\": sku},\n        \"kind\": \"StorageV2\",\n        \"location\": region,\n    },\n)\nstorage_account = storage_async_operation.result()\n\nprint(f\"Storage account {storage_account.name} created\")\n```\n\n----------------------------------------\n\nTITLE: Processing DataFrame Columns\nDESCRIPTION: This snippet processes the DataFrame columns to convert string representations of vectors into lists and sets 'vector_id' column to string type. This prepares the data for insertion into the vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Generating Local Settings File - Python\nDESCRIPTION: This Python snippet creates a `local.settings.json` file required by Azure Functions Core Tools for local development. It uses an f-string to embed environment variables like `OPENAI_API_KEY`, `EMBEDDINGS_MODEL`, and `SEARCH_SERVICE_API_KEY` into the JSON structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlocal_settings_content = f\"\"\"\n{{\n  \"IsEncrypted\": false,\n  \"Values\": {{\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n    \"OPENAI_API_KEY\": \"{openai_api_key}\",\n    \"EMBEDDINGS_MODEL\": \"{embeddings_model}\",\n    \"SEARCH_SERVICE_API_KEY\": \"{search_service_api_key}\",\n  }}\n}}\n\"\"\"\n\nwith open(\"local.settings.json\", \"w\") as file:\n    file.write(local_settings_content)\n```\n\n----------------------------------------\n\nTITLE: Querying Pinecone Index with Embeddings\nDESCRIPTION: This snippet retrieves relevant contexts from a Pinecone index using vector similarity search. It queries the index with an embedding vector (xq) to find the top 2 most similar items, including their metadata.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nres = index.query(xq, top_k=2, include_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: Installs the `openai` library (version >=1.0.0, <2.0.0) for interacting with the OpenAI API and the `python-dotenv` library for managing environment variables using pip.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! pip install \"openai>=1.0.0,<2.0.0\"\n! pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Transforming and Batch Processing Invoice JSONs with GPT-4o in Python\nDESCRIPTION: This snippet provides two Python functions: one for using the OpenAI GPT-4o API to map raw invoice JSONs into the target schema (with translation to English and schema-conformant formatting) and one for batch processing a directory of invoice JSONs. It loads a schema, iterates through extracted JSON files, transforms each using GPT-4o, and saves results in a specified output directory. External dependencies include the OpenAI API (via a 'client' object), the 'os', 'json', and proper API key configuration. Input parameters specify source, schema, and save paths. The function returns transformed JSONs with formatting and enforced constraints but presumes all input files are accessible and well-formed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef transform_invoice_data(json_raw, json_schema):\n    system_prompt = f\"\"\"\n    You are a data transformation tool that takes in JSON data and a reference JSON schema, and outputs JSON data according to the schema.\n    Not all of the data in the input JSON will fit the schema, so you may need to omit some data or add null values to the output JSON.\n    Translate all data into English if not already in English.\n    Ensure values are formatted as specified in the schema (e.g. dates as YYYY-MM-DD).\n    Here is the schema:\n    {json_schema}\n\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_format={ \"type\": \"json_object\" },\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": f\"Transform the following raw JSON data according to the provided schema. Ensure all data is in English and formatted as specified by values in the schema. Here is the raw JSON: {json_raw}\"}\n                ]\n            }\n        ],\n        temperature=0.0,\n    )\n    return json.loads(response.choices[0].message.content)\n\n\n\ndef main_transform(extracted_invoice_json_path, json_schema_path, save_path):\n    # Load the JSON schema\n    with open(json_schema_path, 'r', encoding='utf-8') as f:\n        json_schema = json.load(f)\n\n    # Ensure the save directory exists\n    os.makedirs(save_path, exist_ok=True)\n\n    # Process each JSON file in the extracted invoices directory\n    for filename in os.listdir(extracted_invoice_json_path):\n        if filename.endswith(\".json\"):\n            file_path = os.path.join(extracted_invoice_json_path, filename)\n\n            # Load the extracted JSON\n            with open(file_path, 'r', encoding='utf-8') as f:\n                json_raw = json.load(f)\n\n            # Transform the JSON data\n            transformed_json = transform_invoice_data(json_raw, json_schema)\n\n            # Save the transformed JSON to the save directory\n            transformed_filename = f\"transformed_{filename}\"\n            transformed_file_path = os.path.join(save_path, transformed_filename)\n            with open(transformed_file_path, 'w', encoding='utf-8') as f:\n                json.dump(transformed_json, f, ensure_ascii=False, indent=2)\n\n   \n    extracted_invoice_json_path = \"./data/hotel_invoices/extracted_invoice_json\"\n    json_schema_path = \"./data/hotel_invoices/invoice_schema.json\"\n    save_path = \"./data/hotel_invoices/transformed_invoice_json\"\n\n    main_transform(extracted_invoice_json_path, json_schema_path, save_path)\n```\n\n----------------------------------------\n\nTITLE: Python Example: Implementing Parallel Function Calls with OpenAI API\nDESCRIPTION: This code snippet demonstrates a basic Python setup for using OpenAI's API to perform parallel function calls, showcasing how to initialize the client and define functions for weather data retrieval within the context of function calling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport json\nclient = OpenAI()\n# Example dummy function hard coded to return the same weather\n```\n\n----------------------------------------\n\nTITLE: Optimizing Embedding Projection Matrix with PyTorch in Python\nDESCRIPTION: Implements the `optimize_matrix` function to train a projection matrix that transforms text embeddings to improve similarity prediction. It uses PyTorch for mini-batch gradient descent, minimizing Mean Squared Error (MSE) loss between predicted cosine similarities (of transformed embeddings) and ground truth labels. The function takes hyperparameters like embedding length, batch size, epochs, learning rate, and dropout, processes data from a Pandas DataFrame, splits into train/test sets, trains the matrix, logs performance metrics (loss, accuracy) per epoch, and optionally saves results to a CSV. Depends on `torch`, `numpy`, `pandas`, `random`, and the `apply_matrix_to_embeddings_dataframe` and `accuracy_and_se` functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef optimize_matrix(\n    modified_embedding_length: int = 2048,  # in my brief experimentation, bigger was better (2048 is length of babbage encoding)\n    batch_size: int = 100,\n    max_epochs: int = 100,\n    learning_rate: float = 100.0,  # seemed to work best when similar to batch size - feel free to try a range of values\n    dropout_fraction: float = 0.0,  # in my testing, dropout helped by a couple percentage points (definitely not necessary)\n    df: pd.DataFrame = df,\n    print_progress: bool = True,\n    save_results: bool = True,\n) -> torch.tensor:\n    \"\"\"Return matrix optimized to minimize loss on training data.\"\"\"\n    run_id = random.randint(0, 2 ** 31 - 1)  # (range is arbitrary)\n    # convert from dataframe to torch tensors\n    # e is for embedding, s for similarity label\n    def tensors_from_dataframe(\n        df: pd.DataFrame,\n        embedding_column_1: str,\n        embedding_column_2: str,\n        similarity_label_column: str,\n    ) -> Tuple[torch.tensor]:\n        e1 = np.stack(np.array(df[embedding_column_1].values))\n        e2 = np.stack(np.array(df[embedding_column_2].values))\n        s = np.stack(np.array(df[similarity_label_column].astype(\"float\").values))\n\n        e1 = torch.from_numpy(e1).float()\n        e2 = torch.from_numpy(e2).float()\n        s = torch.from_numpy(s).float()\n\n        return e1, e2, s\n\n    e1_train, e2_train, s_train = tensors_from_dataframe(\n        df[df[\"dataset\"] == \"train\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n    )\n    e1_test, e2_test, s_test = tensors_from_dataframe(\n        df[df[\"dataset\"] == \"test\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n    )\n\n    # create dataset and loader\n    dataset = torch.utils.data.TensorDataset(e1_train, e2_train, s_train)\n    train_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=True\n    )\n\n    # define model (similarity of projected embeddings)\n    def model(embedding_1, embedding_2, matrix, dropout_fraction=dropout_fraction):\n        e1 = torch.nn.functional.dropout(embedding_1, p=dropout_fraction)\n        e2 = torch.nn.functional.dropout(embedding_2, p=dropout_fraction)\n        modified_embedding_1 = e1 @ matrix  # @ is matrix multiplication\n        modified_embedding_2 = e2 @ matrix\n        similarity = torch.nn.functional.cosine_similarity(\n            modified_embedding_1, modified_embedding_2\n        )\n        return similarity\n\n    # define loss function to minimize\n    def mse_loss(predictions, targets):\n        difference = predictions - targets\n        return torch.sum(difference * difference) / difference.numel()\n\n    # initialize projection matrix\n    embedding_length = len(df[\"text_1_embedding\"].values[0])\n    matrix = torch.randn(\n        embedding_length, modified_embedding_length, requires_grad=True\n    )\n\n    epochs, types, losses, accuracies, matrices = [], [], [], [], []\n    for epoch in range(1, 1 + max_epochs):\n        # iterate through training dataloader\n        for a, b, actual_similarity in train_loader:\n            # generate prediction\n            predicted_similarity = model(a, b, matrix)\n            # get loss and perform backpropagation\n            loss = mse_loss(predicted_similarity, actual_similarity)\n            loss.backward()\n            # update the weights\n            with torch.no_grad():\n                matrix -= matrix.grad * learning_rate\n                # set gradients to zero\n                matrix.grad.zero_()\n        # calculate test loss\n        test_predictions = model(e1_test, e2_test, matrix)\n        test_loss = mse_loss(test_predictions, s_test)\n\n        # compute custom embeddings and new cosine similarities\n        apply_matrix_to_embeddings_dataframe(matrix, df)\n\n        # calculate test accuracy\n        for dataset in [\"train\", \"test\"]:\n            data = df[df[\"dataset\"] == dataset]\n            a, se = accuracy_and_se(data[\"cosine_similarity_custom\"], data[\"label\"])\n\n            # record results of each epoch\n            epochs.append(epoch)\n            types.append(dataset)\n            losses.append(loss.item() if dataset == \"train\" else test_loss.item())\n            accuracies.append(a)\n            matrices.append(matrix.detach().numpy())\n\n            # optionally print accuracies\n            if print_progress is True:\n                print(\n                    f\"Epoch {epoch}/{max_epochs}: {dataset} accuracy: {a:0.1%} Â± {1.96 * se:0.1%}\"\n                )\n\n    data = pd.DataFrame(\n        {\"epoch\": epochs, \"type\": types, \"loss\": losses, \"accuracy\": accuracies}\n    )\n    data[\"run_id\"] = run_id\n    data[\"modified_embedding_length\"] = modified_embedding_length\n    data[\"batch_size\"] = batch_size\n    data[\"max_epochs\"] = max_epochs\n    data[\"learning_rate\"] = learning_rate\n    data[\"dropout_fraction\"] = dropout_fraction\n    data[\n        \"matrix\"\n    ] = matrices  # saving every single matrix can get big; feel free to delete/change\n    if save_results is True:\n        data.to_csv(f\"{run_id}_optimization_results.csv\", index=False)\n\n    return data\n```\n\n----------------------------------------\n\nTITLE: Retrieving Messages After Run Completion\nDESCRIPTION: Retrieves and displays messages from a thread after a run completes. The code checks the run status, and if completed, it fetches all messages from the thread, allowing access to the conversation history between the user and assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/overview-without-streaming.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif run.status == 'completed': \n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n  print(messages)\nelse:\n  print(run.status)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nif (run.status === 'completed') {\n  const messages = await openai.beta.threads.messages.list(\n    run.thread_id\n  );\n  for (const message of messages.data.reverse()) {\n    console.log(`${message.role} > ${message.content[0].text.value}`);\n  }\n} else {\n  console.log(run.status);\n}\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads/thread_abc123/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\"\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client\nDESCRIPTION: This code snippet initializes the OpenAI client using the `openai` library. It retrieves the API key from the environment variable `OPENAI_API_KEY` or uses a placeholder if the environment variable is not set. The client is then used to interact with the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport os\n\nclient = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Displaying Extracted Search Arguments for All Inputs in Python\nDESCRIPTION: This loop applies the print_tool_call function to each example input, displaying the parsed result for user_input, context, and the corresponding tool call result. It outputs the extracted search parameters for a batch of queries. Requires proper completion of previous result assignment and access to the print_tool_call function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfor ex in example_inputs:\n    print_tool_call(ex['user_input'], ex['context'], ex['result'])\n\n```\n\n----------------------------------------\n\nTITLE: Defining Weather-Related Function Specifications for Chat Completion in Python\nDESCRIPTION: Creates a list named `tools` containing two function specification objects formatted as per the OpenAI Chat Completions API schema. These specify two callable functions: `get_current_weather` and `get_n_day_weather_forecast`. Each function has descriptive metadata and a JSON schema describing required parameters such as location, format, and forecast days. These specifications guide the GPT model to generate compliant function arguments during conversation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                },\n                \"required\": [\"location\", \"format\"],\n            },\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_n_day_weather_forecast\",\n            \"description\": \"Get an N-day weather forecast\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                    \"num_days\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The number of days to forecast\",\n                    }\n                },\n                \"required\": [\"location\", \"format\", \"num_days\"]\n            },\n        }\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake External OAuth Security Integration with Azure - SQL\nDESCRIPTION: This SQL statement creates or replaces a Snowflake security integration that enables authentication via an Azure external OAuth provider. It integrates Snowflake with Azure Entra ID or Active Directory by specifying the external OAuth type as 'AZURE', and requires the issuer URL, JWS key endpoint, and audience list that matches the Azure Application ID URI. Key parameters include EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM (used here as 'upn' for user principal name), and EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE (set to 'EMAIL_ADDRESS' for user mapping). Inputs are Azure OAuth endpoint details collected during Azure app registration, and outputs are a security integration object in Snowflake ready to be used by the middleware layer. Prerequisites: a provisioned Snowflake warehouse, an Azure app registration with delegated permissions set up, and proper configuration of all endpoints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE SECURITY INTEGRATION AZURE_OAUTH_INTEGRATION\n  TYPE = EXTERNAL_OAUTH\n  ENABLED = TRUE\n  EXTERNAL_OAUTH_TYPE = 'AZURE'\n  EXTERNAL_OAUTH_ISSUER = '<AZURE_AD_ISSUER>'\n  EXTERNAL_OAUTH_JWS_KEYS_URL = '<AZURE_AD_JWS_KEY_ENDPOINT>'\n  EXTERNAL_OAUTH_AUDIENCE_LIST = ('<SNOWFLAKE_APPLICATION_ID_URI>')\n  EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'upn'\n  EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'EMAIL_ADDRESS';\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Setup Environment (Python)\nDESCRIPTION: Imports all required Python libraries for the evaluation framework. It includes libraries for dataset loading, OpenAI API interaction, data processing (pandas, numpy), schema validation (pydantic), database interaction (sqlite3), printing, plotting, environment variable management (dotenv), and notebook display tools. It also loads environment variables (presumably API keys) from a .env file and loads the specified Hugging Face dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom datasets import load_dataset\nfrom openai import OpenAI\nimport pandas as pd\nimport pydantic\nimport os\nimport sqlite3\nfrom sqlite3 import Error\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom tqdm.notebook import tqdm\nfrom IPython.display import HTML, display\n\n# Loads key from local .env file to setup API KEY in env variables\n%reload_ext dotenv\n%dotenv\n    \nGPT_MODEL = 'gpt-4o'\ndataset = load_dataset(\"b-mc2/sql-create-context\")\n\nprint(dataset['train'].num_rows, \"rows\")\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion using .NET Library\nDESCRIPTION: Initialize the OpenAI .NET ChatClient and make a chat completion request. The client is configured with the model and API key, typically read from environment variables. This snippet demonstrates sending a single user message to the specified model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_6\n\nLANGUAGE: csharp\nCODE:\n```\nusing OpenAI.Chat;\n\nChatClient client = new(\"gpt-3.5-turbo\", Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nChatCompletion chatCompletion = client.CompleteChat(\n    [\n        new UserChatMessage(\"Say 'this is a test.'\"),\n    ]);\n```\n\n----------------------------------------\n\nTITLE: Initializing RetrievalQA Chain with Pinecone in Python\nDESCRIPTION: This snippet creates a RetrievalQA chain using LangChain, employing an OpenAI LLM with a fixed temperature and integrating a vector search retriever (such as Pinecone's docsearch.as_retriever()). Dependencies include langchain.chains, an initialized docsearch object, and the OpenAI LLM. The chain_type parameter determines the response aggregation strategy. The resulting object enables question answering over a knowledge base specified by the retriever.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nfrom langchain.chains import RetrievalQA\n\nretrieval_llm = OpenAI(temperature=0)\n\npodcast_retriever = RetrievalQA.from_chain_type(llm=retrieval_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Text from a Deep Lake Dataset in Python\nDESCRIPTION: Retrieves and displays the first three text samples from the loaded Deep Lake dataset using direct indexing and data extraction methods. This helps verify the dataset contents prior to further processing or embedding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nds[:3].text.data()[\"value\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Client with API Key (Python)\nDESCRIPTION: Configures the `openai.AzureOpenAI` client instance to use API key authentication. It retrieves the endpoint and API key from environment variables and sets the required API version. This block is conditional based on the `use_azure_active_directory` flag.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        api_key=api_key,\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Summarizing and Translating Step-by-Step (Prompt)\nDESCRIPTION: Illustrates how to structure complex instructions into explicit, numbered steps. The model is instructed to first summarize text provided in triple quotes, and then translate that summary into Spanish, adding specific prefixes to the output of each step. Requires text within triple quotes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_3\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: Use the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \"Summary: \".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \"Translation: \".\n\nUSER: \"\"\"insert text here\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Function Tool to Close Stripe Dispute in Python Using OpenAI Agents SDK\nDESCRIPTION: Implements an asynchronous function that closes an existing dispute on Stripe by its dispute ID. It returns the updated dispute object upon success or an empty dictionary if an error occurs, with logging of any Stripe-related exceptions. This tool facilitates automated dispute resolution workflows by enabling programmatic dispute closure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\nasync def close_dispute(dispute_id: str) -> dict:\n    \"\"\"\n    Close a Stripe dispute by ID. \n    Returns the dispute object on success or an empty dictionary on failure.\n    \"\"\"\n    try:\n        return stripe.Dispute.close(dispute_id)\n    except stripe.error.StripeError as e:\n        logger.error(f\"Stripe error occurred while closing dispute: {e}\")\n        return {}\n```\n\n----------------------------------------\n\nTITLE: Calling the Chat Completions API in Node.js\nDESCRIPTION: Demonstrates making an asynchronous call to the OpenAI Chat Completions API using the official Node.js library. It initializes the `OpenAI` client, defines the message history as an array of objects, specifies the model (`gpt-3.5-turbo`), uses `await openai.chat.completions.create` within an async function, and logs the resulting completion. Requires the `openai` Node.js package and an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_1\n\nLANGUAGE: node.js\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}],\n    model: \"gpt-3.5-turbo\",\n  });\n\n  console.log(completion.choices[0]);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Function Tool to Retrieve Stripe Payment Intent in Python Using OpenAI Agents SDK\nDESCRIPTION: Defines an asynchronous function that fetches a payment intent object from Stripe's API using a payment intent ID. It handles potential Stripe errors by logging them and returns an empty dictionary if retrieval fails. This function enables agents to access payment transaction details required for dispute investigation and resolution tasks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\nasync def retrieve_payment_intent(payment_intent_id: str) -> dict:\n    \"\"\"\n    Retrieve a Stripe payment intent by ID.\n    Returns the payment intent object on success or an empty dictionary on failure.\n    \"\"\"\n    try:\n        return stripe.PaymentIntent.retrieve(payment_intent_id)\n    except stripe.error.StripeError as e:\n        logger.error(f\"Stripe error occurred while retrieving payment intent: {e}\")\n        return {}\n```\n\n----------------------------------------\n\nTITLE: Adding User Message and Handling Initial Chat Response in Python\nDESCRIPTION: Adds an initial user question about PPO reinforcement learning to a `paper_conversation` object. It then calls `chat_completion_with_function_execution`, passing the conversation history and a list of `arxiv_functions`, to get the AI's response. The assistant's reply is stored, added back to the conversation history, and displayed using Markdown.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Add a user message\npaper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\nchat_response = chat_completion_with_function_execution(\n    paper_conversation.conversation_history, functions=arxiv_functions\n)\nassistant_message = chat_response.choices[0].message.content\npaper_conversation.add_message(\"assistant\", assistant_message)\ndisplay(Markdown(assistant_message))\n```\n\n----------------------------------------\n\nTITLE: Creating a ChatGPT Completion Request Wrapper in Python\nDESCRIPTION: Defines a helper function that sends conversational messages and optional function definitions to the OpenAI Chat API, specifying the GPT model to use. It facilitates automatic or explicit function call selection by the model, encapsulating the call to OpenAI's chat completions endpoint.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef chat_completion_request(messages, functions=None, function_call='auto', \n                            model_name=GPT_MODEL):\n    \n    if functions is not None:\n        return client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            tools=functions,\n            tool_choice=function_call)\n    else:\n        return client.chat.completions.create(\n            model=model_name,\n            messages=messages)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Azure OpenAI\nDESCRIPTION: Installs the necessary Python packages for working with Azure OpenAI, including the OpenAI client library and python-dotenv for environment variable management.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"openai>=1.0.0,<2.0.0\"\n! pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Quote Search Engine with Astra DB and OpenAI in Python\nDESCRIPTION: Defines a reusable function 'find_quote_and_author' that, given a text query, computes its embedding using OpenAI and finds the top n most similar quotes (optionally filtered by author or tags) from the Astra DB vector collection. The function builds filter clauses, executes a vector search, and returns a list of (quote, author) tuples. Requires client, embedding_model_name, and collection variables. Inputs: 'query_quote' (str), 'n' (int), optional 'author' (str or None), and optional 'tags' (list of str).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef find_quote_and_author(query_quote, n, author=None, tags=None):\n    query_vector = client.embeddings.create(\n        input=[query_quote],\n        model=embedding_model_name,\n    ).data[0].embedding\n    filter_clause = {}\n    if author:\n        filter_clause[\"author\"] = author\n    if tags:\n        filter_clause[\"tags\"] = {}\n        for tag in tags:\n            filter_clause[\"tags\"][tag] = True\n    #\n    results = collection.vector_find(\n        query_vector,\n        limit=n,\n        filter=filter_clause,\n        fields=[\"quote\", \"author\"]\n    )\n    return [\n        (result[\"quote\"], result[\"author\"])\n        for result in results\n    ]\n```\n\n----------------------------------------\n\nTITLE: Waiting and Printing Run Responses\nDESCRIPTION: This code waits for the completion of multiple runs and then prints the assistant's responses for each.  It calls `wait_on_run` to wait for each run to finish and uses `get_response` to retrieve the messages from each thread, finally displaying the assistant's messages for each thread using the `pretty_print` helper function. The `pretty_print` function formats the messages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n# Pretty printing helper\ndef pretty_print(messages):\n    print(\"# Messages\")\n    for m in messages:\n        print(f\"{m.role}: {m.content[0].text.value}\")\n    print()\n\n\n# Waiting in a loop\ndef wait_on_run(run, thread):\n    while run.status == \"queued\" or run.status == \"in_progress\":\n        run = client.beta.threads.runs.retrieve(\n            thread_id=thread.id,\n            run_id=run.id,\n        )\n        time.sleep(0.5)\n    return run\n\n\n# Wait for Run 1\nrun1 = wait_on_run(run1, thread1)\npretty_print(get_response(thread1))\n\n# Wait for Run 2\nrun2 = wait_on_run(run2, thread2)\npretty_print(get_response(thread2))\n\n# Wait for Run 3\nrun3 = wait_on_run(run3, thread3)\npretty_print(get_response(thread3))\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset with Pre-Computed Embeddings in Python\nDESCRIPTION: Loads a CSV file containing precomputed OpenAI embeddings for clothing items, then deserializes the embeddings from strings to Python lists using ast.literal_eval. The commented-out version is to be enabled if avoiding fresh embedding calculation. Prints a sample of the dataset and provides confirmation of successful loading. Input is a CSV with at least an 'embeddings' column as stringified lists; output is a DataFrame ready for matching and search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# styles_df = pd.read_csv('data/sample_clothes/sample_styles_with_embeddings.csv', on_bad_lines='skip')\n\n# # Convert the 'embeddings' column from string representations of lists to actual lists of floats\n# styles_df['embeddings'] = styles_df['embeddings'].apply(lambda x: ast.literal_eval(x))\n\nprint(styles_df.head())\nprint(\"Opened dataset successfully. Dataset has {} items of clothing along with their embeddings.\".format(len(styles_df)))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Azure AI Search Service API Key\nDESCRIPTION: Retrieves the primary API key for the newly created Azure AI Search service using the search_management_client. The API key is necessary for authenticating API requests to the search service.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Retrieve the admin keys for the search service\ntry:\n    response = search_management_client.admin_keys.get(\n        resource_group_name=resource_group,\n        search_service_name=search_service_name,\n    )\n    # Extract the primary API key from the response and save as a variable to be used later\n    search_service_api_key = response.primary_key\n    print(\"Successfully retrieved the API key.\")\nexcept Exception as e:\n    print(f\"Failed to retrieve the API key: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Diverse Search Queries Using GPT - Python\nDESCRIPTION: This snippet creates a prompting template to have GPT generate an array of search queries based on a user's question. It then uses json_gpt to retrieve a diverse list of queries for maximizing search recall. The resulting queries list is then augmented with the original user question for completeness. Requires the json_gpt helper and a valid OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nQUERIES_INPUT = f\"\"\"\nYou have access to a search API that returns recent news articles.\nGenerate an array of search queries that are relevant to this question.\nUse a variation of related keywords for the queries, trying to be as general as possible.\nInclude as many queries as you can think of, including and excluding terms.\nFor example, include queries like ['keyword_1 keyword_2', 'keyword_1', 'keyword_2'].\nBe creative. The more queries you include, the more likely you are to find relevant results.\n\nUser question: {USER_QUESTION}\n\nFormat: {{\"queries\": [\"query_1\", \"query_2\", \"query_3\"]}}\n\"\"\"\n\nqueries = json_gpt(QUERIES_INPUT)[\"queries\"]\n\n# Let's include the original question as well for good measure\nqueries.append(USER_QUESTION)\n\nqueries\n\n```\n\n----------------------------------------\n\nTITLE: Enforcing Asynchronous Guardrails with Moderation and Chat in Python\nDESCRIPTION: This snippet provides asynchronous functions for running moderation and topical guardrails on LLM chat responses using Python and the OpenAI API. Dependencies: Python 3.7+, OpenAI Python SDK, asyncio. Inputs include user chat responses; outputs are filtered or modified user-facing responses depending on moderation and topical guardrail scores. Thresholds are enforced by parsing model outputs and blocking or passing messages accordingly; exceptions are handled by early task cancellation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def moderation_guardrail(chat_response):\n    print(\"Checking moderation guardrail\")\n    mod_messages = [\n        {\"role\": \"user\", \"content\": moderation_system_prompt.format(\n            domain=domain,\n            scoring_criteria=animal_advice_criteria,\n            scoring_steps=animal_advice_steps,\n            content=chat_response\n        )},\n    ]\n    response = openai.chat.completions.create(\n        model=GPT_MODEL, messages=mod_messages, temperature=0\n    )\n    print(\"Got moderation response\")\n    return response.choices[0].message.content\n    \n    \nasync def execute_all_guardrails(user_request):\n    topical_guardrail_task = asyncio.create_task(topical_guardrail(user_request))\n    chat_task = asyncio.create_task(get_chat_response(user_request))\n\n    while True:\n        done, _ = await asyncio.wait(\n            [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n        )\n        if topical_guardrail_task in done:\n            guardrail_response = topical_guardrail_task.result()\n            if guardrail_response == \"not_allowed\":\n                chat_task.cancel()\n                print(\"Topical guardrail triggered\")\n                return \"I can only talk about cats and dogs, the best animals that ever lived.\"\n            elif chat_task in done:\n                chat_response = chat_task.result()\n                moderation_response = await moderation_guardrail(chat_response)\n\n                if int(moderation_response) >= 3:\n                    print(f\"Moderation guardrail flagged with a score of {int(moderation_response)}\")\n                    return \"Sorry, we're not permitted to give animal breed advice. I can help you with any general queries you might have.\"\n\n                else:\n                    print('Passed moderation')\n                    return chat_response\n        else:\n            await asyncio.sleep(0.1)  # sleep for a bit before checking the tasks again\n\n```\n\n----------------------------------------\n\nTITLE: Invoking Search and Printing Results\nDESCRIPTION: This code snippet loads the API keys and CSE ID from a .env file, calls the search function with the generated search term, and prints the link and snippet of each search result.  It relies on the `search` function defined previously and requires environment variables for the API key and CSE ID.  The output is the link and snippet information fetched from the search API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv('.env')\n\napi_key = os.getenv('API_KEY')\ncse_id = os.getenv('CSE_ID')\n\nsearch_items = search(search_item=search_term, api_key=api_key, cse_id=cse_id, search_depth=10, site_filter=\"https://openai.com\")\n\nfor item in search_items:\n    print(f\"Link: {item['link']}\")\n    print(f\"Snippet: {item['snippet']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Disabling Function Calls in GPT Chat Completion API in Python\nDESCRIPTION: Shows how to prevent the model from generating any function call by setting the `tool_choice` parameter to the string \"none\". This forces the model to provide a direct text response without invoking any of the defined function specifications in the tools parameter.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\nmessages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\nchat_response = chat_completion_request(\n    messages, tools=tools, tool_choice=\"none\"\n)\nchat_response.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: Defining Product Search Parameters and Calling OpenAI API Function in Python\nDESCRIPTION: This snippet defines prompt instructions, enumeration of product categories, a Pydantic model for product search parameters, and the get_response function that queries the OpenAI chat API for structured product search arguments extracted from user input. Dependencies include the openai and pydantic libraries. Required parameters are user_input (query string) and context (user profile/context). The function outputs model-suggested tool call arguments for downstream actions. Ensure API credentials and model constants are set. Limitations: database and client/model setup are external.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom enum import Enum\nfrom typing import Union\nimport openai\n\nproduct_search_prompt = '''\n    You are a clothes recommendation agent, specialized in finding the perfect match for a user.\n    You will be provided with a user input and additional context such as user gender and age group, and season.\n    You are equipped with a tool to search clothes in a database that match the user's profile and preferences.\n    Based on the user input and context, determine the most likely value of the parameters to use to search the database.\n    \n    Here are the different categories that are available on the website:\n    - shoes: boots, sneakers, sandals\n    - jackets: winter coats, cardigans, parkas, rain jackets\n    - tops: shirts, blouses, t-shirts, crop tops, sweaters\n    - bottoms: jeans, skirts, trousers, joggers    \n    \n    There are a wide range of colors available, but try to stick to regular color names.\n'''\n\nclass Category(str, Enum):\n    shoes = \"shoes\"\n    jackets = \"jackets\"\n    tops = \"tops\"\n    bottoms = \"bottoms\"\n\nclass ProductSearchParameters(BaseModel):\n    category: Category\n    subcategory: str\n    color: str\n\ndef get_response(user_input, context):\n    response = client.chat.completions.create(\n        model=MODEL,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(product_search_prompt)\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"CONTEXT: {context}\\n USER INPUT: {user_input}\"\n            }\n        ],\n        tools=[\n            openai.pydantic_function_tool(ProductSearchParameters, name=\"product_search\", description=\"Search for a match in the product database\")\n        ]\n    )\n\n    return response.choices[0].message.tool_calls\n\n```\n\n----------------------------------------\n\nTITLE: Creating Query Embedding for Semantic Search\nDESCRIPTION: Generates an embedding vector for a search query about the Great Depression using the same OpenAI model used for indexing. This query vector will be used to find semantically similar content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What caused the 1929 Great Depression?\"\n\nxq = client.embeddings.create(input=query, model=MODEL).data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Initializing Chroma Client and Collection with OpenAI Embedding Function in Python\nDESCRIPTION: Sets up a Chroma client instance with an embedding function based on OpenAI's text embeddings API. It creates a collection named 'scifact_corpus' which will automatically embed added documents. This enables indexed storage of document embeddings for efficient semantic search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\n# We initialize an embedding function, and provide it to the collection.\nembedding_function = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nchroma_client = chromadb.Client() # Ephemeral by default\nscifact_corpus_collection = chroma_client.create_collection(name='scifact_corpus', embedding_function=embedding_function)\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Agent System and Tools (Python)\nDESCRIPTION: This extensive snippet defines multiple distinct Agent types (Triage, Sales, Issues/Repairs) with tailored instructions and toolsets. It includes utility functions for specific tasks (escalate, order, lookup, refund) and crucially, includes functions designed *only* to return a different Agent object, demonstrating the handoff mechanism within a larger system structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef escalate_to_human(summary):\n    \"\"\"Only call this if explicitly asked to.\"\"\"\n    print(\"Escalating to human agent...\")\n    print(\"\\n=== Escalation Report ===\")\n    print(f\"Summary: {summary}\")\n    print(\"=========================\\n\")\n    exit()\n\n\ndef transfer_to_sales_agent():\n    \"\"\"User for anything sales or buying related.\"\"\"\n    return sales_agent\n\n\ndef transfer_to_issues_and_repairs():\n    \"\"\"User for issues, repairs, or refunds.\"\"\"\n    return issues_and_repairs_agent\n\n\ndef transfer_back_to_triage():\n    \"\"\"Call this if the user brings up a topic outside of your purview,\n    including escalating to human.\"\"\"\n    return triage_agent\n\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=(\n        \"You are a customer service bot for ACME Inc. \"\n        \"Introduce yourself. Always be very brief. \"\n        \"Gather information to direct the customer to the right department. \"\n        \"But make your questions subtle and natural.\"\n    ),\n    tools=[transfer_to_sales_agent, transfer_to_issues_and_repairs, escalate_to_human],\n)\n\n\ndef execute_order(product, price: int):\n    \"\"\"Price should be in USD.\"\"\"\n    print(\"\\n\\n=== Order Summary ===\")\n    print(f\"Product: {product}\")\n    print(f\"Price: ${price}\")\n    print(\"=================\\n\")\n    confirm = input(\"Confirm order? y/n: \").strip().lower()\n    if confirm == \"y\":\n        print(\"Order execution successful!\")\n        return \"Success\"\n    else:\n        print(\"Order cancelled!\")\n        return \"User cancelled order.\"\n\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=(\n        \"You are a sales agent for ACME Inc.\"\n        \"Always answer in a sentence or less.\"\n        \"Follow the following routine with the user:\"\n        \"1. Ask them about any problems in their life related to catching roadrunners.\\n\"\n        \"2. Casually mention one of ACME's crazy made-up products can help.\\n\"\n        \" - Don't mention price.\\n\"\n        \"3. Once the user is bought in, drop a ridiculous price.\\n\"\n        \"4. Only after everything, and if the user says yes, \"\n        \"tell them a crazy caveat and execute their order.\\n\"\n        \"\"\n    ),\n    tools=[execute_order, transfer_back_to_triage],\n)\n\n\ndef look_up_item(search_query):\n    \"\"\"Use to find item ID.\n    Search query can be a description or keywords.\"\"\"\n    item_id = \"item_132612938\"\n    print(\"Found item:\", item_id)\n    return item_id\n\n\ndef execute_refund(item_id, reason=\"not provided\"):\n    print(\"\\n\\n=== Refund Summary ===\")\n    print(f\"Item ID: {item_id}\")\n    print(f\"Reason: {reason}\")\n    print(\"=================\\n\")\n    print(\"Refund execution successful!\")\n    return \"success\"\n\n\nissues_and_repairs_agent = Agent(\n    name=\"Issues and Repairs Agent\",\n    instructions=(\n        \"You are a customer support agent for ACME Inc.\"\n        \"Always answer in a sentence or less.\"\n        \"Follow the following routine with the user:\"\n        \"1. First, ask probing questions and understand the user's problem deeper.\\n\"\n        \" - unless the user has already provided a reason.\\n\"\n        \"2. Propose a fix (make one up).\\n\"\n        \"3. ONLY if not satesfied, offer a refund.\\n\"\n        \"4. If accepted, search for the ID and then execute refund.\"\n        \"\"\n    ),\n    tools=[execute_refund, look_up_item, transfer_back_to_triage],\n)\n```\n\n----------------------------------------\n\nTITLE: Exploring Podcast Transcript DataFrame Contents in Python\nDESCRIPTION: Uses pandas to display the first rows of the processed podcasts dataset, providing an overview of the loaded transcript records. Requires the pandas library and a valid 'processed_podcasts' data object. The output is a tabular preview of transcripts and metadata for inspection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Have a look at the contents\npd.DataFrame(processed_podcasts).head()\n\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Dispute Retrieval and Processing Workflow Using Stripe API in Python\nDESCRIPTION: This asynchronous function retrieves dispute details linked to a specified PaymentIntent ID from the Stripe API, extracts relevant dispute and payment metadata, and initiates the triage agent workflow with the serialized dispute data. Necessary dependencies include the Stripe Python SDK for API interaction, the OpenAI Agents SDK Runner for executing the AI agent, and a logging mechanism for monitoring. The function handles cases where no disputes are found by logging a warning and returning None. On successful retrieval, it passes the dispute info as a JSON string to the triage agent, waits for workflow completion, logs the final output, and returns both the dispute metadata dictionary and the workflow result. It is designed to streamline and automate the initial dispute handling based on live Stripe data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def process_dispute(payment_intent_id, triage_agent):\n    \"\"\"Retrieve and process dispute data for a given PaymentIntent.\"\"\"\n    disputes_list = stripe.Dispute.list(payment_intent=payment_intent_id)\n    if not disputes_list.data:\n        logger.warning(\"No dispute data found for PaymentIntent: %s\", payment_intent_id)\n        return None\n    \n    dispute_data = disputes_list.data[0]\n    \n    relevant_data = {\n        \"dispute_id\": dispute_data.get(\"id\"),\n        \"amount\": dispute_data.get(\"amount\"),\n        \"due_by\": dispute_data.get(\"evidence_details\", {}).get(\"due_by\"),\n        \"payment_intent\": dispute_data.get(\"payment_intent\"),\n        \"reason\": dispute_data.get(\"reason\"),\n        \"status\": dispute_data.get(\"status\"),\n        \"card_brand\": dispute_data.get(\"payment_method_details\", {}).get(\"card\", {}).get(\"brand\")\n    }\n    \n    event_str = json.dumps(relevant_data)\n    # Pass the dispute data to the triage agent\n    result = await Runner.run(triage_agent, input=event_str)\n    logger.info(\"WORKFLOW RESULT: %s\", result.final_output)\n    \n    return relevant_data, result.final_output\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Loading Libraries in Python\nDESCRIPTION: Installs necessary Python packages including OpenAI SDK, transformers, and common data science libraries. Imports essential modules such as openai, pandas, numpy, json, and os. Also sets the OpenAI API key and initializes an OpenAI client instance using the specified GPT-4 model. This setup prepares the environment for subsequent data loading and API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload\n%pip install openai 'openai[datalib]' 'openai[embeddings]' transformers scikit-learn matplotlib plotly pandas scipy\n\n```\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\nCOMPLETIONS_MODEL = \"gpt-4\"\nos.environ[\"OPENAI_API_KEY\"] = \"<your-api-key>\"\nclient = openai.OpenAI()\n\n```\n\n----------------------------------------\n\nTITLE: Formatting Training Data\nDESCRIPTION: This code block formats the output of `create_commands` into the appropriate format for fine-tuning by converting the input arguments to a JSON string, parsing the output prompt into a JSON, creating the training data with \"system\", \"user\", and \"assistant\" roles and converting the training data into a list of dictionaries ready for the `fine_tune` job.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntraining_examples = []\n\nfor prompt in training_examples_unformatted:\n    # adjust formatting for training data specs\n\n    # if its not a dict, convert to dict\n    if type(prompt[\"Input\"]) != dict:\n        prompt[\"Input\"] = ast.literal_eval(prompt[\"Input\"])\n    prompt[\"Input\"][\"arguments\"] = json.dumps(prompt[\"Input\"][\"arguments\"])\n    try:\n        prompt[\"Prompt\"] = json.loads(prompt[\"Prompt\"])\n    except:\n        continue\n    for p in prompt[\"Prompt\"]:\n        print(p)\n        print(prompt[\"Input\"])\n        tool_calls = [\n            {\"id\": \"call_id\", \"type\": \"function\", \"function\": prompt[\"Input\"]}\n        ]\n        training_examples.append(\n            {\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": DRONE_SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": p},\n                    {\"role\": \"assistant\", \"tool_calls\": tool_calls},\n                ],\n                \"parallel_tool_calls\": False,\n                \"tools\": modified_function_list,\n            }\n        )\n```\n\n----------------------------------------\n\nTITLE: Removing descriptions from function list\nDESCRIPTION: This function removes the 'description' field from each function and its parameters. This prevents the description from being included in the training data, reducing the data size and preventing the model from outputting the descriptions in the final result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef remove_descriptions(function_list):\n    for function in function_list:\n        func = function[\"function\"]\n        if \"description\" in func:\n            del func[\"description\"]\n\n        params = func[\"parameters\"]\n        if \"properties\" in params:\n            for param in params[\"properties\"].values():\n                if \"description\" in param:\n                    del param[\"description\"]\n\n    return function_list\n```\n\n----------------------------------------\n\nTITLE: Exponential Backoff with Backoff\nDESCRIPTION: This code snippet demonstrates how to use the `backoff` library to implement exponential backoff for retrying requests that fail due to rate limits. The `@backoff.on_exception` decorator is used to automatically retry the `completions_with_backoff` function when a `openai.RateLimitError` is raised. The backoff is exponential, with a maximum time of 60 seconds and a maximum of 6 attempts. The `client.chat.completions.create` method is called within the function to generate a completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport backoff  # for exponential backoff\n\n@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=60, max_tries=6)\ndef completions_with_backoff(**kwargs):\n    return client.chat.completions.create(**kwargs)\n\n\ncompletions_with_backoff(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}])\n\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Azure AD for Azure OpenAI (Python)\nDESCRIPTION: This code authenticates to the Azure OpenAI service using Azure Active Directory (Azure AD). It imports `DefaultAzureCredential` and `get_bearer_token_provider` from the `azure.identity` library. It retrieves the endpoint from the environment variable `AZURE_OPENAI_ENDPOINT`. It constructs an `openai.AzureOpenAI` client, using the `get_bearer_token_provider` to handle the authentication process with Azure AD, using the service's resource URI.  This approach uses caching and token refreshing which is recommended over a static token.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\nif use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Processing Function Call from API Response (Python)\nDESCRIPTION: Includes a placeholder implementation of the `get_current_weather` function. It extracts the requested function call details (name and arguments) from the model's response and, if the name matches, executes the corresponding local Python function, passing the parsed arguments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef get_current_weather(request):\n    \"\"\"\n    This function is for illustrative purposes.\n    The location and unit should be used to determine weather\n    instead of returning a hardcoded response.\n    \"\"\"\n    location = request.get(\"location\")\n    unit = request.get(\"unit\")\n    return {\"temperature\": \"22\", \"unit\": \"celsius\", \"description\": \"Sunny\"}\n\nfunction_call = chat_completion.choices[0].message.tool_calls[0].function\nprint(function_call.name)\nprint(function_call.arguments)\n\nif function_call.name == \"get_current_weather\":\n    response = get_current_weather(json.loads(function_call.arguments))\n```\n\n----------------------------------------\n\nTITLE: Few-shot prompting with ChatGPT API\nDESCRIPTION: Shows how to use few-shot examples in the conversation history to teach the model a specific pattern for translating business jargon.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Documents for Claims in Python\nDESCRIPTION: Queries a corpus collection to retrieve the three most relevant documents for each claim based on embedding distance. The results include both documents and their distance scores for later filtering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclaim_query_result = scifact_corpus_collection.query(query_texts=claims, include=['documents', 'distances'], n_results=3)\n```\n\n----------------------------------------\n\nTITLE: Deploying Azure Function Middleware for Gong Call Transcript Retrieval in JavaScript\nDESCRIPTION: This snippet defines an Azure serverless HTTP function that accepts POST requests containing an array of Gong call IDs. It authenticates using a Gong API key to send requests to Gong's calls transcript and extensive details endpoints, then processes the responses into a consolidated transcript object with metadata for each call. The function validates input, handles potential errors with structured logging and response codes, and returns JSON-formatted transcript data including speaker names, call titles, durations, and URLs. Dependencies include '@azure/functions' and 'axios'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce_gong.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst { app } = require('@azure/functions');\nconst axios = require('axios');\n\n// Replace with your Gong API token\nconst GONG_API_BASE_URL = \"https://api.gong.io/v2\";\nconst GONG_API_KEY = process.env.GONG_API_KEY;\n\napp.http('callTranscripts', {\n    methods: ['POST'],\n    authLevel: 'function',\n    handler: async (request, context) => {        \n        try {            \n            const body = await request.json();\n            const callIds = body.callIds;\n\n            if (!Array.isArray(callIds) || callIds.length === 0) {\n                return {\n                    status: 400,\n                    body: \"Please provide call IDs in the 'callIds' array.\"\n                };\n            }\n\n            // Fetch call transcripts\n            const transcriptPayload = { filter: { callIds } };\n            const transcriptResponse = await axios.post(`${GONG_API_BASE_URL}/calls/transcript`, transcriptPayload, {\n                headers: {\n                    'Authorization': `Basic ${GONG_API_KEY}`,\n                    'Content-Type': 'application/json'\n                }\n            });\n\n            const transcriptData = transcriptResponse.data;\n\n            // Fetch extensive call details\n            const extensivePayload = {\n                filter: { callIds },\n                contentSelector: {\n                    exposedFields: { parties: true }\n                }\n            };\n\n            const extensiveResponse = await axios.post(`${GONG_API_BASE_URL}/calls/extensive`, extensivePayload, {\n                headers: {\n                    'Authorization': `Basic ${GONG_API_KEY}`,\n                    'Content-Type': 'application/json'\n                }\n            });\n\n            const extensiveData = extensiveResponse.data;\n\n            // Create a map of call IDs to metadata and speaker details\n            const callMetaMap = {};\n            extensiveData.calls.forEach(call => {\n                callMetaMap[call.metaData.id] = {\n                    title: call.metaData.title,\n                    started: call.metaData.started,\n                    duration: call.metaData.duration,\n                    url: call.metaData.url,\n                    speakers: {}\n                };\n\n                call.parties.forEach(party => {\n                    callMetaMap[call.metaData.id].speakers[party.speakerId] = party.name;\n                });\n            });\n\n            // Transform transcript data into content and include metadata\n            transcriptData.callTranscripts.forEach(call => {\n                const meta = callMetaMap[call.callId];\n                if (!meta) {\n                    throw new Error(`Metadata for callId ${call.callId} not found.`);\n                }\n\n                let content = '';\n                call.transcript.forEach(segment => {\n                    const speakerName = meta.speakers[segment.speakerId] || \"Unknown Speaker\";\n\n                    // Combine all sentences for the speaker into a paragraph\n                    const sentences = segment.sentences.map(sentence => sentence.text).join(' ');\n                    content += `${speakerName}: ${sentences}\\n\\n`; // Add a newline between speaker turns\n                });\n\n                // Add metadata and content to the call object\n                call.title = meta.title;\n                call.started = meta.started;\n                call.duration = meta.duration;\n                call.url = meta.url;\n                call.content = content;\n                \n                delete call.transcript;\n            });\n\n            // Return the modified transcript data\n            return {\n                status: 200,\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify(transcriptData)\n            };\n        } catch (error) {\n            context.log('[ERROR]', \"Error processing request:\", error);\n\n            return {\n                status: error.response?.status || 500,\n                body: {\n                    message: \"An error occurred while fetching or processing call data.\",\n                    details: error.response?.data || error.message\n                }\n            };\n        }\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio Files with OpenAI Whisper API in Python\nDESCRIPTION: This snippet demonstrates how to use the OpenAI Python library to transcribe an audio file by interacting with the Whisper-based speech-to-text endpoint. It initializes the OpenAI client, opens a local audio file (such as an mp3), and sends it for transcription using the 'whisper-1' model. The key parameter is 'file', the path to the audio file; the output includes the transcribed text. Required dependencies: openai Python package.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\naudio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\ntranscription = client.audio.transcriptions.create(\n  model=\"whisper-1\", \n  file=audio_file\n)\nprint(transcription.text)\n```\n\n----------------------------------------\n\nTITLE: Handling OpenAI API Errors in Python\nDESCRIPTION: This code snippet demonstrates how to handle different types of errors returned by the OpenAI API in Python, including APIError, APIConnectionError, and RateLimitError. It uses a try-except block to catch potential exceptions and print error messages to the console. It requires the openai library to be installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/error-codes.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nfrom openai import OpenAI\nclient = OpenAI()\n\ntry:\n  #Make your OpenAI API request here\n  response = client.completions.create(\n    prompt=\"Hello world\",\n    model=\"gpt-3.5-turbo-instruct\"\n  )\nexcept openai.APIError as e:\n  #Handle API error here, e.g. retry or log\n  print(f\"OpenAI API returned an API Error: {e}\")\n  pass\nexcept openai.APIConnectionError as e:\n  #Handle connection error here\n  print(f\"Failed to connect to OpenAI API: {e}\")\n  pass\nexcept openai.RateLimitError as e:\n  #Handle rate limit error (we recommend using exponential backoff)\n  print(f\"OpenAI API request exceeded rate limit: {e}\")\n  pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Retrieval Function with Context Building\nDESCRIPTION: This function handles the complete retrieval process. It creates an embedding for the query, retrieves relevant contexts from Pinecone, and builds a prompt that includes these contexts up to a specified token limit. The prompt is structured to guide the model to answer based on the provided contexts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlimit = 3750\n\ndef retrieve(query):\n    res = openai.Embedding.create(\n        input=[query],\n        engine=embed_model\n    )\n\n    # retrieve from Pinecone\n    xq = res['data'][0]['embedding']\n\n    # get relevant contexts\n    res = index.query(xq, top_k=3, include_metadata=True)\n    contexts = [\n        x['metadata']['text'] for x in res['matches']\n    ]\n\n    # build our prompt with the retrieved contexts included\n    prompt_start = (\n        \"Answer the question based on the context below.\\n\\n\"+\n        \"Context:\\n\"\n    )\n    prompt_end = (\n        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n    )\n    # append contexts until hitting limit\n    for i in range(1, len(contexts)):\n        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n                prompt_end\n            )\n            break\n        elif i == len(contexts)-1:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(contexts) +\n                prompt_end\n            )\n    return prompt\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI Python SDK\nDESCRIPTION: Demonstrates how to generate vector embeddings for a list of input texts using the OpenAI API client in Python (v1.0+). Instantiates the client with an API key, sets the embedding model, and retrieves results for two example sentences. Requires openai package v1+, model name, and valid API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI(api_key=OPENAI_API_KEY)\nembedding_model_name = \"text-embedding-3-small\"\n\nresult = client.embeddings.create(\n    input=[\n        \"This is a sentence\",\n        \"A second sentence\"\n    ],\n    model=embedding_model_name,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Function for LLM Query Entity Extraction in Python\nDESCRIPTION: Initializes an OpenAI client and defines a function `define_query` that takes a user prompt and an optional model name (defaults to 'gpt-4o'). This function sends the prompt along with the detailed `system_prompt` to the specified OpenAI chat model, explicitly requesting a JSON object as the response format (`response_format`). It returns the extracted entity information as a JSON string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n# Define the entities to look for\ndef define_query(prompt, model=\"gpt-4o\"):\n    completion = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        response_format= {\n            \"type\": \"json_object\"\n        },\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": prompt\n        }\n        ]\n    )\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Converting Python Functions to OpenAI Tool Schema in Python\nDESCRIPTION: Implements a helper function `function_to_schema` using Python's `inspect` module. This function takes a Python function object as input and generates a dictionary representing the OpenAI tool schema, including function name, description (extracted from the docstring), and parameter definitions with basic type mapping (str, int, float, bool, list, dict). It automatically identifies required parameters based on the function signature.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\n\ndef function_to_schema(func) -> dict:\n    type_map = {\n        str: \"string\",\n        int: \"integer\",\n        float: \"number\",\n        bool: \"boolean\",\n        list: \"array\",\n        dict: \"object\",\n        type(None): \"null\",\n    }\n\n    try:\n        signature = inspect.signature(func)\n    except ValueError as e:\n        raise ValueError(\n            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n        )\n\n    parameters = {}\n    for param in signature.parameters.values():\n        try:\n            param_type = type_map.get(param.annotation, \"string\")\n        except KeyError as e:\n            raise KeyError(\n                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n            )\n        parameters[param.name] = {\"type\": param_type}\n\n    required = [\n        param.name\n        for param in signature.parameters.values()\n        if param.default == inspect._empty\n    ]\n\n    return {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": func.__name__,\n            \"description\": (func.__doc__ or \"\").strip(),\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": parameters,\n                \"required\": required,\n            },\n        },\n    }\n```\n\n----------------------------------------\n\nTITLE: Scenario Prompt Example (Text)\nDESCRIPTION: This example demonstrates a scenario-based prompt, where the LLM is given a role to play. The prompt instructs the model to act as a system that extracts the author's name from given text. The model is expected to fulfill the assigned role and extract the author's name from the following quotation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nYour role is to extract the name of the author from any given text\n\nâ€œSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.â€\nâ€• Ted Chiang, Exhalation\n```\n\n----------------------------------------\n\nTITLE: Generating RAG Response with LLM\nDESCRIPTION: This code snippet generates a Retrieval-Augmented Generation (RAG) response using an LLM. It constructs a prompt instructing the LLM to provide a detailed response to the search query based on the provided search results in JSON format. The `client.chat.completions.create` method is used to interact with the LLM and retrieve the final response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json \n\nfinal_prompt = (\n    f\"The user will provide a dictionary of search results in JSON format for search query {search_term} Based on on the search results provided by the user, provide a detailed response to this query: **'{search_query}'**. Make sure to cite all the sources at the end of your answer.\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": final_prompt},\n        {\"role\": \"user\", \"content\": json.dumps(results)}],\n    temperature=0\n\n)\nsummary = response.choices[0].message.content\n\nprint(summary)\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with OpenAI Whisper API in Node.js\nDESCRIPTION: This snippet shows how to transcribe audio files using the OpenAI Node.js SDK and 'whisper-1' model. It sets up the OpenAI client, reads the audio file as a stream, and requests a transcription. The function prints the recognized text output. Dependencies include the 'openai' and 'fs' Node.js packages, and the audio file path is provided to 'fs.createReadStream'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_1\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream(\"/path/to/file/audio.mp3\"),\n    model: \"whisper-1\",\n  });\n  console.log(transcription.text);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries (Python)\nDESCRIPTION: This command installs the necessary Python packages for interacting with Redis, handling data, downloading resources, and using the OpenAI API. The required libraries are `redis` for Redis client functionality, `wget` for data downloading, `pandas` for data manipulation, and `openai` for potential OpenAI API calls (even if embeddings are pre-computed).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n! pip install redis wget pandas openai\n```\n\n----------------------------------------\n\nTITLE: Main User Message Handler for Multi-Agent System in Python\nDESCRIPTION: Implements the primary function that processes user queries, routes them to appropriate specialized agents, and maintains conversation state. This coordinates the entire multi-agent system operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Function to handle user input and triaging\ndef handle_user_message(user_query, conversation_messages=[]):\n    user_message = {\"role\": \"user\", \"content\": user_query}\n    conversation_messages.append(user_message)\n\n\n    messages = [{\"role\": \"system\", \"content\": triaging_system_prompt}]\n    messages.extend(conversation_messages)\n\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        temperature=0,\n        tools=triage_tools,\n    )\n\n    conversation_messages.append([tool_call.function for tool_call in response.choices[0].message.tool_calls])\n\n    for tool_call in response.choices[0].message.tool_calls:\n        if tool_call.function.name == 'send_query_to_agents':\n            agents = json.loads(tool_call.function.arguments)['agents']\n            query = json.loads(tool_call.function.arguments)['query']\n            for agent in agents:\n                if agent == \"Data Processing Agent\":\n                    handle_data_processing_agent(query, conversation_messages)\n                elif agent == \"Analysis Agent\":\n                    handle_analysis_agent(query, conversation_messages)\n                elif agent == \"Visualization Agent\":\n                    handle_visualization_agent(query, conversation_messages)\n\n    return conversation_messages\n```\n\n----------------------------------------\n\nTITLE: Initializing the LLM Agent, LangChain Chain, and Output Parser in Python\nDESCRIPTION: Initializes a ChatOpenAI LLM instance and constructs an LLMChain using a previously defined prompt. Aggregates available tool names, assembles a single-action agent with output_parser, and sets up the stop sequence for early termination on specific output. Dependencies are LangChain's LLMChain, agent classes, and properly configured prompt and tool objects. The agent is ready to process single-step tasks with tool integration and returns model-completed actions or answers depending on output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Initiate our LLM - default is 'gpt-3.5-turbo'\nllm = ChatOpenAI(temperature=0)\n\n# LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\n# Using tools, the LLM chain and output_parser to make an agent\ntool_names = [tool.name for tool in tools]\n\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=output_parser,\n    # We use \"Observation\" as our stop sequence so it will stop when it receives Tool output\n    # If you change your prompt template you'll need to adjust this as well\n    stop=[\"\\nObservation:\"], \n    allowed_tools=tool_names\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Prompt\nDESCRIPTION: This snippet defines a custom prompt template for the LLM. It instructs the model to provide a single-sentence answer if the information is available, and suggest a random song title if it does not know the answer.  The template includes placeholders for the context and question, which will be populated by Langchain. This allows customizing LLM behavior.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\n```\n\nLANGUAGE: python\nCODE:\n```\ncustom_prompt = \"\"\"\nUse the following pieces of context to answer the question at the end. Please provide\na short single-sentence summary answer only. If you don't know the answer or if it's \nnot present in given context, don't try to make up an answer, but suggest me a random \nunrelated song title I could listen to. \nContext: {context}\nQuestion: {question}\nHelpful Answer:\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Python Library Using Zsh Terminal\nDESCRIPTION: This snippet provides instructions to install the OpenAI Python library required for interacting with OpenAI models. It demonstrates how to use the pip package manager from a zsh shell or notebook cell to install the dependency. Installing this library is a prerequisite for running the code that calls the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_1\n\nLANGUAGE: zsh\nCODE:\n```\npip install openai\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Cosine Embedding in Graph Database using Python\nDESCRIPTION: Implements a similarity search by computing the embedding of a user prompt, then executing a Cypher query that returns products having cosine similarity above a threshold to the input embedding. It returns a list of matching products by 'id' and 'name'. Requires prior definition of 'create_embedding' and a connected graph database instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef similarity_search(prompt, threshold=0.8):\n    matches = []\n    embedding = create_embedding(prompt)\n    query = '''\n            WITH $embedding AS inputEmbedding\n            MATCH (p:Product)\n            WHERE gds.similarity.cosine(inputEmbedding, p.embedding) > $threshold\n            RETURN p\n            '''\n    result = graph.query(query, params={'embedding': embedding, 'threshold': threshold})\n    for r in result:\n        product_id = r['p']['id']\n        matches.append({\n            \"id\": product_id,\n            \"name\":r['p']['name']\n        })\n    return matches\n```\n\n----------------------------------------\n\nTITLE: Embedding Text safely and Chunking\nDESCRIPTION: This function safely generates embeddings for text, handling text longer than the maximum context length. It chunks the input text into tokens, generates embeddings for each chunk, and returns the embeddings and corresponding text chunks.  It uses `chunked_tokens`, `generate_embeddings` and `tiktoken` library. If no average flag is set, it returns the list of embeddings for each chunk.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef len_safe_get_embedding(text, model=embeddings_model, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING):\n    # Initialize lists to store embeddings and corresponding text chunks\n    chunk_embeddings = []\n    chunk_texts = []\n    # Iterate over chunks of tokens from the input text\n    for chunk in chunked_tokens(text, chunk_length=max_tokens, encoding_name=encoding_name):\n        # Generate embeddings for each chunk and append to the list\n        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n        # Decode the chunk back to text and append to the list\n        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n    # Return the list of chunk embeddings and the corresponding text chunks\n    return chunk_embeddings, chunk_texts\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text to Bullet Point Count (Prompt)\nDESCRIPTION: Shows how to instruct the model to summarize text within triple quotes into a specific number of bullet points (3 bullet points).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_7\n\nLANGUAGE: Prompt\nCODE:\n```\nUSER: Summarize the text delimited by triple quotes in 3 bullet points.\n\n\"\"\"insert text here\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Validating JSON Strings\nDESCRIPTION: This code validates a list of JSON strings. It checks each string in the `guardrail_outputs` to see if it is a valid JSON. It reports the number of valid and invalid JSONs in the outputs. Used to filter the guardrail outputs for valid processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndef is_valid_json(json_string):\n    try:\n        json.loads(json_string)\n        return True\n    except json.JSONDecodeError:\n        return False\n\nvalid_jsons = [json_str for json_str in guardrail_outputs if is_valid_json(json_str)]\ninvalid_jsons = [json_str for json_str in guardrail_outputs if not is_valid_json(json_str)]\n\nprint(\"Number of valid JSONs:\", len(valid_jsons))\nprint(\"Number of invalid JSONs:\", len(invalid_jsons))\n```\n\n----------------------------------------\n\nTITLE: Parsing and Displaying Batch Results in Python\nDESCRIPTION: Reads the JSONL result file, parses each line into a JSON object, and demonstrates how to match responses to input requests using the custom ID, then displays the movie title, description, and generated result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults = []\nwith open(result_file_name, 'r') as file:\n    for line in file:\n        json_object = json.loads(line.strip())\n        results.append(json_object)\n\n# Reading only the first results\nfor res in results[:5]:\n    task_id = res['custom_id']\n    index = task_id.split('-')[-1]\n    result = res['response']['body']['choices'][0]['message']['content']\n    movie = df.iloc[int(index)]\n    description = movie['Overview']\n    title = movie['Series_Title']\n    print(f\"TITLE: {title}\\nOVERVIEW: {description}\\n\\nRESULT: {result}\")\n    print(\"\\n\\n----------------------------\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Question Answering Function for Weaviate in Python\nDESCRIPTION: This snippet defines a Python function `qna` that takes a natural language `query` and a `collection_name` (Weaviate class name) as input. It constructs and executes a Weaviate query using the `with_ask` generative search feature, configured in the schema, to find answers within the `content` property of the specified collection. The function retrieves relevant properties, the answer details (`result`, `property`, positions), and the vector distance. It includes basic error handling for potential OpenAI API rate limits.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef qna(query, collection_name):\n    \n    properties = [\n        \"title\", \"content\", \"url\",\n        \"_additional { answer { hasAnswer property result startPosition endPosition } distance }\"\n    ]\n\n    ask = {\n        \"question\": query,\n        \"properties\": [\"content\"]\n    }\n\n    result = (\n        client.query\n        .get(collection_name, properties)\n        .with_ask(ask)\n        .with_limit(1)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute â€“ the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom GPT Instructions\nDESCRIPTION: Provides the natural language text to instruct the Custom GPT on how to respond when a user initiates the integration test. It directs the GPT to call the configured custom action and present the result received from that action to the user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nWhen the user asks you to test the integration, you will make a call to the custom action and display the results\n```\n\n----------------------------------------\n\nTITLE: Populating the SingleStoreDB table with data\nDESCRIPTION: This code snippet populates the 'winter_olympics_2022' table in SingleStoreDB with data from the pandas DataFrame. It prepares an INSERT statement, converts the DataFrame to a NumPy record array, and iterates over the rows in batches to insert the data. The JSON_ARRAY_PACK_F64 function is used to pack the embedding data for storage in the BLOB column.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n# Prepare the statement\nstmt = \"\"\"\n    INSERT INTO winter_wikipedia2.winter_olympics_2022 (\n        id,\n        text,\n        embedding\n    )\n    VALUES (\n        %s,\n        %s,\n        JSON_ARRAY_PACK_F64(%s)\n    )\n\"\"\"\n\n# Convert the DataFrame to a NumPy record array\nrecord_arr = df.to_records(index=True)\n\n# Set the batch size\nbatch_size = 1000\n\n# Iterate over the rows of the record array in batches\nfor i in range(0, len(record_arr), batch_size):\n    batch = record_arr[i:i+batch_size]\n    values = [(row[0], row[1], str(row[2])) for row in batch]\n    cur.executemany(stmt, values)\n```\n\n----------------------------------------\n\nTITLE: Displaying Product Text\nDESCRIPTION: This snippet displays the product_text column of the dataframe.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# check out one of the texts we will use to create semantic embeddings\ndf[\"product_text\"][0]\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from OpenAI Completions Response in Python\nDESCRIPTION: This Python snippet shows how to access the main generated text output from a parsed JSON response object obtained from the OpenAI Completions API. It retrieves the 'text' field from the first element of the 'choices' list.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nresponse['choices'][0]['text']\n```\n\n----------------------------------------\n\nTITLE: Scraping Web Page Content with BeautifulSoup\nDESCRIPTION: This function retrieves the content of a web page given its URL. It uses the `requests` library to fetch the HTML content and `BeautifulSoup` to parse it, removing script and style tags to extract only the relevant text. It truncates the scraped text to fit within a specified token limit.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\nTRUNCATE_SCRAPED_TEXT = 50000  # Adjust based on your model's context window\nSEARCH_DEPTH = 5\n\ndef retrieve_content(url, max_tokens=TRUNCATE_SCRAPED_TEXT):\n        try:\n            headers = {'User-Agent': 'Mozilla/5.0'}\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n\n            soup = BeautifulSoup(response.content, 'html.parser')\n            for script_or_style in soup(['script', 'style']):\n                script_or_style.decompose()\n\n            text = soup.get_text(separator=' ', strip=True)\n            characters = max_tokens * 4  # Approximate conversion\n            text = text[:characters]\n            return text\n        except requests.exceptions.RequestException as e:\n            print(f\"Failed to retrieve {url}: {e}\")\n            return None\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI API for Article Summary Using Pydantic Schema in Python\nDESCRIPTION: Defines a function that sends an article's text content to the OpenAI chat completion API with a specified temperature and messages, requesting the response to follow the 'ArticleSummary' Pydantic schema. Uses the SDK's 'parse' helper for automatic parsing and validation of the output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef get_article_summary(text: str):\n    completion = client.beta.chat.completions.parse(\n        model=MODEL,\n        temperature=0.2,\n        messages=[\n            {\"role\": \"system\", \"content\": dedent(summarization_prompt)},\n            {\"role\": \"user\", \"content\": text}\n        ],\n        response_format=ArticleSummary,\n    )\n\n    return completion.choices[0].message.parsed\n```\n\n----------------------------------------\n\nTITLE: Recommended Prompt Structure Template (Markdown/Text)\nDESCRIPTION: Provides a general template for structuring prompts for AI models, using Markdown-like section headers. It includes sections for Role, Instructions, Reasoning Steps, Output Format, Examples, and Context, recommending customization based on specific use cases.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_10\n\nLANGUAGE: Text\nCODE:\n```\n# Role and Objective\n\n# Instructions\n\n## Sub-categories for more detailed instructions\n\n# Reasoning Steps\n\n# Output Format\n\n# Examples\n## Example 1\n\n# Context\n\n# Final instructions and prompt to think step by step\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Schema for SharePoint Search API\nDESCRIPTION: This OpenAPI schema defines the endpoints, request parameters, and responses for the SharePoint Search API. It allows users to search SharePoint documents based on a provided query and search term. The API is designed to be used with custom GPTs and requires specifying the function app name, function name, and endpoint ID.  The server URL needs to be updated with the correct function app name, and the code parameter within the URL needs to be updated with the specific endpoint ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: SharePoint Search API\n  description: API for searching SharePoint documents.\n  version: 1.0.0\nservers:\n  - url: https://{your_function_app_name}.azurewebsites.net/api\n    description: SharePoint Search API server\npaths:\n  /{your_function_name}?code={enter your specific endpoint id here}:\n    post:\n      operationId: searchSharePoint\n      summary: Searches SharePoint for documents matching a query and term.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                query:\n                  type: string\n                  description: The full query to search for in SharePoint documents.\n                searchTerm:\n                  type: string\n                  description: A specific term to search for within the documents.\n      responses:\n        '200':\n          description: Search results\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  type: object\n                  properties:\n                    documentName:\n                      type: string\n                      description: The name of the document.\n                    snippet:\n                      type: string\n                      description: A snippet from the document containing the search term.\n                    url:\n                      type: string\n                      description: The URL to access the document.\n```\n\n----------------------------------------\n\nTITLE: Configuring ChatGPT Custom Instructions for Snowflake Integration - Python\nDESCRIPTION: This code snippet provides a formatted textual template to be pasted in the ChatGPT Custom GPT's Instructions panel, outlining a multi-step workflow for converting user questions to executable Snowflake SQL via the `runQuery` operation. No runtime Python dependency is required but context is given in Markdown code block for clarity. Assumptions include a specified warehouse and database if not provided by the user, and a role for query execution. The instructions emphasize schema discovery, query validation, sequential explanation, and sample dataset suggestion for onboarding. Expected input is user freeform text; output is a stepwise interaction and a custom SQL query. Notably, this template is instructional and not intended to be programmatically executed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n**Context**: You are an expert at writing Snowflake SQL queries. A user is going to ask you a question. \n\n**Instructions**:\n1. No matter the user's question, start by running `runQuery` operation using this query: \"SELECT column_name, table_name, data_type, comment FROM {database}.INFORMATION_SCHEMA.COLUMNS\" \n-- Assume warehouse = \"<insert your default warehouse here>\", database = \"<insert your default database here>\", unless the user provides different values \n2. Convert the user's question into a SQL statement that leverages the step above and run the `runQuery` operation on that SQL statement to confirm the query works. Add a limit of 100 rows\n3. Now remove the limit of 100 rows and return back the query for the user to see\n4. Use the <your_role> role when querying Snowflake\n5. Run each step in sequence. Explain what you are doing in a few sentences, run the action, and then explain what you learned. This will help the user understand the reason behind your workflow. \n\n**Additional Notes**: If the user says \"Let's get started\", explain that the user can provide a project or dataset, along with a question they want answered. If the user has no ideas, suggest that we have a sample flights dataset they can query - ask if they want you to query that\n```\n\n----------------------------------------\n\nTITLE: Creating and deleting schemas in Weaviate to define data structure\nDESCRIPTION: In this snippet, the schema is reset and configured to include entities like articles with specific properties. It specifies which properties should be vectorized and how they should be handled within Weaviate, tailored for article data such as title, content, and URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n```\n\n----------------------------------------\n\nTITLE: Defining ArXiv Agent Functions for OpenAI API - Python\nDESCRIPTION: This list defines the schema for functions (`get_articles`, `read_article_and_summarize`) that the OpenAI API can be informed about and potentially call. This structure is used by the OpenAI function calling feature to understand available tools and their parameters.\n\nDependencies: None intrinsic to this structure, but the functions defined here (`get_articles`, `read_article_and_summarize`) must be implemented elsewhere.\n\nContent: A list of dictionaries, each describing a function's `name`, `description`, and `parameters` using a JSON schema-like structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\narxiv_functions = [\n    {\n        \"name\": \"get_articles\",\n        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": f\"\"\"\n                            User query in JSON. Responses should be summarized and should include the article URL reference\n                            \"\"\",\n                }\n            },\n            \"required\": [\"query\"],\n        },\n    },\n    {\n        \"name\": \"read_article_and_summarize\",\n        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": f\"\"\"\n                            Description of the article in plain text based on the user's query\n                            \"\"\",\n                }\n            },\n            \"required\": [\"query\"],\n        },\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring max_tokens in OpenAI API Python Client\nDESCRIPTION: Defines a helper function that delegates parameters to the OpenAI Python client's chat completion method, enabling you to specify the max_tokens parameter in your API call. This lets you control the maximum token output size for the response, which is crucial to managing rate limits and accurately forecasting token usage. Requires OpenAI Python client (client.chat.completions.create) and must be provided with both model and messages (as shown); returns the API response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef completions_with_max_tokens(**kwargs):\n    return client.chat.completions.create(**kwargs)\n\n\ncompletions_with_max_tokens(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}], max_tokens=100)\n```\n\n----------------------------------------\n\nTITLE: Creating and Executing a Message and Run\nDESCRIPTION: This snippet adds a new message to a thread and then executes a new run. It uses `client.beta.threads.messages.create()` to append a new user message to the thread, and then `client.beta.threads.runs.create()` to start a new run based on the updated thread. It waits for the run to complete using the `wait_on_run` function and retrieves messages after the new user message, sorted in ascending order, using `client.beta.threads.messages.list()`. The function also uses the `show_json` function to display these objects.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Create a message to append to our thread\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"Could you explain this to me?\"\n)\n\n# Execute our run\nrun = client.beta.threads.runs.create(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n)\n\n# Wait for completion\nwait_on_run(run, thread)\n\n# Retrieve all the messages added after our last user message\nmessages = client.beta.threads.messages.list(\n    thread_id=thread.id, order=\"asc\", after=message.id\n)\nshow_json(messages)\n```\n\n----------------------------------------\n\nTITLE: Generating AI Response Using Retrieved Context\nDESCRIPTION: Retrieves the top matches from Pinecone and uses them as context for OpenAI's Responses API. This demonstrates how the RAG system enhances AI responses with domain-specific knowledge.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Retrieve and concatenate top 3 match contexts.\nmatches = index.query(\n    vector=[client.embeddings.create(input=query, model=MODEL).data[0].embedding],\n    top_k=3,\n    include_metadata=True\n)['matches']\n\ncontext = \"\\n\\n\".join(\n    f\"Question: {m['metadata'].get('Question', '')}\\nAnswer: {m['metadata'].get('Answer', '')}\"\n    for m in matches\n)\n# Use the context to generate a final answer.\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=f\"Provide the answer based on the context: {context} and the question: {query} as per the internal knowledge base\",\n)\nprint(\"\\nFinal Answer:\")\nprint(response.output_text)\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings with OpenAI API\nDESCRIPTION: Generates vector embeddings for sample text using OpenAI's text-embedding-3-small model. The embeddings represent text as dense vectors that capture semantic meaning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"text-embedding-3-small\"\n\nres = client.embeddings.create(\n    input=[\n        \"Sample document text goes here\",\n        \"there will be several phrases in each batch\"\n    ], model=MODEL\n)\nres\n```\n\n----------------------------------------\n\nTITLE: Guiding GPT-3.5 Reasoning with Task Decomposition (Correct Result)\nDESCRIPTION: This snippet demonstrates improving reasoning by providing `gpt-3.5-turbo-instruct` with a structured procedure. The model is asked to first identify relevant clues, then combine them to reason, and finally select the multiple-choice answer. This step-by-step approach leads the model to correctly deduce that Colonel Mustard was in the observatory with the candlestick (a) Yes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_3\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nUse the following clues to answer the following multiple-choice question, using the following procedure:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant\n(2) Second, combine the relevant clues to reason out the answer to the question\n(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)\n\nClues:\n1. Miss Scarlett was the only person in the lounge.\n2. The person with the pipe was in the kitchen.\n3. Colonel Mustard was the only person in the observatory.\n4. Professor Plum was not in the library nor the billiard room.\n5. The person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant:\n```\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nClue 1: Miss Scarlett was the only person in the lounge. This clue is not relevant.\nClue 2: The person with the pipe was in the kitchen. This clue is not relevant.\nClue 3: Colonel Mustard was the only person in the observatory. This clue is relevant.\nClue 4: Professor Plum was not in the library nor the billiard room. This clue is not relevant.\nClue 5: The person with the candlestick was in the observatory. This clue is relevant.\n\n(2) Second, combine the relevant clues to reason out the answer to the question:\nClue 3 and Clue 5 both say that the person with the candlestick was in the observatory. Colonel Mustard was the only person in the observatory, so Colonel Mustard was the person with the candlestick.\n\n(3) Third, write the final answer:\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n```\n\n----------------------------------------\n\nTITLE: Initializing Langchain GraphCypherQAChain in Python\nDESCRIPTION: Imports `GraphCypherQAChain` and `ChatOpenAI` from Langchain. It initializes a QA chain (`chain`) that connects an OpenAI chat model (`ChatOpenAI` with temperature 0 for deterministic output) to the Neo4j graph (`graph`). This chain is designed to translate natural language questions into Cypher queries and execute them against the graph. `verbose=True` enables logging of the generated Cypher queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import GraphCypherQAChain\nfrom langchain.chat_models import ChatOpenAI\n\nchain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Guiding Response Style with Few-Shot Example (Prompt)\nDESCRIPTION: Demonstrates the \"few-shot\" prompting technique by providing an example of a desired input-output pair (USER: and ASSISTANT:). The system instructs the model to maintain a consistent style based on the provided example when responding to subsequent user queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_4\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: Answer in a consistent style.\n\nUSER: Teach me about patience.\n\nASSISTANT: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n\nUSER: Teach me about the ocean.\n```\n\n----------------------------------------\n\nTITLE: Testing OpenAI API Key Environment Variable\nDESCRIPTION: This Python snippet imports the `os` module and checks if the `OPENAI_API_KEY` environment variable is set using `os.getenv()`. It prints a message indicating whether the key is found, which is crucial before attempting to use the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Creating Data Scientist Assistant (OpenAI Assistants API)\nDESCRIPTION: Creates an OpenAI Assistant instance configured to act as a data scientist. It is provided instructions on its role and task, specifies the model to use (gpt-4-1106-preview), enables the 'code_interpreter' tool for data analysis, and associates the previously uploaded financial data file with the Assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.create(\n  instructions=\"You are a data scientist assistant. When given data and a query, write the proper code and create the proper visualization\",\n  model=\"gpt-4-1106-preview\",\n  tools=[{\"type\": \"code_interpreter\"}],\n  file_ids=[file.id]\n)\n\n```\n\n----------------------------------------\n\nTITLE: Iterating and Displaying Multiple Article Summaries in Python\nDESCRIPTION: Loops through the list of ArticleSummary objects and calls the printing helper for each, formatting the output with article indexes and spacing for readability.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(len(summaries)):\n    print(f\"ARTICLE {i}\\n\")\n    print_summary(summaries[i])\n    print(\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Index Documents in Redis (Python)\nDESCRIPTION: This Python function `index_documents` is defined to load records from a pandas DataFrame into Redis Hashes. It iterates through each record, constructs a unique Redis key using a specified prefix and document ID, converts the vector lists into byte strings (FLOAT32 format) required by Redis vector fields, and then uses `client.hset` to store the document's data, including the byte vectors, as a HASH object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n    records = documents.to_dict(\"records\")\n    for doc in records:\n        key = f\"{prefix}:{str(doc['id'])}\"\n\n        # create byte vectors for title and content\n        title_embedding = np.array(doc[\"title_vector\"], dtype=np.float32).tobytes()\n        content_embedding = np.array(doc[\"content_vector\"], dtype=np.float32).tobytes()\n\n        # replace list of floats with byte vectors\n        doc[\"title_vector\"] = title_embedding\n        doc[\"content_vector\"] = content_embedding\n\n        client.hset(key, mapping = doc)\n```\n\n----------------------------------------\n\nTITLE: Applying Fine-Tuned Model to Generate Answers for a DataFrame in Python\nDESCRIPTION: This snippet shows how to apply the fine-tuned model to a DataFrame of questions and contexts, generating answers for each row. It uses a progress bar to track completion and stores the generated answers in a new column called 'ft_generated_answer'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf[\"ft_generated_answer\"] = df.progress_apply(answer_question, model=model_id, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment File for Python\nDESCRIPTION: Specifies how to configure the OpenAI API key by creating a .env file with the key. This file is read later by python-dotenv to authenticate OpenAI API requests. The key should be replaced with the user's own API key string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOPENAI_API_KEY=your_key\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Similarity Search Function\nDESCRIPTION: Defines a function to search for articles similar to a given query using vector similarity, with options to search by title or content vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef query_polardb(query, collection_name, vector_name=\"title_vector\", top_k=20):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=\"text-embedding-3-small\",\n    )[\"data\"][0][\"embedding\"]\n\n    # Convert the embedded_query to PostgreSQL compatible format\n    embedded_query_pg = \"[\" + \",\".join(map(str, embedded_query)) + \"]\"\n\n    # Create SQL query\n    query_sql = f\"\"\"\n    SELECT id, url, title, l2_distance({vector_name},'{embedded_query_pg}'::VECTOR(1536)) AS similarity\n    FROM {collection_name}\n    ORDER BY {vector_name} <-> '{embedded_query_pg}'::VECTOR(1536)\n    LIMIT {top_k};\n    \"\"\"\n    # Execute the query\n    cursor.execute(query_sql)\n    results = cursor.fetchall()\n\n    return results\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files in Python\nDESCRIPTION: Function that reads a PDF file and extracts text content from all pages, with error handling for corrupted files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    text = \"\"\n    try:\n        with open(pdf_path, \"rb\") as f:\n            reader = PyPDF2.PdfReader(f)\n            for page in reader.pages:\n                page_text = page.extract_text()\n                if page_text:\n                    text += page_text\n    except Exception as e:\n        print(f\"Error reading {pdf_path}: {e}\")\n    return text\n\ndef generate_questions(pdf_path):\n    text = extract_text_from_pdf(pdf_path)\n\n    prompt = (\n        \"Can you generate a question that can only be answered from this document?:\\n\"\n        f\"{text}\\n\\n\"\n    )\n\n    response = client.responses.create(\n        input=prompt,\n        model=\"gpt-4o\",\n    )\n\n    question = response.output[0].content[0].text\n\n    return question\n```\n\n----------------------------------------\n\nTITLE: Defining GPT Action OpenAPI Schema (YAML)\nDESCRIPTION: Presents a template for the OpenAPI 3.1 schema used to configure the GPT Action within ChatGPT. This schema describes the HTTP API of the deployed Google Cloud Function, including server details, path, operation ID, summary, and response structure for the authentication test endpoint, requiring placeholders to be replaced with actual values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: {insert title}\n  description: {insert description}\n  version: 1.0.0\nservers:\n  - url: {url of your Google Cloud Function}\n    description: {insert description}\npaths:\n  /{your_function_name}:\n    get:\n      operationId: {create an operationID}\n      summary: {insert summary}\n      responses:\n        '200':\n          description: {insert description}\n          content:\n            text/plain:\n              schema:\n                type: string\n                example: {example of response}\n```\n\n----------------------------------------\n\nTITLE: Running Conversational Agent Executor for Bank Account Inquiry in Python\nDESCRIPTION: This snippet demonstrates querying the multi-tool conversational agent with a question about living without a bank account. It presumes the agent executor, tools, and chain have already been instantiated. The input is a free-form user question, and the output is a system-generated answer using the integrated tools for retrieval or search as needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nmulti_tool_executor.run(\"Hi, I'd like to know how you can live without a bank account\")\n```\n\n----------------------------------------\n\nTITLE: Chunking Text by Tokens with Sentence Boundary Preference - Python\nDESCRIPTION: This function splits a given text into token chunks of a maximum size 'n', attempting to end chunks at sentence boundaries (identified by '.' or '\\n'). It returns a generator yielding token lists for each chunk.\n\nRequired Dependencies: Requires a tokenizer object (e.g., from `tiktoken`) with `encode` and `decode` methods.\n\nParameters:\n- `text` (str): The input text to be chunked.\n- `n` (int): The maximum number of tokens per chunk.\n- `tokenizer` (object): An object with `encode` and `decode` methods.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef create_chunks(text, n, tokenizer):\n    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n    tokens = tokenizer.encode(text)\n    i = 0\n    while i < len(tokens):\n        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n        j = min(i + int(1.5 * n), len(tokens))\n        while j > i + int(0.5 * n):\n            # Decode the tokens and check for full stop or newline\n            chunk = tokenizer.decode(tokens[i:j])\n            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n                break\n            j -= 1\n        # If no end of sentence found, use n tokens as the chunk size\n        if j == i + int(0.5 * n):\n            j = min(i + n, len(tokens))\n        yield tokens[i:j]\n        i = j\n```\n\n----------------------------------------\n\nTITLE: Mock Multiple Choice Response (Python)\nDESCRIPTION: Defines a function that returns a mock response for a multiple-choice question. It always returns 'a' as the response.  This function simulates user input for testing purposes. It takes no parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef get_mock_response_from_user_multiple_choice():\n    return \"a\"\n```\n\n----------------------------------------\n\nTITLE: Defining Modular Dispute Resolution Agents Using OpenAI Agents SDK in Python\nDESCRIPTION: This snippet defines three specialized agents with distinct functions for automating dispute resolution: the Dispute Intake Agent (investigator_agent) compiles detailed dispute evidence and contacts, the Accept Dispute Agent (accept_dispute_agent) closes valid disputes and provides explanations, and the Triage Agent (triage_agent) decides dispute handling based on order shipment status. Key dependencies include the OpenAI Agents SDK's Agent class and auxiliary tools such as get_emails, get_phone_logs, close_dispute, retrieve_payment_intent, and get_order for fetching relevant data and performing actions. Each agent uses different language models (o3-mini or gpt-4o) and includes structured instructions to guide AI behavior. The agents are structured to enable handoffs from the triage agent to either acceptance or investigation workflows.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninvestigator_agent = Agent(\n    name=\"Dispute Intake Agent\",\n    instructions=(\n        \"As a dispute investigator, please compile the following details in your final output:\\n\\n\"\n        \"Dispute Details:\\n\"\n        \"- Dispute ID\\n\"\n        \"- Amount\\n\"\n        \"- Reason for Dispute\\n\"\n        \"- Card Brand\\n\\n\"\n        \"Payment & Order Details:\\n\"\n        \"- Fulfillment status of the order\\n\"\n        \"- Shipping carrier and tracking number\\n\"\n        \"- Confirmation of TOS acceptance\\n\\n\"\n        \"Email and Phone Records:\\n\"\n        \"- Any relevant email threads (include the full body text)\\n\"\n        \"- Any relevant phone logs\\n\"\n    ),\n    model=\"o3-mini\",\n    tools=[get_emails, get_phone_logs]\n)\n\n\naccept_dispute_agent = Agent(\n    name=\"Accept Dispute Agent\",\n    instructions=(\n        \"You are an agent responsible for accepting disputes. Please do the following:\\n\"\n        \"1. Use the provided dispute ID to close the dispute.\\n\"\n        \"2. Provide a short explanation of why the dispute is being accepted.\\n\"\n        \"3. Reference any relevant order details (e.g., unfulfilled order, etc.) retrieved from the database.\\n\\n\"\n        \"Then, produce your final output in this exact format:\\n\\n\"\n        \"Dispute Details:\\n\"\n        \"- Dispute ID\\n\"\n        \"- Amount\\n\"\n        \"- Reason for Dispute\\n\\n\"\n        \"Order Details:\\n\"\n        \"- Fulfillment status of the order\\n\\n\"\n        \"Reasoning for closing the dispute\\n\"\n    ),\n    model=\"gpt-4o\",\n    tools=[close_dispute]\n)\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=(\n        \"Please do the following:\\n\"\n        \"1. Find the order ID from the payment intent's metadata.\\n\"\n        \"2. Retrieve detailed information about the order (e.g., shipping status).\\n\"\n        \"3. If the order has shipped, escalate this dispute to the investigator agent.\\n\"\n        \"4. If the order has not shipped, accept the dispute.\\n\"\n    ),\n    model=\"gpt-4o\",\n    tools=[retrieve_payment_intent, get_order],\n    handoffs=[accept_dispute_agent, investigator_agent],\n)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Neo4j Vector Indexes for Various Entity Types in Python\nDESCRIPTION: Defines a function `embed_entities` that generalizes the creation of Neo4j vector indexes for different node labels (entity types). It retrieves unique entity types from the `df` DataFrame and iterates through them, calling `embed_entities` for each type to create an index based on the 'value' property. This allows efficient similarity searches across different entity types like 'Category', 'Brand', etc. Requires the `df` DataFrame, Neo4j connection details, and configured OpenAI embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef embed_entities(entity_type):\n    vector_index = Neo4jVector.from_existing_graph(\n        OpenAIEmbeddings(model=embeddings_model),\n        url=url,\n        username=username,\n        password=password,\n        index_name=entity_type,\n        node_label=entity_type,\n        text_node_properties=['value'],\n        embedding_node_property='embedding',\n    )\n    \nentities_list = df['entity_type'].unique()\n\nfor t in entities_list:\n    embed_entities(t)\n```\n\n----------------------------------------\n\nTITLE: Extracting Punctuated Transcript from API Response (Python)\nDESCRIPTION: Retrieves the actual punctuated text content from the OpenAI API `response` object. It accesses the `content` field within the `message` of the first `choice` in the response structure (`response.choices[0].message.content`). This extracted text is stored in the `punctuated_transcript` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Extract the punctuated transcript from the model's response\npunctuated_transcript = response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Querying Document Corpus with Hallucinated Evidence in Python\nDESCRIPTION: Demonstrates how to query a scientific corpus collection using hallucinated abstracts as the query text. The snippet calls the 'query' method on a corpus collection object, requesting the top 3 results including the documents and their distances. The outputs are then filtered using a custom filter_query_result function to retain relevant matches based on criteria such as distance threshold. This step enables retrieval of documents closely aligned with the hallucinated abstracts, improving context for claim assessment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nhallucinated_query_result = scifact_corpus_collection.query(query_texts=hallucinated_evidence, include=['documents', 'distances'], n_results=3)\nfiltered_hallucinated_query_result = filter_query_result(hallucinated_query_result)\n```\n\n----------------------------------------\n\nTITLE: Formatting and Printing Tool Call Results in Python\nDESCRIPTION: This function, print_tool_call, accepts user_input, context, and the tool_call output from the OpenAI model and prints extracted product search arguments in a readable format. It parses the arguments using json.loads and iterates over the resulting key-value pairs. Dependencies include Python's json module. Intended for inspection and debugging use where human-readable output of structured search parameters is required.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef print_tool_call(user_input, context, tool_call):\n    args = tool_call[0].function.arguments\n    print(f\"Input: {user_input}\\n\\nContext: {context}\\n\")\n    print(\"Product search arguments:\")\n    for key, value in json.loads(args).items():\n        print(f\"{key}: '{value}'\")\n    print(\"\\n\\n\")\n\n```\n\n----------------------------------------\n\nTITLE: Getting Prediction Probabilities\nDESCRIPTION: Requesting log probabilities to see the model's confidence in its predictions across different classes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nres = client.completions.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\nres.choices[0].logprobs.top_logprobs\n```\n\n----------------------------------------\n\nTITLE: Defining Quote Generation Prompt Template - Python\nDESCRIPTION: This snippet defines a template for generating philosophical quotes. It uses string formatting to insert the topic and example quotes into a prompt for the OpenAI LLM. The template provides instructions for the LLM to generate a short quote on the given topic, similar in spirit and form to the provided examples. Dependencies: `openai` (client).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncompletion_model_name = \"gpt-3.5-turbo\"\n\ngeneration_prompt_template = \"\"\"Generate a single short philosophical quote on the given topic,\nsimilar in spirit and form to the provided actual example quotes.\nDo not exceed 20-30 words in your quote.\n\nREFERENCE TOPIC: \\\"{topic}\\\"\"\n\nACTUAL EXAMPLES:\n{examples}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Supabase JavaScript Client with npm\nDESCRIPTION: Installs the @supabase/supabase-js client library in a Node.js project. This library is used to interact with the Supabase REST API for data storage and querying. Requires npm and a package.json already initialized.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @supabase/supabase-js\n```\n\n----------------------------------------\n\nTITLE: Waiting for Completion of a Run and Printing Thread Response in Python\nDESCRIPTION: This code waits asynchronously for an assistant run to complete by invoking a `wait_on_run` function, which likely polls or listens for run state changes. Once complete, it retrieves the thread response with `get_response` and pretty-prints it. These utility functions facilitate synchronization and output inspection when working with asynchronous assistant thread executions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nrun = wait_on_run(run, thread)\npretty_print(get_response(thread))\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Evaluate Student Solution Directly (Incorrect)\nDESCRIPTION: This example demonstrates a naive approach to evaluating a student's solution to a math problem, where the model is directly asked to determine if the solution is correct without first working through the problem itself. This can lead to incorrect evaluations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_12\n\nLANGUAGE: N/A\nCODE:\n```\nSYSTEM: Determine if the student's solution is correct or not.\n\nUSER: Problem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\nASSISTANT: The student's solution is correct.\n```\n\n----------------------------------------\n\nTITLE: Defining GPT Instructions for Azure AI Search Action in Python\nDESCRIPTION: This Python snippet constructs a multi-line f-string containing detailed instructions for a custom GPT assistant. The instructions define the assistant's role, how to use a POST request action to query an Azure AI Search index (using variables like `search_service_endpoint`, `index_name`, `categories`), and specifies rules for formulating the query and handling categories based on user input. It requires predefined variables (`search_service_endpoint`, `index_name`, `categories`) and uses the `pyperclip` library to copy the generated instructions to the clipboard before printing them.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ninstructions = f'''\nYou are an OAI docs assistant. You have an action in your knowledge base where you can make a POST request to search for information. The POST request should always include: {{\n    \"search_service_endpoint\": \"{search_service_endpoint}\",\n    \"index_name\": {index_name},\n    \"query\": \"<user_query>\",\n    \"k_nearest_neighbors\": 1,\n    \"search_column\": \"content_vector\",\n    \"use_hybrid_query\": true,\n    \"category\": \"<category>\"\n}}. Only the query and category change based on the user's request. Your goal is to assist users by performing searches using this POST request and providing them with relevant information based on the query.\n\nYou must only include knowledge you get from your action in your response.\nThe category must be from the following list: {categories}, which you should determine based on the user's query. If you cannot determine, then do not include the category in the POST request.\n'''\npyperclip.copy(instructions)\nprint(\"GPT Instructions copied to clipboard\")\nprint(instructions)\n```\n\n----------------------------------------\n\nTITLE: Setting up Paper Directory\nDESCRIPTION: This code snippet sets up the directory and filepath for storing downloaded papers and creates an empty pandas DataFrame to store the metadata related to each paper which is subsequently saved as a CSV file.  The filepath points to `arxiv_library.csv` inside the `./data/papers` directory. This CSV file will be used to store embeddings and details for the downloaded papers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set a directory to store downloaded papers\ndata_dir = os.path.join(os.curdir, \"data\", \"papers\")\npaper_dir_filepath = \"./data/papers/arxiv_library.csv\"\n\n# Generate a blank dataframe where we can store downloaded files\ndf = pd.DataFrame(list())\ndf.to_csv(paper_dir_filepath)\n```\n\n----------------------------------------\n\nTITLE: Comparing Vector Search Performance Between FLAT and HNSW Indices in Redis - Python\nDESCRIPTION: Defines the `time_queries` function to benchmark and compare the query performance of FLAT and HNSW vector indices using the `search_redis` function over 10 iterations, printing average query times and top results for each index type. Requires time, redis, and prior Redis index setup. Useful for evaluating the trade-offs between accuracy and speed on various index strategies. Limitations include timing accuracy depending on server load and dataset state.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n# compare the results of the HNSW index to the FLAT index and time both queries\ndef time_queries(iterations: int = 10):\n    print(\" ----- Flat Index ----- \")\n    t0 = time.time()\n    for i in range(iterations):\n        results_flat = search_redis(redis_client, 'modern art in Europe', k=10, print_results=False)\n    t0 = (time.time() - t0) / iterations\n    results_flat = search_redis(redis_client, 'modern art in Europe', k=10, print_results=True)\n    print(f\"Flat index query time: {round(t0, 3)} seconds\\n\")\n    time.sleep(1)\n    print(\" ----- HNSW Index ------ \")\n    t1 = time.time()\n    for i in range(iterations):\n        results_hnsw = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10, print_results=False)\n    t1 = (time.time() - t1) / iterations\n    results_hnsw = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10, print_results=True)\n    print(f\"HNSW index query time: {round(t1, 3)} seconds\")\n    print(\" ------------------------ \")\ntime_queries()\n```\n\n----------------------------------------\n\nTITLE: Providing Number of Days for Weather Forecast in Chat Completion in Python\nDESCRIPTION: Adds user reply specifying the number of forecast days, then sends the updated conversation to the Chat Completions API. The returned assistant response now includes generated function arguments complete with the number of days, showing how the model completes argument requirements after receiving clarification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmessages.append({\"role\": \"user\", \"content\": \"5 days\"})\nchat_response = chat_completion_request(\n    messages, tools=tools\n)\nchat_response.choices[0]\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion using Python Library\nDESCRIPTION: Initialize the OpenAI Python client and make a basic chat completion request. The client automatically picks up the API key from the OPENAI_API_KEY environment variable by default. This snippet demonstrates sending a simple user message to the gpt-3.5-turbo model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(\n    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n)\n\nchat_completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Batch Import - Python\nDESCRIPTION: Configures the Weaviate client's batch import settings. This enables efficient data ingestion by sending objects in batches, with options for dynamic batch sizing and retry attempts on timeouts. Requires a connected Weaviate client instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n### Step 2 - configure Weaviate Batch, with\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=10, \n    dynamic=True,\n    timeout_retries=3,\n#   callback=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Aggregated User and Product Embeddings with Pandas in Python\nDESCRIPTION: This snippet computes average embedding vectors for each user and for each product using pandas groupby and numpy mean. It enables downstream analysis like similarity computation between users and products. Dependencies: pandas, numpy; input: df with 'UserId', 'ProductId', and 'ada_embedding'; output: user_embeddings and prod_embeddings pandas Series with averaged vectors as values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nuser_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\nprod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)\n```\n\n----------------------------------------\n\nTITLE: Extracting message content from ChatGPT response\nDESCRIPTION: Demonstrates how to extract just the text content from the model's response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresponse.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Searching Quotes Using OpenAI Embeddings in Python\nDESCRIPTION: Defines a Python function to query a vector store for quotes similar to a given input quote by first creating its embedding using OpenAI's embedding API. The function supports optional filtering by author and tags to refine search results via metadata. It returns a list of matching quotes and their authors. Dependencies include an initialized OpenAI client, an embedding model name, and a vector table with ANN search capabilities. Inputs are the query quote string, number of results to return, author string (optional), and tags list (optional). Outputs are tuples of quote texts and authors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef find_quote_and_author(query_quote, n, author=None, tags=None):\n    query_vector = client.embeddings.create(\n        input=[query_quote],\n        model=embedding_model_name,\n    ).data[0].embedding\n    metadata = {}\n    if author:\n        metadata[\"author\"] = author\n    if tags:\n        for tag in tags:\n            metadata[tag] = True\n    #\n    results = v_table.ann_search(\n        query_vector,\n        n=n,\n        metadata=metadata,\n    )\n    return [\n        (result[\"body_blob\"], result[\"metadata\"][\"author\"])\n        for result in results\n    ]\n```\n\n----------------------------------------\n\nTITLE: Get OpenAI API Key Python\nDESCRIPTION: This snippet prompts the user to enter their OpenAI API key using `getpass`.  The API key is required for calling OpenAI's embedding models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n```python\nOPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a Model Evaluation Class in Python\nDESCRIPTION: This Python code defines an `Evaluator` class using pandas, seaborn, and matplotlib to assess the performance of question-answering models. It categorizes model outputs (correct, skipped, wrong, hallucination, I don't know) based on expected answers and context availability, calculates performance metrics, and provides methods for plotting comparisons between different models (e.g., baseline vs. fine-tuned). The class is initialized with a pandas DataFrame containing model predictions and ground truth answers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nclass Evaluator:\n    def __init__(self, df):\n        self.df = df\n        self.y_pred = pd.Series()  # Initialize as empty Series\n        self.labels_answer_expected = [\"âœ… Answered Correctly\", \"âŽ Skipped\", \"âŒ Wrong Answer\"]\n        self.labels_idk_expected = [\"âŒ Hallucination\", \"âœ… I don't know\"]\n\n    def _evaluate_answer_expected(self, row, answers_column):\n        generated_answer = row[answers_column].lower()\n        actual_answers = [ans.lower() for ans in row[\"answers\"]]\n        return (\n            \"âœ… Answered Correctly\" if any(ans in generated_answer for ans in actual_answers)\n            else \"âŽ Skipped\" if generated_answer == \"i don't know\"\n            else \"âŒ Wrong Answer\"\n        )\n\n    def _evaluate_idk_expected(self, row, answers_column):\n        generated_answer = row[answers_column].lower()\n        return (\n            \"âŒ Hallucination\" if generated_answer != \"i don't know\"\n            else \"âœ… I don't know\"\n        )\n\n    def _evaluate_single_row(self, row, answers_column):\n        is_impossible = row[\"is_impossible\"]\n        return (\n            self._evaluate_answer_expected(row, answers_column) if not is_impossible\n            else self._evaluate_idk_expected(row, answers_column)\n        )\n\n    def evaluate_model(self, answers_column=\"generated_answer\"):\n        self.y_pred = pd.Series(self.df.apply(self._evaluate_single_row, answers_column=answers_column, axis=1))\n        freq_series = self.y_pred.value_counts()\n        \n        # Counting rows for each scenario\n        total_answer_expected = len(self.df[self.df['is_impossible'] == False])\n        total_idk_expected = len(self.df[self.df['is_impossible'] == True])\n        \n        freq_answer_expected = (freq_series / total_answer_expected * 100).round(2).reindex(self.labels_answer_expected, fill_value=0)\n        freq_idk_expected = (freq_series / total_idk_expected * 100).round(2).reindex(self.labels_idk_expected, fill_value=0)\n        return freq_answer_expected.to_dict(), freq_idk_expected.to_dict()\n\n    def print_eval(self):\n        answer_columns=[\"generated_answer\", \"ft_generated_answer\"]\n        baseline_correctness, baseline_idk = self.evaluate_model()\n        ft_correctness, ft_idk = self.evaluate_model(self.df, answer_columns[1])\n        print(\"When the model should answer correctly:\")\n        eval_df = pd.merge(\n            baseline_correctness.rename(\"Baseline\"),\n            ft_correctness.rename(\"Fine-Tuned\"),\n            left_index=True,\n            right_index=True,\n        )\n        print(eval_df)\n        print(\"\\n\\n\\nWhen the model should say 'I don't know':\")\n        eval_df = pd.merge(\n            baseline_idk.rename(\"Baseline\"),\n            ft_idk.rename(\"Fine-Tuned\"),\n            left_index=True,\n            right_index=True,\n        )\n        print(eval_df)\n    \n    def plot_model_comparison(self, answer_columns=[\"generated_answer\", \"ft_generated_answer\"], scenario=\"answer_expected\", nice_names=[\"Baseline\", \"Fine-Tuned\"]):\n        \n        results = []\n        for col in answer_columns:\n            answer_expected, idk_expected = self.evaluate_model(col)\n            if scenario == \"answer_expected\":\n                results.append(answer_expected)\n            elif scenario == \"idk_expected\":\n                results.append(idk_expected)\n            else:\n                raise ValueError(\"Invalid scenario\")\n        \n        \n        results_df = pd.DataFrame(results, index=nice_names)\n        if scenario == \"answer_expected\":\n            results_df = results_df.reindex(self.labels_answer_expected, axis=1)\n        elif scenario == \"idk_expected\":\n            results_df = results_df.reindex(self.labels_idk_expected, axis=1)\n        \n        melted_df = results_df.reset_index().melt(id_vars='index', var_name='Status', value_name='Frequency')\n        sns.set_theme(style=\"whitegrid\", palette=\"icefire\")\n        g = sns.catplot(data=melted_df, x='Frequency', y='index', hue='Status', kind='bar', height=5, aspect=2)\n\n        # Annotating each bar\n        for p in g.ax.patches:\n            g.ax.annotate(f\"{p.get_width():.0f}%\", (p.get_width()+5, p.get_y() + p.get_height() / 2),\n                        textcoords=\"offset points\",\n                        xytext=(0, 0),\n                        ha='center', va='center')\n        plt.ylabel(\"Model\")\n        plt.xlabel(\"Percentage\")\n        plt.xlim(0, 100)\n        plt.tight_layout()\n        plt.title(scenario.replace(\"_\", \" \").title())\n        plt.show()\n\n\n# Compare the results by merging into one dataframe\nevaluator = Evaluator(df)\n# evaluator.evaluate_model(answers_column=\"ft_generated_answer\")\n# evaluator.plot_model_comparison([\"generated_answer\", \"ft_generated_answer\"], scenario=\"answer_expected\", nice_names=[\"Baseline\", \"Fine-Tuned\"])\n```\n\n----------------------------------------\n\nTITLE: Making an OpenAI API Call with Tool Definitions in Python\nDESCRIPTION: Demonstrates preparing tools for an API call. It lists the tool functions (`execute_refund`, `look_up_item`), converts them to the required schema format using the previously defined `function_to_schema` helper, and then calls `client.chat.completions.create` passing the generated `tool_schemas` to the `tools` parameter. It then shows how to access the tool call requested by the model from the response message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessages = []\n\ntools = [execute_refund, look_up_item]\ntool_schemas = [function_to_schema(tool) for tool in tools]\n\nresponse = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": \"Look up the black boot.\"}],\n            tools=tool_schemas,\n        )\nmessage = response.choices[0].message\n\nmessage.tool_calls[0].function\n```\n\n----------------------------------------\n\nTITLE: Defining Query Function for Qdrant Search with OpenAI Embeddings\nDESCRIPTION: This Python code defines a function `query_qdrant` that takes a text query, collection name, vector name, and `top_k` limit. It uses the OpenAI API (`text-embedding-ada-002`) to convert the query text into an embedding and then uses the Qdrant client's `search` method to find the nearest neighbors based on the specified vector.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nopenai_client = OpenAI()\n\ndef query_qdrant(query, collection_name, vector_name=\"title\", top_k=20):\n    # Creates embedding vector from user query\n    embedded_query = openai_client.embeddings.create(\n        input=query,\n        model=\"text-embedding-ada-002\",\n    ).data[0].embedding\n\n    query_results = client.search(\n        collection_name=collection_name,\n        query_vector=(\n            vector_name, embedded_query\n        ),\n        limit=top_k,\n    )\n\n    return query_results\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Chat Response Function\nDESCRIPTION: Defines an asynchronous function 'get_chat_response' that interacts with the OpenAI Chat Completion API. It takes system and user messages, a seed value (for reproducibility), and a temperature parameter. It returns the response content and displays system fingerprint, prompt tokens and completion tokens in an HTML table.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def get_chat_response(\n    system_message: str, user_request: str, seed: int = None, temperature: float = 0.7\n):\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_request},\n        ]\n\n        response = openai.chat.completions.create(\n            model=GPT_MODEL,\n            messages=messages,\n            seed=seed,\n            max_tokens=200,\n            temperature=temperature,\n        )\n\n        response_content = response.choices[0].message.content\n        system_fingerprint = response.system_fingerprint\n        prompt_tokens = response.usage.prompt_tokens\n        completion_tokens = response.usage.total_tokens - response.usage.prompt_tokens\n\n        table = f\"\"\"\n        <table>\n        <tr><th>Response</th><td>{response_content}</td></tr>\n        <tr><th>System Fingerprint</th><td>{system_fingerprint}</td></tr>\n        <tr><th>Number of prompt tokens</th><td>{prompt_tokens}</td></tr>\n        <tr><th>Number of completion tokens</th><td>{completion_tokens}</td></tr>\n        </table>\n        \"\"\"\n        display(HTML(table))\n\n        return response_content\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Defining API Call Function in Python\nDESCRIPTION: Initializes the OpenAI client using an API key retrieved from environment variables ('OPENAI_API_KEY'). Defines a helper function 'get_chat_completion' that takes a list of messages and an optional model name (defaulting to 'gpt-4-turbo'), sends a request to the OpenAI Chat Completions API, and returns the content of the assistant's message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef get_chat_completion(messages, model='gpt-4-turbo'):\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0,\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search from Application (JavaScript)\nDESCRIPTION: Demonstrates how to execute a semantic search from a JavaScript application. First, it generates an embedding for a given text query using the OpenAI API (`openai.embeddings.create`). Then, it calls the previously defined `match_documents` PostgreSQL function using the Supabase client's `rpc` method, passing the generated query embedding and a similarity threshold. Finally, it selects the 'content' field and limits the results to the top 5 most similar documents. Requires initialized `openai` and `supabase` clients.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst query = \"What does the cat chase?\";\n\n// First create an embedding on the query itself\nconst result = await openai.embeddings.create({\n  input: query,\n  model: \"text-embedding-3-small\",\n});\n\nconst [{ embedding }] = result.data;\n\n// Then use this embedding to search for matches\nconst { data: documents, error: matchError } = await supabase\n  .rpc(\"match_documents\", {\n    query_embedding: embedding,\n    match_threshold: 0.8,\n  })\n  .select(\"content\")\n  .limit(5);\n```\n\n----------------------------------------\n\nTITLE: Creating Augmented Query Python\nDESCRIPTION: Creates an augmented query by combining the retrieved contexts with the original query. The retrieved texts are extracted from the retrieval results and concatenated. The contexts retrieved are joined together to act as context, adding the original query at the end of the context.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# get list of retrieved text\ncontexts = [item['metadata']['text'] for item in res['matches']]\n\naugmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query\n```\n\n----------------------------------------\n\nTITLE: Defining the 'reject_request' Function (JSON)\nDESCRIPTION: This JSON snippet defines a function named 'reject_request'. It includes a description indicating its use when a user request cannot be fulfilled and specifies that it takes no parameters (empty properties object). This function is part of a larger list of functions provided to the language model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n            \"name\": \"reject_request\",\n            \"description\": \"Use this function if the request is not possible.\",\n            \"parameters\": {\"type\": \"object\", \"properties\": {}},\n        },\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completion with Azure OpenAI and Custom Data Source in Python\nDESCRIPTION: Demonstrates how to perform a streaming chat completion request using the Azure OpenAI client integrated with a custom data source. It sets `stream=True` in the `create` call and iterates through the resulting generator. Each `chunk` is processed to print the delta (changes) in role, content, or context information as it arrives from the API, providing a real-time response effect. The data source configuration uses Azure AI Search parameters from environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"}],\n    model=deployment,\n    extra_body={\n        \"dataSources\": [\n            {\n                \"type\": \"AzureCognitiveSearch\",\n                \"parameters\": {\n                    \"endpoint\": os.environ[\"SEARCH_ENDPOINT\"],\n                    \"key\": os.environ[\"SEARCH_KEY\"],\n                    \"indexName\": os.environ[\"SEARCH_INDEX_NAME\"],\n                }\n            }\n        ]\n    },\n    stream=True,\n)\n\nfor chunk in response:\n    delta = chunk.choices[0].delta\n\n    if delta.role:\n        print(\"\\n\"+ delta.role + \": \", end=\"\", flush=True)\n    if delta.content:\n        print(delta.content, end=\"\", flush=True)\n    if delta.model_extra.get(\"context\"):\n        print(f\"Context: {delta.model_extra['context']}\", end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Querying the Pinecone Index with Embedding Similarity Search\nDESCRIPTION: This snippet demonstrates how to query the index with a new input string by generating its embedding and retrieving the most similar vectors. Parameters control the number of results and which data to include in the output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nquery = \"When was OpenAI founded?\"\n\nx = embed(query)\n\nresults = index.query(\n    namespace=\"ns1\",\n    vector=x,\n    top_k=1,\n    include_values=False,\n    include_metadata=True\n)\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Writing Batch Tasks to JSONL File in Python\nDESCRIPTION: Serializes the list of batch task objects into a newline-delimited JSONL file, which can be uploaded to OpenAI's API for batch processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfile_name = \"data/batch_tasks_movies.jsonl\"\n\nwith open(file_name, 'w') as file:\n    for obj in tasks:\n        file.write(json.dumps(obj) + '\\n')\n```\n\n----------------------------------------\n\nTITLE: Generating Meeting Minutes from Audio\nDESCRIPTION: This code snippet demonstrates the end-to-end process of generating meeting minutes from an audio file. It first transcribes the audio using the `transcribe_audio` function (not defined in the snippet). It then calls the `meeting_minutes` function (also not defined) to generate the meeting minutes from the transcription. Finally, it prints the generated minutes and saves them to a DOCX file using the `save_as_docx` function. It assumes the existence of functions `transcribe_audio` and `meeting_minutes`. It requires an audio file named `Earningscall.wav` and outputs a DOCX file named `meeting_minutes.docx`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\naudio_file_path = \"Earningscall.wav\"\ntranscription = transcribe_audio(audio_file_path)\nminutes = meeting_minutes(transcription)\nprint(minutes)\n\nsave_as_docx(minutes, 'meeting_minutes.docx')\n```\n\n----------------------------------------\n\nTITLE: Generating Qdrant PointStructs from DataFrame in Python\nDESCRIPTION: Defines a function `generate_points_from_dataframe` that takes a Pandas DataFrame, generates embeddings for the 'question' column in batches using the pre-initialized `embedding_model`, and creates a list of Qdrant `PointStruct` objects. Each PointStruct includes a unique ID, the generated embedding vector, and a payload containing relevant metadata like question, title, context, etc. It uses `tqdm` for progress tracking and processes data in batches to manage memory efficiently.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom qdrant_client.http.models import PointStruct # Assuming this import is needed based on usage\n\ndef generate_points_from_dataframe(df: pd.DataFrame) -> List[PointStruct]:\n    batch_size = 512\n    questions = df[\"question\"].tolist()\n    total_batches = len(questions) // batch_size + 1\n    \n    pbar = tqdm(total=len(questions), desc=\"Generating embeddings\")\n    \n    # Generate embeddings in batches to improve performance\n    embeddings = []\n    for i in range(total_batches):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(questions))\n        batch = questions[start_idx:end_idx]\n        \n        batch_embeddings = embedding_model.embed(batch, batch_size=batch_size)\n        embeddings.extend(batch_embeddings)\n        pbar.update(len(batch))\n        \n    pbar.close()\n    \n    # Convert embeddings to list of lists\n    embeddings_list = [embedding.tolist() for embedding in embeddings]\n    \n    # Create a temporary DataFrame to hold the embeddings and existing DataFrame columns\n    temp_df = df.copy()\n    temp_df[\"embeddings\"] = embeddings_list\n    temp_df[\"id\"] = temp_df.index\n    \n    # Generate PointStruct objects using DataFrame apply method\n    points = temp_df.progress_apply(\n        lambda row: PointStruct(\n            id=row[\"id\"],\n            vector=row[\"embeddings\"],\n            payload={\n                \"question\": row[\"question\"],\n                \"title\": row[\"title\"],\n                \"context\": row[\"context\"],\n                \"is_impossible\": row[\"is_impossible\"],\n                \"answers\": row[\"answers\"],\n            },\n        ),\n        axis=1,\n    ).tolist()\n\n    return points\n\n# Example usage:\n# train_df is assumed to be a pre-existing Pandas DataFrame\n# points = generate_points_from_dataframe(train_df)\n```\n\n----------------------------------------\n\nTITLE: Initialize Typesense client for local server\nDESCRIPTION: Configures a connection to a locally hosted Typesense server using specified host, port, and API key, enabling subsequent indexing and search operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport typesense\n\ntypesense_client = \\\n    typesense.Client({\n        \"nodes\": [{\n            \"host\": \"localhost\",\n            \"port\": \"8108\",\n            \"protocol\": \"http\"\n          }],\n          \"api_key\": \"xyz\",\n          \"connection_timeout_seconds\": 60\n        })\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings via OpenAI API (Python, cURL, Node.js)\nDESCRIPTION: Demonstrates how to call the OpenAI Embeddings API endpoint (`/v1/embeddings`) to generate an embedding vector for a given text input using the `text-embedding-3-small` model. Examples are provided in Python using the `openai` library, a cURL command requiring an environment variable `OPENAI_API_KEY`, and Node.js using the `openai` library. The input text and model are specified in the request body, and the resulting embedding vector is printed or logged.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.embeddings.create(\n    input=\"Your text string goes here\",\n    model=\"text-embedding-3-small\"\n)\n\nprint(response.data[0].embedding)\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"input\": \"Your text string goes here\",\n    \"model\": \"text-embedding-3-small\"\n  }'\n```\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const embedding = await openai.embeddings.create({\n    model: \"text-embedding-3-small\",\n    input: \"Your text string goes here\",\n    encoding_format: \"float\",\n  });\n\n  console.log(embedding);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Uploading a Training File and Creating a Fine-Tuning Job with OpenAI Python SDK\nDESCRIPTION: This Python snippet shows how to use the OpenAI SDK to upload a local JSONL file for fine-tuning and launch a new fine-tuning job using the gpt-3.5-turbo model. It requires the 'openai' Python package and a fine-tuning-compatible JSONL data file (e.g., 'marv.jsonl'). The code uploads the file, then initiates the fine-tuning job referencing the uploaded file's ID. Ensure the OpenAI API key is set via environment or client configuration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nfile = client.files.create(\n  file=open(\"marv.jsonl\", \"rb\"),\n  purpose=\"fine-tune\"\n)\n\nclient.fine_tuning.jobs.create(\n  training_file=file.id,\n  model=\"gpt-3.5-turbo\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Entity and Relationship Mappings for LLM Prompting in Python\nDESCRIPTION: Defines three Python dictionaries: `entity_types` (mapping entity labels like 'category' to descriptions), `relation_types` (mapping relationship types like 'hasCategory' to descriptions), and `entity_relationship_match` (linking entity types to their typical relationship type). These structured mappings provide context to the LLM for accurate entity extraction from user prompts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nentity_types = {\n    \"product\": \"Item detailed type, for example 'high waist pants', 'outdoor plant pot', 'chef kitchen knife'\",\n    \"category\": \"Item category, for example 'home decoration', 'women clothing', 'office supply'\",\n    \"characteristic\": \"if present, item characteristics, for example 'waterproof', 'adhesive', 'easy to use'\",\n    \"measurement\": \"if present, dimensions of the item\", \n    \"brand\": \"if present, brand of the item\",\n    \"color\": \"if present, color of the item\",\n    \"age_group\": \"target age group for the product, one of 'babies', 'children', 'teenagers', 'adults'. If suitable for multiple age groups, pick the oldest (latter in the list).\"\n}\n\nrelation_types = {\n    \"hasCategory\": \"item is of this category\",\n    \"hasCharacteristic\": \"item has this characteristic\",\n    \"hasMeasurement\": \"item is of this measurement\",\n    \"hasBrand\": \"item is of this brand\",\n    \"hasColor\": \"item is of this color\", \n    \"isFor\": \"item is for this age_group\"\n }\n\nentity_relationship_match = {\n    \"category\": \"hasCategory\",\n    \"characteristic\": \"hasCharacteristic\",\n    \"measurement\": \"hasMeasurement\", \n    \"brand\": \"hasBrand\",\n    \"color\": \"hasColor\",\n    \"age_group\": \"isFor\"\n}\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Second Pass Query for Excerpts\nDESCRIPTION: This example shows how to improve the retrieval of relevant excerpts by prompting the model with follow-up queries to find any excerpts missed on previous passes. It is designed to avoid repetition.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_18\n\nLANGUAGE: N/A\nCODE:\n```\nSYSTEM: You will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \"What significant paradigm shifts have occurred in the history of artificial intelligence.\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\"excerpt\": \"...\"},\n...\n{\"excerpt\": \"...\"}]\n\nUSER: \"\"\"\"\n\nASSISTANT: [{\"excerpt\": \"the model writes an excerpt here\"},\n...\n{\"excerpt\": \"the model writes another excerpt here\"}]\n\nUSER: Are there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.\n```\n\n----------------------------------------\n\nTITLE: Creating a customizable get_completion function for OpenAI chat models in Python\nDESCRIPTION: Defines a utility function to send chat messages to OpenAI's API with configurable parameters such as model, max tokens, temperature, stop sequences, seed, and logprobs settings. Simplifies API interaction and enables enabling log probabilities and top logprobs for analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef get_completion(\n    messages: list[dict[str, str]],\n    model: str = \"gpt-4\",\n    max_tokens=500,\n    temperature=0,\n    stop=None,\n    seed=123,\n    tools=None,\n    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n    top_logprobs=None,\n) -> str:\n    params = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"stop\": stop,\n        \"seed\": seed,\n        \"logprobs\": logprobs,\n        \"top_logprobs\": top_logprobs,\n    }\n    if tools:\n        params[\"tools\"] = tools\n\n    completion = client.chat.completions.create(**params)\n    return completion\n```\n\n----------------------------------------\n\nTITLE: Defining Weaviate Hybrid Search Function - Python\nDESCRIPTION: Defines a Python function `hybrid_query_weaviate` to perform hybrid (vector + keyword) searches in Weaviate. It takes the query text, collection name, and alpha blending value as input, executes the query, and includes basic error handling for API rate limits. Requires a connected Weaviate client instance and a collection configured for vectorization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndef hybrid_query_weaviate(query, collection_name, alpha_val):\n    \n    nearText = {\n        \"concepts\": [query],\n        \"distance\": 0.7,\n    }\n\n    properties = [\n        \"title\", \"content\", \"url\",\n        \"_additional { score }\"\n    ]\n\n    result = (\n        client.query\n        .get(collection_name, properties)\n        .with_hybrid(nearText, alpha=alpha_val)\n        .with_limit(10)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute â€“ the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n```\n\n----------------------------------------\n\nTITLE: Bulk Indexing DataFrame Rows Into Elasticsearch in Python\nDESCRIPTION: Defines a generator function to yield documents in the format required for Elasticsearch's bulk API, converting vector fields from JSON strings to arrays. Used to efficiently ingest large datasets in batches, where each document contains article metadata and embedding vectors. Depends on the Pandas DataFrame structure and assumes a properly initialized target index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef dataframe_to_bulk_actions(df):\n    for index, row in df.iterrows():\n        yield {\n            \"_index\": 'wikipedia_vector_index',\n            \"_id\": row['id'],\n            \"_source\": {\n                'url' : row[\"url\"],\n                'title' : row[\"title\"],\n                'text' : row[\"text\"],\n                'title_vector' : json.loads(row[\"title_vector\"]),\n                'content_vector' : json.loads(row[\"content_vector\"]),\n                'vector_id' : row[\"vector_id\"]\n            }\n        }\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Batch with OpenAI API - Python\nDESCRIPTION: This snippet demonstrates how to cancel a running batch using the OpenAI API in Python. It requires the `openai` library to be installed. The user must provide the batch ID (`batch_abc123`) to cancel the specified batch. The cancellation process might take up to 10 minutes to complete, after which the status changes to `cancelled`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.batches.cancel(\"batch_abc123\")\n```\n\n----------------------------------------\n\nTITLE: Emulating Concurrent User Requests\nDESCRIPTION: This code demonstrates the parallel execution of multiple user requests by creating and initiating separate threads and runs for each request.  It calls `create_thread_and_run` multiple times, each with a different user input, to simulate concurrent interactions with the assistant.  The requests do not wait for each other and start at the same time.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Emulating concurrent user requests\nthread1, run1 = create_thread_and_run(\n    \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n)\nthread2, run2 = create_thread_and_run(\"Could you explain linear algebra to me?\")\nthread3, run3 = create_thread_and_run(\"I don't like math. What can I do?\")\n\n# Now all Runs are executing...\n```\n\n----------------------------------------\n\nTITLE: Retrieving from Pinecone Python\nDESCRIPTION: Retrieves the relevant document chunks from the Pinecone index. A query vector `xq` is created using an embedding of a sample query. The `index.query()` method is then used to find the top-k (5) most relevant chunks based on the query vector. It retrieves metadata of each chunk, including the original text and URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery = \"how do I use the LLMChain in LangChain?\"\n\nres = openai.Embedding.create(\n    input=[query],\n    engine=embed_model\n)\n\n# retrieve from Pinecone\nxq = res['data'][0]['embedding']\n\n# get relevant contexts (including the questions)\nres = index.query(xq, top_k=5, include_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Parsing PDF and Extracting Visual Data Using Python and GPT-4o Vision\nDESCRIPTION: This snippet covers downloading a multi-page PDF document, splitting it into individual page chunks, converting pages into image data, and preparing for text extraction using GPT-4o vision. It utilizes PyPDF2 to read and split PDF pages, requests to download the file, and BytesIO for in-memory processing. The snippet includes an image encoding function to convert locally saved images into base64 encoded strings for downstream consumption. Key dependencies include PyPDF2, requests, pdf2image, pandas, tqdm, and the OpenAI Python client configured for GPT-4o vision modality. The inputs are the URL of the PDF document, and outputs are structured page chunks with PDF bytes and base64-encoded images.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport requests\nimport os\nimport pandas as pd\nfrom PyPDF2 import PdfReader, PdfWriter\nfrom pdf2image import convert_from_bytes\nfrom io import BytesIO\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# Link to the document we will use as the example \ndocument_to_parse = \"https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf\"\n\n# OpenAI client \noai_client = OpenAI()\n\n\n# Chunk the PDF document into single page chunks \ndef chunk_document(document_url):\n    # Download the PDF document\n    response = requests.get(document_url)\n    pdf_data = response.content\n\n    # Read the PDF data using PyPDF2\n    pdf_reader = PdfReader(BytesIO(pdf_data))\n    page_chunks = []\n\n    for page_number, page in enumerate(pdf_reader.pages, start=1):\n        pdf_writer = PdfWriter()\n        pdf_writer.add_page(page)\n        pdf_bytes_io = BytesIO()\n        pdf_writer.write(pdf_bytes_io)\n        pdf_bytes_io.seek(0)\n        pdf_bytes = pdf_bytes_io.read()\n        page_chunk = {\n            'pageNumber': page_number,\n            'pdfBytes': pdf_bytes\n        }\n        page_chunks.append(page_chunk)\n\n    return page_chunks\n\n\n# Function to encode the image\ndef encode_image(local_image_path):\n    with open(local_image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n```\n\n----------------------------------------\n\nTITLE: Analyzing Images with GPT Model in Python\nDESCRIPTION: This code snippet defines the `analyze_image` function, which takes an image encoded in base64 and a list of subcategories as input.  It calls the OpenAI API to analyze the image using a specified GPT model. The prompt instructs the model to analyze the image of clothing and return a JSON output containing the \"items\" (related clothing titles), \"category\", and \"gender\". The function returns the content of the model's response, which is expected to be a JSON string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef analyze_image(image_base64, subcategories):\n    response = client.chat.completions.create(\n        model=GPT_MODEL,\n        messages=[\n            {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                \"type\": \"text\",\n                \"text\": \"\"\"Given an image of an item of clothing, analyze the item and generate a JSON output with the following fields: \\\"items\\\", \\\"category\\\", and \\\"gender\\\". \n                           Use your understanding of fashion trends, styles, and gender preferences to provide accurate and relevant suggestions for how to complete the outfit.\n                           The items field should be a list of items that would go well with the item in the picture. Each item should represent a title of an item of clothing that contains the style, color, and gender of the item.\n                           The category needs to be chosen between the types in this list: {subcategories}.\n                           You have to choose between the genders in this list: [Men, Women, Boys, Girls, Unisex]\n                           Do not include the description of the item in the picture. Do not include the ```json ``` tag in the output.\n                           \n                           Example Input: An image representing a black leather jacket.\n\n                           Example Output: {\\\"items\\\": [\\\"Fitted White Women's T-shirt\\\", \\\"White Canvas Sneakers\\\", \\\"Women's Black Skinny Jeans\\\"], \\\"category\\\": \\\"Jackets\\\", \\\"gender\\\": \\\"Women\\\"}\n                           \"\"\",\n                },\n                {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_base64}\",\n                },\n                }\n            ],\n            }\n        ]\n    )\n    # Extract relevant features from the response\n    features = response.choices[0].message.content\n    return features\n```\n\n----------------------------------------\n\nTITLE: Mapping Function Names to Python Handler Functions for Execution\nDESCRIPTION: Creates a dictionary that maps function names as strings to their corresponding Python implementations. This mapping enables easy dynamic invocation of the appropriate S3 helper functions based on function calls generated by the ChatGPT model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\navailable_functions = {\n    \"list_buckets\": list_buckets,\n    \"list_objects\": list_objects,\n    \"download_file\": download_file,\n    \"upload_file\": upload_file,\n    \"search_s3_objects\": search_s3_objects\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Table and Indexes in PolarDB-PG\nDESCRIPTION: Creates a table for storing article data with vector columns for title and content embeddings, and sets up vector indexes using IVFFlat for efficient similarity searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_table_sql = '''\nCREATE TABLE IF NOT EXISTS public.articles (\n    id INTEGER NOT NULL,\n    url TEXT,\n    title TEXT,\n    content TEXT,\n    title_vector vector(1536),\n    content_vector vector(1536),\n    vector_id INTEGER\n);\n\nALTER TABLE public.articles ADD PRIMARY KEY (id);\n'''\n\n# SQL statement for creating indexes\ncreate_indexes_sql = '''\nCREATE INDEX ON public.articles USING ivfflat (content_vector) WITH (lists = 1000);\n\nCREATE INDEX ON public.articles USING ivfflat (title_vector) WITH (lists = 1000);\n'''\n\n# Execute the SQL statements\ncursor.execute(create_table_sql)\ncursor.execute(create_indexes_sql)\n\n# Commit the changes\nconnection.commit()\n```\n\n----------------------------------------\n\nTITLE: Extract the ZIP archive of embeddings to data directory\nDESCRIPTION: Unzips the downloaded ZIP file into a specified data folder. Checks if the target CSV file exists after extraction to confirm successful decompression.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport zipfile\nimport os\nimport re\nimport tempfile\n\ncurrent_directory = os.getcwd()\nzip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\noutput_directory = os.path.join(current_directory, \"../../data\")\n\nwith zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n    zip_ref.extractall(output_directory)\n\n# check the csv file exist\nfile_name = \"vector_database_wikipedia_articles_embedded.csv\"\ndata_directory = os.path.join(current_directory, \"../../data\")\nfile_path = os.path.join(data_directory, file_name)\n\nif os.path.exists(file_path):\n    print(f\"The file {file_name} exists in the data directory.\")\nelse:\n    print(f\"The file {file_name} does not exist in the data directory.\")\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI SDK in Node.js via ES Module\nDESCRIPTION: Imports the OpenAI SDK into a Node.js project using ES module syntax. Required as a prerequisite for generating embeddings in JavaScript. Assumes the openai package was previously installed with npm.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport OpenAI from \"openai\";\n```\n\n----------------------------------------\n\nTITLE: Performing Weaviate Hybrid Search (Battles) - Python\nDESCRIPTION: Calls the `hybrid_query_weaviate` function with a different query related to 'Scottish history battles' and an alpha value. It then iterates through and prints the top search results and their scores, demonstrating the search capability with a different topic. Requires the `hybrid_query_weaviate` function and a populated 'Article' collection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nquery_result = hybrid_query_weaviate(\"Famous battles in Scottish history\", \"Article\", 0.5)\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']} (Score: {article['_additional']['score']})\")\n```\n\n----------------------------------------\n\nTITLE: Manual Exponential Backoff Implementation\nDESCRIPTION: This code snippet implements a manual exponential backoff strategy for retrying requests that fail due to rate limits. The `retry_with_exponential_backoff` decorator retries the decorated function with an increasing delay between attempts. The delay is calculated using an exponential base and optional jitter to avoid synchronized retries. The `client.chat.completions.create` method is called within the decorated function to generate a completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# imports\nimport random\nimport time\n\n# define a retry decorator\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.RateLimitError,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            # Retry on specified errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n\n\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return client.chat.completions.create(**kwargs)\n\n\ncompletions_with_backoff(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}])\n```\n\n----------------------------------------\n\nTITLE: Extract Downloaded Data\nDESCRIPTION: Extracts the contents of the downloaded zip file ('vector_database_wikipedia_articles_embedded.zip') into the '../data' directory using the zipfile library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Reading Image Output from Code Interpreter (JSON Response)\nDESCRIPTION: Example JSON structure of an Assistant message response when Code Interpreter generates an image. The image is referenced in the `content` array with `type: \"image_file\"` and includes the `file_id` needed to download the image.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"id\": \"msg_abc123\",\n\t\"object\": \"thread.message\",\n\t\"created_at\": 1698964262,\n\t\"thread_id\": \"thread_abc123\",\n\t\"role\": \"assistant\",\n\t\"content\": [\n    {\n      \"type\": \"image_file\",\n      \"image_file\": {\n        \"file_id\": \"file-abc123\"\n      }\n    }\n  ]\n  # ...\n}\n```\n\n----------------------------------------\n\nTITLE: Preparing context for ChatGPT query\nDESCRIPTION: This code defines functions to prepare a context message for ChatGPT based on relevant text excerpts retrieved from SingleStoreDB. It calculates the number of tokens in a string using the tiktoken library, and it constructs a message containing an introduction, relevant articles, and the user's question. The context message is designed to provide ChatGPT with the information needed to answer the question accurately.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport tiktoken\n\ndef num_tokens(text: str, model: str = GPT_MODEL) -> int:\n    \"\"\"Return the number of tokens in a string.\"\"\"\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n\ndef query_message(\n    query: str,\n    df: pd.DataFrame,\n    model: str,\n    token_budget: int\n) -> str:\n    \"\"\"Return a message for GPT, with relevant source texts pulled from SingleStoreDB.\"\"\"\n    strings, relatednesses = strings_ranked_by_relatedness(query, df, \"winter_olympics_2022\")\n    introduction = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n    question = f\"\\n\\nQuestion: {query}\"\n    message = introduction\n    for string in strings:\n        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n        if (\n            num_tokens(message + next_article + question, model=model)\n            > token_budget\n        ):\n            break\n        else:\n            message += next_article\n    return message + question\n\n\ndef ask(\n    query: str,\n    df: pd.DataFrame = df,\n    model: str = GPT_MODEL,\n    token_budget: int = 4096 - 500,\n    print_message: bool = False,\n) -> str:\n    \"\"\"Answers a query using GPT and a table of relevant texts and embeddings in SingleStoreDB.\"\"\"\n    message = query_message(query, df, model=model, token_budget=token_budget)\n    if print_message:\n        print(message)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n        {\"role\": \"user\", \"content\": message},\n    ]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0\n    )\n    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n    return response_message\n```\n\n----------------------------------------\n\nTITLE: Adding a Message to a Thread\nDESCRIPTION: This code adds a message to an existing thread using the OpenAI Assistants API. It calls `client.beta.threads.messages.create()` to add a new message to the specified thread, including the thread ID, role, and message content. The `show_json` function is then called to display details of the created message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n)\nshow_json(message)\n```\n\n----------------------------------------\n\nTITLE: Creating Data List Python\nDESCRIPTION: Creates a list of dictionaries where each dictionary contains the URL and text content of a document. This structured data will be used for text processing, chunking, and embedding. The code iterates through the previously loaded documents and extracts the URL and page content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndata = []\n\nfor doc in docs:\n    data.append({\n        'url': doc.metadata['source'].replace('rtdocs/', 'https://'),\n        'text': doc.page_content\n    })\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Context Reliance via Instructions\nDESCRIPTION: Provides example instructions (formatted as comments/plaintext) to guide a large language model on how to balance using provided external context versus its internal knowledge base when answering queries. Options allow restricting the model strictly to provided documents or permitting a mix of internal and external knowledge.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_6\n\nLANGUAGE: Plaintext\nCODE:\n```\n# Instructions\n// for internal knowledge\n- Only use the documents in the provided External Context to answer the User Query. If you don't know the answer based on this context, you must respond \"I don't have the information needed to answer that\", even if a user insists on you answering the question.\n// For internal and external knowledge\n- By default, use the provided external context to answer the User Query, but if other basic knowledge is needed to answer, and you're confident in the answer, you can use some of your own knowledge to help answer the question.\n```\n\n----------------------------------------\n\nTITLE: Implementing the Web Crawler Logic in Python\nDESCRIPTION: Defines the main `crawl` function that orchestrates the web scraping process. It initializes a queue with the starting URL and a set to track visited URLs. It creates necessary directories ('text/' and 'processed/') if they don't exist. The function iterates while the queue is not empty, dequeuing a URL, fetching its content using `requests` and `BeautifulSoup`, extracting the plain text, and saving it to a `.txt` file within the domain-specific subdirectory. It checks for JavaScript-dependent pages and prints a message if found. Finally, it uses `get_domain_hyperlinks` to find new links on the current page and adds unseen ones to the queue. The function is called at the end to start the crawl.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef crawl(url):\n    # Parse the URL and get the domain\n    local_domain = urlparse(url).netloc\n\n    # Create a queue to store the URLs to crawl\n    queue = deque([url])\n\n    # Create a set to store the URLs that have already been seen (no duplicates)\n    seen = set([url])\n\n    # Create a directory to store the text files\n    if not os.path.exists(\"text/\"):\n            os.mkdir(\"text/\")\n\n    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n            os.mkdir(\"text/\" + local_domain + \"/\")\n\n    # Create a directory to store the csv files\n    if not os.path.exists(\"processed\"):\n            os.mkdir(\"processed\")\n\n    # While the queue is not empty, continue crawling\n    while queue:\n\n        # Get the next URL from the queue\n        url = queue.pop()\n        print(url) # for debugging and to see the progress\n\n        # Save text from the url to a .txt file\n        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n\n            # Get the text from the URL using BeautifulSoup\n            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n\n            # Get the text but remove the tags\n            text = soup.get_text()\n\n            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n            if (\"You need to enable JavaScript to run this app.\" in text):\n                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n            \n            # Otherwise, write the text to the file in the text directory\n            f.write(text)\n\n        # Get the hyperlinks from the URL and add them to the queue\n        for link in get_domain_hyperlinks(local_domain, url):\n            if link not in seen:\n                queue.append(link)\n                seen.add(link)\n\ncrawl(full_url)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Agent Handoff Between Sales and Refund Agents in Python\nDESCRIPTION: Creates two specialized agents (sales assistant and refund agent) with their respective tools and demonstrates a manual handoff between them based on changing user needs in a conversation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef execute_refund(item_name):\n    return \"success\"\n\nrefund_agent = Agent(\n    name=\"Refund Agent\",\n    instructions=\"You are a refund agent. Help the user with refunds.\",\n    tools=[execute_refund],\n)\n\ndef place_order(item_name):\n    return \"success\"\n\nsales_assistant = Agent(\n    name=\"Sales Assistant\",\n    instructions=\"You are a sales assistant. Sell the user a product.\",\n    tools=[place_order],\n)\n\n\nmessages = []\nuser_query = \"Place an order for a black boot.\"\nprint(\"User:\", user_query)\nmessages.append({\"role\": \"user\", \"content\": user_query})\n\nresponse = run_full_turn(sales_assistant, messages) # sales assistant\nmessages.extend(response)\n\n\nuser_query = \"Actually, I want a refund.\" # implicitly refers to the last item\nprint(\"User:\", user_query)\nmessages.append({\"role\": \"user\", \"content\": user_query})\nresponse = run_full_turn(refund_agent, messages) # refund agent\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens in Strings Using tiktoken in Python\nDESCRIPTION: This snippet demonstrates using the tiktoken library to determine the number of tokens in a given text string according to a specified encoding, which is useful for understanding the cost and limits of API-based embedding models. Dependencies: tiktoken; input: string, encoding name (e.g., 'cl100k_base'); output: integer count of tokens; limitation: designed for OpenAI-type encoding schemes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with OpenAI\nDESCRIPTION: This Python code demonstrates creating embeddings using the OpenAI API. It uses the `text-embedding-ada-002` model and prints the response. It requires the `openai` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.embeddings.create(\n  model=\"text-embedding-ada-002\",\n  input=\"The food was delicious and the waiter...\"\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Data with Pandas\nDESCRIPTION: Loads a CSV file containing embedded Wikipedia article data into a Pandas DataFrame.  It then converts string representations of vectors back into lists using `literal_eval`, and casts vector_id to string type.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n```\n\n----------------------------------------\n\nTITLE: Extracting Keywords from Product Images with GPT-4o mini\nDESCRIPTION: Defines a function that uses GPT-4o mini to analyze a product image and title, generating relevant keywords for search. The system prompt guides the model to focus on product type, material, style, and color.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = '''\n    You are an agent specialized in tagging images of furniture items, decorative items, or furnishings with relevant keywords that could be used to search for these items on a marketplace.\n    \n    You will be provided with an image and the title of the item that is depicted in the image, and your goal is to extract keywords for only the item specified. \n    \n    Keywords should be concise and in lower case. \n    \n    Keywords can describe things like:\n    - Item type e.g. 'sofa bed', 'chair', 'desk', 'plant'\n    - Item material e.g. 'wood', 'metal', 'fabric'\n    - Item style e.g. 'scandinavian', 'vintage', 'industrial'\n    - Item color e.g. 'red', 'blue', 'white'\n    \n    Only deduce material, style or color keywords when it is obvious that they make the item depicted in the image stand out.\n\n    Return keywords in the format of an array of strings, like this:\n    ['desk', 'industrial', 'metal']\n    \n'''\n\ndef analyze_image(img_url, title):\n    response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": img_url,\n                    }\n                },\n            ],\n        },\n        {\n            \"role\": \"user\",\n            \"content\": title\n        }\n    ],\n        max_tokens=300,\n        top_p=0.1\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embedding Model Python\nDESCRIPTION: Imports the `openai` library and initializes the OpenAI API key using `openai.api_key = \"sk-...\".`  It specifies the embedding model as \"text-embedding-3-small\" and defines the input and engine parameters. This step is essential for creating embeddings from the text chunks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\n# initialize openai API key\nopenai.api_key = \"sk-...\"\n\nembed_model = \"text-embedding-3-small\"\n\nres = openai.Embedding.create(\n    input=[\n        \"Sample document text goes here\",\n        \"there will be several phrases in each batch\"\n    ], engine=embed_model\n)\n```\n\n----------------------------------------\n\nTITLE: Chunking Text into Tokens with Tiktoken in Python\nDESCRIPTION: This function encodes text into tokens using Tiktoken and then breaks it into chunks of a specified length. It uses OpenAI's Tiktoken library to tokenize the text. It yields each chunk as an iterator.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef chunked_tokens(text, chunk_length, encoding_name='cl100k_base'):\n    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n    encoding = tiktoken.get_encoding(encoding_name)\n    # Encode the input text into tokens\n    tokens = encoding.encode(text)\n    # Create an iterator that yields chunks of tokens of the specified length\n    chunks_iterator = batched(tokens, chunk_length)\n    # Yield each chunk from the iterator\n    yield from chunks_iterator\n```\n\n----------------------------------------\n\nTITLE: Executing an LLM Tool Call Request in Python\nDESCRIPTION: Defines the `execute_tool_call` function designed to handle a tool call requested by the language model. It takes a `tool_call` object (from the LLM response) and a dictionary mapping tool names to Python function objects (`tools_map`). It extracts the function name and JSON-formatted arguments, prints the call details, executes the corresponding Python function using keyword argument expansion (`**args`), and returns the result. The snippet also shows adding the tool's result back to the conversation history as a 'tool' role message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntools_map = {tool.__name__: tool for tool in tools}\n\ndef execute_tool_call(tool_call, tools_map):\n    name = tool_call.function.name\n    args = json.loads(tool_call.function.arguments)\n\n    print(f\"Assistant: {name}({args})\")\n\n    # call corresponding function with provided arguments\n    return tools_map[name](**args)\n\nfor tool_call in message.tool_calls:\n            result = execute_tool_call(tool_call, tools_map)\n\n            # add result back to conversation \n            result_message = {\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call.id,\n                \"content\": result,\n            }\n            messages.append(result_message)\n```\n\n----------------------------------------\n\nTITLE: Executing Title Vector Search Query - Python\nDESCRIPTION: This snippet demonstrates performing a pure vector search by calling the `search_redis` function with a sample query related to 'modern art in Europe' and requesting 10 results (`k=10`). It implicitly uses the default `title_vector` field for similarity calculation and requires the OpenAI API key to be set for embedding generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# For using OpenAI to generate query embedding\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\", \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\nresults = search_redis(redis_client, 'modern art in Europe', k=10)\n```\n\n----------------------------------------\n\nTITLE: Implementing Amazon S3 Helper Functions in Python\nDESCRIPTION: Defines Python functions to perform Amazon S3 operations: listing buckets, listing objects in a bucket, downloading a file from a bucket to a local directory, uploading a file (local path or remote URL) to a bucket, and searching for files in buckets. These functions handle boto3 API calls and return JSON-encoded responses, facilitating interaction through the OpenAI ChatGPT functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef list_buckets():\n    response = s3_client.list_buckets()\n    return json.dumps(response['Buckets'], default=datetime_converter)\n\ndef list_objects(bucket, prefix=''):\n    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    return json.dumps(response.get('Contents', []), default=datetime_converter)\n\ndef download_file(bucket, key, directory):\n    \n    filename = os.path.basename(key)\n    \n    # Resolve destination to the correct file path\n    destination = os.path.join(directory, filename)\n    \n    s3_client.download_file(bucket, key, destination)\n    return json.dumps({\"status\": \"success\", \"bucket\": bucket, \"key\": key, \"destination\": destination})\n\ndef upload_file(source, bucket, key, is_remote_url=False):\n    if is_remote_url:\n        file_name = os.path.basename(source)\n        urlretrieve(source, file_name)\n        source = file_name\n       \n    s3_client.upload_file(source, bucket, key)\n    return json.dumps({\"status\": \"success\", \"source\": source, \"bucket\": bucket, \"key\": key})\n\ndef search_s3_objects(search_name, bucket=None, prefix='', exact_match=True):\n    search_name = search_name.lower()\n    \n    if bucket is None:\n        buckets_response = json.loads(list_buckets())\n        buckets = [bucket_info[\"Name\"] for bucket_info in buckets_response]\n    else:\n        buckets = [bucket]\n\n    results = []\n\n    for bucket_name in buckets:\n        objects_response = json.loads(list_objects(bucket_name, prefix))\n        if exact_match:\n            bucket_results = [obj for obj in objects_response if search_name == obj['Key'].lower()]\n        else:\n            bucket_results = [obj for obj in objects_response if search_name in obj['Key'].lower()]\n\n        if bucket_results:\n            results.extend([{\"Bucket\": bucket_name, \"Object\": obj} for obj in bucket_results])\n\n    return json.dumps(results)\n```\n\n----------------------------------------\n\nTITLE: Starting Run for Assistant Processing (OpenAI Assistants API)\nDESCRIPTION: Creates a run on the specified thread using the created Assistant. This action triggers the Assistant to process the messages in the thread, execute the requested analysis using the Code Interpreter tool, and generate a response, which is expected to include a visualization based on the initial message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Submitting Tool Outputs Asynchronously Using Assistant Client in Python\nDESCRIPTION: This snippet submits tool outputs back to the assistant's backend API client using the `submit_tool_outputs` method. It requires the thread and run identifiers along with the prepared tool outputs data. The run object returned contains updated execution state or results. The snippet includes a call to `show_json` for formatted output of the submission response. This step is key for reporting tool execution results back to the managing system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.submit_tool_outputs(\n    thread_id=thread.id,\n    run_id=run.id,\n    tool_outputs=tool_outputs\n)\nshow_json(run)\n```\n\n----------------------------------------\n\nTITLE: Processing and Plotting Summary Evaluation Scores using Pandas and Matplotlib in Python\nDESCRIPTION: This Python script processes summary evaluation results stored in a pandas DataFrame. It extracts numerical scores from the 'simple_evaluation' and 'complex_evaluation' columns (excluding 'justification'), calculates the mean score for predefined criteria ('Categorisation', 'Keywords and Tags', etc.), aggregates these averages into a new DataFrame, and then uses `matplotlib.pyplot` to generate a bar chart comparing the average scores for the original (simple) versus the improved (complex) prompts across these criteria. Requires `pandas` and `matplotlib` libraries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd # Assuming pandas is imported as pd\n\ndf[\"simple_scores\"] = df[\"simple_evaluation\"].apply(lambda x: [score for key, score in x.model_dump().items() if key != 'justification'])\ndf[\"complex_scores\"] = df[\"complex_evaluation\"].apply(lambda x: [score for key, score in x.model_dump().items() if key != 'justification'])\n\n\n# Calculate average scores for each criterion\ncriteria = [\n    'Categorisation',\n    'Keywords and Tags',\n    'Sentiment Analysis',\n    'Clarity and Structure',\n    'Detail and Completeness'\n]\n\n# Calculate average scores for each criterion by model\nsimple_avg_scores = df['simple_scores'].apply(pd.Series).mean()\ncomplex_avg_scores = df['complex_scores'].apply(pd.Series).mean()\n\n\n# Prepare data for plotting\navg_scores_df = pd.DataFrame({\n    'Criteria': criteria,\n    'Original Prompt': simple_avg_scores,\n    'Improved Prompt': complex_avg_scores\n})\n\n# Plotting\nax = avg_scores_df.plot(x='Criteria', kind='bar', figsize=(6, 4))\nplt.ylabel('Average Score')\nplt.title('Comparison of Simple vs Complex Prompt Performance by Model')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Embedding Vector Dimensions in Python\nDESCRIPTION: Prints the dimensions of the generated embedding vectors to verify their length. Understanding vector dimensions is important for configuring the Pinecone index correctly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"vector 0: {len(res.data[0].embedding)}\\nvector 1: {len(res.data[1].embedding)}\")\n```\n\n----------------------------------------\n\nTITLE: Default File Writing Function in Python\nDESCRIPTION: A basic implementation of a file writing function using `pathlib`. Takes a file path and content string, ensures the parent directory exists, opens the file in text write mode with UTF-8 encoding, and writes the content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef write_file(path: str, content: str) -> None:\n    target = pathlib.Path(path)\n    target.parent.mkdir(parents=True, exist_ok=True)\n    with target.open(\"wt\", encoding=\"utf-8\") as fh:\n        fh.write(content)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as Environment Variable\nDESCRIPTION: Exports the OpenAI API key as an environment variable for authentication with OpenAI services. This is required for embeddings generation and LLM access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n! export OPENAI_API_KEY=\"your API key\"\n```\n\n----------------------------------------\n\nTITLE: Executing Redis Hybrid Full-Text and Vector Search Query in Python\nDESCRIPTION: This snippet demonstrates how to construct and execute a hybrid search query using the Redis Python client. It defines a sample text, generates its vector embedding (assuming a `get_vector` function exists), creates a query that filters by full-text ('@content:recession') and then performs a KNN search on the '@vector' field, and finally executes the search and prints the results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntext_5 = \"\"\"Ethiopia's crop production up 24%\n\nEthiopia produced 14.27 million tonnes of crops in 2004, 24% higher than in 2003 and 21% more than the average of the past five years, a report says.\n\nIn 2003, crop production totalled 11.49 million tonnes, the joint report from the Food and Agriculture Organisation and the World Food Programme said. Good rains, increased use of fertilizers and improved seeds contributed to the rise in production. Nevertheless, 2.2 million Ethiopians will still need emergency assistance.\n\nThe report calculated emergency food requirements for 2005 to be 387,500 tonnes. On top of that, 89,000 tonnes of fortified blended food and vegetable oil for \"targeted supplementary food distributions for a survival programme for children under five and pregnant and lactating women\" will be needed.\n\nIn eastern and southern Ethiopia, a prolonged drought has killed crops and drained wells. Last year, a total of 965,000 tonnes of food assistance was needed to help seven million Ethiopians. The Food and Agriculture Organisation (FAO) recommend that the food assistance is bought locally. \"Local purchase of cereals for food assistance programmes is recommended as far as possible, so as to assist domestic markets and farmers,\" said Henri Josserand, chief of FAO's Global Information and Early Warning System. Agriculture is the main economic activity in Ethiopia, representing 45% of gross domestic product. About 80% of Ethiopians depend directly or indirectly on agriculture.\n\"\"\"\n\nvec = np.array(get_vector(text_5), dtype=np.float32).tobytes()\nq = Query('@content:recession => [KNN 3 @vector $query_vec AS vector_score]')\\\n    .sort_by('vector_score')\\\n    .return_fields('vector_score', 'content')\\\n    .dialect(2)    \nparams = {\"query_vec\": vec}\n\nresults = client.ft('idx').search(q, query_params=params)\nfor doc in results.docs:\n    print(f\"distance:{round(float(doc['vector_score']),3)} content:{doc['content']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataframe Information\nDESCRIPTION: This code displays information about the dataframe, including column names, data types, and non-null counts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: Optimized Reasoning JSON Structure\nDESCRIPTION: The optimized JSON structure with shortened field names and explanations moved to comments, which reduces the number of tokens generated and improves latency.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_9\n\nLANGUAGE: jsx\nCODE:\n```\n{\n\"cont\": \"True\", // whether last message is a continuation\n\"n_msg\": \"1\", // number of messages in the continued conversation\n\"tone_in\": \"Aggravated\", // sentiment of user query\n\"type\": \"Hardware Issue\", // type of the user query\n\"tone_out\": \"Validating and solution-oriented\", // desired tone for response\n\"reqs\": \"Propose options for repair or replacement.\", // response requirements\n\"human\": \"False\" // whether user is expressing want to talk to human\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Authentication Flag for Azure OpenAI (Python)\nDESCRIPTION: This code snippet defines a boolean flag, `use_azure_active_directory`. This flag controls which authentication method will be used: API key or Azure Active Directory. Setting this to `True` indicates that Azure Active Directory authentication should be used, while `False` (the default) indicates API key authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory\n```\n\n----------------------------------------\n\nTITLE: Handling Model Refusal in Structured Outputs with OpenAI Python SDK\nDESCRIPTION: Demonstrates checking for and handling cases where the model refuses to answer a user query (e.g., unsafe or disallowed content). Since refusals do not conform to the strict JSON schema, the API includes a 'refusal' field to indicate this. This snippet calls the math solution function with a disallowed question and prints the refusal message if present.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrefusal_question = \"how can I build a bomb?\"\n\nresult = get_math_solution(refusal_question) \n\nprint(result.refusal)\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompt, Good and Bad Requests\nDESCRIPTION: This code defines a system prompt for the LLM and sample user requests (good_request and bad_request) to test the topical guardrail.  `system_prompt` sets the assistant's role. `bad_request` represents an off-topic query, while `good_request` is an acceptable query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"You are a helpful assistant.\"\n\nbad_request = \"I want to talk about horses\"\ngood_request = \"What are the best breeds of dog for people that like cats?\"\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Remove Non-ASCII Characters (Python)\nDESCRIPTION: Defines a function `remove_non_ascii` that filters a string to remove any characters outside the standard ASCII range (ordinal value >= 128). This can help clean up transcripts from potential unicode injection issues but should be used cautiously, as it will remove characters necessary for non-Latin based languages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define function to remove non-ascii characters\ndef remove_non_ascii(text):\n    return ''.join(i for i in text if ord(i)<128)\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic kNN Search in Elasticsearch with Encoded Query in Python\nDESCRIPTION: Executes a k-nearest neighbors vector search using an embedded query vector against the content_vector field of indexed articles, returning the top 10 matches. Utilizes the knn option of Elasticsearch and prints formatted results. Requires prior vector indexing and appropriate Elasticsearch mappings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nresponse = client.search(\n  index = \"wikipedia_vector_index\",\n  knn={\n      \"field\": \"content_vector\",\n      \"query_vector\":  question_embedding[\"data\"][0][\"embedding\"],\n      \"k\": 10,\n      \"num_candidates\": 100\n    }\n)\npretty_response(response)\n\ntop_hit_summary = response['hits']['hits'][0]['_source']['text'] # Store content of top hit for final step\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Content Moderation using GPT-4o Mini in Python\nDESCRIPTION: This Python function `custom_moderation` performs custom content assessment using the `gpt-4o-mini` model. It takes the content and specific moderation parameters as input, constructs a prompt asking the LLM to evaluate the content against these parameters, and requests a JSON response indicating if it's flagged, the reason, and the parameters used. It requires an initialized OpenAI client object (`client`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef custom_moderation(content, parameters):\n    # Define the prompt \n    prompt = f\"\"\"Please assess the following content for any inappropriate material. You should base your assessment on the given parameters.\n    Your answer should be in json format with the following fields: \n        - flagged: a boolean indicating whether the content is flagged for any of the categories in the parameters\n        - reason: a string explaining the reason for the flag, if any\n        - parameters: a dictionary of the parameters used for the assessment and their values\n    Parameters: {parameters}\\n\\nContent:\\n{content}\\n\\nAssessment:\"\"\"\n    \n    # Call model with the prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        response_format={ \"type\": \"json_object\" },\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a content moderation assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    \n    # Extract the assessment from the response\n    assessment = response.choices[0].message.content\n    \n    return assessment\n```\n\n----------------------------------------\n\nTITLE: Verifying OPENAI_API_KEY Environment Variable - Windows CMD\nDESCRIPTION: This Windows CMD command echoes the 'OPENAI_API_KEY' environment variable, to verify that it was set correctly. If successful, it will display the API key as output. No additional dependencies are needed, and it can be run in any Windows command prompt session after the variable has been assigned.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_5\n\nLANGUAGE: windows-cmd\nCODE:\n```\necho %OPENAI_API_KEY%\n```\n\n----------------------------------------\n\nTITLE: Recommendation System Using Embeddings\nDESCRIPTION: Implements a basic recommender system that uses embedding similarity to find related items. It takes a list of strings and a source string, computes embeddings, and returns strings ranked by similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef recommendations_from_strings(\n   strings: List[str],\n   index_of_source_string: int,\n   model=\"text-embedding-3-small\",\n) -> List[int]:\n   \"\"\"Return nearest neighbors of a given string.\"\"\"\n\n   # get embeddings for all strings\n   embeddings = [embedding_from_string(string, model=model) for string in strings]\n\n   # get the embedding of the source string\n   query_embedding = embeddings[index_of_source_string]\n\n   # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n   distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n\n   # get indices of nearest neighbors (function from embeddings_utils.py)\n   indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n   return indices_of_nearest_neighbors\n```\n\n----------------------------------------\n\nTITLE: Defining the ask_database SQL Query Execution Function in Python\nDESCRIPTION: This snippet implements the 'ask_database' function, which accepts a database connection and a SQL query as strings, executes the query, and returns the results as a string. It handles exceptions by returning an error message as the function result. Intended use is as a backend callable for tool-invoked SQL queries from language model outputs. Requires a valid conn object; outputs result rows or error strings. Sensitive to malformed SQL; actual database changes are possible if unsafe queries are used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndef ask_database(conn, query):\n    \"\"\"Function to query SQLite database with a provided SQL query.\"\"\"\n    try:\n        results = str(conn.execute(query).fetchall())\n    except Exception as e:\n        results = f\"query failed with error: {e}\"\n    return results\n```\n\n----------------------------------------\n\nTITLE: Splitting Text into Chunks Python\nDESCRIPTION: This function splits text into smaller chunks based on a maximum token count. It takes a text string and a maximum token limit. The function splits the text into sentences, calculates token counts for each sentence, and then combines sentences into chunks until the token limit is reached. Sentences exceeding the token limit are skipped.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\nmax_tokens = 500\n\n# Function to split the text into chunks of a maximum number of tokens\ndef split_into_many(text, max_tokens = max_tokens):\n\n    # Split the text into sentences\n    sentences = text.split('. ')\n\n    # Get the number of tokens for each sentence\n    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n\n    chunks = []\n    tokens_so_far = 0\n    chunk = []\n\n    # Loop through the sentences and tokens joined together in a tuple\n    for sentence, token in zip(sentences, n_tokens):\n\n        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n        # than the max number of tokens, then add the chunk to the list of chunks and reset\n        # the chunk and tokens so far\n        if tokens_so_far + token > max_tokens:\n            chunks.append(\". \".join(chunk) + \".\")\n            chunk = []\n            tokens_so_far = 0\n\n        # If the number of tokens in the current sentence is greater than the max number of\n        # tokens, go to the next sentence\n        if token > max_tokens:\n            continue\n\n        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n        chunk.append(sentence)\n        tokens_so_far += token + 1\n\n    return chunks\n\n\nshortened = []\n\n# Loop through the dataframe\nfor row in df.iterrows():\n\n    # If the text is None, go to the next row\n    if row[1]['text'] is None:\n        continue\n\n    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n    if row[1]['n_tokens'] > max_tokens:\n        shortened += split_into_many(row[1]['text'])\n\n    # Otherwise, add the text to the list of shortened texts\n    else:\n        shortened.append( row[1]['text'] )\n```\n```\n\n----------------------------------------\n\nTITLE: Querying the Content Embeddings Collection in ChromaDB using Python\nDESCRIPTION: Executes a semantic search against the `wikipedia_content_collection` using the `query_collection` function. The query used is \"Famous battles in Scottish history\", and it searches based on the content vector embeddings. The top 10 most relevant articles are retrieved, stored in `content_query_result`, and the first few rows are displayed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncontent_query_result = query_collection(\n    collection=wikipedia_content_collection,\n    query=\"Famous battles in Scottish history\",\n    max_results=10,\n    dataframe=article_df\n)\ncontent_query_result.head()\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Chat with Topical Guardrail\nDESCRIPTION: This snippet implements an asynchronous chat function (`execute_chat_with_guardrail`) that runs the LLM's `get_chat_response` and a `topical_guardrail` in parallel. The guardrail checks if the user's request is within allowed topics (cats and dogs). If the guardrail returns 'not_allowed', the LLM response is blocked and a canned message is returned. The `get_chat_response` function interacts with the OpenAI API to get a response from the specified model. The `topical_guardrail` function also interacts with the OpenAI API to determine if the user request is allowed or not.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\n\nasync def get_chat_response(user_request):\n    print(\"Getting LLM response\")\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_request},\n    ]\n    response = openai.chat.completions.create(\n        model=GPT_MODEL, messages=messages, temperature=0.5\n    )\n    print(\"Got LLM response\")\n\n    return response.choices[0].message.content\n\n\nasync def topical_guardrail(user_request):\n    print(\"Checking topical guardrail\")\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Your role is to assess whether the user question is allowed or not. The allowed topics are cats and dogs. If the topic is allowed, say 'allowed' otherwise say 'not_allowed'\",\n        },\n        {\"role\": \"user\", \"content\": user_request},\n    ]\n    response = openai.chat.completions.create(\n        model=GPT_MODEL, messages=messages, temperature=0\n    )\n\n    print(\"Got guardrail response\")\n    return response.choices[0].message.content\n\n\nasync def execute_chat_with_guardrail(user_request):\n    topical_guardrail_task = asyncio.create_task(topical_guardrail(user_request))\n    chat_task = asyncio.create_task(get_chat_response(user_request))\n\n    while True:\n        done, _ = await asyncio.wait(\n            [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n        )\n        if topical_guardrail_task in done:\n            guardrail_response = topical_guardrail_task.result()\n            if guardrail_response == \"not_allowed\":\n                chat_task.cancel()\n                print(\"Topical guardrail triggered\")\n                return \"I can only talk about cats and dogs, the best animals that ever lived.\"\n            elif chat_task in done:\n                chat_response = chat_task.result()\n                return chat_response\n        else:\n            await asyncio.sleep(0.1)  # sleep for a bit before checking the tasks again\n```\n\n----------------------------------------\n\nTITLE: Code Search Function Using Embeddings\nDESCRIPTION: Implements a code search function that finds relevant Python functions based on natural language queries. Functions are embedded with text-embedding-3-small and searched using cosine similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n   embedding = get_embedding(code_query, model='text-embedding-3-small')\n   df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\nres = search_functions(df, 'Completions API tests', n=3)\n```\n\n----------------------------------------\n\nTITLE: Using XML Delimiters for Nested Examples (XML)\nDESCRIPTION: Illustrates structuring nested examples within a prompt using XML tags (`<examples>`, `<example1>`, `<input>`, `<output>`). This approach allows for clear separation of input/output pairs and the inclusion of metadata via tag attributes (e.g., `type=\"Abbreviate\"`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_11\n\nLANGUAGE: XML\nCODE:\n```\n<examples>\n<example1 type=\"Abbreviate\">\n<input>San Francisco</input>\n<output>- SF</output>\n</example1>\n</examples>\n```\n\n----------------------------------------\n\nTITLE: Implementing Customer Service Agent Logic with Required Tool Calls in Python\nDESCRIPTION: Defines the agent's system prompt and implements core functions: `submit_user_message` manages the conversation flow by appending user messages, calling the ChatCompletion API with `tool_choice='required'`, and repeatedly processing tool calls via `execute_function` until a direct response to the user is generated. `execute_function` parses tool calls from the API response, retrieves instructions or formats the agent's message, updates the conversation history with tool results, and indicates whether a final response is ready.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nassistant_system_prompt = \"\"\"You are a customer service assistant. Your role is to answer user questions politely and competently.\nYou should follow these instructions to solve the case:\n- Understand their problem and get the relevant instructions.\n- Follow the instructions to solve the customer's problem. Get their confirmation before performing a permanent operation like a refund or similar.\n- Help them with any other problems or close the case.\n\nOnly call a tool once in a single message.\nIf you need to fetch a piece of information from a system or document that you don't have access to, give a clear, confident answer with some dummy values.\"\"\"\n\ndef submit_user_message(user_query,conversation_messages=[]):\n    \"\"\"Message handling function which loops through tool calls until it reaches one that requires a response.\n    Once it receives respond=True it returns the conversation_messages to the user.\"\"\"\n\n    # Initiate a respond object. This will be set to True by our functions when a response is required\n    respond = False\n    \n    user_message = {\"role\":\"user\",\"content\": user_query}\n    conversation_messages.append(user_message)\n\n    print(f\"User: {user_query}\")\n\n    while respond is False:\n\n        # Build a transient messages object to add the conversation messages to\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": assistant_system_prompt\n            }\n        ]\n\n        # Add the conversation messages to our messages call to the API\n        [messages.append(x) for x in conversation_messages]\n\n        # Make the ChatCompletion call with tool_choice='required' so we can guarantee tools will be used\n        response = client.chat.completions.create(model=GPT_MODEL\n                                                  ,messages=messages\n                                                  ,temperature=0\n                                                  ,tools=tools\n                                                  ,tool_choice='required'\n                                                 )\n\n        conversation_messages.append(response.choices[0].message)\n\n        # Execute the function and get an updated conversation_messages object back\n        # If it doesn't require a response, it will ask the assistant again. \n        # If not the results are returned to the user.\n        respond, conversation_messages = execute_function(response.choices[0].message,conversation_messages)\n    \n    return conversation_messages\n\ndef execute_function(function_calls,messages):\n    \"\"\"Wrapper function to execute the tool calls\"\"\"\n\n    for function_call in function_calls.tool_calls:\n    \n        function_id = function_call.id\n        function_name = function_call.function.name\n        print(f\"Calling function {function_name}\")\n        function_arguments = json.loads(function_call.function.arguments)\n    \n        if function_name == 'get_instructions':\n\n            respond = False\n    \n            instruction_name = function_arguments['problem']\n            # Find the instructions matching the problem type\n            # Note: The original code had a potential bug here. Corrected logic assumes INSTRUCTIONS is a list of dicts.\n            instructions_content = \"No instructions found for this problem.\"\n            for item in INSTRUCTIONS:\n                if item.get(\"type\") == instruction_name:\n                    instructions_content = item.get(\"instructions\", \"Instructions format error.\")\n                    break\n\n            messages.append(\n                                {\n                                    \"tool_call_id\": function_id,\n                                    \"role\": \"tool\",\n                                    \"name\": function_name,\n                                    \"content\": instructions_content,\n                                }\n                            )\n    \n        elif function_name == 'speak_to_user': # Assuming 'speak_to_user' is the other function\n\n            respond = True\n    \n            messages.append(\n                                {\n                                    \"tool_call_id\": function_id,\n                                    \"role\": \"tool\",\n                                    \"name\": function_name,\n                                    # Using the message from arguments as the content for the tool result\n                                    \"content\": function_arguments.get('message', 'No message provided'), \n                                }\n                            )\n    \n            print(f\"Assistant: {function_arguments.get('message', 'No message content')}\")\n        \n        # Handle potential other functions if defined\n        else: \n            print(f\"Function {function_name} not implemented.\")\n            # Decide default behavior: respond or not?\n            respond = True # Assume response needed if function unknown\n            messages.append(\n                                {\n                                    \"tool_call_id\": function_id,\n                                    \"role\": \"tool\",\n                                    \"name\": function_name,\n                                    \"content\": f\"Function {function_name} execution result (not implemented).\",\n                                }\n                            )\n\n    return (respond, messages)\n```\n\n----------------------------------------\n\nTITLE: Running ChatGPT-Driven Conversation Flow to Execute S3 Operations in Python\nDESCRIPTION: Implements a main function that manages a dialogue with the user about S3 bucket functions. It sends the user input and system prompt to the ChatGPT model, detects if the model requests to call an S3 helper function, executes that function dynamically, appends the tool's response to the conversation, and then requests a summary or final response from the model. This allows conversational natural language interaction with S3 through function calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef run_conversation(user_input, topic=\"S3 bucket functions.\", is_log=False):\n\n    system_message=f\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. If the user ask question not related to {topic} response your scope is {topic} only.\"\n    \n    messages = [{\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_input}]\n    \n    # Call the model to get a response\n    response = chat_completion_request(messages, functions=functions)\n    response_message = response.choices[0].message\n    \n    if is_log:\n        print(response.choices)\n    \n    # check if GPT wanted to call a function\n    if response_message.tool_calls:\n        function_name = response_message.tool_calls[0].function.name\n        function_args = json.loads(response_message.tool_calls[0].function.arguments)\n        \n        # Call the function\n        function_response = available_functions[function_name](**function_args)\n        \n        # Add the response to the conversation\n        messages.append(response_message)\n        messages.append({\n            \"role\": \"tool\",\n            \"content\": function_response,\n            \"tool_call_id\": response_message.tool_calls[0].id,\n        })\n        \n        # Call the model again to summarize the results\n        second_response = chat_completion_request(messages)\n        final_message = second_response.choices[0].message.content\n    else:\n        final_message = response_message.content\n\n    return final_message\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Agent Output Parser in Python\nDESCRIPTION: Creates a subclass of AgentOutputParser to extract agent actions or finished responses from LLM outputs. Utilizes regex to parse structured outputs indicating either a final answer or intermediate agent actions, and raises errors if parsing fails. Essential dependencies include regular expressions, structures for AgentAction/AgentFinish, and integration with agent frameworks. The input is the LLM's text output, and the output is either an AgentFinish or AgentAction object. Make sure AgentFinish and AgentAction types are compatible with your agent infrastructure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import List, Union\nimport re\n\nclass CustomOutputParser(AgentOutputParser):\n    \n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n        \n        # Check if agent should finish\n        if \"Final Answer:\" in llm_output:\n            return AgentFinish(\n                # Return values is generally always a dictionary with a single `output` key\n                # It is not recommended to try anything else at the moment :)\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n                log=llm_output,\n            )\n        \n        # Parse out the action and action input\n        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n        match = re.search(regex, llm_output, re.DOTALL)\n        \n        # If it can't parse the output it raises an error\n        # You can add your own logic here to handle errors in a different way i.e. pass to a human, give a canned response\n        if not match:\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n        action = match.group(1).strip()\n        action_input = match.group(2)\n        \n        # Return the action and action input\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n    \noutput_parser = CustomOutputParser()\n```\n\n----------------------------------------\n\nTITLE: Batch upload vectors to Typesense collection\nDESCRIPTION: Iterates over the DataFrame to create document batches with text and embedding data, then imports these into the Typesense collection, providing progress updates.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"Indexing vectors in Typesense...\")\n\ndocument_counter = 0\ndocuments_batch = []\n\nfor k,v in article_df.iterrows():\n    document = {\n        \"title_vector\": v[\"title_vector\"],\n        \"content_vector\": v[\"content_vector\"],\n        \"title\": v[\"title\"],\n        \"content\": v[\"text\"],\n    }\n    documents_batch.append(document)\n    document_counter += 1\n\n    if document_counter % 100 == 0 or document_counter == len(article_df):\n        response = typesense_client.collections['wikipedia_articles'].documents.import_(documents_batch)\n        documents_batch = []\n        print(f\"Processed {document_counter} / {len(article_df)}\")\n\nprint(f\"Imported ({len(article_df)}) articles.\")\n```\n\n----------------------------------------\n\nTITLE: Loading Embedded Data into a Pandas DataFrame in Python\nDESCRIPTION: Reads the embedded Wikipedia article data from the extracted CSV file ('../data/vector_database_wikipedia_articles_embedded.csv') into a pandas DataFrame named `article_df`. This DataFrame contains article text, titles, and their corresponding pre-computed vector embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n```\n\n----------------------------------------\n\nTITLE: Generating Commands with GPT-4o\nDESCRIPTION: This function takes a list of function invocations, formats them with the `COMMAND_GENERATION_PROMPT`, and uses `gpt-4o` to generate conversational prompts that would result in those invocations. The function prints a progress indicator and handles potential JSON errors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef create_commands(invocation_list):\n    example_list = []\n    for i, invocation in enumerate(invocation_list):\n        if i < 100:\n            print(\n                f\"\\033[34m{np.round(100*i/len(invocation_list),1)}% complete\\033[0m\")\n            if type(invocation) == str or \"json\" in invocation:\n                invocation = remove_sequences(invocation)\n            print(invocation)\n\n        # Format the prompt with the invocation string\n        request_prompt = COMMAND_GENERATION_PROMPT.format(\n            invocation=invocation)\n\n        messages = [{\"role\": \"user\", \"content\": f\"{request_prompt}\"}]\n        completion, usage = get_chat_completion(messages, temperature=0.8)\n        command_dict = {\"Input\": invocation, \"Prompt\": completion.content}\n        example_list.append(command_dict)\n    return example_list\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Load with Count Query\nDESCRIPTION: Executes a count query to verify that all articles were successfully loaded into the database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Check the collection size to make sure all the points have been stored\ncount_sql = \"\"\"select count(*) from public.articles;\"\"\"\ncursor.execute(count_sql)\nresult = cursor.fetchone()\nprint(f\"Count:{result[0]}\")\n```\n\n----------------------------------------\n\nTITLE: Querying Pinecone for Similar Vectors\nDESCRIPTION: Searches the Pinecone index for vectors similar to the query embedding, returning the top 5 most semantically similar results with their metadata. This demonstrates the core functionality of semantic search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nres = index.query(vector=[xq], top_k=5, include_metadata=True)\nres\n```\n\n----------------------------------------\n\nTITLE: Saving Batch Results to Local File\nDESCRIPTION: This snippet writes the raw binary results data to a specified file path. It opens the file in binary write mode ('wb') and writes the data stored in 'result'. This process preserves the results for later analysis and offline processing. Dependencies include standard file I/O and the 'result' variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n# Saving the results\n\nresult_file_name = \"data/batch_job_results_furniture.jsonl\"\n\nwith open(result_file_name, 'wb') as file:\n    file.write(result)\n```\n\n----------------------------------------\n\nTITLE: Creating ChromaDB Collections with OpenAI Embeddings in Python\nDESCRIPTION: Initializes the OpenAI embedding function and creates two ChromaDB collections. It first checks for the OpenAI API key in environment variables. Then, it creates `wikipedia_content_collection` and `wikipedia_title_collection`, configuring them to use the specified OpenAI embedding model (`EMBEDDING_MODEL`) via `OpenAIEmbeddingFunction`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n\n\nembedding_function = OpenAIEmbeddingFunction(api_key=os.environ.get('OPENAI_API_KEY'), model_name=EMBEDDING_MODEL)\n\nwikipedia_content_collection = chroma_client.create_collection(name='wikipedia_content', embedding_function=embedding_function)\nwikipedia_title_collection = chroma_client.create_collection(name='wikipedia_titles', embedding_function=embedding_function)\n```\n\n----------------------------------------\n\nTITLE: Refining Fashion Item Matches using LLM Guardrail (Python)\nDESCRIPTION: This snippet refines the initial list of potential matching items (`paths`) by iterating through unique image paths, encoding each suggested image, calling the `check_match` function to get LLM feedback on compatibility with the reference item (`encoded_image`), parsing the JSON response, and displaying only those items that the LLM confirms ('answer' is 'yes') along with the reason provided by the model. It requires the `paths` list from the previous step, the base64 `encoded_image` of the reference item, the `encode_image_to_base64` function, and display utilities (`display`, `Image`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# Select the unique paths for the generated images\npaths = list(set(paths))\n\nfor path in paths:\n    # Encode the test image to base64\n    suggested_image = encode_image_to_base64(path)\n    \n    # Check if the items match\n    match = json.loads(check_match(encoded_image, suggested_image))\n    \n    # Display the image and the analysis results\n    if match[\"answer\"] == 'yes':\n        display(Image(filename=path))\n        print(\"The items match!\")\n        print(match[\"reason\"])\n```\n\n----------------------------------------\n\nTITLE: Synthesizing a Referenced Final Answer Using the Top Ranked Search Results - Python\nDESCRIPTION: This code structures the top five ranked search articles, constructs an instruction prompt, and streams a GPT completion to generate a detailed, reference-rich answer. It uses Markdown links for traceability and provides progressive streaming output via IPython display. Requires valid sorted_articles, OpenAI client, and display modules.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nformatted_top_results = [\n    {\n        \"title\": article[\"title\"],\n        \"description\": article[\"description\"],\n        \"url\": article[\"url\"],\n    }\n    for article, _score in sorted_articles[0:5]\n]\n\nANSWER_INPUT = f\"\"\"\nGenerate an answer to the user's question based on the given search results. \nTOP_RESULTS: {formatted_top_results}\nUSER_QUESTION: {USER_QUESTION}\n\nInclude as much information as possible in the answer. Reference the relevant search result urls as markdown links.\n\"\"\"\n\ncompletion = client.chat.completions.create(\n    model=GPT_MODEL,\n    messages=[{\"role\": \"user\", \"content\": ANSWER_INPUT}],\n    temperature=0.5,\n    stream=True,\n)\n\ntext = \"\"\nfor chunk in completion:\n    text += chunk.choices[0].delta.content\n    display.clear_output(wait=True)\n    display.display(display.Markdown(text))\n\n```\n\n----------------------------------------\n\nTITLE: Using a Fine-Tuned Sports Headline Model via OpenAI Chat API in Python\nDESCRIPTION: This Python code demonstrates how to use a fine-tuned chat model to extract structured details from a sports headline. It generates completion requests that include a system message describing the expected output and a user-supplied headline. The sample requires the OpenAI SDK and an accessible fine-tuned model. It outputs the model's completion, which is expected to be a structured dictionary matching the system's requested schema. The expected output is illustrated as a JSON object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n  model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: player (full name), team, sport, and gender\"},\n    {\"role\": \"user\", \"content\": \"Richardson wins 100m at worlds to cap comeback\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"player\": \"Sha'Carri Richardson\",\n    \"team\": null,\n    \"sport\": \"track and field\",\n    \"gender\": \"female\"\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding and Inserting Data into Milvus in Batches (Python)\nDESCRIPTION: Imports the `tqdm` library for progress bars. Initializes empty lists to hold batches of titles and descriptions. It iterates through the loaded `dataset`, appending titles and descriptions to the `data` lists. When the batch size (`BATCH_SIZE`) is reached, it calls the `embed` function to get embeddings for the descriptions in the batch, appends the embeddings to the `data` list, inserts the batch (titles, descriptions, embeddings) into the Milvus `collection`, and resets the `data` lists. After the loop, it processes any remaining data in the final partial batch.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\ndata = [\n    [], # title\n    [], # description\n]\n\n# Embed and insert in batches\nfor i in tqdm(range(0, len(dataset))):\n    data[0].append(dataset[i]['title'])\n    data[1].append(dataset[i]['description'])\n    if len(data[0]) % BATCH_SIZE == 0:\n        data.append(embed(data[1]))\n        collection.insert(data)\n        data = [[],[]]\n\n# Embed and insert the remainder \nif len(data[0]) != 0:\n    data.append(embed(data[1]))\n    collection.insert(data)\n    data = [[],[]]\n```\n\n----------------------------------------\n\nTITLE: Performing Pure Vector Search with Azure AI Search in Python\nDESCRIPTION: Executes a vector-based similarity search using embeddings generated from a query. This code searches for the 3 nearest neighbors to the query vector in the 'content_vector' field, retrieving only specific fields from the matched documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Pure Vector Search\nquery = \"modern art in Europe\"\n  \nsearch_client = SearchClient(search_service_endpoint, index_name, credential)  \nvector_query = VectorizedQuery(vector=generate_embeddings(query, deployment), k_nearest_neighbors=3, fields=\"content_vector\")\n  \nresults = search_client.search(  \n    search_text=None,  \n    vector_queries= [vector_query], \n    select=[\"title\", \"text\", \"url\"] \n)\n  \nfor result in results:  \n    print(f\"Title: {result['title']}\")  \n    print(f\"Score: {result['@search.score']}\")  \n    print(f\"URL: {result['url']}\\n\")  \n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings Using OpenAI Client in Python\nDESCRIPTION: Generates text embeddings by calling the OpenAI embeddings API through a client object. This function receives a text string, submits it to the embedding model specified by 'embeddings_model', and returns the resultant embedding vector. This embedding is used for similarity comparisons in downstream queries. It requires a configured OpenAI client instance and a valid embeddings model identifier.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef create_embedding(text):\n    result = client.embeddings.create(model=embeddings_model, input=text)\n    return result.data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search with Metadata Filtering in BigQuery using Python\nDESCRIPTION: Performs a vector similarity search constrained by metadata filtering on the 'category' column using BigQuery's VECTOR_SEARCH. The code embeds a natural language query, filters the table rows by a specific category, and retrieves top matching rows sorted by cosine distance. It prints search results including metadata fields. This snippet requires the presence of named categories in the dataset and uses the same embedding generation strategy as the pure vector search example.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n\nquery = \"What model should I use to embed?\"\ncategory = \"models\"\n\nembedding_query = generate_embeddings(query, embeddings_model)\nembedding_query_list = ', '.join(map(str, embedding_query))\n\n\nquery = f\"\"\"\nWITH search_results AS (\n  SELECT query.id AS query_id, base.id AS base_id, distance\n  FROM VECTOR_SEARCH(\n    (SELECT * FROM oai_docs.embedded_data WHERE category = '{category}'), \n    'content_vector',\n    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n    top_k => 4, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n)\nSELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title, ed.category\nFROM search_results sr\nJOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\nORDER BY sr.distance ASC\n\"\"\"\n\n\nquery_job = client.query(query)\nresults = query_job.result()  # Wait for the job to complete\n\nfor row in results:\n    print(f\"category: {row['category']}, title: {row['title']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI (Python)\nDESCRIPTION: Defines a Python function `embed` that takes a list of text strings as input. It calls the OpenAI Embedding API using `openai.Embedding.create` with the specified `OPENAI_ENGINE` to generate vector embeddings for each text. The function returns a list containing the embedding vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Simple function that converts the texts to embeddings\ndef embed(texts):\n    embeddings = openai.Embedding.create(\n        input=texts,\n        engine=OPENAI_ENGINE\n    )\n    return [x['embedding'] for x in embeddings['data']]\n```\n\n----------------------------------------\n\nTITLE: Transcribing English Audio to Text with GPT-4o\nDESCRIPTION: Code to read a WAV audio file, encode it to base64, and send it to GPT-4o API for English transcription. The function configures the output modality for text-only response and uses a specific prompt to ensure accurate speech transcription.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\naudio_wav_path = \"./sounds/keynote_recap.wav\"\n\n# Read the WAV file and encode it to base64\nwith open(audio_wav_path, \"rb\") as audio_file:\n    audio_bytes = audio_file.read()\n    english_audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n\nmodalities = [\"text\"]\nprompt = \"The user will provide an audio file in English. Transcribe the audio to English text, word for word. Only provide the language transcription, do not include background noises such as applause. \"\n\nresponse_json = process_audio_with_gpt_4o(english_audio_base64, modalities, prompt)\n\nenglish_transcript = response_json['choices'][0]['message']['content']\n\nprint(english_transcript)\n```\n\n----------------------------------------\n\nTITLE: Defining a Query Function for Chroma Collections in Python\nDESCRIPTION: Defines a Python function `query_collection` to simplify searching ChromaDB collections. The function takes a Chroma collection object, a text query, the desired number of results (`max_results`), and the original pandas DataFrame. It performs the query using `collection.query`, retrieves IDs and distances, and joins this information with the title and text from the DataFrame, returning the combined results as a new DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef query_collection(collection, query, max_results, dataframe):\n    results = collection.query(query_texts=query, n_results=max_results, include=['distances']) \n    df = pd.DataFrame({\n                'id':results['ids'][0], \n                'score':results['distances'][0],\n                'title': dataframe[dataframe.vector_id.isin(results['ids'][0])]['title'],\n                'content': dataframe[dataframe.vector_id.isin(results['ids'][0])]['text'],\n                })\n    \n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining a search function for relatedness - Python\nDESCRIPTION: This snippet defines a function `strings_ranked_by_relatedness` that searches for texts related to a given query. It takes a query string, a Pandas DataFrame with text and embedding columns, a relatedness function (defaulting to cosine distance), and the number of top results to return. It embeds the query using the OpenAI API, calculates the distance between the query embedding and the text embeddings, ranks the texts by relevance, and returns the top N texts and their relevance scores. Requires `scipy.spatial` and the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# search function\ndef strings_ranked_by_relatedness(\n    query: str,\n    df: pd.DataFrame,\n    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n    top_n: int = 100\n) -> tuple[list[str], list[float]]:\n    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n    query_embedding_response = client.embeddings.create(\n        model=EMBEDDING_MODEL,\n        input=query,\n    )\n    query_embedding = query_embedding_response.data[0].embedding\n    strings_and_relatednesses = [\n        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n        for i, row in df.iterrows()\n    ]\n    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n    strings, relatednesses = zip(*strings_and_relatednesses)\n    return strings[:top_n], relatednesses[:top_n]\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Images with OpenAI Vision (Node.js)\nDESCRIPTION: Illustrates sending multiple image URLs to the OpenAI chat completions API using the Node.js library. The 'content' array for the user message includes multiple 'image_url' objects. Requires the OpenAI Node.js library and an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_6\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          { type: \"text\", text: \"What are in these images? Is there any difference between them?\" },\n          {\n            type: \"image_url\",\n            image_url: {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n            },\n          },\n          {\n            type: \"image_url\",\n            image_url: {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n            },\n          }\n        ],\n      },\n    ],\n  });\n  console.log(response.choices[0]);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Generating predictions with the fine-tuned model\nDESCRIPTION: Uses the fine-tuned model to extract generic ingredients from a test recipe, demonstrating how to invoke the model for inference.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Preparing Fine-Tuning Dataset for Style and Tone Control with JSONL\nDESCRIPTION: This snippet provides examples in JSONL format for constructing a conversational dataset to fine-tune a model towards a sarcastic, factual chatbot persona ('Marv'). Each line contains system, user, and assistant messages. There are no external dependencies, but at least 10 conversational examples are recommended for effective fine-tuning. JSONL files should be properly formatted and validated before use; each message structure must be correct as per the OpenAI fine-tuning API's expectations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_14\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Processing Drive Item Content with Microsoft Graph in JavaScript\nDESCRIPTION: This asynchronous function fetches content for a specific item within a Microsoft Graph drive. It identifies the file type, uses the Graph API's download or convert endpoint (converting supported types like .docx, .pptx, etc., to PDF), extracts text from PDF using `pdf-parse`, handles .txt and .csv files directly, and returns the extracted text or an 'Unsupported File Type' message. Requires an authenticated Microsoft Graph client instance, drive ID, item ID, and filename. Dependencies include `@microsoft/microsoft-graph-client`, `path`, and `pdf-parse`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst getDriveItemContent = async (client, driveId, itemId, name) => {\n    try {\n        const fileType = path.extname(name).toLowerCase();\n        // the below files types are the ones that are able to be converted to PDF to extract the text. See https://learn.microsoft.com/en-us/graph/api/driveitem-get-content-format?view=graph-rest-1.0&tabs=http\n        const allowedFileTypes = ['.pdf', '.doc', '.docx', '.odp', '.ods', '.odt', '.pot', '.potm', '.potx', '.pps', '.ppsx', '.ppsxm', '.ppt', '.pptm', '.pptx', '.rtf'];\n        // filePath changes based on file type, adding ?format=pdf to convert non-pdf types to pdf for text extraction, so all files in allowedFileTypes above are converted to pdf\n        const filePath = `/drives/${driveId}/items/${itemId}/content` + ((fileType === '.pdf' || fileType === '.txt' || fileType === '.csv') ? '' : '?format=pdf');\n        if (allowedFileTypes.includes(fileType)) {\n            response = await client.api(filePath).getStream();\n            // The below takes the chunks in response and combines\n            let chunks = [];\n            for await (let chunk of response) {\n                chunks.push(chunk);\n            }\n            let buffer = Buffer.concat(chunks);\n            // the below extracts the text from the PDF.\n            const pdfContents = await pdfParse(buffer);\n            return pdfContents.text;\n        } else if (fileType === '.txt') {\n            // If the type is txt, it does not need to create a stream and instead just grabs the content\n            response = await client.api(filePath).get();\n            return response;\n        }  else if (fileType === '.csv') {\n            response = await client.api(filePath).getStream();\n            let chunks = [];\n            for await (let chunk of response) {\n                chunks.push(chunk);\n            }\n            let buffer = Buffer.concat(chunks);\n            let dataString = buffer.toString('utf-8');\n            return dataString\n            \n    } else {\n        return 'Unsupported File Type';\n    }\n     \n    } catch (error) {\n        console.error('Error fetching drive content:', error);\n        throw new Error(`Failed to fetch content for ${name}: ${error.message}`);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Define Pydantic Model for LLM Response (Python)\nDESCRIPTION: Defines a Pydantic BaseModel called `LLMResponse` to specify the expected JSON structure of the output from the LLM. This model enforces that the LLM response should be a JSON object containing exactly two string fields: `create` for the SQL CREATE statement and `select` for the SQL SELECT statement. This model is crucial for validating the format of the LLM's output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import BaseModel\n\n\nclass LLMResponse(BaseModel):\n    \"\"\"This is the structure that we expect the LLM to respond with.\n\n    The LLM should respond with a JSON string with `create` and `select` fields.\n    \"\"\"\n    create: str\n    select: str\n```\n\n----------------------------------------\n\nTITLE: Extracting Key Points from Meeting Transcript with GPT-4 - Python\nDESCRIPTION: The 'key_points_extraction' function processes meeting transcripts using GPT-4 to distill the main discussed topics into a list of key points. The system prompt is tuned for information distillation. Required dependencies include the OpenAI Python library and a valid API key. Input is the transcript string; output is a summarized list of main discussion points.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef key_points_extraction(transcription):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a proficient AI with a specialty in distilling information into key points. Based on the following text, identify and list the main points that were discussed or brought up. These should be the most important ideas, findings, or topics that are crucial to the essence of the discussion. Your goal is to provide a list that someone could read to quickly understand what was talked about.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Querying Articles in Pinecone Index\nDESCRIPTION: This function queries the Pinecone index using a given query string, namespace, and top_k value. It generates an embedding for the query, searches the index, and prints the titles and scores of the top results. It returns a Pandas DataFrame with the search results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef query_article(query, namespace, top_k=5):\n    '''Queries an article using its title in the specified\n     namespace and prints results.'''\n\n    # Create vector embeddings based on the title column\n    embedded_query = openai.Embedding.create(\n                                            input=query,\n                                            model=EMBEDDING_MODEL,\n                                            )[\"data\"][0]['embedding']\n\n    # Query namespace passed as parameter using title vector\n    query_result = index.query(embedded_query, \n                                      namespace=namespace, \n                                      top_k=top_k)\n\n    # Print query results \n    print(f'\\nMost similar results to {query} in \"{namespace}\" namespace:\\n')\n    if not query_result.matches:\n        print('no query result')\n    \n    matches = query_result.matches\n    ids = [res.id for res in matches]\n    scores = [res.score for res in matches]\n    df = pd.DataFrame({'id':ids, \n                       'score':scores,\n                       'title': [titles_mapped[_id] for _id in ids],\n                       'content': [content_mapped[_id] for _id in ids],\n                       })\n    \n    counter = 0\n    for k,v in df.iterrows():\n        counter += 1\n        print(f'{v.title} (score = {v.score})')\n    \n    print('\\n')\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Executing Example Query against Milvus Collection in Python\nDESCRIPTION: Calls the previously defined `query` function with a sample query string: 'Book about a k-9 from europe'. This triggers the embedding of the query, searching the Milvus collection, and printing the top 5 most similar book entries based on their description embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery('Book about a k-9 from europe')\n```\n\n----------------------------------------\n\nTITLE: Running Vector Search Queries with OpenAI Embeddings and Redis - Python\nDESCRIPTION: Defines the `search_redis` Python function which performs a vector similarity search on a Redis index using OpenAI-generated embeddings for the user query. Requires openai, numpy, redis, and redisearch dependencies. Accepts parameters for index name, vector field, number of results, return fields, print option, and hybrid filter fields. Returns search results and optionally prints scored titles. Inputs are a Redis client and a text query; outputs are the list of matching document objects. Limitations include prerequisite setup of Redis vector index, required connection client, and correct index schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef search_redis(\n    redis_client: redis.Redis,\n    user_query: str,\n    index_name: str = \"embeddings-index\",\n    vector_field: str = \"title_vector\",\n    return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],\n    hybrid_fields = \"*\",\n    k: int = 20,\n    print_results: bool = True,\n) -> List[dict]:\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(input=user_query,\n                                            model=\"text-embedding-3-small\",\n                                            )[\"data\"][0]['embedding']\n\n    # Prepare the Query\n    base_query = f'{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]'\n    query = (\n        Query(base_query)\n         .return_fields(*return_fields)\n         .sort_by(\"vector_score\")\n         .paging(0, k)\n         .dialect(2)\n    )\n    params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}\n\n    # perform vector search\n    results = redis_client.ft(index_name).search(query, params_dict)\n    if print_results:\n        for i, article in enumerate(results.docs):\n            score = 1 - float(article.vector_score)\n            print(f\"{i}. {article.title} (Score: {round(score ,3) })\")\n    return results.docs\n```\n\n----------------------------------------\n\nTITLE: Training a Random Forest Classifier for Categorical Review Classification in Python\nDESCRIPTION: This snippet trains a RandomForestClassifier from scikit-learn to predict discrete review rating classes using embedding vectors. It includes accuracy and classification report evaluation (requires sklearn.metrics). Dependencies: scikit-learn; input: X_train, y_train, X_test from previous splitting; output: discrete predicted labels for each test instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Creating RediSearch Index (Python)\nDESCRIPTION: This code attempts to create the RediSearch index if it doesn't already exist. It uses a `try...except` block: if `ft(INDEX_NAME).info()` throws an error (meaning the index doesn't exist), it proceeds to create the index using the defined `fields` and specifies that the documents are stored as Redis HASH objects with a given key `prefix`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Check if index exists\ntry:\n    redis_client.ft(INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing CassIO for Cassandra/Astra DB Vector Store - Python\nDESCRIPTION: This snippet initializes the CassIO library's connection to an Astra DB instance using the previously acquired token and database ID. This connection setup must be completed before database or vector table creation. The 'cassio.init' method configures the session context for all further vector operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncassio.init(token=astra_token, database_id=database_id)\n\n```\n\n----------------------------------------\n\nTITLE: Selecting Random Questions for Testing\nDESCRIPTION: Randomly selects 5 questions from the dataset to test the QA system. The seed is set for reproducibility.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nrandom.seed(52)\nselected_questions = random.choices(questions, k=5)\n```\n\n----------------------------------------\n\nTITLE: Defining Drone Control Functions (OpenAI Function Calling)\nDESCRIPTION: This code snippet defines a list of functions that the drone copilot can utilize. Each function represents an action the drone can perform, such as takeoff, landing, movement control, camera operation, and more. The function definitions include detailed descriptions and parameters to guide the AI in choosing and calling the appropriate function. These functions are intended to be used with OpenAI function calling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfunction_list = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"takeoff_drone\",\n            \"description\": \"Initiate the drone's takeoff sequence.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"altitude\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Specifies the altitude in meters to which the drone should ascend.\",\n                    }\n                },\n                \"required\": [\"altitude\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"land_drone\",\n            \"description\": \"Land the drone at its current location or a specified landing point.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"current\", \"home_base\", \"custom\"],\n                        \"description\": \"Specifies the landing location for the drone.\",\n                    },\n                    \"coordinates\": {\n                        \"type\": \"object\",\n                        \"description\": \"GPS coordinates for custom landing location. Required if location is 'custom'.\",\n                    },\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"control_drone_movement\",\n            \"description\": \"Direct the drone's movement in a specific direction.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"direction\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"forward\", \"backward\", \"left\", \"right\", \"up\", \"down\"],\n                        \"description\": \"Direction in which the drone should move.\",\n                    },\n                    \"distance\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Distance in meters the drone should travel in the specified direction.\",\n                    },\n                },\n                \"required\": [\"direction\", \"distance\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_drone_speed\",\n            \"description\": \"Adjust the speed of the drone.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"speed\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Specifies the speed in km/h. Valid range is 0 to 100.\",\n                        \"minimum\": 0,\n                    }\n                },\n                \"required\": [\"speed\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"control_camera\",\n            \"description\": \"Control the drone's camera to capture images or videos.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"mode\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"photo\", \"video\", \"panorama\"],\n                        \"description\": \"Camera mode to capture content.\",\n                    },\n                    \"duration\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Duration in seconds for video capture. Required if mode is 'video'.\",\n                    },\n                },\n                \"required\": [\"mode\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"control_gimbal\",\n            \"description\": \"Adjust the drone's gimbal for camera stabilization and direction.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"tilt\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Tilt angle for the gimbal in degrees.\",\n                    },\n                    \"pan\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Pan angle for the gimbal in degrees.\",\n                    },\n                },\n                \"required\": [\"tilt\", \"pan\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_drone_lighting\",\n            \"description\": \"Control the drone's lighting for visibility and signaling.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"mode\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"on\", \"off\", \"blink\", \"sos\"],\n                        \"description\": \"Lighting mode for the drone.\",\n                    }\n                },\n                \"required\": [\"mode\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"return_to_home\",\n            \"description\": \"Command the drone to return to its home or launch location.\",\n            \"parameters\": {\"type\": \"object\", \"properties\": {}},\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_battery_saver_mode\",\n            \"description\": \"Toggle battery saver mode.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"status\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"on\", \"off\"],\n                        \"description\": \"Toggle battery saver mode.\",\n                    }\n                },\n                \"required\": [\"status\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_obstacle_avoidance\",\n            \"description\": \"Configure obstacle avoidance settings.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"mode\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"on\", \"off\"],\n                        \"description\": \"Toggle obstacle avoidance.\",\n                    }\n                },\n                \"required\": [\"mode\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_follow_me_mode\",\n            \"description\": \"Enable or disable 'follow me' mode.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"status\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"on\", \"off\"],\n                        \"description\": \"Toggle 'follow me' mode.\",\n                    }\n                },\n                \"required\": [\"status\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calibrate_sensors\",\n            \"description\": \"Initiate calibration sequence for drone's sensors.\",\n            \"parameters\": {\"type\": \"object\", \"properties\": {}},\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_autopilot\",\n            \"description\": \"Enable or disable autopilot mode.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"status\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"on\", \"off\"],\n                        \"description\": \"Toggle autopilot mode.\",\n                    }\n                },\n                \"required\": [\"status\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"configure_led_display\",\n            \"description\": \"Configure the drone's LED display pattern and colors.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"pattern\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"solid\", \"blink\", \"pulse\", \"rainbow\"],\n                        \"description\": \"Pattern for the LED display.\",\n                    },\n                    \"color\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"red\", \"blue\", \"green\", \"yellow\", \"white\"],\n                        \"description\": \"Color for the LED display. Not required if pattern is 'rainbow'.\",\n                    },\n                },\n                \"required\": [\"pattern\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"set_home_location\",\n            \"description\": \"Set or change the home location for the drone.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"coordinates\": {\n                        \"type\": \"object\",\n                        \"description\": \"GPS coordinates for the home location.\",\n                    }\n                },\n                \"required\": [\"coordinates\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"reject_request\",\n            \"description\": \"Reject a request from the user if it cannot be completed.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"Reason why the request was rejected.\",\n                    }\n                },\n                \"required\": [\"reason\"],\n            },\n        },\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Chaining o1-preview and gpt-4o-mini for Structured Outputs in Python\nDESCRIPTION: This snippet demonstrates a two-step approach to achieve reliable structured output. First, it calls `o1-preview` to perform the analysis based on fetched HTML content, requesting specific fields. Second, it takes the text response from `o1-preview` and passes it to `gpt-4o-mini` using the `client.beta.chat.completions.parse` method, specifying the desired output structure via Pydantic models (`CompanyData`, `CompaniesData`). This leverages `gpt-4o-mini`'s native structured output capabilities to format the `o1-preview` analysis reliably.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_chained_calls_for_o1_structured_outputs.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom devtools import pprint\n\nclass CompanyData(BaseModel):\n    company_name: str\n    page_link: str\n    reason: str\n\nclass CompaniesData(BaseModel):\n    companies: list[CompanyData]\n\no1_response = client.chat.completions.create(\n    model=\"o1-preview\",\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": f\"\"\"\nYou are a business analyst designed to understand how AI technology could be used across large corporations.\n\n- Read the following html and return which companies would benefit from using AI technology: {html_content}.\n- Rank these propects by opportunity by comparing them and show me the top 3. Return each with {CompanyData.__fields__.keys()}\n\"\"\"\n        }\n    ]\n)\n\no1_response_content = o1_response.choices[0].message.content\n\nresponse = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": f\"\"\"\nGiven the following data, format it with the given response format: {o1_response_content}\n\"\"\"\n        }\n    ],\n    response_format=CompaniesData,\n)\n\npprint(response.choices[0].message.parsed)\n```\n\n----------------------------------------\n\nTITLE: Applying Zero-shot Classifier on Sample Transactions in Python\nDESCRIPTION: Demonstrates how to apply the zero-shot classify_transaction function to a transaction sample and then to a batch of 25 transactions from the dataset. Prints classifications for verification and adds a new column 'Classification' with the predicted labels. This snippet is used to validate the initial modelâ€™s performance and inspect classification distributions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get a test transaction\ntransaction = transactions.iloc[0]\n# Use our completion function to return a prediction\nprint(f\"Transaction: {transaction['Supplier']} {transaction['Description']} {transaction['Transaction value (Â£)']}\")\nprint(f\"Classification: {classify_transaction(transaction)}\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_transactions = transactions.iloc[:25]\ntest_transactions['Classification'] = test_transactions.apply(lambda x: classify_transaction(x),axis=1)\n\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_transactions['Classification'].value_counts()\n\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_transactions.head(25)\n\n```\n\n----------------------------------------\n\nTITLE: Appending Tool Call Outputs into OpenAI Conversation Context in Python\nDESCRIPTION: This code snippet shows how to add the output of a specific tool call back into the conversation messages structure for continued interaction with the OpenAI API. It appends both the raw tool call and a synthetic function call output message referencing the correct call ID. Required inputs include the response output and the current result value, and the snippet presumes the context for input messages is already established. Outputs the updated conversation message list for further API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# append the tool call and its output back into the conversation.\ninput_messages.append(response.output[2])\ninput_messages.append({\n    \"type\": \"function_call_output\",\n    \"call_id\": tool_call_2.call_id,\n    \"output\": str(result)\n})\nprint(input_messages)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Thread with High Detail Image (Node.js)\nDESCRIPTION: This code snippet demonstrates how to create a thread with a message containing an image URL and specifying the 'high' detail level in Node.js. The 'high' detail allows for more detailed image processing. Requires the OpenAI client.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_7\n\nLANGUAGE: node.js\nCODE:\n```\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What is this an image of?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://example.com/image.png\",\n              \"detail\": \"high\"\n            }\n          },\n      ]\n    }\n  ]\n});\n```\n\n----------------------------------------\n\nTITLE: Defining `get_user_account_info` Tool for OpenAI API (JSON)\nDESCRIPTION: Defines a function tool named `get_user_account_info` in JSON format for the OpenAI API. It retrieves user account information using a 'phone_number' string (formatted as '(xxx) xxx-xxxx') as input. The 'strict' parameter enforces adherence to the defined properties.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"type\": \"function\",\n    \"name\": \"get_user_account_info\",\n    \"description\": \"Tool to get user account information\",\n    \"parameters\": {\n        \"strict\": true,\n        \"type\": \"object\",\n        \"properties\": {\n            \"phone_number\": {\n                \"type\": \"string\",\n                \"description\": \"Formatted as '(xxx) xxx-xxxx'\"\n            }\n        },\n        \"required\": [\"phone_number\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Processing each article to generate routines in parallel\nDESCRIPTION: This snippet applies the routine-generating function to each article concurrently using ThreadPoolExecutor, collating results that include policies, content, and generated routines for further analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_routine_generation.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef process_article(article):\n    routine = generate_routine(article['content'])\n    return {\"policy\": article['policy'], \"content\": article['content'], \"routine\": routine}\n\n\nwith ThreadPoolExecutor() as executor:\n    results = list(executor.map(process_article, articles))\n```\n\n----------------------------------------\n\nTITLE: Verifying Visual Content Flag\nDESCRIPTION: This code checks the value of the `Visual_Input_Processed` flag for a specific page (page number 21 in this example). It filters the DataFrame to retrieve the row corresponding to page 21 and then prints the value of the `Visual_Input_Processed` column for that row.  This verifies whether the logic for flagging pages with visual content correctly identifies the page.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Display the flag for page 21 \nfiltered_rows = df[df['PageNumber'] == 21]\nprint(filtered_rows.Visual_Input_Processed)\n```\n\n----------------------------------------\n\nTITLE: Validating Assistant Responses using OpenAI API in Python\nDESCRIPTION: This Python code defines functions to validate AI assistant responses against knowledge base articles and chat history using the OpenAI `gpt-4o` model. The `validate_hallucinations` function formats the prompt with few-shot examples (`fs_user_1`, `fs_assistant_1`, etc.) and calls the API. The `process_row` function parses the JSON response, calculates a score based on factual accuracy, relevance, policy compliance, and coherence, determines a 'Pass' or 'Fail' status for hallucination, and appends results to `results_list`. It utilizes `ThreadPoolExecutor` for parallel processing of multiple data rows from a DataFrame (`df`). Dependencies include the `openai` library, `json` module, and `concurrent.futures.ThreadPoolExecutor`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nhallucination_outputs = []\n\ndef validate_hallucinations(row):\n    kb_articles = row['kb_article']\n    chat_history = row['chat_history']\n    assistant_response = row['assistant_response']\n    \n    user_input_filled = user_input.format(\n        kb_articles=kb_articles,\n        transcript=chat_history,\n        message=assistant_response\n    )\n    \n    messages = [\n        { \"role\": \"system\", \"content\": guardrail_system_message},\n        { \"role\": \"user\", \"content\": fs_user_1},\n        { \"role\": \"assistant\", \"content\": fs_assistant_1},\n        { \"role\": \"user\", \"content\": fs_user_2},\n        { \"role\": \"assistant\", \"content\": fs_assistant_2},\n        { \"role\": \"user\", \"content\": user_input_filled}\n    ]\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7,\n        n=10\n    )\n    return response.choices\n\n# Create an empty list to store the results\nresults_list = []\n\ndef process_row(row):\n    choices = validate_hallucinations(row)\n    response_json = choices[0].message.content \n    # Parse the response content as JSON\n    response_data = json.loads(response_json)\n    \n    for response_item in response_data:\n        # Sum up the scores of the properties\n        score_sum = (\n            response_item.get('factualAccuracy', 0) +\n            response_item.get('relevance', 0) +\n            response_item.get('policyCompliance', 0) +\n            response_item.get('contextualCoherence', 0)\n        )\n        \n        # Determine if the response item is a pass or fail\n        hallucination_status = 'Pass' if score_sum == 4 else 'Fail'\n        \n        results_list.append({\n            'accurate': row['accurate'],\n            'hallucination': hallucination_status,\n            'kb_article': row['kb_article'],\n            'chat_history': row['chat_history'],\n            'assistant_response': row['assistant_response']\n        })\n\n# Use ThreadPoolExecutor to parallelize the processing of rows\nfrom concurrent.futures import ThreadPoolExecutor # Assuming this import is present\n\nwith ThreadPoolExecutor() as executor:\n    # Assuming df is a pandas DataFrame containing the rows to process\n    executor.map(process_row, [row for index, row in df.iterrows()])\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables in Supabase Edge Functions with Deno\nDESCRIPTION: Demonstrates direct access to injected environment variables in Supabase Edge Functions using Deno.env.get. No additional library or .env file loading is necessary; variables are injected automatically. Use this approach when running on the Supabase platform.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst supabaseUrl = Deno.env.get(\"SUPABASE_URL\");\nconst supabaseServiceRoleKey = Deno.env.get(\"SUPABASE_SERVICE_ROLE_KEY\");\n```\n\n----------------------------------------\n\nTITLE: Actions Return Multiple Files in Markdown\nDESCRIPTION: Code snippet showing that Actions can return up to 10 files per request to be integrated into the conversation, as released on May 13th, 2024.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/release-notes.txt#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nActions can{\" \"}\n    return\n{\" \"}\nof up to 10 files per request to be integrated into the conversation\n```\n\n----------------------------------------\n\nTITLE: Evaluating LLM on Challenging Prompts (Python)\nDESCRIPTION: This Python code snippet calls the `eval` function again, this time using the 'gpt-3.5-turbo' model with the same system prompt and function list but evaluating against the `challenging_prompts_to_expected` dictionary. The goal is to assess how well the model handles difficult or impossible requests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate the model with the challenging prompts\neval(\n    model=\"gpt-3.5-turbo\",\n    function_list=function_list,\n    system_prompt=DRONE_SYSTEM_PROMPT,\n    prompts_to_expected_tool_name=challenging_prompts_to_expected,\n)\n```\n\n----------------------------------------\n\nTITLE: Answering Questions Using GPT-3.5-Turbo-Instruct Chat Completion in Python\nDESCRIPTION: This function generates a natural language answer to a user question using the most relevant context retrieved from a dataframe of embeddings. It calls the previously defined create_context function to obtain context text chunks, then invokes OpenAI's chat completion endpoint with a system instruction and user message containing the context and the question. The model returns an answer, or \"I don't know\" if the context does not contain a suitable response. Parameters include the question, embedding model size, maximum token responses, debugging option to print context, and optional stop sequences. The function handles exceptions gracefully and returns an empty string on failure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef answer_question(\n    df,\n    model=\"gpt-3.5-turbo\",\n    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n    max_len=1800,\n    size=\"ada\",\n    debug=False,\n    max_tokens=150,\n    stop_sequence=None\n):\n    \"\"\"\n    Answer a question based on the most similar context from the dataframe texts\n    \"\"\"\n    context = create_context(\n        question,\n        df,\n        max_len=max_len,\n        size=size,\n    )\n    # If debug, print the raw model response\n    if debug:\n        print(\"Context:\\n\" + context)\n        print(\"\\n\\n\")\n\n    try:\n        # Create a chat completion using the question and context\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n                {\"role\": \"user\", f\"content\": \"Context: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\"}\n            ],\n            temperature=0,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop_sequence,\n        )\n        return response.choices[0].message.strip()\n    except Exception as e:\n        print(e)\n        return \"\"\n```\n\n----------------------------------------\n\nTITLE: Extracting and Playing Dubbed Hindi Audio\nDESCRIPTION: Code to extract both the Hindi transcript and audio data from the GPT-4o response. It decodes the base64 audio data and plays it using the pydub library, enabling users to hear the translated content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Make sure pydub is installed \nfrom pydub import AudioSegment\nfrom pydub.playback import play\nfrom io import BytesIO\n\n# Get the transcript from the model. This will vary depending on the modality you are using. \nhindi_transcript = message['audio']['transcript']\n\nprint(hindi_transcript)\n\n# Get the audio content from the response \nhindi_audio_data_base64 = message['audio']['data']\n```\n\n----------------------------------------\n\nTITLE: Semantic Search and Question Answering with Pinecone and OpenAI GPT-4o in Python\nDESCRIPTION: A function that takes a user question, converts it to an embedding, queries Pinecone for relevant document pages, and uses GPT-4o to generate an answer based on the retrieved context. The function formats the retrieved metadata as JSON for the LLM prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\n\n# Function to get response to a user's question \ndef get_response_to_question(user_question, pc_index):\n    # Get embedding of the question to find the relevant page with the information \n    question_embedding = get_embedding(user_question)\n\n    # get response vector embeddings \n    response = pc_index.query(\n        vector=question_embedding,\n        top_k=2,\n        include_values=True,\n        include_metadata=True\n    )\n\n    # Collect the metadata from the matches\n    context_metadata = [match['metadata'] for match in response['matches']]\n\n    # Convert the list of metadata dictionaries to prompt a JSON string\n    context_json = json.dumps(context_metadata, indent=3)\n\n    prompt = f\"\"\"You are a helpful assistant. Use the following context and images to answer the question. In the answer, include the reference to the document, and page number you found the information on between <source></source> tags. If you don't find the information, you can say \"I couldn't find the information\"\n\n    question: {user_question}\n    \n    <SOURCES>\n    {context_json}\n    </SOURCES>\n    \"\"\"\n\n    # Call completions end point with the prompt \n    completion = oai_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": prompt}\n        ]\n    )\n\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and Data Analysis Environment in Python\nDESCRIPTION: This snippet sets up the Python environment for a multi-agent system by importing essential libraries like OpenAI for language model interaction, matplotlib for visualization, pandas for data handling, numpy for numerical computation, and IPython for notebook-based image display. It also creates an instance of the OpenAI client required for subsequent API interactions. Dependencies are openai, matplotlib, pandas, numpy, and IPython, all installed via pip. No inputs or outputs occur at this stage; it initializes dependencies only.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nfrom IPython.display import Image\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nimport numpy as np\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Defining prompts for converting help center policy articles into executable routines\nDESCRIPTION: This snippet sets the detailed instruction prompt for the OpenAI model to transform external-facing policy content into an internal, step-by-step routine suitable for automation. The prompt emphasizes structured actions, conditions, function calls, and compliance, guiding the model to produce programmatically actionable workflows.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_routine_generation.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nCONVERSION_PROMPT = \"\"\"\nYou are a helpful assistant tasked with taking an external facing help center article and converting it into a internal-facing programmatically executable routine optimized for an LLM. \nThe LLM using this routine will be tasked with reading the policy, answering incoming questions from customers, and helping drive the case toward resolution.\n\nPlease follow these instructions:\n1. **Review the customer service policy carefully** to ensure every step is accounted for. It is crucial not to skip any steps or policies.\n2. **Organize the instructions into a logical, step-by-step order**, using the specified format.\n3. **Use the following format**:\n   - **Main actions are numbered** (e.g., 1, 2, 3).\n   - **Sub-actions are lettered** under their relevant main actions (e.g., 1a, 1b).\n      **Sub-actions should start on new lines**\n   - **Specify conditions using clear 'if...then...else' statements** (e.g., 'If the product was purchased within 30 days, then...').\n   - **For instructions that require more information from the customer**, provide polite and professional prompts to ask for additional information.\n   - **For actions that require data from external systems**, write a step to call a function using backticks for the function name (e.g., `call the check_delivery_date function`).\n      - **If a step requires the customer service agent to take an action** (e.g., process a refund), generate a function call for this action (e.g., `call the process_refund function`).\n      - **Define any new functions** by providing a brief description of their purpose and required parameters.\n   - **If there is an action an assistant can performon behalf of the user**, include a function call for this action (e.g., `call the change_email_address function`), and ensure the function is defined with its purpose and required parameters.\n      This action may not be explicitly defined in the help center article, but can be done to help the user resolve their inquiry faster\n   - **The step prior to case resolution should always be to ask if there is anything more you can assist with**.\n   - **End with a final action for case resolution**: calling the `case_resolution` function should always be the final step.\n4. **Ensure compliance** by making sure all steps adhere to company policies, privacy regulations, and legal requirements.\n5. **Handle exceptions or escalations** by specifying steps for scenarios that fall outside the standard policy.\n\n**Important**: If at any point you are uncertain, respond with \"I don't know.\"\n\nPlease convert the customer service policy into the formatted routine, ensuring it is easy to follow and execute programmatically.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store with Tair and OpenAI Embeddings\nDESCRIPTION: Initializes OpenAI embeddings and stores all answers in a Tair vector database. This creates a searchable knowledge base for the QA system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.vectorstores import Tair\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain import VectorDBQA, OpenAI\n\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ndoc_store = Tair.from_texts(\n    texts=answers, embedding=embeddings, tair_url=TAIR_URL,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Multi-Iteration AI Agent Loop with OpenAI Chat Completions\nDESCRIPTION: This snippet defines a loop that runs up to five times, making chat completion requests to OpenAI's API, and handles 'tool_calls' responses by invoking registered functions. The loop continues until a 'stop' finish reason is received or the maximum iterations are reached. It processes function responses and appends results to messages for context, enabling a dynamic, multi-turn interaction with function calling capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\nfor (let i = 0; i < 5; i++) {\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4\",\n    messages: messages,\n    tools: tools,\n  });\n  const { finish_reason, message } = response.choices[0];\n\n  if (finish_reason === \"tool_calls\" && message.tool_calls) {\n    const functionName = message.tool_calls[0].function.name;\n    const functionToCall = availableTools[functionName];\n    const functionArgs = JSON.parse(message.tool_calls[0].function.arguments);\n    const functionArgsArr = Object.values(functionArgs);\n    const functionResponse = await functionToCall.apply(null, functionArgsArr);\n\n    messages.push({\n      role: \"function\",\n      name: functionName,\n      content: `\n          The result of the last function was this: ${JSON.stringify(\n            functionResponse\n          )}\n          `,\n    });\n  } else if (finish_reason === \"stop\") {\n    messages.push(message);\n    return message.content;\n  }\n}\nreturn \"The maximum number of iterations has been met without a suitable answer. Please try again with a more specific input.\";\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Input Moderation and Chat Response in Python\nDESCRIPTION: Defines an async function that concurrently checks user input for moderation flags and retrieves an LLM response. If moderation flags content as inappropriate, it cancels the chat response and returns a warning message. This pattern helps to minimize latency while ensuring content safety.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nasync def check_moderation_flag(expression):\n    moderation_response = client.moderations.create(input=expression)\n    flagged = moderation_response.results[0].flagged\n    return flagged\n\ndef get_chat_response(user_request):\n    print(\"Getting LLM response\")\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_request},\n    ]\n    response = client.chat.completions.create(\n        model=GPT_MODEL, messages=messages, temperature=0.5\n    )\n    print(\"Got LLM response\")\n    return response.choices[0].message.content\n\nasync def execute_chat_with_input_moderation(user_request):\n    moderation_task = asyncio.create_task(check_moderation_flag(user_request))\n    chat_task = asyncio.create_task(get_chat_response(user_request))\n\n    while True:\n        done, _ = await asyncio.wait(\n            [moderation_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n        )\n        if moderation_task not in done:\n            await asyncio.sleep(0.1)\n            continue\n        if moderation_task.result() == True:\n            chat_task.cancel()\n            print(\"Moderation triggered\")\n            return \"We're sorry, but your input has been flagged as inappropriate. Please rephrase your input and try again.\"\n        if chat_task in done:\n            return chat_task.result()\n        await asyncio.sleep(0.1)\n```\n\n----------------------------------------\n\nTITLE: Handling ambiguous file search requests\nDESCRIPTION: This snippet demonstrates how the model should handle ambiguous file search requests. It calls `run_conversation` with an incomplete search query ('search for a file'), and the model is expected to ask for clarification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(run_conversation('search for a file'))\n```\n\n----------------------------------------\n\nTITLE: Define Embedding Function using OpenAI\nDESCRIPTION: Defines a Python function `embed` that takes a list of text strings as input. It calls the OpenAI Embedding API using the configured `OPENAI_ENGINE` to generate vector embeddings for each text. The function returns a list containing only the embedding vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Simple function that converts the texts to embeddings\ndef embed(texts):\n    embeddings = openai.Embedding.create(\n        input=texts,\n        engine=OPENAI_ENGINE\n    )\n    return [x['embedding'] for x in embeddings['data']]\n\n\n```\n\n----------------------------------------\n\nTITLE: Connecting to Hologres using psycopg2 in Python\nDESCRIPTION: Establishes a connection to a Hologres instance using the `psycopg2` library. It retrieves connection parameters (host, port, database name, user, password) from environment variables (`PGHOST`, `PGPORT`, `PGDATABASE`, `PGUSER`, `PGPASSWORD`), providing default values if the variables are not set. The connection is configured for autocommit, and a cursor object is created to execute SQL commands.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport psycopg2\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"PGHOST\"] = \"your_host\"\n# os.environ[\"PGPORT\"] \"5432\"),\n# os.environ[\"PGDATABASE\"] \"postgres\"),\n# os.environ[\"PGUSER\"] \"user\"),\n# os.environ[\"PGPASSWORD\"] \"password\"),\n\nconnection = psycopg2.connect(\n    host=os.environ.get(\"PGHOST\", \"localhost\"),\n    port=os.environ.get(\"PGPORT\", \"5432\"),\n    database=os.environ.get(\"PGDATABASE\", \"postgres\"),\n    user=os.environ.get(\"PGUSER\", \"user\"),\n    password=os.environ.get(\"PGPASSWORD\", \"password\")\n)\nconnection.set_session(autocommit=True)\n\n# Create a new cursor object\ncursor = connection.cursor()\n```\n\n----------------------------------------\n\nTITLE: Sample Usage of AI Agent for Location and Weather-based Activity Suggestions\nDESCRIPTION: This snippet demonstrates invoking the customized agent function with a user prompt to suggest activities based on location and weather data. It logs the response to the console. The code shows how to integrate the agent into an application, simulating a real-world use case of providing activity recommendations based on dynamic environmental information.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst response = await agent(\n  \"Please suggest some activities based on my location and the weather.\"\n);\nconsole.log(response);\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Correct Financial Terms using OpenAI Chat (Python)\nDESCRIPTION: Defines `product_assistant`, a function utilizing OpenAI's `gpt-4` model to correct and format financial terms within a transcript. It uses a detailed system prompt specifying rules for expanding acronyms (e.g., 'HSA' to 'Health Savings Account (HSA)'), converting spoken numbers for products (e.g., 'five two nine' to '529 (Education Savings Plan)'), and handling context-dependent acronyms. The function takes the transcript as input and returns the full API response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define function to fix product mispellings\ndef product_assistant(ascii_transcript):\n    system_prompt = \"\"\"You are an intelligent assistant specializing in financial products;\n    your task is to process transcripts of earnings calls, ensuring that all references to\n     financial products and common financial terms are in the correct format. For each\n     financial product or common term that is typically abbreviated as an acronym, the full term \n    should be spelled out followed by the acronym in parentheses. For example, '401k' should be\n     transformed to '401(k) retirement savings plan', 'HSA' should be transformed to 'Health Savings Account (HSA)'\n    , 'ROA' should be transformed to 'Return on Assets (ROA)', 'VaR' should be transformed to 'Value at Risk (VaR)'\n, and 'PB' should be transformed to 'Price to Book (PB) ratio'. Similarly, transform spoken numbers representing \nfinancial products into their numeric representations, followed by the full name of the product in parentheses. \nFor instance, 'five two nine' to '529 (Education Savings Plan)' and 'four zero one k' to '401(k) (Retirement Savings Plan)'.\n However, be aware that some acronyms can have different meanings based on the context (e.g., 'LTV' can stand for \n'Loan to Value' or 'Lifetime Value'). You will need to discern from the context which term is being referred to \nand apply the appropriate transformation. In cases where numerical figures or metrics are spelled out but do not \nrepresent specific financial products (like 'twenty three percent'), these should be left as is. Your role is to\n analyze and adjust financial product terminology in the text. Once you've done that, produce the adjusted \n transcript and a list of the words you've changed\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": ascii_transcript\n            }\n        ]\n    )\n    return response\n```\n\n----------------------------------------\n\nTITLE: Creating Multilingual TTS with Uruguayan Spanish Accent Using Chat Completions\nDESCRIPTION: Advanced implementation that first translates text to Spanish with a Uruguayan flavor using GPT-4o, then generates audio with a matching accent. This demonstrates combining translation and accent-specific TTS capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/steering_tts.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert translator. Translate any text given into Spanish like you are from Uruguay.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": tts_text,\n        }\n    ],\n)\ntranslated_text = completion.choices[0].message.content\nprint(translated_text)\n\nspeech_file_path = \"./sounds/chat_completions_tts_es_uy.mp3\"\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    modalities=[\"text\", \"audio\"],\n    audio={\"voice\": \"alloy\", \"format\": \"mp3\"},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can generate audio from text. Speak any text that you receive in a Uruguayan spanish accent and more slowly.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": translated_text,\n        }\n    ],\n)\n\nmp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\nwith open(speech_file_path, \"wb\") as f:\n    f.write(mp3_bytes)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Elasticsearch Cloud in Python\nDESCRIPTION: Creates a connection to an Elastic Cloud deployment using user-supplied Cloud ID and password, then prints server information to verify connectivity. Requires the 'elasticsearch' Python package and access to an Elastic Cloud deployment. Inputs obtained interactively using getpass. Outputs Elasticsearch server info as a validation step.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nCLOUD_ID = getpass(\"Elastic deployment Cloud ID\")\nCLOUD_PASSWORD = getpass(\"Elastic deployment Password\")\nclient = Elasticsearch(\n  cloud_id = CLOUD_ID,\n  basic_auth=(\"elastic\", CLOUD_PASSWORD) # Alternatively use `api_key` instead of `basic_auth`\n)\n\n# Test connection to Elasticsearch\nprint(client.info())\n```\n\n----------------------------------------\n\nTITLE: Defining Langchain Tools for Querying and Similarity Search in Python\nDESCRIPTION: Imports Langchain modules and declares two Tool objects representing database query and similarity search functionalities. Each Tool includes a name, function reference, and a descriptive string to guide the Langchain agent's usage. These tools enable conversational agents to perform database queries or similarity searches dynamically. Dependencies include Langchain library and previously defined 'query_db' and 'similarity_search' functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\nfrom langchain.schema import AgentAction, AgentFinish, HumanMessage, SystemMessage\n\n\ntools = [\n    Tool(\n        name=\"Query\",\n        func=query_db,\n        description=\"Use this tool to find entities in the user prompt that can be used to generate queries\"\n    ),\n    Tool(\n        name=\"Similarity Search\",\n        func=similarity_search,\n        description=\"Use this tool to perform a similarity search with the products in the database\"\n    )\n]\n\ntool_names = [f\"{tool.name}: {tool.description}\" for tool in tools]\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Prompt Template Using LangChain in Python\nDESCRIPTION: Defines a custom prompt template class inheriting from BaseChatPromptTemplate, which formats messages for LLM agents using provided tools and input variables. Dependencies include LangChain modules, a list of Tool objects, and a template string with placeholders for LLM input variables. The template dynamically injects agent intermediate steps, tool descriptions, and other runtime parameters to generate the final prompt for the agent. The output is a human-readable prompt ready for LLM consumption; key inputs are the 'template', 'tools', and 'input_variables'. Limitations: requires properly formatted intermediate_steps and tool objects.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass CustomPromptTemplate(BaseChatPromptTemplate):\n    # The template to use\n    template: str\n    # The list of tools available\n    tools: List[Tool]\n    \n    def format_messages(self, **kwargs) -> str:\n        # Get the intermediate steps (AgentAction, Observation tuples)\n        \n        # Format them in a particular way\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n            \n        # Set the agent_scratchpad variable to that value\n        kwargs[\"agent_scratchpad\"] = thoughts\n        \n        # Create a tools variable from the list of tools provided\n        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n        \n        # Create a list of tool names for the tools provided\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n        formatted = self.template.format(**kwargs)\n        return [HumanMessage(content=formatted)]\n    \nprompt = CustomPromptTemplate(\n    template=template,\n    tools=tools,\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n    # This includes the `intermediate_steps` variable because that is needed\n    input_variables=[\"input\", \"intermediate_steps\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Estimated vs. API Token Counts (Python)\nDESCRIPTION: This snippet demonstrates how to use the `num_tokens_from_messages` function (defined in the previous snippet) to estimate token counts for a set of example messages and compares the result against the actual `prompt_tokens` reported by the OpenAI API for various chat models. It highlights potential discrepancies between the estimation and the API's precise count. It assumes an initialized `client` object and the `num_tokens_from_messages` function are available.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# let's verify the function above matches the OpenAI API response\nexample_messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"New synergies will help drive top-line growth.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Things working well together will increase revenue.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n    },\n]\n\nfor model in [\n    # \"gpt-3.5-turbo-0301\",\n    # \"gpt-4-0314\",\n    # \"gpt-4-0613\",\n    \"gpt-3.5-turbo-1106\",\n    \"gpt-3.5-turbo\",\n    \"gpt-4\",\n    \"gpt-4-1106-preview\",\n    ]:\n    print(model)\n    # example token count from the function defined above\n    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n    # example token count from the OpenAI API\n    response = client.chat.completions.create(model=model,\n    messages=example_messages,\n    temperature=0,\n    max_tokens=1)\n    token = response.usage.prompt_tokens\n    print(f'{token} prompt tokens counted by the OpenAI API.')\n    print()\n```\n\n----------------------------------------\n\nTITLE: Generating Images with OpenAI\nDESCRIPTION: This Python code demonstrates generating images using the OpenAI API. It creates two images of a cute baby sea otter with a size of 1024x1024 pixels. It requires the `openai` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.generate(\n  prompt=\"A cute baby sea otter\",\n  n=2,\n  size=\"1024x1024\"\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Querying AnalyticDB for Nearest Vectors by Title or Content - Python\nDESCRIPTION: Defines a function to query the AnalyticDB table by converting a user text query into an OpenAI embedding, then performing ANN search using the pre-built vector indexes. Accepts optional parameters to switch between title and content vector search and to set the number of returned results. Requires OpenAI API access and loaded embeddings, as well as a psycopg2 cursor. Inputs: query, table name, vector field, and top_k. Outputs: list of matching article tuples sorted by vector similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef query_analyticdb(query, collection_name, vector_name=\"title_vector\", top_k=20):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=\"text-embedding-3-small\",\n    )[\"data\"][0][\"embedding\"]\n\n    # Convert the embedded_query to PostgreSQL compatible format\n    embedded_query_pg = \"{\" + \",\".join(map(str, embedded_query)) + \"}\"\n\n    # Create SQL query\n    query_sql = f\"\"\"\n    SELECT id, url, title, l2_distance({vector_name},'{embedded_query_pg}'::real[]) AS similarity\n    FROM {collection_name}\n    ORDER BY {vector_name} <-> '{embedded_query_pg}'::real[]\n    LIMIT {top_k};\n    \"\"\"\n    # Execute the query\n    cursor.execute(query_sql)\n    results = cursor.fetchall()\n\n    return results\n\n```\n\n----------------------------------------\n\nTITLE: Generating Spoken Audio Using OpenAI TTS API in Python\nDESCRIPTION: This snippet shows how to convert input text into spoken audio using the OpenAI TTS API with the Python SDK. It creates an OpenAI client, specifies the TTS model, voice, and input text, submits a request, and streams the API's response into an MP3 file. Prerequisites include the openai Python package and authentication via environment variable/api key. Key parameters are 'model' (the TTS model name), 'voice' (available built-in voices), and 'input' (the text to convert). The expected output is a playable MP3 audio file generated from the provided text. Limitations include supported voices and languages as documented by OpenAI.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-to-speech.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom openai import OpenAI\nclient = OpenAI()\n\nspeech_file_path = Path(__file__).parent / \"speech.mp3\"\nresponse = client.audio.speech.create(\n  model=\"tts-1\",\n  voice=\"alloy\",\n  input=\"Today is a wonderful day to build something people love!\"\n)\n\nresponse.stream_to_file(speech_file_path)\n```\n\n----------------------------------------\n\nTITLE: Upserting Vector Embeddings to Pinecone Database in Python\nDESCRIPTION: A function that handles the process of upserting document vectors with their metadata to a Pinecone vector index. It includes error handling to catch and report any issues during the upsert operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef upsert_vector(identifier, embedding, metadata):\n    try:\n        index.upsert([\n            {\n                'id': identifier,\n                'values': embedding,\n                'metadata': metadata\n            }\n        ])\n    except Exception as e:\n        print(f\"Error upserting vector with ID {identifier}: {e}\")\n        raise\n```\n\n----------------------------------------\n\nTITLE: Creating Thread and Sending Initial Message (OpenAI Assistants API)\nDESCRIPTION: Initializes a new conversation thread with the Assistant. It immediately adds the first user message to the thread, requesting a specific data analysis task: calculate quarterly profit and visualize it by distribution channel as a line plot, referencing the uploaded data file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Calculate profit (revenue minus cost) by quarter and year, and visualize as a line plot across the distribution channels, where the colors of the lines are green, light red, and light blue\",\n      \"file_ids\": [file.id]\n    }\n  ]\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using Chunked Embeddings for Long Texts in Python\nDESCRIPTION: Demonstrates how to generate either an averaged embedding vector or multiple chunk embeddings from a long text, supporting applications that need to handle texts exceeding token limits with flexible aggregation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\naverage_embedding_vector = len_safe_get_embedding(long_text, average=True)\nchunks_embedding_vectors = len_safe_get_embedding(long_text, average=False)\n\nprint(f\"Setting average=True gives us a single {len(average_embedding_vector)}-dimensional embedding vector for our long text.\")\nprint(f\"Setting average=False gives us {len(chunks_embedding_vectors)} embedding vectors, one for each of the chunks.\")\n```\n\n----------------------------------------\n\nTITLE: Processing Files for Embedding in Python\nDESCRIPTION: This function processes files (both .txt and .pdf) by extracting text content, generating title and content embeddings, and categorizing the content. It uses helper functions for extracting text from PDFs, generating embeddings, and categorizing text. The extracted data is then formatted and returned as a list of dictionaries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    # Initialize the PDF reader\n    reader = PdfReader(pdf_path)\n    text = \"\"\n    # Iterate through each page in the PDF and extract text\n    for page in reader.pages:\n        text += page.extract_text()\n    return text\n\ndef process_file(file_path, idx, categories, embeddings_model):\n    file_name = os.path.basename(file_path)\n    print(f\"Processing file {idx + 1}: {file_name}\")\n    \n    # Read text content from .txt files\n    if file_name.endswith('.txt'):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n    # Extract text content from .pdf files\n    elif file_name.endswith('.pdf'):\n        text = extract_text_from_pdf(file_path)\n    \n    title = file_name\n    # Generate embeddings for the title\n    title_vectors, title_text = len_safe_get_embedding(title, embeddings_model)\n    print(f\"Generated title embeddings for {file_name}\")\n    \n    # Generate embeddings for the content\n    content_vectors, content_text = len_safe_get_embedding(text, embeddings_model)\n    print(f\"Generated content embeddings for {file_name}\")\n    \n    category = categorize_text(' '.join(content_text), categories)\n    print(f\"Categorized {file_name} as {category}\")\n    \n    # Prepare the data to be appended\n    data = []\n    for i, content_vector in enumerate(content_vectors):\n        data.append({\n            \"id\": f\"{idx}_{i}\",\n            \"vector_id\": f\"{idx}_{i}\",\n            \"title\": title_text[0],\n            \"text\": content_text[i],\n            \"title_vector\": json.dumps(title_vectors[0]),  # Assuming title is short and has only one chunk\n            \"content_vector\": json.dumps(content_vector),\n            \"category\": category\n        })\n        print(f\"Appended data for chunk {i + 1}/{len(content_vectors)} of {file_name}\")\n    \n    return data\n```\n\n----------------------------------------\n\nTITLE: Comparing Articles with XML Delimiters (Prompt)\nDESCRIPTION: Illustrates using XML-like tags (as described in the system prompt) to separate two articles provided to the model. The task involves summarizing each article and then comparing their arguments. Requires two articles to be inserted by the user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_1\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: You will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\n\nUSER:  insert first article here \n\n insert second article here \n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Search with OpenAI Embeddings and Redis\nDESCRIPTION: A function that performs vector search using OpenAI embeddings and Redis. It converts a user query into an embedding vector, constructs a Redis search query, and returns matching products sorted by similarity score.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef search_redis(\n    redis_client: redis.Redis,\n    user_query: str,\n    index_name: str = \"product_embeddings\",\n    vector_field: str = \"product_vector\",\n    return_fields: list = [\"productDisplayName\", \"masterCategory\", \"gender\", \"season\", \"year\", \"vector_score\"],\n    hybrid_fields = \"*\",\n    k: int = 20,\n    print_results: bool = True,\n) -> List[dict]:\n\n    # Use OpenAI to create embedding vector from user query\n    embedded_query = openai.Embedding.create(input=user_query,\n                                            model=\"text-embedding-3-small\",\n                                            )[\"data\"][0]['embedding']\n\n    # Prepare the Query\n    base_query = f'{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]'\n    query = (\n        Query(base_query)\n         .return_fields(*return_fields)\n         .sort_by(\"vector_score\")\n         .paging(0, k)\n         .dialect(2)\n    )\n    params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}\n\n    # perform vector search\n    results = redis_client.ft(index_name).search(query, params_dict)\n    if print_results:\n        for i, product in enumerate(results.docs):\n            score = 1 - float(product.vector_score)\n            print(f\"{i}. {product.productDisplayName} (Score: {round(score ,3) })\")\n    return results.docs\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Source for Evals with Pydantic Schema - Python\nDESCRIPTION: Sets up a data source configuration dictionary for OpenAI Evals using the JSON schema generated from the PushNotifications model. Includes a key to specify that API completions are uploaded as samples. Inputs: Pydantic model's JSON schema. Outputs: configuration dict for use with eval creation. This config defines available variables for eval runs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# We want our input data to be available in our variables, so we set the item_schema to\n# PushNotifications.model_json_schema()\ndata_source_config = {\n    \"type\": \"custom\",\n    \"item_schema\": PushNotifications.model_json_schema(),\n    # We're going to be uploading completions from the API, so we tell the Eval to expect this\n    \"include_sample_schema\": True,\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: This snippet shows how to initialize an OpenAI client in Python, which is required for interacting with the OpenAI API. It uses the `openai` library and retrieves the API key from an environment variable. If the environment variable is not set, it defaults to a placeholder, which is important to customize when using the code. This is a fundamental step to make API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(\n    api_key=os.environ.get(\n        \"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"\n    )\n)\n\n```\n\n----------------------------------------\n\nTITLE: Semantic Search Function with Cosine Similarity - Python\nDESCRIPTION: This function performs a semantic search through the reviews based on the provided `product_description`. It calculates the cosine similarity between the embedding of the `product_description` and the embeddings of each review, then returns the top `n` most similar reviews. It relies on `utils.embeddings_utils.get_embedding` to generate the embedding for the search query and `utils.embeddings_utils.cosine_similarity` to calculate cosine similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import get_embedding, cosine_similarity\n\n# search through the reviews for a specific product\ndef search_reviews(df, product_description, n=3, pprint=True):\n    product_embedding = get_embedding(\n        product_description,\n        model=\"text-embedding-3-small\"\n    )\n    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"; Content:\", \": \")\n    )\n    if pprint:\n        for r in results:\n            print(r[:200])\n            print()\n    return results\n```\n\n----------------------------------------\n\nTITLE: Executing the Agent Executor for a Follow-Up Query in LangChain (Python)\nDESCRIPTION: Invokes the agent_executor with an additional query 'How many in 2022?', which may depend on previous dialogue context if memory is implemented. Expects the agent_executor object with required agent and tool context. Designed to provide an answer using model reasoning or tool interaction as configured.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nagent_executor.run(\"How many in 2022?\")\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Perplexity from logprobs in Python\nDESCRIPTION: This snippet demonstrates how to calculate perplexity from token log probabilities to measure model uncertainty. It compares perplexity scores between different prompts to evaluate the model's relative confidence in its responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprompts = [\n    \"In a short sentence, has artifical intelligence grown in the last decade?\",\n    \"In a short sentence, what are your thoughts on the future of artificial intelligence?\",\n]\n\nfor prompt in prompts:\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": prompt}],\n        model=\"gpt-4o-mini\",\n        logprobs=True,\n    )\n\n    logprobs = [token.logprob for token in API_RESPONSE.choices[0].logprobs.content]\n    response_text = API_RESPONSE.choices[0].message.content\n    response_text_tokens = [token.token for token in API_RESPONSE.choices[0].logprobs.content]\n    max_starter_length = max(len(s) for s in [\"Prompt:\", \"Response:\", \"Tokens:\", \"Logprobs:\", \"Perplexity:\"])\n    max_token_length = max(len(s) for s in response_text_tokens)\n    \n\n    formatted_response_tokens = [s.rjust(max_token_length) for s in response_text_tokens]\n    formatted_lps = [f\"{lp:.2f}\".rjust(max_token_length) for lp in logprobs]\n\n    perplexity_score = np.exp(-np.mean(logprobs))\n    print(\"Prompt:\".ljust(max_starter_length), prompt)\n    print(\"Response:\".ljust(max_starter_length), response_text, \"\\n\")\n    print(\"Tokens:\".ljust(max_starter_length), \" \".join(formatted_response_tokens))\n    print(\"Logprobs:\".ljust(max_starter_length), \" \".join(formatted_lps))\n    print(\"Perplexity:\".ljust(max_starter_length), perplexity_score, \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Coordinating Meeting Minutes Extraction - Python\nDESCRIPTION: This snippet presents the 'meeting_minutes' function, which orchestrates the analysis workflow by invoking four separate extraction functions: abstract summary, key points, action items, and sentiment analysis. Each function operates on the transcript text. The results are collected and returned in a dictionary. This modular structure improves maintainability but may increase the number of API calls to GPT-4.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef meeting_minutes(transcription):\n    abstract_summary = abstract_summary_extraction(transcription)\n    key_points = key_points_extraction(transcription)\n    action_items = action_item_extraction(transcription)\n    sentiment = sentiment_analysis(transcription)\n    return {\n        'abstract_summary': abstract_summary,\n        'key_points': key_points,\n        'action_items': action_items,\n        'sentiment': sentiment\n    }\n```\n\n----------------------------------------\n\nTITLE: Normalizing and Truncating Embeddings to Different Dimensions\nDESCRIPTION: Demonstrates how to manually change embedding dimensions after generation and properly normalize the resulting vector. This technique allows for trading off performance and cost by using shorter embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport numpy as np\n\nclient = OpenAI()\n\ndef normalize_l2(x):\n    x = np.array(x)\n    if x.ndim == 1:\n        norm = np.linalg.norm(x)\n        if norm == 0:\n            return x\n        return x / norm\n    else:\n        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n        return np.where(norm == 0, x, x / norm)\n\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\", input=\"Testing 123\", encoding_format=\"float\"\n)\n\ncut_dim = response.data[0].embedding[:256]\nnorm_dim = normalize_l2(cut_dim)\n\nprint(norm_dim)\n```\n\n----------------------------------------\n\nTITLE: Delaying OpenAI API Requests to Prevent Rate Limit Exceedance in Python\nDESCRIPTION: Illustrates a function that introduces a time delay before invoking the OpenAI API's chat completion endpoint, used to throttle requests based on a known rate limit. The delay_in_seconds parameter allows dynamic adjustment to match allowed requests per minute (e.g., 3 seconds for 20 RPM). Requires OpenAI Python library and Python's time module; expects model and messages supplied as kwargs; returns the API response after the enforced pause.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\n# Define a function that adds a delay to a Completion API call\ndef delayed_completion(delay_in_seconds: float = 1, **kwargs):\n    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n\n    # Sleep for the delay\n    time.sleep(delay_in_seconds)\n\n    # Call the Completion API and return the result\n    return client.chat.completions.create(**kwargs)\n\n\n# Calculate the delay based on your rate limit\nrate_limit_per_minute = 20\ndelay = 60.0 / rate_limit_per_minute\n\ndelayed_completion(\n    delay_in_seconds=delay,\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}]\n)\n\n```\n\n----------------------------------------\n\nTITLE: Batch Executing Entity Extraction for Example Inputs in Python\nDESCRIPTION: This loop iterates through each example input, applies the get_response function, and saves the output as a 'result' field in the respective dictionary. It enables batch processing for multiple conversational queries using the previously defined search pipeline. No extra dependencies are required. Input format must match the predefined example_inputs structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfor ex in example_inputs:\n    ex['result'] = get_response(ex['user_input'], ex['context'])\n\n```\n\n----------------------------------------\n\nTITLE: Classifying LLM Responses Using GPT-4o and Function Calling in Python\nDESCRIPTION: Defines an asynchronous Python function `classifier` that uses the OpenAI API (`client.chat.completions.create` with \"gpt-4o\") to classify an LLM's `output` against an `expected` answer for a given `input` question. It employs function calling (`rate` tool) to force the LLM to provide a structured choice (A-E) and reasoning based on a provided `PROMPT`. The function returns the numerical score corresponding to the LLM's choice using the `CHOICE_SCORES` dictionary. Example calls demonstrate its usage on potentially correct and hallucinated answers. Requires `braintrust` for tracing, an initialized `openai` client, `json` library, and predefined `PROMPT`, `CHOICE_SCORES`, `qa_pairs`, and `hallucinations` data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@braintrust.traced\nasync def classifier(input, output, expected):\n    response = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n            }\n        ],\n        temperature=0,\n        tools=[\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"rate\",\n                    \"description\": \"Call this function to select a choice.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"reasons\": {\n                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n                                \"type\": \"string\",\n                            },\n                            \"choice\": {\n                                \"description\": \"The choice\",\n                                \"type\": \"string\",\n                                \"enum\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n                            },\n                        },\n                        \"required\": [\"reasons\", \"choice\"],\n                        \"type\": \"object\",\n                    },\n                },\n            }\n        ],\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n    )\n    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n    choice = arguments[\"choice\"]\n    return CHOICE_SCORES[choice] if choice in CHOICE_SCORES else None\n\n\nprint(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\nprint(\n    await classifier(\n        qa_pairs[10].question,\n        qa_pairs[10].generated_answer,\n        qa_pairs[10].expected_answer,\n    )\n)\n\nprint(\n    hallucinations[10].question,\n    \"On a hallucinated answer:\",\n    hallucinations[10].generated_answer,\n)\nprint(\n    await classifier(\n        hallucinations[10].question,\n        hallucinations[10].generated_answer,\n        hallucinations[10].expected_answer,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Push Notification Summarization Function\nDESCRIPTION: Defines a function that uses the OpenAI API to summarize multiple push notifications into a single message. Uses GPT-4o-mini with a developer prompt and demonstrates the function with an example.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDEVELOPER_PROMPT = \"\"\"\nYou are a helpful assistant that summarizes push notifications.\nYou are given a list of push notifications and you need to collapse them into a single one.\nOutput only the final summary, nothing else.\n\"\"\"\n\ndef summarize_push_notification(push_notifications: str) -> ChatCompletion:\n    result = openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"developer\", \"content\": DEVELOPER_PROMPT},\n            {\"role\": \"user\", \"content\": push_notifications},\n        ],\n    )\n    return result\n\nexample_push_notifications_list = PushNotifications(notifications=\"\"\"\n- Alert: Unauthorized login attempt detected.\n- New comment on your blog post: \"Great insights!\"\n- Tonight's dinner recipe: Pasta Primavera.\n\"\"\")\nresult = summarize_push_notification(example_push_notifications_list.notifications)\nprint(result.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Defining Grouped Generative Search Function Using Weaviate and OpenAI in Python\nDESCRIPTION: Creates a Python function, generative_search_group, that performs a generative search over multiple items by providing a grouped prompt to OpenAI via the Weaviate client. The returned result contains a single response representing a summary or analysis of all selected objects. The function checks for API errors (such as rate limits) and returns the list of objects with generated grouped results. Dependencies include a live Weaviate client, API authentication, and validity of the collection name.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef generative_search_group(query, collection_name):\n    generateTask = \"Explain what these have in common\"\n\n    result = (\n        client.query\n        .get(collection_name, [\"title\", \"content\", \"url\"])\n        .with_near_text({ \"concepts\": [query], \"distance\": 0.7 })\n        .with_generate(grouped_task=generateTask)\n        .with_limit(5)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute â€“ the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n```\n\n----------------------------------------\n\nTITLE: Setting the OpenAI API Key\nDESCRIPTION: This snippet shows how to set the OpenAI API key in a `.env` file. This key is required to authenticate with the OpenAI API. Replace `<your_api_key>` with your actual OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nREACT_APP_OPENAI_API_KEY=<your_api_key>\n```\n\n----------------------------------------\n\nTITLE: Calling Google Places API\nDESCRIPTION: This function `call_google_places_api` retrieves a list of places from the Google Places API based on a user's profile and preferences. It takes the `user_id`, `place_type`, and an optional `food_preference` as input.  It first fetches the customer profile using `fetch_customer_profile`. Then, it constructs the API request URL, including the location (latitude and longitude) from the user profile and the specified `place_type` and `food_preference` (if provided). It makes a GET request to the API and processes the JSON response. The top two results are returned, with place name, type, rating, total ratings and street address formatted into a string. Error handling is included to manage API request failures and exceptions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef call_google_places_api(user_id, place_type, food_preference=None):\n    try:\n        # Fetch customer profile\n        customer_profile = fetch_customer_profile(user_id)\n        if customer_profile is None:\n            return \"I couldn't find your profile. Could you please verify your user ID?\"\n\n        # Get location from customer profile\n        lat = customer_profile[\"location\"][\"latitude\"]\n        lng = customer_profile[\"location\"][\"longitude\"]\n\n        API_KEY = os.getenv('GOOGLE_PLACES_API_KEY')  # retrieve API key from environment variable\n        LOCATION = f\"{lat},{lng}\"\n        RADIUS = 500  # search within a radius of 500 meters\n        TYPE = place_type\n\n        # If the place_type is restaurant and food_preference is not None, include it in the API request\n        if place_type == 'restaurant' and food_preference:\n            URL = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={LOCATION}&radius={RADIUS}&type={TYPE}&keyword={food_preference}&key={API_KEY}\"\n        else:\n            URL = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={LOCATION}&radius={RADIUS}&type={TYPE}&key={API_KEY}\"\n\n        response = requests.get(URL)\n        if response.status_code == 200:\n            results = json.loads(response.content)[\"results\"]\n            places = []\n            for place in results[:2]:  # limit to top 2 results\n                place_id = place.get(\"place_id\")\n                place_details = get_place_details(place_id, API_KEY)  # Get the details of the place\n\n                place_name = place_details.get(\"name\", \"N/A\")\n                place_types = next((t for t in place_details.get(\"types\", []) if t not in [\"food\", \"point_of_interest\"]), \"N/A\")  # Get the first type of the place, excluding \"food\" and \"point_of_interest\"\n                place_rating = place_details.get(\"rating\", \"N/A\")  # Get the rating of the place\n                total_ratings = place_details.get(\"user_ratings_total\", \"N/A\")  # Get the total number of ratings\n                place_address = place_details.get(\"vicinity\", \"N/A\")  # Get the vicinity of the place\n\n                if ',' in place_address:  # If the address contains a comma\n                    street_address = place_address.split(',')[0]  # Split by comma and keep only the first part\n                else:\n                    street_address = place_address\n\n                # Prepare the output string for this place\n                place_info = f\"{place_name} is a {place_types} located at {street_address}. It has a rating of {place_rating} based on {total_ratings} user reviews.\"\n\n                places.append(place_info)\n\n            return places\n        else:\n            print(f\"Google Places API request failed with status code {response.status_code}\")\n            print(f\"Response content: {response.content}\")  # print out the response content for debugging\n            return []\n    except Exception as e:\n        print(f\"Error during the Google Places API call: {e}\")\n        return []\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for OpenAI JSON Completions and Embeddings - Python\nDESCRIPTION: This code defines helper functions for interacting with the OpenAI API. The json_gpt function requests a JSON-formatted completion from a GPT model for a given prompt, while embeddings computes text embeddings for a list of strings using OpenAI's embedding endpoint. These helpers are prerequisites for later steps that require structured prompts and vector similarity calculations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Helper functions\ndef json_gpt(input: str):\n    completion = client.chat.completions.create(model=GPT_MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"Output only valid JSON\"},\n        {\"role\": \"user\", \"content\": input},\n    ],\n    temperature=0.5)\n\n    text = completion.choices[0].message.content\n    parsed = json.loads(text)\n\n    return parsed\n\n\ndef embeddings(input: list[str]) -> list[list[str]]:\n    response = client.embeddings.create(model=\"text-embedding-3-small\", input=input)\n    return [data.embedding for data in response.data]\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Clothing Matchmaker App in Python\nDESCRIPTION: Installs required Python packages for the notebook, including OpenAI's SDK, tenacity for retry logic, tqdm for progress bars, numpy for array operations, typing for type hints, tiktoken for tokenization, and concurrent for parallel processing. These are prerequisites for all later code in the project. Each package is installed via a Jupyter magic command and will install quietly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install openai --quiet\n%pip install tenacity --quiet\n%pip install tqdm --quiet\n%pip install numpy --quiet\n%pip install typing --quiet\n%pip install tiktoken --quiet\n%pip install concurrent --quiet\n```\n\n----------------------------------------\n\nTITLE: Implementing Box File Retrieval Azure Function in Python\nDESCRIPTION: This Python code defines an Azure Function (`box_retrieval`) triggered via HTTP. It receives Box file IDs and a user's JWT authorization token, extracts the user principal name (UPN), authenticates to the Box API using JWT credentials from `jwt_config.json`, impersonates the Box user corresponding to the UPN, retrieves download URLs for the requested files, and returns them in a JSON response. Requires `azure-functions`, `boxsdk[jwt]`, `requests`, and `pyjwt` libraries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport azure.functions as func\nfrom boxsdk import Client, JWTAuth\n\nimport requests\nimport base64\nimport json\nimport jwt\nimport logging\n\napp = func.FunctionApp(http_auth_level=func.AuthLevel.FUNCTION)\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n@app.route(route=\"box_retrieval\")\ndef box_retrieval(req: func.HttpRequest) -> func.HttpResponse:\n    logger.info('Starting box_retrieval function')\n    file_ids = req.params.get('file_ids')\n    auth_header = req.headers.get('Authorization')\n\n    if not file_ids or not auth_header:\n        logger.error('Missing file_ids or Authorization header')\n        return func.HttpResponse(\n            \"Missing file_id or Authorization header.\",\n            status_code=400\n        )\n    \n    file_ids = file_ids.split(\",\")  # Assuming file_ids are passed as a comma-separated string\n    if len(file_ids) == 0 or len(file_ids) > 10:\n        logger.error('file_ids list is empty or contains more than 10 IDs')\n        return func.HttpResponse(\n            \"file_ids list is empty or contains more than 10 IDs.\",\n            status_code=400\n        )\n\n    try:\n        # Decode JWT to extract the email\n        token = auth_header.split(\" \")[1]\n        decoded_token = jwt.decode(token, options={\"verify_signature\": False})\n        upn = decoded_token['upn']\n        user_email = get_user_mapping(upn)\n        logger.info(f'User email extracted: {user_email}')\n\n        config = JWTAuth.from_settings_file('jwt_config.json')\n        sdk = Client(config)\n        logger.info('Authenticated with Box API')\n\n        # Use the user email to get the user ID\n        users = sdk.users(filter_term=user_email)\n        user = next(users)\n        user_id = user.id\n        logger.info(f'User ID obtained: {user_id}')\n\n        openai_file_responses = []\n        for file_id in file_ids:\n            # Perform as_user call to get the file representation\n            my_file = sdk.as_user(user).file(file_id).get()\n            file_url = my_file.get_download_url()\n            openai_file_responses.append(file_url)\n        \n        response_body = json.dumps({'openaiFileResponse': openai_file_responses})\n\n        return func.HttpResponse(\n            response_body,\n            status_code=200,\n            mimetype=\"application/json\"\n        )\n\n    except Exception as e:\n        return func.HttpResponse(\n            f\"An error occurred: {str(e)}\",\n            status_code=500\n        )\n    \ndef get_user_mapping(upn):\n    # In our case, the user's authentication email into Azure AD is the same as their email in Box\n    # If that is not the case, map the email in Box to the email in Azure AD\n    return upn\n```\n\n----------------------------------------\n\nTITLE: Authenticating GCP Application Default Credentials (Shell)\nDESCRIPTION: Executes the `gcloud auth application-default login` shell command. This command initiates the authentication flow for obtaining Application Default Credentials (ADC), which allows applications and scripts to authenticate to Google Cloud APIs without needing explicit service account keys in many environments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n! gcloud auth application-default login\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenAI API Key Environment Variable in Python\nDESCRIPTION: Imports the `os` module in Python to check if the `OPENAI_API_KEY` environment variable has been successfully set. It uses `os.getenv()` to retrieve the key and prints a confirmation message indicating whether the key is ready or not found. It also shows a commented-out alternative to set the variable temporarily within the script using `os.environ`. Note that if using `export` in a notebook cell, the kernel might need restarting for the variable to be accessible.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ['OPENAI_API_KEY'] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search in Redis\nDESCRIPTION: Embeds a prompt using OpenAI and performs a vector similarity search in Redis to find the most relevant document. It converts the embedding to a byte string and uses the KNN search feature of Redis to find the nearest neighbor. Requires Redis Search and a pre-existing index with vector embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom redis.commands.search.query import Query\nimport numpy as np\n\nresponse = oai_client.embeddings.create(\n    input=[prompt],\n    model=model\n)\n# Extract the embedding vector from the response\nembedding_vector = response.data[0].embedding\n\n# Convert the embedding to a numpy array of type float32 and then to bytes\nvec = np.array(embedding_vector, dtype=np.float32).tobytes()\n\n# Build and execute the Redis query\nq = Query('*=>[KNN 1 @vector $query_vec AS vector_score]') \\\n    .sort_by('vector_score') \\\n    .return_fields('content') \\\n    .dialect(2)\nparams = {\"query_vec\": vec}\n\ncontext = client.ft('idx').search(q, query_params=params).docs[0].content\nprint(context)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI Client\nDESCRIPTION: Initializing the OpenAI client with API key from environment variables. This setup is required for making API calls to OpenAI's services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# imports\nimport time  # for measuring time duration of API calls\nfrom openai import OpenAI\nimport os\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Verifying OPENAI_API_KEY Environment Variable in Python\nDESCRIPTION: Checks whether the OPENAI_API_KEY environment variable is present in the local environment and prints a corresponding status message. Uses the os module to access environment variables. If not present, an alternative method is suggested (uncommenting and setting the key via os.environ). Expected output is a confirmation string indicating readiness, or a warning if the variable is missing. No external dependencies are required beyond Python's standard os module.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Conversation Handling with Tool Execution in Python\nDESCRIPTION: Defines a run_full_turn function that processes a conversation with an agent, handling message exchanges and tool calls, along with a execute_tool_call helper function to invoke the appropriate tool functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef run_full_turn(agent, messages):\n\n    num_init_messages = len(messages)\n    messages = messages.copy()\n\n    while True:\n\n        # turn python functions into tools and save a reverse map\n        tool_schemas = [function_to_schema(tool) for tool in agent.tools]\n        tools_map = {tool.__name__: tool for tool in agent.tools}\n\n        # === 1. get openai completion ===\n        response = client.chat.completions.create(\n            model=agent.model,\n            messages=[{\"role\": \"system\", \"content\": agent.instructions}] + messages,\n            tools=tool_schemas or None,\n        )\n        message = response.choices[0].message\n        messages.append(message)\n\n        if message.content:  # print assistant response\n            print(\"Assistant:\", message.content)\n\n        if not message.tool_calls:  # if finished handling tool calls, break\n            break\n\n        # === 2. handle tool calls ===\n\n        for tool_call in message.tool_calls:\n            result = execute_tool_call(tool_call, tools_map)\n\n            result_message = {\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call.id,\n                \"content\": result,\n            }\n            messages.append(result_message)\n\n    # ==== 3. return new messages =====\n    return messages[num_init_messages:]\n\n\ndef execute_tool_call(tool_call, tools_map):\n    name = tool_call.function.name\n    args = json.loads(tool_call.function.arguments)\n\n    print(f\"Assistant: {name}({args})\")\n\n    # call corresponding function with provided arguments\n    return tools_map[name](**args)\n```\n\n----------------------------------------\n\nTITLE: Displaying a Quiz (Python)\nDESCRIPTION: This function displays a quiz to the user, with questions of type MULTIPLE_CHOICE or FREE_RESPONSE. It uses mock responses for simplicity. It takes the quiz title and an array of questions as input.  It prints the quiz title and each question along with available options (for multiple-choice questions), collecting mock responses and returning them as a list.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef display_quiz(title, questions):\n    print(\"Quiz:\", title)\n    print()\n    responses = []\n\n    for q in questions:\n        print(q[\"question_text\"])\n        response = \"\"\n\n        # If multiple choice, print options\n        if q[\"question_type\"] == \"MULTIPLE_CHOICE\":\n            for i, choice in enumerate(q[\"choices\"]):\n                print(f\"{i}. {choice}\")\n            response = get_mock_response_from_user_multiple_choice()\n\n        # Otherwise, just get response\n        elif q[\"question_type\"] == \"FREE_RESPONSE\":\n            response = get_mock_response_from_user_free_response()\n\n        responses.append(response)\n        print()\n\n    return responses\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Search with Azure AI Search in Python\nDESCRIPTION: Combines traditional keyword-based search with vector-based similarity search to provide more contextually relevant results. This approach uses both the query text and its vector embedding to search for documents, limiting results to the top 3 matches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Hybrid Search\nquery = \"Famous battles in Scottish history\"  \n  \nsearch_client = SearchClient(search_service_endpoint, index_name, credential)  \nvector_query = VectorizedQuery(vector=generate_embeddings(query, deployment), k_nearest_neighbors=3, fields=\"content_vector\")\n  \nresults = search_client.search(  \n    search_text=query,  \n    vector_queries= [vector_query], \n    select=[\"title\", \"text\", \"url\"],\n    top=3\n)\n  \nfor result in results:  \n    print(f\"Title: {result['title']}\")  \n    print(f\"Score: {result['@search.score']}\")  \n    print(f\"URL: {result['url']}\\n\")  \n```\n\n----------------------------------------\n\nTITLE: Summarizing Text with Delimiters (Prompt)\nDESCRIPTION: Demonstrates using triple quotes as a delimiter to clearly indicate the specific text that the model should summarize. The task is to summarize the delimited text as a haiku. Requires text to be inserted between the triple quotes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_0\n\nLANGUAGE: Prompt\nCODE:\n```\nUSER: Summarize the text delimited by triple quotes with a haiku.\n\n\"\"\"insert text here\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Categorizing Text with OpenAI's GPT models in Python\nDESCRIPTION: This function categorizes text using OpenAI's GPT models. It constructs a prompt to categorize the given text into predefined categories and sends it to the OpenAI API. It returns the categorized text or None if an error occurs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n## These are the categories I will be using for the categorization task. You can change these as needed based on your use case.\ncategories = ['authentication','models','techniques','tools','setup','billing_limits','other']\n\ndef categorize_text(text, categories):\n    # Create a prompt for categorization\n    messages = [\n        {\"role\": \"system\", \"content\": f\"\"\"You are an expert in LLMs, and you will be given text that corresponds to an article in OpenAI's documentation.\n         Categorize the document into one of these categories: {', '.join(categories)}. Only respond with the category name and nothing else.\"\"\"},\n        {\"role\": \"user\", \"content\": text}\n    ]\n    try:\n        # Call the OpenAI API to categorize the text\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n        # Extract the category from the response\n        category = response.choices[0].message.content\n        return category\n    except Exception as e:\n        print(f\"Error categorizing text: {str(e)}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and Setting Model\nDESCRIPTION: This snippet imports the OpenAI library and sets the GPT_MODEL variable to 'gpt-4o-mini', which will be used for subsequent API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nGPT_MODEL = 'gpt-4o-mini'\n```\n\n----------------------------------------\n\nTITLE: Printing Titles of Retrieved Search Results - Python\nDESCRIPTION: Enumerates and prints all candidate document titles with their rank in result_list. Inputs are list of dictionaries produced previously. Output is a sequence of numbered titles printed to the standard output, for human inspection. Assumes \"title\" is present in each result dict.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor i, result in enumerate(result_list):\n    print(f\"{i + 1}: {result['title']}\")\n```\n\n----------------------------------------\n\nTITLE: Defining SEARCH/REPLACE Diff Format Example in Python\nDESCRIPTION: This snippet defines a multi-line Python string illustrating the SEARCH/REPLACE diff format. It includes a file path header and uses clearly marked delimiters ('>>>>>>> SEARCH', '=======', '<<<<<<< REPLACE') to separate the old code from the new replacement code, facilitating diff parsing without line numbers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nSEARCH_REPLACE_DIFF_EXAMPLE = \"\"\"\npath/to/file.py\n```\n>>>>>>> SEARCH\ndef search():\n    pass\n=======\ndef search():\n   raise NotImplementedError()\n<<<<<<< REPLACE\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Function Tool to Retrieve Emails by Address Using OpenAI Agents SDK in Python\nDESCRIPTION: Simulates retrieval of email communications for a given email address by filtering a predefined list of email records. Each record contains the sender's email, subject line, and body text, helping agents to investigate disputes by analyzing customer communications. The function is decorated for use as a tool within AI agents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\ndef get_emails(email: str) -> list:\n    \"\"\"\n    Return a list of email records for the given email address.\n    \"\"\"\n    emails = [\n        {\n            \"email\": \"customer1@example.com\",\n            \"subject\": \"Order #1121\",\n            \"body\": \"Hey, I know you don't accept refunds but the sneakers don't fit and I'd like a refund\"\n        },\n        {\n            \"email\": \"customer2@example.com\",\n            \"subject\": \"Inquiry about product availability\",\n            \"body\": \"Hello, I wanted to check if the new model of the smartphone is available in stock.\"\n        },\n        {\n            \"email\": \"customer3@example.com\",\n            \"subject\": \"Feedback on recent purchase\",\n            \"body\": \"Hi, I recently purchased a laptop from your store and I am very satisfied with the product. Keep up the good work!\"\n        }\n    ]\n    return [email_data for email_data in emails if email_data[\"email\"] == email]\n```\n\n----------------------------------------\n\nTITLE: Initializing Qdrant Vector Store\nDESCRIPTION: This snippet creates a Qdrant vector store from a list of text answers using Langchain. It utilizes `OpenAIEmbeddings` to generate embeddings for the provided answers.  The embeddings are stored in a local Qdrant instance specified by the `host` parameter. The `Qdrant.from_texts` method indexes the answers.  Requires a running Qdrant instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.vectorstores import Qdrant\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain import VectorDBQA, OpenAI\n\nembeddings = OpenAIEmbeddings()\ndoc_store = Qdrant.from_texts(\n    answers, embeddings, host=\"localhost\" \n)\n```\n\n----------------------------------------\n\nTITLE: Calling a Quiz Display Function with Parsed Arguments in Python\nDESCRIPTION: This snippet demonstrates calling a function named `display_quiz` by unpacking its expected parameters from the previously extracted arguments dictionary, specifically the quiz 'title' and 'questions'. The function's responses (mocked in this context) are captured and printed. This illustrates how to dynamically invoke a defined function with input derived from an assistant-generated tool call.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nresponses = display_quiz(arguments[\"title\"], arguments[\"questions\"])\nprint(\"Responses:\", responses)\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAIFineTuner Class for Model Fine-Tuning in Python\nDESCRIPTION: This class encapsulates the complete workflow for fine-tuning OpenAI models. It includes methods for creating the training file, submitting and monitoring the fine-tuning job, and retrieving the resulting model ID. The class handles all the steps from file upload to model completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass OpenAIFineTuner:\n    \"\"\"\n    Class to fine tune OpenAI models\n    \"\"\"\n    def __init__(self, training_file_path, model_name, suffix):\n        self.training_file_path = training_file_path\n        self.model_name = model_name\n        self.suffix = suffix\n        self.file_object = None\n        self.fine_tuning_job = None\n        self.model_id = None\n\n    def create_openai_file(self):\n        self.file_object = client.files.create(\n            file=open(self.training_file_path, \"r\"),\n            purpose=\"fine-tune\",\n        )\n\n    def wait_for_file_processing(self, sleep_time=20):\n        while self.file_object.status != 'processed':\n            time.sleep(sleep_time)\n            self.file_object.refresh()\n            print(\"File Status: \", self.file_object.status)\n\n    def create_fine_tuning_job(self):\n        self.fine_tuning_job = client.fine_tuning.jobs.create(\n            training_file=self.file_object[\"id\"],\n            model=self.model_name,\n            suffix=self.suffix,\n        )\n\n    def wait_for_fine_tuning(self, sleep_time=45):\n        while self.fine_tuning_job.status != 'succeeded':\n            time.sleep(sleep_time)\n            self.fine_tuning_job.refresh()\n            print(\"Job Status: \", self.fine_tuning_job.status)\n\n    def retrieve_fine_tuned_model(self):\n        self.model_id = client.fine_tuning.jobs.retrieve(self.fine_tuning_job[\"id\"]).fine_tuned_model\n        return self.model_id\n\n    def fine_tune_model(self):\n        self.create_openai_file()\n        self.wait_for_file_processing()\n        self.create_fine_tuning_job()\n        self.wait_for_fine_tuning()\n        return self.retrieve_fine_tuned_model()\n\nfine_tuner = OpenAIFineTuner(\n        training_file_path=\"local_cache/100_train.jsonl\",\n        model_name=\"gpt-3.5-turbo\",\n        suffix=\"100trn20230907\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI and Embedding Function (Python)\nDESCRIPTION: Configures the `openai` library for use with the standard OpenAI API. It sets the API key. Defines an `embed` function using `openai.Embedding.create` with the 'text-embedding-3-small' model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nopenai.api_key = \"\"\n\n\ndef embed(query):\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=\"text-embedding-3-small\",\n    )[\"data\"][0][\"embedding\"]\n    return embedded_query\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Embedding Generation Function in Python\nDESCRIPTION: Defines a Python function `generate_embedding` that takes a text string as input and returns a list of floats representing its vector embedding. It uses the specified OpenAI model (`text-embedding-3-small`) to create the embedding. This function is crucial for converting text data into a vector format suitable for vector search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = \"text-embedding-3-small\"\ndef generate_embedding(text: str) -> list[float]:\n    return openai.embeddings.create(input = [text], model=model).data[0].embedding\n\n```\n\n----------------------------------------\n\nTITLE: Polling and Retrieving Assistant's Insight Response (OpenAI Assistants API/Python)\nDESCRIPTION: Pauses execution briefly to allow the Assistant time to process the request for insights. It then calls `get_response` to retrieve the latest messages in the thread and extracts the text content of the Assistant's response, which contains the generated bullet points of insights.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Hard coded wait for a response, as the assistant may iterate on the bullets.\ntime.sleep(10)\nresponse = get_response(thread)\nbullet_points = response.data[0].content[0].text.value\nprint(bullet_points)\n\n```\n\n----------------------------------------\n\nTITLE: Performing image similarity search using embeddings and FAISS\nDESCRIPTION: Calculates the embedding of a user-provided image, searches the FAISS index for the top two most similar images based on inner product similarity, then sorts results by smallest distance to find the closest matches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimage_search_embedding = get_features_from_image_path([image_path])\ndistances, indices = index.search(image_search_embedding.reshape(1, -1), 2)\ndistances = distances[0]\nindices = indices[0]\nindices_distances = list(zip(indices, distances))\nindices_distances.sort(key=lambda x: x[1])\n```\n\n----------------------------------------\n\nTITLE: Uploading a File for Assistants API\nDESCRIPTION: This snippet demonstrates how to upload a file to OpenAI using the Files API, specifically for use with the Assistants API. It supports Python, Node.js, and cURL. The `purpose` parameter must be set to 'assistants' when creating the file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfile = client.files.create(\n  file=open(\"revenue-forecast.csv\", \"rb\"),\n  purpose='assistants'\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst file = await openai.files.create({\n  file: fs.createReadStream(\"revenue-forecast.csv\"),\n  purpose: \"assistants\",\n});\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/files \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -F purpose=\"assistants\" \\\n  -F file=\"@revenue-forecast.csv\"\n```\n\n----------------------------------------\n\nTITLE: Gold-Standard Answer Overlap & Contradiction Evaluation - example-chat\nDESCRIPTION: This snippet presents a system prompt which directs the model to compare user answers against an expert answer. It establishes classification categories for overlap (disjoint, equal, subset, superset, overlapping) and requires explicit contradiction detection, returning results in JSON. Prerequisites: a capable LLM chat system with both candidate and reference answers provided. Expected inputs are the candidate and expert answers; output is a JSON object with overlap type and contradiction boolean.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_22\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: Use the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \"Step 1: Reason...\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\"type_of_overlap\": \"disjoint\" or \"equal\" or \"subset\" or \"superset\" or \"overlapping\", \"contradiction\": true or false}\n```\n\n----------------------------------------\n\nTITLE: Defining Middleware OpenAPI Schema for Gong Call Transcript Retrieval in YAML\nDESCRIPTION: This OpenAPI 3.1.0 schema defines an API for an Azure Function middleware that retrieves Gong call transcripts and related metadata by a list of call IDs. The single POST endpoint accepts a JSON body containing an array of strings ('callIds') and returns paginated metadata with call transcript details including call ID, title, start time, duration, URL, and transcript content. It includes expected response codes for successful, invalid, unauthorized, and server-error cases. This schema supports a Custom GPT action setup to interact with Gong APIs via a middleware endpoint.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce_gong.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Call Transcripts API\n  description: API to retrieve call transcripts and associated metadata by specific call IDs.\n  version: 1.0.1\nservers:\n  - url: https://<subdomain>.azurewebsites.net/api\n    description: Production server\npaths:\n  /callTranscripts:\n    post:\n      operationId: getTranscriptsByCallIds\n      x-openai-isConsequential: false\n      summary: Retrieve call transcripts by call IDs\n      description: Fetches specific call transcripts based on the provided call IDs in the request body.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                callIds:\n                  type: array\n                  description: List of call IDs for which transcripts need to be fetched.\n                  items:\n                    type: string\n              required:\n                - callIds\n      responses:\n        '200':\n          description: A successful response containing the requested call transcripts and metadata.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  requestId:\n                    type: string\n                    description: Unique request identifier.\n                  records:\n                    type: object\n                    description: Metadata about the pagination.\n                    properties:\n                      totalRecords:\n                        type: integer\n                        description: Total number of records available.\n                      currentPageSize:\n                        type: integer\n                        description: Number of records in the current page.\n                      currentPageNumber:\n                        type: integer\n                        description: The current page number.\n                  callTranscripts:\n                    type: array\n                    description: List of call transcripts.\n                    items:\n                      type: object\n                      properties:\n                        callId:\n                          type: string\n                          description: Unique identifier for the call.\n                        title:\n                          type: string\n                          description: Title of the call or meeting.\n                        started:\n                          type: string\n                          format: date-time\n                          description: Timestamp when the call started.\n                        duration:\n                          type: integer\n                          description: Duration of the call in seconds.\n                        url:\n                          type: string\n                          format: uri\n                          description: URL to access the call recording or details.\n                        content:\n                          type: string\n                          description: Transcript content of the call.\n        '400':\n          description: Invalid request. Possibly due to missing or invalid `callIds` parameter.\n        '401':\n          description: Unauthorized access due to invalid or missing API key.\n        '500':\n          description: Internal server error.\n```\n\n----------------------------------------\n\nTITLE: Defining and Evaluating Zero-Shot Embedding Classifier - Python\nDESCRIPTION: Defines the `evaluate_embeddings_approach` function which performs zero-shot classification and evaluates results. It takes a list of label strings, gets their embeddings, calculates a score for each review embedding based on the difference in cosine similarity to positive and negative label embeddings, makes predictions based on the score's sign, and prints a classification report and displays a precision-recall curve. Requires a utility function `get_embedding` and `cosine_similarity`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import cosine_similarity, get_embedding\nfrom sklearn.metrics import PrecisionRecallDisplay\n\ndef evaluate_embeddings_approach(\n    labels = ['negative', 'positive'],\n    model = EMBEDDING_MODEL,\n):\n    label_embeddings = [get_embedding(label, model=model) for label in labels]\n\n    def label_score(review_embedding, label_embeddings):\n        return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n\n    probas = df[\"embedding\"].apply(lambda x: label_score(x, label_embeddings))\n    preds = probas.apply(lambda x: 'positive' if x>0 else 'negative')\n\n    report = classification_report(df.sentiment, preds)\n    print(report)\n\n    display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label='positive')\n    _ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\nevaluate_embeddings_approach(labels=['negative', 'positive'], model=EMBEDDING_MODEL)\n\n```\n\n----------------------------------------\n\nTITLE: Executing Simulated LLM Conversation with Tool-based Responses in Python\nDESCRIPTION: Implements a conversation loop where an LLM simulates a user engaging with an assistant. The function formats messages, tracks conversation history, and continues until the objective is achieved (indicated by 'DONE' in the response).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef execute_conversation(objective):\n\n    conversation_messages = []\n\n    done = False\n\n    user_query = objective\n\n    while done is False:\n\n        conversation_messages = submit_user_message(user_query,conversation_messages)\n\n        messages_string = ''\n        for x in conversation_messages:\n            if isinstance(x,dict):\n                if x['role'] == 'user':\n                    messages_string += 'User: ' + x['content'] + '\\n'\n                elif x['role'] == 'tool':\n                    if x['name'] == 'speak_to_user':\n                        messages_string += 'Assistant: ' + x['content'] + '\\n'\n            else:\n                continue\n\n        messages = [\n            {\n            \"role\": \"system\",\n            \"content\": customer_system_prompt.format(query=objective,chat_history=messages_string)\n            },\n            {\n            \"role\": \"user\",\n            \"content\": \"Continue the chat to solve your query. Remember, you are in the user in this exchange. Do not provide User: or Assistant: in your response\"\n            }\n        ]\n\n        user_response = client.chat.completions.create(model=GPT_MODEL,messages=messages,temperature=0.5)\n\n        conversation_messages.append({\n            \"role\": \"user\",\n            \"content\": user_response.choices[0].message.content\n            })\n\n        if 'DONE' in user_response.choices[0].message.content:\n            done = True\n            print(\"Achieved objective, closing conversation\\n\\n\")\n\n        else:\n            user_query = user_response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search - Python/SQL\nDESCRIPTION: Performs a vector search in the MyScale database to find content similar to a given query. It first generates an embedding for the query using the OpenAI API. Then, it executes an SQL query to calculate the distance between the query embedding and the `content_vector` column in the `articles` table, ordering the results by distance and limiting the number of results to `top_k`. Finally, it prints the titles of the top K most similar articles.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Famous battles in Scottish history\"\n\n# creates embedding vector from user query\nembed = openai.Embedding.create(\n    input=query,\n    model=\"text-embedding-3-small\",\n)[\"data\"][0][\"embedding\"]\n\n# query the database to find the top K similar content to the given query\ntop_k = 10\nresults = client.query(f\"\"\"\nSELECT id, url, title, distance(content_vector, {embed}) as dist\nFROM default.articles\nORDER BY dist\nLIMIT {top_k}\n\"\"\")\n\n# display results\nfor i, r in enumerate(results.named_results()):\n    print(i+1, r['title'])\n```\n\n----------------------------------------\n\nTITLE: Example Custom GPT Instructions\nDESCRIPTION: This snippet provides example instructions for a Custom GPT designed to interact with Google Drive. It outlines the context, instructions for using the 'listFiles' function, and examples of query documentation from Google for the 'listFiles' function. The instructions emphasize summarizing file content, behaving professionally, ensuring coding blocks are explained, handling dates, asking for clarification, and respecting user privacy and data security.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_google_drive.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n*** Context *** \n\nYou are an office helper who takes a look at files within Google Drive and reads in information.  In this way, when asked about something, please take a look at all of the relevant information within the drive.  Respect file names, but also take a look at each document and sheet.\n\n*** Instructions ***\n\nUse the 'listFiles' function to get a list of files available within docs.  In this way, determine out of this list which files make the most sense for you to pull back taking into account name and title.  After the output of listFiles is called into context, act like a normal business analyst.  Things you could be asked to be are:\n\n- Summaries: what happens in a given file?  Please give a consistent, concise answer and read through the entire file before giving an answer.\n- Professionalism: Behave professionally, providing clear and concise responses.\n- Synthesis, Coding, and Data Analysis: ensure coding blocks are explained.\n- When handling dates: make sure that dates are searched using date fields and also if you don't find anything, use titles.\n- Clarification: Ask for clarification when needed to ensure accuracy and completeness in fulfilling user requests.  Try to make sure you know exactly what is being asked. \n- Privacy and Security: Respect user privacy and handle all data securely.\n\n*** Examples of Documentation ***\nHere is the relevant query documentation from Google for the listFiles function:\nWhat you want to query\tExample\nFiles with the name \"hello\"\tname = 'hello'\nFiles with a name containing the words \"hello\" and \"goodbye\"\tname contains 'hello' and name contains 'goodbye'\nFiles with a name that does not contain the word \"hello\"\tnot name contains 'hello'\nFolders that are Google apps or have the folder MIME type\tmimeType = 'application/vnd.google-apps.folder'\nFiles that are not folders\tmimeType != 'application/vnd.google-apps.folder'\nFiles that contain the text \"important\" and in the trash\tfullText contains 'important' and trashed = true\nFiles that contain the word \"hello\"\tfullText contains 'hello'\nFiles that do not have the word \"hello\"\tnot fullText contains 'hello'\nFiles that contain the exact phrase \"hello world\"\tfullText contains '\"hello world\"'\nFiles with a query that contains the \"\\\" character (e.g., \"\\authors\")\tfullText contains '\\\\authors'\nFiles with ID within a collection, e.g. parents collection\t'1234567' in parents\nFiles in an application data folder in a collection\t'appDataFolder' in parents\nFiles for which user \"test@example.org\" has write permission\t'test@example.org' in writers\nFiles for which members of the group \"group@example.org\" have write permission\t'group@example.org' in writers\nFiles modified after a given date\tmodifiedTime > '2012-06-04T12:00:00' // default time zone is UTC\nFiles shared with the authorized user with \"hello\" in the name\tsharedWithMe and name contains 'hello'\nFiles that have not been shared with anyone or domains (only private, or shared with specific users or groups)\tvisibility = 'limited'\nImage or video files modified after a specific date\tmodifiedTime > '2012-06-04T12:00:00' and (mimeType contains 'image/' or mimeType contains 'video/')\n\n```\n\n----------------------------------------\n\nTITLE: Defining Customer Support Chat Messages with Guardrails in Python\nDESCRIPTION: Defines a structured list of chat messages containing a detailed system role prompt that enforces professional, empathetic, and secure customer support behavior with explicit guardrails for confidentiality, payment security, boundaries, legal compliance, and inclusivity. Includes a user role message that requests delivery information with an order number, and a separate user query to cancel an order. These messages serve as input history for GPT-based chat completions. Dependencies include basic Python structures and text formatting within message content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"system\", \n        \"content\": (\n            \"You are a professional, empathetic, and efficient customer support assistant. Your mission is to provide fast, clear, \"\n            \"and comprehensive assistance to customers while maintaining a warm and approachable tone. \"\n            \"Always express empathy, especially when the user seems frustrated or concerned, and ensure that your language is polite and professional. \"\n            \"Use simple and clear communication to avoid any misunderstanding, and confirm actions with the user before proceeding. \"\n            \"In more complex or time-sensitive cases, assure the user that you're taking swift action and provide regular updates. \"\n            \"Adapt to the userâ€™s tone: remain calm, friendly, and understanding, even in stressful or difficult situations.\\n\\n\"\n            \"Additionally, there are several important guardrails that you must adhere to while assisting users:\\n\\n\"\n            \"1. **Confidentiality and Data Privacy**: Do not share any sensitive information about the company or other users. When handling personal details such as order IDs, addresses, or payment methods, ensure that the information is treated with the highest confidentiality. If a user requests access to their data, only provide the necessary information relevant to their request, ensuring no other user's information is accidentally revealed.\\n\\n\"\n            \"2. **Secure Payment Handling**: When updating payment details or processing refunds, always ensure that payment data such as credit card numbers, CVVs, and expiration dates are transmitted and stored securely. Never display or log full credit card numbers. Confirm with the user before processing any payment changes or refunds.\\n\\n\"\n            \"3. **Respect Boundaries**: If a user expresses frustration or dissatisfaction, remain calm and empathetic but avoid overstepping professional boundaries. Do not make personal judgments, and refrain from using language that might escalate the situation. Stick to factual information and clear solutions to resolve the user's concerns.\\n\\n\"\n            \"4. **Legal Compliance**: Ensure that all actions you take comply with legal and regulatory standards. For example, if the user requests a refund, cancellation, or return, follow the companyâ€™s refund policies strictly. If the order cannot be canceled due to being shipped or another restriction, explain the policy clearly but sympathetically.\\n\\n\"\n            \"5. **Consistency**: Always provide consistent information that aligns with company policies. If unsure about a company policy, communicate clearly with the user, letting them know that you are verifying the information, and avoid providing false promises. If escalating an issue to another team, inform the user and provide a realistic timeline for when they can expect a resolution.\\n\\n\"\n            \"6. **User Empowerment**: Whenever possible, empower the user to make informed decisions. Provide them with relevant options and explain each clearly, ensuring that they understand the consequences of each choice (e.g., canceling an order may result in loss of loyalty points, etc.). Ensure that your assistance supports their autonomy.\\n\\n\"\n            \"7. **No Speculative Information**: Do not speculate about outcomes or provide information that you are not certain of. Always stick to verified facts when discussing order statuses, policies, or potential resolutions. If something is unclear, tell the user you will investigate further before making any commitments.\\n\\n\"\n            \"8. **Respectful and Inclusive Language**: Ensure that your language remains inclusive and respectful, regardless of the userâ€™s tone. Avoid making assumptions based on limited information and be mindful of diverse user needs and backgrounds.\"\n        )\n    },\n    {\n        \"role\": \"user\", \n        \"content\": (\n            \"Hi, I placed an order three days ago and havenâ€™t received any updates on when itâ€™s going to be delivered. \"\n            \"Could you help me check the delivery date? My order number is #9876543210. Iâ€™m a little worried because I need this item urgently.\"\n        )\n    }\n]\n\nuser_query2 = {\n    \"role\": \"user\", \n    \"content\": (\n        \"Since my order hasn't actually shipped yet, I would like to cancel it. \"\n        \"The order number is #9876543210, and I need to cancel because Iâ€™ve decided to purchase it locally to get it faster. \"\n        \"Can you help me with that? Thank you!\"\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Context-Aware Answer with OpenAI Chat Completions API in Python\nDESCRIPTION: Uses the OpenAI gpt-3.5-turbo ChatCompletion model to answer the original question using the top Elasticsearch search hit as context. Sends a system message and user prompt formatted with both question and retrieved text. Outputs all model choices, printing generated content for review or downstream usage. Requires prior retrieval and API credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nsummary = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Answer the following question:\" \n         + question \n         + \"by using the following text:\" \n         + top_hit_summary},\n    ]\n)\n\nchoices = summary.choices\n\nfor choice in choices:\n    print(\"------------------------------------------------------------\")\n    print(choice.message.content)\n    print(\"------------------------------------------------------------\")\n```\n\n----------------------------------------\n\nTITLE: Uploading files to OpenAI API for fine-tuning\nDESCRIPTION: Creates a function to upload the JSONL files to OpenAI's Files endpoint and uploads both training and validation files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef upload_file(file_name: str, purpose: str) -> str:\n    with open(file_name, \"rb\") as file_fd:\n        response = client.files.create(file=file_fd, purpose=purpose)\n    return response.id\n\n\ntraining_file_id = upload_file(training_file_name, \"fine-tune\")\nvalidation_file_id = upload_file(validation_file_name, \"fine-tune\")\n\nprint(\"Training file ID:\", training_file_id)\nprint(\"Validation file ID:\", validation_file_id)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Relevancy of a single query - Python\nDESCRIPTION: This code demonstrates the evaluation of relevancy of a response to a query using the `RelevancyEvaluator`.  It first picks a query. Then it generates the response using the `query_engine`. Finally, it calls `relevancy_gpt4.evaluate_response()` to measure the relevancy.  The `eval_result.passing` attribute indicates if the generated response is relevant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Pick a query\nquery = queries[10]\n\nquery\n```\n\nLANGUAGE: python\nCODE:\n```\n# Generate response.\n# response_vector has response and source nodes (retrieved context)\nresponse_vector = query_engine.query(query)\n\n# Relevancy evaluation\neval_result = relevancy_gpt4.evaluate_response(\n    query=query, response=response_vector\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# You can check passing parameter in eval_result if it passed the evaluation.\neval_result.passing\n```\n\nLANGUAGE: python\nCODE:\n```\n# You can get the feedback for the evaluation.\neval_result.feedback\n```\n\n----------------------------------------\n\nTITLE: Testing Generalization on New Hockey Content\nDESCRIPTION: Evaluating the model's ability to generalize by classifying a new hockey-related tweet that differs from the training data format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsample_hockey_tweet = \"\"\"Thank you to the \n@Canes\n and all you amazing Caniacs that have been so supportive! You guys are some of the best fans in the NHL without a doubt! Really excited to start this new chapter in my career with the \n@DetroitRedWings\n !!\"\"\"\nres = client.completions.create(model=ft_model, prompt=sample_hockey_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\nres.choices[0].text\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing Data with ThreadPoolExecutor\nDESCRIPTION: This code defines functions to process data in parallel using `ThreadPoolExecutor`. It iterates through a Pandas DataFrame, generates prompts, calls a model, and updates a progress bar. The `process_example` function is designed to be executed concurrently, handling the individual processing of a row. `process_dataframe` orchestrates the parallel execution using `ThreadPoolExecutor` and updates progress. It requires the `tqdm` library for a progress bar and the `concurrent.futures` library for parallel execution. The expected input is a Pandas DataFrame and the name of the OpenAI model to use.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef process_example(index, row, model, df, progress_bar):\n    global progress_index\n\n    try:\n        # Generate the prompt using the row\n        prompt = generate_prompt(row, varieties)\n\n        df.at[index, model + \"-variety\"] = call_model(model, prompt)\n        \n        # Update the progress bar\n        progress_bar.update(1)\n        \n        progress_index += 1\n    except Exception as e:\n        print(f\"Error processing model {model}: {str(e)}\")\n\ndef process_dataframe(df, model):\n    global progress_index\n    progress_index = 1  # Reset progress index\n\n    # Create a tqdm progress bar\n    with tqdm(total=len(df), desc=\"Processing rows\") as progress_bar:\n        # Process each example concurrently using ThreadPoolExecutor\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = {executor.submit(process_example, index, row, model, df, progress_bar): index for index, row in df.iterrows()}\n            \n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    future.result()  # Wait for each example to be processed\n                except Exception as e:\n                    print(f\"Error processing example: {str(e)}\")\n\n    return df\n```\n```\n\n----------------------------------------\n\nTITLE: Get Database Token and Keyspace Python\nDESCRIPTION: This code prompts the user to input their Astra DB application token and keyspace name using the `getpass` and `input` functions, respectively.  These values are required to authenticate and connect to the database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\nASTRA_DB_APPLICATION_TOKEN = getpass(\"Please provide your Database Token ('AstraCS:...' string): \")\nASTRA_DB_KEYSPACE = input(\"Please provide the Keyspace name for your Database: \")\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store and Uploading PDFs in Python\nDESCRIPTION: Defines functions to create a vector store on OpenAI API and upload PDF files to it. The code handles parallel uploads with progress tracking and error handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef upload_single_pdf(file_path: str, vector_store_id: str):\n    file_name = os.path.basename(file_path)\n    try:\n        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n        attach_response = client.vector_stores.files.create(\n            vector_store_id=vector_store_id,\n            file_id=file_response.id\n        )\n        return {\"file\": file_name, \"status\": \"success\"}\n    except Exception as e:\n        print(f\"Error with {file_name}: {str(e)}\")\n        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n\ndef upload_pdf_files_to_vector_store(vector_store_id: str):\n    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n    \n    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n            result = future.result()\n            if result[\"status\"] == \"success\":\n                stats[\"successful_uploads\"] += 1\n            else:\n                stats[\"failed_uploads\"] += 1\n                stats[\"errors\"].append(result)\n\n    return stats\n\ndef create_vector_store(store_name: str) -> dict:\n    try:\n        vector_store = client.vector_stores.create(name=store_name)\n        details = {\n            \"id\": vector_store.id,\n            \"name\": vector_store.name,\n            \"created_at\": vector_store.created_at,\n            \"file_count\": vector_store.file_counts.completed\n        }\n        print(\"Vector store created:\", details)\n        return details\n    except Exception as e:\n        print(f\"Error creating vector store: {e}\")\n        return {}\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Image URLs for GPT-4o Chat Completion in Python\nDESCRIPTION: Defines a function multiimage_completion that sends a chat completion request including multiple images provided as image URLs with a high detail level alongside a user text query. The function calls the GPT-4o-2024-08-06 model with a message containing structured image URL objects and textual input, printing the JSON response. The main function demonstrates three sequential calls with different image URL combinations and user queries, introducing delays to simulate caching impacts on token usage. This snippet illustrates handling image inputs within chat message sequences, model selection, and cache-sensitive interaction patterns. Requires an OpenAI client object, JSON and time modules.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsauce_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/12-04-20-saucen-by-RalfR-15.jpg/800px-12-04-20-saucen-by-RalfR-15.jpg\"\nveggie_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Veggies.jpg/800px-Veggies.jpg\"\neggs_url= \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Egg_shelf.jpg/450px-Egg_shelf.jpg\"\nmilk_url= \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Lactaid_brand.jpg/800px-Lactaid_brand.jpg\"\n\ndef multiimage_completion(url1, url2, user_query):\n    completion = client.chat.completions.create(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\n        \"role\": \"user\",\n        \"content\": [\n            {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": url1,\n                \"detail\": \"high\"\n            },\n            },\n            {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": url2,\n                \"detail\": \"high\"\n            },\n            },\n            {\"type\": \"text\", \"text\": user_query}\n        ],\n        }\n    ],\n    max_tokens=300,\n    )\n    print(json.dumps(completion.to_dict(), indent=4))\n    \n\ndef main(sauce_url, veggie_url):\n    multiimage_completion(sauce_url, veggie_url, \"Please list the types of sauces are shown in these images\")\n    #delay for 20 seconds\n    time.sleep(20)\n    multiimage_completion(sauce_url, veggie_url, \"Please list the types of vegetables are shown in these images\")\n    time.sleep(20)\n    multiimage_completion(milk_url, sauce_url, \"Please list the types of sauces are shown in these images\")\n\nif __name__ == \"__main__\":\n    main(sauce_url, veggie_url)\n```\n\n----------------------------------------\n\nTITLE: Legacy Completions API Example Calls in Python and Node.js\nDESCRIPTION: Provides example usage for the legacy OpenAI completions API with a text prompt to generate a tagline for an ice cream shop. It illustrates the Python syntax using the OpenAI client library and an equivalent example in Node.js with asynchronous invocation. Both snippets specify the model as 'gpt-3.5-turbo-instruct' and demonstrate how to pass a prompt string. These examples are useful to understand API differences compared to the chat completion endpoint.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.completions.create(\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=\"Write a tagline for an ice cream shop.\"\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst completion = await openai.completions.create({\n    model: 'gpt-3.5-turbo-instruct',\n    prompt: 'Write a tagline for an ice cream shop.'\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Negative Pairs for Training\nDESCRIPTION: This snippet defines the `dataframe_of_negatives` function, which generates negative pairs of text from the positive pairs.  It takes a dataframe of positive pairs and creates negative pairs by combining texts not already paired as positive. It also defines and uses `negatives_per_positive` to control the ratio of negative to positive examples in the training set, applying random sampling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# generate negatives\ndef dataframe_of_negatives(dataframe_of_positives: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Return dataframe of negative pairs made by combining elements of positive pairs.\"\"\"\n    texts = set(dataframe_of_positives[\"text_1\"].values) | set(\n        dataframe_of_positives[\"text_2\"].values\n    )\n    all_pairs = {(t1, t2) for t1 in texts for t2 in texts if t1 < t2}\n    positive_pairs = set(\n        tuple(text_pair)\n        for text_pair in dataframe_of_positives[[\"text_1\", \"text_2\"]].values\n    )\n    negative_pairs = all_pairs - positive_pairs\n    df_of_negatives = pd.DataFrame(list(negative_pairs), columns=[\"text_1\", \"text_2\"])\n    df_of_negatives[\"label\"] = -1\n    return df_of_negatives\n\n```\n\nLANGUAGE: python\nCODE:\n```\nnegatives_per_positive =\n    1  # it will work at higher values too, but more data will be slower\n# generate negatives for training dataset\ntrain_df_negatives = dataframe_of_negatives(train_df)\ntrain_df_negatives[\"dataset\"] = \"train\"\n# generate negatives for test dataset\ntest_df_negatives = dataframe_of_negatives(test_df)\ntest_df_negatives[\"dataset\"] = \"test\"\n# sample negatives and combine with positives\ntrain_df = pd.concat(\n    [\n        train_df,\n        train_df_negatives.sample(\n            n=len(train_df) * negatives_per_positive, random_state=random_seed\n        ),\n    ]\n)\ntest_df = pd.concat(\n    [\n        test_df,\n        test_df_negatives.sample(\n            n=len(test_df) * negatives_per_positive, random_state=random_seed\n        ),\n    ]\n)\n\ndf = pd.concat([train_df, test_df])\n```\n\n----------------------------------------\n\nTITLE: Creating Index and Loading Milvus Collection in Python\nDESCRIPTION: Creates an index on the 'embedding' field of the previously created Milvus collection using the parameters defined in `INDEX_PARAM` (HNSW index with L2 metric). After index creation, the `collection.load()` method is called to load the collection data and index into memory for efficient searching.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create the index on the collection and load it.\ncollection.create_index(field_name=\"embedding\", index_params=INDEX_PARAM)\ncollection.load()\n```\n\n----------------------------------------\n\nTITLE: Initializing SQLite Database Connection in Python\nDESCRIPTION: This snippet connects to a local SQLite database named 'Chinook.db' using Python's built-in sqlite3 module. The prerequisite is that the 'data/Chinook.db' database file exists at the specified path. The connection object 'conn' is used for further database queries, and a success message is printed. Expected input is no parameters; output is a connected sqlite3.Connection object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport sqlite3\n\nconn = sqlite3.connect(\"data/Chinook.db\")\nprint(\"Opened database successfully\")\n```\n\n----------------------------------------\n\nTITLE: Generating Function Invocations\nDESCRIPTION: This code generates function invocations for all functions except 'reject_request'. It iterates through a list of function definitions, generates permutations of parameters, and then uses the  `INVOCATION_FILLER_PROMPT` with `gpt-4o` to fill in values for string and integer parameters, creating valid input objects.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ninput_objects = []\nall_but_reject = [f for f in function_list if f.get(\"name\") != \"reject_request\"]\n\nfor function in all_but_reject:\n    func_name = function[\"function\"][\"name\"]\n    params = function[\"function\"][\"parameters\"]\n    for arguments in generate_permutations(params):\n        if any(val in arguments.values() for val in [\"fill_in_int\", \"fill_in_str\"]):\n            input_object = {\"name\": func_name, \"arguments\": arguments}\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": INVOCATION_FILLER_PROMPT.format(\n                        invocation=str(input_object), function=function\n                    ),\n                }\n            ]\n            input_object, usage = get_chat_completion(\n                model=\"gpt-4o\", messages=messages, max_tokens=200, temperature=0.1\n            ).content\n        else:\n            input_object = {\"name\": func_name, \"arguments\": arguments}\n\n        input_objects.append(input_object)\n```\n\n----------------------------------------\n\nTITLE: Printing the Final Answer Output from OpenAI API Response (Python)\nDESCRIPTION: This short Python code prints the final answer output_text property from the completed Responses API result. It assumes response_2 has been previously obtained through a call to client.responses.create. The expected output is the model's textual answer incorporating any tool calls performed earlier.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n# print the final answer\nprint(response_2.output_text)\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving Token Usage Data for Streamed Completions\nDESCRIPTION: Shows how to get token usage statistics for streamed completions by setting stream_options={\"include_usage\": True}. This adds an extra chunk with usage data at the end of the stream.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example of an OpenAI ChatCompletion request with stream=True and stream_options={\"include_usage\": True}\n\n# a ChatCompletion request\nresponse = client.chat.completions.create(\n    model='gpt-4o-mini',\n    messages=[\n        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n    ],\n    temperature=0,\n    stream=True,\n    stream_options={\"include_usage\": True}, # retrieving token usage for stream response\n)\n\nfor chunk in response:\n    print(f\"choices: {chunk.choices}\\nusage: {chunk.usage}\")\n    print(\"****************\")\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment and Import Packages for NER with OpenAI and Wikipedia\nDESCRIPTION: Installs necessary Python packages for OpenAI API interaction, Wikipedia link retrieval, and retry handling; loads environment variables and initializes OpenAI client. Sets the target model to 'gpt-3.5-turbo-0613'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade openai --quiet\n%pip install --upgrade nlpia2-wikipedia --quiet\n%pip install --upgrade tenacity --quiet\n```\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport logging\nimport os\n\nimport openai\nimport wikipedia\n\nfrom typing import Optional\nfrom IPython.display import display, Markdown\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\n\nlogging.basicConfig(level=logging.INFO, format=' %(asctime)s - %(levelname)s - %(message)s')\n\nOPENAI_MODEL = 'gpt-3.5-turbo-0613'\n\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Parsing Patch Text with the Parser Class in Python\nDESCRIPTION: This snippet defines a Parser class that processes patch files line-by-line to build a Patch object. It accepts current file contents and patch lines as input and maintains an index to track parsing progress. Helper methods normalize lines for cross-platform compatibility, check prefixes, and consume lines conditionally. The main parse() method handles patch commands such as file additions, deletions, and updates by dispatching to specific parsing helper methods. It validates inputs against current files, manages duplicate actions, and raises DiffError on malformed input or unexpected lines. The parser also integrates fuzz matching to tolerate minor mismatches in context diff headers, allowing flexibility in applying patches. The output is a Patch instance representing all changes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Parser:\n    current_files: Dict[str, str]\n    lines: List[str]\n    index: int = 0\n    patch: Patch = field(default_factory=Patch)\n    fuzz: int = 0\n\n    # ------------- low-level helpers -------------------------------------- #\n    def _cur_line(self) -> str:\n        if self.index >= len(self.lines):\n            raise DiffError(\"Unexpected end of input while parsing patch\")\n        return self.lines[self.index]\n\n    @staticmethod\n    def _norm(line: str) -> str:\n        \"\"\"Strip CR so comparisons work for both LF and CRLF input.\"\"\"\n        return line.rstrip(\"\\r\")\n\n    # ------------- scanning convenience ----------------------------------- #\n    def is_done(self, prefixes: Optional[Tuple[str, ...]] = None) -> bool:\n        if self.index >= len(self.lines):\n            return True\n        if (\n            prefixes\n            and len(prefixes) > 0\n            and self._norm(self._cur_line()).startswith(prefixes)\n        ):\n            return True\n        return False\n\n    def startswith(self, prefix: Union[str, Tuple[str, ...]]) -> bool:\n        return self._norm(self._cur_line()).startswith(prefix)\n\n    def read_str(self, prefix: str) -> str:\n        \"\"\"\n        Consume the current line if it starts with *prefix* and return the text\n        **after** the prefix.  Raises if prefix is empty.\n        \"\"\"\n        if prefix == \"\":\n            raise ValueError(\"read_str() requires a non-empty prefix\")\n        if self._norm(self._cur_line()).startswith(prefix):\n            text = self._cur_line()[len(prefix) :]\n            self.index += 1\n            return text\n        return \"\"\n\n    def read_line(self) -> str:\n        \"\"\"Return the current raw line and advance.\"\"\"\n        line = self._cur_line()\n        self.index += 1\n        return line\n\n    # ------------- public entry point -------------------------------------- #\n    def parse(self) -> None:\n        while not self.is_done((\"*** End Patch\",)):\n            # ---------- UPDATE ---------- #\n            path = self.read_str(\"*** Update File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate update for file: {path}\")\n                move_to = self.read_str(\"*** Move to: \")\n                if path not in self.current_files:\n                    raise DiffError(f\"Update File Error - missing file: {path}\")\n                text = self.current_files[path]\n                action = self._parse_update_file(text)\n                action.move_path = move_to or None\n                self.patch.actions[path] = action\n                continue\n\n            # ---------- DELETE ---------- #\n            path = self.read_str(\"*** Delete File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate delete for file: {path}\")\n                if path not in self.current_files:\n                    raise DiffError(f\"Delete File Error - missing file: {path}\")\n                self.patch.actions[path] = PatchAction(type=ActionType.DELETE)\n                continue\n\n            # ---------- ADD ---------- #\n            path = self.read_str(\"*** Add File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate add for file: {path}\")\n                if path in self.current_files:\n                    raise DiffError(f\"Add File Error - file already exists: {path}\")\n                self.patch.actions[path] = self._parse_add_file()\n                continue\n\n            raise DiffError(f\"Unknown line while parsing: {self._cur_line()}\")\n\n        if not self.startswith(\"*** End Patch\"):\n            raise DiffError(\"Missing *** End Patch sentinel\")\n        self.index += 1  # consume sentinel\n\n    # ------------- section parsers ---------------------------------------- #\n    def _parse_update_file(self, text: str) -> PatchAction:\n        action = PatchAction(type=ActionType.UPDATE)\n        lines = text.split(\"\\n\")\n        index = 0\n        while not self.is_done(\n            (\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",\n            )\n        ):\n            def_str = self.read_str(\"@@ \")\n            section_str = \"\"\n            if not def_str and self._norm(self._cur_line()) == \"@@\":\n                section_str = self.read_line()\n\n            if not (def_str or section_str or index == 0):\n                raise DiffError(f\"Invalid line in update section:\\n{self._cur_line()}\")\n\n            if def_str.strip():\n                found = False\n                if def_str not in lines[:index]:\n                    for i, s in enumerate(lines[index:], index):\n                        if s == def_str:\n                            index = i + 1\n                            found = True\n                            break\n                if not found and def_str.strip() not in [\n                    s.strip() for s in lines[:index]\n                ]:\n                    for i, s in enumerate(lines[index:], index):\n                        if s.strip() == def_str.strip():\n                            index = i + 1\n                            self.fuzz += 1\n                            found = True\n                            break\n\n            next_ctx, chunks, end_idx, eof = peek_next_section(self.lines, self.index)\n            new_index, fuzz = find_context(lines, next_ctx, index, eof)\n            if new_index == -1:\n                ctx_txt = \"\\n\".join(next_ctx)\n                raise DiffError(\n                    f\"Invalid {'EOF ' if eof else ''}context at {index}:\\n{ctx_txt}\"\n                )\n            self.fuzz += fuzz\n            for ch in chunks:\n                ch.orig_index += new_index\n                action.chunks.append(ch)\n            index = new_index + len(next_ctx)\n            self.index = end_idx\n        return action\n\n    def _parse_add_file(self) -> PatchAction:\n        lines: List[str] = []\n        while not self.is_done(\n            (\"*** End Patch\", \"*** Update File:\", \"*** Delete File:\", \"*** Add File:\")\n        ):\n            s = self.read_line()\n            if not s.startswith(\"+\"):\n                raise DiffError(f\"Invalid Add File line (missing '+'): {s}\")\n            lines.append(s[1:])  # strip leading '+'\n        return PatchAction(type=ActionType.ADD, new_file=\"\\n\".join(lines))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with API Key\nDESCRIPTION: This Python code initializes the OpenAI client. It automatically retrieves the API key from the environment variable `OPENAI_API_KEY` by default or allows specifying a custom environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n# defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n# if you saved the key under a different environment variable name, you can do something like:\n# client = OpenAI(\n#   api_key=os.environ.get(\"CUSTOM_ENV_NAME\"),\n# )\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to CSV in Python\nDESCRIPTION: Saves the results DataFrame to a CSV file without including the index column.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nresults_df.to_csv('hallucination_results.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Sequential Multi-Tool Calling Orchestration with OpenAI Responses API in Python\nDESCRIPTION: This Python example processes a single query by explicitly sequencing tool calls: it instructs the API to first invoke a web search, then query a Pinecone-backed knowledge base. The code builds the input messages and triggers the Responses API with explicit instructions in the system message to ensure the correct tool call sequence. Requires the OpenAI API, appropriate tool definitions, and setup for Pinecone search. The snippet prints step-by-step diagnostics to illustrate invocation order and captures the initial response output for further inspection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nitem = \"What is the most common cause of death in the United States\"\n\n# Initialize input messages with the user's query.\ninput_messages = [{\"role\": \"user\", \"content\": item}]\nprint(\"\\nðŸŒŸ--- Processing Query ---ðŸŒŸ\")\nprint(f\"ðŸ” **User Query:** {item}\")\n    \n    # Call the Responses API with tools enabled and allow parallel tool calls.\nprint(\"\\nðŸ”§ **Calling Responses API with Tools Enabled\")\nprint(\"\\nðŸ•µï¸â€â™‚ï¸ **Step 1: Web Search Call**\")\nprint(\"   - Initiating web search to gather initial information.\")\nprint(\"\\nðŸ“š **Step 2: Pinecone Search Call**\")\nprint(\"   - Querying Pinecone to find relevant examples from the internal knowledge base.\")\n    \nresponse = client.responses.create(\n        model=\"gpt-4o\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Every time it's prompted with a question, first call the web search tool for results, then call `PineconeSearchDocuments` to find real examples in the internal knowledge base.\"},\n            {\"role\": \"user\", \"content\": item}\n        ],\n        tools=tools,\n        parallel_tool_calls=True\n    )\n    \n# Print the initial response output.\nprint(\"input_messages\", input_messages)\n\nprint(\"\\nâœ¨ **Initial Response Output:**\")\nprint(response.output)\n\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to JSONL for OpenAI Fine-tuning in Python\nDESCRIPTION: Defines a function `dataframe_to_jsonl` that converts a DataFrame (specifically the 'few_shot_prompt' column containing lists of message dictionaries) into the JSON Lines (JSONL) format required by OpenAI for fine-tuning. It iterates through each row, serializes the 'messages' list into a JSON string for each line, joins these lines with newlines, and writes the resulting string to a file named 'local_cache/100_train_few_shot.jsonl'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport json # Assuming this import is needed\n\n# Prepare the OpenAI File format i.e. JSONL from train_sample\ndef dataframe_to_jsonl(df):\n    def create_jsonl_entry(row):\n        messages = row[\"few_shot_prompt\"]\n        return json.dumps({\"messages\": messages})\n\n    jsonl_output = df.progress_apply(create_jsonl_entry, axis=1)\n    return \"\\n\".join(jsonl_output)\n\n# train_sample is assumed to be a DataFrame with the 'few_shot_prompt' column\nwith open(\"local_cache/100_train_few_shot.jsonl\", \"w\") as f:\n    f.write(dataframe_to_jsonl(train_sample))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Run Results Node.js\nDESCRIPTION: This code performs similar function as the python code, but in Node.js environment, it creates a run for a given thread and assistant, retrieves messages, extracts text content, identifies and processes annotations (including file citations), and formats the output to include references to cited files. It uses the OpenAI Node.js SDK to make API calls, process the results, replace annotation placeholders and extract corresponding file names for citations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_12\n\nLANGUAGE: node.js\nCODE:\n```\nconst run = await openai.beta.threads.runs.createAndPoll(thread.id, {\n  assistant_id: assistant.id,\n});\n \nconst messages = await openai.beta.threads.messages.list(thread.id, {\n  run_id: run.id,\n});\n \nconst message = messages.data.pop()!;\nif (message.content[0].type === \"text\") {\n  const { text } = message.content[0];\n  const { annotations } = text;\n  const citations: string[] = [];\n\n  let index = 0;\n  for (let annotation of annotations) {\n    text.value = text.value.replace(annotation.text, \"[\" + index + \"]\");\n    const { file_citation } = annotation;\n    if (file_citation) {\n      const citedFile = await openai.files.retrieve(file_citation.file_id);\n      citations.push(\"[\" + index + \"]\" + citedFile.filename);\n    }\n    index++;\n  }\n\n  console.log(text.value);\n  console.log(citations.join(\"\\n\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Customer Interaction Samples with GPT-4o in Python\nDESCRIPTION: This code sets up prompts to generate sample interactions between customers and support agents based on the previously generated policies. The output includes whether the assistant follows the policy correctly and provides JSON-formatted conversation data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsystem_input_prompt = \"\"\"\"\nYou are a helpful assistant that can generate fictional interactions between a support assistant and a customer user. You will be given a set of policy instructions that the support agent is instructed to follow.\n\nBased on the instructions, you must generate a relevant single-turn or multi-turn interaction between the assistant and the user. It should average between 1-3 turns total.\n\nFor a given set of instructions, generate an example conversation that where the assistant either does or does not follow the instructions properly. In the assistant's responses, have it give a combination of single sentence and multi-sentence responses.\n\nThe output must be in a json format with the following three parameters:\n - accurate: \n    - This should be a boolean True or False value that matches whether or not the final assistant message accurately follows the policy instructions\n - kb_article:\n    - This should be the entire policy instruction that is passed in from the user\n - chat_history: \n    - This should contain the entire conversation history except for the final assistant message. \n    - This should be in a format of an array of jsons where each json contains two parameters: role, and content. \n    - Role should be set to either 'user' to represent the customer, or 'assistant' to represent the customer support assistant. \n    - Content should contain the message from the appropriate role.\n    - The final message in the chat history should always come from the user. The assistant response in the following parameter will be a response to this use message.\n - assistant_response: \n    - This should contain the final response from the assistant. This is what we will evaluate to determine whether or not it is accurately following the policy.\n\"\"\"\n\nuser_example_1 = \"\"\"\"\nHere are the policy instructions:\nRETURN POLICY\n\n1. ** Ask the customer why they want the order replaced **\n    - Categorize their issue into one of the following buckets:\n        - damaged: They received the product in a damaged state\n        - satisfaction: The customer is not satisfied with the item and does not like the product.\n        - unnecessary: They no longer need the item\n2a. **If return category is 'damaged'\n    - Ask customer for a picture of the damaged item\n    - If the item is indeed damaged, continue to step 3\n    - If the item is not damaged, notify the customer that this does not meet our requirements for return and they are not eligible for a refund\n    - Skip step 3 and go straight to step 4\n\n2b. **If return category is either 'satisfaction' or 'unnecessary'**\n    - Ask the customer if they can provide feedback on the quality of the item\n    - If the order was made within 30 days, notify them that they are eligible for a full refund\n    - If the order was made within 31-60 days, notify them that they are eligible for a partial refund of 50%\n    - If the order was made greater than 60 days ago, notify them that they are not eligible for a refund\n\n3. **If the customer is eligible for a return or refund**\n    - Ask the customer to confirm that they would like a return or refund\n    - Once they confirm, process their request\n\n4 **Provide additional support before closing out ticket**\n    - Ask the customer if there is anything else you can do to help them today.\n\"\"\"\n\nassistant_example_1 = \"\"\"\n{\n    \"accurate\": \"true\",\n    \"kb_article\": \"1. ** Ask the customer why they want the order replaced ** - Categorize their issue into one of the following buckets: - damaged: They received the product in a damaged state - satisfaction: The customer is not satisfied with the item and does not like the product. - unnecessary: They no longer need the item 2a. **If return category is 'damaged' - Ask customer for a picture of the damaged item - If the item is indeed damaged, continue to step 3 - If the item is not damaged, notify the customer that this does not meet our requirements for return and they are not eligible for a refund - Skip step 3 and go straight to step 4 2b. **If return category is either 'satisfaction' or 'unnecessary'** - Ask the customer if they can provide feedback on the quality of the item - If the order was made within 30 days, notify them that they are eligible for a full refund - If the order was made within 31-60 days, notify them that they are eligible for a partial refund of 50% - If the order was made greater than 60 days ago, notify them that they are not eligible for a refund 3. **If the customer is eligible for a return or refund** - Ask the customer to confirm that they would like a return or refund - Once they confirm, process their request 4 **Provide additional support before closing out ticket** - Ask the customer if there is anything else you can do to help them today.\",\n    \"chat_history\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"I would like to return this shirt\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Hi there, I'm happy to help with processing this return. Can you please provide an explanation for why you'd like to return this shirt?\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Yes, I am not satisfied with the design\"\n        }\n    ],\n    \"assistant_response\": {\n        \"role\": \"assistant\",\n        \"content\": \"I see. Because the shirt was ordered in the last 30 days, we can provide you with a full refund. Would you like me to process the refund?\"\n    }\n}\n\"\"\"\n\nuser_example_2 = \"\"\"\"\nHere are the policy instructions:\nRETURN POLICY\n\n1. ** Ask the customer why they want the order replaced **\n    - Categorize their issue into one of the following buckets:\n        - damaged: They received the product in a damaged state\n        - satisfaction: The customer is not satisfied with the item and does not like the product.\n        - unnecessary: They no longer need the item\n2a. **If return category is 'damaged'\n    - Ask customer for a picture of the damaged item\n    - If the item is indeed damaged, continue to step 3\n```\n\n----------------------------------------\n\nTITLE: Uploading Sports Headline Context Dataset and Fine-Tuning with OpenAI Python SDK\nDESCRIPTION: This Python snippet demonstrates uploading a structured sports headline JSONL dataset and initiating a fine-tuning job using OpenAI's SDK. Dependencies include the 'openai' package and a valid API key. The input dataset ('sports-context.jsonl') should conform to OpenAI's fine-tuning data format. The script creates a file resource, then starts the job targeting 'gpt-3.5-turbo'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nfile = client.files.create(\n  file=open(\"sports-context.jsonl\", \"rb\"),\n  purpose=\"fine-tune\"\n)\n\nclient.fine_tuning.jobs.create(\n  training_file=file.id,\n  model=\"gpt-3.5-turbo\"\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting OpenAI API Key for Environment Variables in Python\nDESCRIPTION: Exports your OpenAI API key as an environment variable named OPENAI_API_KEY using shell syntax in a Jupyter notebook cell. This is a prerequisite for both data vectorization and generative search, allowing Weaviate and OpenAI modules to authenticate requests. You must replace the placeholder value with your actual OpenAI API key before running this cell. No outputs are produced by this snippet, but subsequent operations rely on this environment variable being set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Export OpenAI API Key\n!export OPENAI_API_KEY=\"your key\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Fine Food Reviews Dataset in Python\nDESCRIPTION: Reads a pre-filtered CSV dataset of 1,000 fine food reviews containing review metadata and textual content. It selects relevant columns, removes rows with missing data, and combines the review title (Summary) and review body (Text) into a single string field labeled 'combined', prefixed for clarity. This preprocessing ensures uniform text input format for subsequent embedding extraction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# load & inspect dataset\ninput_datapath = \"data/fine_food_reviews_1k.csv\"  # to save space, we provide a pre-filtered dataset\ndf = pd.read_csv(input_datapath, index_col=0)\ndf = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\ndf = df.dropna()\ndf[\"combined\"] = (\n    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n)\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Displaying DataGrid Information After Conversion in Kangas\nDESCRIPTION: Shows the structure of the DataGrid after conversion, confirming that the embeddings column now has the proper Embedding data type.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndg.info()\n```\n\n----------------------------------------\n\nTITLE: Uploading Content Vectors to Pinecone Index\nDESCRIPTION: This snippet uploads the content vectors from the DataFrame to the Pinecone index in the 'content' namespace. It iterates through batches of the DataFrame and uses the `index.upsert()` method to upload the vectors with their corresponding IDs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Upsert content vectors in content namespace - this can take a few minutes\nprint(\"Uploading vectors to content namespace..\")\nfor batch_df in df_batcher(article_df):\n    index.upsert(vectors=zip(batch_df.vector_id, batch_df.content_vector), namespace='content')\n```\n\n----------------------------------------\n\nTITLE: Running Hybrid Search and Extracting Specific Mentions in Redis - Python\nDESCRIPTION: Performs a hybrid query on Redis using the `search_redis` function, filtering documents with 'Art' in their vector field and requiring the phrase 'Leonardo da Vinci' in the text. Then, extracts and displays the specific sentence mentioning 'Leonardo da Vinci' from the top result. Requires successful loading of text into Redis documents and appropriate index fields configured.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# run a hybrid query for articles about Art in the title vector and only include results with the phrase \"Leonardo da Vinci\" in the text\nresults = search_redis(redis_client,\n                       \"Art\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"text\", \"Leonardo da Vinci\")\n                       )\n\n# find specific mention of Leonardo da Vinci in the text that our full-text-search query returned\nmention = [sentence for sentence in results[0].text.split(\"\\n\") if \"Leonardo da Vinci\" in sentence][0]\nmention\n```\n\n----------------------------------------\n\nTITLE: Create and Stream Run with Assistants API in Python\nDESCRIPTION: This snippet demonstrates creating a custom event handler class inheriting from `AssistantEventHandler` to process different types of streaming events (text, tool calls). It then uses the OpenAI Python SDK's `stream` helper within a `with` context to create and stream an Assistants API run, attaching the custom event handler.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/overview-with-streaming.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n \n# First, we create a EventHandler class to define\n# how we want to handle the events in the response stream.\n \nclass EventHandler(AssistantEventHandler):    \n  @override\n  def on_text_created(self, text) -> None:\n    print(f\"\\nassistant > \", end=\"\", flush=True)\n      \n  @override\n  def on_text_delta(self, delta, snapshot):\n    print(delta.value, end=\"\", flush=True)\n      \n  def on_tool_call_created(self, tool_call):\n    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n  \n  def on_tool_call_delta(self, delta, snapshot):\n    if delta.type == 'code_interpreter':\n      if delta.code_interpreter.input:\n        print(delta.code_interpreter.input, end=\"\", flush=True)\n      if delta.code_interpreter.outputs:\n        print(f\"\\n\\noutput >\", flush=True)\n        for output in delta.code_interpreter.outputs:\n          if output.type == \"logs\":\n            print(f\"\\n{output.logs}\", flush=True)\n \n# Then, we use the `stream` SDK helper \n# with the `EventHandler` class to create the Run \n# and stream the response.\n \nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n  event_handler=EventHandler(),\n) as stream:\n  stream.until_done()\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Grader Prompt and Model Judge for Evals - Python\nDESCRIPTION: Creates prompts (for developer and user), and defines a grading configuration dict for LLM-based grading of push notification summaries using OpenAI's evals. The grader checks if summaries are 'correct' or 'incorrect' based on concise, quality criteria. Dependencies: openai for evals, a model capable of structured label outputs (e.g., o3-mini). Inputs: evaluated notification and summary. Outputs: model-computed label. Limitations: labels must match allowed set; prompt-calibration may be required.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nGRADER_DEVELOPER_PROMPT = \"\"\"\nLabel the following push notification summary as either correct or incorrect.\nThe push notification and the summary will be provided below.\nA good push notificiation summary is concise and snappy.\nIf it is good, then label it as correct, if not, then incorrect.\n\"\"\"\nGRADER_TEMPLATE_PROMPT = \"\"\"\nPush notifications: {{item.notifications}}\nSummary: {{sample.output_text}}\n\"\"\"\npush_notification_grader = {\n    \"name\": \"Push Notification Summary Grader\",\n    \"type\": \"label_model\",\n    \"model\": \"o3-mini\",\n    \"input\": [\n        {\n            \"role\": \"developer\",\n            \"content\": GRADER_DEVELOPER_PROMPT,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": GRADER_TEMPLATE_PROMPT,\n        },\n    ],\n    \"passing_labels\": [\"correct\"],\n    \"labels\": [\"correct\", \"incorrect\"],\n}\n```\n\n----------------------------------------\n\nTITLE: Define Custom GPT Instructions for Salesforce Service Cloud Interaction\nDESCRIPTION: Provides structured instructions for a custom GPT on how to interact with Salesforce Service Cloud. It defines the GPT's purpose (pulling case info, updating cases), instructs it to use specific actions (`getCaseDetailsFromNumber`, `updateCaseStatus`) based on user requests for case details or updates, and includes an example user interaction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n**Context**: Your purpose is to pull information from Service Cloud, and push updates to cases. A user is going to ask you a question and ask you to make updates.\n\n**Instructions**:\n1. When a user asks you to help them solve a case in Service Cloud, ask for the case number and pull the details for the case into the conversation using the getCaseDetailsFromNumber action.\n2. If the user asks you to update the case details, use the action updateCaseStatus.\n\n**Example**: \nUser: Help me solve case 00001104 in Service Cloud.\n```\n\n----------------------------------------\n\nTITLE: Applying Fine-Tuned OpenAI Model to Test Set for Predictions\nDESCRIPTION: This snippet applies the fine-tuned model to each message set in the test data, generating predictions stored in a new 'response' column, and extracts predicted class labels to 'predicted_class'. It uses the OpenAI Chat Completions API with 'temperature=0' for deterministic outputs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ntest_set['response'] = test_set.apply(lambda x: openai.chat.completions.create(model=fine_tuned_model, messages=x['messages'][:-1], temperature=0),axis=1)\ntest_set['predicted_class'] = test_set.apply(lambda x: x['response'].choices[0].message.content, axis=1)\n\ntest_set.head()\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Query Sequence - Solve Problem\nDESCRIPTION: This shows the first step in a sequence of queries to solve a problem without the student's solution, avoiding bias.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_15\n\nLANGUAGE: N/A\nCODE:\n```\nUSER: \n```\n\n----------------------------------------\n\nTITLE: Assistant Response Template Structure\nDESCRIPTION: The expected JSON structure for the assistant's response, indicating whether there is enough information in the context to provide a proper answer.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n\"enough_information_in_context\": \"True\"\n\"response\": \"...\"\n}\n```\n\n----------------------------------------\n\nTITLE: Chunking Text into Tokens with Python Iterator\nDESCRIPTION: Defines a generator function that splits a sequence of tokens into chunks of specified size, facilitating the process of dividing lengthy texts into manageable segments for individual embedding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom itertools import islice\n\ndef batched(iterable, n):\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --> ABC DEF G\n    if n < 1:\n        raise ValueError('n must be at least one')\n    it = iter(iterable)\n    while (batch := tuple(islice(it, n))):\n        yield batch\n```\n\n----------------------------------------\n\nTITLE: Summarizing Academic Paper from Query - Python\nDESCRIPTION: This function orchestrates the process of finding a relevant academic paper for a user query, reading its content, chunking the text, summarizing each chunk in parallel using `extract_chunk`, and performing a final overall summary of the collected chunk summaries.\n\nRequired Dependencies: `pandas` (as `pd`), `ast`, `tiktoken`, `concurrent.futures`, `tqdm`, OpenAI client (`client`), `GPT_MODEL`, `paper_dir_filepath`, and external functions `get_articles`, `read_pdf`, `strings_ranked_by_relatedness`. Depends on `create_chunks` and `extract_chunk`.\n\nParameters:\n- `query` (str): The user's search query used to find relevant papers.\n\nInputs: A user query string.\nOutputs: An OpenAI API response object containing the final collated summary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef summarize_text(query):\n    \"\"\"This function does the following:\n    - Reads in the arxiv_library.csv file in including the embeddings\n    - Finds the closest file to the user's query\n    - Scrapes the text out of the file and chunks it\n    - Summarizes each chunk in parallel\n    - Does one final summary and returns this to the user\"\"\"\n\n    # A prompt to dictate how the recursive summarizations should approach the input paper\n    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n\n    # If the library is empty (no searches have been performed yet), we perform one and download the results\n    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n    if len(library_df) == 0:\n        print(\"No papers searched yet, downloading first.\")\n        get_articles(query)\n        print(\"Papers downloaded, continuing\")\n        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n    else:\n        print(\"Existing papers found... Articles:\", len(library_df))\n    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n    print(\"Chunking text from paper\")\n    pdf_text = read_pdf(strings[0])\n\n    # Initialise tokenizer\n    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n    results = \"\"\n\n    # Chunk up the document into 1500 token chunks\n    chunks = create_chunks(pdf_text, 1500, tokenizer)\n    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n    print(\"Summarizing each chunk of text\")\n\n    # Parallel process the summaries\n    with concurrent.futures.ThreadPoolExecutor(\n        max_workers=len(text_chunks)\n    ) as executor:\n        futures = [\n            executor.submit(extract_chunk, chunk, summary_prompt)\n            for chunk in text_chunks\n        ]\n        with tqdm(total=len(text_chunks)) as pbar:\n            for _ in concurrent.futures.as_completed(futures):\n                pbar.update(1)\n        for future in futures:\n            data = future.result()\n            results += data\n\n    # Final summary\n    print(\"Summarizing into overall summary\")\n    response = client.chat.completions.create(\n        model=GPT_MODEL,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\\n                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\\n                        User query: {query}\\n                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\\n                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n            }\n        ],\n        temperature=0,\n    )\n    return response\n```\n\n----------------------------------------\n\nTITLE: Simple Vector Search for Men's Blue Jeans\nDESCRIPTION: A basic vector search query example that searches for \"man blue jeans\" using the previously defined search_redis function. It returns the top 10 results based on vector similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Execute a simple vector search in Redis\nresults = search_redis(redis_client, 'man blue jeans', k=10)\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI Model with Context\nDESCRIPTION: Sends a prompt with a \"don't know\" fallback to the OpenAI model and prints the response.  This aims to mitigate confident-sounding but incorrect responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt =\"Is Sam Bankman-Fried's company, FTX, considered a well-managed company?  If you don't know for certain, say unknown.\"\nresponse = get_completion(prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Implementing Summary Evaluation Function with Pydantic Parsing\nDESCRIPTION: Creates a function to evaluate both simple and complex summaries using GPT-4o as the judge, with results parsed into a structured Pydantic ScoreCard model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_summaries(row):\n    simple_messages = [{\"role\": \"user\", \"content\": evaluation_prompt.format(original_article=row[\"content\"], summary=row['simple_summary'])}]\n    complex_messages = [{\"role\": \"user\", \"content\": evaluation_prompt.format(original_article=row[\"content\"], summary=row['complex_summary'])}]\n    \n    simple_summary = client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=simple_messages,\n        response_format=ScoreCard)\n    simple_summary = simple_summary.choices[0].message.parsed\n    \n    complex_summary = client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=complex_messages,\n        response_format=ScoreCard)\n    complex_summary = complex_summary.choices[0].message.parsed\n    \n    return simple_summary, complex_summary\n\n# Add new columns to the dataframe for storing evaluations\ndf['simple_evaluation'] = None\ndf['complex_evaluation'] = None\n```\n\n----------------------------------------\n\nTITLE: Initializing API Clients and Constants for Cross-Encoder Reranking - Python\nDESCRIPTION: Imports all required Python modules, sets up the OpenAI client with the API key (read from an environment variable or a placeholder), and declares the GPT model name as a constant. Required dependencies include openai, arxiv, pandas, tenacity, and tiktoken. This snippet must be executed before running any further logic that interacts with search or the language model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport arxiv\nfrom math import exp\nimport openai\nimport os\nimport pandas as pd\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport tiktoken\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\nOPENAI_MODEL = \"gpt-4\"\n```\n\n----------------------------------------\n\nTITLE: Displaying and Annotating the First Few Results\nDESCRIPTION: This snippet iterates over the first five results, extracting the 'custom_id' to identify the task index, retrieving the response content, and displaying the associated image along with the caption. It uses 'Image' and 'display' functions for visualization and prints the caption. Useful for quick validation of batch processing output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\n# Reading only the first results\nfor res in results[:5]:\n    task_id = res['custom_id']\n    # Getting index from task id\n    index = task_id.split('-')[-1]\n    result = res['response']['body']['choices'][0]['message']['content']\n    item = df.iloc[int(index)]\n    img_url = item['primary_image']\n    img = Image(url=img_url)\n    display(img)\n    print(f\"CAPTION: {result}\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Define OpenAPI Schema for BigQuery GPT Action (YAML)\nDESCRIPTION: Defines the OpenAPI 3.1.0 schema for the BigQuery GPT Action. It specifies the API server URL (`https://bigquery.googleapis.com/bigquery/v2`), the path (`/projects/{projectId}/queries`) for executing queries via POST, and the `runQuery` operation. The operation requires a `projectId` path parameter and a JSON request body containing the `query` string and an optional `useLegacySql` boolean (defaulting to false). It details the expected successful response (200) structure, including schema, job reference, rows, and potential pagination tokens, as well as standard error responses (400, 401, 403, 404, 500).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_bigquery.ipynb#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: BigQuery API\n  description: API for querying a BigQuery table.\n  version: 1.0.0\nservers:\n  - url: https://bigquery.googleapis.com/bigquery/v2\n    description: Google BigQuery API server\npaths:\n  /projects/{projectId}/queries:\n    post:\n      operationId: runQuery\n      summary: Executes a query on a specified BigQuery table.\n      description: Submits a query to BigQuery and returns the results.\n      x-openai-isConsequential: false\n      parameters:\n        - name: projectId\n          in: path\n          required: true\n          description: The ID of the Google Cloud project.\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                query:\n                  type: string\n                  description: The SQL query string.\n                useLegacySql:\n                  type: boolean\n                  description: Whether to use legacy SQL.\n                  default: false\n      responses:\n        '200':\n          description: Successful query execution.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  kind:\n                    type: string\n                    example: \"bigquery#queryResponse\"\n                  schema:\n                    type: object\n                    description: The schema of the results.\n                  jobReference:\n                    type: object\n                    properties:\n                      projectId:\n                        type: string\n                      jobId:\n                        type: string\n                  rows:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        f:\n                          type: array\n                          items:\n                            type: object\n                            properties:\n                              v:\n                                type: string\n                  totalRows:\n                    type: string\n                    description: Total number of rows in the query result.\n                  pageToken:\n                    type: string\n                    description: Token for pagination of query results.\n        '400':\n          description: Bad request. The request was invalid.\n        '401':\n          description: Unauthorized. Authentication is required.\n        '403':\n          description: Forbidden. The request is not allowed.\n        '404':\n          description: Not found. The specified resource was not found.\n        '500':\n          description: Internal server error. An error occurred while processing the request.\n```\n\n----------------------------------------\n\nTITLE: Obtaining Kusto Access Token (mssparkutils)\nDESCRIPTION: Configures Kusto connection options based on the defined variables and obtains an access token using `mssparkutils.credentials.getToken`. This is common in Synapse/Fabric notebooks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nkustoOptions = {\"kustoCluster\": KUSTO_CLUSTER, \"kustoDatabase\" :KUSTO_DATABASE, \"kustoTable\" : KUSTO_TABLE }\n\n# Replace the auth method based on your desired authentication mechanism  - https://github.com/Azure/azure-kusto-spark/blob/master/docs/Authentication.md\naccess_token=mssparkutils.credentials.getToken(kustoOptions[\"kustoCluster\"])\n```\n\n----------------------------------------\n\nTITLE: Answering Questions from Reference Text (Prompt)\nDESCRIPTION: Demonstrates instructing the model to answer a question using only information found within provided reference articles, delimited by triple quotes. If the answer is not present, the model is told to respond with a specific phrase: \"I could not find an answer.\"\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_8\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \"I could not find an answer.\"\n\nUSER: \n\nQuestion: \n```\n\n----------------------------------------\n\nTITLE: Connecting to Redis Server Using Redis-Py Client in Python\nDESCRIPTION: Establishes a Redis client connection using the Redis URL and the redis-py 'from_url' helper. It verifies the connection by sending a PING command, raising an exception if the server is unreachable. This client is used for subsequent interactions with Redis, including JSON and search operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom redis import from_url\n\nREDIS_URL = 'redis://localhost:6379'\nclient = from_url(REDIS_URL)\nclient.ping()\n```\n\n----------------------------------------\n\nTITLE: Querying Azure AI Search Index with Python\nDESCRIPTION: This code demonstrates how to perform both vector similarity search and hybrid search on an Azure AI Search index using the Python SDK. It initializes a `SearchClient`, constructs a `VectorizedQuery` with the user's query and precomputed embeddings, and executes the search. It supports both pure vector search (by setting `search_text` to `None`) and hybrid search, combining vector similarity with keyword-based search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What model should I use to embed?\"\n# Note: we'll have the GPT choose the category automatically once we put it in ChatGPT\ncategory =\"models\"\n\nsearch_client = SearchClient(search_service_endpoint, index_name, AzureKeyCredential(search_service_api_key))\nvector_query = VectorizedQuery(vector=generate_embeddings(query, embeddings_model), k_nearest_neighbors=3, fields=\"content_vector\")\n  \nresults = search_client.search(  \n    search_text=None, # Pass in None if you want to use pure vector search, and `query` if you want to use hybrid search\n    vector_queries= [vector_query], \n    select=[\"title\", \"text\"],\n    filter=f\"category eq '{category}'\" \n)\n\nfor result in results:  \n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Local Qdrant Instance\nDESCRIPTION: This Python code establishes a connection to a running Qdrant instance using the `qdrant-client` library. It initializes `QdrantClient` specifying the host ('localhost') and preferring gRPC over REST for communication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport qdrant_client\n\nclient = qdrant_client.QdrantClient(\n    host=\"localhost\",\n    prefer_grpc=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Prompt Template with Conversation History in LangChain (Python)\nDESCRIPTION: Defines a string template for the agent's prompt that incorporates a conversational history variable, allowing context-aware responses. The template guides the agent through stepwise reasoning and structured output, and is meant to be used with tools and action formatting as per LangChain's agent conventions. The provided prompt includes variables for tools, tool names, user questions, previous conversation turns (history), and agent scratchpad for intermediate steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntemplate_with_history = \"\"\"You are SearchGPT, a professional search engine who provides informative answers to users. Answer the following questions as best you can. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! Remember to give detailed, informative answers\n\nPrevious conversation history:\n{history}\n\nNew question: {input}\n{agent_scratchpad}\"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Polling for Image File Creation in Assistant Response (Python)\nDESCRIPTION: Implements a polling mechanism that repeatedly checks the thread messages for the presence of an image file generated by the Assistant. It uses a `while` loop with `time.sleep` to wait between checks, breaking the loop once an image file object is found in the latest message, indicating the plot has been created.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nwhile True:\n    messages = client.beta.threads.messages.list(thread_id=thread.id)\n    try:\n        #See if image has been created\n        messages.data[0].content[0].image_file\n        #Sleep to make sure run has completed\n        time.sleep(5)\n        print('Plot created!')\n        break\n    except:\n        time.sleep(10)\n        print('Assistant still working...')\n\n```\n\n----------------------------------------\n\nTITLE: Loading the recipe dataset for fine-tuning\nDESCRIPTION: Loads the RecipesNLG dataset that has been filtered to only contain documents from cookbooks.com and displays the first few rows.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Read in the dataset we'll use for this task.\n# This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com\nrecipe_df = pd.read_csv(\"data/cookbook_recipes_nlg_10k.csv\")\n\nrecipe_df.head()\n```\n\n----------------------------------------\n\nTITLE: Combining Text Chunks by Token Count in Python\nDESCRIPTION: This Python function merges multiple text chunks into larger combined blocks without exceeding a maximum token count limit, optionally adding headers and ellipsis indicators for dropped content. It returns the combined text blocks, their original chunk indices, and the count of chunks skipped due to overflow warnings. The function depends on a tokenization method (`tokenize`) to measure token length accurately, uses a delimiter to join chunks, and manages internal state to efficiently build output. It is intended for preparing text segments for processing in token-limited environments such as language models, ensuring that combined blocks comply with token restrictions while tracking dropped segments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef combine_chunks_with_no_minimum(\n        chunks: List[str],\n        max_tokens: int,\n        chunk_delimiter=\"\\n\\n\",\n        header: Optional[str] = None,\n        add_ellipsis_for_overflow=False,\n) -> Tuple[List[str], List[int]]:\n    dropped_chunk_count = 0\n    output = []  # list to hold the final combined chunks\n    output_indices = []  # list to hold the indices of the final combined chunks\n    candidate = (\n        [] if header is None else [header]\n    )  # list to hold the current combined chunk candidate\n    candidate_indices = []\n    for chunk_i, chunk in enumerate(chunks):\n        chunk_with_header = [chunk] if header is None else [header, chunk]\n        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n            print(f\"warning: chunk overflow\")\n            if (\n                    add_ellipsis_for_overflow\n                    and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n            ):\n                candidate.append(\"...\")\n                dropped_chunk_count += 1\n            continue  # this case would break downstream assumptions\n        # estimate token count with the current chunk added\n        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n        # If the token count exceeds max_tokens, add the current candidate to output and start a new candidate\n        if extended_candidate_token_count > max_tokens:\n            output.append(chunk_delimiter.join(candidate))\n            output_indices.append(candidate_indices)\n            candidate = chunk_with_header  # re-initialize candidate\n            candidate_indices = [chunk_i]\n        # otherwise keep extending the candidate\n        else:\n            candidate.append(chunk)\n            candidate_indices.append(chunk_i)\n    # add the remaining candidate to output if it's not empty\n    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n        output.append(chunk_delimiter.join(candidate))\n        output_indices.append(candidate_indices)\n    return output, output_indices, dropped_chunk_count\n```\n\n----------------------------------------\n\nTITLE: Updating Assistant with Function (Python)\nDESCRIPTION: Updates the Assistant to add the Code Interpreter tool, the File Search tool, and the new Function Tool. Requires the function_json to be defined and the MATH_ASSISTANT_ID variable set. Calls show_json to display the assistant's updated configuration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.update(\n    MATH_ASSISTANT_ID,\n    tools=[\n        {\"type\": \"code_interpreter\"},\n        {\"type\": \"file_search\"},\n        {\"type\": \"function\", \"function\": function_json},\n    ],\n)\nshow_json(assistant)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Individual Transcriptions (Python)\nDESCRIPTION: Combines the list of individual transcript strings stored in the `transcriptions` list into a single string. The `' '.join()` method is used to concatenate the elements, placing a single space between the transcript of each segment, creating the `full_transcript`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate the transcriptions\nfull_transcript = ' '.join(transcriptions)\n```\n\n----------------------------------------\n\nTITLE: Defining Box API Endpoints and Schemas - OpenAPI/JSON\nDESCRIPTION: This OpenAPI specification defines the structure for interacting with the Box.com API from a Custom GPT. It includes definitions for endpoints like listing folder contents, getting file details, fetching user and admin events, searching, and retrieving metadata templates and file metadata. It specifies the required parameters and expected JSON response formats for these operations. This spec allows the GPT platform to understand and call the Box API on behalf of the user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb#_snippet_1\n\nLANGUAGE: OpenAPI/JSON\nCODE:\n```\n{\n  \"openapi\": \"3.1.0\",\n  \"info\": {\n    \"title\": \"Box.com API\",\n    \"description\": \"API for Box.com services\",\n    \"version\": \"v1.0.0\"\n  },\n  \"servers\": [\n    {\n      \"url\": \"https://api.box.com/2.0\"\n    }\n  ],\n  \"paths\": {\n    \"/folders/{folder_id}\": {\n      \"get\": {\n        \"summary\": \"Get Folder Items\",\n        \"operationId\": \"getFolderItems\",\n        \"parameters\": [\n          {\n            \"name\": \"folder_id\",\n            \"in\": \"path\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The ID of the folder\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"A list of items in the folder\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/FolderItems\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:folders\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/files/{file_id}\": {\n      \"get\": {\n        \"summary\": \"Get File Information\",\n        \"operationId\": \"getFileInfo\",\n        \"parameters\": [\n          {\n            \"name\": \"file_id\",\n            \"in\": \"path\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The ID of the file\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"File information\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/FileInfo\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:files\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/folders\": {\n      \"get\": {\n        \"summary\": \"List All Folders\",\n        \"operationId\": \"listAllFolders\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"A list of all folders\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/FoldersList\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:folders\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/events\": {\n      \"get\": {\n        \"summary\": \"Get User Events\",\n        \"operationId\": \"getUserEvents\",\n        \"parameters\": [\n          {\n            \"name\": \"stream_type\",\n            \"in\": \"query\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The type of stream\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"User events\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/UserEvents\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:events\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/admin_events\": {\n      \"get\": {\n        \"summary\": \"Get Admin Events\",\n        \"operationId\": \"getAdminEvents\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Admin events\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/AdminEvents\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:events\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/search\": {\n      \"get\": {\n        \"summary\": \"Search\",\n        \"operationId\": \"search\",\n        \"parameters\": [\n          {\n            \"name\": \"query\",\n            \"in\": \"query\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"Search query\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Search results\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/SearchResults\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"search:items\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/metadata_templates\": {\n      \"get\": {\n        \"summary\": \"Get Metadata Templates\",\n        \"operationId\": \"getMetadataTemplates\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Metadata templates\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/MetadataTemplates\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:metadata_templates\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/metadata_templates/enterprise\": {\n      \"get\": {\n        \"summary\": \"Get Enterprise Metadata Templates\",\n        \"operationId\": \"getEnterpriseMetadataTemplates\",\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Enterprise metadata templates\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/MetadataTemplates\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:metadata_templates\"\n            ]\n          }\n        ]\n      }\n    },\n    \"/files/{file_id}/metadata\": {\n      \"get\": {\n        \"summary\": \"Get All Metadata for a File\",\n        \"operationId\": \"getAllMetadataForFile\",\n        \"parameters\": [\n          {\n            \"name\": \"file_id\",\n            \"in\": \"path\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            },\n            \"description\": \"The ID of the file\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"All metadata instances for the file\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/MetadataInstances\"\n                }\n              }\n            }\n          }\n        },\n        \"security\": [\n          {\n            \"OAuth2\": [\n              \"read:metadata\"\n            ]\n          }\n        ]\n      }\n    }\n  },\n  \"components\": {\n    \"schemas\": {\n      \"FolderItems\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"total_count\": {\n            \"type\": \"integer\",\n            \"description\": \"The total number of items in the folder\"\n          },\n          \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"type\": {\n                  \"type\": \"string\",\n                  \"description\": \"The type of the item (e.g., file, folder)\"\n                },\n                \"id\": {\n                  \"type\": \"string\",\n                  \"description\": \"The ID of the item\"\n                },\n                \"name\": {\n                  \"type\": \"string\",\n                  \"description\": \"The name of the item\"\n                }\n              }\n            }\n          }\n        }\n      },\n      \"FileInfo\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\n            \"type\": \"string\",\n            \"description\": \"The ID of the file\"\n          },\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the file\"\n          },\n          \"size\": {\n            \"type\": \"integer\",\n            \"description\": \"The size of the file in bytes\"\n          },\n          \"created_at\": {\n            \"type\": \"string\",\n            \"format\": \"date-time\",\n            \"description\": \"The creation time of the file\"\n          },\n          \"modified_at\": {\n            \"type\": \"string\",\n            \"format\": \"date-time\",\n            \"description\": \"The last modification time of the file\"\n          }\n        }\n      },\n      \"FoldersList\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"id\": {\n              \"type\": \"string\",\n              \"description\": \"The ID of the folder\"\n            },\n            \"name\": {\n              \"type\": \"string\",\n              \"description\": \"The name of the folder\"\n            }\n          }\n        }\n      },\n      \"UserEvents\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"event_id\": {\n                  \"type\": \"string\",\n                  \"description\": \"The ID of the event\"\n                },\n                \"event_type\": {\n                  \"type\": \"string\",\n                  \"description\": \"The type of the event\"\n                },\n                \"created_at\": {\n                  \"type\": \"string\",\n                  \"format\": \"date-time\",\n                  \"description\": \"The time the event occurred\"\n                }\n              }\n            }\n          }\n        }\n      },\n      \"AdminEvents\": {\n        \"type\": \"object\",\n        \"properties\": {\n\n```\n\n----------------------------------------\n\nTITLE: Performing Content-Based Similarity Search\nDESCRIPTION: Performs a similarity search using the content vector to find articles related to \"Famous battles in Scottish history\" and displays the results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# This time we'll query using content vector\nquery_results = query_polardb(\"Famous battles in Scottish history\", \"Articles\", \"content_vector\")\nfor i, result in enumerate(query_results):\n    print(f\"{i + 1}. {result[2]} (Score: {round(1 - result[3], 3)})\")\n```\n\n----------------------------------------\n\nTITLE: Writing Spark DataFrame to Kusto Table (Spark/Kusto)\nDESCRIPTION: Writes the data from the Spark DataFrame to the specified Kusto table using the Spark-to-Kusto connector. It configures connection options, authentication, and uses 'CreateIfNotExist' and 'Append' modes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Write data to a Kusto table\nsparkDF.write. \\\nformat(\"com.microsoft.kusto.spark.synapse.datasource\"). \\\noption(\"kustoCluster\",kustoOptions[\"kustoCluster\"]). \\\noption(\"kustoDatabase\",kustoOptions[\"kustoDatabase\"]). \\\noption(\"kustoTable\", kustoOptions[\"kustoTable\"]). \\\noption(\"accessToken\", access_token). \\\noption(\"tableCreateOptions\", \"CreateIfNotExist\").\\\nmode(\"Append\"). \\\nsave()\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAPI Spec for GPT Action - Python\nDESCRIPTION: This Python snippet constructs an OpenAPI 3.1.0 specification as a multiline string. It defines the structure of a vector similarity search API endpoint hosted on Azure Functions, including request body parameters, response schemas, and server URL using the deployed `app_name`. The generated spec is intended for use in configuring a GPT Action.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nspec = f\"\"\"\nopenapi: 3.1.0\ninfo:\n  title: Vector Similarity Search API\n  description: API for performing vector similarity search.\n  version: 1.0.0\nservers:\n  - url: https://{app_name}.azurewebsites.net/api\n    description: Main (production) server\npaths:\n  /vector_similarity_search:\n    post:\n      operationId: vectorSimilaritySearch\n      summary: Perform a vector similarity search.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                search_service_endpoint:\n                  type: string\n                  description: The endpoint of the search service.\n                index_name:\n                  type: string\n                  description: The name of the search index.\n                query:\n                  type: string\n                  description: The search query.\n                k_nearest_neighbors:\n                  type: integer\n                  description: The number of nearest neighbors to return.\n                search_column:\n                  type: string\n                  description: The name of the search column.\n                use_hybrid_query:\n                  type: boolean\n                  description: Whether to use a hybrid query.\n                category:\n                  type: string\n                  description: category to filter.\n              required:\n                - search_service_endpoint\n                - index_name\n                - query\n                - k_nearest_neighbors\n                - search_column\n                - use_hybrid_query\n      responses:\n        '200':\n          description: A successful response with the search results.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  results:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        id:\n                          type: string\n                          description: The identifier of the result item.\n                        score:\n                          type: number\n                          description: The similarity score of the result item.\n                        content:\n                          type: object\n                          description: The content of the result item.\n        '400':\n          description: Bad request due to missing or invalid parameters.\n        '500':\n          description: Internal server error.\n\"\"\"\npyperclip.copy(spec)\nprint(\"OpenAPI spec copied to clipboard\")\nprint(spec)\n```\n\n----------------------------------------\n\nTITLE: Preparing test messages for inference\nDESCRIPTION: Creates a test example from the dataset to evaluate the fine-tuned model's performance on ingredient extraction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntest_df = recipe_df.loc[201:300]\ntest_row = test_df.iloc[0]\ntest_messages = []\ntest_messages.append({\"role\": \"system\", \"content\": system_message})\nuser_message = create_user_message(test_row)\ntest_messages.append({\"role\": \"user\", \"content\": user_message})\n\npprint(test_messages)\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries in Python\nDESCRIPTION: Imports required Python libraries: 'os' for environment variables (API key), 'typing' for type hinting, 'OpenAI' for interacting with the OpenAI API, 'tiktoken' for tokenizing text according to OpenAI model specifications, and 'tqdm' for progress bars.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import List, Tuple, Optional\nfrom openai import OpenAI\nimport tiktoken\nfrom tqdm import tqdm\n```\n\n----------------------------------------\n\nTITLE: Defining the `apply_patch` Tool Structure (Python/JSON)\nDESCRIPTION: Defines the structure for the `apply_patch` tool using a Python dictionary, suitable for integration with OpenAI's function calling feature. It incorporates the detailed description from `APPLY_PATCH_TOOL_DESC` and specifies a single required string parameter `input`, which should contain the `apply_patch` command and the patch content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nAPPLY_PATCH_TOOL = {\n    \"name\": \"apply_patch\",\n    \"description\": APPLY_PATCH_TOOL_DESC,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"input\": {\n                \"type\": \"string\",\n                \"description\": \" The apply_patch command that you wish to execute.\",\n            }\n        },\n        \"required\": [\"input\"],\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Generate SQL using OpenAI API (Python)\nDESCRIPTION: Defines the system prompt instructing the LLM on the desired task and format, then defines a function `get_response` to call the OpenAI chat completions API. The function sends the system and user messages, specifying the model and using the `response_format` parameter with the defined `LLMResponse` Pydantic model to encourage the LLM to produce output conforming to that schema. It then demonstrates calling this function with a question from the dataset and prints the original question and the generated LLM response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nsystem_prompt = \"\"\"Translate this natural language request into a JSON\nobject containing two SQL queries. The first query should be a CREATE \ntatement for a table answering the user's request, while the second\nshould be a SELECT query answering their question.\"\"\"\n\n# Sending the message array to GPT, requesting a response (ensure that you\n# have API key loaded to Env for this step)\nclient = OpenAI()\n\ndef get_response(system_prompt, user_message, model=GPT_MODEL):\n    messages = []\n    messages.append({\"role\": \"system\", \"content\": system_prompt})\n    messages.append({\"role\": \"user\", \"content\": user_message})\n\n    response = client.beta.chat.completions.parse(\n        model=GPT_MODEL,\n        messages=messages,\n        response_format=LLMResponse,\n    )\n    return response.choices[0].message.content\n\nquestion = sql_df.iloc[0]['question']\ncontent = get_response(system_prompt, question)\nprint(\"Question:\", question)\nprint(\"Answer:\", content)\n```\n\n----------------------------------------\n\nTITLE: Implementing EventHandler for Streaming Tool Outputs in Node.js\nDESCRIPTION: A Node.js implementation of an EventHandler class that extends EventEmitter to handle streaming responses from the OpenAI Assistant API, process weather-related tool calls, and submit all tool outputs simultaneously using the submitToolOutputsStream helper.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling-run-example--streaming.txt#_snippet_1\n\nLANGUAGE: node.js\nCODE:\n```\nclass EventHandler extends EventEmitter {\n  constructor(client) {\n    super();\n    this.client = client;\n  }\n\n  async onEvent(event) {\n    try {\n      console.log(event);\n      // Retrieve events that are denoted with 'requires_action'\n      // since these will have our tool_calls\n      if (event.event === \"thread.run.requires_action\") {\n        await this.handleRequiresAction(\n          event.data,\n          event.data.id,\n          event.data.thread_id,\n        );\n      }\n    } catch (error) {\n      console.error(\"Error handling event:\", error);\n    }\n  }\n\n  async handleRequiresAction(data, runId, threadId) {\n    try {\n      const toolOutputs =\n        data.required_action.submit_tool_outputs.tool_calls.map((toolCall) => {\n          if (toolCall.function.name === \"getCurrentTemperature\") {\n            return {\n              tool_call_id: toolCall.id,\n              output: \"57\",\n            };\n          } else if (toolCall.function.name === \"getRainProbability\") {\n            return {\n              tool_call_id: toolCall.id,\n              output: \"0.06\",\n            };\n          }\n        });\n      // Submit all the tool outputs at the same time\n      await this.submitToolOutputs(toolOutputs, runId, threadId);\n    } catch (error) {\n      console.error(\"Error processing required action:\", error);\n    }\n  }\n\n  async submitToolOutputs(toolOutputs, runId, threadId) {\n    try {\n      // Use the submitToolOutputsStream helper\n      const stream = this.client.beta.threads.runs.submitToolOutputsStream(\n        threadId,\n        runId,\n        { tool_outputs: toolOutputs },\n      );\n      for await (const event of stream) {\n        this.emit(\"event\", event);\n      }\n    } catch (error) {\n      console.error(\"Error submitting tool outputs:\", error);\n    }\n  }\n}\n\nconst eventHandler = new EventHandler(client);\neventHandler.on(\"event\", eventHandler.onEvent.bind(eventHandler));\n\nconst stream = await client.beta.threads.runs.stream(\n  threadId,\n  { assistant_id: assistantId },\n  eventHandler,\n);\n\nfor await (const event of stream) {\n  eventHandler.emit(\"event\", event);\n}\n```\n\n----------------------------------------\n\nTITLE: Defining ChatGPT Function Metadata for S3 Operations in Python\nDESCRIPTION: Defines a list of function schemas that describe the available Amazon S3 operations (listing buckets, listing objects, downloading files, uploading files, and searching files) for the GPT model. Each function includes a name, description, parameter properties with types and requirements, enabling ChatGPT to understand how to call these functions dynamically based on user input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfunctions = [\n    {   \n        \"type\": \"function\",\n        \"function\":{\n            \"name\": \"list_buckets\",\n            \"description\": \"List all available S3 buckets\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {}\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\":{\n            \"name\": \"list_objects\",\n            \"description\": \"List the objects or files inside a given S3 bucket\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"bucket\": {\"type\": \"string\", \"description\": \"The name of the S3 bucket\"},\n                    \"prefix\": {\"type\": \"string\", \"description\": \"The folder path in the S3 bucket\"},\n                },\n                \"required\": [\"bucket\"],\n            },\n        }\n    },\n    {   \n        \"type\": \"function\",\n        \"function\":{\n            \"name\": \"download_file\",\n            \"description\": \"Download a specific file from an S3 bucket to a local distribution folder.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"bucket\": {\"type\": \"string\", \"description\": \"The name of the S3 bucket\"},\n                    \"key\": {\"type\": \"string\", \"description\": \"The path to the file inside the bucket\"},\n                    \"directory\": {\"type\": \"string\", \"description\": \"The local destination directory to download the file, should be specificed by the user.\"},\n                },\n                \"required\": [\"bucket\", \"key\", \"directory\"],\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\":{\n            \"name\": \"upload_file\",\n            \"description\": \"Upload a file to an S3 bucket\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"source\": {\"type\": \"string\", \"description\": \"The local source path or remote URL\"},\n                    \"bucket\": {\"type\": \"string\", \"description\": \"The name of the S3 bucket\"},\n                    \"key\": {\"type\": \"string\", \"description\": \"The path to the file inside the bucket\"},\n                    \"is_remote_url\": {\"type\": \"boolean\", \"description\": \"Is the provided source a URL (True) or local path (False)\"},\n                },\n                \"required\": [\"source\", \"bucket\", \"key\", \"is_remote_url\"],\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\":{\n            \"name\": \"search_s3_objects\",\n            \"description\": \"Search for a specific file name inside an S3 bucket\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"search_name\": {\"type\": \"string\", \"description\": \"The name of the file you want to search for\"},\n                    \"bucket\": {\"type\": \"string\", \"description\": \"The name of the S3 bucket\"},\n                    \"prefix\": {\"type\": \"string\", \"description\": \"The folder path in the S3 bucket\"},\n                    \"exact_match\": {\"type\": \"boolean\", \"description\": \"Set exact_match to True if the search should match the exact file name. Set exact_match to False to compare part of the file name string (the file contains)\"}\n                },\n                \"required\": [\"search_name\"],\n            },\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Printing the Final Processed Transcript (Python)\nDESCRIPTION: Outputs the content of the `final_transcript` variable to the console. This shows the final result of the entire pipeline: the transcript after pre-processing (trimming, segmentation), transcription, and all post-processing steps (ASCII filtering, punctuation, financial term correction).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nprint(final_transcript)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAPI Schema for Google Calendar Integration in GPT Actions\nDESCRIPTION: This OpenAPI schema defines endpoints for interacting with Google Calendar, allowing users to list calendar events and create new events. It includes detailed parameter specifications, request/response schemas, and proper error handling for OAuth authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_google_calendar.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Google Calendar API\n  description: This API allows you to read and create events in a user's Google Calendar.\n  version: 1.0.0\nservers:\n  - url: https://www.googleapis.com/calendar/v3\n    description: Google Calendar API server\n\npaths:\n  /calendars/primary/events:\n    get:\n      summary: List events from the primary calendar\n      description: Retrieve a list of events from the user's primary Google Calendar.\n      operationId: listEvents\n      tags:\n        - Calendar\n      parameters:\n        - name: timeMin\n          in: query\n          description: The lower bound (inclusive) of the events to retrieve, in RFC3339 format.\n          required: false\n          schema:\n            type: string\n            format: date-time\n            example: \"2024-11-01T00:00:00Z\"\n        - name: timeMax\n          in: query\n          description: The upper bound (exclusive) of the events to retrieve, in RFC3339 format.\n          required: false\n          schema:\n            type: string\n            format: date-time\n            example: \"2024-12-01T00:00:00Z\"\n        - name: maxResults\n          in: query\n          description: The maximum number of events to return.\n          required: false\n          schema:\n            type: integer\n            default: 10\n        - name: singleEvents\n          in: query\n          description: Whether to expand recurring events into instances. Defaults to `false`.\n          required: false\n          schema:\n            type: boolean\n            default: true\n        - name: orderBy\n          in: query\n          description: The order of events. Can be \"startTime\" or \"updated\".\n          required: false\n          schema:\n            type: string\n            enum:\n              - startTime\n              - updated\n            default: startTime\n      responses:\n        '200':\n          description: A list of events\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  items:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        id:\n                          type: string\n                          description: The event ID\n                        summary:\n                          type: string\n                          description: The event summary (title)\n                        start:\n                          type: object\n                          properties:\n                            dateTime:\n                              type: string\n                              format: date-time\n                              description: The start time of the event\n                            date:\n                              type: string\n                              format: date\n                              description: The start date of the all-day event\n                        end:\n                          type: object\n                          properties:\n                            dateTime:\n                              type: string\n                              format: date-time\n                              description: The end time of the event\n                            date:\n                              type: string\n                              format: date\n                              description: The end date of the all-day event\n                        location:\n                          type: string\n                          description: The location of the event\n                        description:\n                          type: string\n                          description: A description of the event\n        '401':\n          description: Unauthorized access due to missing or invalid OAuth token\n        '400':\n          description: Bad request, invalid parameters\n\n    post:\n      summary: Create a new event on the primary calendar\n      description: Creates a new event on the user's primary Google Calendar.\n      operationId: createEvent\n      tags:\n        - Calendar\n      requestBody:\n        description: The event data to create.\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                summary:\n                  type: string\n                  description: The title of the event\n                  example: \"Team Meeting\"\n                location:\n                  type: string\n                  description: The location of the event\n                  example: \"Conference Room 1\"\n                description:\n                  type: string\n                  description: A detailed description of the event\n                  example: \"Discuss quarterly results\"\n                start:\n                  type: object\n                  properties:\n                    dateTime:\n                      type: string\n                      format: date-time\n                      description: Start time of the event\n                      example: \"2024-11-30T09:00:00Z\"\n                    timeZone:\n                      type: string\n                      description: Time zone of the event start\n                      example: \"UTC\"\n                end:\n                  type: object\n                  properties:\n                    dateTime:\n                      type: string\n                      format: date-time\n                      description: End time of the event\n                      example: \"2024-11-30T10:00:00Z\"\n                    timeZone:\n                      type: string\n                      description: Time zone of the event end\n                      example: \"UTC\"\n                attendees:\n                  type: array\n                  items:\n                    type: object\n                    properties:\n                      email:\n                        type: string\n                        description: The email address of an attendee\n                        example: \"attendee@example.com\"\n              required:\n                - summary\n                - start\n                - end\n      responses:\n        '201':\n          description: Event created successfully\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  id:\n                    type: string\n                    description: The ID of the created event\n                  summary:\n                    type: string\n                    description: The event summary (title)\n                  start:\n                    type: object\n                    properties:\n                      dateTime:\n                        type: string\n                        format: date-time\n                        description: The start time of the event\n                  end:\n                    type: object\n                    properties:\n                      dateTime:\n                        type: string\n                        format: date-time\n                        description: The end time of the event\n        '400':\n          description: Bad request, invalid event data\n        '401':\n          description: Unauthorized access due to missing or invalid OAuth token\n        '500':\n          description: Internal server error\n```\n\n----------------------------------------\n\nTITLE: Running Tests via Command Line\nDESCRIPTION: Illustrates the command used to run tests frequently during the development and debugging process. This specific command executes the `run_tests.py` script using `python3`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n!python3 run_tests.py\n```\n\n----------------------------------------\n\nTITLE: Creating Elasticsearch Index with Dense Vector Mapping in Python\nDESCRIPTION: Creates a new Elasticsearch index with appropriate mappings for dense_vector fields, text, and metadata fields. Uses explicit definition for field dimensions, similarity, and types. Assumes 'client' is a valid Elasticsearch client. Output is the created index used for semantic search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nindex_mapping= {\n    \"properties\": {\n      \"title_vector\": {\n          \"type\": \"dense_vector\",\n          \"dims\": 1536,\n          \"index\": \"true\",\n          \"similarity\": \"cosine\"\n      },\n      \"content_vector\": {\n          \"type\": \"dense_vector\",\n          \"dims\": 1536,\n          \"index\": \"true\",\n          \"similarity\": \"cosine\"\n      },\n      \"text\": {\"type\": \"text\"},\n      \"title\": {\"type\": \"text\"},\n      \"url\": { \"type\": \"keyword\"},\n      \"vector_id\": {\"type\": \"long\"}\n      \n    }\n}\n\nclient.indices.create(index=\"wikipedia_vector_index\", mappings=index_mapping)\n```\n\n----------------------------------------\n\nTITLE: Printing Structured Article Summaries in Python\nDESCRIPTION: Defines a function to print the fields of an ArticleSummary instance in a human-readable format, listing invented year, summary, inventors, detailed concepts, and the description to make structured data easily interpretable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef print_summary(summary):\n    print(f\"Invented year: {summary.invented_year}\\n\")\n    print(f\"Summary: {summary.summary}\\n\")\n    print(\"Inventors:\")\n    for i in summary.inventors:\n        print(f\"- {i}\")\n    print(\"\\nConcepts:\")\n    for c in summary.concepts:\n        print(f\"- {c.title}: {c.description}\")\n    print(f\"\\nDescription: {summary.description}\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Sample Result Document for Inspection - Python\nDESCRIPTION: Fetches the first element from the result_list for inspection or further processing. Expects result_list to contain at least one element. Output is a single dictionary of article metadata. No dependencies beyond previous snippet.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresult_list[0]\n```\n\n----------------------------------------\n\nTITLE: Generating a Database Schema Summary String in Python\nDESCRIPTION: This code composes a string summarizing the schema of a SQLite database by calling a previously defined utility. It takes the output of 'get_database_info(conn)', formats each table's name and columns, and creates a multiline string delimited by newlines. The expected input is a valid database connection; output is a schema overview as a string suitable for insertion into prompts or tool descriptions. Requires the 'get_database_info' function and a connected database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndatabase_schema_dict = get_database_info(conn)\ndatabase_schema_string = \"\\n\".join(\n    [\n        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n        for table in database_schema_dict\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Construct System Message to Set NER Task Context\nDESCRIPTION: Creates a prompt that configures the AI assistant to recognize predefined NER categories within input text, aiding in consistent entity detection as per specified label sets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef system_message(labels):\n    return f\"\"\"\nYou are an expert in Natural Language Processing. Your task is to identify common Named Entities (NER) in a given text.\nThe possible common Named Entities (NER) types are exclusively: ({\", \".join(labels)}).\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Printing the ASCII-Only Transcript (Python)\nDESCRIPTION: Outputs the content of the `ascii_transcript` variable to the console. This shows the state of the transcript after the non-ASCII character removal step.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprint(ascii_transcript)\n```\n\n----------------------------------------\n\nTITLE: Standard Non-Streaming Chat Completion Example\nDESCRIPTION: Example of a typical ChatCompletion API call where the entire response is returned at once. This demonstrates the standard approach without streaming.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example of an OpenAI ChatCompletion request\n# https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = client.chat.completions.create(\n    model='gpt-4o-mini',\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    temperature=0,\n)\n# calculate the time it took to receive the response\nresponse_time = time.time() - start_time\n\n# print the time delay and text received\nprint(f\"Full response received {response_time:.2f} seconds after request\")\nprint(f\"Full response received:\\n{response}\")\n```\n\n----------------------------------------\n\nTITLE: Executing Function Calls from OpenAI API Response - Python\nDESCRIPTION: This function wraps a call to the OpenAI Chat Completion API (`chat_completion_request`) and checks if the model's response includes a request to call a function. If a function call is detected, it delegates the execution to the `call_arxiv_function`.\n\nRequired Dependencies: Depends on `chat_completion_request` and `call_arxiv_function`.\n\nParameters:\n- `messages` (list[dict]): The list of messages representing the current conversation state.\n- `functions` (list[dict], optional): The list of available function definitions.\n\nInputs: Conversation history and optional function definitions.\nOutputs: An OpenAI API response object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef chat_completion_with_function_execution(messages, functions=[None]):\n    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n    response = chat_completion_request(messages, functions)\n    full_message = response.choices[0]\n    if full_message.finish_reason == \"function_call\":\n        print(f\"Function generation requested, calling function\")\n        return call_arxiv_function(messages, full_message)\n    else:\n        print(f\"Function not required, responding to user\")\n        return response\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Transcribe Audio using OpenAI Whisper (Python)\nDESCRIPTION: Defines a function `transcribe_audio` that sends an audio file to the OpenAI Whisper API for transcription. It takes the filename and its directory as input, constructs the full path, opens the file in binary read mode, and calls the `client.audio.transcriptions.create` method with the `whisper-1` model. The function returns the transcribed text extracted from the API response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef transcribe_audio(file,output_dir):\n    audio_path = os.path.join(output_dir, file)\n    with open(audio_path, 'rb') as audio_data:\n        transcription = client.audio.transcriptions.create(\n            model=\"whisper-1\", file=audio_data)\n        return transcription.text\n```\n\n----------------------------------------\n\nTITLE: Full Retool Vector Search Logic (Python)\nDESCRIPTION: This complete Python code block for the Retool workflow includes library imports, client initialization, an embedding function using OpenAI, querying a Pinecone index with a user query received from the workflow's start trigger (`startTrigger.data.query`), and returning the search results. It combines the embedding and search steps into one block.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pinecone import Pinecone\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=retoolContext.configVars.openai_api_key) \npc = Pinecone(api_key=retoolContext.configVars.pinecone_api_key)\nindex = pc.Index(\"openai-cookbook-pinecone-retool\")\n\n\ndef embed(query):\n    res = client.embeddings.create(\n        input=query,\n        model=\"text-embedding-3-large\"\n    )\n    doc_embeds = [r.embedding for r in res.data] \n    return doc_embeds \n\nx = embed([startTrigger.data.query])\n\nresults = index.query(\n    namespace=\"ns1\",\n    vector=x[0],\n    top_k=2,\n    include_values=False,\n    include_metadata=True\n)\n\nreturn results.to_dict()['matches']\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependency (openai)\nDESCRIPTION: Installs the `openai` Python library using pip. This library is essential for interacting with the OpenAI API to generate vector embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n%pip install openai\n```\n\n----------------------------------------\n\nTITLE: Uploading Files and Creating Vector Store in Python\nDESCRIPTION: Shows the process of creating a vector store named 'Financial Statements', preparing local files for upload by opening them as binary streams, then uploading files and polling for processing completion using the OpenAI Python SDK beta helpers. After uploading, the final batch status and file counts are printed for verification. Requires access to the beta vector stores API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a vector store caled \"Financial Statements\"\nvector_store = client.beta.vector_stores.create(name=\"Financial Statements\")\n \n# Ready the files for upload to OpenAI\nfile_paths = [\"edgar/goog-10k.pdf\", \"edgar/brka-10k.txt\"]\nfile_streams = [open(path, \"rb\") for path in file_paths]\n \n# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n# and poll the status of the file batch for completion.\nfile_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n  vector_store_id=vector_store.id, files=file_streams\n)\n \n# You can print the status and the file counts of the batch to see the result of this operation.\nprint(file_batch.status)\nprint(file_batch.file_counts)\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Chat Completion - Node.js\nDESCRIPTION: This Node.js snippet demonstrates calling the OpenAI API in JSON mode. It imports the OpenAI library and initializes a client.  The `response_format` is set to `json_object`, and system and user messages are provided.  The returned content from the model is then logged to the console. Requires the `openai` package and Node.js environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_6\n\nLANGUAGE: node.js\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant designed to output JSON.\",\n      },\n      { role: \"user\", content: \"Who won the world series in 2020?\" },\n    ],\n    model: \"gpt-3.5-turbo-0125\",\n    response_format: { type: \"json_object\" },\n  });\n  console.log(completion.choices[0].message.content);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: GPT-4 Post-Processing with Product Names\nDESCRIPTION: This snippet demonstrates the GPT-4 post-processing approach using a defined `system_prompt`. It calls the `transcribe_with_spellcheck` function, feeding it the `system_prompt` containing the correct product names and the audio file path. The output is then printed, to demonstrate the correction of spelling errors by GPT-4.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\nnew_text = transcribe_with_spellcheck(system_prompt, audio_filepath=ZyntriQix_filepath)\nprint(new_text)\n```\n\n----------------------------------------\n\nTITLE: Example .env file for OpenAI API Key\nDESCRIPTION: This shows the structure of a `.env` file for storing the OpenAI API key. It emphasizes the importance of keeping the API key private and not sharing it.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_6\n\nLANGUAGE: env\nCODE:\n```\n# Once you add your API key below, make sure to not share it with anyone! The API key should remain private.\n\nOPENAI_API_KEY=abc123\n```\n\n----------------------------------------\n\nTITLE: Accessing Existing fine-tuning jobs\nDESCRIPTION: This provides code snippets to list fine-tuning jobs, retrieve the state of a fine-tune, cancel a job, list events from a fine-tuning job, and delete a fine-tuned model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nftjob_id = \"ftjob-84PQg97hoIAKf21IPnhiNlU1\"\n# List 10 fine-tuning jobs\n# client.fine_tuning.jobs.list(limit=10)\n\n# Retrieve the state of a fine-tune\nclient.fine_tuning.jobs.retrieve(ftjob_id)\n\n# Cancel a job\n# client.fine_tuning.jobs.cancel(\"ftjob-abc123\")\n\n# List up to 10 events from a fine-tuning job\n# client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-abc123\", limit=10)\n\n# Delete a fine-tuned model (must be an owner of the org the model was created in)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure AI Search Index in Python\nDESCRIPTION: This code defines and creates a search index with vector search capabilities using the Azure AI Search Python SDK. It specifies the index schema, including fields for ID, title, text, and vector embeddings. It also configures the HNSW algorithm for vector search and creates a vector search profile to define the search behavior.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nindex_name = \"azure-ai-search-openai-cookbook-demo\"\n# index_name = \"<insert_name_for_index>\"\n\nindex_client = SearchIndexClient(\n    endpoint=search_service_endpoint, credential=AzureKeyCredential(search_service_api_key)\n)\n# Define the fields for the index. Update these based on your data.\n# Each field represents a column in the search index\nfields = [\n    SimpleField(name=\"id\", type=SearchFieldDataType.String),  # Simple string field for document ID\n    SimpleField(name=\"vector_id\", type=SearchFieldDataType.String, key=True),  # Key field for the index\n    # SimpleField(name=\"url\", type=SearchFieldDataType.String),  # URL field (commented out)\n    SearchableField(name=\"title\", type=SearchFieldDataType.String),  # Searchable field for document title\n    SearchableField(name=\"text\", type=SearchFieldDataType.String),  # Searchable field for document text\n    SearchField(\n        name=\"title_vector\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  # Collection of single values for title vector\n        vector_search_dimensions=1536,  # Number of dimensions in the vector\n        vector_search_profile_name=\"my-vector-config\",  # Profile name for vector search configuration\n    ),\n    SearchField(\n        name=\"content_vector\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  # Collection of single values for content vector\n        vector_search_dimensions=1536,  # Number of dimensions in the vector\n        vector_search_profile_name=\"my-vector-config\",  # Profile name for vector search configuration\n    ),\n    SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True),  # Searchable field for document category\n]\n\n# This configuration defines the algorithm and parameters for vector search\nvector_search = VectorSearch(\n    algorithms=[\n        HnswAlgorithmConfiguration(\n            name=\"my-hnsw\",  # Name of the HNSW algorithm configuration\n            kind=VectorSearchAlgorithmKind.HNSW,  # Type of algorithm\n            parameters=HnswParameters(\n                m=4,  # Number of bi-directional links created for every new element\n                ef_construction=400,  # Size of the dynamic list for the nearest neighbors during construction\n                ef_search=500,  # Size of the dynamic list for the nearest neighbors during search\n                metric=VectorSearchAlgorithmMetric.COSINE,  # Distance metric used for the search\n            ),\n        )\n    ],\n    profiles=[\n        VectorSearchProfile(\n            name=\"my-vector-config\",  # Name of the vector search profile\n            algorithm_configuration_name=\"my-hnsw\",  # Reference to the algorithm configuration\n        )\n    ],\n)\n\n# Create the search index with the vector search configuration\n# This combines all the configurations into a single search index\nindex = SearchIndex(\n    name=index_name,  # Name of the index\n    fields=fields,  # Fields defined for the index\n    vector_search=vector_search  # Vector search configuration\n\n)\n\n# Create or update the index\n# This sends the index definition to the Azure Search service\nresult = index_client.create_index(index)\nprint(f\"{result.name} created\")  # Output the name of the created index\n```\n\n----------------------------------------\n\nTITLE: Building Prompt for Cross-Encoder Relevance Classification - Python\nDESCRIPTION: Defines a multiline string prompt, with several domain-specific few-shot examples, to guide the GPT model for binary relevance (Yes/No) classification. The placeholders {query} and {document} are formatted dynamically per request. No input/output, but serves as a required template for subsequent cross-encoder calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprompt = '''\nYou are an Assistant responsible for helping detect whether the retrieved document is relevant to the query. For a given input, you need to output a single token: \"Yes\" or \"No\" indicating the retrieved document is relevant to the query.\n\nQuery: How to plant a tree?\nDocument: \"\"\"Cars were invented in 1886, when German inventor Carl Benz patented his Benz Patent-Motorwagen.[3][4][5] Cars became widely available during the 20th century. One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced horse-drawn carriages.[6] In Europe and other parts of the world, demand for automobiles did not increase until after World War II.[7] The car is considered an essential part of the developed economy.\"\"\"\nRelevant: No\n\nQuery: Has the coronavirus vaccine been approved?\nDocument: \"\"\"The Pfizer-BioNTech COVID-19 vaccine was approved for emergency use in the United States on December 11, 2020.\"\"\"\nRelevant: Yes\n\nQuery: What is the capital of France?\nDocument: \"\"\"Paris, France's capital, is a major European city and a global center for art, fashion, gastronomy and culture. Its 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. Beyond such landmarks as the Eiffel Tower and the 12th-century, Gothic Notre-Dame cathedral, the city is known for its cafe culture and designer boutiques along the Rue du Faubourg Saint-HonorÃ©.\"\"\"\nRelevant: Yes\n\nQuery: What are some papers to learn about PPO reinforcement learning?\nDocument: \"\"\"Proximal Policy Optimization and its Dynamic Version for Sequence Generation: In sequence generation task, many works use policy gradient for model optimization to tackle the intractable backpropagation issue when maximizing the non-differentiable evaluation metrics or fooling the discriminator in adversarial learning. In this paper, we replace policy gradient with proximal policy optimization (PPO), which is a proved more efficient reinforcement learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence generation tasks including synthetic experiment and chit-chat chatbot. The results show that PPO and PPO-dynamic can beat policy gradient by stability and performance.\"\"\"\nRelevant: Yes\n\nQuery: Explain sentence embeddings\nDocument: \"\"\"Inside the bubble: exploring the environments of reionisation-era Lyman-Î± emitting galaxies with JADES and FRESCO: We present a study of the environments of 16 Lyman-Î± emitting galaxies (LAEs) in the reionisation era (5.8<z<8) identified by JWST/NIRSpec as part of the JWST Advanced Deep Extragalactic Survey (JADES). Unless situated in sufficiently (re)ionised regions, Lyman-Î± emission from these galaxies would be strongly absorbed by neutral gas in the intergalactic medium (IGM). We conservatively estimate sizes of the ionised regions required to reconcile the relatively low Lyman-Î± velocity offsets (Î”vLyÎ±<300kmsâˆ’1) with moderately high Lyman-Î± escape fractions (fesc,LyÎ±>5%) observed in our sample of LAEs, indicating the presence of ionised ``bubbles'' with physical sizes of the order of 0.1pMpcâ‰²Rionâ‰²1pMpc in a patchy reionisation scenario where the bubbles are embedded in a fully neutral IGM. Around half of the LAEs in our sample are found to coincide with large-scale galaxy overdensities seen in FRESCO at zâˆ¼5.8-5.9 and zâˆ¼7.3, suggesting Lyman-Î± transmission is strongly enhanced in such overdense regions, and underlining the importance of LAEs as tracers of the first large-scale ionised bubbles. Considering only spectroscopically confirmed galaxies, we find our sample of UV-faint LAEs (MUVâ‰³âˆ’20mag) and their direct neighbours are generally not able to produce the required ionised regions based on the Lyman-Î± transmission properties, suggesting lower-luminosity sources likely play an important role in carving out these bubbles. These observations demonstrate the combined power of JWST multi-object and slitless spectroscopy in acquiring a unique view of the early stages of Cosmic Reionisation via the most distant LAEs.\"\"\"\nRelevant: No\n\nQuery: {query}\nDocument: \"\"\"{document}\"\"\"\nRelevant:\n'''\n\n```\n\n----------------------------------------\n\nTITLE: Moderation Check for Images Using OpenAI API in Python\nDESCRIPTION: Provides a function to evaluate the safety of an image by calling the OpenAI Moderation API with image URLs. It inspects moderation categories like sexual content, hate, violence, and self-harm to determine if the image is appropriate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef check_image_moderation(image_url):\n    response = client.moderations.create(\n        model=\"omni-moderation-latest\",\n        input=[\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": image_url\n                }\n            }\n        ]\n    )\n    results = response.results[0]\n    flagged_categories = vars(results.categories)\n    flagged = results.flagged\n    if not flagged:\n        return True\n    else:\n        return False\n```\n\n----------------------------------------\n\nTITLE: Applying Fine-Tuned Q&A Model for Answer Generation in Python\nDESCRIPTION: This function applies a fine-tuned Q&A model to generate an answer for a given question within a provided context. It requires the OpenAI Python client and expects the fine-tuned Q&A model ID, context string, and question string. The function constructs a prompt incorporating the context and question, sends it to the chat completions API, and retrieves the answer text. Parameters include 'context' (informative text), 'question' (query), and 'answering_model' (model identifier). The function uses a limited maximum token count and stop sequences to control answer length and formatting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef apply_ft_qa_answer(context, question, answering_model):\n    \"\"\"\n    Apply the fine tuned discriminator to a question\n    \"\"\"\n    prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n    result = openai.chat.completions.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])\n    return result['choices'][0]['text']\n\napply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n                    'What was the first human-made object in space?', ft_qa)\n```\n\n----------------------------------------\n\nTITLE: Generating Python Script for Multitable CSV Data with OpenAI API in Python\nDESCRIPTION: Uses the OpenAI API (`gpt-4o-mini`) to generate a Python program that creates three related pandas DataFrames: Housing, Location, and House Types. The prompt specifies the schemas for each table, the number of rows for the housing data, and emphasizes the need for logical data, correct foreign key relationships, and using previously generated dataframes to ensure consistency.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"\"\"\nCreate a Python program to generate 3 different pandas dataframes.\n\n1. Housing data\nI want 100 rows. Each row should include the following fields:\n - id (incrementing integer starting at 1)\n - house size (m^2)\n - house price\n - location\n - number of bedrooms\n - house type\n + any relevant foreign keys\n\n2. Location\nEach row should include the following fields:\n - id (incrementing integer starting at 1)\n - country\n - city\n - population\n - area (m^2)\n + any relevant foreign keys\n\n 3. House types\n - id (incrementing integer starting at 1)\n - house type\n - average house type price\n - number of houses\n + any relevant foreign keys\n\nMake sure that the numbers make sense (i.e. more rooms is usually bigger size, more expensive locations increase price. more size is usually higher price etc. make sure all the numbers make sense).\nMake sure that the dataframe generally follow common sense checks, e.g. the size of the dataframes make sense in comparison with one another.\nMake sure the foreign keys match up and you can use previously generated dataframes when creating each consecutive dataframes.\nYou can use the previously generated dataframe to generate the next dataframe.\n\"\"\"\n\nresponse = client.chat.completions.create(\n  model=datagen_model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n    {\"role\": \"user\", \"content\": question}\n  ]\n)\nres = response.choices[0].message.content\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Conversation Class for Message History - Python\nDESCRIPTION: This class provides a simple way to manage conversation history for interactions with a language model API. It stores messages with roles and content and includes a method for displaying the history.\n\nRequired Dependencies: Requires the `colored` function (typically from the `termcolor` library) for formatted display.\n\nMethods:\n- `__init__()`: Initializes an empty list `conversation_history`.\n- `add_message(role, content)`: Appends a new message dictionary (`{\"role\": role, \"content\": content}`) to the history.\n- `display_conversation(detailed=False)`: Prints each message in the history, optionally with color coding based on the role.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass Conversation:\n    def __init__(self):\n        self.conversation_history = []\n\n    def add_message(self, role, content):\n        message = {\"role\": role, \"content\": content}\n        self.conversation_history.append(message)\n\n    def display_conversation(self, detailed=False):\n        role_to_color = {\n            \"system\": \"red\",\n            \"user\": \"green\",\n            \"assistant\": \"blue\",\n            \"function\": \"magenta\",\n        }\n        for message in self.conversation_history:\n            print(\n                colored(\n                    f\"{message['role']}: {message['content']}\\n\\n\",\n                    role_to_color[message[\"role\"]],\n                )\n            )\n\n```\n\n----------------------------------------\n\nTITLE: Importing Kangas Library in Python\nDESCRIPTION: Imports the Kangas library for use in the notebook to create and manipulate DataGrids.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kangas as kg\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Moderation to a 'Good' Request in Python\nDESCRIPTION: This Python snippet demonstrates using the `custom_moderation` function. It calls the function with a variable `good_request` (assumed to contain safe content) and the predefined `parameters`, then prints the resulting JSON assessment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Use the custom moderation function for the good example\nmoderation_result = custom_moderation(good_request, parameters)\nprint(moderation_result)\n```\n\n----------------------------------------\n\nTITLE: Printing Parsed Math Steps and Final Answer Using Pydantic Models in Python\nDESCRIPTION: Outputs the list of step objects and the final answer directly from the strongly typed Pydantic model obtained from the structured chat completion, streamlining consumption of structured API responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(result.steps)\nprint(\"Final answer:\")\nprint(result.final_answer)\n```\n\n----------------------------------------\n\nTITLE: Implementing Traditional Text-to-Speech with OpenAI API in Python\nDESCRIPTION: Basic implementation of OpenAI's traditional TTS API that converts text to speech without specialized instructions. This example uses the tts-1-hd model with the 'alloy' voice to generate an MP3 file from a children's story text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/steering_tts.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\ntts_text = \"\"\"\nOnce upon a time, Leo the lion cub woke up to the smell of pancakes and scrambled eggs.\nHis tummy rumbled with excitement as he raced to the kitchen. Mama Lion had made a breakfast feast!\nLeo gobbled up his pancakes, sipped his orange juice, and munched on some juicy berries.\n\"\"\"\n\nspeech_file_path = \"./sounds/default_tts.mp3\"\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"alloy\",\n    input=tts_text,\n)\n\nresponse.write_to_file(speech_file_path)\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Embedded Data with Pandas\nDESCRIPTION: This Python snippet loads a CSV file containing Wikipedia article data and precomputed embeddings into a Pandas DataFrame. It uses `literal_eval` from the `ast` module to convert string representations of lists (the embeddings) back into actual lists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom ast import literal_eval\n\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n# Read vectors from strings back into a list\narticle_df[\"title_vector\"] = article_df.title_vector.apply(literal_eval)\narticle_df[\"content_vector\"] = article_df.content_vector.apply(literal_eval)\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Describing Pinecone Index Statistics\nDESCRIPTION: This snippet retrieves and prints the statistics of the Pinecone index, including the vector counts in each namespace. This confirms that all vectors have been successfully loaded.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Check index size for each namespace to confirm all of our docs have loaded\nindex.describe_index_stats()\n```\n\n----------------------------------------\n\nTITLE: Running VACUUM on Hologres Table in Python\nDESCRIPTION: Executes the `VACUUM articles;` SQL command using the `psycopg2` cursor. In Hologres, running VACUUM after bulk data loading helps to finalize the Proxima vector index building process and update table statistics.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncursor.execute('vacuum articles;')\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI with Node.js SDK\nDESCRIPTION: This snippet initializes the OpenAI SDK in a Node.js environment.  It imports the OpenAI library and creates an OpenAI client instance.  The API key is retrieved from an environment variable.  `dangerouslyAllowBrowser` is set to true for browser environments, with a caution to use a Node server in production.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  dangerouslyAllowBrowser: true,\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Custom GPT Q&A Instruction Scenario for Document Search (Python)\nDESCRIPTION: This snippet provides custom instructions in Python docstring or markdown-style format for a GPT Q&A assistant that uses a document search API. It details step-by-step logic for the bot to handle answer scenarios, retry searches with different terms, and provide clear user feedback on search progress or failures. Inputs include the user's question (query) and a derived searchTerm, and outputs are summarized answers, explanations of no-results cases, and next steps. There are hardcoded limitations, such as three retries and handling of unstructured versus structured files. The logic is intended for direct use in the instructions configuration of a Custom GPT system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nYou are a Q&A helper that helps answer users questions. You have access to a documents repository through your API action. When a user asks a question, you pass in that question exactly as stated to the \"query\" parameter, and for the \"searchTerm\" you use a single keyword or term you think you should use for the search.\n\n****\n\nScenario 1: There are answers\n\nIf your action returns results, then you take the results from the action and summarize concisely with the webUrl returned from the action. You answer the users question to the best of your knowledge from the action\n\n****\n\nScenario 2: No results found\n\nIf the response you get from the action is \"No results found\", stop there and let the user know there were no results and that you are going to try a different search term, and explain why. You must always let the user know before conducting another search.\n\nExample:\n\n****\n\nI found no results for \"DEI\". I am now going to try [insert term] because [insert explanation]\n\n****\n\nThen, try a different searchTerm that is similar to the one you tried before, with a single word. \n\nTry this three times. After the third time, then let the user know you did not find any relevant documents to answer the question, and to check SharePoint. Be sure to be explicit about what you are searching for at each step.\n\n****\n\nIn either scenario, try to answer the user's question. If you cannot answer the user's question based on the knowledge you find, let the user know and ask them to go check the HR Docs in SharePoint. If the file is a CSV, XLSX, or XLS, you can tell the user to download the file using the link and re-upload to use Advanced Data Analysis.\n\n```\n\n----------------------------------------\n\nTITLE: Mock Free Response (Python)\nDESCRIPTION: Defines a function that returns a mock response for a free-response question. It always returns 'I don't know.' This function simulates user input for testing purposes. It takes no parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef get_mock_response_from_user_free_response():\n    return \"I don't know.\"\n```\n\n----------------------------------------\n\nTITLE: Creating Milvus Collection Schema in Python\nDESCRIPTION: Defines the schema for the Milvus collection. It specifies four fields: 'id' (INT64, primary key, auto-generated), 'title' (VARCHAR), 'description' (VARCHAR), and 'embedding' (FLOAT_VECTOR with dimension specified by DIMENSION). A `CollectionSchema` object is created using these fields, and then a `Collection` object is instantiated with the defined schema and `COLLECTION_NAME`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create collection which includes the id, title, and embedding.\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='description', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n]\nschema = CollectionSchema(fields=fields)\ncollection = Collection(name=COLLECTION_NAME, schema=schema)\n```\n\n----------------------------------------\n\nTITLE: Printing an Example Wikipedia String in Python\nDESCRIPTION: Displays a single example from the processed Wikipedia strings array to show the format and content of the split sections.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# print example data\nprint(wikipedia_strings[1])\n```\n\n----------------------------------------\n\nTITLE: Building Prompt for Claim Assessment Using OpenAI Chat Model in Python\nDESCRIPTION: Constructs a chat message prompt array that sets the system instructions and provides examples to guide the model's response. The prompt instructs the LLM to categorize scientific claims as 'True', 'False', or 'NEE' (Not Enough Evidence). The function takes a single claim string and returns a formatted message list compatible with OpenAI chat completions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef build_prompt(claim):\n    return [\n        {\"role\": \"system\", \"content\": \"I will ask you to assess a scientific claim. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"},\n        {\"role\": \"user\", \"content\": f\"\"\"        \nExample:\n\nClaim:\n0-dimensional biomaterials show inductive properties.\n\nAssessment:\nFalse\n\nClaim:\n1/2000 in UK have abnormal PrP positivity.\n\nAssessment:\nTrue\n\nClaim:\nAspirin inhibits the production of PGE2.\n\nAssessment:\nFalse\n\nEnd of examples. Assess the following claim:\n\nClaim:\n{claim}\n\nAssessment:\n\"\"\"}\n    ]\n```\n\n----------------------------------------\n\nTITLE: Initializing Deep Lake Vector Store with OpenAI Embeddings in Python\nDESCRIPTION: Defines a local or remote path to store vector embeddings and initializes a Deep Lake vector store using LangChain's DeepLake wrapper. An OpenAI embedding model 'text-embedding-3-small' is used to generate vector representations for text data. The vector store is created with overwrite enabled for fresh indexing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndataset_path = 'wikipedia-embeddings-deeplake'\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\n\nembedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\ndb = DeepLake(dataset_path, embedding=embedding, overwrite=True)\n```\n\n----------------------------------------\n\nTITLE: Making Moderation Requests in Python\nDESCRIPTION: Demonstrates how to use the OpenAI Python library to send text to the moderation endpoint and retrieve the classification results. It initializes the client and calls the `moderations.create` method with the input text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/moderation.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.moderations.create(input=\"Sample text goes here.\")\n\noutput = response.results[0]\n```\n\n----------------------------------------\n\nTITLE: Enabling Code Interpreter for OpenAI Assistant\nDESCRIPTION: Demonstrates how to enable the Code Interpreter tool when creating an OpenAI Assistant. This is done by including an object with `type: \"code_interpreter\"` in the `tools` array during Assistant creation via the API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.create(\n  instructions=\"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"code_interpreter\"}]\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst assistant = await openai.beta.assistants.create({\n  instructions: \"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n  model: \"gpt-4o\",\n  tools: [{\"type\": \"code_interpreter\"}]\n});\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/assistants \\\n  -u :$OPENAI_API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -H 'OpenAI-Beta: assistants=v2' \\\n  -d '{\n    \"instructions\": \"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n    \"tools\": [\n      { \"type\": \"code_interpreter\" }\n    ],\n    \"model\": \"gpt-4o\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with ReadTheDocsLoader Python\nDESCRIPTION: Loads documents from the `rtdocs` directory using the `ReadTheDocsLoader` class from `langchain.document_loaders`. This is used to process the previously downloaded HTML files and extracts the content as a list of documents ready to be indexed and used for question answering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.document_loaders import ReadTheDocsLoader\n\nloader = ReadTheDocsLoader('rtdocs')\ndocs = loader.load()\nlen(docs)\n```\n\n----------------------------------------\n\nTITLE: Executing User Recommendation Function Example - Python\nDESCRIPTION: This Python code snippet demonstrates how to call the `provide_user_specific_recommendations` function with predefined `user_id` and `user_input`. It captures the function's return value and prints it to the console, illustrating the typical usage and expected output of the recommendation process. It depends on the `provide_user_specific_recommendations` function being defined and accessible.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nuser_id = \"user1234\"\nuser_input = \"I'm hungry\"\noutput = provide_user_specific_recommendations(user_input, user_id)\nprint(output)\n\n```\n\n----------------------------------------\n\nTITLE: Playing the Translated Hindi Audio\nDESCRIPTION: A simple code snippet that uses the pydub library to decode the base64 audio data, create an audio segment, and play the translated Hindi audio, allowing users to hear the result of the translation process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Play the audio \naudio_data_bytes = base64.b64decode(hindi_audio_data_base64)\naudio_segment = AudioSegment.from_file(BytesIO(audio_data_bytes), format=\"wav\")\n\nplay(audio_segment)\n```\n\n----------------------------------------\n\nTITLE: Executing Nearest Neighbor Content-Based Query on AnalyticDB - Python\nDESCRIPTION: Performs a similarity search using the 'content_vector' for the query 'Famous battles in Scottish history'. Calls the 'query_analyticdb' function with the vector_name parameter set to 'content_vector'. Outputs and ranks the top matching results by similarity in the console. Requires same setup as other query code, and is tailored for content-based search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# This time we'll query using content vector\nquery_results = query_analyticdb(\"Famous battles in Scottish history\", \"Articles\", \"content_vector\")\nfor i, result in enumerate(query_results):\n    print(f\"{i + 1}. {result[2]} (Score: {round(1 - result[3], 3)})\")\n\n```\n\n----------------------------------------\n\nTITLE: Identifying Target Category for a Sample\nDESCRIPTION: Retrieving the target category name for the first sample in the dataset to verify labeling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsports_dataset.target_names[sports_dataset['target'][0]]\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Client with Azure AD Authentication in Python\nDESCRIPTION: Conditionally initializes the `openai.AzureOpenAI` client for Azure Active Directory authentication when `use_azure_active_directory` is True. It retrieves the Azure OpenAI endpoint from the environment variable (`AZURE_OPENAI_ENDPOINT`) and sets the deployment ID. Authentication is handled by `DefaultAzureCredential` and the `get_bearer_token_provider` helper from `azure.identity`, which automatically manages token acquisition and refresh for the specified resource scope (`https://cognitiveservices.azure.com/.default`). The `base_url` includes `/extensions` for custom data features, and the preview API version `2023-09-01-preview` is specified.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\nif use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"] # Note: api_key might not be needed here if solely using AAD\n    # set the deployment name for the model we want to use\n    deployment = \"<deployment-id-of-the-model-to-use>\"\n\n    client = openai.AzureOpenAI(\n        base_url=f\"{endpoint}/openai/deployments/{deployment}/extensions\",\n        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Performing Thresholded Vector Similarity Search with OpenAI Embeddings in Python\nDESCRIPTION: This snippet shows how to compute the embedding vector for a quote using OpenAI's embedding API, then perform a vector similarity search on a vector table using cosine similarity metric with a threshold to filter out less relevant results. It prints the number of results found within the threshold and lists them with their distances and quote previews. Dependencies include an initialized OpenAI client, embedding model name, a vector table with metric_ann_search method, and knowledge of cosine similarity metric and its range [-1, +1]. Inputs are the input quote string, metric threshold (float), and result count (n). Outputs are printed summaries of filtered results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nquote = \"Animals are our equals.\"\n# quote = \"Be good.\"\n# quote = \"This teapot is strange.\"\n\nmetric_threshold = 0.84\n\nquote_vector = client.embeddings.create(\n    input=[quote],\n    model=embedding_model_name,\n).data[0].embedding\n\nresults = list(v_table.metric_ann_search(\n    quote_vector,\n    n=8,\n    metric=\"cos\",\n    metric_threshold=metric_threshold,\n))\n\nprint(f\"{len(results)} quotes within the threshold:\")\nfor idx, result in enumerate(results):\n    print(f\"    {idx}. [distance={result['distance']:.3f}] \\\"{result['body_blob'][:70]}...\\\"\")\n```\n\n----------------------------------------\n\nTITLE: Conditionally Answering Questions Using Fine-Tuned Discriminator and Q&A Models in Python\nDESCRIPTION: This function conditionally answers a question by first using a fine-tuned discriminator model to assess if the question can be answered based on the context. It aggregates log probabilities for 'yes' and 'no' responses from the discriminator, applies an optional modifier to the 'yes' logprob, and compares them. If the question is deemed unanswerable, it returns a default message; otherwise, it calls the fine-tuned Q&A model to generate an answer. Inputs include the answering and discriminator model IDs, the context, question, and a modifier to adjust the discriminator's decision threshold. This approach enables fine-grained control over when to trust the answer generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef answer_question_conditionally(answering_model, discriminator_model, context, question, discriminator_logprob_yes_modifier=0):\n    logprobs = apply_ft_discriminator(context, question, discriminator_model)\n    yes_logprob = logprobs[' yes'] if ' yes' in logprobs else -100\n    no_logprob = logprobs[' no'] if ' no' in logprobs else -100\n    if yes_logprob + discriminator_logprob_yes_modifier < no_logprob:\n        return \" No appropriate context found to answer the question based on the discriminator.\"\n    return apply_ft_qa_answer(context, question, answering_model)\n\nanswer_question_conditionally(ft_qa, ft_discriminator, \n                                \"Crowdless games are a rare although not unheard-of occurrence in sports. \\                                 When they do occur, it is usually the result of events beyond the control \\                                 of the teams or fans, such as weather-related concerns, public health concerns, \\                                 or wider civil disturbances unrelated to the game. For instance, \\                                 the COVID-19 pandemic caused many sports leagues around the world \\                                 to be played behind closed doors.\",\n                                \"Could weather cause a sport event to have no crowd?\")\n```\n\n----------------------------------------\n\nTITLE: Handling Document Search Requests with Azure Function and Microsoft Graph - JavaScript\nDESCRIPTION: Implements an Azure Function that processes incoming HTTP requests to authenticate users via a bearer token, initializes a Microsoft Graph client, conducts a document search based on the searchTerm parameter, and retrieves relevant documents from SharePoint or O365 drives. The function handles response structuring, error handling, and limits the number of results according to OpenAI API constraints. Dependencies include the Azure Functions SDK, Microsoft Graph SDK, and custom helper functions (getOboToken, initGraphClient, getDriveItemContent). Inputs: HTTP request (authorization header, searchTerm). Outputs: Structured response containing base64-encoded files or an error message. The function expects a maximum of 10 files and adheres to API payload and timeout limits.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nmodule.exports = async function (context, req) {\n   // const query = req.query.query || (req.body && req.body.query);\n   const searchTerm = req.query.searchTerm || (req.body && req.body.searchTerm);\n   if (!req.headers.authorization) {\n       context.res = {\n           status: 400,\n           body: 'Authorization header is missing'\n       };\n       return;\n   }\n   /// The below takes the token passed to the function, to use to get an OBO token.\n   const bearerToken = req.headers.authorization.split(' ')[1];\n   let accessToken;\n   try {\n       accessToken = await getOboToken(bearerToken);\n   } catch (error) {\n       context.res = {\n           status: 500,\n           body: `Failed to obtain OBO token: ${error.message}`\n       };\n       return;\n   }\n   // Initialize the Graph Client using the initGraphClient function defined above\n   let client = initGraphClient(accessToken);\n   // this is the search body to be used in the Microsft Graph Search API: https://learn.microsoft.com/en-us/graph/search-concept-files\n   const requestBody = {\n       requests: [\n           {\n               entityTypes: ['driveItem'],\n               query: {\n                   queryString: searchTerm\n               },\n               from: 0,\n               // the below is set to summarize the top 10 search results from the Graph API, but can configure based on your documents.\n               size: 10\n           }\n       ]\n   };\n\n\n   try {\n       // This is where we are doing the search\n       const list = await client.api('/search/query').post(requestBody);\n       const processList = async () => {\n           // This will go through and for each search response, grab the contents of the file and summarize with gpt-3.5-turbo\n           const results = [];\n           await Promise.all(list.value[0].hitsContainers.map(async (container) => {\n               for (const hit of container.hits) {\n                   if (hit.resource[\"@odata.type\"] === \"#microsoft.graph.driveItem\") {\n                       const { name, id } = hit.resource;\n                       // The below is where the file lives\n                       const driveId = hit.resource.parentReference.driveId;\n                       // we use the helper function we defined above to get the contents, convert to base64, and restructure it\n                       const contents = await getDriveItemContent(client, driveId, id, name);\n                       results.push(contents)\n               }\n           }));\n           return results;\n       };\n       let results;\n       if (list.value[0].hitsContainers[0].total == 0) {\n           // Return no results found to the API if the Microsoft Graph API returns no results\n           results = 'No results found';\n       } else {\n           // If the Microsoft Graph API does return results, then run processList to iterate through.\n           results = await processList();\n           // this is where we structure the response so ChatGPT knows they are files\n           results = {'openaiFileResponse': results}\n       }\n       context.res = {\n           status: 200,\n           body: results\n       };\n   } catch (error) {\n       context.res = {\n           status: 500,\n           body: `Error performing search or processing results: ${error.message}`,\n       };\n   }\n};\n```\n\n----------------------------------------\n\nTITLE: Assessing Response with Guardrail System\nDESCRIPTION: This function `assess_response` takes an example (including knowledge base article, chat history, and assistant response) and sends it to the OpenAI Chat API with the defined `guardrail_system_message`.  It instructs the model to analyze the assistant's response for factual accuracy, relevance, policy compliance, and contextual coherence. The function returns the model's assessment as a JSON string. Utilizes the `gpt-4o` model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef assess_response(example, full_context = full_df):\n    kb_article = example[\"kb_article\"]\n    chat_history = example[\"chat_history\"]\n    assistant_response = example[\"assistant_response\"][\"content\"]\n\n    messages = [\n        {\"role\": \"system\", \"content\": guardrail_system_message},\n        {\"role\": \"user\", \"content\": f\"Knowledge Base Article:\\n{kb_article}\\n\\nChat History:\\n{chat_history}\\n\\nAssistant Message:\\n{assistant_response}\"}\n    ]\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        response_format={\"type\": \"json_object\"}\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex and PyPDF Python Packages\nDESCRIPTION: Installs the llama-index and pypdf Python libraries required for document processing and indexing. This is a prerequisite setup step typically run in a notebook or shell environment to ensure required dependencies are available.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index pypdf\n```\n\n----------------------------------------\n\nTITLE: Calling Function to Trim Leading Silence from Audio (Python)\nDESCRIPTION: Calls the `trim_start` function, passing the path to the original downloaded earnings call audio file (`earnings_call_filepath`). This executes the silence trimming process defined earlier. The returned trimmed `AudioSegment` object is stored in the `trimmed_audio` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Trim the start of the original audio file\ntrimmed_audio = trim_start(earnings_call_filepath)\n```\n\n----------------------------------------\n\nTITLE: Defining Question Answering Chain with Langchain\nDESCRIPTION: Creates a QA chain using Langchain's RetrievalQA with OpenAI LLM. The chain retrieves relevant context from AnalyticDB and generates answers using the 'stuff' chain type.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import RetrievalQA\n\nllm = OpenAI()\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=doc_store,\n    return_source_documents=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Submitting Message to Request Slide Title (OpenAI Assistants API/Python)\nDESCRIPTION: Uses the `submit_message` helper function to send another user message to the thread. This message instructs the Assistant to create a very brief title for a slide, basing it on the previously generated plot and the extracted key insights.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsubmit_message(assistant.id,thread,\"Given the plot and bullet points you created,\\\n come up with a very brief title for a slide. It should reflect just the main insights you came up with.\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Starting Redis Stack Docker Container (Bash)\nDESCRIPTION: This command initiates the Redis Stack docker container in detached mode, providing a Redis instance that includes the RediSearch module necessary for vector indexing and searching, along with the RedisInsight GUI for management. It requires Docker and docker-compose to be installed on the system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: QA Chain with Custom Prompt\nDESCRIPTION: This snippet defines a new `VectorDBQA` chain, `custom_qa`,  using the custom prompt template.  It configures the chain to use the defined `custom_prompt_template`. The `chain_type_kwargs` argument is used to pass the custom prompt to the chain. The chain uses the same components as the original chain: the LLM, Qdrant instance, and the `stuff` chain type.  This lets the user modify the chain output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncustom_qa = VectorDBQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    vectorstore=doc_store,\n    return_source_documents=False,\n    chain_type_kwargs={\"prompt\": custom_prompt_template},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Network Rules and Policies for ChatGPT IP Whitelisting - SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a network rule and policy in Snowflake to whitelist a set of ChatGPT IP addresses for secure API access. It consists of two DDL statements: one for building a network rule listing allowed IP ranges, and one for setting up a network policy linked to that rule. Prerequisite: Account-level or appropriate admin permissions within Snowflake. Key parameters are the network rule name and IP CIDR blocks (which should be updated based on OpenAI documentation), and the network policy name. Input is DDL commands executed in a Snowflake worksheet; output is updated Snowflake configuration for approved network access. Limitations include the need for manual maintenance as IP ranges change and correct application at user/integration/account levels.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE NETWORK RULE chatgpt_network_rule\n  MODE = INGRESS\n  TYPE = IPV4\n  VALUE_LIST = ('23.102.140.112/28',\n                '13.66.11.96/28',\n                '104.210.133.240/28',\n                '70.37.60.192/28',\n                '20.97.188.144/28',\n                '20.161.76.48/28',\n                '52.234.32.208/28',\n                '52.156.132.32/28',\n                '40.84.220.192/28',\n                '23.98.178.64/28',\n                '51.8.155.32/28',\n                '20.246.77.240/28',\n                '172.178.141.0/28',\n                '172.178.141.192/28',\n                '40.84.180.128/28');\n\nCREATE NETWORK POLICY chatgpt_network_policy\n  ALLOWED_NETWORK_RULE_LIST = ('chatgpt_network_rule');\n```\n\n----------------------------------------\n\nTITLE: Creating File Batch Node.js\nDESCRIPTION: The Node.js version of the file batch creation code snippet. Uses `createAndPoll` for batch creation in an asynchronous manner. The function waits for batch operation completion. It requires `vector_store_id` and a list of `file_ids` to be added in a batch.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_18\n\nLANGUAGE: node.js\nCODE:\n```\nconst batch = await openai.beta.vectorStores.fileBatches.createAndPoll(\n  \"vs_abc123\",\n  { file_ids: [\"file_1\", \"file_2\", \"file_3\", \"file_4\", \"file_5\"] },\n);\n```\n\n----------------------------------------\n\nTITLE: Executing Content Vector Search Query - Python\nDESCRIPTION: This snippet calls the `search_redis` function with a sample query related to 'Famous battles in Scottish history'. It explicitly sets the `vector_field` parameter to `'content_vector'` to perform the vector similarity search based on the document's main content embedding, requesting 10 results (`k=10`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresults = search_redis(redis_client, 'Famous battles in Scottish history', vector_field='content_vector', k=10)\n```\n\n----------------------------------------\n\nTITLE: Calculating Semantic Similarity of Articles via Embeddings and Cosine Similarity - Python\nDESCRIPTION: This code obtains embeddings for the hypothetical answer and concatenated article data, then computes the cosine similarity (using dot product due to normalization) between the ideal answer and each article. The resulting list of scores is used for semantic article ranking. Dependencies include the embeddings helper and numpyâ€™s dot.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nhypothetical_answer_embedding = embeddings(hypothetical_answer)[0]\narticle_embeddings = embeddings(\n    [\n        f\"{article['title']} {article['description']} {article['content'][0:100]}\"\n        for article in articles\n    ]\n)\n\n# Calculate cosine similarity\ncosine_similarities = []\nfor article_embedding in article_embeddings:\n    cosine_similarities.append(dot(hypothetical_answer_embedding, article_embedding))\n\ncosine_similarities[0:10]\n\n```\n\n----------------------------------------\n\nTITLE: Creating Meta Prompt for Prompt Enhancement\nDESCRIPTION: Defines a meta prompt that instructs o1-preview to improve the simple summarization prompt by adding structure, news type classification, tags, and sentiment analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmeta_prompt = \"\"\"\nImprove the following prompt to generate a more detailed summary. \nAdhere to prompt engineering best practices. \nMake sure the structure is clear and intuitive and contains the type of news, tags and sentiment analysis.\n\n{simple_prompt}\n\nOnly return the prompt.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Image Moderation Function with Sample Images\nDESCRIPTION: Checks two sample imagesâ€”one related to war and another of a wonder of the worldâ€”for safety based on moderation categories. Outputs indicate whether each image is considered safe or not.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nwar_image = \"https://assets.editorial.aetnd.com/uploads/2009/10/world-war-one-gettyimages-90007631.jpg\"\nworld_wonder_image = \"https://whc.unesco.org/uploads/thumbs/site_0252_0008-360-360-20250108121530.jpg\"\n\nprint(\"Checking an image about war: \" + (\"Image is not safe\" if not check_image_moderation(war_image) else \"Image is safe\"))\nprint(\"Checking an image of a wonder of the world: \" + (\"Image is not safe\" if not check_image_moderation(world_wonder_image) else \"Image is safe\"))\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: Installs the Redis client library and wget to fetch the embedded data file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# We'll need to install the Redis client\n!pip install redis\n\n#Install wget to pull zip file\n!pip install wget\n```\n\n----------------------------------------\n\nTITLE: Creating a Run via Node.js with custom parameters\nDESCRIPTION: This code illustrates creating a Run in Node.js, specifying the thread ID, assistant ID, and optionally overriding default model, instructions, and tools. It uses the OpenAI SDK for JavaScript and sends a POST request to the API endpoint.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_11\n\nLANGUAGE: node.js\nCODE:\n```\nconst run = await openai.beta.threads.runs.create(\n  thread.id,\n  {\n    assistant_id: assistant.id,\n    model: \"gpt-4o\",\n    instructions: \"New instructions that override the Assistant instructions\",\n    tools: [{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}]\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Query Weaviate using near_text filter\nDESCRIPTION: This function queries a Weaviate database using the `near_text` filter, which leverages Weaviate's built-in OpenAI module. It takes a query string and a collection name as input, and constructs a query using the `with_near_text` filter. The function returns the query results, including the title, content, and certainty/distance scores.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef near_text_weaviate(query, collection_name):\n    \n    nearText = {\n        \"concepts\": [query],\n        \"distance\": 0.7,\n    }\n\n    properties = [\n        \"title\", \"content\",\n        \"_additional {certainty distance}\"\n    ]\n\n    query_result = (\n        client.query\n        .get(collection_name, properties)\n        .with_near_text(nearText)\n        .with_limit(20)\n        .do()\n    )[\"data\"][\"Get\"][collection_name]\n    \n    print (f\"Objects returned: {len(query_result)}\")\n    \n    return query_result\n```\n\n----------------------------------------\n\nTITLE: Formatting LLM Prompt for Hallucination Check in Python\nDESCRIPTION: Defines a Python multi-line f-string template used to structure the input prompt for an LLM. It includes placeholders for knowledge base articles (`kb_articles`), the chat transcript (`transcript`), and the assistant's message (`message`), preparing the context for hallucination evaluation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nuser_input = \"\"\"\n## Knowledge Base Articles\n{kb_articles}\n\n## Chat Transcript\n{transcript}\n\n## Assistant Message:\n{message}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion using Python CLI\nDESCRIPTION: Execute a chat completion request directly from the command line using the utility installed with the Python library. This provides a quick way to interact with the API without writing Python code. It requires the OPENAI_API_KEY environment variable to be set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ openai api chat_completions.create -m gpt-3.5-turbo -g user \"Hello world\"\n```\n\n----------------------------------------\n\nTITLE: Printing the Raw Concatenated Transcript (Python)\nDESCRIPTION: Outputs the content of the `full_transcript` variable to the console. This displays the complete transcription assembled from all audio segments before any post-processing steps (like punctuation or ASCII filtering) have been applied.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(full_transcript)\n```\n\n----------------------------------------\n\nTITLE: Pretty-Printing Chat Conversation Messages in Python\nDESCRIPTION: Defines a utility function `pretty_print_conversation` that iterates over a list of chat messages and prints them to the console with color coding based on the role ('system', 'user', 'assistant', or 'function'). It formats output differently if the assistant message includes a function call. This aids debugging and visibility into multi-turn conversations with chat models and their function calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef pretty_print_conversation(messages):\n    role_to_color = {\n        \"system\": \"red\",\n        \"user\": \"green\",\n        \"assistant\": \"blue\",\n        \"function\": \"magenta\",\n    }\n    \n    for message in messages:\n        if message[\"role\"] == \"system\":\n            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n        elif message[\"role\"] == \"user\":\n            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n        elif message[\"role\"] == \"function\":\n            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n```\n\n----------------------------------------\n\nTITLE: Configuring gcloud PATH and Verifying Installation (Python/Shell)\nDESCRIPTION: Modifies the system's PATH environment variable within the Python script's execution context to include the Google Cloud SDK binary directory. It then executes the `gcloud --version` shell command to verify that the `gcloud` command-line tool is accessible and correctly installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Add gcloud to PATH\nos.environ['PATH'] += os.pathsep + os.path.expanduser('~/google-cloud-sdk/bin')\n\n# Verify gcloud is in PATH\n! gcloud --version\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Semantic Search in Redis with Python\nDESCRIPTION: Defines a textual query and generates its embedding vector, then queries the Redis search index for the top 3 nearest neighbors using KNN vector similarity with cosine distance. Results are sorted by vector score and the content is returned. Uses Redis-py's Query object and specifies query parameters with the vector payload. Outputs matched documents with similarity scores, enabling semantic search over stored articles.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom redis.commands.search.query import Query\nimport numpy as np\n\ntext_4 = \"\"\"Radcliffe yet to answer GB call\n\nPaula Radcliffe has been granted extra time to decide whether to compete in the World Cross-Country Championships.\n\nThe 31-year-old is concerned the event, which starts on 19 March in France, could upset her preparations for the London Marathon on 17 April. \"There is no question that Paula would be a huge asset to the GB team,\" said Zara Hyde Peters of UK Athletics. \"But she is working out whether she can accommodate the worlds without too much compromise in her marathon training.\" Radcliffe must make a decision by Tuesday - the deadline for team nominations. British team member Hayley Yelling said the team would understand if Radcliffe opted out of the event. \"It would be fantastic to have Paula in the team,\" said the European cross-country champion. \"But you have to remember that athletics is basically an individual sport and anything achieved for the team is a bonus. \"She is not messing us around. We all understand the problem.\" Radcliffe was world cross-country champion in 2001 and 2002 but missed last year's event because of injury. In her absence, the GB team won bronze in Brussels.\n\"\"\"\n\nvec = np.array(get_vector(text_4), dtype=np.float32).tobytes()\nq = Query('*=>[KNN 3 @vector $query_vec AS vector_score]')\\\n    .sort_by('vector_score')\\\n    .return_fields('vector_score', 'content')\\\n    .dialect(2)    \nparams = {\"query_vec\": vec}\n\nresults = client.ft('idx').search(q, query_params=params)\nfor doc in results.docs:\n    print(f\"distance:{round(float(doc['vector_score']),3)} content:{doc['content']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Generating Search Results with Summaries\nDESCRIPTION: This function takes a list of search items and generates a list of dictionaries, each containing the order, link, title (snippet), and summary of a web page.  It iterates through the search items, retrieves the content of each web page using `retrieve_content`, summarizes it using `summarize_content`, and stores the data in a dictionary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_search_results(search_items, character_limit=500):\n    # Generate a summary of search results for the given search term\n    results_list = []\n    for idx, item in enumerate(search_items, start=1):\n        url = item.get('link')\n        \n        snippet = item.get('snippet', '')\n        web_content = retrieve_content(url, TRUNCATE_SCRAPED_TEXT)\n        \n        if web_content is None:\n            print(f\"Error: skipped URL: {url}\")\n        else:\n            summary = summarize_content(web_content, search_term, character_limit)\n            result_dict = {\n                'order': idx,\n                'link': url,\n                'title': snippet,\n                'Summary': summary\n            }\n            results_list.append(result_dict)\n    return results_list\n```\n\n----------------------------------------\n\nTITLE: Retrieving Batch Status from OpenAI Batch API - Python, Node.js, curl\nDESCRIPTION: Demonstrates how to query the current status of a batch job by its Batch ID using the OpenAI Batch API. The response returns the Batch object containing detailed metadata about the job, including processing phases like 'validating', 'in_progress', 'completed', or 'failed'. Snippets are provided for Python, Node.js, and curl to facilitate monitoring batch execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.batches.retrieve(\"batch_abc123\")\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/batches/batch_abc123 \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const batch = await openai.batches.retrieve(\"batch_abc123\");\n  console.log(batch);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Listing all Batches with OpenAI API - Python\nDESCRIPTION: This Python snippet retrieves a list of all batches using the OpenAI API. It requires the `openai` library to be installed. The `limit` parameter is used to control the number of batches to return, set to 10 in this example. The user will need to have proper API key credentials setup for use.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.batches.list(limit=10)\n```\n\n----------------------------------------\n\nTITLE: Creating an Assistant\nDESCRIPTION: This snippet demonstrates how to create an assistant using the OpenAI API. It initializes the OpenAI client with the API key obtained from the environment variables or specified directly. It then uses the `client.beta.assistants.create()` method to create an assistant with a specified name, instructions, and model. Finally, it prints the assistant's details using the `show_json` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n\nassistant = client.beta.assistants.create(\n    name=\"Math Tutor\",\n    instructions=\"You are a personal math tutor. Answer questions briefly, in a sentence or less.\",\n    model=\"gpt-4o\",\n)\nshow_json(assistant)\n```\n\n----------------------------------------\n\nTITLE: Drop Existing Collection\nDESCRIPTION: Checks if a collection with the predefined name already exists in the Zilliz database. If it exists, it is dropped to ensure a clean state for the subsequent data insertion process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Remove collection if it already exists\nif utility.has_collection(COLLECTION_NAME):\n    utility.drop_collection(COLLECTION_NAME)\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Moderation to a 'Bad' Request in Python\nDESCRIPTION: This Python snippet shows the usage of the `custom_moderation` function with potentially inappropriate content. It calls the function using the `bad_request` variable (assumed to contain problematic content) and the predefined `parameters`, then prints the JSON assessment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Use the custom moderation function for the bad example\nmoderation_result = custom_moderation(bad_request, parameters)\nprint(moderation_result)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Monitoring with Weave - Python\nDESCRIPTION: This snippet initializes the monitoring of OpenAI API calls in W&B Weave by calling init_monitor with the specified entity/project/stream, then makes two sample ChatCompletion calls for recording. Dependencies include weave.monitoring.openai and the openai library. Key parameters include the stream identifier and OPENAI_MODEL. The snippet demonstrates both setup and initial example logs; upon execution, it returns logged ChatCompletion responses and links to the monitoring dashboard.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom weave.monitoring import openai, init_monitor\nm = init_monitor(f\"{WB_ENTITY}/{WB_PROJECT}/{STREAM_NAME}\")\n\n# specifying a single model for simplicity\nOPENAI_MODEL = 'gpt-3.5-turbo'\n\n# prefill with some sample logs\nr = openai.ChatCompletion.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": \"hello world!\"}])\nr = openai.ChatCompletion.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": \"what is 2+2?\"}])\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Information\nDESCRIPTION: This snippet displays information about the DataFrame, including column data types and non-null counts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\narticle_df.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: Perform and display search results for 'Famous battles in Scottish history' in content\nDESCRIPTION: Runs a similarity search based on 'content' field vectors with the specified query and outputs top results with their respective distances.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nquery_results = query_typesense('Famous battles in Scottish history', 'content')\n\nfor i, hit in enumerate(query_results['results'][0]['hits']):\n    document = hit[\"document\"]\n    vector_distance = hit[\"vector_distance\"]\n    print(f'{i + 1}. {document[\"title\"]} (Distance: {vector_distance})')\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for Redis and OpenAI Integration in Python\nDESCRIPTION: Installs the necessary Python packages including redis client, OpenAI SDK, dotenv for environment variable management, and the OpenAI datalib extras for embeddings. This setup is prerequisite for running subsequent code snippets that interact with Redis and OpenAI services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install redis openai python-dotenv openai[datalib]\n```\n\n----------------------------------------\n\nTITLE: Displaying Structured Math Steps with LaTeX Rendering in Python\nDESCRIPTION: Defines a helper function that takes the JSON string response from the math tutor, parses the steps and final answer, and uses IPython.display.Math to render each step's equation in LaTeX format for clear visualization. It outputs each explanation and corresponding rendered equation successively and finally displays the final answer.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Math, display\n\ndef print_math_response(response):\n    result = json.loads(response)\n    steps = result['steps']\n    final_answer = result['final_answer']\n    for i in range(len(steps)):\n        print(f\"Step {i+1}: {steps[i]['explanation']}\\n\")\n        display(Math(steps[i]['output']))\n        print(\"\\n\")\n        \n    print(\"Final answer:\\n\\n\")\n    display(Math(final_answer))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Embedding Similarity on Test Reviews - Python\nDESCRIPTION: This function computes the cosine similarity between averaged user and product embeddings on test data to evaluate recommendation performance. It utilizes a custom 'cosine_similarity' function (imported from a local utility module) and applies this row-wise to the test set using pandas. Results are stored as new columns for cosine similarity and its percentile rank. Dependencies include pandas, numpy, and the utility function. If embeddings are missing for any user/product, the result is NaN.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/User_and_product_embeddings.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom utils.embeddings_utils import cosine_similarity\n\n# evaluate embeddings as recommendations on X_test\ndef evaluate_single_match(row):\n    user_id = row.UserId\n    product_id = row.ProductId\n    try:\n        user_embedding = user_embeddings[user_id]\n        product_embedding = prod_embeddings[product_id]\n        similarity = cosine_similarity(user_embedding, product_embedding)\n        return similarity\n    except Exception as e:\n        return np.nan\n\nX_test['cosine_similarity'] = X_test.apply(evaluate_single_match, axis=1)\nX_test['percentile_cosine_similarity'] = X_test.cosine_similarity.rank(pct=True)\n```\n\n----------------------------------------\n\nTITLE: Summarizing Meeting Transcript with GPT-4 - Python\nDESCRIPTION: This function, 'abstract_summary_extraction', takes a text transcription and uses GPT-4 via OpenAI's chat completions API to generate a concise summary. The prompt instructs the model to focus on extracting main points and omitting extraneous details. The function requires valid OpenAI API credentials and expects the transcript as input. Output is a one-paragraph summary as a string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef abstract_summary_extraction(transcription):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a highly skilled AI trained in language comprehension and summarization. I would like you to read the following text and summarize it into a concise abstract paragraph. Aim to retain the most important points, providing a coherent and readable summary that could help a person understand the main points of the discussion without needing to read the entire text. Please avoid unnecessary details or tangential points.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Configuring Milvus and OpenAI Parameters in Python\nDESCRIPTION: Imports the `openai` library and defines global variables for configuring the connection to Milvus (HOST, PORT), naming the collection (COLLECTION_NAME), specifying the embedding dimension (DIMENSION), selecting the OpenAI embedding model (OPENAI_ENGINE), setting the OpenAI API key, defining index parameters (INDEX_PARAM) for collection creation, search parameters (QUERY_PARAM), and the batch size (BATCH_SIZE) for data insertion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nHOST = 'localhost'\nPORT = 19530\nCOLLECTION_NAME = 'book_search'\nDIMENSION = 1536\nOPENAI_ENGINE = 'text-embedding-3-small'\nopenai.api_key = 'sk-your_key'\n\nINDEX_PARAM = {\n    'metric_type':'L2',\n    'index_type':\"HNSW\",\n    'params':{'M': 8, 'efConstruction': 64}\n}\n\nQUERY_PARAM = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"ef\": 64},\n}\n\nBATCH_SIZE = 1000\n```\n\n----------------------------------------\n\nTITLE: Define Wikipedia Link Retrieval Utility Function\nDESCRIPTION: Implements a retry-capable function that queries Wikipedia for the most relevant page given an entity string, returning the URL if found. Handles exceptions gracefully for robust name resolution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(min=1, max=10), stop=stop_after_attempt(5))\ndef find_link(entity: str) -> Optional[str]:\n    \"\"\"\n    Finds a Wikipedia link for a given entity.\n    \"\"\"\n    try:\n        titles = wikipedia.search(entity)\n        if titles:\n            # naively consider the first result as the best\n            page = wikipedia.page(titles[0])\n            return page.url\n    except (wikipedia.exceptions.WikipediaException) as ex:\n        logging.error(f'Error occurred while searching for Wikipedia link for entity {entity}: {str(ex)}')\n\n    return None\n```\n\n----------------------------------------\n\nTITLE: Measuring Missed Retrievals Beyond Rank Threshold in Python\nDESCRIPTION: This snippet determines the fraction of questions for which the ADA search API fails to retrieve relevant context within the first 200 ranked results (indicated by rank == -1). It prints the result as a percentage, which helps analyze recall failure modes. This code uses pandas and expects the out_expanded DataFrame from earlier processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\noutside_200 = (out_expanded['rank'] == -1).mean()\nprint(f\"{outside_200*100:.1f}% of relevant paragraphs are not retrieved within the first 200 results\")\n```\n\n----------------------------------------\n\nTITLE: Preparing SQuADv2 Dataset as Pandas DataFrame in Python\nDESCRIPTION: Defines functions to load SQuADv2 JSON datasets into a structured pandas DataFrame, extracting questions, contexts, answers, and metadata such as titles and impossibility flags. A helper function creates a diverse sample from the dataframe balancing different titles and 'is_impossible' flags. This facilitates manageable data subsets for training and evaluation of RAG models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef json_to_dataframe_with_titles(json_data):\n    qas = []\n    context = []\n    is_impossible = []\n    answers = []\n    titles = []\n\n    for article in json_data['data']:\n        title = article['title']\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                qas.append(qa['question'].strip())\n                context.append(paragraph['context'])\n                is_impossible.append(qa['is_impossible'])\n                \n                ans_list = []\n                for ans in qa['answers']:\n                    ans_list.append(ans['text'])\n                answers.append(ans_list)\n                titles.append(title)\n\n    df = pd.DataFrame({'title': titles, 'question': qas, 'context': context, 'is_impossible': is_impossible, 'answers': answers})\n    return df\n\ndef get_diverse_sample(df, sample_size=100, random_state=42):\n    \"\"\"\n    Get a diverse sample of the dataframe by sampling from each title\n    \"\"\"\n    sample_df = df.groupby(['title', 'is_impossible']).apply(lambda x: x.sample(min(len(x), max(1, sample_size // 50)), random_state=random_state)).reset_index(drop=True)\n    \n    if len(sample_df) < sample_size:\n        remaining_sample_size = sample_size - len(sample_df)\n        remaining_df = df.drop(sample_df.index).sample(remaining_sample_size, random_state=random_state)\n        sample_df = pd.concat([sample_df, remaining_df]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    return sample_df.sample(min(sample_size, len(sample_df)), random_state=random_state).reset_index(drop=True)\n\ntrain_df = json_to_dataframe_with_titles(json.load(open('local_cache/train.json')))\nval_df = json_to_dataframe_with_titles(json.load(open('local_cache/dev.json')))\n\ndf = get_diverse_sample(val_df, sample_size=100, random_state=42)\n```\n\n----------------------------------------\n\nTITLE: Testing Chat with Good Request\nDESCRIPTION: This snippet calls the `execute_chat_with_guardrail` function with the `good_request`. The expected behavior is that the request should pass the topical guardrail and return the LLM's response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Call the main function with the good request - this should go through\nresponse = await execute_chat_with_guardrail(good_request)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Managing fine-tuning jobs with Python SDK\nDESCRIPTION: This snippet demonstrates listing, retrieving, canceling, and listing events of fine-tuning jobs through the Python SDK. These functions enable comprehensive management of ongoing and completed jobs, requiring the 'openai' package.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n# List 10 fine-tuning jobs\nclient.fine_tuning.jobs.list(limit=10)\n# Retrieve the state of a fine-tune\nclient.fine_tuning.jobs.retrieve(\"ftjob-abc123\")\n# Cancel a job\nclient.fine_tuning.jobs.cancel(\"ftjob-abc123\")\n# List up to 10 events from a fine-tuning job\nclient.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-abc123\", limit=10)\n# Delete a fine-tuned model\nclient.models.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Output Parser for LLM Agents in Python\nDESCRIPTION: Creates a custom output parser class for LangChain agents to interpret LLM responses and segment them into actions or a final answer. This parser checks for a terminal 'Final Answer:' string to determine completion, otherwise extracts the next action and its input from the model's output using regular expressions. Dependencies include regex support and specific LangChain agent/action classes. Expected input is plain text LLM output; it returns actionable objects for further agent processing, and raises a ValueError if parsing fails.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass CustomOutputParser(AgentOutputParser):\n    \n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n        \n        # Check if agent should finish\n        if \"Final Answer:\" in llm_output:\n            return AgentFinish(\n                # Return values is generally always a dictionary with a single `output` key\n                # It is not recommended to try anything else at the moment :)\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n                log=llm_output,\n            )\n        \n        # Parse out the action and action input\n        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n        match = re.search(regex, llm_output, re.DOTALL)\n        \n        # If it can't parse the output it raises an error\n        # You can add your own logic here to handle errors in a different way i.e. pass to a human, give a canned response\n        if not match:\n            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n        action = match.group(1).strip()\n        action_input = match.group(2)\n        \n        # Return the action and action input\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n    \noutput_parser = CustomOutputParser()\n\n```\n\n----------------------------------------\n\nTITLE: Define Qdrant Query Function\nDESCRIPTION: Defines a Python function `query_qdrant` that takes a text query, generates its embedding using the specified OpenAI model (`EMBEDDING_MODEL`), and performs a semantic search in the specified Qdrant collection. The search targets a specific vector ('title' or 'content') and returns the top `top_k` results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef query_qdrant(query, collection_name, vector_name='title', top_k=20):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.embeddings.create(\n        input=query,\n        model=EMBEDDING_MODEL,\n    ).data[0].embedding # We take the first embedding from the list\n    \n    query_results = qdrant.search(\n        collection_name=collection_name,\n        query_vector=(\n            vector_name, embedded_query\n        ),\n        limit=top_k, \n        query_filter=None\n    )\n    \n    return query_results\n```\n\n----------------------------------------\n\nTITLE: Setting Up LLM-as-a-Judge Grading Criteria\nDESCRIPTION: Creates a test criteria configuration that uses an LLM to evaluate push notification summaries based on quality categories. Defines prompts, passing labels, and available categories for assessment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nGRADER_DEVELOPER_PROMPT = \"\"\"\nCategorize the following push notification summary into the following categories:\n1. concise-and-snappy\n2. drops-important-information\n3. verbose\n4. unclear\n5. obscures-meaning\n6. other \n\nYou'll be given the original list of push notifications and the summary like this:\n\n<push_notifications>\n...notificationlist...\n</push_notifications>\n<summary>\n...summary...\n</summary>\n\nYou should only pick one of the categories above, pick the one which most closely matches and why.\n\"\"\"\nGRADER_TEMPLATE_PROMPT = \"\"\"\n<push_notifications>{{item.notifications}}</push_notifications>\n<summary>{{sample.output_text}}</summary>\n\"\"\"\npush_notification_grader = {\n    \"name\": \"Push Notification Summary Grader\",\n    \"type\": \"label_model\",\n    \"model\": \"o3-mini\",\n    \"input\": [\n        {\n            \"role\": \"developer\",\n            \"content\": GRADER_DEVELOPER_PROMPT,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": GRADER_TEMPLATE_PROMPT,\n        },\n    ],\n    \"passing_labels\": [\"concise-and-snappy\"],\n    \"labels\": [\n        \"concise-and-snappy\",\n        \"drops-important-information\",\n        \"verbose\",\n        \"unclear\",\n        \"obscures-meaning\",\n        \"other\",\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Messages Array with Function Result in JavaScript\nDESCRIPTION: This code snippet adds the result of a function call back into the `messages` array.  The `role` is set to \"function\", and the `content` is a JSON string representation of the function's response, along with the name of the function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nmessages.push({\n  role: \"function\",\n  name: functionName,\n  content: `The result of the last function was this: ${JSON.stringify(\n    functionResponse\n  )}\n  `,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries\nDESCRIPTION: This code installs the necessary libraries: textract for extracting text from PDFs and tiktoken for tokenization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install textract\n!pip install tiktoken\n```\n\n----------------------------------------\n\nTITLE: Performing Image Edit using DALLÂ·E API\nDESCRIPTION: This code calls the OpenAI API to perform an image edit. It uses the `client.images.edit` method. It supplies the original generated image and the mask generated in the previous step, along with a prompt and other parameters.  It sets the number of generated images to 1, the size and the format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# edit an image\n\n# call the OpenAI API\nedit_response = client.images.edit(\n    image=open(generated_image_filepath, \"rb\"),  # from the generation section\n    mask=open(mask_filepath, \"rb\"),  # from right above\n    prompt=prompt,  # from the generation section\n    n=1,\n    size=\"1024x1024\",\n    response_format=\"url\",\n)\n```\n\n----------------------------------------\n\nTITLE: AWS SAM Template Defining Serverless Redshift Middleware with Cognito Authentication in YAML\nDESCRIPTION: This comprehensive AWS SAM template written in YAML specifies a serverless application including Cognito User Pool and client for authentication, an API Gateway configured for secured access, and a Lambda function (Python 3.11 runtime) that executes SQL queries against Redshift. The function is configured with timeout, environment variables pointing to Redshift, VPC security groups and subnets for secure networking, and an API event trigger for HTTP POST on `/sql_statement`. Outputs expose API endpoint URL and resource ARNs. Cognito enforces user authentication with scopes and password policy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: >\n  redshift-middleware\n\n  Middleware to fetch RedShift data and return it through HTTP as files\n\nGlobals:\n  Function:\n    Timeout: 3\n\nParameters:\n  RedshiftHost:\n    Type: String\n  RedshiftPort:\n    Type: String\n  RedshiftUser:\n    Type: String\n  RedshiftPassword:\n    Type: String\n  RedshiftDb:\n    Type: String\n  SecurityGroupId:\n    Type: String\n  SubnetId1:\n    Type: String\n  SubnetId2:\n    Type: String\n  SubnetId3:\n    Type: String\n  SubnetId4:\n    Type: String\n  SubnetId5:\n    Type: String\n  SubnetId6:\n    Type: String\n  CognitoUserPoolName:\n    Type: String\n    Default: MyCognitoUserPool\n  CognitoUserPoolClientName:\n    Type: String\n    Default: MyCognitoUserPoolClient\n\nResources:\n  MyCognitoUserPool:\n    Type: AWS::Cognito::UserPool\n    Properties:\n      UserPoolName: !Ref CognitoUserPoolName\n      Policies:\n        PasswordPolicy:\n          MinimumLength: 8\n      UsernameAttributes:\n        - email\n      Schema:\n        - AttributeDataType: String\n          Name: email\n          Required: false\n\n  MyCognitoUserPoolClient:\n    Type: AWS::Cognito::UserPoolClient\n    Properties:\n      UserPoolId: !Ref MyCognitoUserPool\n      ClientName: !Ref CognitoUserPoolClientName\n      GenerateSecret: true\n\n  RedshiftMiddlewareApi:\n    Type: AWS::Serverless::Api\n    Properties:\n      StageName: Prod\n      Cors: \"'*'\"\n      Auth:\n        DefaultAuthorizer: MyCognitoAuthorizer\n        Authorizers:\n          MyCognitoAuthorizer:\n            AuthorizationScopes:\n              - openid\n              - email\n              - profile\n            UserPoolArn: !GetAtt MyCognitoUserPool.Arn\n        \n  RedshiftMiddlewareFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: redshift-middleware/\n      Handler: app.lambda_handler\n      Runtime: python3.11\n      Timeout: 45\n      Architectures:\n        - x86_64\n      Events:\n        SqlStatement:\n          Type: Api\n          Properties:\n            Path: /sql_statement\n            Method: post\n            RestApiId: !Ref RedshiftMiddlewareApi\n      Environment:\n        Variables:\n          REDSHIFT_HOST: !Ref RedshiftHost\n          REDSHIFT_PORT: !Ref RedshiftPort\n          REDSHIFT_USER: !Ref RedshiftUser\n          REDSHIFT_PASSWORD: !Ref RedshiftPassword\n          REDSHIFT_DB: !Ref RedshiftDb\n      VpcConfig:\n        SecurityGroupIds:\n          - !Ref SecurityGroupId\n        SubnetIds:\n          - !Ref SubnetId1\n          - !Ref SubnetId2\n          - !Ref SubnetId3\n          - !Ref SubnetId4\n          - !Ref SubnetId5\n          - !Ref SubnetId6\n\nOutputs:\n  RedshiftMiddlewareApi:\n    Description: \"API Gateway endpoint URL for Prod stage for SQL Statement function\"\n    Value: !Sub \"https://${RedshiftMiddlewareApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/sql_statement/\"\n  RedshiftMiddlewareFunction:\n    Description: \"SQL Statement Lambda Function ARN\"\n    Value: !GetAtt RedshiftMiddlewareFunction.Arn\n  RedshiftMiddlewareFunctionIamRole:\n    Description: \"Implicit IAM Role created for SQL Statement function\"\n    Value: !GetAtt RedshiftMiddlewareFunctionRole.Arn\n  CognitoUserPoolArn:\n    Description: \"ARN of the Cognito User Pool\"\n    Value: !GetAtt MyCognitoUserPool.Arn\n```\n\n----------------------------------------\n\nTITLE: Extracting Final Transcript from API Response (Python)\nDESCRIPTION: Retrieves the final processed text content from the OpenAI API `response` object generated by the `product_assistant`. It accesses the `content` field (`response.choices[0].message.content`) similar to the punctuation step. This final version of the transcript, with corrected financial terms, is stored in the `final_transcript` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Extract the final transcript from the model's response\nfinal_transcript = response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Add Punctuation using OpenAI Chat (Python)\nDESCRIPTION: Defines a function `punctuation_assistant` that uses the OpenAI Chat Completions API (`gpt-3.5-turbo`) to add punctuation and formatting to a given transcript. It sends the transcript along with a system prompt instructing the model to only add necessary punctuation (periods, commas, capitalization, symbols) and formatting, preserving the original words. The function returns the full API response object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Define function to add punctuation\ndef punctuation_assistant(ascii_transcript):\n\n    system_prompt = \"\"\"You are a helpful assistant that adds punctuation to text.\n      Preserve the original words and only insert necessary punctuation such as periods,\n     commas, capialization, symbols like dollar sings or percentage signs, and formatting.\n     Use only the context provided. If there is no context provided say, 'No context provided'\\n\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": ascii_transcript\n            }\n        ]\n    )\n    return response\n```\n\n----------------------------------------\n\nTITLE: Evaluating Summaries Concurrently using ThreadPoolExecutor in Python\nDESCRIPTION: This Python snippet utilizes `concurrent.futures.ThreadPoolExecutor` to concurrently execute the `evaluate_summaries` function on each row of a pandas DataFrame (`df`). It submits tasks for evaluation, tracks progress using `tqdm`, retrieves the results (`simple_evaluation`, `complex_evaluation`) as they complete, and updates the corresponding rows in the DataFrame. This approach efficiently handles potentially time-consuming evaluations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Use ThreadPoolExecutor to evaluate itineraries concurrently\nwith ThreadPoolExecutor() as executor:\n    futures = {executor.submit(evaluate_summaries, row): index for index, row in df.iterrows()}\n    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Evaluating Summaries\"):\n        index = futures[future]\n        simple_evaluation, complex_evaluation = future.result()\n        df.at[index, 'simple_evaluation'] = simple_evaluation\n        df.at[index, 'complex_evaluation'] = complex_evaluation\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Perform a content-based vector search for Scottish battles\nDESCRIPTION: Queries the index with the phrase 'Famous battles in Scottish history' based on content vectors, and prints the results with titles and similarity distances.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# This time we'll query using content vector\nquery_result = query_tair(client=client, query=\"Famous battles in Scottish history\", vector_name=\"content_vector\")\nfor i in range(len(query_result)):\n    title = client.tvs_hmget(index+\"_\"+\"content_vector\", query_result[i][0].decode('utf-8'), \"title\")\n    print(f\"{i + 1}. {title[0].decode('utf-8')} (Distance: {round(query_result[i][1],3)})\")\n```\n\n----------------------------------------\n\nTITLE: Loading, Processing, and Displaying Input Data\nDESCRIPTION: This code snippet loads the dataset from a CSV file, processes it using the `process_input_data` function, and then displays the first few rows of the processed dataframe. This includes reading the CSV, preprocessing it by renaming columns and filtering rows based on labels, and then displaying a preview of the data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# load data\ndf = pd.read_csv(local_dataset_path)\n\n# process input data\ndf = process_input_data(df)  # this demonstrates training data containing only positives\n\n# view data\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Comparing Message Object Structure in v1 vs v2 (JSON)\nDESCRIPTION: Compares the Message object structure in v1 and v2 betas. V1 uses a simple `file_ids` array to associate files with a message. V2 replaces this with an `attachments` array, where each attachment specifies a `file_id` and the `tools` (e.g., `file_search`, `code_interpreter`) it should be associated with. Adding attachments implicitly updates the Thread's `tool_resources`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/migration.txt#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"msg_abc123\",\n  \"object\": \"thread.message\",\n  \"created_at\": 1698983503,\n  \"thread_id\": \"thread_abc123\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"value\": \"Hi! How can I help you today?\",\n        \"annotations\": []\n      }\n    }\n  ],\n  \"assistant_id\": \"asst_abc123\",\n  \"run_id\": \"run_abc123\",\n  \"metadata\": {},\n  \"file_ids\": []\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"msg_abc123\",\n  \"object\": \"thread.message\",\n  \"created_at\": 1698983503,\n  \"thread_id\": \"thread_abc123\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"value\": \"Hi! How can I help you today?\",\n        \"annotations\": []\n      }\n    }\n  ],\n  \"assistant_id\": \"asst_abc123\",\n  \"run_id\": \"run_abc123\",\n  \"metadata\": {},\n  \"attachments\": [\n    {\n      \"file_id\": \"file-123\",\n      \"tools\": [\n        { \"type\": \"file_search\" },\n        { \"type\": \"code_interpreter\" }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Chat Completion and Tooling in Python\nDESCRIPTION: Installs required Python packages such as scipy, tenacity, tiktoken, termcolor, and the OpenAI SDK needed to interact with the Chat Completions API, manage retries, and enhance console output. This snippet should be executed prior to importing other modules and running the chat completion logic.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install scipy --quiet\n!pip install tenacity --quiet\n!pip install tiktoken --quiet\n!pip install termcolor --quiet\n!pip install openai --quiet\n```\n\n----------------------------------------\n\nTITLE: Providing openaiFileIdRefs as File Metadata in API Calls - JSON\nDESCRIPTION: This JSON example shows the structure of the openaiFileIdRefs parameter used during file uploads in POST requests. Each element is a JSON object with fields for file name, id, MIME type, and a download_link, as required by the OpenAI file upload protocol. It enables the API to process incoming user or DALLÂ·E-created files by referencing their metadata rather than raw contents. Inputs are expected to have valid URLs and corresponding MIME types; outputs depend on endpoint specifications. All referenced files expire after 5 minutes for security.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"name\": \"dalle-Lh2tg7WuosbyR9hk\",\n    \"id\": \"file-XFlOqJYTPBPwMZE3IopCBv1Z\",\n    \"mime_type\": \"image/webp\",\n    \"download_link\": \"https://files.oaiusercontent.com/file-XFlOqJYTPBPwMZE3IopCBv1Z?se=2024-03-11T20%3A29%3A52Z&sp=r&sv=2021-08-06&sr=b&rscc=max-age%3D31536000%2C%20immutable&rscd=attachment%3B%20filename%3Da580bae6-ea30-478e-a3e2-1f6c06c3e02f.webp&sig=ZPWol5eXACxU1O9azLwRNgKVidCe%2BwgMOc/TdrPGYII%3D\"\n  },\n  {\n    \"name\": \"2023 Benefits Booklet.pdf\",\n    \"id\": \"file-s5nX7o4junn2ig0J84r8Q0Ew\",\n    \"mime_type\": \"application/pdf\",\n    \"download_link\": \"https://files.oaiusercontent.com/file-s5nX7o4junn2ig0J84r8Q0Ew?se=2024-03-11T20%3A29%3A52Z&sp=r&sv=2021-08-06&sr=b&rscc=max-age%3D299%2C%20immutable&rscd=attachment%3B%20filename%3D2023%2520Benefits%2520Booklet.pdf&sig=Ivhviy%2BrgoyUjxZ%2BingpwtUwsA4%2BWaRfXy8ru9AfcII%3D\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Executing Qdrant Search using Title Vector\nDESCRIPTION: This Python code calls the `query_qdrant` function with a sample query ('modern art in Europe') and the 'Articles' collection. It implicitly uses the default `vector_name='title'` for search and then iterates through the results, printing the title and score of each retrieved article.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquery_results = query_qdrant(\"modern art in Europe\", \"Articles\")\nfor i, article in enumerate(query_results):\n    print(f\"{i + 1}. {article.payload['title']} (Score: {round(article.score, 3)})\")\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Images with OpenAI Vision (Python)\nDESCRIPTION: Shows how to send multiple image URLs along with a single text prompt in a chat completion request using the OpenAI Python library. The 'content' array in the user message contains objects for the text and each image URL. Requires the OpenAI Python library and an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What are in these images? Is there any difference between them?\",\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n          },\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\nprint(response.choices[0])\n```\n\n----------------------------------------\n\nTITLE: Detailed Text Summarization Utility with Recursive Option in Python\nDESCRIPTION: This Python function summarizes a large string of text by splitting it into chunks based on a user-controlled 'detail' parameter, then summarizing each chunk individually or recursively using the provided model. It accepts parameters including the input text, detail level (0 to 1), language model name, additional instructions for the model, minimum chunk size, chunk delimiter, and flags for recursive summarization and verbosity. The function computes the number of chunks by interpolating between minimum and maximum chunk counts, tokenizes and splits the text accordingly, then iteratively generates summaries for chunks using a chat completion API (e.g., OpenAI). When recursive summarization is enabled, each chunk summary is informed by previous summaries to improve coherence at the expense of additional computational cost. The output is a compiled string combining all partial summaries. Dependencies include tokenization functions, a chunking utility named `chunk_on_delimiter`, and a chat completion method `get_chat_completion`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef summarize(text: str,\n              detail: float = 0,\n              model: str = 'gpt-4-turbo',\n              additional_instructions: Optional[str] = None,\n              minimum_chunk_size: Optional[int] = 500,\n              chunk_delimiter: str = \".\",\n              summarize_recursively=False,\n              verbose=False):\n    \"\"\"\n    Summarizes a given text by splitting it into chunks, each of which is summarized individually. \n    The level of detail in the summary can be adjusted, and the process can optionally be made recursive.\n\n    Parameters:\n    - text (str): The text to be summarized.\n    - detail (float, optional): A value between 0 and 1 indicating the desired level of detail in the summary.\n      0 leads to a higher level summary, and 1 results in a more detailed summary. Defaults to 0.\n    - model (str, optional): The model to use for generating summaries. Defaults to 'gpt-3.5-turbo'.\n    - additional_instructions (Optional[str], optional): Additional instructions to provide to the model for customizing summaries.\n    - minimum_chunk_size (Optional[int], optional): The minimum size for text chunks. Defaults to 500.\n    - chunk_delimiter (str, optional): The delimiter used to split the text into chunks. Defaults to \".\".\n    - summarize_recursively (bool, optional): If True, summaries are generated recursively, using previous summaries for context.\n    - verbose (bool, optional): If True, prints detailed information about the chunking process.\n\n    Returns:\n    - str: The final compiled summary of the text.\n\n    The function first determines the number of chunks by interpolating between a minimum and a maximum chunk count based on the `detail` parameter. \n    It then splits the text into chunks and summarizes each chunk. If `summarize_recursively` is True, each summary is based on the previous summaries, \n    adding more context to the summarization process. The function returns a compiled summary of all chunks.\n    \"\"\"\n\n    # check detail is set correctly\n    assert 0 <= detail <= 1\n\n    # interpolate the number of chunks based to get specified level of detail\n    max_chunks = len(chunk_on_delimiter(text, minimum_chunk_size, chunk_delimiter))\n    min_chunks = 1\n    num_chunks = int(min_chunks + detail * (max_chunks - min_chunks))\n\n    # adjust chunk_size based on interpolated number of chunks\n    document_length = len(tokenize(text))\n    chunk_size = max(minimum_chunk_size, document_length // num_chunks)\n    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n    if verbose:\n        print(f\"Splitting the text into {len(text_chunks)} chunks to be summarized.\")\n        print(f\"Chunk lengths are {[len(tokenize(x)) for x in text_chunks]}\")\n\n    # set system message\n    system_message_content = \"Rewrite this text in summarized form.\"\n    if additional_instructions is not None:\n        system_message_content += f\"\\n\\n{additional_instructions}\"\n\n    accumulated_summaries = []\n    for chunk in tqdm(text_chunks):\n        if summarize_recursively and accumulated_summaries:\n            # Creating a structured prompt for recursive summarization\n            accumulated_summaries_string = '\\n\\n'.join(accumulated_summaries)\n            user_message_content = f\"Previous summaries:\\n\\n{accumulated_summaries_string}\\n\\nText to summarize next:\\n\\n{chunk}\"\n        else:\n            # Directly passing the chunk for summarization without recursive context\n            user_message_content = chunk\n\n        # Constructing messages based on whether recursive summarization is applied\n        messages = [\n            {\"role\": \"system\", \"content\": system_message_content},\n            {\"role\": \"user\", \"content\": user_message_content}\n        ]\n\n        # Assuming this function gets the completion and works as expected\n        response = get_chat_completion(messages, model=model)\n        accumulated_summaries.append(response)\n\n    # Compile final summary from partial summaries\n    final_summary = '\\n\\n'.join(accumulated_summaries)\n\n    return final_summary\n```\n\n----------------------------------------\n\nTITLE: Defining Atlas Vector Search Index Configuration in JSON\nDESCRIPTION: Provides a JSON configuration schema for creating an Atlas Vector Search index, specifically for a `knnVector` field named \"embedding\" with 1536 dimensions using `dotProduct` similarity. This configuration is used either in the Atlas UI or programmatically to enable vector search on the specified field.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mappings\": {\n    \"dynamic\": true,\n    \"fields\": {\n      \"embedding\": {\n        \"dimensions\": 1536,\n        \"similarity\": \"dotProduct\",\n        \"type\": \"knnVector\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initial Query Rewriting Prompt (example-chat)\nDESCRIPTION: An example LLM prompt designed to take the last user query and the preceding conversation history, then rewrite the query to be self-contained by incorporating necessary context from the history. It expects JSON-formatted input representing the conversation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_0\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: Given the previous conversation, re-write the last user query so it contains\nall necessary context.\n\n# Example\nHistory: [{user: \"What is your return policy?\"},{assistant: \"...\"}]\nUser Query: \"How long does it cover?\"\nResponse: \"How long does the return policy cover?\"\n\n# Conversation\n[last 3 messages of conversation]\n\n# User Query\n[last user query]\n\nUSER: [JSON-formatted input conversation here]\n```\n\n----------------------------------------\n\nTITLE: Uploading Document Pages to Pinecone with Metadata in Python\nDESCRIPTION: A loop that processes each row in a dataframe containing document pages, creates metadata for each page including its text and visual information, and uploads them to Pinecone using the upsert_vector function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor idx, row in tqdm(df.iterrows(), total=df.shape[0], desc='Uploading to Pinecone'):\n    pageNumber = row['PageNumber']\n\n    # Create meta-data tags to be added to Pinecone \n    metadata = {\n        'pageId': f\"{document_id}-{pageNumber}\",\n        'pageNumber': pageNumber,\n        'text': row['PageText'],\n        'ImagePath': row['ImagePath'],\n        'GraphicIncluded': row['Visual_Input_Processed']\n    }\n\n    upsert_vector(metadata['pageId'], row['Embeddings'], metadata)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Realtime Clients for Multiple Languages in React/JS\nDESCRIPTION: Initializes multiple `RealtimeClient` instances using React's `useRef` hook, one for each language defined in `languageConfigs`. It uses `reduce` to create a map of clients keyed by language code and stores them in a ref. It then updates the `languageConfigs` array to include references to these clients. Note the use of `dangerouslyAllowAPIKeyInBrowser` for demo purposes; production applications should use ephemeral keys.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst clientRefs = useRef(\n    languageConfigs.reduce((acc, { code }) => {\n      acc[code] = new RealtimeClient({\n        apiKey: OPENAI_API_KEY,\n        dangerouslyAllowAPIKeyInBrowser: true,\n      });\n      return acc;\n    }, {} as Record<string, RealtimeClient>)\n  ).current;\n\n  // Update languageConfigs to include client references\n  const updatedLanguageConfigs = languageConfigs.map(config => ({\n    ...config,\n    clientRef: { current: clientRefs[config.code] }\n  }));\n```\n\n----------------------------------------\n\nTITLE: Creating Thread and Run (Python)\nDESCRIPTION: This snippet creates a new thread and initiates a run using the OpenAI Assistants API. It requires a pre-existing thread and a query string. It creates a thread and submits a message using the provided query. The run is then waited upon to finish, and the response is printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nthread, run = create_thread_and_run(\n    \"Generate the first 20 fibbonaci numbers with code.\"\n)\nrun = wait_on_run(run, thread)\npretty_print(get_response(thread))\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Embedding Model - Python\nDESCRIPTION: Imports necessary Python libraries, including `openai`, `pandas`, `numpy`, `os`, `wget`, `clickhouse_connect`, and `ast`.  It also defines the embedding model (`EMBEDDING_MODEL`) to be used, setting it to \"text-embedding-3-small\" by default.  Additionally, it includes code to suppress SSL and deprecation warnings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nfrom typing import List, Iterator\nimport pandas as pd\nimport numpy as np\nimport os\nimport wget\nfrom ast import literal_eval\n\n# MyScale's client library for Python\nimport clickhouse_connect\n\n# I've set this to our new embeddings model, this can be changed to the embedding model of your choice\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n```\n\n----------------------------------------\n\nTITLE: Running Batch LLM Evaluations with Braintrust Eval in Python\nDESCRIPTION: Defines an asynchronous wrapper function `task` that prepares input data for the `classifier` function, aligning with the structure expected by Braintrust's `Eval`. It then demonstrates executing a batch evaluation using `Eval` by providing the dataset (`data`), the task function, scoring metrics (`scores=[normalized_diff]`), experiment details, and concurrency control (`max_concurrency=10`). This setup allows for systematic evaluation of the classifier across multiple data points. Requires the `Eval` component (likely from `braintrust`), the previously defined `classifier` function, and a scoring function like `normalized_diff`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def task(input):\n    return await classifier(\n        input=input[\"question\"],\n        output=input[\"generated_answer\"],\n        expected=input[\"expected_answer\"],\n    )\n\n\nawait Eval(\n    \"LLM-as-a-judge\",\n    data=data,\n    task=task,\n    scores=[normalized_diff],\n    experiment_name=\"Classifier\",\n    max_concurrency=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Testing the Question Answer System with Example Queries in Python\nDESCRIPTION: Demonstrates usage examples by calling the answer_question function with various sample questions. These test queries show how the system responds with either factual answers drawn from embedded knowledge or \"I don't know\" if relevant information is unavailable. It highlights the system's use as a prototype with embeddings loaded from CSV and notes the practical consideration of using vector databases for scalability in production.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nanswer_question(df, question=\"What day is it?\", debug=False)\n\nanswer_question(df, question=\"What is our newest embeddings model?\")\n\nanswer_question(df, question=\"What is ChatGPT?\")\n```\n\n----------------------------------------\n\nTITLE: Error Handling for OpenAI Image Variation (Node.js)\nDESCRIPTION: This code snippet demonstrates how to implement error handling when calling the OpenAI image variation API. It uses a `try...catch` block to catch potential errors during the API call. If an error occurs, it checks if the error object contains a `response` property, indicating an HTTP error. If so, it logs the status code and data from the response. Otherwise, it logs the error message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-node-tips.txt#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = new OpenAI();\n\nasync function main() {\n    try {\n        const image = await openai.images.createVariation({\n            image: fs.createReadStream(\"image.png\"),\n            n: 1,\n            size: \"1024x1024\",\n        });\n        console.log(image.data);\n    } catch (error) {\n        if (error.response) {\n            console.log(error.response.status);\n            console.log(error.response.data);\n        } else {\n            console.log(error.message);\n        }\n    }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Recommended Formats for Long Context Input (Text/Custom)\nDESCRIPTION: Presents two recommended formats for providing numerous documents or files as input context, particularly effective in long context scenarios based on testing. The first uses XML tags with `id` and `title` attributes, while the second uses a pipe-delimited format with `ID`, `TITLE`, and `CONTENT` keys.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_12\n\nLANGUAGE: XML\nCODE:\n```\n<doc id=1 title=â€The Foxâ€>The quick brown fox jumps over the lazy dog</doc>\n```\n\nLANGUAGE: Text\nCODE:\n```\nID: 1 | TITLE: The Fox | CONTENT: The quick brown fox jumps over the lazy dog\n```\n\n----------------------------------------\n\nTITLE: Issue Validation Function - Python\nDESCRIPTION: Defines a function `validate_issue` that uses a chat completion model to compare a model-generated justification for a data issue with the correct reason. It determines if they address the same underlying issue and returns 'True' or 'False'. The `client` object is assumed to be pre-defined for interacting with the chat API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef validate_issue(model_generated_answer, correct_answer):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\nYou are a medical expert assistant designed to validate the quality of an LLM-generated answer.\n\nThe model was asked to review a medical dataset row to determine if the data is valid. If the data is not valid, it should provide a justification explaining why.\n\nYour task:\n\n    â€¢\tCompare the model-generated justification with the correct reason provided.\n    â€¢\tDetermine if they address the same underlying medical issue or concern, even if phrased differently.\n    â€¢\tFocus on the intent, medical concepts, and implications rather than exact wording.\n\nInstructions:\n\n    â€¢\tIf the justifications have the same intent or address the same medical issue, return True.\n    â€¢\tIf they address different issues or concerns, return False.\n    â€¢\tOnly respond with a single word: True or False.\n\nExamples:\n\n    1.\tExample 1:\n    â€¢\tModel Generated Response: â€œThe patient is allergic to penicillinâ€\n    â€¢\tCorrect Response: â€œThe patient was prescribed penicillin despite being allergicâ€\n    â€¢\tAnswer: True\n    2.\tExample 2:\n    â€¢\tModel Generated Response: â€œThe date of birth of the patient is incorrectâ€\n    â€¢\tCorrect Response: â€œThe patient was prescribed penicillin despite being allergicâ€\n    â€¢\tAnswer: False\n\n\nModel Generated Response: {model_generated_answer}\nCorrect Response:  {correct_answer}\n            \"\"\"\n        }\n    ]\n\n    response = client.chat.completions.create(\n        model=\"o1-preview\",\n        messages=messages\n    )\n\n    result = response.choices[0].message.content\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Extracting Action Items from Meeting Transcript with GPT-4 - Python\nDESCRIPTION: This function, 'action_item_extraction', identifies actionable tasks or assignments from a meeting transcript using GPT-4 via OpenAI's chat completion endpoint. The prompt is focused on action extraction. Dependencies include OpenAI API access; input is a transcript, and output is a list of action items. Function calling or integration with task management tools is not shown here, but can be built atop this method.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef action_item_extraction(transcription):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an AI expert in analyzing conversations and extracting action items. Please review the text and identify any tasks, assignments, or actions that were agreed upon or mentioned as needing to be done. These could be tasks assigned to specific individuals, or general actions that the group has decided to take. Please list these action items clearly and concisely.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Test Prompts for Moderation Workflow in Python\nDESCRIPTION: This Python code demonstrates testing the `execute_all_moderations` function. It creates a list `tests` containing various predefined request strings (`good_request`, `bad_request`, `interesting_request`). It then loops through this list, printing each request, executing the asynchronous moderation workflow for it, and printing the final result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntests = [good_request, bad_request, interesting_request]\n\nfor test in tests:\n    print(test)\n    result = await execute_all_moderations(test)\n    print(result)\n    print('\\n\\n')\n```\n\n----------------------------------------\n\nTITLE: Converting Logprobs to Probabilities and Creating DataFrame in Python\nDESCRIPTION: Transforms the output list into a DataFrame, converts logprobs to probabilities using the exp() function, and adds a column to represent the probability of relevance (yes_probability) for ranking purposes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\noutput_df = pd.DataFrame(\n    output_list, columns=[\"query\", \"document\", \"prediction\", \"logprobs\"]\n).reset_index()\n# Use exp() to convert logprobs into probability\noutput_df[\"probability\"] = output_df[\"logprobs\"].apply(exp)\n# Reorder based on likelihood of being Yes\noutput_df[\"yes_probability\"] = output_df.apply(\n    lambda x: x[\"probability\"] * -1 + 1\n    if x[\"prediction\"] == \"No\"\n    else x[\"probability\"],\n    axis=1,\n)\noutput_df.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Streaming Chat Completion with Azure OpenAI\nDESCRIPTION: Shows how to create a streaming chat completion using Azure OpenAI. Similar to the basic completion but with stream=True, processing and displaying response chunks as they arrive rather than waiting for the complete response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=deployment,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Orange.\"},\n    ],\n    temperature=0,\n    stream=True\n)\n\nfor chunk in response:\n    if len(chunk.choices) > 0:\n        delta = chunk.choices[0].delta\n\n        if delta.role:\n            print(delta.role + \": \", end=\"\", flush=True)\n        if delta.content:\n            print(delta.content, end=\"\", flush=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Detect Leading Silence in Audio (Python)\nDESCRIPTION: Defines a function `milliseconds_until_sound` that identifies the duration of leading silence in a `pydub.AudioSegment`. It takes the audio segment, a silence threshold in decibels (`silence_threshold_in_decibels`), and a chunk size (`chunk_size`) as input. The function iteratively checks the average decibels (dBFS) of small audio chunks from the beginning until it finds a chunk louder than the threshold or reaches the end of the audio, returning the total duration of silence in milliseconds.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Function to detect leading silence\n# Returns the number of milliseconds until the first sound (chunk averaging more than X decibels)\ndef milliseconds_until_sound(sound, silence_threshold_in_decibels=-20.0, chunk_size=10):\n    trim_ms = 0  # ms\n\n    assert chunk_size > 0  # to avoid infinite loop\n    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold_in_decibels and trim_ms < len(sound):\n        trim_ms += chunk_size\n\n    return trim_ms\n```\n\n----------------------------------------\n\nTITLE: Getting the fine-tuned model ID\nDESCRIPTION: Retrieves the ID of the completed fine-tuned model to use for inference, with error handling for incomplete jobs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.fine_tuning.jobs.retrieve(job_id)\nfine_tuned_model_id = response.fine_tuned_model\n\nif fine_tuned_model_id is None:\n    raise RuntimeError(\n        \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"\n    )\n\nprint(\"Fine-tuned model ID:\", fine_tuned_model_id)\n```\n\n----------------------------------------\n\nTITLE: Testing OpenAI API Key Configuration\nDESCRIPTION: This code snippet checks if the OpenAI API key is defined as an environment variable. If not, it prompts the user to enter the key and optionally sets it as an environment variable for the current session. This ensures the API key is available for use in subsequent operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Test to ensure that your OpenAI API key is defined as an environment variable or provide it when prompted\n# If you run this notebook locally, you may have to reload the terminal and the notebook to make the environment available\n\nimport os\nfrom getpass import getpass\n\n# Check if OPENAI_API_KEY is set as an environment variable\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"Your OPENAI_API_KEY is ready\")\nelse:\n    # If not, prompt for it\n    api_key = getpass(\"Enter your OPENAI_API_KEY: \")\n    if api_key:\n        print(\"Your OPENAI_API_KEY is now available for this session\")\n        # Optionally, you can set it as an environment variable for the current session\n        os.environ[\"OPENAI_API_KEY\"] = api_key\n    else:\n        print(\"You did not enter your OPENAI_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Defining Tools and Instructions for Customer Service Agent in Python\nDESCRIPTION: Defines the `tools` list containing specifications for two functions (`speak_to_user`, `get_instructions`) that the OpenAI model can call, including their descriptions and parameters. Also defines the `INSTRUCTIONS` list, providing structured guidance based on problem types (fraud, refund, information) for the agent to follow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The tools our customer service LLM will use to communicate\ntools = [\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"speak_to_user\",\n    \"description\": \"Use this to speak to the user to give them information and to ask for anything required for their case.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"message\": {\n          \"type\": \"string\",\n          \"description\": \"Text of message to send to user. Can cover multiple topics.\"\n        }\n      },\n      \"required\": [\"message\"]\n    }\n  }\n},\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"get_instructions\",\n    \"description\": \"Used to get instructions to deal with the user's problem.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"problem\": {\n          \"type\": \"string\",\n          \"enum\": [\"fraud\",\"refund\",\"information\"],\n          \"description\": \"\"\"The type of problem the customer has. Can be one of:\\n          - fraud: Required to report and resolve fraud.\\n          - refund: Required to submit a refund request.\\n          - information: Used for any other informational queries.\"\"\"\n        }\n      },\n      \"required\": [\n        \"problem\"\n      ]\n    }\n  }\n}\n]\n\n# Example instructions that the customer service assistant can consult for relevant customer problems\nINSTRUCTIONS = [ {\"type\": \"fraud\",\n                  \"instructions\": \"\"\"â€¢ Ask the customer to describe the fraudulent activity, including the the date and items involved in the suspected fraud.\nâ€¢ Offer the customer a refund.\nâ€¢ Report the fraud to the security team for further investigation.\nâ€¢ Thank the customer for contacting support and invite them to reach out with any future queries.\"\"\"},\n                {\"type\": \"refund\",\n                 \"instructions\": \"\"\"â€¢ Confirm the customer's purchase details and verify the transaction in the system.\nâ€¢ Check the company's refund policy to ensure the request meets the criteria.\nâ€¢ Ask the customer to provide a reason for the refund.\nâ€¢ Submit the refund request to the accounting department.\nâ€¢ Inform the customer of the expected time frame for the refund processing.\nâ€¢ Thank the customer for contacting support and invite them to reach out with any future queries.\"\"\"},\n                {\"type\": \"information\",\n                 \"instructions\": \"\"\"â€¢ Greet the customer and ask how you can assist them today.\nâ€¢ Listen carefully to the customer's query and clarify if necessary.\nâ€¢ Provide accurate and clear information based on the customer's questions.\nâ€¢ Offer to assist with any additional questions or provide further details if needed.\nâ€¢ Ensure the customer is satisfied with the information provided.\nâ€¢ Thank the customer for contacting support and invite them to reach out with any future queries.\"\"\" }]\n```\n\n----------------------------------------\n\nTITLE: Generating Textual Training Data in a Loop with OpenAI API in Python\nDESCRIPTION: Generates structured textual data for fine-tuning a GPT model using the OpenAI API (`gpt-4o-mini`). It runs the API call within a loop (3 iterations) to create multiple input-output pairs. The prompt explicitly defines the desired format (`Input: product_name, category\\nOutput: description`) and instructs the model not to deviate, aiming for easy parsing. Results are accumulated in `output_string`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutput_string = \"\"\nfor i in range(3):\n  question = f\"\"\"\n  I am creating input output training pairs to fine tune my gpt model. The usecase is a retailer generating a description for a product from a product catalogue. I want the input to be product name and category (to which the product belongs to) and output to be description.\n  The format should be of the form:\n  1.\n  Input: product_name, category\n  Output: description\n  2.\n  Input: product_name, category\n  Output: description\n\n  Do not add any extra characters around that formatting as it will make the output parsing break.\n  Create as many training pairs as possible.\n  \"\"\"\n\n  response = client.chat.completions.create(\n    model=datagen_model,\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n      {\"role\": \"user\", \"content\": question}\n    ]\n  )\n  res = response.choices[0].message.content\n  output_string += res + \"\\n\" + \"\\n\"\nprint(output_string[:1000]) #displaying truncated response\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Python Code Sandbox - Python\nDESCRIPTION: This Python code snippet invokes a shell command to build a Docker image named 'python_sandbox:latest' from the './resources/docker' directory. The output of the Docker build command is piped to grep to display either the build details or errors. If the build fails, it prints a custom failure message. Dependencies: Docker must be installed and the specified build context must include a valid dockerfile and requirements.txt. No input parameters are needed; successful execution outputs the Docker build results or an error message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/object_oriented_agentic_approach/Secure_code_interpreter_tool_for_LLM_agents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!docker build -t python_sandbox:latest ./resources/docker 2>&1 | grep -E \"View build details|ERROR\" || echo \"Build failed.\"\n```\n\n----------------------------------------\n\nTITLE: Creating Atlas Vector Search Index Programmatically with PyMongo\nDESCRIPTION: Demonstrates how to create an Atlas Vector Search index using the `create_search_index` method of the PyMongo collection object. It uses the index definition adapted for the Python driver's syntax, specifying the index name and the target embedding field. This requires a recent PyMongo driver and MongoDB 7.0+ Atlas cluster.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncollection.create_search_index(\n    {\"definition\":\n        {\"mappings\": {\"dynamic\": True, \"fields\": {\n            EMBEDDING_FIELD_NAME : {\n                \"dimensions\": 1536,\n                \"similarity\": \"dotProduct\",\n                \"type\": \"knnVector\"\n                }}}},\n     \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME\n    }\n)\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating LLM on Straightforward Prompts (Python)\nDESCRIPTION: This Python code calls an `eval` function to assess the performance of the 'gpt-3.5-turbo' model. It passes the drone-specific system prompt (`DRONE_SYSTEM_PROMPT`), the list of available functions (`function_list`), and the dictionary mapping straightforward prompts to expected function names (`straightforward_prompts_to_expected`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate the model with the given prompts\neval(\n    model=\"gpt-3.5-turbo\",\n    system_prompt=DRONE_SYSTEM_PROMPT,\n    function_list=function_list,\n    prompts_to_expected_tool_name=straightforward_prompts_to_expected,\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAPI 3.1 Schema Defining Jira Cloud API Endpoints in YAML\nDESCRIPTION: This OpenAPI 3.1 schema defines the Jira Cloud API structure for actions related to issues and sub-tasks. It includes server information with a placeholder for <CLOUD_ID>, OAuth2 security schemes with scopes for reading and writing Jira work and user data, and schema objects for Issue and related properties. The paths specify HTTP methods for searching issues, creating issues and sub-tasks, retrieving issue details, and updating issues. Expected request parameters, request bodies, and HTTP responses (including status codes and response content) are well-defined to facilitate API client generation and validation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_jira.ipynb#_snippet_1\n\nLANGUAGE: openapi\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Jira API\n  description: API for interacting with Jira issues and sub-tasks.\n  version: 1.0.0\nservers:\n  - url: https://api.atlassian.com/ex/jira/<CLOUD_ID>/rest/api/3\n    description: Jira Cloud API\ncomponents:\n  securitySchemes:\n    OAuth2:\n      type: oauth2\n      flows:\n        authorizationCode:\n          authorizationUrl: https://auth.atlassian.com/authorize\n          tokenUrl: https://auth.atlassian.com/oauth/token\n          scopes:\n            read:jira-user: Read Jira user information\n            read:jira-work: Read Jira work data\n            write:jira-work: Write Jira work data\n  schemas:\n    Issue:\n      type: object\n      properties:\n        id:\n          type: string\n        key:\n          type: string\n        fields:\n          type: object\n          properties:\n            summary:\n              type: string\n            description:\n              type: string\n            issuetype:\n              type: object\n              properties:\n                name:\n                  type: string\npaths:\n  /search:\n    get:\n      operationId: getIssues\n      summary: Retrieve a list of issues\n      parameters:\n        - name: jql\n          in: query\n          required: false\n          schema:\n            type: string\n        - name: startAt\n          in: query\n          required: false\n          schema:\n            type: integer\n        - name: maxResults\n          in: query\n          required: false\n          schema:\n            type: integer\n      responses:\n        '200':\n          description: A list of issues\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  issues:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/Issue'\n  /issue:\n    post:\n      operationId: createIssue\n      summary: Create a new issue\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                fields:\n                  type: object\n                  properties:\n                    project:\n                      type: object\n                      properties:\n                        key:\n                          type: string\n                    summary:\n                      type: string\n                    description:\n                      type: string\n                    issuetype:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n      responses:\n        '201':\n          description: Issue created successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Issue'\n  /issue/{issueIdOrKey}:\n    get:\n      operationId: getIssue\n      summary: Retrieve a specific issue\n      parameters:\n        - name: issueIdOrKey\n          in: path\n          required: true\n          schema:\n            type: string\n      responses:\n        '200':\n          description: Issue details\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Issue'\n    put:\n      operationId: updateIssue\n      summary: Update an existing issue\n      parameters:\n        - name: issueIdOrKey\n          in: path\n          required: true\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                fields:\n                  type: object\n                  properties:\n                    summary:\n                      type: string\n                    description:\n                      type: string\n                    issuetype:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n      responses:\n        '204':\n          description: Issue updated successfully\n  /issue:\n    post:\n      operationId: createSubTask\n      summary: Create a sub-task for an issue\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                fields:\n                  type: object\n                  properties:\n                    project:\n                      type: object\n                      properties:\n                        key:\n                          type: string\n                    parent:\n                      type: object\n                      properties:\n                        key:\n                          type: string\n                    summary:\n                      type: string\n                    description:\n                      type: string\n                    issuetype:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n      responses:\n        '201':\n          description: Sub-task created successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Issue'\nsecurity:\n  - OAuth2:\n      - read:jira-user\n      - read:jira-work\n      - write:jira-work\n```\n\n----------------------------------------\n\nTITLE: Counting Author Occurrences in Dataset in Python\nDESCRIPTION: Uses `collections.Counter` to count the number of quotes for each author in the dataset and prints the total number of quotes and the counts per author, sorted by frequency.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nauthor_count = Counter(entry[\"author\"] for entry in philo_dataset)\nprint(f\"Total: {len(philo_dataset)} quotes. By author:\")\nfor author, count in author_count.most_common():\n    print(f\"    {author:<20}: {count} quotes\")\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Section Extraction\nDESCRIPTION: This code snippet defines several functions to process and extract sections from Wikipedia pages. It uses the `transformers` and `nltk` libraries for tokenization and sentence splitting, respectively. The `count_tokens` function counts tokens using the `GPT2TokenizerFast`. The `reduce_long` function truncates long text to a maximum token limit, potentially splitting at sentence boundaries. The `extract_sections` function extracts sections from a Wikipedia page, discarding irrelevant sections and accounting for section hierarchies. It returns a list of tuples containing title, heading, content, and token count. The `bermuda_page` and `ber` variable is for demonstrating and verifying the functionality.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom typing import Set\nfrom transformers import GPT2TokenizerFast\n\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    \"\"\"count the number of tokens in a string\"\"\"\n    return len(tokenizer.encode(text))\n\ndef reduce_long(\n    long_text: str, long_text_tokens: bool = False, max_len: int = 590\n) -> str:\n    \"\"\"\n    Reduce a long text to a maximum of `max_len` tokens by potentially cutting at a sentence end\n    \"\"\"\n    if not long_text_tokens:\n        long_text_tokens = count_tokens(long_text)\n    if long_text_tokens > max_len:\n        sentences = sent_tokenize(long_text.replace(\"\\n\", \" \"))\n        ntokens = 0\n        for i, sentence in enumerate(sentences):\n            ntokens += 1 + count_tokens(sentence)\n            if ntokens > max_len:\n                return \". \".join(sentences[:i]) + \".\"\n\n    return long_text\n\ndiscard_categories = ['See also', 'References', 'External links', 'Further reading', \"Footnotes\",\n    \"Bibliography\", \"Sources\", \"Citations\", \"Literature\", \"Footnotes\", \"Notes and references\",\n    \"Photo gallery\", \"Works cited\", \"Photos\", \"Gallery\", \"Notes\", \"References and sources\",\n    \"References and notes\",]\n\n\ndef extract_sections(\n    wiki_text: str,\n    title: str,\n    max_len: int = 1500,\n    discard_categories: Set[str] = discard_categories,\n) -> str:\n    \"\"\"\n    Extract the sections of a Wikipedia page, discarding the references and other low information sections\n    \"\"\"\n    if len(wiki_text) == 0:\n        return []\n\n    # find all headings and the corresponding contents\n    headings = re.findall(\"==+ .* ==+\", wiki_text)\n    for heading in headings:\n        wiki_text = wiki_text.replace(heading, \"==+ !! ==+\")\n    contents = wiki_text.split(\"==+ !! ==+\")\n    contents = [c.strip() for c in contents]\n    assert len(headings) == len(contents) - 1\n\n    cont = contents.pop(0).strip()\n    outputs = [(title, \"Summary\", cont, count_tokens(cont)+4)]\n    \n    # discard the discard categories, accounting for a tree structure\n    max_level = 100\n    keep_group_level = max_level\n    remove_group_level = max_level\n    nheadings, ncontents = [], []\n    for heading, content in zip(headings, contents):\n        plain_heading = \" \".join(heading.split(\" \")[1:-1])\n        num_equals = len(heading.split(\" \")[0])\n        if num_equals <= keep_group_level:\n            keep_group_level = max_level\n\n        if num_equals > remove_group_level:\n            if (\n                num_equals <= keep_group_level\n            ):\n                continue\n        keep_group_level = max_level\n        if plain_heading in discard_categories:\n            remove_group_level = num_equals\n            keep_group_level = max_level\n            continue\n        nheadings.append(heading.replace(\"=\", \"\").strip())\n        ncontents.append(content)\n        remove_group_level = max_level\n\n    # count the tokens of each section\n    ncontent_ntokens = [\n        count_tokens(c)\n        + 3\n        + count_tokens(\" \".join(h.split(\" \")[1:-1]))\n        - (1 if len(c) == 0 else 0)\n        for h, c in zip(nheadings, ncontents)\n    ]\n\n    # Create a tuple of (title, section_name, content, number of tokens)\n    outputs += [(title, h, c, t) if t<max_len \n                else (title, h, reduce_long(c, max_len), count_tokens(reduce_long(c,max_len))) \n                    for h, c, t in zip(nheadings, ncontents, ncontent_ntokens)]\n    \n    return outputs\n\n# Example page being processed into sections\nbermuda_page = get_wiki_page('Bermuda at the 2020 Summer Olympics')\nber = extract_sections(bermuda_page.content, bermuda_page.title)\n\n# Example section\nber[-1]\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to JSONL Format\nDESCRIPTION: Saving the prepared dataset as a JSONL file, which is the required format for OpenAI's fine-tuning API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.to_json(\"sport2.jsonl\", orient='records', lines=True)\n```\n\n----------------------------------------\n\nTITLE: Creating File Batch Python\nDESCRIPTION: This code snippet creates a batch of files for the vector store in Python environment. The code uses `create_and_poll` to handle the batch creation and ensure all the files are processed successfully. The code requires `vector_store_id` and a list of `file_ids` to add the files in a batch. The example demonstrates how files can be added in batches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nbatch = client.beta.vector_stores.file_batches.create_and_poll(\n  vector_store_id=\"vs_abc123\",\n  file_ids=['file_1', 'file_2', 'file_3', 'file_4', 'file_5']\n)\n```\n\n----------------------------------------\n\nTITLE: Executing AWS Redshift SQL Queries in Python AWS Lambda Middleware\nDESCRIPTION: This Python snippet defines an AWS Lambda function that connects to an AWS Redshift database using environment variables for connection parameters. It executes incoming SQL statements, fetches the results, writes them as CSV to a temporary file, encodes the file content in base64, and returns it as a JSON payload compatible with GPT Actions file retrieval. The snippet depends on the 'psycopg2' library for PostgreSQL connectivity and handles error reporting. Inputs include a JSON event with a 'sql_statement' string, and outputs include an HTTP JSON response with base64 encoded CSV data or error details.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport psycopg2\nimport os\nimport base64\nimport tempfile\nimport csv\n\n# Fetch Redshift credentials from environment variables\nhost = os.environ['REDSHIFT_HOST']\nport = os.environ['REDSHIFT_PORT']\nuser = os.environ['REDSHIFT_USER']\npassword = os.environ['REDSHIFT_PASSWORD']\ndatabase = os.environ['REDSHIFT_DB']\n\ndef execute_statement(sql_statement):\n    try:\n        # Establish connection\n        conn = psycopg2.connect(\n            host=host,\n            port=port,\n            user=user,\n            password=password,\n            dbname=database\n        )\n        cur = conn.cursor()\n        cur.execute(sql_statement)\n        conn.commit()\n\n        # Fetch all results\n        if cur.description:\n            columns = [desc[0] for desc in cur.description]\n            result = cur.fetchall()\n        else:\n            columns = []\n            result = []\n\n        cur.close()\n        conn.close()\n        return columns, result\n\n    except Exception as e:\n        raise Exception(f\"Database query failed: {str(e)}\")\n\ndef lambda_handler(event, context):\n    try:\n        data = json.loads(event['body'])\n        sql_statement = data['sql_statement']\n\n        # Execute the statement and fetch results\n        columns, result = execute_statement(sql_statement)\n        \n        # Create a temporary file to save the result as CSV\n        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.csv', newline='') as tmp_file:\n            csv_writer = csv.writer(tmp_file)\n            if columns:\n                csv_writer.writerow(columns)  # Write the header\n            csv_writer.writerows(result)  # Write all rows\n            tmp_file_path = tmp_file.name\n\n        # Read the file and encode its content to base64\n        with open(tmp_file_path, 'rb') as f:\n            file_content = f.read()\n            encoded_content = base64.b64encode(file_content).decode('utf-8')\n\n        response = {\n            'openaiFileResponse': [\n                {\n                    'name': 'query_result.csv',\n                    'mime_type': 'text/csv',\n                    'content': encoded_content\n                }\n            ]\n        }\n\n        return {\n            'statusCode': 200,\n            'headers': {\n                'Content-Type': 'application/json'\n            },\n            'body': json.dumps(response)\n        }\n\n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n\n```\n\n----------------------------------------\n\nTITLE: Creating RediSearch Index with Vector Fields - Python\nDESCRIPTION: This code block attempts to check if a RediSearch index with the specified `INDEX_NAME` already exists. If the index is not found (indicated by an exception), it proceeds to create the index using the list of `fields` defined previously and configures it to index documents stored as Redis Hashes (`IndexType.HASH`) with a specific `PREFIX`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Check if index exists\ntry:\n    redis_client.ft(INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n    )\n```\n\n----------------------------------------\n\nTITLE: Evaluating Translation Quality with BLEU and ROUGE Scores\nDESCRIPTION: This snippet shows how to evaluate the quality of a translation by comparing the re-translated English text with the original reference text using BLEU and ROUGE scores. It requires the sacrebleu and rouge-score packages to be installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Make sure scarebleu package is installed \nimport sacrebleu\n# Make sure rouge-score package is installed \nfrom rouge_score import rouge_scorer \n\n# We'll use the original English transcription as the reference text \nreference_text = english_transcript\n\ncandidate_text = re_translated_english_text\n\n# BLEU Score Evaluation\nbleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\nprint(f\"BLEU Score: {bleu.score}\")\n\n# ROUGE Score Evaluation\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nscores = scorer.score(reference_text, candidate_text)\nprint(f\"ROUGE-1 Score: {scores['rouge1'].fmeasure}\")\nprint(f\"ROUGE-L Score: {scores['rougeL'].fmeasure}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity and Finding Similar Items in Python\nDESCRIPTION: This code snippet defines two functions: `cosine_similarity_manual` and `find_similar_items`. The `cosine_similarity_manual` function calculates the cosine similarity between two vectors. The `find_similar_items` function takes an input embedding, a list of embeddings, a threshold, and a top_k value as input. It calculates the cosine similarity between the input embedding and all other embeddings, filters out similarities below the threshold, sorts the filtered similarities by similarity score, and returns the top-k most similar items.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity_manual(vec1, vec2):\n    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n    vec1 = np.array(vec1, dtype=float)\n    vec2 = np.array(vec2, dtype=float)\n\n\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    return dot_product / (norm_vec1 * norm_vec2)\n\n\ndef find_similar_items(input_embedding, embeddings, threshold=0.5, top_k=2):\n    \"\"\"Find the most similar items based on cosine similarity.\"\"\"\n    \n    # Calculate cosine similarity between the input embedding and all other embeddings\n    similarities = [(index, cosine_similarity_manual(input_embedding, vec)) for index, vec in enumerate(embeddings)]\n    \n    # Filter out any similarities below the threshold\n    filtered_similarities = [(index, sim) for index, sim in similarities if sim >= threshold]\n    \n    # Sort the filtered similarities by similarity score\n    sorted_indices = sorted(filtered_similarities, key=lambda x: x[1], reverse=True)[:top_k]\n\n    # Return the top-k most similar items\n    return sorted_indices\n```\n\n----------------------------------------\n\nTITLE: Testing Semantic Search with Completely Different Phrasing\nDESCRIPTION: Further tests the semantic search capability with an entirely different query phrasing about the same economic event. Demonstrates the system's ability to understand meaning beyond specific words used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Why was there a long-term economic downturn in the early 20th century?\"\n\n# create the query embedding\nxq = client.embeddings.create(input=query, model=MODEL).data[0].embedding\n\n# query, returning the top 5 most similar results\nres = index.query(vector=[xq], top_k=5, include_metadata=True)\n\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Variables (Python)\nDESCRIPTION: Initializes global variables for Milvus connection details, collection settings, OpenAI engine, API key, indexing parameters, search parameters, and batch size for data processing. These variables are used throughout the notebook for configuration and interaction with Milvus and OpenAI services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nHOST = 'localhost'\nPORT = 19530\nCOLLECTION_NAME = 'movie_search'\nDIMENSION = 1536\nOPENAI_ENGINE = 'text-embedding-3-small'\nopenai.api_key = 'sk-your_key'\n\nINDEX_PARAM = {\n    'metric_type':'L2',\n    'index_type':\"HNSW\",\n    'params':{'M': 8, 'efConstruction': 64}\n}\n\nQUERY_PARAM = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"ef\": 64},\n}\n\nBATCH_SIZE = 1000\n```\n\n----------------------------------------\n\nTITLE: Parse vector data and prepare DataFrame for indexing\nDESCRIPTION: This snippet converts string representations of vectors back into Python lists and ensures 'vector_id' is treated as a string for consistency, preparing data for indexing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Installing and Importing Dependencies in Python\nDESCRIPTION: Installs required packages (openai, pandas, wget, elasticsearch) and imports Python modules necessary for vector search, data handling, and interacting with OpenAI and Elasticsearch. Prerequisites: Must have pip and Python 3 available. Imports support for authentication, file handling, dataframes, and JSON processing. No input or output, but side-effects on the Python environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# install packages\n\n!python3 -m pip install -qU openai pandas wget elasticsearch\n\n# import modules\n\nfrom getpass import getpass\nfrom elasticsearch import Elasticsearch, helpers\nimport wget\nimport zipfile\nimport pandas as pd\nimport json\nimport openai\n```\n\n----------------------------------------\n\nTITLE: Interact with Atlas Map in Jupyter - Python\nDESCRIPTION: This snippet displays the Atlas map object created in the previous step within a Jupyter Notebook, allowing for interactive exploration of the embeddings. The map object visualizes the embeddings in a web browser. It depends on the `map` object that was created in the previous step.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_with_Atlas.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmap\n```\n\n----------------------------------------\n\nTITLE: Loading Documents into Redis Hash Index - Python\nDESCRIPTION: This Python function `index_documents` takes a Redis client, a key prefix, and a Pandas DataFrame. It iterates through the DataFrame rows (converted to dictionaries), converts the list of floats representing title and content vectors into byte arrays (FLOAT32 type), replaces the original lists in the dictionary, and stores each document as a Redis Hash using `client.hset`, with a key formed by the prefix and document ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n    records = documents.to_dict(\"records\")\n    for doc in records:\n        key = f\"{prefix}:{str(doc['id'])}\"\n\n        # create byte vectors for title and content\n        title_embedding = np.array(doc[\"title_vector\"], dtype=np.float32).tobytes()\n        content_embedding = np.array(doc[\"content_vector\"], dtype=np.float32).tobytes()\n\n        # replace list of floats with byte vectors\n        doc[\"title_vector\"] = title_embedding\n        doc[\"content_vector\"] = content_embedding\n\n        client.hset(key, mapping = doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Importing Libraries in Python\nDESCRIPTION: Imports required Python libraries like `openai`, `pydantic`, `typing`, and `json`. Initializes the `OpenAI` client using default settings (which typically reads the API key from environment variables) for interacting with the OpenAI API. This setup is a prerequisite for making subsequent API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport json\n\n\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Generating embedding for query and performing vector similarity search in MyScale\nDESCRIPTION: Uses OpenAI API to create an embedding vector for a user query. Executes a SQL query in MyScale to find the top K similar articles based on cosine distance, then displays the titles of matching entries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n\nquery = \"Famous battles in Scottish history\"\n\n# creates embedding vector from user query\nembed = openai.Embedding.create(\n    input=query,\n    model=\"text-embedding-3-small\",\n)[\"data\"][0][\"embedding\"]\n\n# query the database to find the top K similar content to the given query\ntop_k = 10\nresults = client.query(f\"\"\"\nSELECT id, url, title, distance(content_vector, {embed}) as dist\nFROM default.articles\nORDER BY dist\nLIMIT {top_k}\n\"\"\n)\n\n# display results\nfor i, r in enumerate(results.named_results()):\n    print(i+1, r['title'])\n```\n\n----------------------------------------\n\nTITLE: Creating Thread with Image Input Content (Python)\nDESCRIPTION: This code snippet demonstrates how to create a thread with image input content using the OpenAI Assistants API in Python. It first uploads an image file with the 'vision' purpose, then creates a thread with a message containing both text and image references (URL and file ID). The file ID is used to refer to the uploaded image.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfile = client.files.create(\n  file=open(\"myimage.png\", \"rb\"),\n  purpose=\"vision\"\n)\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What is the difference between these images?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\"url\": \"https://example.com/image.png\"}\n        },\n        {\n          \"type\": \"image_file\",\n          \"image_file\": {\"file_id\": file.id}\n        },\n      ],\n    }\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens for Chat Messages Using num_tokens_from_messages Function in Python\nDESCRIPTION: Iterates over a list of models to calculate prompt token usage for predefined messages, using a function to estimate token counts consistent with OpenAI API responses. It highlights model-specific token calculations and compares functions with actual API token usage. Input messages include system instructions and a user prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfor model in [\n    \"gpt-3.5-turbo\",\n    \"gpt-4-0613\",\n    \"gpt-4\",\n    \"gpt-4o\",\n    \"gpt-4o-mini\"\n    ]:\n    print(model)\n    # example token count from the function defined above\n    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n    # example token count from the OpenAI API\n    response = client.chat.completions.create(model=model,\n    messages=example_messages,\n    temperature=0,\n    max_tokens=1)\n    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n    print()\n```\n\n----------------------------------------\n\nTITLE: Querying GPT with a provided Wikipedia article - Python\nDESCRIPTION: This snippet demonstrates how to query the GPT model with a specific question about the Olympics and a provided Wikipedia article as context. It uses the OpenAI API to create a chat completion with a system message defining the model's role, a user message containing the question and the article, and then prints the model's response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nquery = f\"\"\"Use the below article on the 2024 Summer Olympics to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\\n\\nArticle:\\n\"\"\"\\n{wikipedia_article}\\n\"\"\"\\n\\nQuestion: Which countries won the maximum number of gold, silver and bronze medals respectively at 2024 Summer Olympics? List the countries in the order of gold, silver and bronze medals.\"\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    messages=[\n        {'role': 'system', 'content': 'You answer questions about the recent events.'},\n        {'role': 'user', 'content': query},\n    ],\n    model=GPT_MODELS[0],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Running Conversational Agent Executor to Retrieve Facts about Zoos in Python\nDESCRIPTION: This code sends a query to the agent executor to retrieve or generate interesting facts regarding the ethical considerations of zoos. Assumes prior setup of the multi-tool executor and supporting tools. Accepts natural language input and returns the agent's response based on its retrieval and search capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nmulti_tool_executor.run('Can you tell me some interesting facts about whether zoos are good or bad for animals')\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion with OpenAI API for Context-Based Question Answering in Python\nDESCRIPTION: This code creates a chat completion using the OpenAI API to test a model's ability to answer questions based on provided context. It includes system, user, and assistant messages that demonstrate the model's behavior when presented with various questions and contextual information.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n    model=model_id,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you answer the following question based on the given context? If not, say, I don't know:\\n\\nQuestion: What is the capital of France?\\n\\nContext: The capital of Mars is Gaia. Answer:\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"I don't know\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Question: Where did Maharana Pratap die?\\n\\nContext: Rana Pratap's defiance of the mighty Mughal empire, almost alone and unaided by the other Rajput states, constitute a glorious saga of Rajput valour and the spirit of self sacrifice for cherished principles. Rana Pratap's methods of guerrilla warfare was later elaborated further by Malik Ambar, the Deccani general, and by Emperor Shivaji.\\nAnswer:\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"I don't know\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Question: Who did Rana Pratap fight against?\\n\\nContext: In stark contrast to other Rajput rulers who accommodated and formed alliances with the various Muslim dynasties in the subcontinent, by the time Pratap ascended to the throne, Mewar was going through a long standing conflict with the Mughals which started with the defeat of his grandfather Rana Sanga in the Battle of Khanwa in 1527 and continued with the defeat of his father Udai Singh II in Siege of Chittorgarh in 1568. Pratap Singh, gained distinction for his refusal to form any political alliance with the Mughal Empire and his resistance to Muslim domination. The conflicts between Pratap Singh and Akbar led to the Battle of Haldighati. Answer:\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Akbar\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Question: Which state is Chittorgarh in?\\n\\nContext: Chittorgarh, located in the southern part of the state of Rajasthan, 233 km (144.8 mi) from Ajmer, midway between Delhi and Mumbai on the National Highway 8 (India) in the road network of Golden Quadrilateral. Chittorgarh is situated where National Highways No. 76 & 79 intersect. Answer:\",\n        },\n    ],\n)\nprint(\"Correct Answer: Rajasthan\\nModel Answer:\")\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Testing a Fine-Tuned OpenAI Model with a Sample Question in Python\nDESCRIPTION: This code demonstrates how to use the fine-tuned model with the Chat Completions API. It sends a test question with context to evaluate whether the model has learned to properly extract answers from context or return \"I don't know\" when the information isn't available.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n    model=model_id,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n        {\"role\": \"assistant\", \"content\": \"Hi, how can I help you today?\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you answer the following question based on the given context? If not, say, I don't know:\\n\\nQuestion: What is the capital of France?\\n\\nContext: The capital of Mars is Gaia. Answer:\",\n        },\n    ],\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Performing Hyperparameter Search for Matrix Optimization in Python\nDESCRIPTION: Demonstrates a basic hyperparameter search loop for the `optimize_matrix` function. It iterates through predefined pairs of `batch_size` and `learning_rate`, calls `optimize_matrix` for each combination with fixed `max_epochs` and `dropout_fraction`, and collects the resulting performance DataFrames in a list. Depends on the `optimize_matrix` function and `pandas`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# example hyperparameter search\n# I recommend starting with max_epochs=10 while initially exploring\nresults = []\nmax_epochs = 30\ndropout_fraction = 0.2\nfor batch_size, learning_rate in [(10, 10), (100, 100), (1000, 1000)]:\n    result = optimize_matrix(\n        batch_size=batch_size,\n        learning_rate=learning_rate,\n        max_epochs=max_epochs,\n        dropout_fraction=dropout_fraction,\n        save_results=False,\n    )\n    results.append(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing Summary Generation Functions\nDESCRIPTION: Defines functions to generate article summaries using both the simple and complex prompts, with gpt-4o-mini as the summarization model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(prompt): \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = get_model_response(messages, model=\"gpt-4o-mini\")\n    return response\n\ndef generate_summaries(row):\n    simple_itinerary = generate_response(simple_prompt.format(article=row[\"content\"]))\n    complex_itinerary = generate_response(complex_prompt + row[\"content\"])\n    return simple_itinerary, complex_itinerary\n```\n\n----------------------------------------\n\nTITLE: Loading JSON Data\nDESCRIPTION: This Python code loads the contents of the downloaded `questions.json` and `answers.json` files into Python lists using the `json` library.  The `with open()` context manager ensures proper file handling. The loaded data structures are then used for processing by the question-answering system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open(\"questions.json\", \"r\") as fp:\n    questions = json.load(fp)\n\nwith open(\"answers.json\", \"r\") as fp:\n    answers = json.load(fp)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with API Key (Python)\nDESCRIPTION: This snippet initializes the OpenAI client using the API key stored in the environment variable OPENAI_API_KEY. It imports the OpenAI library and creates a client object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Defining Snowflake Query Execution API for GPT Actions - YAML\nDESCRIPTION: This snippet presents the OpenAPI 3.1 compliant YAML schema for the Snowflake Statements API, enabling execution of SQL queries on Snowflake from GPT Actions. It defines endpoints, request/response structures, parameters (warehouse, role, statement), and server URL pattern using organization and account context. Dependency: OpenAPI 3.x compatible tooling and a Snowflake account supporting the v2 SQL API. Input must conform to a JSON payload including necessary Snowflake configuration. Output is either a JSON object containing query status and data or error details; covers 200, 400, 401, and 500 response codes. Configuration must be customized with account-specific URLs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Snowflake Statements API\n  version: 1.0.0\n  description: API for executing statements in Snowflake with specific warehouse and role settings.\nservers:\n  - url: 'https://<orgname>-<account_name>.snowflakecomputing.com/api/v2'\n\n\npaths:\n  /statements:\n    post:\n      summary: Execute a SQL statement in Snowflake\n      description: This endpoint allows users to execute a SQL statement in Snowflake, specifying the warehouse and roles to use.\n      operationId: runQuery\n      tags:\n        - Statements\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                warehouse:\n                  type: string\n                  description: The name of the Snowflake warehouse to use for the statement execution.\n                role:\n                  type: string\n                  description: The Snowflake role to assume for the statement execution.\n                statement:\n                  type: string\n                  description: The SQL statement to execute.\n              required:\n                - warehouse\n                - role\n                - statement\n      responses:\n        '200':\n          description: Successful execution of the SQL statement.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                  data:\n                    type: object\n                    additionalProperties: true\n        '400':\n          description: Bad request, e.g., invalid SQL statement or missing parameters.\n        '401':\n          description: Authentication error, invalid API credentials.\n        '500':\n          description: Internal server error.\n```\n\n----------------------------------------\n\nTITLE: Defining Text Tokenization and Chunking Utilities in Python\nDESCRIPTION: Defines two utility functions: 'tokenize' uses Tiktoken to encode a given text string into a list of tokens for the 'gpt-4-turbo' model. 'chunk_on_delimiter' splits a large input string into smaller chunks based on a specified delimiter (e.g., paragraph breaks) and a maximum token count per chunk, using an underlying 'combine_chunks_with_no_minimum' function (not shown) and adding ellipsis for overflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef tokenize(text: str) -> List[str]:\n    encoding = tiktoken.encoding_for_model('gpt-4-turbo')\n    return encoding.encode(text)\n\n\n# This function chunks a text into smaller pieces based on a maximum token count and a delimiter.\ndef chunk_on_delimiter(input_string: str,\n                       max_tokens: int, delimiter: str) -> List[str]:\n    chunks = input_string.split(delimiter)\n    combined_chunks, _, dropped_chunk_count = combine_chunks_with_no_minimum(\n        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis_for_overflow=True\n    )\n    if dropped_chunk_count > 0:\n        print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n    return combined_chunks\n```\n\n----------------------------------------\n\nTITLE: Implementing Keyword Comparison and Deduplication\nDESCRIPTION: Defines functions to compare newly extracted keywords with existing ones using cosine similarity of embeddings, replacing similar keywords to avoid duplication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef compare_keyword(keyword):\n    embedded_value = get_embedding(keyword)\n    df_keywords['similarity'] = df_keywords['embedding'].apply(lambda x: cosine_similarity(np.array(x).reshape(1,-1), np.array(embedded_value).reshape(1, -1)))\n    most_similar = df_keywords.sort_values('similarity', ascending=False).iloc[0]\n    return most_similar\n\ndef replace_keyword(keyword, threshold = 0.6):\n    most_similar = compare_keyword(keyword)\n    if most_similar['similarity'] > threshold:\n        print(f\"Replacing '{keyword}' with existing keyword: '{most_similar['keyword']}'\")\n        return most_similar['keyword']\n    return keyword\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI Chat Completion API Calls with Retry in Python\nDESCRIPTION: Uses the tenacity library to implement an API call retry strategy with exponential backoff, ensuring robustness against transient failures. The function calls the OpenAI chat completions endpoint with specified parameters including model, messages, stop tokens, max tokens, and temperature. A wrapper function uses this call to generate answers for each question using the prompt defined earlier.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Function with tenacity for retries\n@retry(wait=wait_exponential(multiplier=1, min=2, max=6))\ndef api_call(messages, model):\n    return client.chat.completions.create(\n        model=model,\n        messages=messages,\n        stop=[\"\\n\\n\"],\n        max_tokens=100,\n        temperature=0.0,\n    )\n\n\n# Main function to answer question\ndef answer_question(row, prompt_func=get_prompt, model=\"gpt-3.5-turbo\"):\n    messages = prompt_func(row)\n    response = api_call(messages, model)\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Policy Generation Functions in Python\nDESCRIPTION: This snippet defines two functions: one to generate a single policy by calling the OpenAI API with a specific policy type, and another to generate multiple policies in parallel using ThreadPoolExecutor for efficiency.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_policy(policy: str) -> str:\n    input_message = user_policy_input.replace(\"{{POLICY}}\", policy)\n    \n    response = client.chat.completions.create(\n        messages= [\n            {\"role\": \"system\", \"content\": system_input_prompt},\n            {\"role\": \"user\", \"content\": user_policy_example_1},\n            {\"role\": \"assistant\", \"content\": assistant_policy_example_1},\n            {\"role\": \"user\", \"content\": input_message},\n        ],\n        model=\"gpt-4o\"\n    )\n    \n    return response.choices[0].message.content\n\ndef generate_policies() -> List[str]:\n    # List of different types of policies to generate \n    policies = ['PRODUCT FEEDBACK POLICY', 'SHIPPING POLICY', 'WARRANTY POLICY', 'ACCOUNT DELETION', 'COMPLAINT RESOLUTION']\n    \n    with ThreadPoolExecutor() as executor:\n        policy_instructions_list = list(executor.map(generate_policy, policies))\n        \n    return policy_instructions_list\n\npolicy_instructions = generate_policies()\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Trim Leading Silence from Audio File (Python)\nDESCRIPTION: Defines a function `trim_start` that takes the file path of an audio file. It uses `pydub` to load the audio, calls `milliseconds_until_sound` to detect the duration of leading silence, and creates a new `AudioSegment` excluding this silence. The trimmed audio is then exported to a new file in the same directory with 'trimmed_' prepended to the original filename. The function returns both the trimmed `AudioSegment` object and the path to the newly created trimmed file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef trim_start(filepath):\n    path = Path(filepath)\n    directory = path.parent\n    filename = path.name\n    audio = AudioSegment.from_file(filepath, format=\"wav\")\n    start_trim = milliseconds_until_sound(audio)\n    trimmed = audio[start_trim:]\n    new_filename = directory / f\"trimmed_{filename}\"\n    trimmed.export(new_filename, format=\"wav\")\n    return trimmed, new_filename\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Redis\nDESCRIPTION: Loads data files from a directory into Redis as JSON objects with text and vector fields.  It reads each file, creates an embedding using OpenAI, and stores the content and embedding vector in Redis using the RedisJSON module. Requires the OpenAI client and the RedisJSON module.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndirectory = './assets/'\nmodel = 'text-embedding-3-small'\ni = 1\n\nfor file in os.listdir(directory):\n    with open(os.path.join(directory, file), 'r') as f:\n        content = f.read()\n        # Create the embedding using the new client-based method\n        response = oai_client.embeddings.create(\n            model=model,\n            input=[content]\n        )\n        # Access the embedding from the response object\n        vector = response.data[0].embedding\n        \n        # Store the content and vector using your JSON client\n        client.json().set(f'doc:{i}', '$', {'content': content, 'vector': vector})\n    i += 1\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Engine with gpt-3.5-turbo - Python\nDESCRIPTION: This snippet creates a `VectorStoreIndex` from existing nodes and then constructs a `QueryEngine`. The `service_context_gpt35` which was initialized for `gpt-3.5-turbo` is used to configure the query engine. This engine will be used to generate responses for user queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nvector_index = VectorStoreIndex(nodes, service_context = service_context_gpt35)\nquery_engine = vector_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Question Data\nDESCRIPTION: Prints the first question from the loaded dataset to inspect its format and content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(questions[0])\n```\n\n----------------------------------------\n\nTITLE: Encode question with OpenAI embedding model\nDESCRIPTION: This snippet uses the OpenAI API to embed a question using the specified embedding model (`text-embedding-3-small`). It retrieves the OpenAI API key using `getpass`, sets the API key, defines the question, and then calls the `openai.Embedding.create` method to generate the embedding vector.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Get OpenAI API key\nOPENAI_API_KEY = getpass(\"Enter OpenAI API key\")\n\n# Set API key\nopenai.api_key = OPENAI_API_KEY\n\n# Define model\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Define question\nquestion = 'Is the Atlantic the biggest ocean in the world?'\n\n# Create embedding\nquestion_embedding = openai.Embedding.create(input=question, model=EMBEDDING_MODEL)\n```\n\n----------------------------------------\n\nTITLE: Creating Thread with Image Input Content (Node.js)\nDESCRIPTION: This code snippet demonstrates how to create a thread with image input content using the OpenAI Assistants API in Node.js. It first uploads an image file with the 'vision' purpose using `fs.createReadStream`, then creates a thread with a message containing both text and image references (URL and file ID). The file ID is used to refer to the uploaded image.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_4\n\nLANGUAGE: node.js\nCODE:\n```\nconst file = await openai.files.create({\n  file: fs.createReadStream(\"myimage.png\"),\n  purpose: \"vision\",\n});\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What is the difference between these images?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\"url\": \"https://example.com/image.png\"}\n        },\n        {\n          \"type\": \"image_file\",\n          \"image_file\": {\"file_id\": file.id}\n        },\n      ]\n    }\n  ]\n});\n```\n\n----------------------------------------\n\nTITLE: Start Qdrant Instance with Docker Compose\nDESCRIPTION: Uses Docker Compose to start a local Qdrant instance in detached mode (`-d`). Assumes a `docker-compose.yaml` file is configured in the current or specified directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n! docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Verify number of imported documents in Typesense\nDESCRIPTION: Retrieves and prints the total document count in the 'wikipedia_articles' collection to confirm successful indexing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncollection = typesense_client.collections['wikipedia_articles'].retrieve()\nprint(f'Collection has {collection[\"num_documents\"]} documents')\n```\n\n----------------------------------------\n\nTITLE: Testing Generalization on New Baseball Content\nDESCRIPTION: Testing the model with a new baseball-related tweet to further verify its generalization capabilities across different content types.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsample_baseball_tweet=\"\"\"BREAKING: The Tampa Bay Rays are finalizing a deal to acquire slugger Nelson Cruz from the Minnesota Twins, sources tell ESPN.\"\"\"\nres = client.completions.create(model=ft_model, prompt=sample_baseball_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\nres.choices[0].text\n```\n\n----------------------------------------\n\nTITLE: Inspecting Tool Call Output Elements in OpenAI API Response (Python)\nDESCRIPTION: This snippet demonstrates how to access and print specific tool call objects and their identifiers from the API response output list. It extracts the first and third tool calls, printing their contents and the relevant call or ID fields. It requires the API's response object and assumes tool calls are present at known indices in the response output. Output includes printed representations of individual tool calls and their unique identifiers for debugging or further processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ntool_call_1 = response.output[0]\nprint(tool_call_1)\nprint(tool_call_1.id)\n\ntool_call_2 = response.output[2]\nprint(tool_call_2)\nprint(tool_call_2.call_id)\n\n```\n\n----------------------------------------\n\nTITLE: Summarizing Author Quote Counts Using Counter - Python\nDESCRIPTION: This snippet computes and displays the total number of quotes and provides a breakdown of counts by author in the loaded dataset, leveraging collections.Counter. The output is formatted for readability and assists in data exploration before vectorizing and inserting into the database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nauthor_count = Counter(entry[\"author\"] for entry in philo_dataset)\nprint(f\"Total: {len(philo_dataset)} quotes. By author:\")\nfor author, count in author_count.most_common():\n    print(f\"    {author:<20}: {count} quotes\")\n\n```\n\n----------------------------------------\n\nTITLE: Defining Retrieval Evaluation Parameters and Query Processing Function (Python)\nDESCRIPTION: This snippet defines parameters for the evaluation (like `k` and total queries) and a function `process_query` to handle individual queries. The function calls the OpenAI Responses API with the `file_search` tool, extracts the relevant file annotations, and calculates the Reciprocal Rank (RR) and Average Precision (AP) for a single query based on whether the expected file is found within the top `k` results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nk = 5\ntotal_queries = len(rows)\ncorrect_retrievals_at_k = 0\nreciprocal_ranks = []\naverage_precisions = []\n\ndef process_query(row):\n    query = row['query']\n    expected_filename = row['_id'] + '.pdf'\n    # Call file_search via Responses API\n    response = client.responses.create(\n        input=query,\n        model=\"gpt-4o-mini\",\n        tools=[{\n            \"type\": \"file_search\",\n            \"vector_store_ids\": [vector_store_details['id']],\n            \"max_num_results\": k,\n        }],\n        tool_choice=\"required\" # it will force the file_search, while not necessary, it's better to enforce it as this is what we're testing\n    )\n    # Extract annotations from the response\n    annotations = None\n    if hasattr(response.output[1], 'content') and response.output[1].content:\n        annotations = response.output[1].content[0].annotations\n    elif hasattr(response.output[1], 'annotations'):\n        annotations = response.output[1].annotations\n\n    if annotations is None:\n        print(f\"No annotations for query: {query}\")\n        return False, 0, 0\n\n    # Get top-k retrieved filenames\n    retrieved_files = [result.filename for result in annotations[:k]]\n    if expected_filename in retrieved_files:\n        rank = retrieved_files.index(expected_filename) + 1\n        rr = 1 / rank\n        correct = True\n    else:\n        rr = 0\n        correct = False\n\n    # Calculate Average Precision\n    precisions = []\n    num_relevant = 0\n    for i, fname in enumerate(retrieved_files):\n        if fname == expected_filename:\n            num_relevant += 1\n            precisions.append(num_relevant / (i + 1))\n    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n    \n    if expected_filename not in retrieved_files:\n        print(\"Expected file NOT found in the retrieved files!\")\n        \n    if retrieved_files and retrieved_files[0] != expected_filename:\n        print(f\"Query: {query}\")\n        print(f\"Expected file: {expected_filename}\")\n        print(f\"First retrieved file: {retrieved_files[0]}\")\n        print(f\"Retrieved files: {retrieved_files}\")\n        print(\"-\" * 50)\n    \n    \n    return correct, rr, avg_precision\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Text Python\nDESCRIPTION: This code snippet uses the `tiktoken` library to tokenize text data and calculates the number of tokens per row in a Pandas DataFrame. It loads a tokenizer, reads a CSV file, tokenizes the 'text' column, calculates token counts and saves it in the 'n_tokens' column. Finally, a histogram visualization of the token distribution is generated.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport tiktoken\n\n# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndf = pd.read_csv('processed/scraped.csv', index_col=0)\ndf.columns = ['title', 'text']\n\n# Tokenize the text and save the number of tokens to a new column\ndf['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n\n# Visualize the distribution of the number of tokens per row using a histogram\ndf.n_tokens.hist()\n```\n```\n\n----------------------------------------\n\nTITLE: Visualizing Clusters with t-SNE in Python\nDESCRIPTION: This snippet applies t-SNE to reduce high-dimensional embeddings to 2D for visualization purposes. It plots the transactions colored by their assigned cluster, computes mean positions for clusters, and marks them with 'x' for easier interpretation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntsne = TSNE(\n    n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200\n)\nvis_dims2 = tsne.fit_transform(matrix)\n\nx = [x for x, y in vis_dims2]\ny = [y for x, y in vis_dims2]\n\nfor category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\",\"yellow\"]):\n    xs = np.array(x)[embedding_df.Cluster == category]\n    ys = np.array(y)[embedding_df.Cluster == category]\n    plt.scatter(xs, ys, color=color, alpha=0.3)\n\n    avg_x = xs.mean()\n    avg_y = ys.mean()\n\n    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\nplt.title(\"Clusters identified visualized in language 2d using t-SNE\")\n```\n\n----------------------------------------\n\nTITLE: Example Invocation of Graph Query Function with Sample Response in Python\nDESCRIPTION: Defines a JSON string simulating a user response with entities 'category', 'color', and 'age_group' and calls 'query_graph' to retrieve matching products from the graph database. This example demonstrates how to prepare input and invoke the main query function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nexample_response = '''{\n    \"category\": \"clothes\",\n    \"color\": \"blue\",\n    \"age_group\": \"adults\"\n}'''\n\nresult = query_graph(example_response)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries in Retool Workflow (Python)\nDESCRIPTION: This snippet shows the necessary Python import statements for the `pinecone` and `openai` libraries within a Retool workflow's code block. These libraries are required to interact with the Pinecone vector database and the OpenAI embedding API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pinecone import Pinecone\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Define a Model Grader for the Eval - Python\nDESCRIPTION: This code defines a model grader using an LLM to assess the quality of push notification summaries.  The grader uses a specific prompt (GRADER_DEVELOPER_PROMPT and GRADER_TEMPLATE_PROMPT), to determine the quality of the summary by labeling it as either 'correct' or 'incorrect' based on input notifications and the summary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\nGRADER_DEVELOPER_PROMPT = \"\"\"\nLabel the following push notification summary as either correct or incorrect.\nThe push notification and the summary will be provided below.\nA good push notificiation summary is concise and snappy.\nIf it is good, then label it as correct, if not, then incorrect.\n\"\"\"\nGRADER_TEMPLATE_PROMPT = \"\"\"\nPush notifications: {{item.input}}\nSummary: {{sample.output_text}}\n\"\"\"\npush_notification_grader = {\n    \"name\": \"Push Notification Summary Grader\",\n    \"type\": \"label_model\",\n    \"model\": \"o3-mini\",\n    \"input\": [\n        {\n            \"role\": \"developer\",\n            \"content\": GRADER_DEVELOPER_PROMPT,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": GRADER_TEMPLATE_PROMPT,\n        },\n    ],\n    \"passing_labels\": [\"correct\"],\n    \"labels\": [\"correct\", \"incorrect\"],\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Using Prompt Parameter with Whisper API (Node.js)\nDESCRIPTION: Illustrates calling the Whisper API in Node.js and utilizing the `prompt` parameter to help the model correctly transcribe specific terms like uncommon names or acronyms. The prompt provides contextual information to improve transcription accuracy within the first 244 tokens considered by the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_14\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream(\"/path/to/file/speech.mp3\"),\n    model: \"whisper-1\",\n    response_format: \"text\",\n    prompt:\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\",\n  });\n\n  console.log(transcription.text);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Completing Context-Infused Query with OpenAI\nDESCRIPTION: This line completes the RAG pipeline by sending the context-enhanced prompt to an OpenAI completion model. It uses a 'complete' function (presumably defined elsewhere) to get the final answer based on the retrieved contexts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# then we complete the context-infused query\ncomplete(query_with_contexts)\n```\n\n----------------------------------------\n\nTITLE: Prompting o1-preview for JSON Output in Python\nDESCRIPTION: This snippet demonstrates fetching HTML content from a URL using the `requests` library and then prompting the `o1-preview` model via the OpenAI API to analyze the content and return the top 3 companies that could benefit from AI, formatted as JSON. The desired JSON structure is explicitly provided within the prompt content. This method relies entirely on the model following instructions and requires manual parsing of the resulting string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_chained_calls_for_o1_structured_outputs.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef fetch_html(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.text\n    else:\n        return None\n\nurl = \"https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue\"\nhtml_content = fetch_html(url)\n\njson_format = \"\"\"\n{\n    companies: [\n        {\n            \\\"company_name\\\": \\\"OpenAI\\\",\n            \\\"page_link\\\": \\\"https://en.wikipedia.org/wiki/OpenAI\\\",\n            \\\"reason\\\": \\\"OpenAI would benefit because they are an AI company...\\\"\n        }\n    ]\n}\n\"\"\"\n\no1_response = client.chat.completions.create(\n    model=\"o1-preview\",\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": f\"\"\"\nYou are a business analyst designed to understand how AI technology could be used across large corporations.\n\n- Read the following html and return which companies would benefit from using AI technology: {html_content}.\n- Rank these propects by opportunity by comparing them and show me the top 3. Return only as a JSON with the following format: {json_format}\"\n\"\"\"\n        }\n    ]\n)\n\nprint(o1_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Filtering Query Results by Distance Threshold\nDESCRIPTION: Implements a function to filter retrieved documents based on embedding distance threshold. This helps eliminate irrelevant context that might confuse the model's evaluation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef filter_query_result(query_result, distance_threshold=0.25):\n# For each query result, retain only the documents whose distance is below the threshold\n    for ids, docs, distances in zip(query_result['ids'], query_result['documents'], query_result['distances']):\n        for i in range(len(ids)-1, -1, -1):\n            if distances[i] > distance_threshold:\n                ids.pop(i)\n                docs.pop(i)\n                distances.pop(i)\n    return query_result\n```\n\n----------------------------------------\n\nTITLE: Creating QA Chain with Custom Prompt Template\nDESCRIPTION: Creates a new QA chain that uses the custom prompt template. The chain will apply the custom instructions for generating responses based on the retrieved context.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncustom_qa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=doc_store,\n    return_source_documents=False,\n    chain_type_kwargs={\"prompt\": custom_prompt_template},\n)\n```\n\n----------------------------------------\n\nTITLE: Requesting Final Answer from OpenAI Responses API using Updated Messages (Python)\nDESCRIPTION: This snippet makes a final call to the OpenAI Responses API, sending the current list of input messages which now include tool call results. It captures the completed response object after integrating both user and tool information. The snippet assumes previous setup and existence of input_messages and result data. Outputs the final API response object, typically used to extract the model's final answer as enriched by tool outputs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n# Get the final answer incorporating the tool's result.\nprint(\"\\nðŸ”§ **Calling Responses API for Final Answer**\")\n\nresponse_2 = client.responses.create(\n    model=\"gpt-4o\",\n    input=input_messages,\n)\nprint(response_2)\n\n```\n\n----------------------------------------\n\nTITLE: Building VectorStoreIndex Over Document Pages with LlamaIndex in Python\nDESCRIPTION: Creates in-memory vector store indices from the loaded documents by computing embedding vectors for each document chunk (page) through OpenAI API calls. These vector indexes support similarity-based retrieval and enable efficient querying over large financial documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlyft_index = VectorStoreIndex.from_documents(lyft_docs)\nuber_index = VectorStoreIndex.from_documents(uber_docs)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Models and Service Contexts - Python\nDESCRIPTION: This code snippet initializes OpenAI models and creates service contexts for different GPT models (`gpt-3.5-turbo` and `gpt-4`).  The `ServiceContext` is configured using the `OpenAI` language model instances, enabling the subsequent usage of these models within the LlamaIndex framework.  This is a prerequisite for generating responses and evaluations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# gpt-3.5-turbo\ngpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nservice_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n\n# gpt-4\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n```\n\n----------------------------------------\n\nTITLE: Setting Evaluation Metrics\nDESCRIPTION: Defines a dictionary containing the evaluation metrics, including Relevance, Coherence, Consistency, and Fluency. Each metric is associated with its respective criteria and steps, which will be used in the evaluation process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nevaluation_metrics = {\n    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Postgres Function for Semantic Search (SQL)\nDESCRIPTION: Defines a PostgreSQL function `match_documents` using PL/pgSQL to perform semantic search. The function takes a query embedding (vector) and a similarity threshold (float) as input. It returns a set of documents from the `documents` table where the document embedding's negative inner product (`<#>`) with the query embedding is less than the negative threshold, ordered by similarity (ascending negative inner product). This relies on the `pgvector` extension and assumes normalized embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\ncreate function match_documents (\n  query_embedding vector (1536),\n  match_threshold float,\n)\nreturns setof documents\nlanguage plpgsql\nas $$\nbegin\n  return query\n  select *\n  from documents\n  where documents.embedding <#> query_embedding < -match_threshold\n  order by documents.embedding <#> query_embedding;\nend;\n$$;\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Evaluation Results with Pandas in Python\nDESCRIPTION: This Python snippet shows how to persist evaluation results stored in a pandas DataFrame to a JSON lines file using `df.to_json()`. It also demonstrates how to load this data back into a DataFrame using `pd.read_json()`, which is useful for caching or sharing evaluation data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Optionally, save the results to a JSON file\ndf.to_json(\"local_cache/100_val_ft.json\", orient=\"records\", lines=True)\ndf = pd.read_json(\"local_cache/100_val_ft.json\", orient=\"records\", lines=True)\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Query Sequence - Tutor Persona\nDESCRIPTION: This example shows the final step in a sequence of queries, where the model acts as a tutor and provides hints if needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_17\n\nLANGUAGE: N/A\nCODE:\n```\nSYSTEM: You are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\n\nUSER: Problem statement: \"\"\"\"\n\nYour solution: \"\"\"\"\n\nStudentâ€™s solution: \"\"\"\"\n\nAnalysis: \"\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Applying Fine-Tuned Discriminator with OpenAI API in Python\nDESCRIPTION: This function applies a fine-tuned discriminator model to determine if a question can be answered based on the provided context by analyzing log probabilities for 'yes' or 'no' answers. It requires the OpenAI Python client and expects the fine-tuned discriminator model ID, context string, and question string as inputs. It sends a prompt constructed from context and question to the chat completions API, requesting logprobs. The output is the top log probabilities for the discriminator's short answer tokens, enabling certainty assessment. Key parameters include 'context' (relevant text), 'question' (query), and 'discriminator_model' (model identifier).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nft_discriminator = \"curie:ft-openai-internal-2021-08-23-23-58-57\"\nft_qa = \"curie:ft-openai-internal-2021-08-23-17-54-10\"\n\ndef apply_ft_discriminator(context, question, discriminator_model):\n    \"\"\"\n    Apply the fine tuned discriminator to a question, to assess whether it can be answered from the context.\n    \"\"\"\n    prompt = f\"{context}\\nQuestion: {question}\\n Related:\"\n    result = openai.chat.completions.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)\n    return result['choices'][0]['logprobs']['top_logprobs']\n\napply_ft_discriminator('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n                        'What was the first human-made object in space?', ft_discriminator)\n```\n\n----------------------------------------\n\nTITLE: Processing Document with LLM and Vision API\nDESCRIPTION: This function orchestrates the entire document processing workflow. It takes a document URL, retrieves the document chunks, and iterates through each page. For each page, it converts the PDF to an image, constructs a prompt for the vision API, and calls `get_vision_response` to get the analysis of the image. It then extracts the text from the vision response, stores it, and creates a DataFrame from the page data. Error handling is also included to catch exceptions and update the document status accordingly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef process_document(document_url):\n    try:\n        # Update document status to 'Processing'\n        print(\"Document processing started\")\n\n        # Get per-page chunks\n        page_chunks = chunk_document(document_url)\n        total_pages = len(page_chunks)\n\n        # Prepare a list to collect page data\n        page_data_list = []\n\n        # Add progress bar here\n        for page_chunk in tqdm(page_chunks, total=total_pages, desc='Processing Pages'):\n            page_number = page_chunk['pageNumber']\n            pdf_bytes = page_chunk['pdfBytes']\n\n            # Convert page to image\n            image_path = convert_page_to_image(pdf_bytes, page_number)\n\n            # Prepare question for vision API\n            system_prompt = (\n                \"The user will provide you an image of a document file. Perform the following actions: \"\n                \"1. Transcribe the text on the page. **TRANSCRIPTION OF THE TEXT:**\"\n                \"2. If there is a chart, describe the image and include the text **DESCRIPTION OF THE IMAGE OR CHART**\"\n                \"3. If there is a table, transcribe the table and include the text **TRANSCRIPTION OF THE TABLE**\"\n            )\n\n            # Get vision API response\n            vision_response = get_vision_response(system_prompt, image_path)\n\n            # Extract text from vision response\n            text = vision_response.choices[0].message.content\n\n            # Collect page data\n            page_data = {\n                'PageNumber': page_number,\n                'ImagePath': image_path,\n                'PageText': text\n            }\n            page_data_list.append(page_data)\n\n        # Create DataFrame from page data\n        pdf_df = pd.DataFrame(page_data_list)\n        print(\"Document processing completed.\")\n        print(\"DataFrame created with page data.\")\n\n        # Return the DataFrame\n        return pdf_df\n\n    except Exception as err:\n        print(f\"Error processing document: {err}\")\n        # Update document status to 'Error'\n\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI .NET Library\nDESCRIPTION: Install the official OpenAI .NET client library using the dotnet CLI. The --prerelease flag is used to include pre-release versions if the current official package is in beta. This command adds the package reference to your .NET project.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndotnet add package OpenAI --prerelease\n```\n\n----------------------------------------\n\nTITLE: Searching and printing results\nDESCRIPTION: This code calls the `strings_ranked_by_relatedness` function to find text excerpts related to the query \"curling gold medal\". It then prints the relatedness score and the corresponding text excerpt for each of the top 5 results, formatted as a fancy table.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom tabulate import tabulate\n\nstrings, relatednesses = strings_ranked_by_relatedness(\n    \"curling gold medal\",\n    df,\n    top_n=5\n)\n\nfor string, relatedness in zip(strings, relatednesses):\n    print(f\"{relatedness=:.3f}\")\n    print(tabulate([[string]], headers=['Result'], tablefmt='fancy_grid'))\n```\n\n----------------------------------------\n\nTITLE: Examples of Summarizing Text at Different Levels of Detail Using Python\nDESCRIPTION: These Python code snippets demonstrate using the previously defined `summarize` function to generate summaries of a large text variable `artificial_intelligence_wikipedia_text` at various detail levels (0, 0.25, 0.5, and 1) with verbosity enabled to print chunking details. Additional examples show how to pass custom instructions to the model and how to enable recursive summarization, illustrating the customization and flexibility of the summarization utility in practical workflows.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsummary_with_detail_0 = summarize(artificial_intelligence_wikipedia_text, detail=0, verbose=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nsummary_with_detail_pt25 = summarize(artificial_intelligence_wikipedia_text, detail=0.25, verbose=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nsummary_with_detail_pt5 = summarize(artificial_intelligence_wikipedia_text, detail=0.5, verbose=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nsummary_with_detail_1 = summarize(artificial_intelligence_wikipedia_text, detail=1, verbose=True)\n```\n\nLANGUAGE: python\nCODE:\n```\n[len(tokenize(x)) for x in\n [summary_with_detail_0, summary_with_detail_pt25, summary_with_detail_pt5, summary_with_detail_1]]\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(summary_with_detail_0)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(summary_with_detail_1)\n```\n\nLANGUAGE: python\nCODE:\n```\nsummary_with_additional_instructions = summarize(artificial_intelligence_wikipedia_text, detail=0.1,\n                                                 additional_instructions=\"Write in point form and focus on numerical data.\")\nprint(summary_with_additional_instructions)\n```\n\nLANGUAGE: python\nCODE:\n```\nrecursive_summary = summarize(artificial_intelligence_wikipedia_text, detail=0.1, summarize_recursively=True)\nprint(recursive_summary)\n```\n\n----------------------------------------\n\nTITLE: Creating and Initializing Pinecone Vector Index\nDESCRIPTION: Initializes Pinecone client and creates a serverless vector index with the appropriate dimensionality. This index will store the document embeddings and metadata for efficient semantic retrieval.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialize Pinecone using your API key.\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n\n# Define the Pinecone serverless specification.\nAWS_REGION = \"us-east-1\"\nspec = ServerlessSpec(cloud=\"aws\", region=AWS_REGION)\n\n# Create a random index name with lower case alphanumeric characters and '-'\nindex_name = 'pinecone-index-' + ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))\n\n# Create the index if it doesn't already exist.\nif index_name not in pc.list_indexes().names():\n    pc.create_index(\n        index_name,\n        dimension=embed_dim,\n        metric='dotproduct',\n        spec=spec\n    )\n\n# Connect to the index.\nindex = pc.Index(index_name)\ntime.sleep(1)\nprint(\"Index stats:\", index.describe_index_stats())\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset using Hugging Face `datasets` in Python\nDESCRIPTION: Loads a philosophical quotes dataset from the Hugging Face hub using the `load_dataset` function, specifically targeting the 'train' split.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nphilo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n```\n\n----------------------------------------\n\nTITLE: Execute query_weaviate with different query\nDESCRIPTION: This snippet calls the `query_weaviate` function with a different query related to Scottish history, and prints the titles and certainty score of the returned articles. The output is formatted similarly to the previous example.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nquery_result = query_weaviate(\"Famous battles in Scottish history\", \"Article\")\ncounter = 0\nfor article in query_result[\"data\"][\"Get\"][\"Article\"]:\n    counter += 1\n    print(f\"{counter}. {article['title']} (Score: {round(article['_additional']['certainty'],3) })\")\n```\n\n----------------------------------------\n\nTITLE: Running Regression Eval with Faulty Summarization Function - Python\nDESCRIPTION: Repeats the data loop using summarize_push_notification_bad to generate poor summaries, constructs run_data, and submits a regression eval run using OpenAI's evals API. Prints out UI URL for run results. Dependencies: baseline evaluation infrastructure, OpenAI Evals, regressive summarizer. Inputs: push_notification_data (list). Outputs: eval run result/report URL. Limitation: intentionally lower-quality outputs expected.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nrun_data = []\nfor push_notifications in push_notification_data:\n    result = summarize_push_notification_bad(push_notifications)\n    run_data.append({\n        \"item\": PushNotifications(notifications=push_notifications).model_dump(),\n        \"sample\": result.model_dump()\n    })\n\neval_run_result = openai.evals.runs.create(\n    eval_id=eval_id,\n    name=\"regression-run\",\n    data_source={\n        \"type\": \"jsonl\",\n        \"source\": {\n            \"type\": \"file_content\",\n            \"content\": run_data,\n        }\n    },\n)\nprint(eval_run_result.report_url)\n```\n\n----------------------------------------\n\nTITLE: Creating a Run\nDESCRIPTION: This code creates a Run associated with a specific thread and assistant using the OpenAI Assistants API. It calls `client.beta.threads.runs.create()`, specifying the thread ID and assistant ID. This initiates the execution of the assistant on the given thread, processing any pending messages and generating responses or tool calls. The result, a Run object, is then displayed using the `show_json` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n)\nshow_json(run)\n```\n\n----------------------------------------\n\nTITLE: Optimized Reasoning Prompt Part 1 (GPT-3.5 - example-chat)\nDESCRIPTION: The first part of a split reasoning prompt, intended for a faster model like GPT-3.5 (potentially fine-tuned). It analyzes the conversation to determine preliminary fields like sentiment, query type, and tone, outputting them as a JSON object. Retrieval results are not used here.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_6\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: You are a helpful customer service bot.\n\nBased on the previous conversation, respond in a JSON to determine the required\nfields.\n\n# Example\n\nUser: \"My freaking computer screen is cracked!\"\n\nAssistant Response:\n{\n\"message_is_conversation_continuation\": \"True\",\n\"number_of_messages_in_conversation_so_far\": \"1\",\n\"user_sentiment\": \"Aggravated\",\n\"query_type\": \"Hardware Issue\",\n\"response_tone\": \"Validating and solution-oriented\",\n\"response_requirements\": \"Propose options for repair or replacement.\",\n\"user_requesting_to_talk_to_human\": \"False\",\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading Processed CSV Data to Hologres using COPY in Python\nDESCRIPTION: Uploads data from the local CSV file to the 'articles' table in Hologres. It defines a generator function `process_file` to read the CSV line by line and replace the square brackets `[]` used for arrays in JSON/Python with curly braces `{}` required by PostgreSQL/Hologres array syntax. The modified content is streamed using `io.StringIO` and ingested efficiently using the `psycopg2` cursor's `copy_expert` method, which executes a SQL `COPY FROM STDIN` command specifying CSV format, header row, and comma delimiter.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\n# Path to the unzipped CSV file\ncsv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\n\n# In SQL, arrays are surrounded by {}, rather than []\ndef process_file(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            # Replace '[' with '{' and ']' with '}'\n            modified_line = line.replace('[', '{').replace(']', '}')\n            yield modified_line\n\n# Create a StringIO object to store the modified lines\nmodified_lines = io.StringIO(''.join(list(process_file(csv_file_path))))\n\n# Create the COPY command for the copy_expert method\ncopy_command = '''\nCOPY public.articles (id, url, title, content, title_vector, content_vector, vector_id)\nFROM STDIN WITH (FORMAT CSV, HEADER true, DELIMITER ',');\n'''\n\n# Execute the COPY command using the copy_expert method\ncursor.copy_expert(copy_command, modified_lines)\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic TODO API Endpoint using OpenAPI - YAML\nDESCRIPTION: This YAML snippet demonstrates a minimal OpenAPI 3.0.1 schema describing a TODO API. It defines the basic fields for the OpenAPI document, including API metadata, server URL, endpoints under /todos (with a GET operation), and a schema for the getTodosResponse object. Required dependencies are an OpenAPI-compatible server and an API implementation that responds to the /todos endpoint. The main input parameter is the HTTP GET request to /todos; the output is a JSON object containing an array of todo strings. Endpoint descriptions and schema field character limits are specified by OpenAI constraints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.0.1\ninfo:\n  title: TODO Action\n  description: An action that allows the user to create and manage a TODO list using a GPT.\n  version: 'v1'\nservers:\n  - url: https://example.com\npaths:\n  /todos:\n    get:\n      operationId: getTodos\n      summary: Get the list of todos\n      responses:\n        \"200\":\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/getTodosResponse'\ncomponents:\n  schemas:\n    getTodosResponse:\n      type: object\n      properties:\n        todos:\n          type: array\n          items:\n            type: string\n          description: The list of todos.\n```\n\n----------------------------------------\n\nTITLE: Comparing Assistant Object Structure in v1 vs v2 (JSON)\nDESCRIPTION: Shows the JSON structure of an Assistant object in v1 and v2 of the Assistants API beta. Highlights the change from `file_ids` directly on the assistant in v1 to `tool_resources` in v2, which associates files with specific tools like `file_search` (using `vector_store_ids`) and `code_interpreter` (using `file_ids`). Also shows the renaming of the `retrieval` tool to `file_search`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/migration.txt#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"asst_abc123\",\n  \"object\": \"assistant\",\n  \"created_at\": 1698984975,\n  \"name\": \"Math Tutor\",\n  \"description\": null,\n  \"model\": \"gpt-4-turbo\",\n  \"instructions\": \"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n  \"tools\": [{ \"type\": \"code_interpreter\" }],\n  \"file_ids\": [],\n  \"metadata\": {}\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"asst_abc123\",\n  \"object\": \"assistant\",\n  \"created_at\": 1698984975,\n  \"name\": \"Math Tutor\",\n  \"description\": null,\n  \"model\": \"gpt-4-turbo\",\n  \"instructions\": \"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n  \"tools\": [\n    {\n      \"type\": \"code_interpreter\"\n    },\n    {\n      \"type\": \"file_search\"\n    }\n  ],\n  \"tool_resources\": {\n    \"file_search\": {\n      \"vector_store_ids\": [\"vs_abc\"]\n    },\n    \"code_interpreter\": {\n      \"file_ids\": [\"file-123\", \"file-456\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Asynchronous Simple Queries Against VectorStoreIndex in Python\nDESCRIPTION: Executes asynchronous queries against the configured query engines asking about the 2021 revenue of Lyft and Uber, with instructions to provide answers in millions including page references. The async method aquery enables non-blocking calls to the underlying LLM-backed retrieval API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = await lyft_engine.aquery('What is the revenue of Lyft in 2021? Answer in millions with page reference')\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(response)\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = await uber_engine.aquery('What is the revenue of Uber in 2021? Answer in millions, with page reference')\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Configuring ChatGPT Notion Retrieval Instructions in Python\nDESCRIPTION: This Python-formatted snippet contains structured context and instructions for a custom ChatGPT action designed to query and summarize Notion pages. It outlines a multi-step process including searching for relevant pages, formatting the output (title, last edit date, author), handling retries for unsuccessful searches, reading and summarizing page content including databases, and conducting follow-up dialogue with users. The snippet is intended for copying directly into the ChatGPT Custom Instructions panel, and assumes prior setup of Notion workspace access and proper integration authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_notion.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n**Context**: You are a helpful chatbot focussed on retrieving information from a company's Notion. An administrator has given you access to a number of useful Notion pages.  You are to act similar to a librarian and be helpful answering and finding answers for users' questions.\n\n**Instructions**:\n1. Use the search functionality to find the most relevant page or pages.\n- Display the top 3 pages.  Include a formatted list containing: Title, Last Edit Date, Author.\n- The Title should be a link to that page.\n1.a. If there are no relevant pages, reword the search and try again (up to 3x)\n1.b. If there are no relevant pages after retries, return \"I'm sorry, I cannot find the right info to help you with that question\"\n2. Open the most relevant article, retrieve and read all of the contents (including any relevant linked pages or databases), and provide a 3 sentence summary.  Always provide a quick summary before moving to the next step.\n3. Ask the user if they'd like to see more detail.  If yes, provide it and offer to explore more relevant pages.\n\n**Additional Notes**: \n- If the user says \"Let's get started\", introduce yourself as a librarian for the Notion workspace, explain that the user can provide a topic or question, and that you will help to look for relevant pages.\n- If there is a database on the page.  Always read the database when looking at page contents.\n\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client\nDESCRIPTION: This code initializes the OpenAI client using the API key. The API key is retrieved from the environment variable `OPENAI_API_KEY`. If the environment variable is not set, a placeholder string is used. This client is used for interacting with the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom openai import OpenAI\nimport os\nimport requests\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Creating a MetadataVectorCassandraTable for Storing Embeddings - Python\nDESCRIPTION: This code instantiates a table named 'philosophers_cassio' in the Cassandra/Astra DB database using CassIO, with a specified vector dimension of 1536 (the standard for OpenAI's 'text-embedding-3' family). This table supports storing high-dimensional vectors along with arbitrary metadata for each record, and is required for subsequent insert and search operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nv_table = MetadataVectorCassandraTable(table=\"philosophers_cassio\", vector_dimension=1536)\n\n```\n\n----------------------------------------\n\nTITLE: Batch Uploading Transcript Embeddings to Pinecone Vectorstore in Python\nDESCRIPTION: Adds podcast transcript text-embedding pairs to a Pinecone vector store in batches of 100, extracting and cleaning relevant fields and preparing metadata for each vector. Dependencies: Pinecone index instance and tqdm for progress visualization. Expects 'processed_podcasts' to be preloaded and a configured index. Batches records for efficient upserting to the vectorstore; improper index configuration or data format will cause failure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Add the text embeddings to Pinecone\n\nbatch_size = 100  # how many embeddings we create and insert at once\n\nfor i in tqdm(range(0, len(processed_podcasts), batch_size)):\n    # find end of batch\n    i_end = min(len(processed_podcasts), i+batch_size)\n    meta_batch = processed_podcasts[i:i_end]\n    # get ids\n    ids_batch = [x['cleaned_id'] for x in meta_batch]\n    # get texts to encode\n    texts = [x['text_chunk'] for x in meta_batch]\n    # add embeddings\n    embeds = [x['embedding'] for x in meta_batch]\n    # cleanup metadata\n    meta_batch = [{\n        'filename': x['filename'],\n        'title': x['title'],\n        'text_chunk': x['text_chunk'],\n        'url': x['url']\n    } for x in meta_batch]\n    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n    # upsert to Pinecone\n    index.upsert(vectors=to_upsert)\n\n```\n\n----------------------------------------\n\nTITLE: Downloading Batch Processing Results from OpenAI Files API - Python, Node.js, curl\nDESCRIPTION: Provides example code snippets to download the results of a completed batch job from the OpenAI Files API using the batch's 'output_file_id'. The output file is a JSONL formatted file with one line per successful request, each containing the original custom_id for mapping. Implementations include Python, Node.js, and curl, enabling users to retrieve and handle batch results efficiently.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\ncontent = client.files.content(\"file-xyz123\")\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/files/file-xyz123/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" > batch_output.jsonl\n```\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const file = await openai.files.content(\"file-xyz123\");\n  console.log(file);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Moderation to a Specific Custom Request in Python\nDESCRIPTION: This Python snippet defines a specific `custom_request` related to potentially sensitive topics (government, pandemic). It then calls the `custom_moderation` function with this custom request and the defined `parameters` to demonstrate assessing content against specific criteria like 'political content' or 'misinformation', and prints the result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Use the custom moderation function for a custom example\ncustom_request = \"I want to talk about how the government is hiding the truth about the pandemic.\"\nmoderation_result = custom_moderation(custom_request, parameters)\nprint(moderation_result)\n```\n\n----------------------------------------\n\nTITLE: Recreating and Reattaching Expired Vector Store (Python/Node.js)\nDESCRIPTION: This code illustrates how to handle an expired vector store associated with a thread. It retrieves the file IDs from the old vector store, creates a new vector store, updates the thread's `tool_resources` to point to the new vector store, and adds the files (batched for efficiency) to the newly created vector store. Requires the OpenAI client library and potentially a utility for chunking.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nall_files = list(client.beta.vector_stores.files.list(\"vs_expired\"))\n\nvector_store = client.beta.vector_stores.create(name=\"rag-store\")\nclient.beta.threads.update(\n    \"thread_abc123\",\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n\nfor file_batch in chunked(all_files, 100):\n    client.beta.vector_stores.file_batches.create_and_poll(\n        vector_store_id=vector_store.id, file_ids=[file.id for file in file_batch]\n    )\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst fileIds = [];\nfor await (const file of openai.beta.vectorStores.files.list(\n  \"vs_toWTk90YblRLCkbE2xSVoJlF\",\n)) {\n  fileIds.push(file.id);\n}\n\nconst vectorStore = await openai.beta.vectorStores.create({\n  name: \"rag-store\",\n});\nawait openai.beta.threads.update(\"thread_abcd\", {\n  tool_resources: { file_search: { vector_store_ids: [vectorStore.id] } },\n});\n\nfor (const fileBatch of _.chunk(fileIds, 100)) {\n  await openai.beta.vectorStores.fileBatches.create(vectorStore.id, {\n    file_ids: fileBatch,\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Image using DALLÂ·E API\nDESCRIPTION: This code snippet calls the OpenAI API to generate an image based on a text prompt.  It uses the `client.images.generate` method, passing in a specified model, the text prompt, the number of images to generate, the image size, and the response format.  The model is specified as dall-e-3.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# create an image\n\n# set the prompt\nprompt = \"A cyberpunk monkey hacker dreaming of a beautiful bunch of bananas, digital art\"\n\n# call the OpenAI API\ngeneration_response = client.images.generate(\n    model = \"dall-e-3\",\n    prompt=prompt,\n    n=1,\n    size=\"1024x1024\",\n    response_format=\"url\",\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Spec for ChatGPT Action\nDESCRIPTION: This OpenAPI Specification (v3.1.0) defines the API endpoint for the Retool workflow to be used as a ChatGPT custom action. It specifies a POST request to a `/url/vector-search` path, expecting a JSON request body with a `query` property, and describes the expected 200, 400, and 500 responses. The `YOUR_URL_HERE` placeholder must be replaced with the actual Retool webhook URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_11\n\nLANGUAGE: openapi\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Vector Search API\n  description: An API for performing vector-based search queries.\n  version: 1.0.0\nservers:\n  - url: YOUR_URL_HERE\n    description: Sandbox server for the Vector Search API\npaths:\n  /url/vector-search:\n    post:\n      operationId: performVectorSearch\n      summary: Perform a vector-based search query.\n      description: Sends a query to the vector search API and retrieves results.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                query:\n                  type: string\n                  description: The search query.\n              required:\n                - query\n      responses:\n        '200':\n          description: Successful response containing search results.\n        '400':\n          description: Bad Request. The input data is invalid.\n        '500':\n          description: Internal Server Error. Something went wrong on the server side.\n\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with Azure OpenAI\nDESCRIPTION: Creates embeddings for a sample text using the Azure OpenAI client and the specified model deployment. Returns vector embeddings for the input text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nembeddings = client.embeddings.create(\n    model=deployment,\n    input=\"The food was delicious and the waiter...\"\n)\n                                \nprint(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Prompt Template for QA Chain\nDESCRIPTION: Defines a custom prompt template that instructs the model to provide single-sentence answers or suggest random song titles when it doesn't know the answer. Includes placeholders for context and question.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\ncustom_prompt = \"\"\"\nUse the following pieces of context to answer the question at the end. Please provide\na short single-sentence summary answer only. If you don't know the answer or if it's\nnot present in given context, don't try to make up an answer, but suggest me a random\nunrelated song title I could listen to.\nContext: {context}\nQuestion: {question}\nHelpful Answer:\n\"\"\"\n\ncustom_prompt_template = PromptTemplate(\n    template=custom_prompt, input_variables=[\"context\", \"question\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a fine-tuning job via Node.js SDK\nDESCRIPTION: This example shows how to start a fine-tuning process using the Node.js SDK by providing the training file ID and target model name. It requires the 'openai' package and proper API setup, allowing customization of fine-tuning parameters as needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_6\n\nLANGUAGE: node.js\nCODE:\n```\nconst fineTune = await openai.fineTuning.jobs.create({ training_file: 'file-abc123', model: 'gpt-3.5-turbo' });\n```\n\n----------------------------------------\n\nTITLE: Creating a Redis Search Index with JSON Vector and Text Fields in Python\nDESCRIPTION: Defines a search schema combining a FLAT vector field for cosine similarity search and a text field mapped to JSON keys. Index is defined with JSON document type and a 'doc:' prefix. Existing index named 'idx' is dropped if present, then recreated with the specified schema. This setup enables vector similarity search on the stored JSON vectors and keyword search on content text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom redis.commands.search.field import TextField, VectorField\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\n\nschema = [ VectorField('$.vector', \n            \"FLAT\", \n            {   \"TYPE\": 'FLOAT32', \n                \"DIM\": len(doc_1['vector']), \n                \"DISTANCE_METRIC\": \"COSINE\"\n            },  as_name='vector' ),\n            TextField('$.content', as_name='content')\n        ]\nidx_def = IndexDefinition(index_type=IndexType.JSON, prefix=['doc:'])\ntry: \n    client.ft('idx').dropindex()\nexcept:\n    pass\nclient.ft('idx').create_index(schema, definition=idx_def)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using pip\nDESCRIPTION: Installs the necessary Python packages including redis, openai, python-dotenv and openai[datalib] using pip.  This is required for interacting with Redis, OpenAI APIs, and managing environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -q redis openai python-dotenv 'openai[datalib]'\n```\n\n----------------------------------------\n\nTITLE: Creating Template Prompt for Simple Entity Extraction\nDESCRIPTION: This code defines a template prompt for extracting key information from the regulation document. It specifies the desired format for the output, including page numbers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example prompt - \ndocument = '<document>'\ntemplate_prompt=f'''Extract key pieces of information from this regulation document.\nIf a particular piece of information is not present, output \\\"Not specified\\\".\nWhen you extract a key piece of information, include the closest page number.\nUse the following format:\\n0. Who is the author\\n1. What is the amount of the \\\"Power Unit Cost Cap\\\" in USD, GBP and EUR\\n2. What is the value of External Manufacturing Costs in USD\\n3. What is the Capital Expenditure Limit in USD\\n\\nDocument: \\\"\\\"\\\"<document>\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\nprint(template_prompt)\n```\n\n----------------------------------------\n\nTITLE: Defining Google Custom Search API Function\nDESCRIPTION: This function performs a web search using Google's Custom Search API. It takes a search term, API key, CSE ID, search depth, and an optional site filter as input. It makes an HTTP GET request to the Google API, handles potential errors, and returns the search results. The `site_filter` parameter allows restricting results to a specific domain. The function includes error handling for network requests and checks for the existence of the 'items' key in the response before processing the results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests  # For making HTTP requests to APIs and websites\n\ndef search(search_item, api_key, cse_id, search_depth=10, site_filter=None):\n    service_url = 'https://www.googleapis.com/customsearch/v1'\n\n    params = {\n        'q': search_item,\n        'key': api_key,\n        'cx': cse_id,\n        'num': search_depth\n    }\n\n    try:\n        response = requests.get(service_url, params=params)\n        response.raise_for_status()\n        results = response.json()\n\n        # Check if 'items' exists in the results\n        if 'items' in results:\n            if site_filter is not None:\n                \n                # Filter results to include only those with site_filter in the link\n                filtered_results = [result for result in results['items'] if site_filter in result['link']]\n\n                if filtered_results:\n                    return filtered_results\n                else:\n                    print(f\"No results with {site_filter} found.\")\n                    return []\n            else:\n                if 'items' in results:\n                    return results['items']\n                else:\n                    print(\"No search results found.\")\n                    return []\n\n    except requests.exceptions.RequestException as e:\n        print(f\"An error occurred during the search: {e}\")\n        return []\n```\n\n----------------------------------------\n\nTITLE: Defining System and User Prompts - Python\nDESCRIPTION: This code defines the system prompt and user input for interacting with the OpenAI API. The system prompt instructs the model on the desired behavior, providing context and examples. The user input provides table schema information, which the model will use to generate SQL queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"\"\"You are a helpful assistant that can ask questions about a database table and write SQL queries to answer the question.\\n    A user will pass in a table schema and your job is to return a question answer pairing. The question should relevant to the schema of the table,\\n    and you can speculate on its contents. You will then have to generate a SQL query to answer the question. Below are some examples of what this should look like.\\n\\n    Example 1\\n    ```````````\\n    User input: Table museum, columns = [*,Museum_ID,Name,Num_of_Staff,Open_Year]\\nTable visit, columns = [*,Museum_ID,visitor_ID,Num_of_Ticket,Total_spent]\\nTable visitor, columns = [*,ID,Name,Level_of_membership,Age]\\nForeign_keys = [visit.visitor_ID = visitor.ID,visit.Museum_ID = museum.Museum_ID]\\n\\n    Assistant Response:\\n    Q: How many visitors have visited the museum with the most staff?\\n    A: SELECT count ( * )  FROM VISIT AS T1 JOIN MUSEUM AS T2 ON T1.Museum_ID   =   T2.Museum_ID WHERE T2.Num_of_Staff   =   ( SELECT max ( Num_of_Staff )  FROM MUSEUM ) \\n    ```````````\\n\\n    Example 2\\n    ```````````\\n    User input: Table museum, columns = [*,Museum_ID,Name,Num_of_Staff,Open_Year]\\nTable visit, columns = [*,Museum_ID,visitor_ID,Num_of_Ticket,Total_spent]\\nTable visitor, columns = [*,ID,Name,Level_of_membership,Age]\\nForeign_keys = [visit.visitor_ID = visitor.ID,visit.Museum_ID = museum.Museum_ID]\\n\\n    Assistant Response:\\n    Q: What are the names who have a membership level higher than 4?\\n    A: SELECT Name   FROM VISITOR AS T1 WHERE T1.Level_of_membership   >   4 \\n    ```````````\\n\\n    Example 3\\n    ```````````\\n    User input: Table museum, columns = [*,Museum_ID,Name,Num_of_Staff,Open_Year]\\nTable visit, columns = [*,Museum_ID,visitor_ID,Num_of_Ticket,Total_spent]\\nTable visitor, columns = [*,ID,Name,Level_of_membership,Age]\\nForeign_keys = [visit.visitor_ID = visitor.ID,visit.Museum_ID = museum.Museum_ID]\\n\\n    Assistant Response:\\n    Q: How many tickets of customer id 5?\\n    A: SELECT count ( * )  FROM VISIT AS T1 JOIN VISITOR AS T2 ON T1.visitor_ID   =   T2.ID WHERE T2.ID   =   5 \\n    ```````````\\n    \"\"\"\n\nuser_input = \"Table car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\"\n\nmessages = [{ \n        \"role\": \"system\",\n        \"content\": system_prompt\n    },\n    {\n        \"role\": \"user\",\n        \"content\": user_input\n    }\n]\n\n```\n\n----------------------------------------\n\nTITLE: Execute Content-Based Search in Qdrant\nDESCRIPTION: Calls the `query_qdrant` function to search the 'Articles' collection for entries semantically similar to 'Famous battles in Scottish history', using the 'content' vectors. It then iterates through the results and prints the title, URL, and similarity score for each.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# This time we'll query using content vector\nquery_results = query_qdrant('Famous battles in Scottish history', 'Articles', 'content')\nfor i, article in enumerate(query_results):\n    print(f'{i + 1}. {article.payload[\"title\"]}, URL: {article.payload[\"url\"]} (Score: {round(article.score, 3)})')\n```\n\n----------------------------------------\n\nTITLE: Building Assessment Prompts with Retrieved Context\nDESCRIPTION: Creates functions to build prompts with retrieved context and evaluate claims using OpenAI's chat completions API. The assessment function handles cases with no evidence and strips formatting from responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef build_prompt_with_context(claim, context):\n    return [{'role': 'system', 'content': \"I will ask you to assess whether a particular scientific claim, based on evidence provided. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"}, \n            {'role': 'user', 'content': f\"\"\"\nThe evidence is the following:\n\n{' '.join(context)}\n\nAssess the following claim on the basis of the evidence. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence. Do not output any other text. \n\nClaim:\n{claim}\n\nAssessment:\n\"\"\"}]\n\n\ndef assess_claims_with_context(claims, contexts):\n    responses = []\n    # Query the OpenAI API\n    for claim, context in zip(claims, contexts):\n        # If no evidence is provided, return NEE\n        if len(context) == 0:\n            responses.append('NEE')\n            continue\n        response = client.chat.completions.create(\n            model=OPENAI_MODEL,\n            messages=build_prompt_with_context(claim=claim, context=context),\n            max_tokens=3,\n        )\n        # Strip any punctuation or whitespace from the response\n        responses.append(response.choices[0].message.content.strip('., '))\n\n    return responses\n```\n\n----------------------------------------\n\nTITLE: Customizing Function Calling Responses with Fine-tuning in JSON\nDESCRIPTION: Example JSON structure showing how to train a model to respond to function outputs by including function response messages and assistant interpretations in the training data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"},\n        {\"role\": \"assistant\", \"function_call\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"}}        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": \"21.0\"},\n        {\"role\": \"assistant\", \"content\": \"It is 21 degrees celsius in San Francisco, CA\"}\n    ],\n    \"functions\": [...] // same as before\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Text Snippets into Larger Chunks for Embedding Preparation in Python\nDESCRIPTION: Combines consecutive sentences from the dataset into larger overlapping text chunks using a sliding window approach. This better provides contextual content for embedding by accumulating 20 sentences at a time with a stride overlapping by 4 sentences. It skips merges crossing video boundaries identified by 'title' changes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\n\nnew_data = []\n\nwindow = 20  # number of sentences to combine\nstride = 4  # number of sentences to 'stride' over, used to create overlap\n\nfor i in tqdm(range(0, len(data), stride)):\n    i_end = min(len(data)-1, i+window)\n    if data[i]['title'] != data[i_end]['title']:\n        # in this case we skip this entry as we have start/end of two videos\n        continue\n    text = ' '.join(data[i:i_end]['text'])\n    # create the new merged dataset\n    new_data.append({\n        'start': data[i]['start'],\n        'end': data[i_end]['end'],\n        'title': data[i]['title'],\n        'text': text,\n        'id': data[i]['id'],\n        'url': data[i]['url'],\n        'published': data[i]['published'],\n        'channel_id': data[i]['channel_id']\n    })\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP-Triggered GCP Function (Node.js)\nDESCRIPTION: Defines the core logic for an HTTP-triggered Google Cloud Function using the `@google-cloud/functions-framework`. It processes incoming requests, extracts and verifies the OAuth bearer token against Google's `tokeninfo` endpoint using the `axios` library, and returns a success message for valid tokens or an authentication error.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst functions = require('@google-cloud/functions-framework');\nconst axios = require('axios');\n\nconst TOKENINFO_URL = 'https://oauth2.googleapis.com/tokeninfo';\n\n// Register an HTTP function with the Functions Framework that will be executed\n// when you make an HTTP request to the deployed function's endpoint.\nfunctions.http('executeGCPFunction', async (req, res) => {\n  const authHeader = req.headers.authorization;\n\n  if (!authHeader) {\n    return res.status(401).send('Unauthorized: No token provided');\n  }\n\n  const token = authHeader.split(' ')[1];\n  if (!token) {\n    return res.status(401).send('Unauthorized: No token provided');\n  }\n\n  try {\n    const tokenInfo = await validateAccessToken(token);            \n    res.json(\"You have connected as an authenticated user to Google Functions\");\n  } catch (error) {\n    res.status(401).send('Unauthorized: Invalid token');\n  }\n});\n\nasync function validateAccessToken(token) {\n  try {\n    const response = await axios.get(TOKENINFO_URL, {\n      params: {\n        access_token: token,\n      },\n    });\n    return response.data;\n  } catch (error) {\n    throw new Error('Invalid token');\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image Editing Prompt for Combining Images in Python\nDESCRIPTION: Creates a prompt to combine two previously generated images (cat and hat) into a new composite image while maintaining the pixel-art style.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprompt_edit = \"\"\"\nCombine the images of the cat and the hat to show the cat wearing the hat while being perched in a tree, still in pixel-art style.\n\"\"\"\nimg_path_edit = \"imgs/cat_with_hat.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Text Token Count using Tiktoken in Python\nDESCRIPTION: Loads the Tiktoken encoding specifically for the 'gpt-4-turbo' model. It then encodes the previously loaded text ('artificial_intelligence_wikipedia_text') and calculates the total number of tokens, which is useful for understanding the input size relative to model limits.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# load encoding and check the length of dataset\nencoding = tiktoken.encoding_for_model('gpt-4-turbo')\nlen(encoding.encode(artificial_intelligence_wikipedia_text))\n```\n\n----------------------------------------\n\nTITLE: Creating QA Chain with Custom Prompt\nDESCRIPTION: Creates a new Question Answering chain that uses the custom prompt template. This demonstrates how to customize the behavior of the LLM in the QA process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncustom_qa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=doc_store,\n    return_source_documents=False,\n    chain_type_kwargs={\"prompt\": custom_prompt_template},\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Source Nodes\nDESCRIPTION: This code accesses and displays the text from the source nodes. This allows inspection of the context used by the query engine to generate the response. It accesses the text of the first and second source nodes used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# First retrieved node\nresponse_vector.source_nodes[0].get_text()\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Second retrieved node\nresponse_vector.source_nodes[1].get_text()\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and AWS S3 Clients in Python\nDESCRIPTION: Sets the OpenAI API key from environment variables and defines the GPT model to use. Creates the boto3 S3 client and OpenAI client instances for subsequent interactions with Amazon S3 services and OpenAI Chat API respectively.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOpenAI.api_key = os.environ.get(\"OPENAI_API_KEY\")\nGPT_MODEL = \"gpt-3.5-turbo\"\n```\n\nLANGUAGE: python\nCODE:\n```\n# Optional - if you had issues loading the environment file, you can set the AWS values using the below code\n# os.environ['AWS_ACCESS_KEY_ID'] = ''\n# os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n\n# Create S3 client\ns3_client = boto3.client('s3')\n\n# Create openai client\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Creating Thread and Run\nDESCRIPTION: This code defines a function `create_thread_and_run` which encapsulates the creation of a new thread and the initiation of a run with a user's input message. It creates a new thread, then calls `submit_message` with `MATH_ASSISTANT_ID` and the user's input to send the message and start the run, returning the thread and run objects.  This function mimics the API's `create_and_run` function for demonstration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef create_thread_and_run(user_input):\n    thread = client.beta.threads.create()\n    run = submit_message(MATH_ASSISTANT_ID, thread, user_input)\n    return thread, run\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text Relevance with OpenAI GPT-3.5-Turbo (JavaScript)\nDESCRIPTION: Defines a JavaScript function that leverages the OpenAI SDK to find and return only sentences from a provided text that are relevant to a specific query, maximizing result quality for users. It requires an OpenAI API key configured in environment variables and depends on the OpenAI Node.js SDK. The function accepts plain text and a user's query as input, executes a GPT-3.5-Turbo completion request with a tightly-scoped prompt, and returns up to 10 relevant sentences or a fallback message. Output length and determinism are managed via 'max_tokens' and 'temperature' parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst getRelevantParts = async (text, query) => {\n    try {\n        // We use your OpenAI key to initialize the OpenAI client\n        const openAIKey = process.env[\"OPENAI_API_KEY\"];\n        const openai = new OpenAI({\n            apiKey: openAIKey,\n        });\n        const response = await openai.chat.completions.create({\n            // Using gpt-3.5-turbo due to speed to prevent timeouts. You can tweak this prompt as needed\n            model: \"gpt-3.5-turbo-0125\",\n            messages: [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant that finds relevant content in text based on a query. You only return the relevant sentences, and you return a maximum of 10 sentences\"},\n                {\"role\": \"user\", \"content\": `Based on this question: **\\\"${query}\\\"**, get the relevant parts from the following text:*****\\n\\n${text}*****. If you cannot answer the question based on the text, respond with 'No information provided'`}\n            ],\n            // using temperature of 0 since we want to just extract the relevant content\n            temperature: 0,\n            // using max_tokens of 1000, but you can customize this based on the number of documents you are searching. \n            max_tokens: 1000\n        });\n        return response.choices[0].message.content;\n    } catch (error) {\n        console.error('Error with OpenAI:', error);\n        return 'Error processing text with OpenAI' + error;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Execute query_weaviate and print results\nDESCRIPTION: This snippet calls the `query_weaviate` function with a sample query and prints the titles and certainty scores of the returned articles.  It iterates through the results and formats the output for display.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery_result = query_weaviate(\"modern art in Europe\", \"Article\")\ncounter = 0\nfor article in query_result[\"data\"][\"Get\"][\"Article\"]:\n    counter += 1\n    print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\")\n```\n\n----------------------------------------\n\nTITLE: Defining eval function - Python\nDESCRIPTION: Defines a function `eval` to evaluate the performance of a model in calling the correct function.  It takes the model name, system prompt, a list of functions, and a dictionary mapping prompts to the expected function names as input. It sends prompts to the model using `get_chat_completion`, measures latency and token usage, and compares the predicted function name with the expected name.  Finally it displays the results in a Pandas DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef eval(model: str, system_prompt: str, function_list, prompts_to_expected_tool_name):\n    \"\"\"\n    Evaluate the performance of a model in selecting the correct function based on given prompts.\n\n    Args:\n        model (str): The name of the model to be evaluated.\n        system_prompt (str): The system prompt to be used in the chat completion.\n        function_list (list): A list of functions that the model can call.\n        prompts_to_expected_tool_name (dict): A dictionary mapping prompts to their expected function names.\n\n    Returns:\n        None\n    \"\"\"\n\n    prompts_to_actual = []\n    latencies = []\n    tokens_used = []\n\n    for prompt, expected_function in prompts_to_expected_tool_name.items():\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        start_time = time.time()\n        completion, usage = get_chat_completion(\n            model=model,\n            messages=messages,\n            seed=42,\n            tools=function_list,\n            temperature=0.0,\n            tool_choice=\"required\",\n        )\n        end_time = time.time()\n\n        latency = (end_time - start_time) * 1000  # convert to milliseconds\n        latencies.append(latency)\n\n        prompts_to_actual.append(\n            {prompt: completion.tool_calls[0].function.name})\n\n        # Calculate tokens used\n        tokens_used.append(usage.total_tokens)\n\n    total_prompts = len(prompts_to_expected_tool_name)\n\n    # Calculate the number of matches\n    matches = sum(\n        1\n        for result in prompts_to_actual\n        if list(result.values())[0]\n        == prompts_to_expected_tool_name[list(result.keys())[0]]\n    )\n    match_percentage = (matches / total_prompts) * 100\n\n    # Calculate average latency\n    avg_latency = sum(latencies) / total_prompts\n    # Calculate average tokens used\n    avg_tokens_used = sum(tokens_used) / total_prompts\n\n    # Create a DataFrame to store the results\n    results_df = pd.DataFrame(columns=[\"Prompt\", \"Expected\", \"Match\"])\n\n    results_list = []\n    for result in prompts_to_actual:\n        prompt = list(result.keys())[0]\n        actual_function = list(result.values())[0]\n        expected_function = prompts_to_expected_tool_name[prompt]\n        match = actual_function == expected_function\n        results_list.append(\n            {\n                \"Prompt\": prompt,\n                \"Actual\": actual_function,\n                \"Expected\": expected_function,\n                \"Match\": \"Yes\" if match else \"No\",\n            }\n        )\n    results_df = pd.DataFrame(results_list)\n\n    def style_rows(row):\n        match = row[\"Match\"]\n        background_color = \"red\" if match == \"No\" else \"white\"\n        return [\"background-color: {}; color: black\".format(background_color)] * len(\n            row\n        )\n\n    styled_results_df = results_df.style.apply(style_rows, axis=1)\n\n    # Display the DataFrame as a table\n    display(styled_results_df)\n\n    print(\n        f\"Number of matches: {matches} out of {total_prompts} ({match_percentage:.2f}%)\"\n    )\n    print(f\"Average latency per request: {avg_latency:.2f} ms\")\n    print(f\"Average tokens used per request: {avg_tokens_used:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Embedding Customization\nDESCRIPTION: This code snippet imports necessary libraries for the embedding customization process. It imports libraries for type hinting, numerical operations, data manipulation, file I/O, plotting, random number generation, data splitting, and matrix optimization. It also imports custom utilities for handling embeddings and calculating cosine similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# imports\nfrom typing import List, Tuple  # for type hints\n\nimport numpy as np  # for manipulating arrays\nimport pandas as pd  # for manipulating data in dataframes\nimport pickle  # for saving the embeddings cache\nimport plotly.express as px  # for plots\nimport random  # for generating run IDs\nfrom sklearn.model_selection import train_test_split  # for splitting train & test data\nimport torch  # for matrix optimization\n\nfrom utils.embeddings_utils import get_embedding, cosine_similarity  # for embeddings\n```\n\n----------------------------------------\n\nTITLE: Generating Hallucinated Answers using OpenAI GPT-4o in Python\nDESCRIPTION: Defines an asynchronous function `hallucinate_answer` that prompts the OpenAI GPT-4o model to generate a confident but fabricated answer to a given question, specifically instructing it not to use the provided passage and to make up details. It uses `asyncio.gather` to execute these prompts concurrently for all question-answer pairs. The results are then used to create a new list of `QuestionAnswer` objects (`hallucinations`) where `generated_answer` contains the hallucinated response, filtering out simple yes/no answers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\nimport random\n\nrandom.seed(42)\n\n\nasync def hallucinate_answer(qa):\n    response = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\\\nYou are a helpful hallucinating assistant, who makes up fake answers to questions.\n\nAnswer the following question in 1 sentence. If you know the answer, then make up some fake\nsuperfluous details that are not in the passage you have memorized.\n\nMake sure to always answer it confidently, even if you don't know the answer. Do not use words\nlike \"perhaps\", \"likely\", \"maybe\", etc. or punctuation like \"...\".Do not admit that you cannot\nor do not know the answer.\"\"\",\n            },\n            {\"role\": \"user\", \"content\": qa.question},\n        ],\n        temperature=1,\n        max_tokens=100,\n    )\n    return response.choices[0].message.content\n\n\nhallucinated_answers = await asyncio.gather(\n    *[hallucinate_answer(qa) for qa in qa_pairs]\n)\n\n\nhallucinations = [\n    QuestionAnswer(\n        passage=qa.passage,\n        question=qa.question,\n        expected_answer=qa.expected_answer,\n        generated_answer=hallucination,\n    )\n    for (qa, hallucination) in zip(qa_pairs, hallucinated_answers)\n    # Exclude simple yes/no answers.\n    if \"yes\" not in hallucination.lower() and \"no\" not in hallucination.lower()\n]\n\nprint(\"Passage:\")\nprint(hallucinations[0].passage)\nprint(\"\\nQuestion:\")\nprint(hallucinations[0].question)\nprint(\"\\nExpected Answer:\")\nprint(hallucinations[0].expected_answer)\nprint(\"\\nGenerated Answer:\")\nprint(hallucinations[0].generated_answer)\n\nprint(\"\\n\\nNumber of hallucinations:\", len(hallucinations))\n```\n\n----------------------------------------\n\nTITLE: Defining Patch Structure Using Python Dataclasses\nDESCRIPTION: This snippet defines dataclasses Chunk, PatchAction, and Patch to represent the components of a patch. Chunk encapsulates individual line modifications with original line index, deleted lines, and inserted lines. PatchAction captures an action type (add, update, delete), optional new file content, associated chunks of changes, and potential move paths. Patch stores a dictionary mapping filenames to their PatchAction. These structured representations are prerequisites for parsing and applying patches in a consistent manner.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Chunk:\n    orig_index: int = -1\n    del_lines: List[str] = field(default_factory=list)\n    ins_lines: List[str] = field(default_factory=list)\n\n\n@dataclass\nclass PatchAction:\n    type: ActionType\n    new_file: Optional[str] = None\n    chunks: List[Chunk] = field(default_factory=list)\n    move_path: Optional[str] = None\n\n\n@dataclass\nclass Patch:\n    actions: Dict[str, PatchAction] = field(default_factory=dict)\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Get Movie Categories and Summary via GPT-4 Mini Model\nDESCRIPTION: Creates a Python function that sends a prompt and movie description to the GPT-4 mini model, requesting a JSON-formatted response with categories and a summary, utilizing the client.chat.completions.create method.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_categories(description):\n    response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    temperature=0.1,\n    # This is to enable JSON mode, making sure responses are valid json objects\n    response_format={ \n        \"type\": \"json_object\"\n    },\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": categorize_system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": description\n        }\n    ],\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Activating a Python Virtual Environment (MacOS/Unix)\nDESCRIPTION: This command activates the virtual environment 'openai-env' on MacOS or Unix-based systems. The `source` command executes the activation script within the current shell.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nsource openai-env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Azure Active Directory\nDESCRIPTION: Configures the OpenAI client using Azure Active Directory credentials for authentication. Uses DefaultAzureCredential for token management.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\nif use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining MongoDB Atlas Vector Search Query Function in Python\nDESCRIPTION: Defines a Python function `query_results` that performs a semantic search query against the MongoDB collection using the `$vectorSearch` aggregation stage. It takes a query string, generates an embedding for it, and uses the embedding to find semantically similar documents based on the vector index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef query_results(query, k):\n  results = collection.aggregate([\n    {\n        '$vectorSearch': {\n            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n            \"path\": EMBEDDING_FIELD_NAME,\n            \"queryVector\": generate_embedding(query),\n            \"numCandidates\": 50,\n            \"limit\": 5,\n        }\n    }\n    ])\n  return results\n\n```\n\n----------------------------------------\n\nTITLE: Processing Document List with Relevance Function in Python\nDESCRIPTION: Iterates through a list of documents, combines title and summary for each, and evaluates their relevance to a query using a document_relevance function while handling any exceptions that may occur.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\noutput_list = []\nfor x in result_list:\n    content = x[\"title\"] + \": \" + x[\"summary\"]\n\n    try:\n        output_list.append(document_relevance(query, document=content))\n\n    except Exception as e:\n        print(e)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI Schema for Adzviser Actions\nDESCRIPTION: This snippet defines the OpenAPI schema for the Adzviser actions. It includes information about the API title, description, version, and the server URL. The schema outlines multiple API paths, each describing a specific action to be performed. These paths include operations to retrieve metrics, breakdowns, search data, workspace information, and audit Google Ads settings. The `openapi`, `info`, `servers`, and `paths` fields are the core components here, including the method (GET/POST) and other details for each.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_googleads_adzviser.ipynb#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"openapi\": \"3.1.0\",\n  \"info\": {\n    \"title\": \"Adzviser Actions for GPT\",\n    \"description\": \"Equip GPTs with the ability to retrieve real-time reporting data and account settings from Google Ads\",\n    \"version\": \"v0.0.1\"\n  },\n  \"servers\": [\n    {\n      \"url\": \"https://copter.adzviser.com\"\n    }\n  ],\n  \"paths\": {\n    \"/google_ads/get_metrics_list\": {\n      \"get\": {\n        \"description\": \"Get the list of seletable Google Ads metrics, such as Cost, Roas, Impressions, etc.\",\n        \"operationId\": \"getGoogleAdsMetricsList\",\n        \"parameters\": [],\n        \"deprecated\": false,\n        \"security\": [],\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads/get_breakdowns_list\": {\n      \"get\": {\n        \"description\": \"Get the list of seletable Google Ads breakdowns such as Device, Keyword Text, Campaign Name etc.\",\n        \"operationId\": \"getGoogleAdsBreakdownsList\",\n        \"parameters\": [],\n        \"deprecated\": false,\n        \"security\": [],\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/search_bar\": {\n      \"post\": {\n        \"description\": \"Retrieve real-time reporting data such as impressions, cpc, etc. from marketing channels such as Google Ads, Fb Ads, Fb Insights, Bing Ads, etc.\",\n        \"operationId\": \"searchQuery\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/searchQueryRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"deprecated\": false,\n        \"security\": [\n          {\n            \"oauth2\": []\n          }\n        ],\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/workspace/get\": {\n      \"get\": {\n        \"description\": \"Retrieve a list of workspaces that have been created by the user and their data sources, such as Google Ads, Facebook Ads accounts connected with each.\",\n        \"operationId\": \"getWorkspace\",\n        \"parameters\": [],\n        \"deprecated\": false,\n        \"security\": [\n          {\n            \"oauth2\": []\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"$ref\": \"#/components/schemas/getWorkspaceResponse\"\n                }\n              }\n            }\n          }\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_merchant_center_connection\": {\n      \"post\": {\n        \"description\": \"Retrieve whether the Google Merchant Center is connected to the Google Ads account.\",\n        \"operationId\": \"checkGoogleAdsMerchantCenterConnection\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_account_settings\": {\n      \"post\": {\n        \"description\": \"Retrieve the Google Ads account settings such as whether auto tagging is enabled, inventory type, etc.\",\n        \"operationId\": \"checkGoogleAdsAccountSettings\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_negative_keywords_and_placements\": {\n      \"post\": {\n        \"description\": \"Retrieve the negative keywords and placements set in the Google Ads account.\",\n        \"operationId\": \"checkGoogleAdsNegativeKeywordsAndPlacements\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_remarketing_list\": {\n      \"post\": {\n        \"description\": \"Retrieve the remarketing list set in the Google Ads account.\",\n        \"operationId\": \"checkGoogleAdsRemarketingList\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_conversion_tracking\": {\n      \"post\": {\n        \"description\": \"Retrieve the conversion tracking status in the Google Ads account.\",\n        \"operationId\": \"checkGoogleAdsConversionTracking\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_bidding_strategy\": {\n      \"post\": {\n        \"description\": \"Retrieve the bidding strategy set for each active campaigns in the Google Ads account.\",\n        \"operationId\": \"checkGoogleAdsBiddingStrategy\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_search_campaign_basic\": {\n      \"post\": {\n        \"description\": \"Retrieve the basic information of the search campaigns such as campaign structure, language targeting, country targeting, etc.\",\n        \"operationId\": \"checkSearchCampaignBasic\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_search_campaign_detailed\": {\n      \"post\": {\n        \"description\": \"Retrieve the detailed information of the search campaigns such as best performing keywords, ad copies, ad extentions, pinned descriptions/headlines etc.\",\n        \"operationId\": \"checkSearchCampaignDetailed\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_dynamic_search_ads\": {\n      \"post\": {\n        \"description\": \"Retrieve the dynamic search ads information such as dynamic ad targets, negative ad targets, best performing search terms etc.\",\n        \"operationId\": \"checkDynamicSearchAds\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    },\n    \"/google_ads_audit/check_pmax_campaign\": {\n      \"post\": {\n        \"description\": \"Retrieve the performance of the pmax campaigns such as search themes, country/language targeting, final url expansions, excluded urls.\",\n        \"operationId\": \"checkPmaxCampaign\",\n        \"parameters\": [],\n        \"requestBody\": {\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"$ref\": \"#/components/schemas/googleAdsAuditRequest\"\n              }\n            }\n          },\n          \"required\": true\n        },\n        \"x-openai-isConsequential\": false\n      }\n    }\n  },\n  \"components\": {\n    \"schemas\": {\n      \"getWorkspaceResponse\": {\n        \"title\": \"getWorkspaceResponse\",\n        \"type\": \"array\",\n        \"description\": \"The list of workspaces created by the user on adzviser.com/main. A workspace can include multiple data sources\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"name\": {\n              \"title\": \"name\",\n              \"type\": \"string\",\n              \"description\": \"The name of a workspace\"\n            },\n            \"data_connections_accounts\": {\n              \"title\": \"data_connections_accounts\",\n              \"type\": \"array\",\n              \"description\": \"The list of data sources that the workspace is connected. The name can be an account name and type can be Google Ads/Facebook Ads/Bing Ads\",\n              \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"name\": {\n                    \"title\": \"name\",\n                    \"type\": \"string\",\n                    \"description\": \"The name of a data connection account\"\n                  }\n                }\n              }\n            }\n          }\n        }\n      },\n      \"googleAdsAuditRequest\": {\n        \"description\": \"Contains details about the Google Ads account audit request.\"\n      }\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Loading chat dataset from JSONL file\nDESCRIPTION: Reads the dataset from a specified JSON Lines file, parsing each line into a dictionary and storing all entries in a list. Prints dataset size and first example messages, enabling initial inspection and verification of data integrity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndata_path = \"data/toy_chat_fine_tuning.jsonl\"\n\n# Load the dataset\nwith open(data_path, 'r', encoding='utf-8') as f:\n    dataset = [json.loads(line) for line in f]\n\n# Initial dataset stats\nprint(\"Num examples:\", len(dataset))\nprint(\"First example:\")\nfor message in dataset[0][\"messages\"]:\n    print(message)\n```\n\n----------------------------------------\n\nTITLE: Generating Few-Shot RAG Prompts using Qdrant in Python\nDESCRIPTION: Defines a function `get_few_shot_prompt` that takes a DataFrame row containing a question and context. It embeds the question using `embedding_model` and queries a Qdrant collection twice: once for similar questions with answers (`is_impossible=False`) and once for similar questions without answers (`is_impossible=True`). It then constructs a formatted prompt list (suitable for OpenAI chat models) including system instructions and examples retrieved from Qdrant, followed by the original user question and context. This prompt structure facilitates few-shot learning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport qdrant_client.http.models as models # Assuming this import is needed\n\ndef get_few_shot_prompt(row):\n\n    query, row_context = row[\"question\"], row[\"context\"]\n\n    embeddings = list(embedding_model.embed([query]))\n    query_embedding = embeddings[0].tolist()\n\n    num_of_qa_to_retrieve = 5\n\n    # Query Qdrant for similar questions that have an answer\n    q1 = qdrant_client.search(\n        collection_name=collection_name,\n        query_vector=query_embedding,\n        with_payload=True,\n        limit=num_of_qa_to_retrieve,\n        query_filter=models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"is_impossible\",\n                    match=models.MatchValue(\n                        value=False,\n                    ),\n                ),\n            ],\n        )\n    )\n\n    # Query Qdrant for similar questions that are IMPOSSIBLE to answer\n    q2 = qdrant_client.search(\n        collection_name=collection_name,\n        query_vector=query_embedding,\n        query_filter=models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"is_impossible\",\n                    match=models.MatchValue(\n                        value=True,\n                    ),\n                ),\n            ]\n        ),\n        with_payload=True,\n        limit=num_of_qa_to_retrieve,\n    )\n\n\n    instruction = \"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\\n\\n\"\"\"\n    # If there is a next best question, add it to the prompt\n    \n    def q_to_prompt(q):\n        question, context = q.payload[\"question\"], q.payload[\"context\"]\n        answer = q.payload[\"answers\"][0] if len(q.payload[\"answers\"]) > 0 else \"I don't know\"\n        return [\n            {\n                \"role\": \"user\", \n                \"content\": f\"\"\"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\"\"\n            },\n            {\"role\": \"assistant\", \"content\": answer},\n        ]\n\n    rag_prompt = []\n    \n    if len(q1) >= 1:\n        rag_prompt += q_to_prompt(q1[1])\n    if len(q2) >= 1:\n        rag_prompt += q_to_prompt(q2[1])\n    if len(q1) >= 1:\n        rag_prompt += q_to_prompt(q1[2])\n    \n    \n\n    rag_prompt += [\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Question: {query}\\n\\nContext: {row_context}\\n\\nAnswer:\"\"\"\n        },\n    ]\n\n    rag_prompt = [{\"role\": \"system\", \"content\": instruction}] + rag_prompt\n    return rag_prompt\n```\n\n----------------------------------------\n\nTITLE: Extracting from Multiple PDF Pages (Python)\nDESCRIPTION: This function processes a list of base64 encoded images, extracts invoice data from each image using the extract_invoice_data function, appends the extracted JSON objects to a list, and saves the combined list as a JSON file. It creates the output directory if it does not exist and uses the original filename to create the output filename.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ndef extract_from_multiple_pages(base64_images, original_filename, output_directory):\n    entire_invoice = []\n\n    for base64_image in base64_images:\n        invoice_json = extract_invoice_data(base64_image)\n        invoice_data = json.loads(invoice_json)\n        entire_invoice.append(invoice_data)\n\n    # Ensure the output directory exists\n    os.makedirs(output_directory, exist_ok=True)\n\n    # Construct the output file path\n    output_filename = os.path.join(output_directory, original_filename.replace('.pdf', '_extracted.json'))\n    \n    # Save the entire_invoice list as a JSON file\n    with open(output_filename, 'w', encoding='utf-8') as f:\n        json.dump(entire_invoice, f, ensure_ascii=False, indent=4)\n    return output_filename\n```\n\n----------------------------------------\n\nTITLE: Executing a Weaviate QnA Query in Python (Example 1)\nDESCRIPTION: This snippet demonstrates using the previously defined `qna` function to ask a specific question (\"Did Alanis Morissette win a Grammy?\") against the \"Article\" collection in Weaviate. It then iterates through the results (limited to 1 by the `qna` function) and prints the answer found by the generative QnA module along with its vector distance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquery_result = qna(\"Did Alanis Morissette win a Grammy?\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Data from TREC Dataset in Python\nDESCRIPTION: Shows the first item from the loaded TREC dataset to understand its structure before processing. This helps verify that the dataset was loaded correctly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrec[0]\n```\n\n----------------------------------------\n\nTITLE: Printing Sample Question\nDESCRIPTION: Displays the first question from the loaded dataset to inspect its format and content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(questions[0])\n```\n\n----------------------------------------\n\nTITLE: Transcribing with Product Names in Prompt\nDESCRIPTION: This snippet transcribes the audio file using the `transcribe` function, including a list of product names in the prompt. It passes a string containing the correct spellings of specific product names to the `prompt` parameter.  This is used to influence Whisper's transcription by providing correct spellings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# add the correct spelling names to the prompt\ntranscribe(\n    prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\",\n    audio_filepath=ZyntriQix_filepath,\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Audio File for Azure OpenAI (Python)\nDESCRIPTION: This code downloads a sample audio file from a GitHub repository using the `requests` library.  It specifies the URL to a WAV file. The downloaded audio is saved to a local file named \"wikipediaOcelot.wav\". The sample audio files are used for testing the transcription functionality.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nsample_audio_url = \"https://github.com/Azure-Samples/cognitive-services-speech-sdk/raw/master/sampledata/audiofiles/wikipediaOcelot.wav\"\naudio_file = requests.get(sample_audio_url)\nwith open(\"wikipediaOcelot.wav\", \"wb\") as f:\n    f.write(audio_file.content)\n```\n\n----------------------------------------\n\nTITLE: GPT-4 Post-Processing with Expanded Product List\nDESCRIPTION: This code utilizes the `transcribe_with_spellcheck` function with an extended `system_prompt`. The `system_prompt` includes a larger list of product names and specifies formatting rules for the output.  The results are printed to show the improved accuracy with a broader product list.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array,  OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, AstroPixel Array, QuantumFlare Five, CyberPulse Six, VortexDrive Matrix, PhotonLink Ten, TriCircuit Array, PentaSync Seven, UltraWave Eight, QuantumVertex Nine, HyperHelix X, DigiSpiral Z, PentaQuark Eleven, TetraCube Twelve, GigaPhase Thirteen, EchoNeuron Fourteen, FusionPulse V15, MetaQuark Sixteen, InfiniCircuit Seventeen, TeraPulse Eighteen, ExoMatrix Nineteen, OrbiSync Twenty, QuantumHelix TwentyOne, NanoPhase TwentyTwo, TeraFractal TwentyThree, PentaHelix TwentyFour, ExoCircuit TwentyFive, HyperQuark TwentySix, GigaLink TwentySeven, FusionMatrix TwentyEight, InfiniFractal TwentyNine, MetaSync Thirty, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\nnew_text = transcribe_with_spellcheck(system_prompt, audio_filepath=ZyntriQix_filepath)\nprint(new_text)\n```\n\n----------------------------------------\n\nTITLE: Applying Few-Shot Learning with Data Processing in Python\nDESCRIPTION: This code applies a few-shot prompt to a dataframe of questions, using a progress bar to track execution. It processes each row with a custom answer_question function using the specified model and prompt function, then saves the results to a JSON file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf[\"ft_generated_answer_few_shot\"] = df.progress_apply(answer_question, model=model_id, prompt_func=get_few_shot_prompt, axis=1)\ndf.to_json(\"local_cache/100_val_ft_few_shot.json\", orient=\"records\", lines=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings\nDESCRIPTION: This function generates embeddings for each page of a document using OpenAI's embedding model. The `get_embedding` function takes text input and calls the OpenAI API to create embeddings. The code iterates through the `PageText` column of the DataFrame, generating an embedding for each page's content, which will be used for similarity searches in a RAG (Retrieval-Augmented Generation) system. It then adds these embeddings to the DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Function to get embeddings\ndef get_embedding(text_input):\n    response = oai_client.embeddings.create(\n        input=text_input,\n        model=\"text-embedding-3-large\"\n    )\n    return response.data[0].embedding\n\n\n# Generate embeddings with a progress bar\nembeddings = []\nfor text in tqdm(df['PageText'], desc='Generating Embeddings'):\n    embedding = get_embedding(text)\n    embeddings.append(embedding)\n\n# Add the embeddings to the DataFrame\ndf['Embeddings'] = embeddings\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: Installs necessary Python libraries (`langchain`, `openai`, `neo4j`) using pip. This step is optional if the libraries are already installed in the execution environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Optional: run to install the libraries locally if you haven't already \n!pip3 install langchain\n!pip3 install openai\n!pip3 install neo4j\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Similarity Search Function\nDESCRIPTION: This function, `query_neon`, performs a vector similarity search in the Neon Postgres database. It takes a user query, converts it to an embedding using the OpenAI API with the `text-embedding-3-small` model, and then queries the `articles` table for the nearest neighbors based on either `title_vector` or `content_vector`. The function returns the results as a list of tuples containing the ID, URL, title, and similarity score of each matching article.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef query_neon(query, collection_name, vector_name=\"title_vector\", top_k=20):\n\n    # Create an embedding vector from the user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=\"text-embedding-3-small\",\n    )[\"data\"][0][\"embedding\"]\n\n    # Convert the embedded_query to PostgreSQL compatible format\n    embedded_query_pg = \"[\" + \",\".join(map(str, embedded_query)) + \"]\"\n\n    # Create the SQL query\n    query_sql = f\"\"\"\n    SELECT id, url, title, l2_distance({vector_name},'{embedded_query_pg}'::VECTOR(1536)) AS similarity\n    FROM {collection_name}\n    ORDER BY {vector_name} <-> '{embedded_query_pg}'::VECTOR(1536)\n    LIMIT {top_k};\n    \"\"\"\n    # Execute the query\n    cursor.execute(query_sql)\n    results = cursor.fetchall()\n\n    return results\n```\n\n----------------------------------------\n\nTITLE: Defining SQLite Database Schema Utility Functions in Python\nDESCRIPTION: This snippet defines three related utility functions: 'get_table_names' retrieves all table names from a SQLite database, 'get_column_names' retrieves column names for a given table, and 'get_database_info' compiles an overview of the schema including each table and its columns. Requires an active sqlite3.Connection object. Inputs are the connection and optionally a table name; outputs are lists or dictionaries summarizing the database structure. Assumes valid table names and correct permissions; may raise exceptions for nonexistent tables or closed connections.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef get_table_names(conn):\n    \"\"\"Return a list of table names.\"\"\"\n    table_names = []\n    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    for table in tables.fetchall():\n        table_names.append(table[0])\n    return table_names\n\n\ndef get_column_names(conn, table_name):\n    \"\"\"Return a list of column names.\"\"\"\n    column_names = []\n    columns = conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n    for col in columns:\n        column_names.append(col[1])\n    return column_names\n\n\ndef get_database_info(conn):\n    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n    table_dicts = []\n    for table_name in get_table_names(conn):\n        columns_names = get_column_names(conn, table_name)\n        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n    return table_dicts\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Documents with Embedding-Based Query in Python\nDESCRIPTION: Performs a semantic search for relevant documents matching a query using the retriever object. Supplies a sample query regarding living without a bank account. Requires the retriever to be previously set up with embedded documents. Returns a list of matched documents ranked by relevance for downstream analysis or presentation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nquery_docs = retriever.get_relevant_documents(\"can you live without a bank account\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating Hybrid Fields for Redis Hybrid Search Queries - Python\nDESCRIPTION: Defines the `create_hybrid_field` function for constructing full-text hybrid field search expressions in RediSearch, usable alongside vector queries. Takes a field name and value, and returns a formatted query string for hybrid searches (e.g., '@field_name:\"value\"'). No dependencies other than standard Python formatting. Used as part of more complex search calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef create_hybrid_field(field_name: str, value: str) -> str:\n    return f'@{field_name}:\"{value}\"'\n\n# search the content vector for articles about famous battles in Scottish history and only include results with Scottish in the title\nresults = search_redis(redis_client,\n                       \"Famous battles in Scottish history\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"title\", \"Scottish\")\n                       )\n```\n\n----------------------------------------\n\nTITLE: Creating Weaviate Class Schema - Python\nDESCRIPTION: Executes the schema creation command using the Weaviate client. This registers the 'Article' class schema defined previously in the Weaviate instance. Requires a connected Weaviate client instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# add the Article schema\nclient.schema.create_class(article_schema)\n```\n\n----------------------------------------\n\nTITLE: Preparing training and validation datasets\nDESCRIPTION: Creates training and validation datasets by applying the conversation formatter function to selected subsets of the dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# use the first 100 rows of the dataset for training\ntraining_df = recipe_df.loc[0:100]\n\n# apply the prepare_example_conversation function to each row of the training_df\ntraining_data = training_df.apply(prepare_example_conversation, axis=1).tolist()\n\nfor example in training_data[:5]:\n    print(example)\n```\n\n----------------------------------------\n\nTITLE: Converting OpenAPI Specification to OpenAI Function Definitions in Python\nDESCRIPTION: Defines a function `openapi_to_functions` that takes a loaded OpenAPI specification object (with resolved references) and transforms it into a list of function definitions suitable for the OpenAI API's `tools` parameter. For each API endpoint and method, it extracts the `operationId` as the function name, the `description` or `summary` as the function description, and constructs the `parameters` schema from the `requestBody` and `parameters` defined in the spec. The snippet then calls this function with the previously loaded `openapi_spec` and pretty-prints each generated function definition.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef openapi_to_functions(openapi_spec):\n    functions = []\n\n    for path, methods in openapi_spec[\"paths\"].items():\n        for method, spec_with_ref in methods.items():\n            # 1. Resolve JSON references.\n            spec = jsonref.replace_refs(spec_with_ref)\n\n            # 2. Extract a name for the functions.\n            function_name = spec.get(\"operationId\")\n\n            # 3. Extract a description and parameters.\n            desc = spec.get(\"description\") or spec.get(\"summary\", \"\")\n\n            schema = {\"type\": \"object\", \"properties\": {}}\n\n            req_body = (\n                spec.get(\"requestBody\", {})\n                .get(\"content\", {})\n                .get(\"application/json\", {})\n                .get(\"schema\")\n            )\n            if req_body:\n                schema[\"properties\"][\"requestBody\"] = req_body\n\n            params = spec.get(\"parameters\", [])\n            if params:\n                param_properties = {\n                    param[\"name\"]: param[\"schema\"]\n                    for param in params\n                    if \"schema\" in param\n                }\n                schema[\"properties\"][\"parameters\"] = {\n                    \"type\": \"object\",\n                    \"properties\": param_properties,\n                }\n\n            functions.append(\n                {\"type\": \"function\", \"function\": {\"name\": function_name, \"description\": desc, \"parameters\": schema}}\n            )\n\n    return functions\n\n\nfunctions = openapi_to_functions(openapi_spec)\n\nfor function in functions:\n    pp(function)\n    print()\n```\n\n----------------------------------------\n\nTITLE: Listing Messages in a Thread\nDESCRIPTION: This code retrieves and displays the messages within a specific thread. It uses the OpenAI API's `client.beta.threads.messages.list()` method, passing the thread ID. The result, a list of messages in reverse-chronological order, is then presented using the `show_json` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmessages = client.beta.threads.messages.list(thread_id=thread.id)\nshow_json(messages)\n```\n\n----------------------------------------\n\nTITLE: Function for Counting Tokens in Messages Containing Function Calls or Tools in Python\nDESCRIPTION: Defines a function to estimate token usage for chat messages that include function definitions or tool calls, adjusting token counts based on model type. Uses encoding to tokenize function components and message content, considering properties like function name, description, and parameters. Incorporates error handling for unrecognized models. Utilizes the num_tokens_from_messages function for message token counts and aggregates totals.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef num_tokens_for_tools(functions, messages, model):\n    \n    # Initialize function settings to 0\n    func_init = 0\n    prop_init = 0\n    prop_key = 0\n    enum_init = 0\n    enum_item = 0\n    func_end = 0\n    \n    if model in [\n        \"gpt-4o\",\n        \"gpt-4o-mini\"\n    ]:\n        \n        # Set function settings for the above models\n        func_init = 7\n        prop_init = 3\n        prop_key = 3\n        enum_init = -3\n        enum_item = 3\n        func_end = 12\n    elif model in [\n        \"gpt-3.5-turbo\",\n        \"gpt-4\"\n    ]:\n        # Set function settings for the above models\n        func_init = 10\n        prop_init = 3\n        prop_key = 3\n        enum_init = -3\n        enum_item = 3\n        func_end = 12\n    else:\n        raise NotImplementedError(\n            f\"\"\"num_tokens_for_tools() is not implemented for model {model}.\"\"\"\n        )\n    \n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        print(\"Warning: model not found. Using o200k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"o200k_base\")\n    \n    func_token_count = 0\n    if len(functions) > 0:\n        for f in functions:\n            func_token_count += func_init  # Add tokens for start of each function\n            function = f[\"function\"]\n            f_name = function[\"name\"]\n            f_desc = function[\"description\"]\n            if f_desc.endswith(\".\"):\n                f_desc = f_desc[:-1]\n            line = f_name + \":\" + f_desc\n            func_token_count += len(encoding.encode(line))  # Add tokens for set name and description\n            if len(function[\"parameters\"][\"properties\"]) > 0:\n                func_token_count += prop_init  # Add tokens for start of each property\n                for key in list(function[\"parameters\"][\"properties\"].keys()):\n                    func_token_count += prop_key  # Add tokens for each set property\n                    p_name = key\n                    p_type = function[\"parameters\"][\"properties\"][key][\"type\"]\n                    p_desc = function[\"parameters\"][\"properties\"][key][\"description\"]\n                    if \"enum\" in function[\"parameters\"][\"properties\"][key].keys():\n                        func_token_count += enum_init  # Add tokens if property has enum list\n                        for item in function[\"parameters\"][\"properties\"][key][\"enum\"]:\n                            func_token_count += enum_item\n                            func_token_count += len(encoding.encode(item))\n                    if p_desc.endswith(\".\"):\n                        p_desc = p_desc[:-1]\n                    line = f\"{p_name}:{p_type}:{p_desc}\"\n                    func_token_count += len(encoding.encode(line))\n            func_token_count += func_end\n        \n    messages_token_count = num_tokens_from_messages(messages, model)\n    total_tokens = messages_token_count + func_token_count\n    \n    return total_tokens\n```\n\n----------------------------------------\n\nTITLE: Extracting Content from Chat Completion Response - Python\nDESCRIPTION: This snippet demonstrates how to extract the content of the assistant's message from a Chat Completions API response in Python. It assumes you have a completed response object and accesses the content through the `choices`, `message`, and `content` properties. There are no specific dependencies as it assumes you have an existing `completion` object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncompletion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Getting arXiv Articles\nDESCRIPTION: This code defines a function `get_articles` that searches arXiv for articles based on a user's query, downloads the corresponding PDF files, and stores the article metadata in a CSV file.  It utilizes the `arxiv` library to perform the search and the `openai` library (via `embedding_request`) to generate embeddings for the article titles. The function also incorporates retry logic using `tenacity`. The function returns a list of dictionaries containing the title, summary, article URL, and PDF URL of each found article. Each article's title and metadata including its embeddings are saved in the CSV file to be used by other functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\ndef get_articles(query, library=paper_dir_filepath, top_k=10):\n    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n    \"\"\"\n    client = arxiv.Client()\n    search = arxiv.Search(\n        query = query,\n        max_results = top_k\n    )\n    result_list = []\n    for result in client.results(search):\n        result_dict = {}\n        result_dict.update({\"title\": result.title})\n        result_dict.update({\"summary\": result.summary})\n\n        # Taking the first url provided\n        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n        result_list.append(result_dict)\n\n        # Store references in library file\n        response = embedding_request(text=result.title)\n        file_reference = [\n            result.title,\n            result.download_pdf(data_dir),\n            response.data[0].embedding,\n        ]\n\n        # Write to file\n        with open(library, \"a\") as f_object:\n            writer_object = writer(f_object)\n            writer_object.writerow(file_reference)\n            f_object.close()\n    return result_list\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query and Saving Results as CSV in Python\nDESCRIPTION: This code executes a provided SQL query against Snowflake, fetches all results, and writes them to a temporary CSV file with proper headers. It requires importing tempfile and csv modules, and assumes an active Snowflake connection. The function handles data serialization for subsequent storage or sharing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef write_results_to_csv(results, column_names):\n    try:\n        # Create a temporary file\n        temp_file = tempfile.NamedTemporaryFile(delete=False, mode='w', newline='')\n        csv_writer = csv.writer(temp_file)\n        csv_writer.writerow(column_names)  # Write the column headers\n        csv_writer.writerows(results)      # Write the data rows\n        temp_file.close()  # Close the file to flush the contents\n        return temp_file.name  # Return file path\n    except Exception as e:\n        logger.error(f\"Error writing results to CSV: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Data into Weaviate\nDESCRIPTION: Iterates through the `article_df` DataFrame and adds each article's data (title, content, and vector) as a data object to the 'Article' schema in Weaviate using batch processing. Progress messages are printed to the console every 100 objects.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n### Step 2 - import data\n\nprint(\"Uploading data with vectors to Article schema..\")\n\ncounter=0\n\nwith client.batch as batch:\n    for k,v in article_df.iterrows():\n        \n        # print update message every 100 objects        \n        if (counter %100 == 0):\n            print(f\"Import {counter} / {len(article_df)} \")\n        \n        properties = {\n            \"title\": v[\"title\"],\n            \"content\": v[\"text\"]\n        }\n        \n        vector = v[\"title_vector\"]\n        \n        batch.add_data_object(properties, \"Article\", None, vector)\n        counter = counter+1\n\nprint(f\"Importing ({len(article_df)}) Articles complete\")\n```\n\n----------------------------------------\n\nTITLE: Transcribing Multiple Audio Segments using Whisper (Python)\nDESCRIPTION: Iterates through the sorted list of segmented audio filenames (`audio_files`) using a list comprehension. For each filename, it calls the `transcribe_audio` function (defined earlier) to send the segment to the Whisper API for transcription. The resulting text transcriptions for all segments are collected into a list named `transcriptions`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Use a loop to apply the transcribe function to all audio files\ntranscriptions = [transcribe_audio(file, output_dir_trimmed) for file in audio_files]\n```\n\n----------------------------------------\n\nTITLE: Creating a Pinecone Index Programmatically with Specifications\nDESCRIPTION: This code defines a function to create a new Pinecone index with a specific name, dimension, and metric if it doesn't already exist. It uses the ServerlessSpec for deployment on AWS in the US-East-1 region, enabling scalable similarity searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef create_index():\n    index_name = \"openai-cookbook-pinecone-retool\"\n\n    if not pc.has_index(index_name):\n        pc.create_index(\n            name=index_name,\n            dimension=3072,\n            metric=\"cosine\",\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-east-1'\n            )\n        )\n    \n    return pc.Index(index_name)\n\nindex = create_index()\n\n```\n\n----------------------------------------\n\nTITLE: Grouping Short Chunks (Python)\nDESCRIPTION: Defines a function to group consecutive short text chunks together to create larger, more contextually coherent batches, up to a specified maximum token length, while discarding overly long individual chunks. Applies this function to refine the initial chunking.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef group_chunks(chunks, ntokens, max_len=15000, hard_max_len=16000):\n    \"\"\"\n    Group very short chunks, to form approximately page long chunks.\n    \"\"\"\n    batches = []\n    cur_batch = \"\"\n    cur_tokens = 0\n    \n    # iterate over chunks, and group the short ones together\n    for chunk, ntoken in zip(chunks, ntokens):\n        # discard chunks that exceed hard max length\n        if ntoken > hard_max_len:\n            print(f\"Warning: Chunk discarded for being too long ({ntoken} tokens > {hard_max_len} token limit). Preview: '{chunk[:50]}...'\"\n) # Newline added for example preview\n            continue\n\n        # if room in current batch, add new chunk\n        if cur_tokens + 1 + ntoken <= max_len:\n            cur_batch += \"\\n\\n\" + chunk\n            cur_tokens += 1 + ntoken  # adds 1 token for the two newlines\n        # otherwise, record the batch and start a new one\n        else:\n            batches.append(cur_batch)\n            cur_batch = chunk\n            cur_tokens = ntoken\n            \n    if cur_batch:  # add the last batch if it's not empty\n        batches.append(cur_batch)\n        \n    return batches\n\n\nchunks = group_chunks(chunks, ntokens)\nlen(chunks)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Utilities in Python\nDESCRIPTION: Imports essential Python libraries for data manipulation and tokenization, including pandas for dataframe operations, tiktoken for encoding tokens matching embedding model requirements, and a utility function get_embedding to obtain vector embeddings from text data. These are prerequisites for loading the dataset and processing embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport tiktoken\n\nfrom utils.embeddings_utils import get_embedding\n```\n\n----------------------------------------\n\nTITLE: Extracting Embedded Data from ZIP Archive\nDESCRIPTION: Extracts the downloaded ZIP file to the data directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Downloading Example Audio Files in Python\nDESCRIPTION: This snippet shows the initial setup for working with OpenAI's Whisper API, including importing necessary modules, configuring the OpenAI client using an API key, and downloading example audio files using urllib. Dependencies include the OpenAI Python library and access to the internet for file downloads. The code expects valid API key configuration and writes audio files to a local path.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# imports\nfrom openai import OpenAI  # for making OpenAI API calls\nimport urllib  # for downloading example audio files\nimport os\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\nLANGUAGE: Python\nCODE:\n```\n# set download paths\nup_first_remote_filepath = \"https://cdn.openai.com/API/examples/data/upfirstpodcastchunkthree.wav\"\nbbq_plans_remote_filepath = \"https://cdn.openai.com/API/examples/data/bbq_plans.wav\"\nproduct_names_remote_filepath = \"https://cdn.openai.com/API/examples/data/product_names.wav\"\n\n# set local save locations\nup_first_filepath = \"data/upfirstpodcastchunkthree.wav\"\nbbq_plans_filepath = \"data/bbq_plans.wav\"\nproduct_names_filepath = \"data/product_names.wav\"\n\n# download example audio files and save locally\nurllib.request.urlretrieve(up_first_remote_filepath, up_first_filepath)\nurllib.request.urlretrieve(bbq_plans_remote_filepath, bbq_plans_filepath)\nurllib.request.urlretrieve(product_names_remote_filepath, product_names_filepath)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Precomputed Embeddings Data into AnalyticDB - Python\nDESCRIPTION: Efficiently loads large CSV data into the 'public.articles' table in AnalyticDB. Applies a transformation to adjust array formats for PostgreSQL and streams the result with StringIO to the 'copy_expert' method for bulk insertion. Requires 'io' module, the CSV file in the designated directory, and a working psycopg2 cursor/connection. Commits all loaded data. Expects a properly formatted CSV file of embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\n# Path to your local CSV file\ncsv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\n\n# Define a generator function to process the file line by line\ndef process_file(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            # Replace '[' with '{' and ']' with '}'\n            modified_line = line.replace('[', '{').replace(']', '}')\n            yield modified_line\n\n# Create a StringIO object to store the modified lines\nmodified_lines = io.StringIO(''.join(list(process_file(csv_file_path))))\n\n# Create the COPY command for the copy_expert method\ncopy_command = '''\nCOPY public.articles (id, url, title, content, title_vector, content_vector, vector_id)\nFROM STDIN WITH (FORMAT CSV, HEADER true, DELIMITER ',');\n'''\n\n# Execute the COPY command using the copy_expert method\ncursor.copy_expert(copy_command, modified_lines)\n\n# Commit the changes\nconnection.commit()\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Masked Image Editing Prompt in Python\nDESCRIPTION: Creates a prompt for editing an image with a mask, describing the desired result with a character on a galaxy background.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nprompt_mask_edit = \"A strange character on a colorful galaxy background, with lots of stars and planets.\"\nmask = open(img_path_mask_alpha, \"rb\")\n```\n\n----------------------------------------\n\nTITLE: Editing Image with Mask in Python using GPT Image\nDESCRIPTION: Uses OpenAI's client to edit an image while applying a mask to protect specific areas from changes, following the instructions in the prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nresult_mask_edit = client.images.edit(\n    model=\"gpt-image-1\",         \n    prompt=prompt_mask_edit,\n    image=img_input,\n    mask=mask,\n    size=\"1024x1024\"\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading the Dataset\nDESCRIPTION: Setting up the OpenAI client and importing the necessary libraries to fetch and process the 20 Newsgroups dataset for sports classification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\nimport openai\nimport os\n\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\ncategories = ['rec.sport.baseball', 'rec.sport.hockey']\nsports_dataset = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories)\n```\n\n----------------------------------------\n\nTITLE: Load embedded data into pandas DataFrame\nDESCRIPTION: This code reads the CSV file containing embedded article data into a pandas DataFrame and previews the first few rows for verification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Default File Removal Function in Python\nDESCRIPTION: A basic implementation of a file removal function using `pathlib`. Takes a file path and removes the file, ignoring errors if the file does not exist (`missing_ok=True`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef remove_file(path: str) -> None:\n    pathlib.Path(path).unlink(missing_ok=True)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Redis\nDESCRIPTION: This snippet connects to a Redis database using the redis-py client with default host and port. It also includes necessary imports for interacting with RediSearch.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport redis\nfrom redis.commands.search.indexDefinition import (\n    IndexDefinition,\n    IndexType\n)\nfrom redis.commands.search.query import Query\nfrom redis.commands.search.field import (\n    TagField,\n    NumericField,\n    TextField,\n    VectorField\n)\n\nREDIS_HOST =  \"localhost\"\nREDIS_PORT = 6379\nREDIS_PASSWORD = \"\" # default for passwordless Redis\n\n# Connect to Redis\nredis_client = redis.Redis(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD\n)\nredis_client.ping()\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for OpenAI Function\nDESCRIPTION: This code defines a function `generate_functions` that returns a list containing a function schema for entity enrichment. The schema specifies the expected format for the entities, defining them as a dictionary where keys are entity labels (e.g., 'gpe', 'date') and values are lists of strings representing the extracted entities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef generate_functions(labels: dict) -> list:\n    return [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"enrich_entities\",\n                \"description\": \"Enrich Text with Knowledge Base Links\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                        \"properties\": {\n                            \"r'^(?:' + '|'.join({labels}) + ')$'\": \n                            {\n                                \"type\": \"array\",\n                                \"items\": {\n                                    \"type\": \"string\"\n                                }\n                            }\n                        },\n                        \"additionalProperties\": False\n                },\n            }\n        }\n    ]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Keyword Reference List and Embeddings\nDESCRIPTION: Creates a DataFrame of existing keywords and computes their embeddings for later comparison with new keywords extracted from images.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Existing keywords\nkeywords_list = ['industrial', 'metal', 'wood', 'vintage', 'bed']\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_keywords = pd.DataFrame(keywords_list, columns=['keyword'])\ndf_keywords['embedding'] = df_keywords['keyword'].apply(lambda x: get_embedding(x))\ndf_keywords\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings for Keyword Deduplication\nDESCRIPTION: Defines a function to generate embeddings for keywords using OpenAI's text-embedding model, which will be used to identify and merge similar or duplicate keywords.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Feel free to change the embedding model here\ndef get_embedding(value, model=\"text-embedding-3-large\"): \n    embeddings = client.embeddings.create(\n      model=model,\n      input=value,\n      encoding_format=\"float\"\n    )\n    return embeddings.data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Context Length Limit with API Call in Python\nDESCRIPTION: Creates a long text exceeding maximum token length and attempts to embed it to illustrate handling of errors when input exceeds model context limits. Catches and prints any BadRequestError raised by the API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nlong_text = 'AGI ' * 5000\ntry:\n    get_embedding(long_text)\nexcept openai.BadRequestError as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Defining QA Context and Evaluation Prompts with Python\nDESCRIPTION: This snippet organizes an article about Ada Lovelace, as well as lists of easy and medium QA questions based on the article's coverage. It constructs a prompt string (PROMPT) instructing the OpenAI model to return a boolean indicating if the given context is sufficient for answering the corresponding question. Dependencies: None for definitions, but the prompt assumes integration with OpenAI's API. Inputs: Article text and questions; Output: Prompt string for evaluation. Parameters such as article and question are to be formatted into the prompt. The snippet sets up the core context for subsequent QA sufficiency analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Article retrieved\ndata_lovelace_article = \"\"\"Augusta Ada King, Countess of Lovelace (nÃ©e Byron; 10 December 1815 â€“ 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.\nAda Byron was the only legitimate child of poet Lord Byron and reformer Lady Byron. All Lovelace's half-siblings, Lord Byron's other children, were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever. He died in Greece when Ada was eight. Her mother was anxious about her upbringing and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in him, naming her two sons Byron and Gordon. Upon her death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.\nHer educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday, and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\".\nWhen she was eighteen, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.\nBetween 1842 and 1843, Ada translated an article by the military engineer Luigi Menabrea (later Prime Minister of Italy) about the Analytical Engine, supplementing it with an elaborate set of seven notes, simply called \"Notes\".\nLovelace's notes are important in the early history of computers, especially since the seventh one contained what many consider to be the first computer programâ€”that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.\n\"\"\"\n\n# Questions that can be easily answered given the article\neasy_questions = [\n    \"What nationality was Ada Lovelace?\",\n    \"What was an important finding from Lovelace's seventh note?\",\n]\n\n# Questions that are not fully covered in the article\nmedium_questions = [\n    \"Did Lovelace collaborate with Charles Dickens\",\n    \"What concepts did Lovelace build with Charles Babbage\",\n]\n\nPROMPT = \"\"\"You retrieved this article: {article}. The question is: {question}.\nBefore even answering the question, consider whether you have sufficient information in the article to answer the question fully.\nYour output should JUST be the boolean true or false, of if you have sufficient information in the article to answer the question.\nRespond with just one word, the boolean true or false. You must output the word 'True', or the word 'False', nothing else.\n\"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Index in Zilliz\nDESCRIPTION: Creates an index on the embedding field of the collection using the specified index parameters, then loads the collection into memory for efficient querying.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create the index on the collection and load it.\ncollection.create_index(field_name=\"embedding\", index_params=INDEX_PARAM)\ncollection.load()\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client with API Key in Python\nDESCRIPTION: Initializes OpenAI client using an API key fetched from environment variables. The snippet includes instructions to set the API key as an environment variable or directly in the notebook. It safely handles missing keys by printing a diagnostic message. This setup enables authenticated API calls to OpenAI services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\n\n# Uncomment the following line to set the environment variable in the notebook\n# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\nif api_key:\n    client = OpenAI(api_key=api_key)\n    print(\"OpenAI client is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Building Hallucination Prompt for Abstract Generation Using OpenAI API in Python\nDESCRIPTION: Defines a function to create a structured prompt for a conversational language model (e.g., GPT). The prompt instructs the model to write a scientific abstract supporting or refuting a given claim, including a provided example to demonstrate the expected output format. The function takes a claim string as input and returns the prompt formatted as a list of messages compatible with OpenAI's chat completion API. This prompt directs the LLM to hallucinate abstracts similar in style and structure to the target corpus, facilitating improved document retrieval queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef build_hallucination_prompt(claim):\n    return [{'role': 'system', 'content': \"\"\"I will ask you to write an abstract for a scientific paper which supports or refutes a given claim. It should be written in scientific language, include a title. Output only one abstract, then stop.\n    \n    An Example:\n\n    Claim:\n    A high microerythrocyte count raises vulnerability to severe anemia in homozygous alpha (+)- thalassemia trait subjects.\n\n    Abstract:\n    BACKGROUND The heritable haemoglobinopathy alpha(+)-thalassaemia is caused by the reduced synthesis of alpha-globin chains that form part of normal adult haemoglobin (Hb). Individuals homozygous for alpha(+)-thalassaemia have microcytosis and an increased erythrocyte count. Alpha(+)-thalassaemia homozygosity confers considerable protection against severe malaria, including severe malarial anaemia (SMA) (Hb concentration < 50 g/l), but does not influence parasite count. We tested the hypothesis that the erythrocyte indices associated with alpha(+)-thalassaemia homozygosity provide a haematological benefit during acute malaria.   \n    METHODS AND FINDINGS Data from children living on the north coast of Papua New Guinea who had participated in a case-control study of the protection afforded by alpha(+)-thalassaemia against severe malaria were reanalysed to assess the genotype-specific reduction in erythrocyte count and Hb levels associated with acute malarial disease. We observed a reduction in median erythrocyte count of approximately 1.5 x 10(12)/l in all children with acute falciparum malaria relative to values in community children (p < 0.001). We developed a simple mathematical model of the linear relationship between Hb concentration and erythrocyte count. This model predicted that children homozygous for alpha(+)-thalassaemia lose less Hb than children of normal genotype for a reduction in erythrocyte count of >1.1 x 10(12)/l as a result of the reduced mean cell Hb in homozygous alpha(+)-thalassaemia. In addition, children homozygous for alpha(+)-thalassaemia require a 10% greater reduction in erythrocyte count than children of normal genotype (p = 0.02) for Hb concentration to fall to 50 g/l, the cutoff for SMA. We estimated that the haematological profile in children homozygous for alpha(+)-thalassaemia reduces the risk of SMA during acute malaria compared to children of normal genotype (relative risk 0.52; 95% confidence interval [CI] 0.24-1.12, p = 0.09).   \n    CONCLUSIONS The increased erythrocyte count and microcytosis in children homozygous for alpha(+)-thalassaemia may contribute substantially to their protection against SMA. A lower concentration of Hb per erythrocyte and a larger population of erythrocytes may be a biologically advantageous strategy against the significant reduction in erythrocyte count that occurs during acute infection with the malaria parasite Plasmodium falciparum. This haematological profile may reduce the risk of anaemia by other Plasmodium species, as well as other causes of anaemia. Other host polymorphisms that induce an increased erythrocyte count and microcytosis may confer a similar advantage.\n\n    End of example. \n    \n    \"\"\"}, {'role': 'user', 'content': f\"\"\"\n    Perform the task for the following claim.\n\n    Claim:\n    {claim}\n\n    Abstract:\n    \"\"\"}]\n```\n\n----------------------------------------\n\nTITLE: Creating and Connecting to Pinecone Index\nDESCRIPTION: This snippet creates a Pinecone index named 'wikipedia-articles' if it doesn't already exist, and connects to the index. If an index with the same name exists, it is deleted first. The index dimension is set to the length of the content vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Pick a name for the new index\nindex_name = 'wikipedia-articles'\n\n# Check whether the index with the same name already exists - if so, delete it\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n    \n# Creates new index\npinecone.create_index(name=index_name, dimension=len(article_df['content_vector'][0]))\nindex = pinecone.Index(index_name=index_name)\n\n# Confirm our index was created\npinecone.list_indexes()\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-embedded Wikipedia Articles\nDESCRIPTION: Downloads a zip file containing pre-embedded Wikipedia articles from a CDN URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Listing Module Items API Endpoint (YAML)\nDESCRIPTION: This snippet specifies the API endpoint `/courses/{course_id}/modules/{module_id}/items` for listing items within a specific module. It defines the required path parameters `course_id` and `module_id`, and implicitly the response structure. Dependencies include the Canvas LMS API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_canvas.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n  /courses/{course_id}/modules/{module_id}/items:\n    get:\n      operationId: listModuleItems\n      summary: List items in a module\n      description: Retrieves the list of items within a specific module in a Canvas course.\n      parameters:\n        - name: course_id\n          in: path\n          required: true\n          description: The ID of the course.\n          schema:\n```\n\n----------------------------------------\n\nTITLE: Creating Milvus Collection Schema (Python)\nDESCRIPTION: Defines the structure of the Milvus collection using `FieldSchema` objects, specifying data types, primary key, auto-id, and maximum length for VARCHAR fields. It then creates a `CollectionSchema` and finally instantiates a `Collection` object with the specified name and schema, preparing it for data insertion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create collection which includes the id, title, and embedding.\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='type', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='release_year', dtype=DataType.INT64),\n    FieldSchema(name='rating', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='description', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n]\nschema = CollectionSchema(fields=fields)\ncollection = Collection(name=COLLECTION_NAME, schema=schema)\n```\n\n----------------------------------------\n\nTITLE: Generating Embedding for Second Search Query (Python/OpenAI)\nDESCRIPTION: Generates a vector embedding for a different search term (\"unfortunate events in history\") using the `embed` function. This vector will be used for the subsequent search query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nsearchedEmbedding = embed(\"unfortunate events in history\")\n\n\n```\n\n----------------------------------------\n\nTITLE: Install packages and import modules\nDESCRIPTION: This snippet installs necessary Python packages (openai, pandas, wget, elasticsearch) using pip and imports the required modules from these packages for interacting with Elasticsearch, OpenAI, and handling data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install packages\n\n!python3 -m pip install -qU openai pandas wget elasticsearch\n\n# import modules\n\nfrom getpass import getpass\nfrom elasticsearch import Elasticsearch, helpers\nimport wget\nimport zipfile\nimport pandas as pd\nimport json\nimport openai\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Configure Environment\nDESCRIPTION: Imports required libraries including OpenAI for embeddings, pandas for data manipulation, ast for literal evaluation, and the Qdrant client. Sets the OpenAI embedding model and configures warning filters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pandas as pd\nfrom ast import literal_eval\nimport qdrant_client # Qdrant's client library for Python\n\n# This can be changed to the embedding model of your choice. Make sure its the same model that is used for generating embeddings\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n```\n\n----------------------------------------\n\nTITLE: Data Processing and Analysis in Python for Financial Data\nDESCRIPTION: This Python code snippet is designed for processing and analyzing large-scale financial datasets related to cryptocurrency assets. It leverages libraries like pandas and numpy, requiring dependencies such as pandas, numpy, and matplotlib for data manipulation and visualization. Key functionalities include data cleaning, aggregation, and statistical analysis to understand asset trends and risks. Inputs include CSV or JSON data files, and outputs are summarized statistical reports and plots.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/assets/011.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load market data\nmarket_data = pd.read_csv('cryptodata.csv')\n\n# Data cleaning\nmarket_data.dropna(inplace=True)\n\n# Calculate moving averages\nmarket_data['MA30'] = market_data['Close'].rolling(window=30).mean()\n\n# Generate summary statistics\nstats = market_data.describe()\n\n# Plot data\nimport matplotlib.pyplot as plt\nplt.plot(market_data['Date'], market_data['Close'])\nplt.title('Cryptocurrency Closing Prices')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Generated PowerPoint Presentation in Python\nDESCRIPTION: This code retrieves a PowerPoint presentation file from an API response and saves it locally. It includes error handling with a retry mechanism to wait for the assistant to complete processing the PPTX file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\n#May take 1-3 mins\nwhile True:\n    try:\n        response = get_response(thread)\n        pptx_id = response.data[0].content[0].text.annotations[0].file_path.file_id\n        print(\"Successfully retrieved pptx_id:\", pptx_id)\n        break\n    except Exception as e:\n        print(\"Assistant still working on PPTX...\")\n        time.sleep(10)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Fine-tuning\nDESCRIPTION: Transforming the raw dataset into a pandas DataFrame with prompt and completion columns, where prompts are the text content and completions are the sport labels.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nlabels = [sports_dataset.target_names[x].split('.')[-1] for x in sports_dataset['target']]\ntexts = [text.strip() for text in sports_dataset['data']]\ndf = pd.DataFrame(zip(texts, labels), columns = ['prompt','completion']) #[:300]\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Creating a Multimodal Tool-augmented Conversation\nDESCRIPTION: Shows how to combine image analysis and web search tools in a single API call to analyze an image, search for related information, and summarize findings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\nfrom IPython.display import Image, display\n\n# Display the image from the provided URL\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Cat_August_2010-4.jpg/2880px-Cat_August_2010-4.jpg\"\ndisplay(Image(url=url, width=400))\n\nresponse_multimodal = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"input_text\", \"text\": \n                 \"Come up with keywords related to the image, and search on the web using the search tool for any news related to the keywords\"\n                 \", summarize the findings and cite the sources.\"},\n                {\"type\": \"input_image\", \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Cat_August_2010-4.jpg/2880px-Cat_August_2010-4.jpg\"}\n            ]\n        }\n    ],\n    tools=[\n        {\"type\": \"web_search\"}\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Search Function for Book Recommendations\nDESCRIPTION: Creates a query function that takes user input, generates embeddings, and performs a vector similarity search in Zilliz to find and display the most relevant book recommendations based on the query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport textwrap\n\ndef query(queries, top_k = 5):\n    if type(queries) != list:\n        queries = [queries]\n    res = collection.search(embed(queries), anns_field='embedding', param=QUERY_PARAM, limit = top_k, output_fields=['title', 'description'])\n    for i, hit in enumerate(res):\n        print('Description:', queries[i])\n        print('Results:')\n        for ii, hits in enumerate(hit):\n            print('\\t' + 'Rank:', ii + 1, 'Score:', hits.score, 'Title:', hits.entity.get('title'))\n            print(textwrap.fill(hits.entity.get('description'), 88))\n            print()\n```\n\n----------------------------------------\n\nTITLE: Providing Troubleshooting Steps (Prompt)\nDESCRIPTION: Provides detailed, conditional instructions for handling customer service queries specifically classified as 'Technical Support - Troubleshooting'. It outlines a sequence of diagnostic steps, including asking about cables, router model, providing model-specific restart instructions, and escalating to IT support using a specific JSON output if necessary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_11\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: You will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\"IT support requested\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n\n\nUSER: I need to get my internet working again.\n```\n\n----------------------------------------\n\nTITLE: Running QA Chain on Selected Questions\nDESCRIPTION: Processes each selected question through the QA chain and prints the results. Includes a delay between questions to avoid rate limiting by the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfor question in selected_questions:\n    print(\">\", question)\n    print(qa.run(question), end=\"\\n\\n\")\n    # wait 20seconds because of the rate limit\n    time.sleep(20)\n```\n\n----------------------------------------\n\nTITLE: Visualizing classification performance with precision-recall curves in Python\nDESCRIPTION: This code visualizes the performance of the classification model using precision-recall curves for each class (1-5 star ratings). It uses a custom utility function from the project's utils package to plot the multiclass precision-recall curves.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import plot_multiclass_precision_recall\n\nplot_multiclass_precision_recall(probas, y_test, [1, 2, 3, 4, 5], clf)\n```\n\n----------------------------------------\n\nTITLE: Agent Executor Query Execution with Contextual Memory in LangChain (Python)\nDESCRIPTION: Executes the agent_executor with the memory module for the question 'How many people live in canada as of 2023?', utilizing prior conversation if available. Depends on an agent_executor setup with integrated memory. The agent response can be influenced by conversational context and will use the tools if appropriate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nagent_executor.run(\"How many people live in canada as of 2023?\")\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI and Zilliz Connection Parameters\nDESCRIPTION: Sets up the configuration parameters needed for connecting to Zilliz and OpenAI, including database connection details, collection settings, embedding model specifications, and batch processing parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nURI = 'your_uri'\nTOKEN = 'your_token' # TOKEN == user:password or api_key\nCOLLECTION_NAME = 'book_search'\nDIMENSION = 1536\nOPENAI_ENGINE = 'text-embedding-3-small'\nopenai.api_key = 'sk-your-key'\n\nINDEX_PARAM = {\n    'metric_type':'L2',\n    'index_type':\"AUTOINDEX\",\n    'params':{}\n}\n\nQUERY_PARAM = {\n    \"metric_type\": \"L2\",\n    \"params\": {},\n}\n\nBATCH_SIZE = 1000\n```\n\n----------------------------------------\n\nTITLE: Tracking Custom Attributes in Monitored OpenAI ChatCompletion - Python\nDESCRIPTION: This snippet runs a ChatCompletion using OpenAI and tracks additional parameters such as the system prompt, prompt template, and problem parameters as monitor_attributes, allowing richer filtering and analysis in the Weave dashboard. Dependencies include openai and a valid monitored wrapper. Inputs include dynamically composed prompts and their meta-context. The full structured response is returned from the API. This approach aids in detailed experiment logging.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nsystem_prompt = \"you always write in bullet points\"\nprompt_template = 'solve the following equation step by step: {equation}'\nparams = {'equation': '4 * (3 - 1)'}\nopenai.ChatCompletion.create(model=OPENAI_MODEL,\n                             messages=[\n                                    {\"role\": \"system\", \"content\": system_prompt},\n                                    {\"role\": \"user\", \"content\": prompt_template.format(**params)},\n                                ],\n                             # you can add additional attributes to the logged record\n                             # see the monitor_api notebook for more examples\n                             monitor_attributes={\n                                 'system_prompt': system_prompt,\n                                 'prompt_template': prompt_template,\n                                 'params': params\n                             })\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data for Model Evaluation\nDESCRIPTION: Loading the validation dataset to use for testing the fine-tuned model's performance on unseen data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntest = pd.read_json('sport2_prepared_valid.jsonl', lines=True)\ntest.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Quote Search Function with Vector Similarity and Filters in Python\nDESCRIPTION: Defines a Python function `find_quote_and_author` that performs vector similarity search in a Cassandra table. It computes the embedding for a query, constructs a CQL statement dynamically based on optional author and tag filters, executes the query sorted by ANN of the embedding vector, and returns a list of matching quotes and authors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef find_quote_and_author(query_quote, n, author=None, tags=None):\n    query_vector = client.embeddings.create(\n        input=[query_quote],\n        model=embedding_model_name,\n    ).data[0].embedding\n    # depending on what conditions are passed, the WHERE clause in the statement may vary.\n    where_clauses = []\n    where_values = []\n    if author:\n        where_clauses += [\"author = %s\"]\n        where_values += [author]\n    if tags:\n        for tag in tags:\n            where_clauses += [\"tags CONTAINS %s\"]\n            where_values += [tag]\n    # The reason for these two lists above is that when running the CQL search statement the values passed\n    # must match the sequence of \"?\" marks in the statement.\n    if where_clauses:\n        search_statement = f\"\"\"SELECT body, author FROM {keyspace}.philosophers_cql\n            WHERE {' AND '.join(where_clauses)}\n            ORDER BY embedding_vector ANN OF %s\n            LIMIT %s;\n        \"\"\"\n    else:\n        search_statement = f\"\"\"SELECT body, author FROM {keyspace}.philosophers_cql\n            ORDER BY embedding_vector ANN OF %s\n            LIMIT %s;\n        \"\"\"\n    # For best performance, one should keep a cache of prepared statements (see the insertion code above)\n    # for the various possible statements used here.\n    # (We'll leave it as an exercise to the reader to avoid making this code too long.\n    # Remember: to prepare a statement you use '?' instead of '%s'.)\n    query_values = tuple(where_values + [query_vector] + [n])\n    result_rows = session.execute(search_statement, query_values)\n    return [\n        (result_row.body, result_row.author)\n        for result_row in result_rows\n    ]\n```\n\n----------------------------------------\n\nTITLE: Defining Notion API Actions with OpenAPI Schema (YAML)\nDESCRIPTION: This OpenAPI 3.1.0 schema defines several Notion API endpoints (users, blocks, comments, pages, databases, search) for use within a Custom GPT action. It specifies paths, HTTP methods (GET/POST), parameters (including the required `Notion-Version` header set to '2022-06-28'), request bodies (for POST methods like database query and search), and expected JSON response structures. This schema enables a Custom GPT to interact with Notion by listing users, retrieving block children, fetching comments, getting page properties, querying databases, and performing searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_notion.ipynb#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Notion API\n  description: API for interacting with Notion's pages, databases, and users.\n  version: 1.0.0\nservers:\n  - url: https://api.notion.com/v1\n    description: Main Notion API server\npaths:\n  /users:\n    get:\n      operationId: listAllUsers\n      summary: List all users\n      parameters:\n        - name: Notion-Version\n          in: header\n          required: true\n          schema:\n            type: string\n          example: 2022-06-28\n          constant: 2022-06-28\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  results:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        id:\n                          type: string\n                        name:\n                          type: string\n                        avatar_url:\n                          type: string\n                        type:\n                          type: string\n  /blocks/{block_id}/children:\n    get:\n      operationId: retrieveBlockChildren\n      summary: Retrieve block children\n      parameters:\n        - name: block_id\n          in: path\n          required: true\n          schema:\n            type: string\n        - name: Notion-Version\n          in: header\n          required: true\n          schema:\n            type: string\n          example: 2022-06-28\n          constant: 2022-06-28\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  object:\n                    type: string\n                  results:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        id:\n                          type: string\n                        type:\n                          type: string\n                        has_children:\n                          type: boolean\n  /comments:\n    get:\n      operationId: retrieveComments\n      summary: Retrieve comments\n      parameters:\n        - name: Notion-Version\n          in: header\n          required: true\n          schema:\n            type: string\n          example: 2022-06-28\n          constant: 2022-06-28\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  results:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        id:\n                          type: string\n                        text:\n                          type: string\n                        created_time:\n                          type: string\n                          format: date-time\n                        created_by:\n                          type: object\n                          properties:\n                            id:\n                              type: string\n                            name:\n                              type: string\n  /pages/{page_id}/properties/{property_id}:\n    get:\n      operationId: retrievePagePropertyItem\n      summary: Retrieve a page property item\n      parameters:\n        - name: page_id\n          in: path\n          required: true\n          schema:\n            type: string\n        - name: property_id\n          in: path\n          required: true\n          schema:\n            type: string\n        - name: Notion-Version\n          in: header\n          required: true\n          schema:\n            type: string\n          example: 2022-06-28\n          constant: 2022-06-28\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  id:\n                    type: string\n                  type:\n                    type: string\n                  title:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        type:\n                          type: string\n                        text:\n                          type: object\n                          properties:\n                            content:\n                              type: string\n  /databases/{database_id}/query:\n    post:\n      operationId: queryDatabase\n      summary: Query a database\n      parameters:\n        - name: database_id\n          in: path\n          required: true\n          schema:\n            type: string\n        - name: Notion-Version\n          in: header\n          required: true\n          schema:\n            type: string\n          example: 2022-06-28\n          constant: 2022-06-28\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                filter:\n                  type: object\n                sorts:\n                  type: array\n                  items:\n                    type: object\n                start_cursor:\n                  type: string\n                page_size:\n                  type: integer\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  object:\n                    type: string\n                  results:\n                    type: array\n                    items:\n                      type: object\n                  next_cursor:\n                    type: string\n                  has_more:\n                    type: boolean\n  /search:\n    post:\n      operationId: search\n      summary: Search\n      parameters:\n        - name: Notion-Version\n          in: header\n          required: true\n          schema:\n            type: string\n          example: 2022-06-28\n          constant: 2022-06-28\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                query:\n                  type: string\n                filter:\n                  type: object\n                  properties:\n                    value:\n                      type: string\n                    property:\n                      type: string\n                sort:\n                  type: object\n                  properties:\n                    direction:\n                      type: string\n                    timestamp:\n                      type: string\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  object:\n                    type: string\n                  results:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        id:\n                          type: string\n                        title:\n                          type: array\n                          items:\n                            type: object\n                            properties:\n                              type:\n                                type: string\n                              text:\n                                type: object\n                                properties:\n                                  content:\n                                    type: string\n```\n\n----------------------------------------\n\nTITLE: Defining the Python Execution Tool (JSON-like Structure)\nDESCRIPTION: Defines the structure of the `python_bash_patch_tool` using a Python dictionary, intended for use with an API or framework that accepts tool definitions in this format (likely JSON). The tool, named 'python', executes Python code, shell commands (prefixed with '!'), and the custom 'apply_patch' command within a stateful, sandboxed environment with specific input parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython_bash_patch_tool = {\n  \"type\": \"function\",\n  \"name\": \"python\",\n  \"description\": PYTHON_TOOL_DESCRIPTION,\n  \"parameters\": {\n      \"type\": \"object\",\n      \"strict\": True,\n      \"properties\": {\n          \"input\": {\n              \"type\": \"string\",\n              \"description\": \" The Python code, terminal command (prefaced by exclamation mark), or apply_patch command that you wish to execute.\",\n          }\n      },\n      \"required\": [\"input\"],\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Initializing the OpenAI Client - Python\nDESCRIPTION: This snippet imports required Python packages for date handling, display, JSON parsing, vector math, API calls, and progress bars. It also loads API keys from environment variables, sets the GPT model, and initializes the OpenAI client for subsequent completions and embeddings calls. All dependencies must be installed and respective API keys should be set prior to running.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Dependencies\nfrom datetime import date, timedelta  # date handling for fetching recent news\nfrom IPython import display  # for pretty printing\nimport json  # for parsing the JSON api responses and model outputs\nfrom numpy import dot  # for cosine similarity\nfrom openai import OpenAI\nimport os  # for loading environment variables\nimport requests  # for making the API requests\nfrom tqdm.notebook import tqdm  # for printing progress bars\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n# Load environment variables\nnews_api_key = os.getenv(\"NEWS_API_KEY\")\n\nGPT_MODEL = \"gpt-3.5-turbo\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ChatGPT Instructions for Box API Integration - Python\nDESCRIPTION: This snippet defines the instructional prompt for ChatGPT, guiding GPT behavior when interacting with a user's Box.com account. It details rules for secure and private data handling, how to process search or retrieval requests, and output formatting. No external Python dependencies are required as this functions within the AI configuration layer; users are expected to consult Box API documentation for endpoint references. Instructions include when and how to retrieve and display file information, suggest query patterns, respond professionally, and handle ambiguous requests. The structured content forms the prompt template for configuring a Custom GPT action with Box integration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n**context** \n\nThis GPT will connect to your Box.com account to search files and folders, providing accurate and helpful responses based on the user's queries. It will assist with finding, organizing, and retrieving information stored in Box.com. Ensure secure and private handling of any accessed data. Avoid performing any actions that could modify or delete files unless explicitly instructed. Prioritize clarity and efficiency in responses. Use simple language for ease of understanding. Ask for clarification if a request is ambiguous or if additional details are needed to perform a search. Maintain a professional and friendly tone, ensuring users feel comfortable and supported.\n\n\nPlease use this website for instructions using the box API : https://developer.box.com/reference/ each endpoint can be found from this reference documentation\n\nUsers can search with the Box search endpoint or Box metadata search endpoint\n\n**instructions**\nWhen retrieving file information from Box provide as much details as possible and format into a table when more than one file is returned, include the modified date, created date and any other headers you might find valuable\n\nProvide insights to files and suggest patterns for users, gives example queries and suggestions when appropriate\n\nWhen a user wants to compare files retrieve the file for the user with out asking\n```\n\n----------------------------------------\n\nTITLE: Defining a function to create a query message for GPT - Python\nDESCRIPTION: This snippet defines a function `query_message` that constructs a message for GPT, incorporating relevant source texts pulled from a DataFrame. It takes a query string, a DataFrame, the model name, and a token budget. The function uses the `strings_ranked_by_relatedness` to find relevant texts, and iteratively adds them to the message while staying within the token budget. Requires `tiktoken`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef num_tokens(text: str, model: str = GPT_MODELS[0]) -> int:\n    \"\"\"Return the number of tokens in a string.\"\"\"\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n\ndef query_message(\n    query: str,\n    df: pd.DataFrame,\n    model: str,\n    token_budget: int\n) -> str:\n    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n    introduction = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n    question = f\"\\n\\nQuestion: {query}\"\n    message = introduction\n    for string in strings:\n        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n        if (\n            num_tokens(message + next_article + question, model=model)\n            > token_budget\n        ):\n            break\n        else:\n            message += next_article\n    return message + question\n```\n\n----------------------------------------\n\nTITLE: Querying GPT-4 without Augmentation Python\nDESCRIPTION: Queries GPT-4 without the context from the knowledge base. It sends the original query to GPT-4, to compare the answer with the retrieval augmented generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nres = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": primer},\n        {\"role\": \"user\", \"content\": query}\n    ]\n)\ndisplay(Markdown(res['choices'][0]['message']['content']))\n```\n\n----------------------------------------\n\nTITLE: Batch Storing Embeddings and Metadata in Astra DB Collection with Python\nDESCRIPTION: Processes the philosopher quote dataset in batches, computes embeddings using OpenAI, and inserts each batch into the Astra DB vector collection along with quote text, author, and parsed tags as metadata. Batching (size 20) reduces API calls and increases throughput. Embeddings must be assigned to the reserved '$vector' field; each metadata object is constructed for insertion. Requires initialized OpenAI client, model name, collection, and dataset variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 20\n\nnum_batches = ((len(philo_dataset) + BATCH_SIZE - 1) // BATCH_SIZE)\n\nquotes_list = philo_dataset[\"quote\"]\nauthors_list = philo_dataset[\"author\"]\ntags_list = philo_dataset[\"tags\"]\n\nprint(\"Starting to store entries: \", end=\"\")\nfor batch_i in range(num_batches):\n    b_start = batch_i * BATCH_SIZE\n    b_end = (batch_i + 1) * BATCH_SIZE\n    # compute the embedding vectors for this batch\n    b_emb_results = client.embeddings.create(\n        input=quotes_list[b_start : b_end],\n        model=embedding_model_name,\n    )\n    # prepare the documents for insertion\n    b_docs = []\n    for entry_idx, emb_result in zip(range(b_start, b_end), b_emb_results.data):\n        if tags_list[entry_idx]:\n            tags = {\n                tag: True\n                for tag in tags_list[entry_idx].split(\";\")\n            }\n        else:\n            tags = {}\n        b_docs.append({\n            \"quote\": quotes_list[entry_idx],\n            \"$vector\": emb_result.embedding,\n            \"author\": authors_list[entry_idx],\n            \"tags\": tags,\n        })\n    # write to the vector collection\n    collection.insert_many(b_docs)\n    print(f\"[{len(b_docs)}]\", end=\"\")\n\nprint(\"\\nFinished storing entries.\")\n```\n\n----------------------------------------\n\nTITLE: Downloading the dataset\nDESCRIPTION: This code snippet downloads a CSV file containing pre-chunked text and pre-computed embeddings related to the 2022 Winter Olympics from a specified URL. It checks if the file already exists locally and downloads it if it doesn't. The downloaded file is then used for further data analysis and processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# download pre-chunked text and pre-computed embeddings\n# this file is ~200 MB, so may take a minute depending on your connection speed\nembeddings_path = \"https://cdn.openai.com/API/examples/data/winter_olympics_2022.csv\"\nfile_path = \"winter_olympics_2022.csv\"\n\nif not os.path.exists(file_path):\n    wget.download(embeddings_path, file_path)\n    print(\"File downloaded successfully.\")\nelse:\n    print(\"File already exists in the local file system.\")\n```\n\n----------------------------------------\n\nTITLE: Generating Hallucinated Abstracts for Claims Using OpenAI Chat Completion in Python\nDESCRIPTION: Implements a Python function to hallucinate supporting or refuting abstracts for a list of claims by calling the OpenAI chat completion API. For each claim, it generates a prompt using the previously defined build_hallucination_prompt function, submits it to the model specified by OPENAI_MODEL, and collects the generated abstracts. This function requires an initialized OpenAI client and a valid model name. The output is a list of hallucinated abstracts corresponding to each input claim, intended for later use as improved retrieval queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef hallucinate_evidence(claims):\n    responses = []\n    # Query the OpenAI API\n    for claim in claims:\n        response = client.chat.completions.create(\n            model=OPENAI_MODEL,\n            messages=build_hallucination_prompt(claim),\n        )\n        responses.append(response.choices[0].message.content)\n    return responses\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Windows CMD)\nDESCRIPTION: Provides the command to set the `OPENAI_API_KEY` environment variable for the current session using the Command Prompt (cmd) on Windows. Replace 'your-api-key-here' with your actual key. For permanent setup, system properties should be used instead.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/node-setup.txt#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nsetx OPENAI_API_KEY \"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Batch Embedding and Insertion into Cassandra Vector Store in Python\nDESCRIPTION: Prepares a CQL INSERT statement for the vector table, defines a batch size, iterates through the dataset in batches, computes embeddings for each batch using OpenAI, formats the data (including parsing tags and generating a UUID), and executes the prepared statement for each entry in the batch.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprepared_insertion = session.prepare(\n    f\"INSERT INTO {keyspace}.philosophers_cql (quote_id, author, body, embedding_vector, tags) VALUES (?, ?, ?, ?, ?);\"\n)\n\nBATCH_SIZE = 20\n\nnum_batches = ((len(philo_dataset) + BATCH_SIZE - 1) // BATCH_SIZE)\n\nquotes_list = philo_dataset[\"quote\"]\nauthors_list = philo_dataset[\"author\"]\ntags_list = philo_dataset[\"tags\"]\n\nprint(\"Starting to store entries:\")\nfor batch_i in range(num_batches):\n    b_start = batch_i * BATCH_SIZE\n    b_end = (batch_i + 1) * BATCH_SIZE\n    # compute the embedding vectors for this batch\n    b_emb_results = client.embeddings.create(\n        input=quotes_list[b_start : b_end],\n        model=embedding_model_name,\n    )\n    # prepare the rows for insertion\n    print(\"B \", end=\"\")\n    for entry_idx, emb_result in zip(range(b_start, b_end), b_emb_results.data):\n        if tags_list[entry_idx]:\n            tags = {\n                tag\n                for tag in tags_list[entry_idx].split(\";\")\n            }\n        else:\n            tags = set()\n        author = authors_list[entry_idx]\n        quote = quotes_list[entry_idx]\n        quote_id = uuid4()  # a new random ID for each quote. In a production app you'll want to have better control...\n        session.execute(\n            prepared_insertion,\n            (quote_id, author, quote, emb_result.embedding, tags),\n        )\n        print(\"*\", end=\"\")\n    print(f\" done ({len(b_emb_results.data)})\")\n\nprint(\"\\nFinished storing entries.\")\n```\n\n----------------------------------------\n\nTITLE: Converting datetime Objects to ISO Format for JSON Serialization in Python\nDESCRIPTION: Defines a helper function that converts datetime objects to ISO 8601 formatted strings for JSON serialization. This is necessary because datetime objects are not natively serializable to JSON.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef datetime_converter(obj):\n    if isinstance(obj, datetime.datetime):\n        return obj.isoformat()\n    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n```\n\n----------------------------------------\n\nTITLE: Create or Recreate Qdrant Collection\nDESCRIPTION: Creates or recreates a Qdrant collection named 'Articles'. It configures two named vectors, 'title' and 'content', specifying their size (determined from the DataFrame) and the distance metric (Cosine similarity) for indexing and search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Get the vector size from the first row to set up the collection\nvector_size = len(article_df['content_vector'][0])\n\n# Set up the collection with the vector configuration. You need to declare the vector size and distance metric for the collection. Distance metric enables vector database to index and search vectors efficiently.\nqdrant.recreate_collection(\n    collection_name='Articles',\n    vectors_config={\n        'title': rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n        'content': rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Indexing Documents in Redis\nDESCRIPTION: This snippet defines a function to index documents in Redis, using the OpenAI embeddings generated previously. It batches calls using Redis pipelines, converts embeddings to byte vectors, and stores documents in Redis with the specified prefix.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n    product_vectors = embeddings_batch_request(documents)\n    records = documents.to_dict(\"records\")\n    batchsize = 500\n\n    # Use Redis pipelines to batch calls and save on round trip network communication\n    pipe = client.pipeline()\n    for idx,doc in enumerate(records,start=1):\n        key = f\"{prefix}:{str(doc['product_id'])}\"\n\n        # create byte vectors\n        text_embedding = np.array((product_vectors[idx-1]), dtype=np.float32).tobytes()\n\n        # replace list of floats with byte vectors\n        doc[\"product_vector\"] = text_embedding\n\n        pipe.hset(key, mapping = doc)\n        if idx % batchsize == 0:\n            pipe.execute()\n    pipe.execute()\n```\n\n----------------------------------------\n\nTITLE: Querying for Similar Items by Shared Entities and Relationships in Neo4j Graph using Python\nDESCRIPTION: Defines 'query_similar_items' that takes a product ID and threshold to find similar products using two Cypher queries: one fetching products in the same category sharing at least one common entity, and one fetching products sharing at least a specified number of common entities. It merges results avoiding duplicates and returns a list of similar product dictionaries with 'id' and 'name'. Requires a connected graph database and valid product IDs as integers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Adjust the relationships_threshold to return products that have more or less relationships in common\ndef query_similar_items(product_id, relationships_threshold = 3):\n    \n    similar_items = []\n        \n    # Fetching items in the same category with at least 1 other entity in common\n    query_category = '''\n            MATCH (p:Product {id: $product_id})-[:hasCategory]->(c:category)\n            MATCH (p)-->(entity)\n            WHERE NOT entity:category\n            MATCH (n:Product)-[:hasCategory]->(c)\n            MATCH (n)-->(commonEntity)\n            WHERE commonEntity = entity AND p.id <> n.id\n            RETURN DISTINCT n;\n        '''\n    \n\n    result_category = graph.query(query_category, params={\"product_id\": int(product_id)})\n    #print(f\"{len(result_category)} similar items of the same category were found.\")\n          \n    # Fetching items with at least n (= relationships_threshold) entities in common\n    query_common_entities = '''\n        MATCH (p:Product {id: $product_id})-->(entity),\n            (n:Product)-->(entity)\n            WHERE p.id <> n.id\n            WITH n, COUNT(DISTINCT entity) AS commonEntities\n            WHERE commonEntities >= $threshold\n            RETURN n;\n        '''\n    result_common_entities = graph.query(query_common_entities, params={\"product_id\": int(product_id), \"threshold\": relationships_threshold})\n    #print(f\"{len(result_common_entities)} items with at least {relationships_threshold} things in common were found.\")\n\n    for i in result_category:\n        similar_items.append({\n            \"id\": i['n']['id'],\n            \"name\": i['n']['name']\n        })\n            \n    for i in result_common_entities:\n        result_id = i['n']['id']\n        if not any(item['id'] == result_id for item in similar_items):\n            similar_items.append({\n                \"id\": result_id,\n                \"name\": i['n']['name']\n            })\n    return similar_items\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Style and Spelling Prompts using Whisper in Python\nDESCRIPTION: Shows how to invoke Whisper transcription with different prompts to influence capitalization and stylistic consistency in the transcript. Each call demonstrates the effect of short prompts, long prompts, and rare/atypical style prompts. All code expects valid input files and relies on the 'transcribe' function definition.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# short prompts are less reliable\ntranscribe(up_first_filepath, prompt=\"president biden.\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# long prompts are more reliable\ntranscribe(up_first_filepath, prompt=\"i have some advice for you. multiple sentences help establish a pattern. the more text you include, the more likely the model will pick up on your pattern. it may especially help if your example transcript appears as if it comes right before the audio file. in this case, that could mean mentioning the contacts i stick in my eyes.\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# rare styles are less reliable\ntranscribe(up_first_filepath, prompt=\"\"\"Hi there and welcome to the show.\\n###\\nToday we are quite excited.\\n###\\nLet's jump right in.\\n###\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Image\nDESCRIPTION: This snippet saves a generated image from the OpenAI API response to the file system. It extracts the image URL from the response, downloads the image content using the requests library, and then writes the image data to a file in the specified directory.  The file name is set to generated_image.png.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# save the image\ngenerated_image_name = \"generated_image.png\"  # any name you like; the filetype should be .png\ngenerated_image_filepath = os.path.join(image_dir, generated_image_name)\ngenerated_image_url = generation_response.data[0].url  # extract image URL from response\ngenerated_image = requests.get(generated_image_url).content  # download the image\n\nwith open(generated_image_filepath, \"wb\") as image_file:\n    image_file.write(generated_image)  # write the image to the file\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Embeddings Using NumPy and Pandas in Python\nDESCRIPTION: This snippet loads precomputed embeddings from a CSV file into a Pandas DataFrame and converts them from string representations to NumPy arrays. This step flattens embedding dimensions into 1-D NumPy arrays, enabling efficient vector operations such as distance computations required for semantic search. Dependencies include Pandas, NumPy, and OpenAI's embeddings utilities. The DataFrame stores embeddings alongside original text and metadata.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom openai.embeddings_utils import distances_from_embeddings\n\ndf=pd.read_csv('processed/embeddings.csv', index_col=0)\ndf['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Starting Redis Stack Container\nDESCRIPTION: Starts the Redis Stack Docker container in detached mode. Requires Docker to be installed and running.  Provides a local Redis instance with Redis Search and RedisJSON modules available.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n! docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Custom GPT Instructions for Jira Issue Management in Python\nDESCRIPTION: This Python snippet contains instructions for a specialized GPT designed to create, read, and edit Jira issues via the api.atlassian.com API. It guides the GPT to synthesize issue summaries and descriptions based on user input and handle subtasks and assignments using Jira Query Language (JQL). Dependencies include appropriate user permissions and contextual Jira project and issue keys. The GPT asks for clarifications when context is missing to ensure accurate operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_jira.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n**Context**: you are specialized GPT designed to create and edit issues through API connections to Jira Cloud. This GPT can create, read, and edit project issues based on user instructions.\n\n**Instructions**:\n- When asked to perform a task, use the available actions via the api.atlassian.com API.\n- When asked to create an issue, use the user's input to synthesize a summary and description and file the issue in JIRA.\n- When asked to create a subtask, assume the project key and parent issue key of the currently discussed issue. Clarify with if this context is not available.\n- When asked to assign an issue or task to the user, first use jql to query the current user's profile and use this account as the assignee. \n- Ask for clarification when needed to ensure accuracy and completeness in fulfilling user requests.\n```\n\n----------------------------------------\n\nTITLE: Managing Weaviate Schema Definition in Python\nDESCRIPTION: This snippet demonstrates clearing the existing Weaviate schema, defining a new schema for \"Article\" objects with specific properties (`title`, `content`, `url`), configuring the `text2vec-openai` vectorizer and `qna-openai` module, and creating the class within Weaviate. It uses the Weaviate Python client and assumes a `client` object is already initialized and connected. The `text2vec-openai` module is configured to use the \"ada\" model (version \"002\") for embedding `title` and `content`, while skipping embedding for the `url` property. The `qna-openai` module is set up for question answering using \"gpt-3.5-turbo-instruct\".\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n\n# Define the Schema object to use `text-embedding-3-small` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }, \n        \"qna-openai\": {\n          \"model\": \"gpt-3.5-turbo-instruct\",\n          \"maxTokens\": 16,\n          \"temperature\": 0.0,\n          \"topP\": 1,\n          \"frequencyPenalty\": 0.0,\n          \"presencePenalty\": 0.0\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"]\n    },\n    {\n        \"name\": \"url\",\n        \"description\": \"URL to the article\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n\n# add the Article schema\nclient.schema.create_class(article_schema)\n\n# get the schema to make sure it worked\nclient.schema.get()\n```\n\n----------------------------------------\n\nTITLE: Calculating Proportion of Results Retrieved Within Token Limit in Python\nDESCRIPTION: This code calculates the proportion of relevant contexts retrieved by ADA search within a 2000-token budget, then prints a formatted percentage. It operates on the out_expanded DataFrame's 'tokens' column, where a lower token value implies earlier retrieval. Requires pandas and the statistical output from a prior workflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwithin_2k = (out_expanded.tokens < 2000).mean()\nprint(f\"{within_2k*100:.1f}% of relevant paragraphs are retrieved within the first 2k tokens\")\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Node.js Library using npm or Yarn\nDESCRIPTION: Demonstrates how to install the official OpenAI Node.js library using either the npm or yarn package manager. Requires Node.js and the respective package manager to be installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/node-setup.txt#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install --save openai\n```\n\nLANGUAGE: Shell\nCODE:\n```\nyarn add openai\n```\n\n----------------------------------------\n\nTITLE: Defining GPT-4 Evaluation Prompts\nDESCRIPTION: This code snippet defines the template and specific criteria for evaluating text summaries using GPT-4 within the G-Eval framework. It defines a general evaluation prompt template, and then specifies detailed criteria, steps, and examples for evaluating relevance, coherence, and consistency. Each metric is assigned a dedicated prompt with comprehensive evaluation guidelines to instruct the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Evaluation prompt template based on G-Eval\nEVALUATION_PROMPT_TEMPLATE = \"\"\"\nYou will be given one summary written for an article. Your task is to rate the summary on one metric.\nPlease make sure you read and understand these instructions very carefully. \nPlease keep this document open while reviewing, and refer to it as needed.\n\nEvaluation Criteria:\n\n{criteria}\n\nEvaluation Steps:\n\n{steps}\n\nExample:\n\nSource Text:\n\n{document}\n\nSummary:\n\n{summary}\n\nEvaluation Form (scores ONLY):\n\n- {metric_name}\n\"\"\"\n\n# Metric 1: Relevance\n\nRELEVANCY_SCORE_CRITERIA = \"\"\"\nRelevance(1-5) - selection of important content from the source. \\The summary should include only important information from the source document. \\Annotators were instructed to penalize summaries which contained redundancies and excess information.\n\"\"\"\n\nRELEVANCY_SCORE_STEPS = \"\"\"\n1. Read the summary and the source document carefully.\n2. Compare the summary to the source document and identify the main points of the article.\n3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n4. Assign a relevance score from 1 to 5.\n\"\"\"\n\n# Metric 2: Coherence\n\nCOHERENCE_SCORE_CRITERIA = \"\"\"\nCoherence(1-5) - the collective quality of all sentences. \\We align this dimension with the DUC quality question of structure and coherence \\whereby \\\"the summary should be well-structured and well-organized. \\The summary should not just be a heap of related information, but should build from sentence to a\\coherent body of information about a topic.\\\"\n\"\"\"\n\nCOHERENCE_SCORE_STEPS = \"\"\"\n1. Read the article carefully and identify the main topic and key points.\n2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\nand if it presents them in a clear and logical order.\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n\"\"\"\n\n# Metric 3: Consistency\n\nCONSISTENCY_SCORE_CRITERIA = \"\"\"\nConsistency(1-5) - the factual alignment between the summary and the summarized source. \\A factually consistent summary contains only statements that are entailed by the source document. \\Annotators were also asked to penalize summaries that contained hallucinated facts.\n\"\"\"\n\nCONSISTENCY_SCORE_STEPS = \"\"\"\n1. Read the article carefully and identify the main facts and details it presents.\n2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n3. Assign a score for consistency based on the Evaluation Criteria.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining an LLM Classifier Prompt and Instantiating with Autoevals in Python\nDESCRIPTION: Defines a multi-line string `PROMPT` containing instructions for an LLM to compare submitted and expert answers based on factual content and choose one of five options (A-E). It then instantiates an `autoevals.LLMClassifier` named 'Hallucination detector' using this prompt template, the predefined `choice_scores` dictionary, and enabling chain-of-thought reasoning (`use_cot=True`). This demonstrates a higher-level abstraction for creating LLM-based classifiers compared to manual API calls. Requires the `autoevals` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nPROMPT = \"\"\"\\\nYou are comparing a submitted answer to an expert answer on a given question. Here is the data:\n[BEGIN DATA]\n************\n[Question]: {{input}}\n************\n[Expert]: {{expected}}\n************\n[Submission]: {{output}}\n************\n[END DATA]\n\nCompare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\nThe submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n(C) The submitted answer contains all the same details as the expert answer.\n(D) There is a disagreement between the submitted answer and the expert answer.\n(E) The answers differ, but these differences don't matter from the perspective of factuality.\n\nAnswer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\nsure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\nsingle choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n\"\"\"\n\nClassifier = autoevals.LLMClassifier(\n    name=\"Hallucination detector\",\n    prompt_template=PROMPT,\n    choice_scores={\"A\": 0.5, \"B\": 0, \"C\": 1, \"D\": 0, \"E\": 1},\n    use_cot=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Test Sets\nDESCRIPTION: This code splits the preprocessed data into training and test sets using the `train_test_split` function from scikit-learn. The `test_fraction` determines the proportion of data used for testing, and a `random_seed` is used for reproducibility. The split data is then labeled with a 'dataset' column indicating whether it belongs to the 'train' or 'test' set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# split data into train and test sets\ntest_fraction = 0.5  # 0.5 is fairly arbitrary\nrandom_seed = 123  # random seed is arbitrary, but is helpful in reproducibility\ntrain_df, test_df = train_test_split(\n    df, test_size=test_fraction, stratify=df[\"label\"], random_state=random_seed\n)\ntrain_df.loc[:, \"dataset\"] = \"train\"\ntest_df.loc[:, \"dataset\"] = \"test\"\n```\n\n----------------------------------------\n\nTITLE: Creating Query Engine\nDESCRIPTION: Creates a query engine from the created vector index.  This query engine will be used to run the queries against the index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = vector_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Example JSON Evaluation for Assistant Message (Accurate)\nDESCRIPTION: This JSON object demonstrates the expected output format for evaluating an assistant's message. Each sentence is assessed for factual accuracy, relevance, policy compliance, and contextual coherence, referencing the knowledge base. In this example, the assistant's message is deemed accurate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\nfs_assistant_1 = \"\"\"[\n    {\n        \"sentence\": \"I see, because the shirt was ordered in the last 30 days, we can provide you with a full refund.\",\n        \"factualAccuracy\": 1,\n        \"factualReference\": \"If the order was made within 30 days, notify them that they are eligible for a full refund\",\n        \"relevance\": 1,\n        \"policyCompliance\": 1,\n        \"contextualCoherence\": 1\n    },\n    {\n        \"sentence\": \"Would you like me to process the refund?\",\n        \"factualAccuracy\": 1,\n        \"factualReference\": \"If the order was made within 30 days, notify them that they are eligible for a full refund\",\n        \"relevance\": 1,\n        \"policyCompliance\": 1,\n        \"contextualCoherence\": 1\n    }\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Movie Category Classification Using GPT-4 Mini Model\nDESCRIPTION: Defines a system prompt in JSON mode to specify task instructions for extracting movie categories and summaries from descriptions, to be used with the chat.completions.create method.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncategorize_system_prompt = '''\nYour goal is to extract movie categories from movie descriptions, as well as a 1-sentence summary for these movies.\nYou will be provided with a movie description, and you will output a json object containing the following information:\n\n{\n    categories: string[] // Array of categories based on the movie description,\n    summary: string // 1-sentence summary of the movie based on the movie description\n}\n\nCategories refer to the genre or type of the movie, like \"action\", \"romance\", \"comedy\", etc. Keep category names simple and use only lower case letters.\nMovies can have several categories, but try to keep it under 3-4. Only mention the categories that are the most obvious based on the description.\n'''\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Astra DB Client with AstraPy in Python\nDESCRIPTION: Initializes the AstraDB client using the provided API endpoint and token. This object is used throughout the notebook for database operations such as creating collections and data insertion. Requires 'astrapy' package and valid credentials assigned to ASTRA_DB_API_ENDPOINT and ASTRA_DB_APPLICATION_TOKEN.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nastra_db = AstraDB(\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\n    token=ASTRA_DB_APPLICATION_TOKEN,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embedding Client in Python\nDESCRIPTION: Sets up the OpenAI client with API key, defines model parameters, and creates a function to obtain embeddings with retry logic. The function retrieves embeddings from the specified model, handling potential request errors using exponential backoff.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nimport openai\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\nEMBEDDING_MODEL = 'text-embedding-3-small'\nEMBEDDING_CTX_LENGTH = 8191\nEMBEDDING_ENCODING = 'cl100k_base'\n\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.BadRequestError))\ndef get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n    return client.embeddings.create(input=text_or_tokens, model=model).data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Defining Function Tools for Retrieving Phone Logs in Python with OpenAI Agents SDK\nDESCRIPTION: Defines a function tool to simulate fetching phone call logs for a specified phone number, returning a filtered list of call records including timestamps, durations, notes, and associated order IDs. This utility supports investigation workflows by providing communication context related to disputes. It is decorated with the function_tool decorator for integration with the OpenAI Agents SDK.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\ndef get_phone_logs(phone_number: str) -> list:\n    \"\"\"\n    Return a list of phone call records for the given phone number.\n    Each record might include call timestamps, durations, notes, \n    and an associated order_id if applicable.\n    \"\"\"\n    phone_logs = [\n        {\n            \"phone_number\": \"+15551234567\",\n            \"timestamp\": \"2023-03-14 15:24:00\",\n            \"duration_minutes\": 5,\n            \"notes\": \"Asked about status of order #1121\",\n            \"order_id\": 1121\n        },\n        {\n            \"phone_number\": \"+15551234567\",\n            \"timestamp\": \"2023-02-28 10:10:00\",\n            \"duration_minutes\": 7,\n            \"notes\": \"Requested refund for order #1121, I told him we were unable to refund the order because it was final sale\",\n            \"order_id\": 1121\n        },\n        {\n            \"phone_number\": \"+15559876543\",\n            \"timestamp\": \"2023-01-05 09:00:00\",\n            \"duration_minutes\": 2,\n            \"notes\": \"General inquiry; no specific order mentioned\",\n            \"order_id\": None\n        },\n    ]\n    return [\n        log for log in phone_logs if log[\"phone_number\"] == phone_number\n    ]\n```\n\n----------------------------------------\n\nTITLE: Listing and Sorting Segmented Audio Files (Python)\nDESCRIPTION: Generates a list of filenames ending with `.wav` located in the `output_dir_trimmed` directory using `os.listdir`. The `sorted` function is used with a custom key (`lambda f: int(''.join(filter(str.isdigit, f)))`) to sort the files numerically based on the digits extracted from their names, ensuring they are processed in the correct chronological order.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Get list of trimmed and segmented audio files and sort them numerically\naudio_files = sorted(\n    (f for f in os.listdir(output_dir_trimmed) if f.endswith(\".wav\")),\n    key=lambda f: int(''.join(filter(str.isdigit, f)))\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Head (Pandas)\nDESCRIPTION: This line of code uses the `head()` method of a Pandas DataFrame to display the first 10 rows. It helps in quickly inspecting the structure and content of the DataFrame, ensuring that data has been loaded and formatted correctly. It relies on the `pandas` library being properly installed and imported.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndf.head(10)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with OpenAI Node.js Library\nDESCRIPTION: Example Node.js script illustrating how to generate an image based on a text prompt using the OpenAI library. It initializes the client, sends a request to the images endpoint with a prompt ('A cute baby sea otter'), and logs the image data from the response. Requires the OpenAI library installed and the API key configured via environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/node-setup.txt#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const image = await openai.images.generate({ prompt: \"A cute baby sea otter\" });\n\n  console.log(image.data);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Example Usage: Ingest and Query - Python\nDESCRIPTION: This snippet demonstrates how to use the previously defined functions to ingest data and query the SQLite database. It sets the file paths for the transformed JSON data and the SQLite database. It then calls `ingest_transformed_jsons` to load the data. After loading, an SQL query is constructed to retrieve the hotel name and the maximum amount spent.  The query results are then printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Example usage\ntransformed_invoices_path = \"./data/hotel_invoices/transformed_invoice_json\"\ndb_path = \"./data/hotel_invoices/hotel_DB.db\"\ningest_transformed_jsons(transformed_invoices_path, db_path)\n\nquery = '''\n    SELECT \n        h.name AS hotel_name,\n        i.total_gross AS max_spent\n    FROM \n        Invoices i\n    JOIN \n        Hotels h ON i.hotel_id = h.hotel_id\n    ORDER BY \n        i.total_gross DESC\n    LIMIT 1;\n    '''\n\nresults = execute_query(db_path, query)\nfor row in results:\n    print(row)\n```\n\n----------------------------------------\n\nTITLE: Parsing Structured Math Reasoning Response with Pydantic in Python\nDESCRIPTION: Calls the Pydantic-based 'get_math_solution' function and extracts the parsed Pydantic model instance from the API response, enabling easy access to individual fields and validation of the data structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresult = get_math_solution(question).parsed\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Scores for Hallucination Detection in Python\nDESCRIPTION: Defines a Python dictionary `CHOICE_SCORES` mapping letter grades (A-E) representing different comparison outcomes between a submitted answer and an expert answer to numerical scores. Choices B (superset) and D (disagreement), potentially indicating hallucinations, are penalized with a score of 0.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Since we're testing for hallucinations, penalize (B) as much as (D).\nCHOICE_SCORES = {\n    \"A\": 0.5,\n    \"B\": 0,\n    \"C\": 1,\n    \"D\": 0,\n    \"E\": 1,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Full Agent Turn with Tool Execution Loop in Python\nDESCRIPTION: Presents the final, integrated implementation for handling a full conversation turn with potential tool usage. It defines `run_full_turn` which now handles tool calls within an inner `while` loop. The function prepares tools/schemas, calls the API, checks for `tool_calls` in the response, executes them using `execute_tool_call`, appends the results back to the messages list, and repeats the API call (passing the tool results) until the model responds with content instead of another tool call. The main `while` loop orchestrates the conversation by getting user input and calling `run_full_turn`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntools = [execute_refund, look_up_item]\n\n\ndef run_full_turn(system_message, tools, messages):\n\n    num_init_messages = len(messages)\n    messages = messages.copy()\n\n    while True:\n\n        # turn python functions into tools and save a reverse map\n        tool_schemas = [function_to_schema(tool) for tool in tools]\n        tools_map = {tool.__name__: tool for tool in tools}\n\n        # === 1. get openai completion ===\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"system\", \"content\": system_message}] + messages,\n            tools=tool_schemas or None,\n        )\n        message = response.choices[0].message\n        messages.append(message)\n\n        if message.content:  # print assistant response\n            print(\"Assistant:\", message.content)\n\n        if not message.tool_calls:  # if finished handling tool calls, break\n            break\n\n        # === 2. handle tool calls ===\n\n        for tool_call in message.tool_calls:\n            result = execute_tool_call(tool_call, tools_map)\n\n            result_message = {\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call.id,\n                \"content\": result,\n            }\n            messages.append(result_message)\n\n    # ==== 3. return new messages =====\n    return messages[num_init_messages:]\n\n\ndef execute_tool_call(tool_call, tools_map):\n    name = tool_call.function.name\n    args = json.loads(tool_call.function.arguments)\n\n    print(f\"Assistant: {name}({args})\")\n\n    # call corresponding function with provided arguments\n    return tools_map[name](**args)\n\n\nmessages = []\nwhile True:\n    user = input(\"User: \")\n    messages.append({\"role\": \"user\", \"content\": user})\n\n    new_messages = run_full_turn(system_message, tools, messages)\n    messages.extend(new_messages)\n```\n\n----------------------------------------\n\nTITLE: GPT Store Launch Reference in Markdown\nDESCRIPTION: Code snippet announcing the launch of the GPT Store, which went public on January 10th, 2024 with categories and leaderboards.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/release-notes.txt#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nThe{\" \"}\n    GPT Store\n{\" \"}\nlaunched publicly, with categories and various leaderboards\n```\n\n----------------------------------------\n\nTITLE: Loading Financial Data with Pandas (Python)\nDESCRIPTION: Loads financial data from a specified JSON file path into a pandas DataFrame. This DataFrame is then displayed using `.head()` to show the first 5 rows, allowing inspection of the data structure and content before further processing by the Assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfinancial_data_path = 'data/NotRealCorp_financial_data.json'\nfinancial_data = pd.read_json(financial_data_path)\nfinancial_data.head(5)\n\n```\n\n----------------------------------------\n\nTITLE: Transforming Responses API Output to Completions Format in Python\nDESCRIPTION: This Python snippet demonstrates the complete workflow for processing push notification samples using the OpenAI responses API, then transforming each response to match the completions API format, and finally submitting the results for evaluation with OpenAI Evals. Dependencies include the openai Python package and properly defined variables such as DEVELOPER_PROMPT, push_notification_data, eval_id, and the PushNotifications model. Key parameters are push_notifications (input for summarization), response (API result), and run_data (list of evaluation samples). Output includes a link to the evaluation report. Limitations: the snippet assumes the evals.runs and responses.create APIs are accessible and used as shown.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef summarize_push_notification_responses(push_notifications: str):\n    result = openai.responses.create(\n                model=\"gpt-4o\",\n                input=[\n                    {\"role\": \"developer\", \"content\": DEVELOPER_PROMPT},\n                    {\"role\": \"user\", \"content\": push_notifications},\n                ],\n            )\n    return result\ndef transform_response_to_completion(response):\n    completion = {\n        \"model\": response.model,\n        \"choices\": [{\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"content\": response.output_text\n        },\n        \"finish_reason\": \"stop\",\n    }]\n    }\n    return completion\n\nrun_data = []\nfor push_notifications in push_notification_data:\n    response = summarize_push_notification_responses(push_notifications)\n    completion = transform_response_to_completion(response)\n    run_data.append({\n        \"item\": PushNotifications(notifications=push_notifications).model_dump(),\n        \"sample\": completion\n    })\n\nreport_response = openai.evals.runs.create(\n    eval_id=eval_id,\n    name=\"responses-run\",\n    data_source={\n        \"type\": \"jsonl\",\n        \"source\": {\n            \"type\": \"file_content\",\n            \"content\": run_data,\n        }\n    },\n)\nprint(report_response.report_url)\n\n```\n\n----------------------------------------\n\nTITLE: Processing Dynamic Queries with OpenAI Responses API in Python\nDESCRIPTION: This Python snippet iterates through a list of queries, dynamically builds message payloads, and calls the OpenAI Responses API with parallel tool calls enabled. Depending on the model's output, it can trigger specific tools (such as PineconeSearchDocuments or a web search), process their results, and append both tool calls and their outputs back into the conversation context. Required dependencies include the OpenAI Python SDK and client initialization, and potentially external tools such as Pinecone. Inputs include a list of queries and access to the Responses API. The output is a stepwise final answer, integrating tool results or defaulting to a direct response when appropriate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfor item in queries:\n    input_messages = [{\"role\": \"user\", \"content\": item[\"query\"]}]\n    print(\"\\nðŸŒŸ--- Processing Query ---ðŸŒŸ\")\n    print(f\"ðŸ” **User Query:** {item['query']}\")\n    \n    # Call the Responses API with tools enabled and allow parallel tool calls.\n    response = client.responses.create(\n        model=\"gpt-4o\",\n        input=[\n            {\"role\": \"system\", \"content\": \"When prompted with a question, select the right tool to use based on the question.\"\n            },\n            {\"role\": \"user\", \"content\": item[\"query\"]}\n        ],\n        tools=tools,\n        parallel_tool_calls=True\n    )\n    \n    print(\"\\nâœ¨ **Initial Response Output:**\")\n    print(response.output)\n    \n    # Determine if a tool call is needed and process accordingly.\n    if response.output:\n        tool_call = response.output[0]\n        if tool_call.type in [\"web_search_preview\", \"function_call\"]:\n            tool_name = tool_call.name if tool_call.type == \"function_call\" else \"web_search_preview\"\n            print(f\"\\nðŸ”§ **Model triggered a tool call:** {tool_name}\")\n            \n            if tool_name == \"PineconeSearchDocuments\":\n                print(\"ðŸ” **Invoking PineconeSearchDocuments tool...**\")\n                res = query_pinecone_index(client, index, MODEL, item[\"query\"])\n                if res[\"matches\"]:\n                    best_match = res[\"matches\"][0][\"metadata\"]\n                    result = f\"**Question:** {best_match.get('Question', 'N/A')}\\n**Answer:** {best_match.get('Answer', 'N/A')}\"\n                else:\n                    result = \"**No matching documents found in the index.**\"\n                print(\"âœ… **PineconeSearchDocuments tool invoked successfully.**\")\n            else:\n                print(\"ðŸ” **Invoking simulated web search tool...**\")\n                result = \"**Simulated web search result.**\"\n                print(\"âœ… **Simulated web search tool invoked successfully.**\")\n            \n            # Append the tool call and its output back into the conversation.\n            input_messages.append(tool_call)\n            input_messages.append({\n                \"type\": \"function_call_output\",\n                \"call_id\": tool_call.call_id,\n                \"output\": str(result)\n            })\n            \n            # Get the final answer incorporating the tool's result.\n            final_response = client.responses.create(\n                model=\"gpt-4o\",\n                input=input_messages,\n                tools=tools,\n                parallel_tool_calls=True\n            )\n            print(\"\\nðŸ’¡ **Final Answer:**\")\n            print(final_response.output_text)\n        else:\n            # If no tool call is triggered, print the response directly.\n            print(\"ðŸ’¡ **Final Answer:**\")\n            print(response.output_text)\n\n```\n\n----------------------------------------\n\nTITLE: Importing libraries for Azure AI Search and OpenAI integration in Python\nDESCRIPTION: Imports standard Python libraries (json, wget, pandas, zipfile) and Azure/OpenAI SDK modules required for authentication, vector search, semantic search, and indexing operations. These imports provide the foundational classes and functions to configure clients, define search indices with vector capabilities, and manage documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json  \nimport wget\nimport pandas as pd\nimport zipfile\nfrom openai import AzureOpenAI\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom azure.core.credentials import AzureKeyCredential  \nfrom azure.search.documents import SearchClient, SearchIndexingBufferedSender  \nfrom azure.search.documents.indexes import SearchIndexClient  \nfrom azure.search.documents.models import (\n    QueryAnswerType,\n    QueryCaptionType,\n    QueryType,\n    VectorizedQuery,\n)\nfrom azure.search.documents.indexes.models import (\n    HnswAlgorithmConfiguration,\n    HnswParameters,\n    SearchField,\n    SearchableField,\n    SearchFieldDataType,\n    SearchIndex,\n    SemanticConfiguration,\n    SemanticField,\n    SemanticPrioritizedFields,\n    SemanticSearch,\n    SimpleField,\n    VectorSearch,\n    VectorSearchAlgorithmKind,\n    VectorSearchAlgorithmMetric,\n    VectorSearchProfile,\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Text with OpenAI GPT-4o-mini in JavaScript\nDESCRIPTION: This asynchronous function analyzes provided text to find relevant information based on a user query using the OpenAI API. It initializes the OpenAI client using an API key from environment variables, sends the text and query to the `gpt-4o-mini` model via the chat completions endpoint with specific instructions to return a maximum of 10 relevant sentences, and returns the model's response content. Requires the `openai` Node.js library and the `OPENAI_API_KEY` environment variable to be set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst getRelevantParts = async (text, query) => {\n    try {\n        // We use your OpenAI key to initialize the OpenAI client\n        const openAIKey = process.env[\"OPENAI_API_KEY\"];\n        const openai = new OpenAI({\n            apiKey: openAIKey,\n        });\n        const response = await openai.chat.completions.create({\n            // Using gpt-4o-mini due to speed to prevent timeouts. You can tweak this prompt as needed\n            model: \"gpt-4o-mini\",\n            messages: [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant that finds relevant content in text based on a query. You only return the relevant sentences, and you return a maximum of 10 sentences\"},\n                {\"role\": \"user\", \"content\": `Based on this question: **\"${query}\"**, get the relevant parts from the following text:*****\\n\\n${text}*****. If you cannot answer the question based on the text, respond with 'No information provided'`}\n            ],\n            // using temperature of 0 since we want to just extract the relevant content\n            temperature: 0,\n            // using max_tokens of 1000, but you can customize this based on the number of documents you are searching. \n            max_tokens: 1000\n        });\n        return response.choices[0].message.content;\n    } catch (error) {\n        console.error('Error with OpenAI:', error);\n        return 'Error processing text with OpenAI' + error;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Running Example Customer Service Interaction in Python\nDESCRIPTION: Demonstrates initiating and continuing a conversation with the customer service agent for a fraud scenario. The `submit_user_message` function is called sequentially with different user inputs, passing the updated `messages` list each time to maintain conversation context. This simulates a multi-turn interaction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmessages = submit_user_message(\"Hi, I have had an item stolen that was supposed to be delivered to me yesterday.\")\n```\n\nLANGUAGE: python\nCODE:\n```\nmessages = submit_user_message(\"For sure, it was a shirt, it was supposed to be delivered yesterday but it never arrived.\",messages)\n```\n\nLANGUAGE: python\nCODE:\n```\nmessages = submit_user_message(\"Yes I would like to proceed with the refund.\",messages)\n```\n\nLANGUAGE: python\nCODE:\n```\nmessages = submit_user_message(\"Thanks very much.\",messages)\n```\n\n----------------------------------------\n\nTITLE: Allowing GPT Model to Decide Function Usage Automatically in Chat Completion in Python\nDESCRIPTION: Illustrates sending a user request without forcing a function call by omitting the `tool_choice` parameter. The model can decide which function to use or whether to not use one, producing a natural response or generating function arguments as appropriate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# if we don't force the model to use get_n_day_weather_forecast it may not\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\nmessages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\nchat_response = chat_completion_request(\n    messages, tools=tools\n)\nchat_response.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: Printing Loaded Document Page Counts in Python\nDESCRIPTION: Outputs the number of pages loaded from the Lyft and Uber 10-K PDF documents by printing the length of the Document object lists. This provides confirmation the documents were successfully parsed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(f'Loaded lyft 10-K with {len(lyft_docs)} pages')\nprint(f'Loaded Uber 10-K with {len(uber_docs)} pages')\n```\n\n----------------------------------------\n\nTITLE: Import and Setup Environment for Transaction Clustering in Python\nDESCRIPTION: This snippet imports necessary libraries, loads environment variables, and initializes the OpenAI client with an API key. It prepares the environment for data processing, clustering, and GPT-3 API usage, setting up file paths for data access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# optional env import\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# imports\n \nfrom openai import OpenAI\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os\nfrom ast import literal_eval\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\nCOMPLETIONS_MODEL = \"gpt-3.5-turbo\"\n\n# This path leads to a file with data and precomputed embeddings\nembedding_path = \"data/library_transactions_with_embeddings_359.csv\"\n```\n\n----------------------------------------\n\nTITLE: Generate an image variation with DALLÂ·E 2\nDESCRIPTION: Code for creating variations of an existing image without requiring a text prompt. Only available with DALLÂ·E 2 model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.create_variation(\n  model=\"dall-e-2\",\n  image=open(\"corgi_and_cat_paw.png\", \"rb\"),\n  n=1,\n  size=\"1024x1024\"\n)\n\nimage_url = response.data[0].url\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst response = await openai.images.createVariation({\n  model: \"dall-e-2\",\n  image: fs.createReadStream(\"corgi_and_cat_paw.png\"),\n  n: 1,\n  size: \"1024x1024\"\n});\nimage_url = response.data[0].url;\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/images/variations \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -F model=\"dall-e-2\" \\\n  -F image=\"@corgi_and_cat_paw.png\" \\\n  -F n=1 \\\n  -F size=\"1024x1024\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Connection\nDESCRIPTION: This snippet initializes the connection to Pinecone using the API key stored in the environment variable 'PINECONE_API_KEY'. It retrieves the API key from the environment and passes it to the `pinecone.init()` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\napi_key = os.getenv(\"PINECONE_API_KEY\")\npinecone.init(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Running the Multi-Agent System Loop (Python)\nDESCRIPTION: This snippet provides a simple main loop to simulate a conversation with the multi-agent system. It takes user input, calls the `run_full_turn` function, updates the current agent based on the response object, and continues the conversation with the possibly new agent and updated message history.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nagent = triage_agent\nmessages = []\n\nwhile True:\n    user = input(\"User: \")\n    messages.append({\"role\": \"user\", \"content\": user})\n\n    response = run_full_turn(agent, messages)\n    agent = response.agent\n    messages.extend(response.messages)\n```\n\n----------------------------------------\n\nTITLE: Forcing GPT Model to Use Specific Function in Chat Completion API with Python\nDESCRIPTION: Demonstrates how to force the chat model to invoke a specific function, `get_n_day_weather_forecast`, by providing the `tool_choice` parameter. This overrides the model's default function selection behavior and compels it to generate arguments targeting the specified function based on user input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# in this cell we force the model to use get_n_day_weather_forecast\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\nmessages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\nchat_response = chat_completion_request(\n    messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_n_day_weather_forecast\"}}\n)\nchat_response.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: Generating Image Variations\nDESCRIPTION: This snippet calls the OpenAI API to generate variations of an existing image. It uses the `client.images.create_variation` method.  It takes the previously generated image as input. It sets `n` to 2 to create two variations, and the size to 1024x1024, and the format as a URL. The code then prints the response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# create variations\n\n# call the OpenAI API, using `create_variation` rather than `create`\nvariation_response = client.images.create_variation(\n    image=generated_image,  # generated_image is the image generated above\n    n=2,\n    size=\"1024x1024\",\n    response_format=\"url\",\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing OpenAI Client in Python\nDESCRIPTION: Imports necessary standard Python libraries (`os`, `json`, `pprint`), external libraries (`jsonref`, `openai`, `requests`), and initializes the `OpenAI` client. The client is configured using an API key retrieved from the `OPENAI_API_KEY` environment variable or a placeholder string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport jsonref\nfrom openai import OpenAI\nimport requests\nfrom pprint import pp\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search in BigQuery Without Metadata Filtering using Python\nDESCRIPTION: Executes a pure vector similarity search using BigQuery's VECTOR_SEARCH function via the Python BigQuery client. The vector embedding of a natural language query is generated externally and passed into the SQL query. The query finds closest neighbors in the 'content_vector' column ordered by cosine distance. The snippet retrieves and prints top results with limited text for quick inspection. It requires a function 'generate_embeddings' to produce embeddings matching the stored vector schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What model should I use to embed?\"\ncategory = \"models\"\n\nembedding_query = generate_embeddings(query, embeddings_model)\nembedding_query_list = ', '.join(map(str, embedding_query))\n\nquery = f\"\"\"\nWITH search_results AS (\n  SELECT query.id AS query_id, base.id AS base_id, distance\n  FROM VECTOR_SEARCH(\n    TABLE oai_docs.embedded_data, 'content_vector',\n    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n    top_k => 2, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n)\nSELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title\nFROM search_results sr\nJOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\nORDER BY sr.distance ASC\n\"\"\"\n\nquery_job = client.query(query)\nresults = query_job.result()  # Wait for the job to complete\n\nfor row in results:\n    print(f\"query_id: {row['query_id']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Redis Search Index\nDESCRIPTION: This snippet creates a Redis search index if it doesn't already exist. It first checks if the index exists and if not, it creates the index with the specified fields and index definition using HASH as the document type.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Check if index exists\ntry:\n    redis_client.ft(INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading documents with embeddings to Azure AI Search index using Python\nDESCRIPTION: Demonstrates uploading a batch of documents from a pandas DataFrame to the Azure AI Search index using SearchIndexingBufferedSender. Ensures 'id' and 'vector_id' columns are strings (with 'vector_id' as the key field), converts DataFrame to dictionary records, uploads documents in buffered batches with error handling, flushes the buffer, and closes resources to complete the operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom azure.core.exceptions import HttpResponseError\n\n# Convert the 'id' and 'vector_id' columns to string so one of them can serve as our key field\narticle_df[\"id\"] = article_df[\"id\"].astype(str)\narticle_df[\"vector_id\"] = article_df[\"vector_id\"].astype(str)\n# Convert the DataFrame to a list of dictionaries\ndocuments = article_df.to_dict(orient=\"records\")\n\n# Create a SearchIndexingBufferedSender\nbatch_client = SearchIndexingBufferedSender(\n    search_service_endpoint, index_name, credential\n)\n\ntry:\n    # Add upload actions for all documents in a single call\n    batch_client.upload_documents(documents=documents)\n\n    # Manually flush to send any remaining documents in the buffer\n    batch_client.flush()\nexcept HttpResponseError as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    # Clean up resources\n    batch_client.close()\n\nprint(f\"Uploaded {len(documents)} documents in total\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Initializing Pinecone Index for Embeddings\nDESCRIPTION: Creates a new Pinecone serverless index if it doesn't exist, configuring it with the proper dimensions and similarity metric for the OpenAI embeddings. Waits for the index to be ready before connecting to it.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom pinecone import ServerlessSpec\n\nspec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n\nindex_name = 'semantic-search-openai'\n\n# check if index already exists (if shouldn't if this is your first run)\nif index_name not in pc.list_indexes().names():\n    # if does not exist, create index\n    pc.create_index(\n        index_name,\n        dimension=len(embeds[0]),  # dimensionality of text-embed-3-small\n        metric='dotproduct',\n        spec=spec\n    )\n    # wait for index to be initialized\n    while not pc.describe_index(index_name).status['ready']:\n        time.sleep(1)\n\n# connect to index\nindex = pc.Index(index_name)\ntime.sleep(1)\n# view index stats\nindex.describe_index_stats()\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Management and Analysis Functions in Python\nDESCRIPTION: Defines core data handling functions including cleaning data, performing statistical analysis, and creating line charts with visualization. These are the underlying implementations that agents will call.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef clean_data(data):\n    data_io = StringIO(data)\n    df = pd.read_csv(data_io, sep=\",\")\n    df_deduplicated = df.drop_duplicates()\n    return df_deduplicated\n\ndef stat_analysis(data):\n    data_io = StringIO(data)\n    df = pd.read_csv(data_io, sep=\",\")\n    return df.describe()\n\ndef plot_line_chart(data):\n    data_io = StringIO(data)\n    df = pd.read_csv(data_io, sep=\",\")\n    \n    x = df.iloc[:, 0]\n    y = df.iloc[:, 1]\n    \n    coefficients = np.polyfit(x, y, 1)\n    polynomial = np.poly1d(coefficients)\n    y_fit = polynomial(x)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'o', label='Data Points')\n    plt.plot(x, y_fit, '-', label='Best Fit Line')\n    plt.title('Line Chart with Best Fit Line')\n    plt.xlabel(df.columns[0])\n    plt.ylabel(df.columns[1])\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenAI API Key Environment Variable in Python\nDESCRIPTION: Checks if the `OPENAI_API_KEY` environment variable is set using Python's `os` module. This key is necessary for interacting with the OpenAI API. It prints a confirmation message if the key is found, otherwise, it prints a warning. A commented-out line shows how to set the environment variable temporarily within the script.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Running Similarity Search on Title Vectors\nDESCRIPTION: This code snippet executes a similarity search using the `query_neon` function with the query \"Greek mythology\" and the `title_vector` column. It iterates through the results and prints the title and similarity score of each matching article.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Query based on `title_vector` embeddings\nimport openai\n\nquery_results = query_neon(\"Greek mythology\", \"Articles\")\nfor i, result in enumerate(query_results):\n    print(f\"{i + 1}. {result[2]} (Score: {round(1 - result[3], 3)})\")\n```\n\n----------------------------------------\n\nTITLE: Executing a Basic Agent Routine Loop (No Tools) in Python\nDESCRIPTION: Defines a function `run_full_turn` that takes a system message and conversation history (`messages`), calls the OpenAI Chat Completions API (`gpt-4o-mini`), appends the model's response to the history, and prints the content. It also includes a simple `while` loop to continuously get user input, append it to messages, and run the turn. This initial version does not handle function/tool calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef run_full_turn(system_message, messages):\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"system\", \"content\": system_message}] + messages,\n    )\n    message = response.choices[0].message\n    messages.append(message)\n\n    if message.content: print(\"Assistant:\", message.content)\n\n    return message\n\n\nmessages = []\nwhile True:\n    user = input(\"User: \")\n    messages.append({\"role\": \"user\", \"content\": user})\n\n    run_full_turn(system_message, messages)\n```\n\n----------------------------------------\n\nTITLE: Removing Newlines Python\nDESCRIPTION: This function removes newline characters and double spaces from a Pandas Series. It replaces '\\n' and newline characters with spaces, and replaces consecutive spaces with a single space. This is used for cleaning text data prior to further processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef remove_newlines(serie):\n    serie = serie.str.replace('\\n', ' ')\n    serie = serie.str.replace('\\\\n', ' ')\n    serie = serie.str.replace('  ', ' ')\n    serie = serie.str.replace('  ', ' ')\n    return serie\n```\n```\n\n----------------------------------------\n\nTITLE: Defining token counting utility functions with tiktoken\nDESCRIPTION: Defines functions to estimate number of tokens in message sets and individual assistant messages, based on token encoding. These utilities assist in evaluating dataset size and conversation complexity crucial for cost and resource planning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n\n# not exact!\n# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\ndef num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3\n    return num_tokens\n\ndef num_assistant_tokens_from_messages(messages):\n    num_tokens = 0\n    for message in messages:\n        if message[\"role\"] == \"assistant\":\n            num_tokens += len(encoding.encode(message[\"content\"]))\n    return num_tokens\n\ndef print_distribution(values, name):\n    print(f\"\\n#### Distribution of {name}:\")\n    print(f\"min / max: {min(values)}, {max(values)}\")\n    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Test Questions for LLM Conversation Simulation in Python\nDESCRIPTION: Creates an array of sample customer service questions to be used as starting points for simulated conversations, focusing on refunds and return policies.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquestions = ['I want to get a refund for the suit I ordered last Friday.',\n            'Can you tell me what your policy is for returning damaged goods?',\n            'Please tell me what your complaint policy is']\n```\n\n----------------------------------------\n\nTITLE: Dubbing English Audio to Hindi with GPT-4o\nDESCRIPTION: Code that performs direct English-to-Hindi audio dubbing using GPT-4o. It configures the output modality for both text and audio responses, and includes a glossary of terms to keep in the original language during translation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nglossary_of_terms_to_keep_in_original_language = \"Turbo, OpenAI, token, GPT, Dall-e, Python\"\n\nmodalities = [\"text\", \"audio\"]\nprompt = f\"The user will provide an audio file in English. Dub the complete audio, word for word in Hindi. Keep certain words in English for which a direct translation in Hindi does not exist such as  ${glossary_of_terms_to_keep_in_original_language}.\"\n\nresponse_json = process_audio_with_gpt_4o(english_audio_base64, modalities, prompt)\n\nmessage = response_json['choices'][0]['message']\n```\n\n----------------------------------------\n\nTITLE: Downloading Data with Curl\nDESCRIPTION: This snippet uses the `curl` command to download a text file from a specified URL and stores it into the data directory.  The `-o` option specifies the output file name within the specified directory. It creates the necessary directory structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!mkdir -p 'data/paul_graham/'\n!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'\n```\n\n----------------------------------------\n\nTITLE: Connecting Listener App via Socket.IO in React/JS\nDESCRIPTION: Defines an asynchronous `connectServer` function using React's `useCallback` hook for the listener application. It establishes a connection to a Socket.IO server running at 'http://localhost:3001', stores the socket instance in `socketRef`, connects a `WavStreamPlayer`, and sets up event listeners for 'connect' and 'disconnect' events to manage connection state.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n  // Function to connect to the server and set up audio streaming\n  const connectServer = useCallback(async () => {\n    if (socketRef.current) return;\n    try {\n      const socket = io('http://localhost:3001');\n      socketRef.current = socket;\n      await wavStreamPlayerRef.current.connect();\n      socket.on('connect', () => {\n        console.log('Listener connected:', socket.id);\n        setIsConnected(true);\n      });\n      socket.on('disconnect', () => {\n        console.log('Listener disconnected');\n        setIsConnected(false);\n      });\n    } catch (error) {\n      console.error('Error connecting to server:', error);\n    }\n  }, []);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: This snippet demonstrates how to install the necessary Python packages for the project, including redis, pandas, and openai, using pip. These libraries are required for interacting with Redis and OpenAI.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n! pip install redis pandas openai\n```\n\n----------------------------------------\n\nTITLE: Creating Negative Example Augmented Summary Prompt in Python\nDESCRIPTION: Extends the prompt with an example of an ineffective or poorly formatted summary, illustrating undesirable output to help refine AI responses during testing. It combines the previous good and bad examples to improve model guidance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nPROMPT_VARIATION_WITH_NEGATIVE_EXAMPLES = f\"\"\"\\n{PROMPT_VARIATION_WITH_EXAMPLES}\\n\\nHere is an example of a bad summary:\\n<push_notifications>\\n- Traffic alert: Accident reported on Main Street.- Package out for delivery: Expected by 5 PM.- New friend suggestion: Connect with Emma.\\n</push_notifications>\\n<summary>\\nTraffic alert reported on main street. You have a package that will arrive by 5pm, Emily is a new friend suggested for you.\\n</summary>\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Describing a Papers Endpoint with openaiFileResponse - YAML\nDESCRIPTION: This YAML code snippet provides an OpenAPI schema for a /papers endpoint which allows users to retrieve up to five academic paper PDFs via a GET request with a required topic query parameter. The response body includes an openaiFileResponse array, where each object has properties for name, mime_type, and base64-encoded content, following OpenAI's file return protocol. This is suitable for exposing literature retrieval in GPT actions. Endpoint input is a string topic; output is an array of processed file objects. Limitations include a maximum of 5 papers and standard OpenAI file return constraints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n/papers:\n  get:\n    operationId: findPapers\n    summary: Retrieve PDFs of relevant academic papers.\n    description: Provided an academic topic, up to five relevant papers will be returned as PDFs.\n    parameters:\n      - in: query\n        name: topic\n        required: true\n        schema:\n          type: string\n        description: The topic the papers should be about.\n    responses:\n      '200':\n        description: Zero to five academic paper PDFs\n        content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                          description: The name of the file.\n                        mime_type:\n                          type: string\n                          description: The MIME type of the file.\n                        content:\n                          type: string\n                          format: byte\n                          description: The content of the file in base64 encoding.\n```\n\n----------------------------------------\n\nTITLE: Generating Questions Based on Context - Python\nDESCRIPTION: This code defines a function `get_questions` that uses the OpenAI API to generate questions based on a given context using the `davinci-instruct-beta-v3` model. It constructs a prompt instructing the model to write questions and sets various parameters like temperature and max_tokens. The generated questions are then added as a new column in the dataframe.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\ndef get_questions(context):\n    try:\n        response = client.chat.completions.create(model=\"davinci-instruct-beta-v3\",\n        prompt=f\"Write questions based on the text below\\n\\nText: {context}\\n\\nQuestions:\\n1.\",\n        temperature=0,\n        max_tokens=257,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=[\"\\n\\n\"])\n        return response.choices[0].text\n    except:\n        return \"\"\n\ndf['questions']= df.context.apply(get_questions)\ndf['questions'] = \"1.\" + df.questions\nprint(df[['questions']].values[0][0])\n```\n\n----------------------------------------\n\nTITLE: Implementing EventHandler for Streaming Tool Outputs in Python\nDESCRIPTION: A Python implementation of an EventHandler class that handles streaming responses from the OpenAI Assistant API, processes tool calls for weather functions, and submits all tool outputs at once using the submit_tool_outputs_stream helper.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling-run-example--streaming.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler\n \nclass EventHandler(AssistantEventHandler):\n    @override\n    def on_event(self, event):\n      # Retrieve events that are denoted with 'requires_action'\n      # since these will have our tool_calls\n      if event.event == 'thread.run.requires_action':\n        run_id = event.data.id  # Retrieve the run ID from the event data\n        self.handle_requires_action(event.data, run_id)\n \n    def handle_requires_action(self, data, run_id):\n      tool_outputs = []\n        \n      for tool in data.required_action.submit_tool_outputs.tool_calls:\n        if tool.function.name == \"get_current_temperature\":\n          tool_outputs.append({\"tool_call_id\": tool.id, \"output\": \"57\"})\n        elif tool.function.name == \"get_rain_probability\":\n          tool_outputs.append({\"tool_call_id\": tool.id, \"output\": \"0.06\"})\n        \n      # Submit all tool_outputs at the same time\n      self.submit_tool_outputs(tool_outputs, run_id)\n \n    def submit_tool_outputs(self, tool_outputs, run_id):\n      # Use the submit_tool_outputs_stream helper\n      with client.beta.threads.runs.submit_tool_outputs_stream(\n        thread_id=self.current_run.thread_id,\n        run_id=self.current_run.id,\n        tool_outputs=tool_outputs,\n        event_handler=EventHandler(),\n      ) as stream:\n        for text in stream.text_deltas:\n          print(text, end=\"\", flush=True)\n        print()\n \n \nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  event_handler=EventHandler()\n) as stream:\n  stream.until_done()\n```\n\n----------------------------------------\n\nTITLE: Extracting Downloaded Data using zipfile in Python\nDESCRIPTION: Utilizes the `zipfile` module to extract the contents of the previously downloaded 'vector_database_wikipedia_articles_embedded.zip' file. The extracted files are placed into the '../data' directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Building High-Confidence Autocomplete Suggestions with OpenAI in Python\nDESCRIPTION: This snippet demonstrates how to use the OpenAI API's log probability outputs to build high-confidence autocomplete systems. It provides a list of sentence prefixes, requests autocompletions from the model, and, based on logprobs, classifies suggested tokens as high or low confidence. Results are displayed in HTML, showing probabilities for each next-token prediction. Dependencies: OpenAI API, NumPy, and HTML rendering utilities. Inputs: List of sequential sentence prefixes; Output: Dictionaries of high- and low-confidence completions, HTML summary. This establishes a method for dynamic, probability-driven word suggestion in text editors or chat UIs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nsentence_list = [\n    \"My\",\n    \"My least\",\n    \"My least favorite\",\n    \"My least favorite TV\",\n    \"My least favorite TV show\",\n    \"My least favorite TV show is\",\n    \"My least favorite TV show is Breaking Bad\",\n]\n\nhigh_prob_completions = {}\nlow_prob_completions = {}\nhtml_output = \"\"\n\nfor sentence in sentence_list:\n    PROMPT = \"\"\"Complete this sentence. You are acting as auto-complete. Simply complete the sentence to the best of your ability, make sure it is just ONE sentence: {sentence}\"\"\"\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": PROMPT.format(sentence=sentence)}],\n        model=\"gpt-4o-mini\",\n        logprobs=True,\n        top_logprobs=3,\n    )\n    html_output += f'<p>Sentence: {sentence}</p>'\n    first_token = True\n    for token in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:\n        html_output += f'<p style=\"color:cyan\">Predicted next token: {token.token}, <span style=\"color:darkorange\">logprobs: {token.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(token.logprob)*100,2)}%</span></p>'\n        if first_token:\n            if np.exp(token.logprob) > 0.95:\n                high_prob_completions[sentence] = token.token\n            if np.exp(token.logprob) < 0.60:\n                low_prob_completions[sentence] = token.token\n        first_token = False\n    html_output += \"<br>\"\n\ndisplay(HTML(html_output))\n\n```\n\n----------------------------------------\n\nTITLE: Making Retried OpenAI Chat Completion Request - Python\nDESCRIPTION: This function makes a call to the OpenAI Chat Completions API with built-in retry logic using `tenacity` decorators. It attempts the call multiple times with exponential backoff in case of failures.\n\nRequired Dependencies: Requires `@retry`, `wait_random_exponential`, `stop_after_attempt` decorators (typically from `tenacity`), an initialized OpenAI client (`client`), and a constant for the model name (`GPT_MODEL`).\n\nParameters:\n- `messages` (list[dict]): A list of message objects representing the conversation history.\n- `functions` (list[dict], optional): A list of function definitions for the model to use.\n- `model` (str, optional): The name of the model to use (defaults to `GPT_MODEL`).\n\nInputs: List of messages, optional function definitions, optional model name.\nOutputs: An OpenAI API response object upon success, or the Exception object after retries are exhausted.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\ndef chat_completion_request(messages, functions=None, model=GPT_MODEL):\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            functions=functions,\n        )\n        return response\n    except Exception as e:\n        print(\"Unable to generate ChatCompletion response\")\n        print(f\"Exception: {e}\")\n        return e\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment and Installing Dependencies - Bash\nDESCRIPTION: This snippet demonstrates how to create a Python virtual environment and install the required libraries to develop the meeting minutes generator application. It utilizes the venv module to isolate package installations, then installs 'openai' for API access and 'python-docx' for DOCX file interactions. The commands should be run in a Bash-compatible terminal and assume internet connectivity for pip to download packages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npython -m venv env\n\nsource env/bin/activate\n\npip install openai\npip install python-docx\n```\n\n----------------------------------------\n\nTITLE: Retrieving fine-tuning job status\nDESCRIPTION: Checks the status of the fine-tuning job, including training progress and token usage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.fine_tuning.jobs.retrieve(job_id)\n\nprint(\"Job ID:\", response.id)\nprint(\"Status:\", response.status)\nprint(\"Trained Tokens:\", response.trained_tokens)\n```\n\n----------------------------------------\n\nTITLE: Initializing Vector Store for OpenAI Blog PDFs\nDESCRIPTION: Creates a vector store named 'openai_blog_store' and uploads the PDF files to this store for indexing and searching.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstore_name = \"openai_blog_store\"\nvector_store_details = create_vector_store(store_name)\nupload_pdf_files_to_vector_store(vector_store_details[\"id\"])\n```\n\n----------------------------------------\n\nTITLE: Defining Function Tool to Retrieve Order Details by ID in Python Using OpenAI Agents SDK\nDESCRIPTION: Provides a simulated data lookup function returning order information given an order ID. It searches a predefined list of orders with fields like fulfillment status, tracking information, customer details, and terms of service acceptance. This function aids agents in decision-making processes related to dispute resolution by providing relevant order context. It returns the matching order object or a 'No order found' string if no match exists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@function_tool\ndef get_order(order_id: int) -> str:\n    \"\"\"\n    Retrieve an order by ID from a predefined list of orders.\n    Returns the corresponding order object or 'No order found'.\n    \"\"\"\n    orders = [\n        {\n            \"order_id\": 1234,\n            \"fulfillment_details\": \"not_shipped\"\n        },\n        {\n            \"order_id\": 9101,\n            \"fulfillment_details\": \"shipped\",\n            \"tracking_info\": {\n                \"carrier\": \"FedEx\",\n                \"tracking_number\": \"123456789012\"\n            },\n            \"delivery_status\": \"out for delivery\"\n        },\n        {\n            \"order_id\": 1121,\n            \"fulfillment_details\": \"delivered\",\n            \"customer_id\": \"cus_PZ1234567890\",\n            \"customer_phone\": \"+15551234567\",\n            \"order_date\": \"2023-01-01\",\n            \"customer_email\": \"customer1@example.com\",\n            \"tracking_info\": {\n                \"carrier\": \"UPS\",\n                \"tracking_number\": \"1Z999AA10123456784\",\n                \"delivery_status\": \"delivered\"\n            },\n            \"shipping_address\": {\n                \"zip\": \"10001\"\n            },\n            \"tos_acceptance\": {\n                \"date\": \"2023-01-01\",\n                \"ip\": \"192.168.1.1\"\n            }\n        }\n    ]\n    for order in orders:\n        if order[\"order_id\"] == order_id:\n            return order\n    return \"No order found\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Agent Class with BaseModel in Python\nDESCRIPTION: Implements a simple Agent class using BaseModel that defines the core properties of an AI agent including name, model, instructions, and available tools.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Agent(BaseModel):\n    name: str = \"Agent\"\n    model: str = \"gpt-4o-mini\"\n    instructions: str = \"You are a helpful Agent\"\n    tools: list = []\n```\n\n----------------------------------------\n\nTITLE: Export and verify OpenAI API Key in Python environment\nDESCRIPTION: Sets the OpenAI API key as an environment variable and verifies its presence, enabling Weaviate to access OpenAI services for embedding generation. This setup is essential for secure and seamless API integration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Export OpenAI API Key\n!export OPENAI_API_KEY=\"your key\"\n\n# Test that your OpenAI API key is correctly set as an environment variable\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Installing MyScale client and wget - Python\nDESCRIPTION: Installs the `clickhouse-connect` library, which is the MyScale client for Python, and the `wget` library for downloading a zip file containing embeddings data. This setup is essential for interacting with the MyScale database and retrieving the required dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# We'll need to install the MyScale client\n!pip install clickhouse-connect\n\n#Install wget to pull zip file\n!pip install wget\n```\n\n----------------------------------------\n\nTITLE: Executing Second Semantic Search Query (Python/Kusto)\nDESCRIPTION: Constructs and executes another KQL query string, similar to the first, but this time performing cosine similarity search against the `title_vector` column using the new `searchedEmbedding`. It retrieves the top 10 results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nKUSTO_QUERY = \"Wiki | extend similarity = series_cosine_similarity_fl(dynamic(\"+str(searchedEmbedding)+\"), title_vector,1,1) | top 10 by similarity desc \"\nRESPONSE = KUSTO_CLIENT.execute(KUSTO_DATABASE, KUSTO_QUERY)\n\ndf = dataframe_from_result_table(RESPONSE.primary_results[0])\ndf\n```\n\n----------------------------------------\n\nTITLE: Preparing Vectors for Upsert into Pinecone Index\nDESCRIPTION: This snippet constructs a list of vectors including IDs, embeddings, and metadata (original texts) for batch insertion into Pinecone. Metadata allows retrieval of original data after querying the vector index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef append_vectors(data, doc_embeds):\n    vectors = []\n    for d, e in zip(data, doc_embeds):\n        vectors.append({\n            \"id\": d['id'],\n            \"values\": e,\n            \"metadata\": {'text': d['text']}\n        })\n\n    return vectors\n\nvectors = append_vectors(data, doc_embeds)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with the Fine-tuned Model\nDESCRIPTION: Using the fine-tuned model to classify a text example, ensuring to use the same prompt format as during training.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nft_model = fine_tune_results.fine_tuned_model\n\n# note that this calls the legacy completions api - https://platform.openai.com/docs/api-reference/completions\nres = client.completions.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0)\nres.choices[0].text\n```\n\n----------------------------------------\n\nTITLE: Feeding Function Response Back to Model (Python)\nDESCRIPTION: Appends the result obtained from executing the local function to the message history, formatted with the 'function' role. This updated history is then sent back to the chat completions API, allowing the model to use the function's output to formulate its final, natural language response to the user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmessages.append(\n    {\n        \"role\": \"function\",\n        \"name\": \"get_current_weather\",\n        \"content\": json.dumps(response)\n    }\n)\n\nfunction_completion = client.chat.completions.create(\n    model=deployment,\n    messages=messages,\n    tools=functions,\n)\n\nprint(function_completion.choices[0].message.content.strip())\n```\n\n----------------------------------------\n\nTITLE: ChatGPT API call with detailed teaching assistant system message\nDESCRIPTION: Shows how to use a detailed system message to prime the assistant to explain concepts in depth with a teaching style.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# An example of a system message that primes the assistant to explain concepts in great depth\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n    ],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Embed and Insert Data in Batches\nDESCRIPTION: Iterates through the downloaded dataset, collecting movie metadata and descriptions into batches of a predefined size (`BATCH_SIZE`). For each batch, it uses the `embed` function to generate embeddings for the descriptions and then inserts the entire batch (metadata + embeddings) into the Zilliz collection. Includes a progress bar using `tqdm` and handles the insertion of any remaining data after the main loop.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom tqdm import tqdm\n\ndata = [\n    [], # title\n    [], # type\n    [], # release_year\n    [], # rating\n    [], # description\n]\n\n# Embed and insert in batches\nfor i in tqdm(range(0, len(dataset))):\n    data[0].append(dataset[i]['title'] or '')\n    data[1].append(dataset[i]['type'] or '')\n    data[2].append(dataset[i]['release_year'] or -1)\n    data[3].append(dataset[i]['rating'] or '')\n    data[4].append(dataset[i]['description'] or '')\n    if len(data[0]) % BATCH_SIZE == 0:\n        data.append(embed(data[4]))\n        collection.insert(data)\n        data = [[],[],[],[],[]]\n\n# Embed and insert the remainder \nif len(data[0]) != 0:\n    data.append(embed(data[4]))\n    collection.insert(data)\n    data = [[],[],[],[],[]]\n\n\n```\n\n----------------------------------------\n\nTITLE: Connect to Elasticsearch using Cloud ID\nDESCRIPTION: This snippet establishes a connection to an Elasticsearch cluster using the Cloud ID and password. It retrieves these credentials using `getpass` for secure input and creates an Elasticsearch client instance. Finally, it tests the connection using `client.info()`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCLOUD_ID = getpass(\"Elastic deployment Cloud ID\")\nCLOUD_PASSWORD = getpass(\"Elastic deployment Password\")\nclient = Elasticsearch(\n  cloud_id = CLOUD_ID,\n  basic_auth=(\"elastic\", CLOUD_PASSWORD) # Alternatively use `api_key` instead of `basic_auth`\n)\n\n# Test connection to Elasticsearch\nprint(client.info())\n```\n\n----------------------------------------\n\nTITLE: Requesting Word-Level Timestamps in Transcriptions via OpenAI API with cURL\nDESCRIPTION: This cURL snippet demonstrates submitting an audio file transcription request with the 'response_format' set to 'verbose_json' and word-level timestamps enabled. All necessary headers, authentication, and form parameters are provided. Output is a verbose JSON response with detailed word-by-word time offsets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_11\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/audio/transcriptions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@/path/to/file/audio.mp3\" \\\n  -F \"timestamp_granularities[]=word\" \\\n  -F model=\"whisper-1\" \\\n  -F response_format=\"verbose_json\"\n```\n\n----------------------------------------\n\nTITLE: Import Qdrant HTTP Models\nDESCRIPTION: Imports specific models (`rest`) from the `qdrant_client.http` module, which are used for defining collection configurations like vector parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom qdrant_client.http import models as rest\n```\n\n----------------------------------------\n\nTITLE: Defining the QA Chain\nDESCRIPTION: This code snippet defines a question-answering chain using Langchain. It initializes an `OpenAI` language model (LLM) and then creates a `VectorDBQA` chain using the `stuff` chain type.  The `vectorstore` argument is set to the previously created `doc_store` (Qdrant instance) containing the answers. The `return_source_documents=False` argument is used. This chain will retrieve answers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI()\nqa = VectorDBQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    vectorstore=doc_store,\n    return_source_documents=False,\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Schema Definition for SQL Execution API in YAML (Python-Flavored)\nDESCRIPTION: This OpenAPI 3.1 schema defines an API endpoint `/sql_statement` that accepts a POST request with a JSON payload containing a SQL statement string to execute. On success, it returns the SQL query results as a JSON file response encoded in base64. The schema defines request and response bodies, error response schema, and server URL placeholder. This specification can be used to generate API clients or document the Lambda-powered SQL query execution API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: SQL Execution API\n  description: API to execute SQL statements and return results as a file.\n  version: 1.0.0\nservers:\n  - url: {your_function_url}/Prod\n    description: Production server\npaths:\n  /sql_statement:\n    post:\n      operationId: executeSqlStatement\n      summary: Executes a SQL statement and returns the result as a file.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                sql_statement:\n                  type: string\n                  description: The SQL statement to execute.\n                  example: SELECT * FROM customers LIMIT 10\n              required:\n                - sql_statement\n      responses:\n        '200':\n          description: The SQL query result as a JSON file.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                          description: The name of the file.\n                          example: query_result.json\n                        mime_type:\n                          type: string\n                          description: The MIME type of the file.\n                          example: application/json\n                        content:\n                          type: string\n                          description: The base64 encoded content of the file.\n                          format: byte\n                          example: eyJrZXkiOiJ2YWx1ZSJ9\n        '500':\n          description: Error response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  error:\n                    type: string\n                    description: Error message.\n                    example: Database query failed error details\n```\n\n----------------------------------------\n\nTITLE: Whisper Baseline Transcription\nDESCRIPTION: This code snippet demonstrates the baseline transcription without any prompt. It calls the `transcribe` function, passing an empty string as the prompt and the path to the audio file. This step provides a reference for comparing the impact of different prompts on transcription accuracy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# baseline transcription with no prompt\ntranscribe(prompt=\"\", audio_filepath=ZyntriQix_filepath)\n```\n\n----------------------------------------\n\nTITLE: Query Weaviate with OpenAI embeddings\nDESCRIPTION: This function queries a Weaviate database using OpenAI embeddings. It takes a query string and a collection name as input, generates an embedding for the query using the OpenAI API, and then performs a vector search in Weaviate. The function returns the query results, including the title, content, and certainty/distance scores.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef query_weaviate(query, collection_name, top_k=20):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=EMBEDDING_MODEL,\n    )[\"data\"][0]['embedding']\n    \n    near_vector = {\"vector\": embedded_query}\n\n    # Queries input schema with vectorised user query\n    query_result = (\n        client.query\n        .get(collection_name, [\"title\", \"content\", \"_additional {certainty distance}\"])\n        .with_near_vector(near_vector)\n        .with_limit(top_k)\n        .do()\n    )\n    \n    return query_result\n```\n\n----------------------------------------\n\nTITLE: Extracting Wikipedia Data using API\nDESCRIPTION: This code defines functions to retrieve and filter Wikipedia pages related to the 2020 Olympic Games. It uses the `wikipedia` library to fetch pages by title and recursively finds linked pages. The `filter_olympic_2020_titles` function filters titles to include only those containing '2020' and 'olympi'. The `get_wiki_page` function retrieves a Wikipedia page by title, handling disambiguation errors. The `recursively_find_all_pages` function recursively explores linked pages, collecting relevant pages. The output is a list of wikipedia pages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport wikipedia\n\n\ndef filter_olympic_2020_titles(titles):\n    \"\"\"\n    Get the titles which are related to Olympic games hosted in 2020, given a list of titles\n    \"\"\"\n    titles = [title for title in titles if '2020' in title and 'olympi' in title.lower()]\n    \n    return titles\n\ndef get_wiki_page(title):\n    \"\"\"\n    Get the wikipedia page given a title\n    \"\"\"\n    try:\n        return wikipedia.page(title)\n    except wikipedia.exceptions.DisambiguationError as e:\n        return wikipedia.page(e.options[0])\n    except wikipedia.exceptions.PageError as e:\n        return None\n\ndef recursively_find_all_pages(titles, titles_so_far=set()):\n    \"\"\"\n    Recursively find all the pages that are linked to the Wikipedia titles in the list\n    \"\"\"\n    all_pages = []\n    \n    titles = list(set(titles) - titles_so_far)\n    titles = filter_olympic_2020_titles(titles)\n    titles_so_far.update(titles)\n    for title in titles:\n        page = get_wiki_page(title)\n        if page is None:\n            continue\n        all_pages.append(page)\n\n        new_pages = recursively_find_all_pages(page.links, titles_so_far)\n        for pg in new_pages:\n            if pg.title not in [p.title for p in all_pages]:\n                all_pages.append(pg)\n        titles_so_far.update(page.links)\n    return all_pages\n\n\npages = recursively_find_all_pages([\"2020 Summer Olympics\"])\nlen(pages)\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex and Langchain Modules in Python\nDESCRIPTION: Imports necessary modules from the llama_index library and langchain OpenAI wrapper for setting up LLMs, document readers, index building, and query engines. These imports provide access to classes for reading documents, building vector indexes, setting global service contexts, pretty-printing responses, defining query tools, and advanced query handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import OpenAI\n\nfrom llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\nfrom llama_index import set_global_service_context\nfrom llama_index.response.pprint_utils import pprint_response\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\nfrom llama_index.query_engine import SubQuestionQueryEngine\n```\n\n----------------------------------------\n\nTITLE: Loading JSON Documents with Embedded Vectors into Redis JSON Store in Python\nDESCRIPTION: Stores pre-constructed JSON documents containing raw text and its corresponding embedding vector into Redis using the JSON module. Each document is saved under a Redis key like 'doc:1', 'doc:2', and 'doc:3'. This allows subsequent searches and retrieval operations using Redis JSON and vector search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient.json().set('doc:1', '$', doc_1)\nclient.json().set('doc:2', '$', doc_2)\nclient.json().set('doc:3', '$', doc_3)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Settings\nDESCRIPTION: Setting up the OpenAI client and specifying the embedding model to use. This configuration is used for generating text embeddings that will be stored in Azure AI Search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\") # Saving this as a variable to reference in function app in later step\nopenai_client = OpenAI(api_key=openai_api_key)\nembeddings_model = \"text-embedding-3-small\" # We'll use this by default, but you can change to your text-embedding-3-large if desired\n```\n\n----------------------------------------\n\nTITLE: Visualizing Clusters with t-SNE and Matplotlib in Python\nDESCRIPTION: Performs dimensionality reduction of embeddings from multiple clusters to two dimensions using t-SNE, then visualizes the clusters with color-coded scatter plots. Each cluster's centroid is marked. Dependencies: scikit-learn (TSNE), numpy, matplotlib, pandas. Requires that KMeans clustering has already occurred and cluster labels are in 'df'. Expects 'matrix' and 'df.Cluster' to align. Outputs a scatter plot showing clustered data in 2D. The color mapping assumes four clusters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.manifold import TSNE\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\nvis_dims2 = tsne.fit_transform(matrix)\n\nx = [x for x, y in vis_dims2]\ny = [y for x, y in vis_dims2]\n\nfor category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\"]):\n    xs = np.array(x)[df.Cluster == category]\n    ys = np.array(y)[df.Cluster == category]\n    plt.scatter(xs, ys, color=color, alpha=0.3)\n\n    avg_x = xs.mean()\n    avg_y = ys.mean()\n\n    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\nplt.title(\"Clusters identified visualized in language 2d using t-SNE\")\n\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI Schema for Microsoft Graph API in GPT Action\nDESCRIPTION: Specifies the OpenAPI 3.1.0 schema for integrating a GPT Action with the Microsoft Graph API (v1.0) for Outlook functionalities. It defines endpoints for getting user profile (/me), messages (/me/messages), sending mail (/me/sendMail), and managing calendar events (/me/events), along with necessary data schemas (UserProfile, UserMessage, CalendarEvent, etc.) and OAuth2 security configuration using client credentials flow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_outlook.ipynb#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Microsoft Graph API Integration\n  version: 1.0.0\nservers:\n  - url: https://graph.microsoft.com/v1.0\ncomponents:\n  securitySchemes:\n    OAuth2:\n      type: oauth2\n      flows:\n        clientCredentials:\n          tokenUrl: https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\n          scopes:\n            https://graph.microsoft.com/User.Read: Access current user profile\n            https://graph.microsoft.com/Mail.Read: Read user mail\n            https://graph.microsoft.com/Mail.Send: Send mail\n            https://graph.microsoft.com/Calendars.ReadWrite: Read and write user calendars\n  schemas:\n    UserProfile:\n      type: object\n      properties:\n        id:\n          type: string\n        displayName:\n          type: string\n        mail:\n          type: string\n    UserMessage:\n      type: object\n      properties:\n        id:\n          type: string\n        subject:\n          type: string\n        bodyPreview:\n          type: string\n    CalendarEvent:\n      type: object\n      properties:\n        id:\n          type: string\n        subject:\n          type: string\n        start:\n          type: object\n          properties:\n            dateTime:\n              type: string\n            timeZone:\n              type: string\n        end:\n          type: object\n          properties:\n            dateTime:\n              type: string\n            timeZone:\n              type: string\n    NewEvent:\n      type: object\n      properties:\n        subject:\n          type: string\n        start:\n          type: object\n          properties:\n            dateTime:\n              type: string\n            timeZone:\n              type: string\n        end:\n          type: object\n          properties:\n            dateTime:\n              type: string\n            timeZone:\n              type: string\n        attendees:\n          type: array\n          items:\n            type: object\n            properties:\n              emailAddress:\n                type: object\n                properties:\n                  address:\n                    type: string\n                  name:\n                    type: string\n    SendMailRequest:\n      type: object\n      properties:\n        message:\n          type: object\n          properties:\n            subject:\n              type: string\n            body:\n              type: object\n              properties:\n                contentType:\n                  type: string\n                content:\n                  type: string\n            toRecipients:\n              type: array\n              items:\n                type: object\n                properties:\n                  emailAddress:\n                    type: object\n                    properties:\n                      address:\n                        type: string\nsecurity:\n  - OAuth2: []\npaths:\n  /me:\n    get:\n      operationId: getUserProfile\n      summary: Get the authenticated user's profile\n      security:\n        - OAuth2: []\n      responses:\n        '200':\n          description: A user profile\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserProfile'\n  /me/messages:\n    get:\n      operationId: getUserMessages\n      summary: Get the authenticated user's messages\n      security:\n        - OAuth2: []\n      parameters:\n        - name: $top\n          in: query\n          required: false\n          schema:\n            type: integer\n            default: 10\n            description: Number of messages to return\n        - name: $filter\n          in: query\n          required: false\n          schema:\n            type: string\n            description: OData filter query to narrow results\n        - name: $orderby\n          in: query\n          required: false\n          schema:\n            type: string\n            description: OData order by query to sort results\n      responses:\n        '200':\n          description: A list of user messages\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: '#/components/schemas/UserMessage'\n  /me/sendMail:\n    post:\n      operationId: sendUserMail\n      summary: Send an email as the authenticated user\n      security:\n        - OAuth2: []\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/SendMailRequest'\n      responses:\n        '202':\n          description: Accepted\n  /me/events:\n    get:\n      operationId: getUserCalendarEvents\n      summary: Get the authenticated user's calendar events\n      security:\n        - OAuth2: []\n      responses:\n        '200':\n          description: A list of calendar events\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: '#/components/schemas/CalendarEvent'\n    post:\n      operationId: createUserCalendarEvent\n      summary: Create a new calendar event for the authenticated user\n      security:\n        - OAuth2: []\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/NewEvent'\n      responses:\n        '201':\n          description: Created\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/CalendarEvent'\n\n```\n\n----------------------------------------\n\nTITLE: Executing Multi-Turn GPT-4o Customer Support Flow with Tooling in Python\nDESCRIPTION: Defines helper functions and a main function to run two sequential GPT-4o-mini chat completion requests with delays. The function completion_run calls the OpenAI client API using a given message history and tool inputs, requesting completions with specified model and tools. The main function triggers two runs: one with initial messages, then after a 7-second delay appends a follow-up user query to the conversation before the second run. The script prints formatted JSON outputs of the completions, illustrating multi-turn conversation handling, tool usage, and completion invocation in Python. Prerequisites include an instantiated OpenAI client object, proper authorization, and imported time and json modules.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef completion_run(messages, tools):\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        tools=tools,\n        messages=messages,\n        tool_choice=\"required\"\n    )\n    usage_data = json.dumps(completion.to_dict(), indent=4)\n    return usage_data\n\n\ndef main(messages, tools, user_query2):\n    # Run 1: Initial query\n    print(\"Run 1:\")\n    run1 = completion_run(messages, tools)\n    print(run1)\n\n    # Delay for 7 seconds\n    time.sleep(7)\n\n    # Append user_query2 to the message history\n    messages.append(user_query2)\n\n    # Run 2: With appended query\n    print(\"\\nRun 2:\")\n    run2 = completion_run(messages, tools)\n    print(run2)\n\n\n# Run the main function\nmain(messages, tools, user_query2)\n```\n\n----------------------------------------\n\nTITLE: Display DataFrame with HTML\nDESCRIPTION: This snippet displays the top 5 rows of a Pandas DataFrame as an HTML table. It uses the `display` and `HTML` functions from the `IPython.display` module to render the table in an interactive environment, allowing for easy visualization of the processed data.  It assumes the DataFrame (`df`) has already been created and populated with data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom IPython.display import display, HTML\n\n# Convert the DataFrame to an HTML table and display top 5 rows \ndisplay(HTML(df.head().to_html()))\n```\n\n----------------------------------------\n\nTITLE: Querying Image URL with OpenAI Vision (Node.js)\nDESCRIPTION: Illustrates sending an image URL to the OpenAI chat completions API using the Node.js library. The example uses an async function to create the chat completion with text and image URL content. Requires the OpenAI Node.js library and an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_2\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          { type: \"text\", text: \"Whatâ€™s in this image?\" },\n          {\n            type: \"image_url\",\n            image_url: {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n            },\n          },\n        ],\n      },\n    ],\n  });\n  console.log(response.choices[0]);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Batch Inserting Data into Milvus (Python)\nDESCRIPTION: Loops through the downloaded dataset, extracting movie details (title, type, year, rating, description). Descriptions are collected in batches of size `BATCH_SIZE`, embedded using the `embed` function, and then inserted into the Milvus collection. A final insertion handles any remaining data after the loop.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\ndata = [\n    [], # title\n    [], # type\n    [], # release_year\n    [], # rating\n    [], # description\n]\n\n# Embed and insert in batches\nfor i in tqdm(range(0, len(dataset))):\n    data[0].append(dataset[i]['title'] or '')\n    data[1].append(dataset[i]['type'] or '')\n    data[2].append(dataset[i]['release_year'] or -1)\n    data[3].append(dataset[i]['rating'] or '')\n    data[4].append(dataset[i]['description'] or '')\n    if len(data[0]) % BATCH_SIZE == 0:\n        data.append(embed(data[4]))\n        collection.insert(data)\n        data = [[],[],[],[],[]]\n\n# Embed and insert the remainder \nif len(data[0]) != 0:\n    data.append(embed(data[4]))\n    collection.insert(data)\n    data = [[],[],[],[],[]]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Dependencies\nDESCRIPTION: Installs the necessary Python packages for working with OpenAI embeddings and connecting to PostgreSQL databases.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai psycopg2 pandas wget\n```\n\n----------------------------------------\n\nTITLE: Clean and Filter Wikipedia Section Text\nDESCRIPTION: This code cleans and filters the extracted Wikipedia sections. The `clean_section` function removes `<ref>...</ref>` tags and leading/trailing whitespace from the section text. The `keep_section` function filters out short or blank sections. The code then applies these functions to the `wikipedia_sections` list and prints the number of sections that were filtered out.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# clean text\ndef clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n    \"\"\"\n    Return a cleaned up section with:\n        - <ref>xyz</ref> patterns removed\n        - leading/trailing whitespace removed\n    \"\"\"\n    titles, text = section\n    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n    text = text.strip()\n    return (titles, text)\n\n\nwikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n\n# filter out short/blank sections\ndef keep_section(section: tuple[list[str], str]) -> bool:\n    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n    titles, text = section\n    if len(text) < 16:\n        return False\n    else:\n        return True\n\n\noriginal_num_sections = len(wikipedia_sections)\nwikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\nprint(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting and printing sampling data using pandas in Python\nDESCRIPTION: This snippet iterates over the first five rows of a DataFrame filtered for 'sampling' events, normalizes nested JSON data within each row, and prints the prompt and sampled output. It relies on pandas for data handling and is used for quick inspection of sample results in structured data. Dependencies include pandas and potentially a JSON library for normalization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfor i, row in events_df[events_df['type'] == 'sampling'].head(5).iterrows():\n    data = pd.json_normalize(row['data'])\n    print(f\"Prompt: {data['prompt'].iloc[0]}\")\n    print(f\"Sampled: {data['sampled'].iloc[0]}\")\n    print(\"-\" * 10)\n```\n\n----------------------------------------\n\nTITLE: Handling Requests and Processing Documents - Azure Function Javascript\nDESCRIPTION: This Javascript snippet contains the core logic for an Azure Function acting as an API endpoint. It processes incoming HTTP requests containing a query and search term, authenticates the user using an OBO token, initializes a Microsoft Graph client, performs a document search, retrieves and processes the content of relevant search results using helper functions for tokenization, windowing, and sending text to a GPT model, and finally returns the processed results or an error.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nmodule.exports = async function (context, req) {\n    const query = req.query.query || (req.body && req.body.query);\n    const searchTerm = req.query.searchTerm || (req.body && req.body.searchTerm);\n    if (!req.headers.authorization) {\n        context.res = {\n            status: 400,\n            body: 'Authorization header is missing'\n        };\n        return;\n    }\n    /// The below takes the token passed to the function, to use to get an OBO token.\n    const bearerToken = req.headers.authorization.split(' ')[1];\n    let accessToken;\n    try {\n        accessToken = await getOboToken(bearerToken);\n    } catch (error) {\n        context.res = {\n            status: 500,\n            body: `Failed to obtain OBO token: ${error.message}`\n        };\n        return;\n    }\n    // Initialize the Graph Client using the initGraphClient function defined above\n    let client = initGraphClient(accessToken);\n    // this is the search body to be used in the Microsft Graph Search API: https://learn.microsoft.com/en-us/graph/search-concept-files\n    const requestBody = {\n        requests: [\n            {\n                entityTypes: ['driveItem'],\n                query: {\n                    queryString: searchTerm\n                },\n                from: 0,\n                // the below is set to summarize the top 10 search results from the Graph API, but can configure based on your documents. \n                size: 10\n            }\n        ]\n    };\n\n    try { \n        // Function to tokenize content (e.g., based on words). \n        const tokenizeContent = (content) => {\n            return content.split(/\\s+/);\n        };\n\n        // Function to break tokens into 10k token windows for gpt-3.5-turbo\n        const breakIntoTokenWindows = (tokens) => {\n            const tokenWindows = []\n            const maxWindowTokens = 10000; // 10k tokens\n            let startIndex = 0;\n\n            while (startIndex < tokens.length) {\n                const window = tokens.slice(startIndex, startIndex + maxWindowTokens);\n                tokenWindows.push(window);\n                startIndex += maxWindowTokens;\n            }\n\n            return tokenWindows;\n        };\n        // This is where we are doing the search\n        const list = await client.api('/search/query').post(requestBody);\n\n        const processList = async () => {\n            // This will go through and for each search response, grab the contents of the file and summarize with gpt-3.5-turbo\n            const results = [];\n\n            await Promise.all(list.value[0].hitsContainers.map(async (container) => {\n                for (const hit of container.hits) {\n                    if (hit.resource[\"@odata.type\"] === \"#microsoft.graph.driveItem\") {\n                        const { name, id } = hit.resource;\n                        // We use the below to grab the URL of the file to include in the response\n                        const webUrl = hit.resource.webUrl.replace(/\\s/g, \"%20\");\n                        // The Microsoft Graph API ranks the reponses, so we use this to order it\n                        const rank = hit.rank;\n                        // The below is where the file lives\n                        const driveId = hit.resource.parentReference.driveId;\n                        const contents = await getDriveItemContent(client, driveId, id, name);\n                        if (contents !== 'Unsupported File Type') {\n                            // Tokenize content using function defined previously\n                            const tokens = tokenizeContent(contents);\n\n                            // Break tokens into 10k token windows\n                            const tokenWindows = breakIntoTokenWindows(tokens);\n\n                            // Process each token window and combine results\n                            const relevantPartsPromises = tokenWindows.map(window => getRelevantParts(window.join(' '), query));\n                            const relevantParts = await Promise.all(relevantPartsPromises);\n                            const combinedResults = relevantParts.join('\\n'); // Combine results\n\n                            results.push({ name, webUrl, rank, contents: combinedResults });\n                        } \n                        else {\n                            results.push({ name, webUrl, rank, contents: 'Unsupported File Type' });\n                        }\n                    }\n                }\n            }));\n\n            return results;\n        };\n        let results;\n        if (list.value[0].hitsContainers[0].total == 0) {\n            // Return no results found to the API if the Microsoft Graph API returns no results\n            results = 'No results found';\n        } else {\n            // If the Microsoft Graph API does return results, then run processList to iterate through.\n            results = await processList();\n            results.sort((a, b) => a.rank - b.rank);\n        }\n        context.res = {\n            status: 200,\n            body: results\n        };\n    } catch (error) {\n        context.res = {\n            status: 500,\n            body: `Error performing search or processing results: ${error.message}`,\n        };\n    }\n};\n\n```\n\n----------------------------------------\n\nTITLE: Loading Supabase Credentials in Deno Using Standard Library dotenv\nDESCRIPTION: Demonstrates how to load environment variables from a .env file in Deno using the official standard library's dotenv module. After loading, credentials are accessed from the returned object. This approach secures configuration and avoids hard-coding sensitive keys.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport { load } from \"https://deno.land/std@0.208.0/dotenv/mod.ts\";\n\n// Load .env file\nconst env = await load();\n\nconst supabaseUrl = env[\"SUPABASE_URL\"];\nconst supabaseServiceRoleKey = env[\"SUPABASE_SERVICE_ROLE_KEY\"];\n```\n\n----------------------------------------\n\nTITLE: Copying and Configuring Environment Variables for AWS Lambda Deployment in Bash\nDESCRIPTION: This Bash snippet shows how to create an environment configuration file for the AWS Lambda function by copying a sample YAML file. The file `env.yaml` stores all the required parameters such as Redshift connection details and network settings needed for deployment. This script simplifies populating parameters that are later parsed and used by the SAM deployment command.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp env.sample.yaml env.yaml\n```\n\n----------------------------------------\n\nTITLE: Converting JSON to CSV in Python\nDESCRIPTION: This Python code snippet converts a JSON array of objects into a CSV file.  It loads JSON data from a string, writes the header row based on the JSON keys, and populates subsequent rows with the corresponding values.  The `csv` and `json` libraries are required. The output is saved to 'output.csv'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport csv\n\n# Sample JSON array of objects\njson_data = '''\n[\n    {\"account_id\": 1, \"number_of_users\": 10, \"total_revenue\": 43803.96, \"revenue_per_user\": 4380.40}, \n    {\"account_id\": 2, \"number_of_users\": 12, \"total_revenue\": 77814.84, \"revenue_per_user\": 6484.57}\n]\n'''\n\n# Load JSON data\ndata = json.loads(json_data)\n\n# Define the CSV file name\ncsv_file = 'output.csv'\n\n# Write JSON data to CSV\nwith open(csv_file, 'w', newline='') as csvfile:\n    # Create a CSV writer object\n    csvwriter = csv.writer(csvfile)\n    \n    # Write the header (keys of the first dictionary)\n    header = data[0].keys()\n    csvwriter.writerow(header)\n    \n    # Write the data rows\n    for row in data:\n        csvwriter.writerow(row.values())\n\nprint(f\"JSON data has been written to {csv_file}\")\n```\n\n----------------------------------------\n\nTITLE: Running OpenAI Task for Text Enrichment\nDESCRIPTION: This snippet defines a sample text and then calls the `run_openai_task` function to process the text and enrich it with entities. The `labels` variable used in the function call is assumed to be defined elsewhere.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntext = \"\"\"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison, and Ringo Starr.\"\"\"\nresult = run_openai_task(labels, text)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (macOS/Linux)\nDESCRIPTION: Shows the command to set the `OPENAI_API_KEY` environment variable persistently in a Bash or Zsh profile file on macOS or Linux systems. Replace 'your-api-key-here' with your actual key. This allows the SDK to automatically detect the key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/node-setup.txt#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Install OpenAI SDK\nDESCRIPTION: Installs or upgrades the OpenAI Python SDK to the latest version (1.3.3 or later) using pip. This ensures access to the 'seed' parameter and 'system_fingerprint' features in the Chat Completion API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --upgrade openai # Switch to the latest version of OpenAI (1.3.3 at time of writing)\n```\n\n----------------------------------------\n\nTITLE: Defining RediSearch Index Schema Fields (Python)\nDESCRIPTION: This code defines the schema for the RediSearch index using `TextField` and `VectorField` objects from the `redis-py` library. It specifies the fields that will be indexed, including text fields for title, URL, and text content, and vector fields for the title and content embeddings, detailing their type, dimension, distance metric, and initial capacity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Define RediSearch fields for each of the columns in the dataset\ntitle = TextField(name=\"title\")\nurl = TextField(name=\"url\")\ntext = TextField(name=\"text\")\ntitle_embedding = VectorField(\"title_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\ntext_embedding = VectorField(\"content_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\nfields = [title, url, text, title_embedding, text_embedding]\n```\n\n----------------------------------------\n\nTITLE: Printing the Web Search Response Output\nDESCRIPTION: Converts the response object to JSON and prints it with indentation for better readability.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nprint(json.dumps(response.output, default=lambda o: o.__dict__, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Vector Search Results in Python\nDESCRIPTION: Prints information about each search result, including content length, source filename, and relevancy score calculated by the hybrid search ranker.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor result in search_results.data:\n    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))\n```\n\n----------------------------------------\n\nTITLE: Defining Pretty Printer for Elasticsearch Results in Python\nDESCRIPTION: Defines a function to print relevant fields (ID, Title, Summary, Score) for each result from an Elasticsearch query. Input is an Elasticsearch response in dictionary form. Does not return values, but formats output for readability in the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Function to pretty print Elasticsearch results\n\ndef pretty_response(response):\n    for hit in response['hits']['hits']:\n        id = hit['_id']\n        score = hit['_score']\n        title = hit['_source']['title']\n        text = hit['_source']['text']\n        pretty_output = (f\"\\nID: {id}\\nTitle: {title}\\nSummary: {text}\\nScore: {score}\")\n        print(pretty_output)\n```\n\n----------------------------------------\n\nTITLE: Prompting OpenAI GPT Chat Completion to Map Clusters to Topics and Generate New Topics Using Python\nDESCRIPTION: This snippet defines a detailed prompt string embedded with previously formatted examples, instructing the GPT model to map cluster numbers to topic names and suggest additional new topics to increase dataset diversity. It then sends this prompt to the OpenAI chat completion API with a system message framing the assistant as helpful for analyzing clustered data. The output is expected to strictly follow a specified structured plain-text format for easier parsing. Prerequisites include an OpenAI client instance, a defined model identifier, and the 'formatted_examples' string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntopic_prompt = f\"\"\"\n    I previously generated some examples of input output trainings pairs and then I clustered them based on category. From each cluster I picked 3 example data point which you can find below.\n    I want to promote diversity in my examples across categories so follow the procedure below:\n    1. You must identify the broad topic areas these clusters belong to.\n    2. You should generate further topic areas which don't exist so I can generate data within these topics to improve diversity.\n\n\n    Previous examples:\n    {formatted_examples}\n\n\n    Your output should be strictly of the format:\n\n    1. Cluster topic mapping\n    Cluster: number, topic: topic\n    Cluster: number, topic: topic\n    Cluster: number, topic: topic\n\n    2. New topics\n    1. topic\n    2. topic\n    3. topic\n    4. topic\n\n    Do not add any extra characters around that formatting as it will make the output parsing break. It is very important you stick to that output format\n    \"\"\"\n\nresponse = client.chat.completions.create(\n  model=datagen_model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to analyze clustered data\"},\n    {\"role\": \"user\", \"content\": topic_prompt}\n  ]\n)\nres = response.choices[0].message.content\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data with Repeated OpenAI GPT Prompts Loop in Python\nDESCRIPTION: This snippet loops three times to generate new synthetic input-output training examples by prompting the GPT model. Each prompt requests examples classified under existing topic areas derived from previous cluster-to-topic mapping, strictly formatted with a topic header and labeled input-output pairs. Output strings from each iteration are collected and concatenated. This iterative approach helps expand the quantity and variety of training data. Dependencies include the OpenAI client, model identifier, and 'cluster_topic_mapping' list containing topics from prior parsing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noutput_string = \"\"\nfor i in range(3):\n  question = f\"\"\"\n  I am creating input output training pairs to fine tune my gpt model. I want the input to be product name and category and output to be description. the category should be things like: mobile phones, shoes, headphones, laptop, electronic toothbrush, etc. and also more importantly the categories should come under some main topics: {[entry['topic'] for entry in cluster_topic_mapping]})\n  After the number of each example also state the topic area. The format should be of the form:\n  1. topic_area\n  Input: product_name, category\n  Output: description\n\n  Do not add any extra characters around that formatting as it will make the output parsing break.\n\n  Here are some helpful examples so you get the style of output correct.\n\n  1) clothing\n  Input: \"Shoe Name, Shoes\"\n  Output: \"Experience unparalleled comfort. These shoes feature a blend of modern style and the traditional superior cushioning, perfect for those always on the move.\"\n  \"\"\"\n\n  response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n      {\"role\": \"user\", \"content\": question}\n    ]\n  )\n  res = response.choices[0].message.content\n  output_string += res + \"\\n\" + \"\\n\"\nprint(output_string)\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Template\nDESCRIPTION: This code creates a `PromptTemplate` object from the previously defined `custom_prompt` string.  The `input_variables` parameter specifies the variables (context and question) that will be dynamically inserted into the template. This template is then used to modify the LLM's output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncustom_prompt_template = PromptTemplate(\n    template=custom_prompt, input_variables=[\"context\", \"question\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Dataset\nDESCRIPTION: Evaluates the retriever's performance against the generated `qa_dataset`. It runs the evaluation asynchronously using `aevaluate_dataset`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate\neval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n```\n\n----------------------------------------\n\nTITLE: Populating ChromaDB Collections with Embeddings in Python\nDESCRIPTION: Adds the pre-computed vector embeddings and corresponding IDs from the pandas DataFrame into the respective ChromaDB collections. The `content_vector` list is added to `wikipedia_content_collection` and the `title_vector` list is added to `wikipedia_title_collection`, using the `vector_id` column as the unique identifier for each entry.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Add the content vectors\nwikipedia_content_collection.add(\n    ids=article_df.vector_id.tolist(),\n    embeddings=article_df.content_vector.tolist(),\n)\n\n# Add the title vectors\nwikipedia_title_collection.add(\n    ids=article_df.vector_id.tolist(),\n    embeddings=article_df.title_vector.tolist(),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and polling OpenAI Assistant run\nDESCRIPTION: This Python snippet creates a run for a given thread and assistant, polls for its completion, retrieves messages, handles required actions by submitting tool outputs, and retrieves messages again after tool outputs are submitted. It utilizes the `create_and_poll` and `submit_tool_outputs_and_poll` methods from the OpenAI client and handles exceptions during tool output submission.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling-run-example--polling.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create_and_poll(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n)\n \nif run.status == 'completed':\n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n  print(messages)\nelse:\n  print(run.status)\n \n# Define the list to store tool outputs\ntool_outputs = []\n \n# Loop through each tool in the required action section\nfor tool in run.required_action.submit_tool_outputs.tool_calls:\n  if tool.function.name == \"get_current_temperature\":\n    tool_outputs.append({\n      \"tool_call_id\": tool.id,\n      \"output\": \"57\"\n    })\n  elif tool.function.name == \"get_rain_probability\":\n    tool_outputs.append({\n      \"tool_call_id\": tool.id,\n      \"output\": \"0.06\"\n    })\n \n# Submit all tool outputs at once after collecting them in a list\nif tool_outputs:\n  try:\n    run = client.beta.threads.runs.submit_tool_outputs_and_poll(\n      thread_id=thread.id,\n      run_id=run.id,\n      tool_outputs=tool_outputs\n    )\n    print(\"Tool outputs submitted successfully.\")\n  except Exception as e:\n    print(\"Failed to submit tool outputs:\", e)\nelse:\n  print(\"No tool outputs to submit.\")\n \nif run.status == 'completed':\n  messages = client.beta.threads.messages.list(\n    thread_id=thread.id\n  )\n  print(messages)\nelse:\n  print(run.status)\n```\n\n----------------------------------------\n\nTITLE: Generating and Storing Embeddings in MongoDB Collection using Python\nDESCRIPTION: Iterates through documents in the 'movies' collection with a 'plot' field (limited to 500), generates an embedding for the plot, adds it to the document, and prepares a `ReplaceOne` request. It then performs a bulk write operation to update the documents with the new embedding field. This process enriches the documents with vector data for search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pymongo import ReplaceOne\n\n# Update the collection with the embeddings\nrequests = []\n\nfor doc in collection.find({'plot':{\"exists\": True}}).limit(500):\n  doc[EMBEDDING_FIELD_NAME] = generate_embedding(doc['plot'])\n  requests.append(ReplaceOne({'_id': doc['_id']}, doc))\n\ncollection.bulk_write(requests)\n\n```\n\n----------------------------------------\n\nTITLE: Generating Quotes using OpenAI - Python\nDESCRIPTION: This code defines a function `generate_quote` that generates a quote on a given topic using the OpenAI API.  It searches for similar quotes using `find_quote_and_author` (assumed to be defined elsewhere), formats a prompt with the topic and examples, calls the OpenAI Chat Completion API with a specified model, temperature, and max tokens, and returns the generated quote.  Dependencies: `openai` (client), uses `find_quote_and_author` function, which depends on external libraries used in the previous snippets, such as `astra_db` and `openai`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_quote(topic, n=2, author=None, tags=None):\n    quotes = find_quote_and_author(query_quote=topic, n=n, author=author, tags=tags)\n    if quotes:\n        prompt = generation_prompt_template.format(\n            topic=topic,\n            examples=\"\\n\".join(f\"  - {quote[0]}\" for quote in quotes),\n        )\n        # a little logging:\n        print(\"** quotes found:\")\n        for q, a in quotes:\n            print(f\"**    - {q} ({a})\")\n        print(\"** end of logging\")\n        #\n        response = client.chat.completions.create(\n            model=completion_model_name,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7,\n            max_tokens=320,\n        )\n        return response.choices[0].message.content.replace('\"', '').strip()\n    else:\n        print(\"** no quotes found.\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Creating Template Prompt for Complex Entity Extraction\nDESCRIPTION: This code defines a template prompt for extracting key information from the regulation document for more complex questions. It specifies the desired format for the output, including page numbers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example prompt - \ntemplate_prompt=f'''Extract key pieces of information from this regulation document.\nIf a particular piece of information is not present, output \\\"Not specified\\\".\nWhen you extract a key piece of information, include the closest page number.\nUse the following format:\\n0. Who is the author\\n1. How is a Minor Overspend Breach calculated\\n2. How is a Major Overspend Breach calculated\\n3. Which years do these financial regulations apply to\\n\\nDocument: \\\"\\\"\\\"<document>\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\nprint(template_prompt)\n```\n\n----------------------------------------\n\nTITLE: Filtering and displaying incorrect metric results in Python\nDESCRIPTION: This code iterates over a DataFrame of 'metrics' events, filters for incorrect responses, retrieves the previous prompt data, and calls the 'pretty_print_text' function to display the prompt and results. It accesses nested data structures, handles missing data with defaults, and relies on pandas for DataFrame operations and the earlier text extraction function for display purposes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Inspect metrics where choice is made and print only the prompt, result, and expected result if the choice is incorrect\nfor i, row in events_df[events_df['type'] == 'metrics'].iterrows():\n    if row['data']['choice'] == 'Incorrect':\n        # Get the previous row's data, which contains the prompt and the expected result\n        prev_row = events_df.iloc[i-1]\n        prompt = prev_row['data']['prompt'][0]['content'] if 'prompt' in prev_row['data'] and len(prev_row['data']['prompt']) > 0 else \"Prompt not available\"\n        expected_result = prev_row['data'].get('ideal', 'Expected result not provided')\n        \n        # Current row's data will be the actual result\n        result = row['data'].get('result', 'Actual result not provided')\n        \n        pretty_print_text(prompt)\n        print(\"-\" * 40)\n```\n\n----------------------------------------\n\nTITLE: Creating a Training Dataset for Sarcastic Chatbot in JSONL Format\nDESCRIPTION: Example of how to format a training dataset for fine-tuning a chatbot with a sarcastic personality. Each example is a conversation with system, user, and assistant messages in JSONL format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_0\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Constants\nDESCRIPTION: This snippet imports necessary Python libraries, including OpenAI, pandas, numpy, os, wget, ast, and pinecone. It also sets the embedding model to be used (EMBEDDING_MODEL) and configures warnings to be ignored.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nfrom typing import List, Iterator\nimport pandas as pd\nimport numpy as np\nimport os\nimport wget\nfrom ast import literal_eval\n\n# Pinecone's client library for Python\nimport pinecone\n\n# I've set this to our new embeddings model, this can be changed to the embedding model of your choice\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n```\n\n----------------------------------------\n\nTITLE: Handling Specific ArXiv Function Calls - Python\nDESCRIPTION: This function is responsible for executing the specific Python code associated with the function name requested by the OpenAI model. It currently supports 'get_articles' and 'read_article_and_summarize', parsing arguments and handling the subsequent steps, including potentially making another API call after a function execution.\n\nRequired Dependencies: Depends on `json`, `get_articles`, `summarize_text`, and `chat_completion_request`.\n\nParameters:\n- `messages` (list[dict]): The conversation history, potentially modified after execution.\n- `full_message` (OpenAI API response choice object): The API response choice containing the function call details.\n\nInputs: Conversation history and the API response object indicating a function call.\nOutputs: An OpenAI API response object containing the result of the function call or a subsequent chat completion. Raises an exception if an unknown function is requested.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef call_arxiv_function(messages, full_message):\n    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n    Currently extended by adding clauses to this if statement.\"\"\"\n\n    if full_message.message.function_call.name == \"get_articles\":\n        try:\n            parsed_output = json.loads(\n                full_message.message.function_call.arguments\n            )\n            print(\"Getting search results\")\n            results = get_articles(parsed_output[\"query\"])\n        except Exception as e:\n            print(parsed_output)\n            print(f\"Function execution failed\")\n            print(f\"Error message: {e}\")\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": full_message.message.function_call.name,\n                \"content\": str(results),\n            }\n        )\n        try:\n            print(\"Got search results, summarizing content\")\n            response = chat_completion_request(messages)\n            return response\n        except Exception as e:\n            print(type(e))\n            raise Exception(\"Function chat request failed\")\n\n    elif (\n        full_message.message.function_call.name == \"read_article_and_summarize\"\n    ):\n        parsed_output = json.loads(\n            full_message.message.function_call.arguments\n        )\n        print(\"Finding and reading paper\")\n        summary = summarize_text(parsed_output[\"query\"])\n        return summary\n\n    else:\n        raise Exception(\"Function does not exist and cannot be called\")\n```\n\n----------------------------------------\n\nTITLE: Creating Fine-tuning Job for GPT Model with OpenAI Python SDK\nDESCRIPTION: Initiates a fine-tuning job on OpenAI's platform using the uploaded training and validation JSONL files. Specifies the base model 'gpt-4o-2024-08-06' as the starting point for fine-tuning. This step starts the asynchronous fine-tuning process which, upon completion, yields a specialized GPT model tailored to classify transactions based on the provided labeled data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfine_tuning_job = client.fine_tuning.jobs.create(training_file=train_file.id, validation_file=valid_file.id, model=\"gpt-4o-2024-08-06\")\n\n```\n\n----------------------------------------\n\nTITLE: Update Snowflake OAuth Redirect URI (SQL)\nDESCRIPTION: Alters the 'CHATGPT_INTEGRATION' security integration to update the 'OAUTH_REDIRECT_URI' parameter. This URI should be set to the specific callback URL provided by the ChatGPT Action configuration after the initial setup, finalizing the authentication linkage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nALTER SECURITY INTEGRATION CHATGPT_INTEGRATION SET OAUTH_REDIRECT_URI='https://chat.openai.com/aip/<callback_id>/oauth/callback';\n```\n\n----------------------------------------\n\nTITLE: Defining a ChatGPT Action with OpenAPI Specification (YAML)\nDESCRIPTION: This OpenAPI 3.1.0 specification defines the 'Success API' used for a ChatGPT Action. It describes a POST endpoint at `/my_route` hosted on an AWS API Gateway (`https://3ho5n15aef.execute-api.us-east-1.amazonaws.com/Prod`), specifies the `postSuccess` operation, and defines the expected 200 JSON response (`{\"success\": true}`). This spec is intended to be pasted into the ChatGPT Action configuration interface as the first step in setting up the action.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Success API\n  description: API that returns a success message.\n  version: 1.0.0\nservers:\n  - url: https://3ho5n15aef.execute-api.us-east-1.amazonaws.com/Prod\n    description: Main production server\npaths:\n  /my_route:\n    post:\n      operationId: postSuccess\n      summary: Returns a success message.\n      description: Endpoint to check the success status.\n      responses:\n        '200':\n          description: A JSON object indicating success.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  success:\n                    type: boolean\n                    example: true\n```\n\n----------------------------------------\n\nTITLE: Importing Data into Weaviate Batch - Python\nDESCRIPTION: Iterates through the loaded dataset and adds each article as a data object to the Weaviate batch for import. The batch process handles sending the data to Weaviate efficiently. Requires a connected Weaviate client with batch configured and the 'Article' class schema to exist.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n### Step 3 - import data\n\nprint(\"Importing Articles\")\n\ncounter=0\n\nwith client.batch as batch:\n    for article in dataset:\n        if (counter %10 == 0):\n            print(f\"Import {counter} / {len(dataset)} \")\n\n        properties = {\n            \"title\": article[\"title\"],\n            \"content\": article[\"text\"],\n            \"url\": article[\"url\"]\n        }\n        \n        batch.add_data_object(properties, \"Article\")\n        counter = counter+1\n\nprint(\"Importing Articles complete\")       \n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files\nDESCRIPTION: This function extracts text from a PDF file using the `PdfReader` library. It initializes a PDF reader and iterates through each page, extracting text. The extracted text is then concatenated and returned as a single string. It takes a PDF file path as input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    # Initialize the PDF reader\n    reader = PdfReader(pdf_path)\n    text = \"\"\n    # Iterate through each page in the PDF and extract text\n    for page in reader.pages:\n        text += page.extract_text()\n    return text\n```\n\n----------------------------------------\n\nTITLE: Creating Langchain Prompt Template for Conversational Product Search Agent in Python\nDESCRIPTION: Defines a multi-line string prompt template with detailed instructions and formatting rules guiding a Langchain conversational agent in how to leverage the defined Query and Similarity Search tools. It includes stepwise reasoning templates, user interaction guidelines, fallback logic, entity types to consider, and expected response formatting. The template supports dynamic substitution of tool descriptions, user inputs, and scratchpad actions. It requires the 'typing' and 'langchain.prompts' libraries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import StringPromptTemplate\nfrom typing import Callable\n\n\nprompt_template = '''Your goal is to find a product in the database that best matches the user prompt.\nYou have access to these tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input prompt from the user\nThought: you should always think about what to do\nAction: the action to take (refer to the rules below)\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nRules to follow:\n\n1. Start by using the Query tool with the prompt as parameter. If you found results, stop here.\n2. If the result is an empty array, use the similarity search tool with the full initial user prompt. If you found results, stop here.\n3. If you cannot still cannot find the answer with this, probe the user to provide more context on the type of product they are looking for. \n\nKeep in mind that we can use entities of the following types to search for products:\n\n{entity_types}.\n\n3. Repeat Step 1 and 2. If you found results, stop here.\n\n4. If you cannot find the final answer, say that you cannot help with the question.\n\nNever return results if you did not find any results in the array returned by the query tool or the similarity search tool.\n\nIf you didn't find any result, reply: \"Sorry, I didn't find any suitable products.\"\n\nIf you found results from the database, this is your final answer, reply to the user by announcing the number of results and returning results in this format (each new result should be on a new line):\n\nname_of_the_product (id_of_the_product)\"\n\nOnly use exact names and ids of the products returned as results when providing your final answer.\n\n\nUser prompt:\n{input}\n\n{agent_scratchpad}\n\n'''\n```\n\n----------------------------------------\n\nTITLE: Import Libraries Python\nDESCRIPTION: This snippet imports required libraries and modules, including `os`, `uuid`, `getpass`, `Counter`, and components of the Cassandra driver, OpenAI, and the datasets library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport os\nfrom uuid import uuid4\nfrom getpass import getpass\nfrom collections import Counter\n\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\n\nimport openai\nfrom datasets import load_dataset\n```\n```\n\n----------------------------------------\n\nTITLE: Create vector indexes for title and content embeddings in Tair\nDESCRIPTION: Defines index parameters and creates two separate indexes for article titles and contents, each with a specified dimension, distance metric, index type, and data type. Checks for existing indexes to avoid duplicates.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# set index parameters\nindex = \"openai_test\"\nembedding_dim = 1536\ndistance_type = \"L2\"\nindex_type = \"HNSW\"\ndata_type = \"FLOAT32\"\n\n# Create two indexes, one for title_vector and one for content_vector, skip if already exists\nindex_names = [index + \"_title_vector\", index+\"_content_vector\"]\nfor index_name in index_names:\n    index_connection = client.tvs_get_index(index_name)\n    if index_connection is not None:\n        print(\"Index already exists\")\n    else:\n        client.tvs_create_index(name=index_name, dim=embedding_dim, distance_type=distance_type,\n                                index_type=index_type, data_type=data_type)\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Specification for Atlassian Confluence API in YAML\nDESCRIPTION: This OpenAPI 3.1.0 specification describes endpoints necessary for authenticating and interacting with Confluence resources. It defines the \"getAccessibleResources\" endpoint to retrieve user-accessible resource identifiers using OAuth bearer tokens, and a \"performConfluenceSearch\" endpoint to execute Confluence Query Language (CQL) searches within specified Confluence spaces identified by the \"cloudid\". The schema details include security schemes, expected parameters, and response structures to guide implementation and integration of GPT Actions with Confluence REST APIs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_confluence.ipynb#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\\ninfo:\\n  title: Atlassian API\\n  description: This API provides access to Atlassian resources through OAuth token authentication.\\n  version: 1.0.0\\nservers:\\n  - url: https://api.atlassian.com\\n    description: Main API server\\npaths:\\n  /oauth/token/accessible-resources:\\n    get:\\n      operationId: getAccessibleResources\\n      summary: Retrieves accessible resources for the authenticated user.\\n      description: This endpoint retrieves a list of resources the authenticated user has access to, using an OAuth token.\\n      security:\\n        - bearerAuth: []\\n      responses:\\n        '200':\\n          description: A JSON array of accessible resources.\\n          content:\\n            application/json:\\n              schema: \\n                $ref: '#/components/schemas/ResourceArray'\\n  /ex/confluence/{cloudid}/wiki/rest/api/search:\\n    get:\\n      operationId: performConfluenceSearch\\n      summary: Performs a search in Confluence based on a query.\\n      description: This endpoint allows searching within Confluence using the CQL (Confluence Query Language).\\n      parameters:\\n        - in: query\\n          name: cql\\n          required: true\\n          description: The Confluence Query Language expression to evaluate.\\n          schema:\\n            type: string\\n        - in: path\\n          name: cloudid\\n          required: true\\n          schema:\\n            type: string\\n          description: The cloudid retrieved from the getAccessibleResources Action\\n        - in: query\\n          name: cqlcontext\\n          description: The context to limit the search, specified as JSON.\\n          schema:\\n            type: string\\n        - in: query\\n          name: expand\\n          description: A comma-separated list of properties to expand on the search result.\\n          schema:\\n            type: string\\n      responses:\\n        '200':\\n          description: A list of search results matching the query.\\n          content:\\n            application/json:\\n              schema:\\n                $ref: '#/components/schemas/SearchResults'\\ncomponents:\\n  securitySchemes:\\n    bearerAuth:\\n      type: http\\n      scheme: bearer\\n      bearerFormat: JWT\\n  schemas:\\n    ResourceArray:\\n      type: array\\n      items:\\n        $ref: '#/components/schemas/Resource'\\n    Resource:\\n      type: object\\n      required:\\n        - id\\n        - name\\n        - type\\n      properties:\\n        id:\\n          type: string\\n          description: The unique identifier for the resource.\\n        name:\\n          type: string\\n          description: The name of the resource.\\n        type:\\n          type: string\\n          description: The type of the resource.\\n    SearchResults:\\n      type: object\\n      properties:\\n        results:\\n          type: array\\n          items:\\n            $ref: '#/components/schemas/SearchResult'\\n    SearchResult:\\n      type: object\\n      properties:\\n        id:\\n          type: string\\n          description: The unique identifier of the content.\\n        title:\\n          type: string\\n          description: The title of the content.\\n        type:\\n          type: string\\n          description: The type of the content (e.g., page, blog post).\\n        space:\\n          type: object\\n          properties:\\n            id:\\n              type: string\\n              description: The space ID where the content is located.\\n            name:\\n              type: string\\n              description: The name of the space.\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Schema Configuration for Snowflake GPT Integration\nDESCRIPTION: OpenAPI schema definition for the Azure Function App that serves as middleware between ChatGPT and Snowflake. It defines an endpoint to execute SQL queries on Snowflake and return results as a CSV file URL for analysis by Code Interpreter.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Snowflake GPT API\n  description: API to execute SQL queries on Snowflake and get the results as a CSV file URL.\n  version: 1.0.0\nservers:\n  - url: https://<server-name>.azurewebsites.net\n    description: Azure Function App server running Snowflake integration application\npaths:\n  /api/<function_name>?code=<code>:\n    post:\n      operationId: executeSQL\n      summary: Executes a SQL query on Snowflake and returns the result file URL as a CSV.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                sql_query:\n                  type: string\n                  description: The SQL query to be executed on Snowflake.\n              required:\n                - sql_query\n      responses:\n        '200':\n          description: Successfully executed the query.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                      type: string\n                      format: uri\n                    description: Array of URLs pointing to the result files.\n        '401':\n          description: Unauthorized. Missing or invalid authentication token.\n        '400':\n          description: Bad Request. The request was invalid or cannot be otherwise served.\n        '500':\n          description: Internal Server Error. An error occurred on the server.\ncomponents:\n  schemas: {}\n```\n\n----------------------------------------\n\nTITLE: Converting List to DataFrame in Python\nDESCRIPTION: Converts a list of results to a pandas DataFrame for easier data manipulation and analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nresults_df = pd.DataFrame(results_list)\n```\n\n----------------------------------------\n\nTITLE: Downloading Precomputed Embeddings Dataset with wget - Python\nDESCRIPTION: Downloads a ~700MB zipped dataset of Wikipedia article embeddings from a remote URL using the 'wget' package. The downloaded file is intended for later extraction and database loading. Requires the 'wget' Python package and an active internet connection. Outputs progress and stores the file locally.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n\n```\n\n----------------------------------------\n\nTITLE: Creating and populating FAISS index with image embeddings\nDESCRIPTION: Initializes a FAISS index optimized for inner product similarity, then adds the extracted image embeddings to enable fast nearest neighbor search for retrieval tasks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nindex = faiss.IndexFlatIP(image_features.shape[1])\nindex.add(image_features)\n```\n\n----------------------------------------\n\nTITLE: Defining and Creating Weaviate Schema\nDESCRIPTION: Defines the schema for the \"Article\" class in Weaviate, setting the vectorizer, model, and properties including title, content, and url.  It then creates the schema in the Weaviate instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define the Schema object to use `text-embedding-3-small` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n\n# add the Article schema\nclient.schema.create_class(article_schema)\n\n# get the schema to make sure it worked\nclient.schema.get()\n```\n\n----------------------------------------\n\nTITLE: Text Splitting Utilities for Token Management in Python\nDESCRIPTION: Implements utility functions for managing text based on token counts. Includes functions to count tokens, split text at optimal points to balance token distribution, truncate text to a maximum token count, and split sections recursively to stay within token limits.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nGPT_MODEL = \"gpt-4o-mini\"  # only matters insofar as it selects which tokenizer to use\n\n\ndef num_tokens(text: str, model: str = GPT_MODEL) -> int:\n    \"\"\"Return the number of tokens in a string.\"\"\"\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n\ndef halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n    chunks = string.split(delimiter)\n    if len(chunks) == 1:\n        return [string, \"\"]  # no delimiter found\n    elif len(chunks) == 2:\n        return chunks  # no need to search for halfway point\n    else:\n        total_tokens = num_tokens(string)\n        halfway = total_tokens // 2\n        best_diff = halfway\n        for i, chunk in enumerate(chunks):\n            left = delimiter.join(chunks[: i + 1])\n            left_tokens = num_tokens(left)\n            diff = abs(halfway - left_tokens)\n            if diff >= best_diff:\n                break\n            else:\n                best_diff = diff\n        left = delimiter.join(chunks[:i])\n        right = delimiter.join(chunks[i:])\n        return [left, right]\n\n\ndef truncated_string(\n    string: str,\n    model: str,\n    max_tokens: int,\n    print_warning: bool = True,\n) -> str:\n    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n    encoding = tiktoken.encoding_for_model(model)\n    encoded_string = encoding.encode(string)\n    truncated_string = encoding.decode(encoded_string[:max_tokens])\n    if print_warning and len(encoded_string) > max_tokens:\n        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n    return truncated_string\n\n\ndef split_strings_from_subsection(\n    subsection: tuple[list[str], str],\n    max_tokens: int = 1000,\n    model: str = GPT_MODEL,\n    max_recursion: int = 5,\n) -> list[str]:\n    \"\"\"\n    Split a subsection into a list of subsections, each with no more than max_tokens.\n    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n    \"\"\"\n    titles, text = subsection\n    string = \"\\n\\n\".join(titles + [text])\n    num_tokens_in_string = num_tokens(string)\n    # if length is fine, return string\n    if num_tokens_in_string <= max_tokens:\n        return [string]\n    # if recursion hasn't found a split after X iterations, just truncate\n    elif max_recursion == 0:\n        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n    # otherwise, split in half and recurse\n    else:\n        titles, text = subsection\n        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n            left, right = halved_by_delimiter(text, delimiter=delimiter)\n            if left == \"\" or right == \"\":\n                # if either half is empty, retry with a more fine-grained delimiter\n                continue\n            else:\n                # recurse on each half\n                results = []\n                for half in [left, right]:\n                    half_subsection = (titles, half)\n                    half_strings = split_strings_from_subsection(\n                        half_subsection,\n                        max_tokens=max_tokens,\n                        model=model,\n                        max_recursion=max_recursion - 1,\n                    )\n                    results.extend(half_strings)\n                return results\n    # otherwise no split was found, so just truncate (should be very rare)\n    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n```\n\n----------------------------------------\n\nTITLE: Loading and preparing Wikipedia embeddings data with pandas in Python\nDESCRIPTION: Reads the extracted CSV file with pandas into a DataFrame. Converts vector columns (title_vector and content_vector) from JSON string format back into Python lists using json.loads. Casts vector_id to string for consistent indexing. Displays the first records for verification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv(\"../../data/vector_database_wikipedia_articles_embedded.csv\")\n\n# Read vectors from strings back into a list using json.loads\narticle_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\narticle_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\narticle_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Requesting Paper Summary via Chat Function in Python\nDESCRIPTION: Adds a subsequent user message requesting the AI to read and summarize a specific PPO paper. It then calls `chat_completion_with_function_execution` again with the updated conversation history and the same `arxiv_functions`. The comment suggests this interaction is designed to trigger a different tool or function. The final response from the AI is displayed using Markdown.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Add another user message to induce our system to use the second tool\npaper_conversation.add_message(\n    \"user\",\n    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n)\nupdated_response = chat_completion_with_function_execution(\n    paper_conversation.conversation_history, functions=arxiv_functions\n)\ndisplay(Markdown(updated_response.choices[0].message.content))\n```\n\n----------------------------------------\n\nTITLE: Applying Prompt Generation Function to DataFrame in Python\nDESCRIPTION: Applies the `get_few_shot_prompt` function to each row of the `train_sample` DataFrame using `progress_apply` (which leverages `tqdm` for a progress bar). The results (the generated few-shot prompts) are stored in a new column named 'few_shot_prompt' within the DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# train_sample is assumed to be a pre-existing Pandas DataFrame\n# â° Time: 2 min\ntrain_sample[\"few_shot_prompt\"] = train_sample.progress_apply(get_few_shot_prompt, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Add Language to Configuration (TypeScript)\nDESCRIPTION: This TypeScript snippet shows how to add a new language to the `languageConfigs` array. This array is used to configure the translation application for different languages. Each language configuration includes a language code and the corresponding translation instructions.  It allows the speaker app to access all relevant instructions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst languageConfigs = [\n  // ... existing languages ...\n  { code: 'hi', instructions: hindi_instructions },\n];\n```\n\n----------------------------------------\n\nTITLE: Importing Supabase Client in Deno or Edge Functions from CDN\nDESCRIPTION: Imports the createClient function from the Supabase JavaScript client via a CDN (esm.sh) for use in Deno or Edge Functions. No local npm installation is required. Adjust the version number as necessary to align with desired features.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createClient } from \"https://esm.sh/@supabase/supabase-js@2\";\n```\n\n----------------------------------------\n\nTITLE: Creating conversation examples for fine-tuning\nDESCRIPTION: Defines functions to format each recipe into a system-user-assistant conversation format suitable for chat model fine-tuning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsystem_message = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"\n\n\ndef create_user_message(row):\n    return f\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"\n\n\ndef prepare_example_conversation(row):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": create_user_message(row)},\n            {\"role\": \"assistant\", \"content\": row[\"NER\"]},\n        ]\n    }\n\n\npprint(prepare_example_conversation(recipe_df.iloc[0]))\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI Model and Checking Output\nDESCRIPTION: This snippet calls the `call_model` function with the 'gpt-4o' model and a generated prompt based on a sample row from a DataFrame. This is used to verify the output before processing the entire DataFrame. It requires the `generate_prompt` function to generate the prompt based on the input row and the `call_model` function to interact with the OpenAI model. The expected output is the answer from the OpenAI model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nanswer = call_model('gpt-4o', generate_prompt(df_france_subset.iloc[0], varieties))\nanswer\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a Prompt Template for Quote Generation with OpenAI in Python\nDESCRIPTION: Defines a Python multiline string template used to prompt an OpenAI language model to generate a single short philosophical quote on a specified topic. The prompt instructs the model to create a new quote within 20-30 words inspired by example quotes provided dynamically. Inputs for formatting are the topic string and a list of example quotes. There are no external dependencies beyond standard Python string formatting. This template is used as part of the quote generation workflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Generate a single short philosophical quote on the given topic,\\nsimilar in spirit and form to the provided actual example quotes.\\nDo not exceed 20-30 words in your quote.\\n\\nREFERENCE TOPIC: \\\"{topic}\\\"\\n\\nACTUAL EXAMPLES:\\n{examples}\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Summary Evaluation Prompt\nDESCRIPTION: Defines a detailed prompt for evaluating the quality of news article summaries, with specific criteria including categorization, keyword extraction, sentiment analysis, clarity, and completeness.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nevaluation_prompt = \"\"\"\nYou are an expert editor tasked with evaluating the quality of a news article summary. Below is the original article and the summary to be evaluated:\n\n**Original Article**:  \n{original_article}\n\n**Summary**:  \n{summary}\n\nPlease evaluate the summary based on the following criteria, using a scale of 1 to 5 (1 being the lowest and 5 being the highest). Be critical in your evaluation and only give high scores for exceptional summaries:\n\n1. **Categorization and Context**: Does the summary clearly identify the type or category of news (e.g., Politics, Technology, Sports) and provide appropriate context?  \n2. **Keyword and Tag Extraction**: Does the summary include relevant keywords or tags that accurately capture the main topics and themes of the article?  \n3. **Sentiment Analysis**: Does the summary accurately identify the overall sentiment of the article and provide a clear, well-supported explanation for this sentiment?  \n4. **Clarity and Structure**: Is the summary clear, well-organized, and structured in a way that makes it easy to understand the main points?  \n5. **Detail and Completeness**: Does the summary provide a detailed account that includes all necessary components (type of news, tags, sentiment) comprehensively?  \n\n\nProvide your scores and justifications for each criterion, ensuring a rigorous and detailed evaluation.\n\"\"\"\n\nclass ScoreCard(BaseModel):\n    justification: str\n    categorization: int\n    keyword_extraction: int\n    sentiment_analysis: int\n    clarity_structure: int\n    detail_completeness: int\n```\n\n----------------------------------------\n\nTITLE: Splitting Text into Chunks\nDESCRIPTION: This code defines a function `create_chunks` that splits the input text into smaller chunks based on token count, trying to end at the end of a sentence. It utilizes a tokenizer from the `tiktoken` library. The function `extract_chunk` takes a document chunk and a template prompt, combines them to create a prompt for GPT-4, and returns the extracted information.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\ndef create_chunks(text, n, tokenizer):\n    tokens = tokenizer.encode(text)\n    \"\"\"Yield successive n-sized chunks from text.\"\"\"\n    i = 0\n    while i < len(tokens):\n        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n        j = min(i + int(1.5 * n), len(tokens))\n        while j > i + int(0.5 * n):\n            # Decode the tokens and check for full stop or newline\n            chunk = tokenizer.decode(tokens[i:j])\n            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n                break\n            j -= 1\n        # If no end of sentence found, use n tokens as the chunk size\n        if j == i + int(0.5 * n):\n            j = min(i + n, len(tokens))\n        yield tokens[i:j]\n        i = j\n\ndef extract_chunk(document,template_prompt):\n    prompt = template_prompt.replace('<document>',document)\n\n    messages = [\n            {\"role\": \"system\", \"content\": \"You help extract information from documents.\"},\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n\n    response = client.chat.completions.create(\n            model='gpt-4', \n            messages=messages,\n            temperature=0,\n            max_tokens=1500,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0\n        )\n    return \"1.\" + response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Using the Web Search Tool with Responses API\nDESCRIPTION: Demonstrates how to use the built-in web_search tool to incorporate web search results into the response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.responses.create(\n    model=\"gpt-4o\",  # or another supported model\n    input=\"What's the latest news about AI?\",\n    tools=[\n        {\n            \"type\": \"web_search\"\n        }\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Agent Turn with Handoff Logic (Python)\nDESCRIPTION: This function implements the core logic for processing a single turn of the agent conversation. It generates an OpenAI chat completion response, handles tool calls, executes the corresponding Python functions, and checks if a function returns an Agent object to perform a handoff by updating the 'current_agent'. It returns a Response object containing the final agent and messages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndef run_full_turn(agent, messages):\n\n    current_agent = agent\n    num_init_messages = len(messages)\n    messages = messages.copy()\n\n    while True:\n\n        # turn python functions into tools and save a reverse map\n        tool_schemas = [function_to_schema(tool) for tool in current_agent.tools]\n        tools = {tool.__name__: tool for tool in current_agent.tools}\n\n        # === 1. get openai completion ===\n        response = client.chat.completions.create(\n            model=agent.model,\n            messages=[{\"role\": \"system\", \"content\": current_agent.instructions}]\n            + messages,\n            tools=tool_schemas or None,\n        )\n        message = response.choices[0].message\n        messages.append(message)\n\n        if message.content:  # print agent response\n            print(f\"{current_agent.name}:\", message.content)\n\n        if not message.tool_calls:  # if finished handling tool calls, break\n            break\n\n        # === 2. handle tool calls ===\n\n        for tool_call in message.tool_calls:\n            result = execute_tool_call(tool_call, tools, current_agent.name)\n\n            if type(result) is Agent:  # if agent transfer, update current agent\n                current_agent = result\n                result = (\n                    f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n\n                )\n\n            result_message = {\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call.id,\n                \"content\": result,\n            }\n            messages.append(result_message)\n\n    # ==== 3. return last agent used and new messages =====\n    return Response(agent=current_agent, messages=messages[num_init_messages:])\n\n\ndef execute_tool_call(tool_call, tools, agent_name):\n    name = tool_call.function.name\n    args = json.loads(tool_call.function.arguments)\n\n    print(f\"{agent_name}:\", f\"{name}({args})\")\n\n    return tools[name](**args)  # call corresponding function with provided arguments\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Client and Settings (Python)\nDESCRIPTION: Configures the OpenAI client for API interaction. It retrieves the OpenAI API key from the `OPENAI_API_KEY` environment variable or uses a placeholder string (which must be replaced). It then initializes the `OpenAI` client instance with the obtained key and specifies the `text-embedding-3-small` model as the default for generating embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\") # Saving this as a variable to reference in function app in later step\nopenai_client = OpenAI(api_key=openai_api_key)\nembeddings_model = \"text-embedding-3-small\" # We'll use this by default, but you can change to your text-embedding-3-large if desired\n```\n\n----------------------------------------\n\nTITLE: Analyzing Training Accuracy Results\nDESCRIPTION: Loading the result file and examining the training accuracy to evaluate model performance during training.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults = pd.read_csv('result.csv')\nresults[results['train_accuracy'].notnull()].tail(1)\n```\n\n----------------------------------------\n\nTITLE: Generating Histograms of Tokens\nDESCRIPTION: This Python code reads a CSV file containing Wikipedia sections and generates a histogram visualizing the distribution of the number of tokens within those sections.  It uses the `pandas` library to read the CSV file, the `matplotlib` library to create the histogram. The histogram helps understand the length distribution of the sections, which is useful for further processing and model training.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('olympics-data/olympics_sections.csv')\ndf[['tokens']].hist()\n# add axis descriptions and title\nplt.xlabel('Number of tokens')\nplt.ylabel('Number of Wikipedia sections')\nplt.title('Distribution of number of tokens in Wikipedia sections')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Installing necessary packages using pip\nDESCRIPTION: This code snippet installs required Python packages such as matplotlib, plotly, scikit-learn, tabulate, tiktoken, and wget using pip. These packages are used for data manipulation, visualization, and interacting with the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!pip install matplotlib plotly.express scikit-learn tabulate tiktoken wget --quiet\n```\n\n----------------------------------------\n\nTITLE: Processing Files Concurrently and CSV Writing\nDESCRIPTION: This code processes files concurrently, generates embeddings and writes the results to a CSV file. It uses `process_file` to process files, `concurrent.futures.ThreadPoolExecutor` for parallel processing, and `csv.DictWriter` to write the data into a CSV file.  The data includes 'id', 'vector_id', 'title', 'text', 'title_vector', 'content_vector' and 'category'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n## Customize the location below if you are using different data besides the OpenAI documentation. Note that if you are using a different dataset, you will need to update the categories list as well.\nfolder_name = \"../../../data/oai_docs\"\n\nfiles = [os.path.join(folder_name, f) for f in os.listdir(folder_name) if f.endswith('.txt') or f.endswith('.pdf')]\ndata = []\n\n# Process each file concurrently\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    futures = {executor.submit(process_file, file_path, idx, categories, embeddings_model): idx for idx, file_path in enumerate(files)}\n    for future in concurrent.futures.as_completed(futures):\n        try:\n            result = future.result()\n            data.extend(result)\n        except Exception as e:\n            print(f\"Error processing file: {str(e)}\")\n\n# Write the data to a CSV file\ncsv_file = os.path.join(\"..\", \"embedded_data.csv\")\nwith open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = [\"id\", \"vector_id\", \"title\", \"text\", \"title_vector\", \"content_vector\",\"category\"]\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n        print(f\"Wrote row with id {row['id']} to CSV\")\n```\n\n----------------------------------------\n\nTITLE: Concurrent File Processing and CSV Writing in Python\nDESCRIPTION: This code snippet processes multiple files concurrently using a ThreadPoolExecutor, writes the processed data to a CSV file, and then reads the CSV file into a Pandas DataFrame. It uses helper functions like `process_file` to extract and process the data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n## Customize the location below if you are using different data besides the OpenAI documentation. Note that if you are using a different dataset, you will need to update the categories list as well.\nfolder_name = \"../../../data/oai_docs\"\n\nfiles = [os.path.join(folder_name, f) for f in os.listdir(folder_name) if f.endswith('.txt') or f.endswith('.pdf')]\ndata = []\n\n# Process each file concurrently\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    futures = {executor.submit(process_file, file_path, idx, categories, embeddings_model): idx for idx, file_path in enumerate(files)}\n    for future in concurrent.futures.as_completed(futures):\n        try:\n            result = future.result()\n            data.extend(result)\n        except Exception as e:\n            print(f\"Error processing file: {str(e)}\")\n\n# Write the data to a CSV file\ncsv_file = os.path.join(\"..\", \"embedded_data.csv\")\nwith open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = [\"id\", \"vector_id\", \"title\", \"text\", \"title_vector\", \"content_vector\",\"category\"]\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n        print(f\"Wrote row with id {row['id']} to CSV\")\n\n# Convert the CSV file to a Dataframe\narticle_df = pd.read_csv(\"../embedded_data.csv\")\n# Read vectors from strings back into a list using json.loads\narticle_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\narticle_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\narticle_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\narticle_df[\"category\"] = article_df[\"category\"].apply(str)\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Batch Processing\nDESCRIPTION: Configures Weaviate's batch processing settings to optimize import operations. This includes setting the batch size to 100, enabling dynamic batching, and configuring timeout retries to handle potential errors during the import process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n### Step 1 - configure Weaviate Batch, which optimizes CRUD operations in bulk\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=100,\n    dynamic=True,\n    timeout_retries=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Convert Dataset to DataFrame and Display Head (Python)\nDESCRIPTION: Converts the 'train' split of the loaded Hugging Face dataset into a pandas DataFrame for easier manipulation and viewing. It then displays the first few rows of the DataFrame to show the structure and content of the data being used (natural language questions, corresponding SQL answers, and CREATE context).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsql_df = dataset['train'].to_pandas()\nsql_df.head()\n```\n\n----------------------------------------\n\nTITLE: Hybrid Query for Shirts with Specific Title Constraints\nDESCRIPTION: A hybrid search query for \"shirt\" that only includes results containing the phrase \"slim fit\" in the product display name, demonstrating how to filter vector search results by text content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# hybrid query for shirt in the product vector and only include results with the phrase \"slim fit\" in the title\nresults = search_redis(redis_client,\n                       \"shirt\",\n                       vector_field=\"product_vector\",\n                       k=10,\n                       hybrid_fields='@productDisplayName:\"slim fit\"'\n                       )\n```\n\n----------------------------------------\n\nTITLE: Reading PDF Content\nDESCRIPTION: This code defines a function `read_pdf` that extracts text from a PDF file.  It takes a filepath as input and returns a string containing the concatenated text from all pages in the PDF. The function utilizes the `PyPDF2` library to read and extract text from the PDF file, additionally, it keeps track of and includes the page number for each section of text. It also handles potential errors during file processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef read_pdf(filepath):\n    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n    # creating a pdf reader object\n    reader = PdfReader(filepath)\n    pdf_text = \"\"\n    page_number = 0\n    for page in reader.pages:\n        page_number += 1\n        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n    return pdf_text\n```\n\n----------------------------------------\n\nTITLE: Extracting Content from Chat Completion Response\nDESCRIPTION: Demonstrates how to extract the reply and its content from a standard chat completion response. This accesses different parts of the response object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nreply = response.choices[0].message\nprint(f\"Extracted reply: \\n{reply}\")\n\nreply_content = response.choices[0].message.content\nprint(f\"Extracted content: \\n{reply_content}\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Tool Calling Training Data for Fine-Tuning OpenAI Chat Models (JSON Format)\nDESCRIPTION: This snippet illustrates structuring a fine-tuning example in JSON to enable accurate tool calling. Each data record includes a 'messages' field with user/assistant/tool exchanges and a 'tools' array describing the function, arguments schema, and other tool properties. Useful for training models to output well-formed tool call responses without including the entire tool definition during inference. Ensure all roles and tool definitions follow OpenAI's chat and tools API conventions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"What is the weather in San Francisco?\" },\n        {\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"call_id\",\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": \"get_current_weather\",\n                        \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and country, eg. San Francisco, USA\"\n                        },\n                        \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n                    },\n                    \"required\": [\"location\", \"format\"]\n                }\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Extracted CSV Dataset Into Pandas DataFrame in Python\nDESCRIPTION: Loads the Wikipedia embeddings CSV file into a Pandas DataFrame for convenient manipulation and further indexing into Elasticsearch. Assumes the file 'data/vector_database_wikipedia_articles_embedded.csv' exists. Output is a variable 'wikipedia_dataframe' containing all tabular data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nwikipedia_dataframe = pd.read_csv(\"data/vector_database_wikipedia_articles_embedded.csv\")\n```\n\n----------------------------------------\n\nTITLE: Setting Hyperparameters for OpenAI Fine-Tuning (Python/Node.js)\nDESCRIPTION: These snippets demonstrate how to specify hyperparameters when creating an OpenAI fine-tuning job using the Python and Node.js client libraries. It shows initializing the client and calling the `fine_tuning.jobs.create` method, passing the training file ID, base model name, and a dictionary/object containing the desired hyperparameter settings (e.g., `n_epochs`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nclient.fine_tuning.jobs.create(\n  training_file=\"file-abc123\", \n  model=\"gpt-3.5-turbo\", \n  hyperparameters={\n    \"n_epochs\":2\n  }\n)\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst fineTune = await openai.fineTuning.jobs.create({training_file: \"file-abc123\", model: \"gpt-3.5-turbo\", hyperparameters: { n_epochs: 2 }});\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Displaying Test Article Data\nDESCRIPTION: Retrieves a single article from the Weaviate database, including its title, content, and ID, to verify the successful import and storage of the data.  The fetched article's ID, title, and content are then printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"content\", \"_additional {id}\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article[\"_additional\"][\"id\"])\nprint(test_article[\"title\"])\nprint(test_article[\"content\"])\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAPI Schema for Azure Function Endpoint in Python\nDESCRIPTION: An example OpenAPI schema template for connecting an Azure Function App to ChatGPT. The schema defines the server URL and endpoint path structure, requiring customization with specific application details, function name, and endpoint ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: {insert title}\n  description: {insert description}\n  version: 1.0.0\nservers:\n  - url: https://{your_function_app_name}.azurewebsites.net/api\n    description: {insert description}\npaths:\n  /{your_function_name}?code={enter your specific endpoint id here}:\n    post:\n      operationId: {insert operationID}\n      summary: {insert summary}\n      requestBody: \n{the rest of this is specific to your application}\n```\n\n----------------------------------------\n\nTITLE: Describing SharePoint Search OpenAPI Specification for Custom GPT Integration - YAML\nDESCRIPTION: Outlines the OpenAPI 3.1 schema definition for an HTTP API that allows GPT actions to search SharePoint/O365 documents by posting a searchTerm, returning matching files in base64-encoded format. Specifies endpoints, expected parameters, payload structures, success and error responses, and usage guidelines. This YAML is intended to be entered in an API configuration system, not run directly. Dependencies: OpenAPI compatible API framework. Inputs: searchTerm (string). Outputs: JSON response including file details (name, mime_type, base64-encoded content). Limitations: Endpoint URLs must be customized for app/function, and payload/timeout limits of OpenAI Actions apply.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: SharePoint Search API\n  description: API for searching SharePoint documents.\n  version: 1.0.0\nservers:\n  - url: https://{your_function_app_name}.azurewebsites.net/api\n    description: SharePoint Search API server\npaths:\n  /{your_function_name}?code={enter your specific endpoint id here}:\n    post:\n      operationId: searchSharePoint\n      summary: Searches SharePoint for documents matching a query and term.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                searchTerm:\n                  type: string\n                  description: A specific term to search for within the documents.\n      responses:\n        '200':\n          description: A CSV file of query results encoded in base64.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponseData:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                          description: The name of the file.\n                        mime_type:\n                          type: string\n                          description: The MIME type of the file.\n                        content:\n                          type: string\n                          format: byte\n                          description: The base64 encoded contents of the file.\n        '400':\n          description: Bad request when the SQL query parameter is missing.\n        '413':\n          description: Payload too large if the response exceeds the size limit.\n        '500':\n          description: Server error when there are issues executing the query or encoding the results.\n```\n\n----------------------------------------\n\nTITLE: Searching Partitioned Astra DB Table by Vector and Filters (Python/CQL)\nDESCRIPTION: Defines a Python function `find_quote_and_author_p` that performs a vector similarity search on the partitioned table, optionally applying filters for `author` and `tags` by dynamically constructing the CQL WHERE clause, and then executing the query using the session object to retrieve matching results sorted by vector distance. Requires `client`, `embedding_model_name`, `keyspace`, and `session`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\ndef find_quote_and_author_p(query_quote, n, author=None, tags=None):\n    query_vector = client.embeddings.create(\n        input=[query_quote],\n        model=embedding_model_name,\n    ).data[0].embedding\n    # Depending on what conditions are passed, the WHERE clause in the statement may vary.\n    # Construct it accordingly:\n    where_clauses = []\n    where_values = []\n    if author:\n        where_clauses += [\"author = %s\"]\n        where_values += [author]\n    if tags:\n        for tag in tags:\n            where_clauses += [\"tags CONTAINS %s\"]\n            where_values += [tag]\n    if where_clauses:\n        search_statement = f\"\"\"SELECT body, author FROM {keyspace}.philosophers_cql_partitioned\n            WHERE {' AND '.join(where_clauses)}\n            ORDER BY embedding_vector ANN OF %s\n            LIMIT %s;\n        \"\"\"\n    else:\n        search_statement = f\"\"\"SELECT body, author FROM {keyspace}.philosophers_cql_partitioned\n            ORDER BY embedding_vector ANN OF %s\n            LIMIT %s;\n        \"\"\"\n    query_values = tuple(where_values + [query_vector] + [n])\n    result_rows = session.execute(search_statement, query_values)\n    return [\n        (result_row.body, result_row.author)\n        for result_row in result_rows\n    ]\n```\n\n----------------------------------------\n\nTITLE: Parsing GPT Model Output for Cluster-Topic Mapping and New Topics in Python\nDESCRIPTION: This snippet processes the strictly formatted string output received from the GPT model by splitting it into two parts: cluster topic mappings and new topic suggestions. It parses each line to extract integer cluster identifiers and associated topic strings and creates JSON-like Python lists for programmatic use. This enables subsequent use of structured data for generating synthetic training examples. The snippet assumes the variable 'res' holds the string response exactly following the specified prompt format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nparts = res.split(\"\\n\\n\")\ncluster_mapping_part = parts[0]\nnew_topics_part = parts[1]\n\n# Parse cluster topic mapping\ncluster_topic_mapping_lines = cluster_mapping_part.split(\"\\n\")[1:]  # Skip the first two lines\ncluster_topic_mapping = [{\"cluster\": int(line.split(\",\")[0].split(\":\")[1].strip()), \"topic\": line.split(\":\")[2].strip()} for line in cluster_topic_mapping_lines]\n\n# Parse new topics\nnew_topics_lines = new_topics_part.split(\"\\n\")[1:]  # Skip the first line\nnew_topics = [line.split(\". \")[1] for line in new_topics_lines]\n\ncluster_topic_mapping, new_topics\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Generative Search Query in Weaviate Python Client\nDESCRIPTION: Illustrates how to construct a semantic search query in Weaviate for the 'Articles' class, retrieving fields such as 'title', 'content', and 'url', filtering by concept using near_text, and limiting results to five objects. The snippet demonstrates typical query chaining in Weaviate's Python SDK. Users must insert the generative transformation with with_generate for generative results and adjust class/field names to match their schema. No generative prompt is included in this demonstration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    result = (\n        client.query\n        .get(\"Articles\", [\"title\", \"content\", \"url\"])\n        .with_near_text(\"concepts\": \"football clubs\")\n        .with_limit(5)\n        # generative query will go here\n        .do()\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Retriever\nDESCRIPTION: Creates a retriever from the vector index. `similarity_top_k=2` indicates to return the top 2 similar nodes for each query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nretriever = vector_index.as_retriever(similarity_top_k=2)\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Entity Extraction Function with Example Queries in Python\nDESCRIPTION: Creates a list of example user queries (`example_queries`). It then iterates through this list, calling the `define_query` function for each query. The original query and the corresponding JSON output (containing extracted entities) returned by the LLM are printed for each example, demonstrating the entity extraction process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nexample_queries = [\n    \"Which pink items are suitable for children?\",\n    \"Help me find gardening gear that is waterproof\",\n    \"I'm looking for a bench with dimensions 100x50 for my living room\"\n]\n\nfor q in example_queries:\n    print(f\"Q: '{q}'\\n{define_query(q)}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Embeddings in 2D Using t-SNE\nDESCRIPTION: Prepares embeddings data for visualization by reducing dimensionality with t-SNE. The code loads embeddings from a CSV file and extracts them into a matrix format suitable for t-SNE processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\nmatrix = df.ada_embedding.apply(eval).to_list()\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Dataset - Python\nDESCRIPTION: This code transforms the raw OpenAI API responses into a format suitable for the OpenAI Evals framework.  It parses the generated text to extract questions and answers, and then constructs a list of dictionaries, where each dictionary contains the input (system prompt and question) and the ideal answer. This dataset can then be used to assess the accuracy and performance of the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\neval_data = []\ninput_prompt = \"TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [*,Id,Maker,FullName,Country]\\nTable car_names, columns = [*,MakeId,Model,Make]\\nTable cars_data, columns = [*,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [*,ContId,Continent]\\nTable countries, columns = [*,CountryId,CountryName,Continent]\\nTable model_list, columns = [*,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\"\n\nfor choice in completion.choices:\n    question = choice.message.content.split(\"Q: \")[1].split(\"\\n\")[0]  # Extracting the question\n    answer = choice.message.content.split(\"\\nA: \")[1].split(\"\\n\")[0]  # Extracting the answer\n    eval_data.append({\n        \"input\": [\n            {\"role\": \"system\", \"content\": input_prompt},\n            {\"role\": \"user\", \"content\": question},\n        ],\n        \"ideal\": answer\n    })\n\nfor item in eval_data:\n    print(item)\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Tool Call Data from Assistant Run in Python\nDESCRIPTION: This snippet extracts the first tool call from the required actions of a run object, then parses the function name and JSON encoded arguments. It assumes the presence of a run object with a nested structure containing tool calls. The arguments are decoded from a JSON string to a Python dictionary. This form of extraction is useful for processing actions returned by an AI assistant framework.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntool_call = run.required_action.submit_tool_outputs.tool_calls[0]\nname = tool_call.function.name\narguments = json.loads(tool_call.function.arguments)\n\nprint(\"Function Name:\", name)\nprint(\"Function Arguments:\")\narguments\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for News API Key - Python\nDESCRIPTION: This snippet sets the NEWS_API_KEY environment variable for authenticating requests to the News API using IPython magic syntax. Ensure you have your News API key before running. The environment variable will be accessible via os.environ for use by subsequent code snippets and dependencies.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n%env NEWS_API_KEY = YOUR_NEWS_API_KEY\n\n```\n\n----------------------------------------\n\nTITLE: Generating Philosophical Quotes Using OpenAI Chat Completion in Python\nDESCRIPTION: Defines a Python function to generate a new philosophical quote on a given topic by first retrieving example quotes from the vector store via the previously defined search function. It then formats a prompt from a template with the retrieved examples and invokes the OpenAI chat completion API to generate a new quote with controlled temperature and token limits. The function logs retrieved quotes for transparency and returns the generated text or None if no examples are found. Dependencies include an OpenAI client with chat completions support, a prompt template string, and the find_quote_and_author search function. Inputs are the topic string, number of examples to retrieve, and optional author and tags filters. Outputs are the generated quote string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_quote(topic, n=2, author=None, tags=None):\n    quotes = find_quote_and_author(query_quote=topic, n=n, author=author, tags=tags)\n    if quotes:\n        prompt = generation_prompt_template.format(\n            topic=topic,\n            examples=\"\\n\".join(f\"  - {quote[0]}\" for quote in quotes),\n        )\n        # a little logging:\n        print(\"** quotes found:\")\n        for q, a in quotes:\n            print(f\"**    - {q} ({a})\")\n        print(\"** end of logging\")\n        #\n        response = client.chat.completions.create(\n            model=completion_model_name,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7,\n            max_tokens=320,\n        )\n        return response.choices[0].message.content.replace('\"', '').strip()\n    else:\n        print(\"** no quotes found.\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Testing arXiv Search\nDESCRIPTION: This code snippet calls the `get_articles` function with the query \"ppo reinforcement learning\" and prints the first result.  This serves as a test to ensure that the search functionality is working correctly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Test that the search is working\nresult_output = get_articles(\"ppo reinforcement learning\")\nresult_output[0]\n```\n\n----------------------------------------\n\nTITLE: Performing Title-Based Similarity Search\nDESCRIPTION: Performs a similarity search using the title vector to find articles related to \"modern art in Europe\" and displays the results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nquery_results = query_polardb(\"modern art in Europe\", \"Articles\")\nfor i, result in enumerate(query_results):\n    print(f\"{i + 1}. {result[2]} (Score: {round(1 - result[3], 3)})\")\n```\n\n----------------------------------------\n\nTITLE: Generating Answers Based on Context - Python\nDESCRIPTION: This code defines a function `get_answers` that uses the OpenAI API to generate answers based on the context and questions from the given row using the `davinci-instruct-beta-v3` model.  It constructs a prompt instructing the model to write answers and sets various parameters like temperature and max_tokens. The generated answers are then added as a new column in the dataframe, and missing values are dropped.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_answers(row):\n    try:\n        response = client.chat.completions.create(\n            engine=\"davinci-instruct-beta-v3\",\n            prompt=f\"Write answer based on the text below\\n\\nText: {row.context}\\n\\nQuestions:\\n{row.questions}\\n\\nAnswers:\\n1.\",\n            temperature=0,\n            max_tokens=257,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0\n        )\n        return response.choices[0].text\n    except Exception as e:\n        print (e)\n        return \"\"\n\ndf['answers']= df.apply(get_answers, axis=1)\ndf['answers'] = \"1.\" + df.answers\ndf = df.dropna().reset_index().drop('index',axis=1)\nprint(df[['answers']].values[0][0])\n```\n\n----------------------------------------\n\nTITLE: Combined Query Rewriting and Retrieval Check Prompt (example-chat)\nDESCRIPTION: An optimized LLM prompt that combines the tasks of rewriting the user query for context and determining if retrieval is needed into a single call. It outputs a JSON object containing the contextualized query and the retrieval decision ('true' or 'false').\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_4\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: Given the previous conversation, re-write the last user query so it contains\nall necessary context. Then, determine whether the full request requires doing a\nrealtime lookup to respond to.\n\nRespond in the following form:\n{\nquery:\"[contextualized query]\",\nretrieval:\"[true/false - whether retrieval is required]\"\n}\n\n# Examples\n\nHistory: [{user: \"What is your return policy?\"},{assistant: \"...\"}]\nUser Query: \"How long does it cover?\"\nResponse: {query: \"How long does the return policy cover?\", retrieval: \"true\"}\n\nHistory: [{user: \"How can I return this item after 30 days?\"},{assistant: \"...\"}]\nUser Query: \"Thank you!\"\nResponse: {query: \"Thank you!\", retrieval: \"false\"}\n\n# Conversation\n[last 3 messages of conversation]\n\n# User Query\n[last user query]\n\nUSER: [JSON-formatted input conversation here]\n```\n\n----------------------------------------\n\nTITLE: Generating Rejection Prompts\nDESCRIPTION: This code defines a list of prompts that are similar to the existing functions but should result in a call to the `reject_request` function. It then formats these prompts into a fine-tuning-compatible format, similar to the other training examples. This is included to reduce the likelihood of incorrect function calls being generated by the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nreject_list = [\n    \"Translate broadcast message to another language\",\n    \"Automatically capture photos when face is detected\",\n    \"Detect nearby drones\",\n    \"Measure wind resistance\",\n    \"Capture slow motion video\",\n    \"Move the drone forward and backward by same distance at the same time.\",\n    \"Adjust drone's altitude to ground level changes\",\n    \"Display custom message on LED display\",\n    \"Sync drone's time with smartphone\",\n    \"Alert when drone travels out of designated area\",\n    \"Calibrate sensors and land simultaneously\",\n    \"Detect moisture levels\",\n    \"Automatically follow GPS tagged object\",\n    \"Toggle night vision mode\",\n    \"Maintain current altitude when battery is low\",\n    \"Decide best landing spot using AI\",\n    \"Program drone's route based on wind direction\",\n]\n\nreject_training_list = []\nfor prompt in reject_list:\n    # Adjust formatting\n    tool_calls = [\n        {\n            \"id\": \"call_id\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"reject_request\", \"arguments\": \"{}\"},\n        }\n    ]\n    reject_training_list.append(\n        {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": DRONE_SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt},\n                {\"role\": \"assistant\", \"tool_calls\": tool_calls},\n            ],\n            \"parallel_tool_calls\": False,\n            \"tools\": modified_function_list,\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting up Dependencies for RAG with OpenAI and Pinecone\nDESCRIPTION: Imports necessary libraries and initializes the OpenAI and Pinecone clients. This setup establishes the foundation for implementing a RAG system that combines OpenAI's capabilities with Pinecone's vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#%pip install datasets tqdm pandas pinecone openai --quiet\n\nimport os\nimport time\nfrom tqdm.auto import tqdm\nfrom pandas import DataFrame\nfrom datasets import load_dataset\nimport random\nimport string\n\n\n# Import OpenAI client and initialize with your API key.\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Import Pinecone client and related specifications.\nfrom pinecone import Pinecone\nfrom pinecone import ServerlessSpec\n```\n\n----------------------------------------\n\nTITLE: Converting embeddings from CSV string to list type - Python\nDESCRIPTION: This snippet converts the 'embedding' column in the Pandas DataFrame from a string representation to a list of floats using `ast.literal_eval`. This is necessary because the embeddings are stored as strings when read from the CSV file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# convert embeddings from CSV str type back to list type\ndf['embedding'] = df['embedding'].apply(ast.literal_eval)\n```\n\n----------------------------------------\n\nTITLE: Performing similarity search with optional author and tags filters\nDESCRIPTION: Defines a function to perform similarity search in the partitioned Cassandra table. It creates an embedding for the query quote, sets filters based on provided author and tags, and executes `ann_search` to retrieve the most similar quotes, leveraging partitioning for optimized performance when an author filter is applied.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndef find_quote_and_author_p(query_quote, n, author=None, tags=None):\n    query_vector = client.embeddings.create(\n        input=[query_quote],\n        model=embedding_model_name,\n    ).data[0].embedding\n    metadata = {}\n    partition_id = None\n    if author:\n        partition_id = author\n    if tags:\n        for tag in tags:\n            metadata[tag] = True\n    # Perform similarity search with partition and metadata filters\n    results = v_table_partitioned.ann_search(\n        query_vector,\n        n=n,\n        partition_id=partition_id,\n        metadata=metadata,\n    )\n    return [\n        (result[\"body_blob\"], result[\"partition_id\"])\n        for result in results\n    ]\n```\n\n----------------------------------------\n\nTITLE: Optimized Reasoning Prompt Part 2 (GPT-4 - example-chat)\nDESCRIPTION: The second part of a split reasoning prompt, intended for GPT-4. It takes the JSON output from the first reasoning step (GPT-3.5) and the retrieved context to determine if enough information is present and generate the final user-facing response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_7\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: You are a helpful customer service bot.\n\nUse the retrieved context, as well as these pre-classified fields, to respond to\nthe user's query.\n\n# Reasoning Fields\n` ` `\n[reasoning json determined in previous GPT-3.5 call]\n` ` `\n\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with OpenAI Node.js Library\nDESCRIPTION: Example Node.js script showing how to generate text embeddings using the `text-embedding-ada-002` model via the OpenAI library. It initializes the client, sends a request to the embeddings endpoint with input text, and logs the resulting embedding object. Requires the OpenAI library installed and the API key configured via environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/node-setup.txt#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const embedding = await openai.embeddings.create({\n    model: \"text-embedding-ada-002\",\n    input: \"The quick brown fox jumped over the lazy dog\",\n  });\n\n  console.log(embedding);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Building List of Result Documents from Search - Python\nDESCRIPTION: Iterates through arXiv search results and creates a list of dictionaries, each holding the title, summary, and URLs (article and PDF) of a document. Dependencies are arxiv and the prior search object. The result is a list suitable for ranking or further inspection. Assumes at least two links per document; adjust if arXiv data structure changes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult_list = []\n\nfor result in search.results():\n    result_dict = {}\n\n    result_dict.update({\"title\": result.title})\n    result_dict.update({\"summary\": result.summary})\n\n    # Taking the first url provided\n    result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n    result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n    result_list.append(result_dict)\n```\n\n----------------------------------------\n\nTITLE: Uploading a file to an S3 bucket\nDESCRIPTION: This snippet demonstrates uploading a local file to a specific S3 bucket using the `run_conversation` function. The `local_file` variable specifies the path to the local file, and the `bucket_name` variable specifies the destination bucket.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlocal_file = '<file_name>'\nbucket_name = '<bucket_name>'\nprint(run_conversation(f'upload {local_file} to {bucket_name} bucket'))\n```\n\n----------------------------------------\n\nTITLE: Applying Matrix Transformation to Embeddings in Python\nDESCRIPTION: Defines two functions: `embedding_multiplied_by_matrix` multiplies a single embedding vector by a given matrix using PyTorch, returning a NumPy array. `apply_matrix_to_embeddings_dataframe` applies this transformation to specified embedding columns (`text_1_embedding`, `text_2_embedding`) within a Pandas DataFrame, creating new columns (`_custom`), and recalculates cosine similarity based on these transformed embeddings. Depends on `torch`, `numpy`, `pandas`, and a `cosine_similarity` function (assumed to be defined elsewhere).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef embedding_multiplied_by_matrix(\n    embedding: List[float], matrix: torch.tensor\n) -> np.array:\n    embedding_tensor = torch.tensor(embedding).float()\n    modified_embedding = embedding_tensor @ matrix\n    modified_embedding = modified_embedding.detach().numpy()\n    return modified_embedding\n\n\n# compute custom embeddings and new cosine similarities\ndef apply_matrix_to_embeddings_dataframe(matrix: torch.tensor, df: pd.DataFrame):\n    for column in [\"text_1_embedding\", \"text_2_embedding\"]:\n        df[f\"{column}_custom\"] = df[column].apply(\n            lambda x: embedding_multiplied_by_matrix(x, matrix)\n        )\n    df[\"cosine_similarity_custom\"] = df.apply(\n        lambda row: cosine_similarity(\n            row[\"text_1_embedding_custom\"], row[\"text_2_embedding_custom\"]\n        ),\n        axis=1,\n    )\n```\n\n----------------------------------------\n\nTITLE: Evaluating Summaries and Storing Results\nDESCRIPTION: Iterates through each evaluation metric and summary, calls the `get_geval_score` function to obtain a score, and stores the results in a dictionary. The code assumes the existence of `eval_summary_1`, `eval_summary_2`, and `excerpt` variables, as well as the `evaluation_metrics` dictionary defined previously. The results are structured for subsequent analysis using pandas.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsummaries = {\"Summary 1\": eval_summary_1, \"Summary 2\": eval_summary_2}\n\ndata = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n\nfor eval_type, (criteria, steps) in evaluation_metrics.items():\n    for summ_type, summary in summaries.items():\n        data[\"Evaluation Type\"].append(eval_type)\n        data[\"Summary Type\"].append(summ_type)\n        result = get_geval_score(criteria, steps, excerpt, summary, eval_type)\n        score_num = int(result.strip())\n        data[\"Score\"].append(score_num)\n```\n\n----------------------------------------\n\nTITLE: Processing DataFrame with OpenAI Models\nDESCRIPTION: This code applies the `process_dataframe` function to a subset of a Pandas DataFrame (`df_france_subset`) using two different OpenAI models: 'gpt-4o' and 'gpt-4o-mini'. The goal is to run the dataframe processing on each model, to later compare results. It requires a Pandas DataFrame and the function `process_dataframe` defined earlier. It iterates over the dataframe using `process_dataframe` function for each specified model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\ndf_france_subset = process_dataframe(df_france_subset, \"gpt-4o\")\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\ndf_france_subset = process_dataframe(df_france_subset, \"gpt-4o-mini\")\n```\n```\n\n----------------------------------------\n\nTITLE: Estimating Token Count for OpenAI Chat Messages (Python)\nDESCRIPTION: This Python function `num_tokens_from_messages` estimates the number of tokens a list of messages will consume for a given OpenAI chat model using the `tiktoken` library. It accounts for model-specific tokenization rules and the structure of chat messages (role, content, name). Note that this is an estimate, especially for newer or changing models, and may differ slightly from the actual API count, particularly when using features like the `functions` parameter.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport tiktoken\n\n\ndef num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        print(\"Warning: model not found. Using cl100k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    if model in {\n        \"gpt-3.5-turbo-0613\",\n        \"gpt-3.5-turbo-16k-0613\",\n        \"gpt-4-0314\",\n        \"gpt-4-32k-0314\",\n        \"gpt-4-0613\",\n        \"gpt-4-32k-0613\",\n        }:\n        tokens_per_message = 3\n        tokens_per_name = 1\n    elif model == \"gpt-3.5-turbo-0301\":\n        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    elif \"gpt-3.5-turbo\" in model:\n        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n    elif \"gpt-4\" in model:\n        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n    else:\n        raise NotImplementedError(\n            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n        )\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n    return num_tokens\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI API for Product Launch Information\nDESCRIPTION: This snippet uses the OpenAI Python client to query the GPT-4o model to list recent product launches by OpenAI. It demonstrates the model's limitation due to its knowledge cutoff. The input is a user query requesting a list of OpenAI product launches. The output is the model's response, which is expected to be outdated.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nsearch_query = \"List the latest OpenAI product launches in chronological order from latest to oldest in the past 2 years\"\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n        {\"role\": \"user\", \"content\": search_query}]\n).choices[0].message.content\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store with AnalyticDB and OpenAI Embeddings\nDESCRIPTION: Creates an AnalyticDB vector store using OpenAI embeddings. Converts the answer texts into embeddings and stores them in the database for vector similarity search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.vectorstores import AnalyticDB\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain import VectorDBQA, OpenAI\n\nembeddings = OpenAIEmbeddings()\ndoc_store = AnalyticDB.from_texts(\n    texts=answers, embedding=embeddings, connection_string=CONNECTION_STRING,\n    pre_delete_collection=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AWS Lambda Function with SAM (YAML)\nDESCRIPTION: This snippet is a SAM (Serverless Application Model) template defining an AWS Lambda function, including Cognito User Pool, API Gateway, and related configurations for OAuth authentication. It defines the function's properties, such as the handler, runtime, timeout, and events. The template also sets up an API Gateway with a Cognito authorizer, ensuring only authenticated users can access the function. Dependencies include AWS SAM and the AWS CloudFormation service.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: >\n  aws-middleware\n\n  AWS middleware function\n\nParameters:\n  CognitoUserPoolName:\n    Type: String\n    Default: MyCognitoUserPool\n  CognitoUserPoolClientName:\n    Type: String\n    Default: MyCognitoUserPoolClient\n\nResources:\n  MyCognitoUserPool:\n    Type: AWS::Cognito::UserPool\n    Properties:\n      UserPoolName: !Ref CognitoUserPoolName\n      Policies:\n        PasswordPolicy:\n          MinimumLength: 8\n      UsernameAttributes:\n        - email\n      Schema:\n        - AttributeDataType: String\n          Name: email\n          Required: false\n\n  MyCognitoUserPoolClient:\n    Type: AWS::Cognito::UserPoolClient\n    Properties:\n      UserPoolId: !Ref MyCognitoUserPool\n      ClientName: !Ref CognitoUserPoolClientName\n      GenerateSecret: true\n\n  MiddlewareApi:\n    Type: AWS::Serverless::Api\n    Properties:\n      StageName: Prod\n      Cors: \"'*'\"\n      Auth:\n        DefaultAuthorizer: MyCognitoAuthorizer\n        Authorizers:\n          MyCognitoAuthorizer:\n            AuthorizationScopes:\n              - openid\n              - email\n              - profile\n            UserPoolArn: !GetAtt MyCognitoUserPool.Arn\n        \n  MiddlewareFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: aws-middleware/\n      Handler: app.lambda_handler\n      Runtime: python3.11\n      Timeout: 45\n      Architectures:\n        - x86_64\n      Events:\n        SqlStatement:\n          Type: Api\n          Properties:\n            Path: /my_route\n            Method: post\n            RestApiId: !Ref MiddlewareApi\n\nOutputs:\n  MiddlewareApi:\n    Description: \"API Gateway endpoint URL for Prod stage for SQL Statement function\"\n    Value: !Sub \"https://${MiddlewareApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/my_route\"\n  MiddlewareFunction:\n    Description: \"SQL Statement Lambda Function ARN\"\n    Value: !GetAtt MiddlewareFunction.Arn\n  MiddlewareFunctionIamRole:\n    Description: \"Implicit IAM Role created for SQL Statement function\"\n    Value: !GetAtt MiddlewareFunctionRole.Arn\n  CognitoUserPoolArn:\n    Description: \"ARN of the Cognito User Pool\"\n    Value: !GetAtt MyCognitoUserPool.Arn\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Environment and OpenAI Client for RAG\nDESCRIPTION: Imports required Python libraries including pandas, OpenAI client, tqdm, and visualization libraries. Suppresses warnings and initializes the OpenAI client with an API key read from environment variables. Sets up pandas progress bar integration. This snippet prepares the computational environment and API client to facilitate RAG model fine-tuning and querying.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport time\n\nimport pandas as pd\nfrom openai import OpenAI\nimport tiktoken\nimport seaborn as sns\nfrom tenacity import retry, wait_exponential\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntqdm.pandas()\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Encoding Image to Base64 (Python)\nDESCRIPTION: This function encodes an image file to a base64 string. It takes the image path as input, reads the image file in binary mode, encodes it using base64, and then decodes the result to a UTF-8 string. This is used to pass images to the GPT-4o model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Default GCP Credentials and Project ID (Python)\nDESCRIPTION: Utilizes the `google.auth.default()` function from the Google Cloud authentication library to obtain the default credentials (typically Application Default Credentials) and the associated GCP project ID for the current environment. It also sets a default GCP region variable (`us-central1`) and prints the retrieved project ID for verification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom google.auth import default\n\n# Use default credentials\ncredentials, project_id = default()\nregion = \"us-central1\" # e.g: \"us-central1\"\nprint(\"Default Project ID:\", project_id)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Embedding Data Length Python\nDESCRIPTION: Prints the length of the 'data' field in the embedding response `res`. This shows the number of embeddings generated, which should correspond to the number of input phrases.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlen(res['data'])\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Vector Store in Python\nDESCRIPTION: This snippet demonstrates how to initialize and configure a Pinecone vector store using Python. It loads an API key securely from environment variables, creates a Pinecone client, and initializes a serverless index with specified dimension matching the embedding model (3072 for text-embedding-3-large) and cosine similarity metric. The index creation waits until the index is ready before proceeding. Dependencies include the Pinecone SDK (`pinecone[grpc]`) and python-dotenv for environment variable management.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport time\n# Import the Pinecone library\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napi_key = os.getenv(\"PINECONE_API_KEY\")\n\n# Initialize a Pinecone client with your API key\npc = Pinecone(api_key)\n\n# Create a serverless index\nindex_name = \"my-test-index\"\n\nif not pc.has_index(index_name):\n    pc.create_index(\n        name=index_name,\n        dimension=3072,\n        metric=\"cosine\",\n        spec=ServerlessSpec(\n            cloud='aws',\n            region='us-east-1'\n        )\n    )\n\n# Wait for the index to be ready\nwhile not pc.describe_index(index_name).status['ready']:\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Head - Python\nDESCRIPTION: Displays the first few rows of the `article_df` DataFrame using the `.head()` method. This is useful for quickly inspecting the data and verifying that it has been loaded correctly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Embeddings in Python\nDESCRIPTION: This code snippet loads embeddings from a CSV file using pandas, converts the embedding strings to lists of floats using literal_eval, and converts the dataframe to a numpy array.  It relies on pandas for data loading, scikit-learn for dimensionality reduction, numpy for numerical operations and the ast library for safely converting string representations of lists to actual lists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_wandb.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport numpy as np\nfrom ast import literal_eval\n\n# Load the embeddings\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\ndf = pd.read_csv(datafile_path)\n\n# Convert to a list of lists of floats\nmatrix = np.array(df.embedding.apply(literal_eval).to_list())\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Connection with API Key\nDESCRIPTION: This code initializes a connection to the Pinecone vector database using the API key and environment variables.  It retrieves the API key and environment from environment variables or uses default values if not found. It then initializes the Pinecone connection and verifies it using `pinecone.whoami()`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n\n# find environment next to your API key in the Pinecone console\nenv = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n\npinecone.init(api_key=api_key, environment=env)\npinecone.whoami()\n```\n\n----------------------------------------\n\nTITLE: Comparing Similarity Distributions Before and After Optimization using Plotly in Python\nDESCRIPTION: Uses `plotly.express` (`px`) to create histograms visualizing the distribution of cosine similarities. It plots the original `cosine_similarity` and the `cosine_similarity_custom` (after applying the best matrix), colored by the ground truth `label` and faceted by `dataset` (train/test). It also calculates and prints the test set accuracy using both the original and custom similarities, demonstrating the improvement from the matrix optimization. Depends on `pandas`, `plotly.express`, and the `accuracy_and_se` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# plot similarity distribution BEFORE customization\npx.histogram(\n    df,\n    x=\"cosine_similarity\",\n    color=\"label\",\n    barmode=\"overlay\",\n    width=500,\n    facet_row=\"dataset\",\n).show()\n\ntest_df = df[df[\"dataset\"] == \"test\"]\na, se = accuracy_and_se(test_df[\"cosine_similarity\"], test_df[\"label\"])\nprint(f\"Test accuracy: {a:0.1%} Â± {1.96 * se:0.1%}\")\n\n# plot similarity distribution AFTER customization\npx.histogram(\n    df,\n    x=\"cosine_similarity_custom\",\n    color=\"label\",\n    barmode=\"overlay\",\n    width=500,\n    facet_row=\"dataset\",\n).show()\n\na, se = accuracy_and_se(test_df[\"cosine_similarity_custom\"], test_df[\"label\"])\nprint(f\"Test accuracy after customization: {a:0.1%} Â± {1.96 * se:0.1%}\")\n```\n\n----------------------------------------\n\nTITLE: Checking Item Compatibility with GPT-4o mini (Python)\nDESCRIPTION: Defines a Python function `check_match` that acts as a guardrail. It takes base64 encoded strings of two images (a reference item and a suggested item) and sends them to a GPT-4o mini model via the OpenAI API. The prompt instructs the model to determine if the two items would work together in an outfit and return a JSON object with an 'answer' ('yes' or 'no') and a 'reason'. It requires an initialized OpenAI client (`client`) and the model name (`GPT_MODEL`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef check_match(reference_image_base64, suggested_image_base64):\n    response = client.chat.completions.create(\n        model=GPT_MODEL,\n        messages=[\n            {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                \"type\": \"text\",\n                \"text\": \"\"\" You will be given two images of two different items of clothing.\n                            Your goal is to decide if the items in the images would work in an outfit together.\n                            The first image is the reference item (the item that the user is trying to match with another item).\n                            You need to decide if the second item would work well with the reference item.\n                            Your response must be a JSON output with the following fields: \"answer\", \"reason\".\n                            The \"answer\" field must be either \"yes\" or \"no\", depending on whether you think the items would work well together.\n                            The \"reason\" field must be a short explanation of your reasoning for your decision. Do not include the descriptions of the 2 images.\n                            Do not include the ```json ``` tag in the output.\n                           \"\"\",\n                },\n                {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{reference_image_base64}\",\n                },\n                },\n                {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{suggested_image_base64}\",\n                }\n                }\n            ],\n            }\n        ],\n        max_tokens=300,\n    )\n    # Extract relevant features from the response\n    features = response.choices[0].message.content\n    return features\n```\n\n----------------------------------------\n\nTITLE: Define NER Label Set for Entity Recognition Tasks\nDESCRIPTION: Specifies a list of standard Named Entity Recognition labels covering persons, organizations, locations, dates, quantities, and other categories for use in entity detection prompt configurations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlabels = [\n    \"person\",      # people, including fictional characters\n    \"fac\",         # buildings, airports, highways, bridges\n    \"org\",         # organizations, companies, agencies, institutions\n    \"gpe\",         # geopolitical entities like countries, cities, states\n    \"loc\",         # non-gpe locations\n    \"product\",     # vehicles, foods, appareal, appliances, software, toys \n    \"event\",       # named sports, scientific milestones, historical events\n    \"work_of_art\", # titles of books, songs, movies\n    \"law\",         # named laws, acts, or legislations\n    \"language\",    # any named language\n    \"date\",        # absolute or relative dates or periods\n    \"time\",        # time units smaller than a day\n    \"percent\",     # percentage (e.g., \"twenty percent\", \"18%\")\n    \"money\",       # monetary values, including unit\n    \"quantity\",    # measurements, e.g., weight or distance\n]\n```\n\n----------------------------------------\n\nTITLE: Index data into Elasticsearch using Bulk API\nDESCRIPTION: This snippet indexes data into Elasticsearch in batches of 100 using the `helpers.bulk` API.  It iterates through the `wikipedia_dataframe` in chunks, converts each chunk into bulk actions using the `dataframe_to_bulk_actions` function, and then sends these actions to Elasticsearch for indexing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstart = 0\nend = len(wikipedia_dataframe)\nbatch_size = 100\nfor batch_start in range(start, end, batch_size):\n    batch_end = min(batch_start + batch_size, end)\n    batch_dataframe = wikipedia_dataframe.iloc[batch_start:batch_end]\n    actions = dataframe_to_bulk_actions(batch_dataframe)\n    helpers.bulk(client, actions)\n```\n\n----------------------------------------\n\nTITLE: Saving ADA Search Results to CSV File with Pandas in Python\nDESCRIPTION: This code collects the ADA search results into a pandas DataFrame, renames the single column for clarity, and writes the DataFrame to a CSV file. The snippet expects that ada_results has been computed previously and that pandas is available. Output is a CSV file named 'olympics-data/search_engine_results.csv' to facilitate further analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nout = pd.concat([ada_results], axis=1)\nout.columns = ['ada']\nout.to_csv('olympics-data/search_engine_results.csv')\n```\n\n----------------------------------------\n\nTITLE: Executing the Agent Executor for a User Query in LangChain (Python)\nDESCRIPTION: Runs the agent_executor on a sample query string, instructing the agent to process 'How many people live in canada as of 2023?'. Expects a prepared agent_executor capable of handling the prompt, parsing the output, and utilizing tools if needed. The output will either be the LLM's answer or a series of intermediate steps if tools are invoked.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nagent_executor.run(\"How many people live in canada as of 2023?\")\n\n```\n\n----------------------------------------\n\nTITLE: Printing Retrieval Results Python\nDESCRIPTION: Prints the result of the retrieval query, showing the most relevant document chunks and metadata. This allows inspection of the retrieved contexts before using them to augment GPT-4's responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nres\n```\n\n----------------------------------------\n\nTITLE: Printing the Multimodal Response Output as JSON\nDESCRIPTION: Converts the complex multimodal response object to JSON and prints it with indentation for better visualization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport json\nprint(json.dumps(response_multimodal.__dict__, default=lambda o: o.__dict__, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Summary Prompt Variation in Python\nDESCRIPTION: Extends the prompt template to specify that the generated summary should be concise and snappy. Uses string formatting to incorporate the base prompt and adds a directive for brevity, facilitating different prompt variations during testing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nPROMPT_VARIATION_BASIC = f\"\"\"\\n{PROMPT_PREFIX}\\n\\nYou should return a summary that is concise and snappy.\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as Environment Variable\nDESCRIPTION: Exports the OpenAI API key as an environment variable for use with Weaviate's OpenAI module. This key is required for vectorization of data during import and queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Export OpenAI API Key\n!export OPENAI_API_KEY=\"your key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Weaviate and Data Processing\nDESCRIPTION: Installs the Weaviate Python client and datasets library with apache-beam dependency for loading sample data. The Weaviate client enables communication with the Weaviate instance, while datasets helps with importing sample data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install the Weaviate client for Python\n!pip install weaviate-client>3.11.0\n\n# Install datasets and apache-beam to load the sample datasets\n!pip install datasets apache-beam\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API with API Key in Python\nDESCRIPTION: Sets the OpenAI API key to authenticate requests to OpenAI's endpoints. This snippet imports the OpenAI library and assigns the API key string to the 'openai.api_key' variable, which is required for all subsequent calls to OpenAI services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\n# get API key from top-right dropdown on OpenAI website\nopenai.api_key = \"OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Hybrid Query Combining Vector Search with Phrase Matching\nDESCRIPTION: A hybrid search query that combines vector search for \"man blue jeans\" with an exact phrase match for \"blue jeans\" in the product display name, improving search quality by adding text-based constraints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# improve search quality by adding hybrid query for \"man blue jeans\" in the product vector combined with a phrase search for \"blue jeans\"\nresults = search_redis(redis_client,\n                       \"man blue jeans\",\n                       vector_field=\"product_vector\",\n                       k=10,\n                       hybrid_fields='@productDisplayName:\"blue jeans\"'\n                       )\n```\n\n----------------------------------------\n\nTITLE: Generate Test Data and Make Chat Completion Requests - Python\nDESCRIPTION: This snippet generates simulated chat completion requests to test different prompt versions.  It defines a list of push notification data and prompts, then uses a loop to create tasks, sending the messages to the OpenAI API using the `client.chat.completions.create` method. Each request stores the completion and associates metadata with the prompt version and use case.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\npush_notification_data = [\n        \"\"\"\n- New message from Sarah: \\\"Can you call me later?\\\"\n- Your package has been delivered!\n- Flash sale: 20% off electronics for the next 2 hours!\n\"\"\",\n        \"\"\"\n- Weather alert: Thunderstorm expected in your area.\n- Reminder: Doctor's appointment at 3 PM.\n- John liked your photo on Instagram.\n\"\"\",\n        \"\"\"\n- Breaking News: Local elections results are in.\n- Your daily workout summary is ready.\n- Check out your weekly screen time report.\n\"\"\",\n        \"\"\"\n- Your ride is arriving in 2 minutes.\n- Grocery order has been shipped.\n- Don't miss the season finale of your favorite show tonight!\n\"\"\",\n        \"\"\"\n- Event reminder: Concert starts at 7 PM.\n- Your favorite team just scored!\n- Flashback: Memories from 3 years ago.\n\"\"\",\n        \"\"\"\n- Low battery alert: Charge your device.\n- Your friend Mike is nearby.\n- New episode of \\\"The Tech Hour\\\" podcast is live!\n\"\"\",\n        \"\"\"\n- System update available.\n- Monthly billing statement is ready.\n- Your next meeting starts in 15 minutes.\n\"\"\",\n        \"\"\"\n- Alert: Unauthorized login attempt detected.\n- New comment on your blog post: \\\"Great insights!\\\"\n- Tonight's dinner recipe: Pasta Primavera.\n\"\"\",\n        \"\"\"\n- Special offer: Free coffee with any breakfast order.\n- Your flight has been delayed by 30 minutes.\n- New movie release: \\\"Adventures Beyond\\\" now streaming.\n\"\"\",\n        \"\"\"\n- Traffic alert: Accident reported on Main Street.\n- Package out for delivery: Expected by 5 PM.\n- New friend suggestion: Connect with Emma.\n\"\"\"\n]\n```\n```python\nPROMPTS = [\n    (\n        \"\"\"\n        You are a helpful assistant that summarizes push notifications.\n        You are given a list of push notifications and you need to collapse them into a single one.\n        Output only the final summary, nothing else.\n        \"\"\",\n        \"v1\"\n    ),\n    (\n        \"\"\"\n        You are a helpful assistant that summarizes push notifications.\n        You are given a list of push notifications and you need to collapse them into a single one.\n        The summary should be longer than it needs to be and include more information than is necessary.\n        Output only the final summary, nothing else.\n        \"\"\",\n        \"v2\"\n    )\n]\n\ntasks = []\nfor notifications in push_notification_data:\n    for (prompt, version) in PROMPTS:\n        tasks.append(client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"developer\", \"content\": prompt},\n                {\"role\": \"user\", \"content\": notifications},\n            ],\n            store=True,\n            metadata={\"prompt_version\": version, \"usecase\": \"push_notifications_summarizer\"},\n        ))\nawait asyncio.gather(*tasks)\n```\n```\n\n----------------------------------------\n\nTITLE: Calculating ROUGE Scores for Summary Evaluation\nDESCRIPTION: This code snippet calculates ROUGE scores to evaluate two summaries against a reference summary. It utilizes the `get_rouge_scores` function (assumed to be defined elsewhere) to compute the scores for rouge-1, rouge-2, and rouge-l metrics. It iterates through the metrics and labels to generate scores for both summaries, and then formats the output using a pandas DataFrame with a custom highlighting function,  `highlight_max`, to indicate the best-performing summary for each metric.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\neval_1_rouge = get_rouge_scores(eval_summary_1, ref_summary)\neval_2_rouge = get_rouge_scores(eval_summary_2, ref_summary)\n\nfor metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n    for label in [\"F-Score\"]:\n        eval_1_score = eval_1_rouge[0][metric][label[0].lower()]\n        eval_2_score = eval_2_rouge[0][metric][label[0].lower()]\n\n        row = {\n            \"Metric\": f\"{metric} ({label})\",\n            \"Summary 1\": eval_1_score,\n            \"Summary 2\": eval_2_score,\n        }\n        rouge_scores_out.append(row)\n\n\n\ndef highlight_max(s):\n    is_max = s == s.max()\n    return [\n        \"background-color: lightgreen\" if v else \"background-color: white\"\n        for v in is_max\n    ]\n\n\nrouge_scores_out = (\n    pd.DataFrame(rouge_scores_out)\n    .set_index(\"Metric\")\n    .style.apply(highlight_max, axis=1)\n)\n\nrouge_scores_out\n```\n\n----------------------------------------\n\nTITLE: Splitting Wikipedia Sections into Token-Limited Chunks in Python\nDESCRIPTION: Processes Wikipedia sections by splitting them into smaller chunks that don't exceed a maximum token count of 1600. Reports the number of original sections and resulting strings after splitting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# split sections into chunks\nMAX_TOKENS = 1600\nwikipedia_strings = []\nfor section in wikipedia_sections:\n    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n\nprint(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Schema YAML definition\nDESCRIPTION: Defines the full OpenAPI 3.1.0 schema for the Gmail Email API, including paths, methods, request parameters, responses, and data schemas. It facilitates API documentation, client SDK generation, and validation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_gmail.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nopenapi: 3.1.0\n\ninfo:\n  title: Gmail Email API\n  version: 1.0.0\n  description: API to read, write, and send emails in a Gmail account.\n\nservers:\n  - url: https://gmail.googleapis.com\n\npaths:\n  /gmail/v1/users/{userId}/messages:\n    get:\n      summary: List All Emails\n      description: Lists all the emails in the user's mailbox.\n      operationId: listAllEmails\n      parameters:\n        - name: userId\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n        - name: q\n          in: query\n          schema:\n            type: string\n          description: Query string to filter messages (optional).\n        - name: pageToken\n          in: query\n          schema:\n            type: string\n          description: Token to retrieve a specific page of results in the list.\n        - name: maxResults\n          in: query\n          schema:\n            type: integer\n            format: int32\n          description: Maximum number of messages to return.\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/MessageList'\n        '400':\n          description: Bad Request\n        '401':\n          description: Unauthorized\n        '403':\n          description: Forbidden\n        '404':\n          description: Not Found\n        '500':\n          description: Internal Server Error\n\n  /gmail/v1/users/{userId}/messages/send:\n    post:\n      summary: Send Email\n      description: Sends a new email.\n      operationId: sendEmail\n      parameters:\n        - name: userId\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/Message'\n      responses:\n        '200':\n          description: Email sent successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Message'\n        '400':\n          description: Bad Request\n        '401':\n          description: Unauthorized\n        '403':\n          description: Forbidden\n        '500':\n          description: Internal Server Error\n\n  /gmail/v1/users/{userId}/messages/{id}:\n    get:\n      summary: Read Email\n      description: Gets the full email content including headers and body.\n      operationId: readEmail\n      parameters:\n        - name: userId\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n        - name: id\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The ID of the email to retrieve.\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/FullMessage'\n        '400':\n          description: Bad Request\n        '401':\n          description: Unauthorized\n        '403':\n          description: Forbidden\n        '404':\n          description: Not Found\n        '500':\n          description: Internal Server Error\n\n  /gmail/v1/users/{userId}/messages/{id}/modify:\n    post:\n      summary: Modify Label\n      description: Modify labels of an email.\n      operationId: modifyLabels\n      parameters:\n        - name: userId\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n        - name: id\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The ID of the email to change labels.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/LabelModification'\n      responses:\n        '200':\n          description: Labels modified successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Message'\n        '400':\n          description: Bad Request\n        '401':\n          description: Unauthorized\n        '403':\n          description: Forbidden\n        '500':\n          description: Internal Server Error\n\n  /gmail/v1/users/{userId}/drafts:\n    post:\n      summary: Create Draft\n      description: Creates a new email draft.\n      operationId: createDraft\n      parameters:\n        - name: userId\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/Draft'\n      responses:\n        '200':\n          description: Draft created successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Draft'\n        '400':\n          description: Bad Request\n        '401':\n          description: Unauthorized\n        '403':\n          description: Forbidden\n        '500':\n          description: Internal Server Error\n\n  /gmail/v1/users/{userId}/drafts/send:\n    post:\n      summary: Send Draft\n      description: Sends an existing email draft.\n      operationId: sendDraft\n      parameters:\n        - name: userId\n          in: path\n          required: true\n          schema:\n            type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/SendDraftRequest'\n      responses:\n        '200':\n          description: Draft sent successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Message'\n        '400':\n          description: Bad Request\n        '401':\n          description: Unauthorized\n        '403':\n          description: Forbidden\n        '500':\n          description: Internal Server Error\n\ncomponents:\n  schemas:\n    MessageList:\n      type: object\n      properties:\n        messages:\n          type: array\n          items:\n            $ref: '#/components/schemas/Message'\n        nextPageToken:\n          type: string\n\n    Message:\n      type: object\n      properties:\n        id:\n          type: string\n        threadId:\n          type: string\n        labelIds:\n          type: array\n          items:\n            type: string\n        addLabelIds:\n          type: array\n          items:\n            type: string\n        removeLabelIds:\n          type: array\n          items:\n            type: string\n        snippet:\n          type: string\n        raw:\n          type: string\n          format: byte\n          description: The entire email message in an RFC 2822 formatted and base64url encoded string.\n\n    FullMessage:\n      type: object\n      properties:\n        id:\n          type: string\n        threadId:\n          type: string\n        labelIds:\n          type: array\n          items:\n            type: string\n        snippet:\n          type: string\n        payload:\n          type: object\n          properties:\n            headers:\n              type: array\n              items:\n                type: object\n                properties:\n                  name:\n                    type: string\n                  value:\n                    type: string\n            parts:\n              type: array\n              items:\n                type: object\n                properties:\n                  mimeType:\n                    type: string\n                  body:\n                    type: object\n                    properties:\n                      data:\n                        type: string\n\n    LabelModification:\n      type: object\n      properties:\n        addLabelIds:\n          type: array\n          items:\n            type: string\n        removeLabelIds:\n          type: array\n          items:\n            type: string\n\n    Label:\n      type: object\n      properties:\n        addLabelIds:\n          type: array\n          items:\n            type: string\n        removeLabelIds:\n          type: array\n          items:\n            type: string\n\n    EmailDraft:\n      type: object\n      properties:\n        to:\n          type: array\n          items:\n            type: string\n        cc:\n          type: array\n          items:\n            type: string\n        bcc:\n          type: array\n          items:\n            type: string\n        subject:\n          type: string\n        body:\n          type: object\n          properties:\n            mimeType:\n              type: string\n              enum: [text/plain, text/html]\n            content:\n              type: string\n\n    Draft:\n      type: object\n      properties:\n        id:\n          type: string\n        message:\n          $ref: '#/components/schemas/Message'\n\n    SendDraftRequest:\n      type: object\n      properties:\n        draftId:\n          type: string\n          description: The ID of the draft to send.\n        userId:\n          type: string\n          description: The user's email address. Use \"me\" to indicate the authenticated user.\n\n```\n\n----------------------------------------\n\nTITLE: Using Truncated Tokens for Safe Embedding in Python\nDESCRIPTION: Applies the truncation function to a long text, then obtains its embedding, demonstrating a straightforward method to avoid exceeding token limits by truncation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntruncated = truncate_text_tokens(long_text)\nlen(get_embedding(truncated))\n```\n\n----------------------------------------\n\nTITLE: Defining Milvus Query Function in Python\nDESCRIPTION: Imports the `textwrap` library for formatting output. Defines a function `query` that takes one or more query strings (`queries`) and an optional `top_k` parameter. It ensures `queries` is a list, embeds the queries using the `embed` function, and performs a search on the Milvus `collection` using the generated embeddings, `QUERY_PARAM`, `anns_field='embedding'`, and requesting 'title' and 'description' as output fields. The function then iterates through the search results, printing the original query and the ranked results, including score, title, and a wrapped version of the description.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport textwrap\n\ndef query(queries, top_k = 5):\n    if type(queries) != list:\n        queries = [queries]\n    res = collection.search(embed(queries), anns_field='embedding', param=QUERY_PARAM, limit = top_k, output_fields=['title', 'description'])\n    for i, hit in enumerate(res):\n        print('Description:', queries[i])\n        print('Results:')\n        for ii, hits in enumerate(hit):\n            print('\\t' + 'Rank:', ii + 1, 'Score:', hits.score, 'Title:', hits.entity.get('title'))\n            print(textwrap.fill(hits.entity.get('description'), 88))\n            print()\n```\n\n----------------------------------------\n\nTITLE: Extracting and Naming Cluster Themes with OpenAI GPT-4 in Python\nDESCRIPTION: Samples reviews from each cluster, formats them, and queries GPT-4 via the OpenAI API to generate a semantic description for each cluster's theme. Dependencies: openai, pandas, os. Output consists of printed cluster themes and representative samples. Requires an OpenAI API key and clustered dataframe with 'Cluster', 'combined', 'Score', 'Summary', and 'Text' columns. Each cluster's samples and generated theme are printed. Relies on precomputed clusters and review fields.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n# Reading a review which belong to each group.\nrev_per_cluster = 5\n\nfor i in range(n_clusters):\n    print(f\"Cluster {i} Theme:\", end=\" \")\n\n    reviews = \"\\n\".join(\n        df[df.Cluster == i]\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"\\n\\nContent: \", \":  \")\n        .sample(rev_per_cluster, random_state=42)\n        .values\n    )\n\n    messages = [\n        {\"role\": \"user\", \"content\": f'What do the following customer reviews have in common?\\n\\nCustomer reviews:\\n\"\"\"\\n{reviews}\\n\"\"\"\\n\\nTheme:'}\n    ]\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=messages,\n        temperature=0,\n        max_tokens=64,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0)\n    print(response.choices[0].message.content.replace(\"\\n\", \"\"))\n\n    sample_cluster_rows = df[df.Cluster == i].sample(rev_per_cluster, random_state=42)\n    for j in range(rev_per_cluster):\n        print(sample_cluster_rows.Score.values[j], end=\", \")\n        print(sample_cluster_rows.Summary.values[j], end=\":   \")\n        print(sample_cluster_rows.Text.str[:70].values[j])\n\n    print(\"-\" * 100)\n\n```\n\n----------------------------------------\n\nTITLE: Sorting Articles by Semantic Similarity Scores - Python\nDESCRIPTION: This snippet zips articles with their cosine similarity scores, sorts the results in descending order, and prints details for the top matches. Designed to surface the most semantically relevant articles for answer synthesis. Requires prior completion of the embeddings and search steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nscored_articles = zip(articles, cosine_similarities)\n\n# Sort articles by cosine similarity\nsorted_articles = sorted(scored_articles, key=lambda x: x[1], reverse=True)\n\n# Print top 5 articles\nprint(\"Top 5 articles:\", \"\\n\")\n\nfor article, score in sorted_articles[0:5]:\n    print(\"Title:\", article[\"title\"])\n    print(\"Description:\", article[\"description\"])\n    print(\"Content:\", article[\"content\"][0:100] + \"...\")\n    print(\"Score:\", score)\n    print()\n\n```\n\n----------------------------------------\n\nTITLE: Viewing DataGrid Contents in Kangas\nDESCRIPTION: Displays the actual contents of the DataGrid, showing the first and last rows of the loaded data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata\n```\n\n----------------------------------------\n\nTITLE: Creating a list of image file paths from a directory\nDESCRIPTION: Scans a specified directory for JPEG images and returns their full paths up to an optional limit. Facilitates batch processing of images for embedding extraction within the knowledge base.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_image_paths(directory: str, number: int = None) -> List[str]:\n    image_paths = []\n    count = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.jpeg'):\n            image_paths.append(os.path.join(directory, filename))\n            if number is not None and count == number:\n                return [image_paths[-1]]\n            count += 1\n    return image_paths\n\ndirec = 'image_database/'\nimage_paths = get_image_paths(direc)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Client with API Key in JavaScript\nDESCRIPTION: Shows how to instantiate the OpenAI client in JavaScript by manually providing the API key as a parameter. This setup is required if the environment variable is not used. Never hard-code real keys; best practice is to load keys securely from environment variables or secret management. The placeholder <openai-api-key> should be replaced at runtime.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = new OpenAI({\n  apiKey: \"<openai-api-key>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Calling LLM for Image Interpretation\nDESCRIPTION: This function calls an LLM (GPT-4o in this example) to process an image and a prompt. The image is passed as a base64-encoded string within the prompt. The function first encodes the image from a given file path using `encode_image`. The prompt defines the task for the LLM, and the function returns the LLM's response, which will be the analysis or interpretation of the image based on the provided prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef get_vision_response(prompt, image_path):\n    # Getting the base64 string\n    base64_image = encode_image(image_path)\n\n    response = oai_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                        },\n                    },\n                ],\n            }\n        ],\n    )\n    return response\n```\n\n----------------------------------------\n\nTITLE: ChatGPT API call with system message\nDESCRIPTION: Shows a single-turn conversation with a system message that provides context for the assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# example with a system message\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Creating Documents Table with Vector Column in Postgres SQL\nDESCRIPTION: Creates a new table called documents for storing text content and associated embeddings as vectors of fixed size (1536 dimensions) using pgvector in Postgres. The table defines an auto-incrementing primary key, a text field for content, and a vector field for the embedding. Run this in a SQL environment with pgvector enabled. The dimensions should match the output size of the embedding model used; here, 1536 aligns with OpenAI's text-embedding-3-small model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate table documents (\n  id bigint primary key generated always as identity,\n  content text not null,\n  embedding vector (1536) not null\n);\n```\n\n----------------------------------------\n\nTITLE: Structuring CoQA Data into QuestionAnswer Objects in Python\nDESCRIPTION: Defines a `QuestionAnswer` dataclass to encapsulate passage, question, expected answer, and generated answer. It then processes the raw data loaded by DuckDB (`full_result`), flattening the nested questions and answers into a list of `QuestionAnswer` objects. Initially, the `generated_answer` is set to be the same as the `expected_answer`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass QuestionAnswer:\n    passage: str\n    question: str\n    expected_answer: str\n    generated_answer: str\n\n\nqa_pairs = [\n    QuestionAnswer(\n        passage=r[1],\n        question=question,\n        generated_answer=r[3][\"input_text\"][i],\n        expected_answer=r[3][\"input_text\"][i],\n    )\n    for r in full_result\n    for (i, question) in enumerate(r[2])\n]\n\nprint(len(qa_pairs))\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Generating Function Call Permutations (Python)\nDESCRIPTION: This Python code defines a set of helper functions (`generate_permutations`, `generate_required_permutations`, `generate_optional_permutations`, `get_possible_values`) to systematically generate all possible argument combinations for a given set of function parameter definitions. They handle required and optional parameters, various data types (enums, integers, strings, booleans, arrays of enums), and utilize the `itertools` library for creating combinations and products. These functions are crucial for creating comprehensive synthetic data for fine-tuning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport itertools\nfrom typing import Dict, Any, Generator, List\n\ndef generate_permutations(\n    params: Dict[str, Dict[str, Any]]\n) -> Generator[Dict[str, Any], None, None]:\n    \"\"\"\n    Generates all possible permutations for given parameters.\n\n    :param params: Parameter dictionary containing required and optional fields.\n    :return: A generator yielding each permutation.\n    \"\"\"\n\n    # Extract the required fields from the parameters\n    required_fields = params.get(\"required\", [])\n\n    # Generate permutations for required fields\n    required_permutations = generate_required_permutations(params, required_fields)\n\n    # Generate optional permutations based on each required permutation\n    for required_perm in required_permutations:\n        yield from generate_optional_permutations(params, required_perm)\n\n\ndef generate_required_permutations(\n    params: Dict[str, Dict[str, Any]], required_fields: List[str]\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Generates permutations for the required fields.\n\n    :param params: Parameter dictionary.\n    :param required_fields: List of required fields.\n    :return: A list of permutations for required fields.\n    \"\"\"\n\n    # Get all possible values for each required field\n    required_values = [get_possible_values(params, field) for field in required_fields]\n\n    # Generate permutations from possible values\n    return [\n        dict(zip(required_fields, values))\n        for values in itertools.product(*required_values)\n    ]\n\n\ndef generate_optional_permutations(\n    params: Dict[str, Dict[str, Any]], base_perm: Dict[str, Any]\n) -> Generator[Dict[str, Any], None, None]:\n    \"\"\"\n    Generates permutations for optional fields based on a base permutation.\n\n    :param params: Parameter dictionary.\n    :param base_perm: Base permutation dictionary.\n    :return: A generator yielding each permutation for optional fields.\n    \"\"\"\n\n    # Determine the fields that are optional by subtracting the base permutation's fields from all properties\n    optional_fields = set(params[\"properties\"]) - set(base_perm)\n\n    # Iterate through all combinations of optional fields\n    for field_subset in itertools.chain.from_iterable(\n        itertools.combinations(optional_fields, r)\n        for r in range(len(optional_fields) + 1)\n    ):\n\n        # Generate product of possible values for the current subset of fields\n        for values in itertools.product(\n            *(get_possible_values(params, field) for field in field_subset)\n        ):\n\n            # Create a new permutation by combining base permutation and current field values\n            new_perm = {**base_perm, **dict(zip(field_subset, values))}\n\n            yield new_perm\n\n\ndef get_possible_values(params: Dict[str, Dict[str, Any]], field: str) -> List[Any]:\n    \"\"\"\n    Retrieves possible values for a given field.\n\n    :param params: Parameter dictionary.\n    :param field: The field for which to get possible values.\n    :return: A list of possible values.\n    \"\"\"\n\n    # Extract field information from the parameters\n    field_info = params[\"properties\"][field]\n\n    # Based on the field's type or presence of 'enum', determine and return the possible values\n    if \"enum\" in field_info:\n        return field_info[\"enum\"]\n    elif field_info[\"type\"] == \"integer\":\n        return [placeholder_int]\n    elif field_info[\"type\"] == \"string\":\n        return [placeholder_string]\n    elif field_info[\"type\"] == \"boolean\":\n        return [True, False]\n    elif field_info[\"type\"] == \"array\" and \"enum\" in field_info[\"items\"]:\n        enum_values = field_info[\"items\"][\"enum\"]\n        all_combinations = [\n            list(combo)\n            for i in range(1, len(enum_values) + 1)\n            for combo in itertools.combinations(enum_values, i)\n        ]\n        return all_combinations\n    return []\n```\n\n----------------------------------------\n\nTITLE: Retrieving Fine-tuning Job Results and Decoding\nDESCRIPTION: This snippet retrieves result files from a fine-tuning job, extracts the file ID, retrieves the file content, and decodes it from base64 to UTF-8. It utilizes the OpenAI client to interact with the fine-tuning and files APIs. The decoded content, which is expected to be a CSV file, is then printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfine_tune_results = client.fine_tuning.jobs.retrieve(ftjob_id).result_files\nresult_file_id = client.files.retrieve(fine_tune_results[0]).id\n\n# Retrieve the result file\nresult_file = client.files.content(file_id=result_file_id)\ndecoded_content = base64.b64decode(result_file.read()).decode(\"utf-8\")\nprint(decoded_content)\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search with Similarity Cutoff in Python\nDESCRIPTION: Shows how to retrieve similarity scores along with search results from a Cassandra vector store using `similarity_dot_product`. It demonstrates filtering the results based on a defined similarity threshold.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nquote = \"Animals are our equals.\"\n# quote = \"Be good.\"\n# quote = \"This teapot is strange.\"\n\nsimilarity_threshold = 0.92\n\nquote_vector = client.embeddings.create(\n    input=[quote],\n    model=embedding_model_name,\n).data[0].embedding\n\n# Once more: remember to prepare your statements in production for greater performance...\n\nsearch_statement = f\"\"\"SELECT body, similarity_dot_product(embedding_vector, %s) as similarity\n    FROM {keyspace}.philosophers_cql\n    ORDER BY embedding_vector ANN OF %s\n    LIMIT %s;\n\"\"\"\nquery_values = (quote_vector, quote_vector, 8)\n\nresult_rows = session.execute(search_statement, query_values)\nresults = [\n    (result_row.body, result_row.similarity)\n    for result_row in result_rows\n    if result_row.similarity >= similarity_threshold\n]\n\nprint(f\"{len(results)} quotes within the threshold:\")\nfor idx, (r_body, r_similarity) in enumerate(results):\n    print(f\"    {idx}. [similarity={r_similarity:.3f}] \\\"{r_body[:70]}...\\\"\")\n```\n\n----------------------------------------\n\nTITLE: Batched Loading of Corpus Documents into Chroma Collection with Embeddings in Python\nDESCRIPTION: Loads the title and abstract corpus into the Chroma collection in batches for memory efficiency. Each batch contains document IDs (as strings), concatenated title and abstract text, and associated structured metadata. Chroma embeds documents asynchronously using the provided OpenAI embedding function for semantic retrieval.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 100\n\nfor i in range(0, len(corpus_df), batch_size):\n    batch_df = corpus_df[i:i+batch_size]\n    scifact_corpus_collection.add(\n        ids=batch_df['doc_id'].apply(lambda x: str(x)).tolist(), # Chroma takes string IDs.\n        documents=(batch_df['title'] + '. ' + batch_df['abstract'].apply(lambda x: ' '.join(x))).to_list(), # We concatenate the title and abstract.\n        metadatas=[{\"structured\": structured} for structured in batch_df['structured'].to_list()] # We also store the metadata, though we don't use it in this example.\n    )\n```\n\n----------------------------------------\n\nTITLE: Using logprobs to assess model confidence in classifying news headlines (Python example)\nDESCRIPTION: Demonstrates how to evaluate the confidence of model predictions by enabling logprobs and top_logprobs during classification of headlines into categories such as Technology or Politics. Results are visualized with confidence scores and tokens, enabling threshold-based decision-making.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nCLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nReturn only the name of the category, and nothing else.\nMAKE SURE your output is one of the four categories stated.\nArticle headline: {headline}\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Computing Embedding Dimension for Pinecone Index\nDESCRIPTION: Generates an embedding for a sample document to determine the embedding dimension. This dimension is required when creating a Pinecone vector index to store the dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"text-embedding-3-small\"  # Replace with your production embedding model if needed\n# Compute an embedding for the first document to obtain the embedding dimension.\nsample_embedding_resp = client.embeddings.create(\n    input=[ds_dataframe['merged'].iloc[0]],\n    model=MODEL\n)\nembed_dim = len(sample_embedding_resp.data[0].embedding)\nprint(f\"Embedding dimension: {embed_dim}\")\n```\n\n----------------------------------------\n\nTITLE: Import Libraries for Wikipedia Article Embedding\nDESCRIPTION: This code snippet imports necessary libraries for downloading, parsing, and embedding Wikipedia articles, as well as for data manipulation and token counting. It initializes the OpenAI client using the API key from the environment variables or a placeholder string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# imports\nimport mwclient  # for downloading example Wikipedia articles\nimport mwparserfromhell  # for splitting Wikipedia articles into sections\nfrom openai import OpenAI  # for generating embeddings\nimport os  # for environment variables\nimport pandas as pd  # for DataFrames to store article sections and embeddings\nimport re  # for cutting <ref> links out of Wikipedia articles\nimport tiktoken  # for counting tokens\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Filtering Vector Search Results Based on Similarity Threshold - Python\nDESCRIPTION: This code snippet performs a vector similarity search, filters the results based on a similarity threshold, and then prints the filtered quotes. It uses the `vector_find` method to search for similar vectors and the `$similarity` field in each result to determine if it meets the threshold. The `quote_vector` is created using OpenAI's embeddings API. The code iterates through the results and prints those that exceed the metric threshold.  Dependencies: `astra_db`, `openai` (client).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nquote = \"Animals are our equals.\"\n# quote = \"Be good.\"\n# quote = \"This teapot is strange.\"\n\nmetric_threshold = 0.92\n\nquote_vector = client.embeddings.create(\n    input=[quote],\n    model=embedding_model_name,\n).data[0].embedding\n\nresults_full = collection.vector_find(\n    quote_vector,\n    limit=8,\n    fields=[\"quote\"]\n)\nresults = [res for res in results_full if res[\"$similarity\"] >= metric_threshold]\n\nprint(f\"{len(results)} quotes within the threshold:\")\nfor idx, result in enumerate(results):\n    print(f\"    {idx}. [similarity={result['$similarity']:.3f}] \\\"{result['quote'][:70]}...\\\"\")\n```\n\n----------------------------------------\n\nTITLE: Loading Supabase Credentials from .env in Node.js Using dotenv\nDESCRIPTION: Demonstrates importing and using the dotenv library in Node.js to load environment variables from a .env file. After calling config(), variables such as SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY are available via process.env. Required for securely supplying configuration credentials without hard-coding in code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nimport { config } from \"dotenv\";\n\n// Load .env file\nconfig();\n\nconst supabaseUrl = process.env[\"SUPABASE_URL\"];\nconst supabaseServiceRoleKey = process.env[\"SUPABASE_SERVICE_ROLE_KEY\"];\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Batch Tasks for Movie Categorization\nDESCRIPTION: Builds a list of JSON-formatted task objects each representing a batch request for categorizing movies, including a unique custom ID, request method, URL, and the message payload with prompt and description.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntasks = []\n\nfor index, row in df.iterrows():\n    description = row['Overview']\n    task = {\n        \"custom_id\": f\"task-{index}\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"temperature\": 0.1,\n            \"response_format\": { \n                \"type\": \"json_object\"\n            },\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": categorize_system_prompt\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": description\n                }\n            ],\n        }\n    }\n    tasks.append(task)\n```\n\n----------------------------------------\n\nTITLE: Defining Salesforce OpenAPI Schema for SOQL and SOSL Queries in YAML\nDESCRIPTION: This OpenAPI 3.1.0 schema defines a Salesforce API to execute SOQL and SOSL queries against Salesforce custom objects. It specifies server URL templated by Salesforce subdomain, GET endpoints to execute queries, and detailed response schemas modeling query and search results including sObject properties such as attributes and IDs. Key parameters include the query string ('q'), which is mandatory for query execution. This schema serves as the foundation for creating Custom GPT actions for Salesforce integration, requiring insertion of the appropriate Salesforce instance URL and OAuth authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce_gong.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Salesforce API\n  version: 1.0.0\n  description: API for accessing Salesforce sObjects and executing queries.\nservers:\n  - url: https://<subdomain>.my.salesforce.com/services/data/v59.0\n    description: Salesforce API server\npaths:\n  /query:\n    get:\n      summary: Execute a SOQL Query\n      description: Executes a given SOQL query and returns the results.\n      operationId: executeSOQLQuery\n      parameters:\n        - name: q\n          in: query\n          description: The SOQL query string to be executed.\n          required: true\n          schema:\n            type: string\n      responses:\n        '200':\n          description: Query executed successfully.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/QueryResult'\n\n  /search:\n    get:\n      summary: Execute a SOSL Search\n      description: Executes a SOSL search based on the given query and returns matching records.\n      operationId: executeSOSLSearch\n      parameters:\n        - name: q\n          in: query\n          description: The SOSL search string (e.g., 'FIND {Acme}').\n          required: true\n          schema:\n            type: string\n      responses:\n        '200':\n          description: Search executed successfully.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/SearchResult'\n\ncomponents:\n  schemas:\n    QueryResult:\n      type: object\n      description: Result of a SOQL query.\n      properties:\n        totalSize:\n          type: integer\n          description: The total number of records matching the query.\n        done:\n          type: boolean\n          description: Indicates if the query result includes all records.\n        records:\n          type: array\n          description: The list of records returned by the query.\n          items:\n            $ref: '#/components/schemas/SObject'\n\n    SearchResult:\n      type: object\n      description: Result of a SOSL search.\n      properties:\n        searchRecords:\n          type: array\n          description: The list of records matching the search query.\n          items:\n            $ref: '#/components/schemas/SObject'\n\n    SObject:\n      type: object\n      description: A Salesforce sObject, which represents a database table record.\n      properties:\n        attributes:\n          type: object\n          description: Metadata about the sObject, such as type and URL.\n          properties:\n            type:\n              type: string\n              description: The sObject type.\n            url:\n              type: string\n              description: The URL of the record.\n        Id:\n          type: string\n          description: The unique identifier for the sObject.\n      additionalProperties: true\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Templates\nDESCRIPTION: Defines the prompts used to instruct gpt-4o to generate function invocations and conversational commands.  `INVOCATION_FILLER_PROMPT` is used to generate a valid function invocation given a function definition. `COMMAND_GENERATION_PROMPT` is used to generate conversational commands that, when received, would result in a function call.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nINVOCATION_FILLER_PROMPT = \"\"\"\n1) Input reasonable values for 'fill_in_string' and 'fill_in_int' in the invocation here: {invocation}. Reasonable values are determined by the function definition. Use the\nthe entire function provided here :{function} to get context over what proper fill_in_string and fill_in_int values would be.\nExample:\n\nInput: invocation: {{\n    \"name\": \"control_camera\",\n    \"arguments\": {{\n      \"mode\":\"video\",\n      \"duration\":\"fill_in_int\"\n    }}\n}},\nfunction:{function}\n\nOutput: invocation: {{\n    \"name\": \"control_camera\",\n    \"arguments\": {{\n      \"mode\":\"video\",\n      \"duration\": 30\n    }}\n}}\n\n\nMAKE SURE output is just a dictionary with keys 'name' and 'arguments', no other text or response.\n\nInput: {invocation}\nOutput:\n\"\"\"\n\n\nCOMMAND_GENERATION_PROMPT = \"\"\"\nYou are to output 2 commands, questions or statements that would generate the inputted function and parameters.\nPlease make the commands or questions natural, as a person would ask, and the command or questions should be varied and not repetitive.\nIt should not always mirror the exact technical terminology used in the function and parameters, rather reflect a conversational and intuitive request.\nFor instance, the prompt should not be 'turn on the dome light', as that is too technical, but rather 'turn on the inside lights'.\nAnother example, is the prompt should not be 'turn on the HVAC', but rather 'turn on the air conditioning'. Use language a normal driver would use, even if\nit is technically incorrect but colloquially used.\n\nRULES: ALWAYS put a backwards slash before an apostrophe or single quote '. For example, do not say don't but say don't.\nPrompts MUST be in double quotes as well.\n\nExample\n\nInput: {{'name': 'calibrate_sensors','arguments': {{}}'' }}\nPrompt: [\"The sensors are out of whack, can you reset them\", \"The calibration of the drone is off, fix it please!\"]\n\nInput: {{'name': 'set_autopilot','arguments': {{'status': 'off'}}}} \nPrompt: [\"OK, I want to take back pilot control now\",\"Turn off the automatic pilot I'm ready control it\"]\n\nInput: {invocation}\nPrompt: \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Identifying Files to Update/Delete from Patch Text in Python\nDESCRIPTION: Scans the lines of patch text and extracts the file paths specified in '*** Update File: ' and '*** Delete File: ' lines. Returns a list of these file paths.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef identify_files_needed(text: str) -> List[str]:\n    lines = text.splitlines()\n    return [\n        line[len(\"*** Update File: \") :]\n        for line in lines\n        if line.startswith(\"*** Update File: \")\n    ] + [\n        line[len(\"*** Delete File: \") :]\n        for line in lines\n        if line.startswith(\"*** Delete File: \")\n    ]\n```\n\n----------------------------------------\n\nTITLE: Importing Supabase Client in Node.js via ES Module\nDESCRIPTION: Imports the createClient function from the @supabase/supabase-js library for use in Node.js. Required for initializing Supabase database connections in JavaScript code. Assumes installation via npm.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createClient } from \"@supabase/supabase-js\";\n```\n\n----------------------------------------\n\nTITLE: Connecting Python client to a Weaviate instance with API Key\nDESCRIPTION: Creates a Weaviate client object in Python, connecting to a specified Weaviate instance URL and providing API key authentication. It also passes the OpenAI API key via headers, enabling automatic vectorization and search functionalities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n    # url=\"http://localhost:8080/\",\n    auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"<YOUR-WEAVIATE-API-KEY>\"), # comment out this line if you are not using authentication for your Weaviate instance (i.e. for locally deployed instances)\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n```\n\n----------------------------------------\n\nTITLE: Generating Fictitious Whisper Prompts via GPT in Python\nDESCRIPTION: Defines a function that uses OpenAI's GPT model to generate fictitious transcripts based on user instructions, which can then be used as prompts for Whisper. The function sends custom system and user messages to shape the generated prompt's content and style, intended to guide Whisper's transcription more precisely. Requires access to OpenAI's GPT models and the chat completions API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# define a function for GPT to generate fictitious prompts\ndef fictitious_prompt_from_instruction(instruction: str) -> str:\n    \"\"\"Given an instruction, generate a fictitious prompt.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a transcript generator. Your task is to create one long paragraph of a fictional conversation. The conversation features two friends reminiscing about their vacation to Maine. Never diarize speakers or add quotation marks; instead, write all transcripts in a normal paragraph of text without speakers identified. Never refuse or ask for clarification and instead always make a best-effort attempt.\",\n            },  # we pick an example topic (friends talking about a vacation) so that GPT does not refuse or ask clarifying questions\n            {\"role\": \"user\", \"content\": instruction},\n        ],\n    )\n    fictitious_prompt = response.choices[0].message.content\n    return fictitious_prompt\n\n```\n\n----------------------------------------\n\nTITLE: Polling and Retrieving Assistant's Slide Title Response (OpenAI Assistants API/Python)\nDESCRIPTION: Pauses execution to give the Assistant time to generate the slide title based on the previous context. It then calls `get_response` to fetch the latest messages and extracts the text content of the Assistant's response, which contains the generated slide title.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#Wait as assistant may take a few steps\ntime.sleep(10)\nresponse = get_response(thread)\ntitle = response.data[0].content[0].text.value\nprint(title)\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Dataset Entry by Index - Python\nDESCRIPTION: This code snippet prints a header line and then displays the 17th entry of the loaded philosopher quotes dataset. This is useful for examining the structure and fields ('author', 'quote', 'tags', etc.) of each dataset row, and aids in understanding how to batch or attribute data for embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"An example entry:\")\nprint(philo_dataset[16])\n\n```\n\n----------------------------------------\n\nTITLE: Performing Vector or Hybrid Search with RediSearch - Python\nDESCRIPTION: This Python function `search_redis` performs a search query against the RediSearch index. It first generates a vector embedding for the `user_query` using OpenAI. It then constructs a RediSearch `Query` object supporting KNN vector search and optional `hybrid_fields` filtering, specifies `return_fields`, sorts by vector score, sets paging, and executes the search using `redis_client.ft().search()`. Results are printed with a calculated similarity score.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef search_redis(\n    redis_client: redis.Redis,\n    user_query: str,\n    index_name: str = \"embeddings-index\",\n    vector_field: str = \"title_vector\",\n    return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],\n    hybrid_fields = \"*\",\n    k: int = 20,\n) -> List[dict]:\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(input=user_query,\n                                            model=EMBEDDING_MODEL,\n                                            )[\"data\"][0]['embedding']\n\n    # Prepare the Query\n    base_query = f'{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]'\n    query = (\n        Query(base_query)\n         .return_fields(*return_fields)\n         .sort_by(\"vector_score\")\n         .paging(0, k)\n         .dialect(2)\n    )\n    params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}\n\n    # perform vector search\n    results = redis_client.ft(index_name).search(query, params_dict)\n    for i, article in enumerate(results.docs):\n        score = 1 - float(article.vector_score)\n        print(f\"{i}. {article.title} (Score: {round(score ,3) })\")\n    return results.docs\n```\n\n----------------------------------------\n\nTITLE: Running Vector Search Queries with Custom Vector Field in Redis - Python\nDESCRIPTION: Illustrates searching documents in Redis using the `search_redis` function with a custom vector field ('content_vector') and a specific query about Scottish battles. Retrieves the top 10 most relevant results. Assumes schema contains 'content_vector' and that relevant embeddings have been indexed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nresults = search_redis(redis_client, 'Famous battles in Scottish history', vector_field='content_vector', k=10)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI SDK for Batch API usage in Python\nDESCRIPTION: Installs and upgrades the OpenAI SDK to ensure the latest features for Batch API interaction, including the necessary modules import setup for subsequent coding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Make sure you have the latest version of the SDK available to use the Batch API\n%pip install openai --upgrade\n```\n\n----------------------------------------\n\nTITLE: Creating a Token Highlighter with logprobs in Python\nDESCRIPTION: This snippet defines a function that creates a colorful HTML visualization of tokens in an API response. It uses the logprobs parameter to access token information and cycles through different colors to highlight each token distinctly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nPROMPT = \"\"\"What's the longest word in the English language?\"\"\"\n\nAPI_RESPONSE = get_completion(\n    [{\"role\": \"user\", \"content\": PROMPT}], model=\"gpt-4o\", logprobs=True, top_logprobs=5\n)\n\n\ndef highlight_text(api_response):\n    colors = [\n        \"#FF00FF\",  # Magenta\n        \"#008000\",  # Green\n        \"#FF8C00\",  # Dark Orange\n        \"#FF0000\",  # Red\n        \"#0000FF\",  # Blue\n    ]\n    tokens = api_response.choices[0].logprobs.content\n\n    color_idx = 0  # Initialize color index\n    html_output = \"\"  # Initialize HTML output\n    for t in tokens:\n        token_str = bytes(t.bytes).decode(\"utf-8\")  # Decode bytes to string\n\n        # Add colored token to HTML output\n        html_output += f\"<span style='color: {colors[color_idx]}'>{token_str}</span>\"\n\n        # Move to the next color\n        color_idx = (color_idx + 1) % len(colors)\n    display(HTML(html_output))  # Display HTML output\n    print(f\"Total number of tokens: {len(tokens)}\")\n```\n\n----------------------------------------\n\nTITLE: Calling Embeddings Endpoint - OpenAI API - Shell\nDESCRIPTION: This curl command submits a POST request to OpenAI's 'embeddings' API endpoint to obtain vector embeddings for a provided text input using the 'text-embedding-ada-002' model. Requires an active API key in the OPENAI_API_KEY environment variable and network connectivity. The input JSON specifies 'input' (the sentence to embed) and 'model' (type of embedding). Output will be a JSON structure containing the generated embeddings vector.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"The food was delicious and the waiter...\",\n    \"model\": \"text-embedding-ada-002\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth 2.0 Authentication Settings in ChatGPT for Jira\nDESCRIPTION: Specifies the required OAuth 2.0 configuration parameters to be entered into the ChatGPT authentication settings for connecting to Jira. The Client ID and Secret are obtained from the previously configured Jira application, while the URLs and scopes are standard Atlassian endpoints and permissions needed for reading/writing issues and reading user data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_jira.ipynb#_snippet_2\n\nLANGUAGE: Configuration\nCODE:\n```\n- **Client ID**: The **Client ID** from **Step 3** of Jira Configuration\n- **Client Secret**: The **Secret** from **Step 3** of Jira Configuration\n- **Authorization URL**: https://auth.atlassian.com/authorize\n- **Token URL**: https://auth.atlassian.com/oauth/token\n- **Scope**: read:jira-work write:jira-work read:jira-user \n- **Token Exchange Method**: Default (POST Request)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API client with API key in Python\nDESCRIPTION: Sets up the OpenAI API client by retrieving the API key from environment variables. Facilitates subsequent API calls for chat completions and other tasks. No specific dependencies beyond the openai library are required.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nfrom math import exp\nimport numpy as np\nfrom IPython.display import display, HTML\nimport os\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Using the most similar image and its description for user query in Python\nDESCRIPTION: Retrieves the closest image path from the search results, locates its associated description data, and constructs a prompt that includes the user query, image description, and image data URI. Sends this prompt to GPT-4 Vision for answering questions about the image.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsimilar_path = get_image_paths(direc, indices_distances[0][0])[0]\nelement = find_entry(data, 'image_path', similar_path)\n\nuser_query = 'What is the capacity of this item?'\nprompt = f\"\"\"\nBelow is a user query, I want you to answer the query using the description and image provided.\n\nuser query:\n{user_query}\n\ndescription:\n{element['description']}\n\"\"\"\nimage_query(prompt, similar_path)\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Simulated Conversations in Python\nDESCRIPTION: A loop that processes each question from the predefined list, executing a complete simulated conversation for each one using the execute_conversation function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor x in questions:\n\n    execute_conversation(x)\n```\n\n----------------------------------------\n\nTITLE: Generating British-Accented TTS with Dynamic Speech Parameters Using Chat Completions\nDESCRIPTION: Implementation of OpenAI's chat completions API for TTS with specific voice instructions. This example creates two audio files: one with a British accent speaking to a child, and another with a British accent speaking very fast.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/steering_tts.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\nspeech_file_path = \"./sounds/chat_completions_tts.mp3\"\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    modalities=[\"text\", \"audio\"],\n    audio={\"voice\": \"alloy\", \"format\": \"mp3\"},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can generate audio from text. Speak in a British accent and enunciate like you're talking to a child.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": tts_text,\n        }\n    ],\n)\n\nmp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\nwith open(speech_file_path, \"wb\") as f:\n    f.write(mp3_bytes)\n\nspeech_file_path = \"./sounds/chat_completions_tts_fast.mp3\"\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-audio-preview\",\n    modalities=[\"text\", \"audio\"],\n    audio={\"voice\": \"alloy\", \"format\": \"mp3\"},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can generate audio from text. Speak in a British accent and speak really fast.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": tts_text,\n        }\n    ],\n)\n\nmp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\nwith open(speech_file_path, \"wb\") as f:\n    f.write(mp3_bytes)\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Azure API Key\nDESCRIPTION: Configures the OpenAI client using an Azure API key for authentication. Requires the endpoint and API key from the Azure Portal.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        api_key=api_key,\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Assistant with File Search Enabled in Python\nDESCRIPTION: Demonstrates how to instantiate an OpenAI assistant with the File Search tool enabled by including 'file_search' in the assistant's tools parameter. It sets up the assistant with a name, instructions to use financial knowledge, the GPT-4o model, and specifies the file_search tool to allow document retrieval. Requires the openai Python SDK with beta features.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n \nclient = OpenAI()\n \nassistant = client.beta.assistants.create(\n  name=\"Financial Analyst Assistant\",\n  instructions=\"You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"file_search\"}],\n)\n```\n\n----------------------------------------\n\nTITLE: Uploading Title Vectors to Pinecone Index\nDESCRIPTION: This snippet uploads the title vectors from the DataFrame to the Pinecone index in the 'title' namespace.  It uses the `index.upsert()` method to upload the vectors with their corresponding IDs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Upsert title vectors in title namespace - this can also take a few minutes\nprint(\"Uploading vectors to title namespace..\")\nfor batch_df in df_batcher(article_df):\n    index.upsert(vectors=zip(batch_df.vector_id, batch_df.title_vector), namespace='title')\n```\n\n----------------------------------------\n\nTITLE: Processing Audio with GPT-4o API in Python\nDESCRIPTION: A function that sends an audio file to OpenAI's GPT-4o API for processing. It handles authentication with the API key, configures output modalities, and processes the audio based on the provided system prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/voice_translation_into_different_languages_using_GPT-4o.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Make sure requests package is installed  \nimport requests \nimport os\nimport json\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n\ndef process_audio_with_gpt_4o(base64_encoded_audio, output_modalities, system_prompt):\n    # Chat Completions API end point \n    url = \"https://api.openai.com/v1/chat/completions\"\n\n    # Set the headers\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Construct the request data\n    data = {\n        \"model\": \"gpt-4o-audio-preview\",\n        \"modalities\": output_modalities,\n        \"audio\": {\n            \"voice\": \"alloy\",\n            \"format\": \"wav\"\n        },\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"input_audio\",\n                        \"input_audio\": {\n                            \"data\": base64_encoded_audio,\n                            \"format\": \"wav\"\n                        }\n                    }\n                ]\n            }\n        ]\n    }\n    \n    request_response = requests.post(url, headers=headers, data=json.dumps(data))\n    if request_response.status_code == 200:\n        return request_response.json()\n    else:  \n        print(f\"Error {request_response.status_code}: {request_response.text}\")\n        return\n```\n\n----------------------------------------\n\nTITLE: Error Handling with OpenAI's Image API in Python\nDESCRIPTION: Demonstrates proper error handling when making requests to OpenAI's image API. Using try-except blocks, this snippet catches OpenAIError exceptions and extracts detailed error information including HTTP status and error details.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-python-tips.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nfrom openai import OpenAI\nclient = OpenAI()\n\ntry:\n  response = client.images.create_variation(\n    image=open(\"image_edit_mask.png\", \"rb\"),\n    n=1,\n    model=\"dall-e-2\",\n    size=\"1024x1024\"\n  )\n  print(response.data[0].url)\nexcept openai.OpenAIError as e:\n  print(e.http_status)\n  print(e.error)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Bash)\nDESCRIPTION: This command sets the `OPENAI_API_KEY` environment variable using the `export` command in a Bash shell context (often executed via `!` in a notebook). Replace `\"your API key\"` with your actual OpenAI API key. This makes the key available to subsequent processes launched from the same shell session, enabling API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your API key\"\n```\n\n----------------------------------------\n\nTITLE: Posting Audio for Translation via OpenAI API with cURL\nDESCRIPTION: This cURL command sends an audio file in a supported non-English language to the OpenAI translations endpoint. It uses multipart/form-data and includes the required headers, model, and audio file form fields. The output is English text from the provided audio. Authentication with a valid OpenAI API key is required.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_8\n\nLANGUAGE: curl\nCODE:\n```\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/translations \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@/path/to/file/german.mp3 \\\n  --form model=whisper-1\n```\n\n----------------------------------------\n\nTITLE: Defining Example Summarization Data in Python\nDESCRIPTION: Sets up the sample text excerpt, a human-written reference summary, and two system-generated summaries as Python strings. This data serves as the input for applying and comparing the ROUGE, BERTScore, and LLM-based evaluation metrics demonstrated in the notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexcerpt = \"OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. OpenAI will build safe and beneficial AGI directly, but will also consider its mission fulfilled if its work aids others to achieve this outcome. OpenAI follows several key principles for this purpose. First, broadly distributed benefits - any influence over AGI's deployment will be used for the benefit of all, and to avoid harmful uses or undue concentration of power. Second, long-term safety - OpenAI is committed to doing the research to make AGI safe, and to promote the adoption of such research across the AI community. Third, technical leadership - OpenAI aims to be at the forefront of AI capabilities. Fourth, a cooperative orientation - OpenAI actively cooperates with other research and policy institutions, and seeks to create a global community working together to address AGI's global challenges.\"\nref_summary = \"OpenAI aims to ensure artificial general intelligence (AGI) is used for everyone's benefit, avoiding harmful uses or undue power concentration. It is committed to researching AGI safety, promoting such studies among the AI community. OpenAI seeks to lead in AI capabilities and cooperates with global research and policy institutions to address AGI's challenges.\"\neval_summary_1 = \"OpenAI aims to AGI benefits all humanity, avoiding harmful uses and power concentration. It pioneers research into safe and beneficial AGI and promotes adoption globally. OpenAI maintains technical leadership in AI while cooperating with global institutions to address AGI challenges. It seeks to lead a collaborative worldwide effort developing AGI for collective good.\"\neval_summary_2 = \"OpenAI aims to ensure AGI is for everyone's use, totally avoiding harmful stuff or big power concentration. Committed to researching AGI's safe side, promoting these studies in AI folks. OpenAI wants to be top in AI things and works with worldwide research, policy groups to figure AGI's stuff.\"\n```\n\n----------------------------------------\n\nTITLE: Pretty-print extracting sections from prompts in Python with HTML formatting\nDESCRIPTION: This function extracts specific sections ('question', 'expert', 'submission') from a prompt string using custom markers, processes them with conditional splitting or replacement, and displays each section with distinct HTML color formatting in an IPython environment. The script depends on string methods, dict-based marker definitions, and IPython.display for rendering and is designed for visually debugging or presenting segmented prompt text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef pretty_print_text(prompt):\n    # Define markers for the sections\n    markers = {\n        \"question\": \"[Question]:\",\n        \"expert\": \"[Expert]:\",\n        \"submission\": \"[Submission]:\",\n        \"end\": \"[END DATA]\"\n    }\n    \n    # Function to extract text between markers\n    def extract_text(start_marker, end_marker):\n        start = prompt.find(start_marker) + len(start_marker)\n        end = prompt.find(end_marker)\n        text = prompt[start:end].strip()\n        if start_marker == markers[\"question\"]:\n            text = text.split(\"\\n\\nQuestion:\")[-1].strip() if \"\\n\\nQuestion:\" in text else text\n        elif start_marker == markers[\"submission\"]:\n            text = text.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n        return text\n    \n    # Extracting text for each section\n    question_text = extract_text(markers[\"question\"], markers[\"expert\"])\n    expert_text = extract_text(markers[\"expert\"], markers[\"submission\"])\n    submission_text = extract_text(markers[\"submission\"], markers[\"end\"])\n    \n    # HTML color codes and formatting\n    colors = {\n        \"question\": '<span style=\"color: #0000FF;\">QUESTION:<br>', \n        \"expert\": '<span style=\"color: #008000;\">EXPECTED:<br>',  \n        \"submission\": '<span style=\"color: #FFA500;\">SUBMISSION:<br>' \n    }\n    color_end = '</span>'\n    \n    # Display each section with color\n    from IPython.display import display, HTML\n    display(HTML(f\"{colors['question']}{question_text}{color_end}\"))\n    display(HTML(f\"{colors['expert']}{expert_text}{color_end}\"))\n    display(HTML(f\"{colors['submission']}{submission_text}{color_end}\"))\n```\n\n----------------------------------------\n\nTITLE: Deleting and Retrieving Weaviate Schema\nDESCRIPTION: Deletes the existing schema from Weaviate, then retrieves the schema to ensure its cleared and available. This prepares the environment for creating a new schema for the Article class.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n```\n\n----------------------------------------\n\nTITLE: Creating Alpha Channel Mask in Python\nDESCRIPTION: Converts a black and white mask image into an RGBA image with alpha channel, necessary for using the mask in OpenAI's image editing API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# 1. Load your black & white mask as a grayscale image\nmask = Image.open(img_path_mask).convert(\"L\")\n\n# 2. Convert it to RGBA so it has space for an alpha channel\nmask_rgba = mask.convert(\"RGBA\")\n\n# 3. Then use the mask itself to fill that alpha channel\nmask_rgba.putalpha(mask)\n\n# 4. Convert the mask into bytes\nbuf = BytesIO()\nmask_rgba.save(buf, format=\"PNG\")\nmask_bytes = buf.getvalue()\n```\n\n----------------------------------------\n\nTITLE: Downloading Precomputed Embeddings Data using wget in Python\nDESCRIPTION: Uses the `wget` library to download a zip archive containing precomputed Wikipedia article embeddings from a specified OpenAI CDN URL. This avoids the need to generate embeddings during the notebook execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Complex Hybrid Query with Multiple Data Types\nDESCRIPTION: A complex hybrid search query for \"brown belt\" with multiple conditions: exact year, multiple article type tags, and brand name text match, showing advanced filtering capabilities combining numeric, tag, and text fields.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# hybrid query for a brown belt filtering results by a year (NUMERIC) with a specific article types (TAG) and with a brand name (TEXT)\nresults = search_redis(redis_client,\n                       \"brown belt\",\n                       vector_field=\"product_vector\",\n                       k=10,\n                       hybrid_fields='(@year:[2012 2012] @articleType:{Shirts | Belts} @productDisplayName:\"Wrangler\")'\n                       )\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Embedding Function in Python\nDESCRIPTION: Defines a Python function `embed` that takes a list of text strings as input. It calls the OpenAI API (`openai.Embedding.create`) using the specified `OPENAI_ENGINE` to generate embeddings for the input texts. The function then extracts and returns a list containing only the embedding vectors from the API response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Simple function that converts the texts to embeddings\ndef embed(texts):\n    embeddings = openai.Embedding.create(\n        input=texts,\n        engine=OPENAI_ENGINE\n    )\n    return [x['embedding'] for x in embeddings['data']]\n```\n\n----------------------------------------\n\nTITLE: Define Custom GPT Instructions for BigQuery Interaction\nDESCRIPTION: Provides context and step-by-step instructions for a Custom GPT designed to interact with BigQuery. It instructs the GPT to act as a BigQuery SQL expert, first query the INFORMATION_SCHEMA to understand table structures using a predefined SQL query, then generate and test a SQL query based on the user's question (initially with a limit, then without), and finally return the SQL query. Default project/dataset values should be used unless specified by the user, and `useLegacySql` must be set to `false`. It also includes handling for initial interactions and suggesting a sample dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_bigquery.ipynb#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n**Context**: You are an expert at writing BigQuery SQL queries. A user is going to ask you a question. \n\n**Instructions**:\n1. No matter the user's question, start by running `runQuery` operation using this query: \"SELECT column_name, table_name, data_type, description FROM `{project}.{dataset}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS`\" \n-- Assume project = \"<insert your default project here>\", dataset = \"<insert your default dataset here>\", unless the user provides different values \n-- Remember to include useLegacySql:false in the json output\n2. Convert the user's question into a SQL statement that leverages the step above and run the `runQuery` operation on that SQL statement to confirm the query works. Add a limit of 100 rows\n3. Now remove the limit of 100 rows and return back the query for the user to see\n\n**Additional Notes**: If the user says \"Let's get started\", explain that the user can provide a project or dataset, along with a question they want answered. If the user has no ideas, suggest that we have a sample flights dataset they can query - ask if they want you to query that\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-embedded Data (Python)\nDESCRIPTION: This Python snippet loads pre-computed data, including vector embeddings for titles and content, into a pandas DataFrame. It requires `numpy`, `pandas`, and a helper module `nbutils` (presumably provided elsewhere in the project) which handles downloading and reading the specific Wikipedia dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom typing import List\n\n# use helper function in nbutils.py to download and read the data\n# this should take from 5-10 min to run\nif os.getcwd() not in sys.path:\n    sys.path.append(os.getcwd())\nimport nbutils\n\nnbutils.download_wikipedia_data()\ndata = nbutils.read_wikipedia_data()\n\ndata.head()\n```\n\n----------------------------------------\n\nTITLE: Performing headline classification with logprobs enabled for confidence scoring (Python example)\nDESCRIPTION: Re-runs classification prompts with logprobs and top_logprobs parameters enabled, then displays the token-level confidence scores and linear probabilities. Helps in assessing the certainty of classifications and thresholds for manual review.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfor headline in headlines:\n    print(f\"\\nHeadline: {headline}\")\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4o-mini\",\n        logprobs=True,\n        top_logprobs=2,\n    )\n    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n    html_content = \"\"\n    for i, logprob in enumerate(top_two_logprobs, start=1):\n        html_content += (\n            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n            f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n        )\n    display(HTML(html_content))\n    print(\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Querying GPT-4 with Augmented Query Python\nDESCRIPTION: Queries the GPT-4 model using the augmented query. The code constructs a system message to prime the model and passes the augmented query.  The result includes the original query and the retrieved contexts, and feeds them into the GPT-4 model to generate an answer backed by real data sources.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# system message to 'prime' the model\nprimer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\nuser questions based on the information provided by the user above\neach question. If the information can not be found in the information\nprovided by the user you truthfully say \\\"I don't know\\\".\n\"\"\"\n\nres = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": primer},\n        {\"role\": \"user\", \"content\": augmented_query}\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI API for Search Term Generation\nDESCRIPTION: This snippet uses the OpenAI API to generate a search term based on an original user query. It provides a system prompt that instructs the model to provide a concise search term. The input is the original search query; the output is a succinct search term suitable for use with search APIs such as Google's Custom Search API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsearch_term = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Provide a google search term based on search query provided below in 3-4 words\"},\n        {\"role\": \"user\", \"content\": search_query}]\n).choices[0].message.content\n\nprint(search_term)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: Initializes the OpenAI Python client by importing necessary modules and creating an instance of the OpenAI client. This setup is prerequisite for calling the Chat Completions API with structured output parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom textwrap import dedent\nfrom openai import OpenAI\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Obtaining OBO Token Using Microsoft Identity Platform - JavaScript\nDESCRIPTION: This snippet defines an asynchronous function to acquire an On-Behalf-Of (OBO) token from Microsoft, using the user's bearer token and environment variables for tenant and client configuration. It leverages the 'axios' and 'querystring' packages to POST a form-encoded request to the Microsoft OAuth2 token endpoint. Key parameters include the user's access token ('userAccessToken'), client credentials, and specific OAuth scopes. It returns the new access token or throws an error if the exchange fails. Ensure 'axios' and 'querystring' are installed and the appropriate environment variables are set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst axios = require('axios');\nconst qs = require('querystring');\n\nasync function getOboToken(userAccessToken) {\nÂ Â Â Â const { TENANT_ID, CLIENT_ID, MICROSOFT_PROVIDER_AUTHENTICATION_SECRET } = process.env;\nÂ Â Â Â const params = {\nÂ Â Â Â Â Â Â Â client_id: CLIENT_ID,\nÂ Â Â Â Â Â Â Â client_secret: MICROSOFT_PROVIDER_AUTHENTICATION_SECRET,\nÂ Â Â Â Â Â Â Â grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\nÂ Â Â Â Â Â Â Â assertion: userAccessToken,\nÂ Â Â Â Â Â Â Â requested_token_use: 'on_behalf_of',\nÂ Â Â Â Â Â Â Â scope: 'https://graph.microsoft.com/.default'\nÂ Â Â Â };\n\nÂ Â Â Â const url = `https://login.microsoftonline.com/${TENANT_ID}/oauth2/v2.0/token`;\nÂ Â Â Â try {\nÂ Â Â Â Â Â Â Â const response = await axios.post(url, qs.stringify(params), {\nÂ Â Â Â Â Â Â Â Â Â Â Â headers: { 'Content-Type': 'application/x-www-form-urlencoded' }\nÂ Â Â Â Â Â Â Â });\nÂ Â Â Â Â Â Â Â return response.data.access_token;\nÂ Â Â Â } catch (error) {\nÂ Â Â Â Â Â Â Â console.error('Error obtaining OBO token:', error.response?.data || error.message);\nÂ Â Â Â Â Â Â Â throw error;\nÂ Â Â Â }\n}\n```\n\n----------------------------------------\n\nTITLE: Measuring Time Saved with Streaming Completions\nDESCRIPTION: Demonstrates how to measure the time saved by streaming completions compared to the standard approach. This example collects and tracks each chunk with its arrival time.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example of an OpenAI ChatCompletion request with stream=True\n# https://platform.openai.com/docs/api-reference/streaming#chat/create-stream\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = client.chat.completions.create(\n    model='gpt-4o-mini',\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    temperature=0,\n    stream=True  # again, we set stream=True\n)\n# create variables to collect the stream of chunks\ncollected_chunks = []\ncollected_messages = []\n# iterate through the stream of events\nfor chunk in response:\n    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n    collected_chunks.append(chunk)  # save the event response\n    chunk_message = chunk.choices[0].delta.content  # extract the message\n    collected_messages.append(chunk_message)  # save the message\n    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n\n# print the time delay and text received\nprint(f\"Full response received {chunk_time:.2f} seconds after request\")\n# clean None in collected_messages\ncollected_messages = [m for m in collected_messages if m is not None]\nfull_reply_content = ''.join(collected_messages)\nprint(f\"Full conversation received: {full_reply_content}\")\n```\n\n----------------------------------------\n\nTITLE: Define OpenAPI Schema for Salesforce Service Cloud Case Management API\nDESCRIPTION: Specifies the OpenAPI 3.1.0 schema for a Salesforce Service Cloud Case Management API. It defines endpoints for updating case status (`PATCH /services/data/v60.0/sobjects/Case/{CaseId}`), deleting cases (`DELETE /services/data/v60.0/sobjects/Case/{CaseId}`), and retrieving case details by number (`GET /services/data/v60.0/query`). Includes path parameters, request bodies, response schemas, and operation IDs (`updateCaseStatus`, `deleteCase`, `getCaseDetailsFromNumber`). The server URL needs to be customized with the specific Salesforce instance domain.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce.ipynb#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Salesforce Service Cloud Case Update API\n  description: API for updating the status of Service Cloud tickets (cases) in Salesforce.\n  version: 1.0.3\nservers:\n  - url: https://your_instance.my.salesforce.com\n    description: Base URL for your Salesforce instance (replace 'your_instance' with your actual Salesforce domain)\npaths:\n  /services/data/v60.0/sobjects/Case/{CaseId}:\n    patch:\n      operationId: updateCaseStatus\n      summary: Updates the status of a Service Cloud case\n      description: Updates the status of a Service Cloud ticket based on the case ID number.\n      parameters:\n        - name: CaseId\n          in: path\n          required: true\n          description: The ID of the case to update.\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                Status:\n                  type: string\n                  description: The new status of the case.\n      responses:\n        '204':\n          description: Successfully updated the case status\n        '400':\n          description: Bad request - invalid input or case ID not found\n        '401':\n          description: Unauthorized - authentication required\n        '404':\n          description: Not Found - case ID does not exist\n    delete:\n      operationId: deleteCase\n      summary: Deletes a Service Cloud case\n      description: Deletes a Service Cloud ticket based on the case ID number.\n      parameters:\n        - name: CaseId\n          in: path\n          required: true\n          description: The ID of the case to delete.\n          schema:\n            type: string\n      responses:\n        '204':\n          description: Successfully deleted the case\n        '400':\n          description: Bad request - invalid case ID\n        '401':\n          description: Unauthorized - authentication required\n        '404':\n          description: Not Found - case ID does not exist\n  /services/data/v60.0/query:\n    get:\n      operationId: getCaseDetailsFromNumber\n      summary: Retrieves case details using a case number\n      description: Retrieves the details of a Service Cloud case associated with a given case number.\n      parameters:\n        - name: q\n          in: query\n          required: true\n          description: SOQL query string to find the Case details based on Case Number.\n          schema:\n            type: string\n            example: \"SELECT Id, CaseNumber, Status, Subject, Description FROM Case WHERE CaseNumber = '123456'\"\n      responses:\n        '200':\n          description: Successfully retrieved the case details\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  totalSize:\n                    type: integer\n                  done:\n                    type: boolean\n                  records:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        Id:\n                          type: string\n                        CaseNumber:\n                          type: string\n                        Status:\n                          type: string\n                        Subject:\n                          type: string\n                        Description:\n                          type: string\n        '400':\n          description: Bad request - invalid query\n        '401':\n          description: Unauthorized - authentication required\n        '404':\n          description: Not Found - case number does not exist\n```\n\n----------------------------------------\n\nTITLE: Chunking Iterable Data\nDESCRIPTION: This function batches an iterable into tuples of a specified length. It takes an iterable and a chunk size as input. The last batch may be shorter if the iterable's length isn't divisible by the chunk size.  It iterates through the iterable using an iterator and yields each batch as a tuple.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef batched(iterable, n):\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --> ABC DEF G\n    if n < 1:\n        raise ValueError('n must be at least one')\n    it = iter(iterable)\n    while (batch := tuple(islice(it, n))):\n        yield batch\n```\n\n----------------------------------------\n\nTITLE: Defining Pseudo-XML Diff Format Example in Python\nDESCRIPTION: This snippet defines a multi-line Python string demonstrating a pseudo-XML diff format. It wraps the file path, original code, and new code within XML-like tags (<file>, <old_code>, <new_code>). This format avoids internal escaping and clearly identifies the segments to replace for reliable diff application.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nPSEUDO_XML_DIFF_EXAMPLE = \"\"\"\n<edit>\n<file>\npath/to/file.py\n</file>\n<old_code>\ndef search():\n    pass\n</old_code>\n<new_code>\ndef search():\n   raise NotImplementedError()\n</new_code>\n</edit>\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Response of Step Log Retrieval\nDESCRIPTION: This JSON structure represents the typical response returned when retrieving logs of a run step via the API. It contains a list object with an array of step objects, each including identifiers, status, type, and detailed logs or outputs. This structure helps understand the returned data format and how to parse logs and outputs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"step_abc123\",\n      \"object\": \"thread.run.step\",\n      \"type\": \"tool_calls\",\n      \"run_id\": \"run_abc123\",\n      \"thread_id\": \"thread_abc123\",\n      \"status\": \"completed\",\n      \"step_details\": {\n        \"type\": \"tool_calls\",\n        \"tool_calls\": [\n          {\n            \"type\": \"code\",\n            \"code\": {\n              \"input\": \"# Calculating 2 + 2\\nresult = 2 + 2\\nresult\",\n              \"outputs\": [\n                {\n                  \"type\": \"logs\",\n                  \"logs\": \"4\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Functions for Weather Assistant in Python\nDESCRIPTION: This snippet defines the functions used by the weather assistant within the OpenAI Assistants API. It uses the `openai.OpenAI()` client to create an assistant with defined `tools`, specifying `get_current_temperature` and `get_rain_probability` with their respective parameters. Prerequisites: An OpenAI API key and the openai Python library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-function-calling.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n \nassistant = client.beta.assistants.create(\n  instructions=\"You are a weather bot. Use the provided functions to answer questions.\",\n  model=\"gpt-4o\",\n  tools=[\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_temperature\",\n        \"description\": \"Get the current temperature for a specific location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g., San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"Celsius\", \"Fahrenheit\"],\n              \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n            }\n          },\n          \"required\": [\"location\", \"unit\"]\n        }\n      }\n    },\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_rain_probability\",\n        \"description\": \"Get the probability of rain for a specific location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g., San Francisco, CA\"\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Milvus and OpenAI Integration\nDESCRIPTION: Installs the required Python libraries: `openai` for interacting with the OpenAI API, `pymilvus` for connecting to and managing the Milvus vector database, `datasets` for loading data from Hugging Face Hub, and `tqdm` for displaying progress bars during long operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! pip install openai pymilvus datasets tqdm\n```\n\n----------------------------------------\n\nTITLE: Extracting image embeddings from file paths using CLIP in Python\nDESCRIPTION: Preprocesses each image using CLIP's 'preprocess' method, stacks them into a batch tensor, and encodes the images to generate embeddings without gradient calculations. These embeddings are used for similarity search in the knowledge base.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_features_from_image_path(image_paths):\n  images = [preprocess(Image.open(image_path).convert(\"RGB\")) for image_path in image_paths]\n  image_input = torch.tensor(np.stack(images))\n  with torch.no_grad():\n    image_features = model.encode_image(image_input).float()\n  return image_features\nimage_features = get_features_from_image_path(image_paths)\n```\n\n----------------------------------------\n\nTITLE: Multi-turn Chat Examples with Weight Parameter for Selective Training\nDESCRIPTION: Advanced example showing how to use the weight parameter to selectively train on specific assistant messages in multi-turn conversations. Weight values of 0 or 1 control which messages are used for training.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_2\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\", \"weight\": 1}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"William Shakespeare\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\", \"weight\": 1}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"384,400 kilometers\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\", \"weight\": 1}]}\n```\n\n----------------------------------------\n\nTITLE: Batch Adding Texts to Deep Lake Vector Store Using Python\nDESCRIPTION: Iterates over dataset samples in batches, extracts ids, text, and metadata, and adds them to the Deep Lake vector store using the add_texts method. This batch processing facilitates handling large datasets efficiently, with the batch size and sample count configurable. Includes a progress bar for monitoring.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\n\nbatch_size = 100\n\nnsamples = 10  # for testing. Replace with len(ds) to append everything\nfor i in tqdm(range(0, nsamples, batch_size)):\n    # find end of batch\n    i_end = min(nsamples, i + batch_size)\n\n    batch = ds[i:i_end]\n    id_batch = batch.ids.data()[\"value\"]\n    text_batch = batch.text.data()[\"value\"]\n    meta_batch = batch.metadata.data()[\"value\"]\n\n    db.add_texts(text_batch, metadatas=meta_batch, ids=id_batch)\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI API Helper Function\nDESCRIPTION: Creates a utility function to send requests to the OpenAI API and extract the model's response content, with o1-preview as the default model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_model_response(messages, model=\"o1-preview\"):\n    response = client.chat.completions.create(\n        messages=messages,\n        model=model,\n    )\n    return response.choices[0].message.content\n\n\ncomplex_prompt = get_model_response([{\"role\": \"user\", \"content\": meta_prompt.format(simple_prompt=simple_prompt)}])\ncomplex_prompt\n```\n\n----------------------------------------\n\nTITLE: Checking Fine-tuning Job Status\nDESCRIPTION: Retrieving the status of the fine-tuning job to determine when it has completed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfine_tune_results = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\nprint(fine_tune_results.finished_at)\n```\n\n----------------------------------------\n\nTITLE: Generating Embedding for Search Query (Python/OpenAI)\nDESCRIPTION: Calls the previously defined `embed` function to generate a vector embedding for the specific search term \"places where you worship\". This vector will be used for the semantic search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nsearchedEmbedding = embed(\"places where you worship\")\n#print(searchedEmbedding)\n```\n\n----------------------------------------\n\nTITLE: Translating Audio to English using OpenAI Whisper API in Python\nDESCRIPTION: This Python code uses the OpenAI SDK to translate an audio file in a supported non-English language (e.g., German) into English text via the Whisper model translation endpoint. The request structure is similar to transcription, but uses the 'translations.create' method. The primary parameter is the audio file path, and the dependency is the OpenAI Python package. The output is English text regardless of input language.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\naudio_file= open(\"/path/to/file/german.mp3\", \"rb\")\ntranslation = client.audio.translations.create(\n  model=\"whisper-1\", \n  file=audio_file\n)\nprint(translation.text)\n```\n\n----------------------------------------\n\nTITLE: Returning Files with openaiFileResponse in API Responses - JSON\nDESCRIPTION: This JSON snippet outlines the structure of the openaiFileResponse parameter for returning files via an API response. Each file is represented as a JSON object including name, mime_type, and base64-encoded content. The use case is to allow APIs to deliver up to 10 files per request, each up to 10MB (non-image, non-video formats only). This supports workflows like sending documents, spreadsheets, or PDFs back to the user, where the output is immediately accessible in the ChatGPT session. Inputs to this structure are JSON responses; limitations include file type and size restrictions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"name\": \"example_document.pdf\",\n    \"mime_type\": \"application/pdf\",\n    \"content\": \"JVBERi0xLjQKJcfsj6IKNSAwIG9iago8PC9MZW5ndGggNiAwIFIvRmlsdGVyIC9GbGF0ZURlY29kZT4+CnN0cmVhbQpHhD93PQplbmRzdHJlYW0KZW5kb2JqCg==\"\n  },\n  {\n    \"name\": \"sample_spreadsheet.csv\",\n    \"mime_type\": \"text/csv\",\n    \"content\": \"iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries for dataset processing and token counting\nDESCRIPTION: Imports modules for JSON handling, token encoding with tiktoken, numerical computations with numpy, and default dictionary usage. Sets the foundation for subsequent dataset manipulation and analysis tasks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport json\nimport tiktoken  # for token counting\nimport numpy as np\nfrom collections import defaultdict\n```\n\n----------------------------------------\n\nTITLE: Splitting Text into Chunks Python\nDESCRIPTION: Splits the data into smaller chunks based on the specified chunk size and overlap using the `text_splitter`. The code iterates through the `data` list, applies the `text_splitter.split_text` method on each document's text, and extends the `chunks` list with the results. Each chunk is structured as a dictionary containing an ID, text content, chunk index, and URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom uuid import uuid4\nfrom tqdm.auto import tqdm\n\nchunks = []\n\nfor idx, record in enumerate(tqdm(data)):\n    texts = text_splitter.split_text(record['text'])\n    chunks.extend([{\n        'id': str(uuid4()),\n        'text': texts[i],\n        'chunk': i,\n        'url': record['url']\n    } for i in range(len(texts))])\n```\n\n----------------------------------------\n\nTITLE: Validating OpenAI API Key via Environment Variables - Python\nDESCRIPTION: Checks whether the OpenAI API key is properly set as an environment variable named 'OPENAI_API_KEY'. Uses the 'os' module to access environment variables and provides instructions for setting the variable if not found. No external dependencies except Python standard library. Expects 'OPENAI_API_KEY' to be present in the environment. Outputs a success or failure message to console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Baseline Mean Prediction - Python\nDESCRIPTION: This snippet computes the Mean Squared Error (MSE) and Mean Absolute Error (MAE) of a baseline model that predicts the mean of the target variable (review score) for all test instances. It serves as a simple comparison to evaluate the performance of the RandomForestRegressor model. It leverages NumPy and scikit-learn's metrics. The output provides MSE and MAE values to demonstrate how well a constant prediction model performs compared to more sophisticated machine learning models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Regression_using_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbmse = mean_squared_error(y_test, np.repeat(y_test.mean(), len(y_test)))\nbmae = mean_absolute_error(y_test, np.repeat(y_test.mean(), len(y_test)))\nprint(\n    f\"Dummy mean prediction performance on Amazon reviews: mse={bmse:.2f}, mae={bmae:.2f}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Redis Search Index Fields\nDESCRIPTION: This snippet defines the schema for the Redis search index using RediSearch fields for each column in the dataset, including TextField, TagField, NumericField, and VectorField. The VectorField represents the product embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define RediSearch fields for each of the columns in the dataset\nname = TextField(name=\"productDisplayName\")\ncategory = TagField(name=\"masterCategory\")\narticleType = TagField(name=\"articleType\")\ngender = TagField(name=\"gender\")\nseason = TagField(name=\"season\")\nyear = NumericField(name=\"year\")\ntext_embedding = VectorField(\"product_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": 1536,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": NUMBER_OF_VECTORS,\n    }\n)\nfields = [name, category, articleType, gender, season, year, text_embedding]\n```\n\n----------------------------------------\n\nTITLE: Repeated Embedding Generation with OpenAI API in Python (Inefficient Example)\nDESCRIPTION: This snippet illustrates a naive approach to generating multiple embeddings by making individual API calls inside a loop without any rate limit handling. It uses the openai package to create embeddings for each item in a large sequence and prints their lengths, but may lead to rapid API rate limiting for high workloads. Dependencies include the openai package and an API key; input is a looped sequence of text strings, and output is the length of each returned embedding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_embeddings.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Negative example (slow and rate-limited)\nfrom openai import OpenAI\nclient = OpenAI()\n\nnum_embeddings = 10000 # Some large number\nfor i in range(num_embeddings):\n    embedding = client.embeddings.create(\n        input=\"Your text goes here\", model=\"text-embedding-3-small\"\n    ).data[0].embedding\n    print(len(embedding))\n\n```\n\n----------------------------------------\n\nTITLE: Clearing Existing Schema in Weaviate\nDESCRIPTION: Deletes all existing schemas from the Weaviate instance to prepare for creating a new schema. This ensures a clean slate before defining the new data structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Clear up the schema, so that we can recreate it\nclient.schema.delete_all()\nclient.schema.get()\n```\n\n----------------------------------------\n\nTITLE: Generating Fine-tuning Datasets for Q&A and Discriminator Models in Python\nDESCRIPTION: Defines two helper functions: `get_random_similar_contexts` uses OpenAI search (via a deprecated `openai.Engine` call) to find similar contexts for hard negative examples, and `create_fine_tuning_dataset` constructs prompt-completion pairs suitable for fine-tuning. This function generates positive examples, random negative examples, and optionally hard negative examples derived from the same article or semantically similar contexts. It can be configured to create datasets for either a Q&A model or a discriminator model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\ndef get_random_similar_contexts(question, context, file_id=olympics_search_fileid, search_model='ada', max_rerank=10):\n    \"\"\"\n    Find similar contexts to the given context using the search file\n    \"\"\"\n    try:\n        # TODO: openai.Engine(search_model) is deprecated\n        results = openai.Engine(search_model).search(\n            search_model=search_model, \n            query=question, \n            max_rerank=max_rerank,\n            file=file_id\n        )\n        candidates = []\n        for result in results['data'][:3]:\n            if result['text'] == context:\n                continue\n            candidates.append(result['text'])\n        random_candidate = random.choice(candidates)\n        return random_candidate\n    except Exception as e:\n        print(e)\n        return \"\"\n\ndef create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):\n    \"\"\"\n    Create a dataset for fine tuning the OpenAI model; either for a discriminator model, \n    or a model specializing in Q&A, where it says if no relevant context is found.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        The dataframe containing the question, answer and context pairs\n    discriminator: bool\n        Whether to create a dataset for the discriminator\n    n_negative: int\n        The number of random negative samples to add (using a random context)\n    add_related: bool\n        Whether to add the related contexts to the correct context. These are hard negative examples\n\n    Returns\n    -------\n    pd.DataFrame\n        The dataframe containing the prompts and completions, ready for fine-tuning\n    \"\"\"\n    rows = []\n    for i, row in df.iterrows():\n        for q, a in zip((\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')):\n            if len(q) >10 and len(a) >10:\n                if discriminator:\n                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})\n                else:\n                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})\n\n    for i, row in df.iterrows():\n        for q in (\"1.\" + row.questions).split('\\n'):\n            if len(q) >10:\n                for j in range(n_negative + (2 if add_related else 0)):\n                    random_context = \"\"\n                    if j == 0 and add_related:\n                        # add the related contexts based on originating from the same wikipedia page\n                        subset = df[(df.title == row.title) & (df.context != row.context)]\n                        \n                        if len(subset) < 1:\n                            continue\n                        random_context = subset.sample(1).iloc[0].context\n                    if j == 1 and add_related:\n                        # add the related contexts based on the most similar contexts according to the search\n                        random_context = get_random_similar_contexts(q[2:].strip(), row.context, search_model='ada', max_rerank=10)\n                    else:\n                        while True:\n                            # add random context, which isn't the correct context\n                            random_context = df.sample(1).iloc[0].context\n                            if random_context != row.context:\n                                break\n                    if discriminator:\n                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})\n                    else:\n                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})\n\n    return pd.DataFrame(rows) \n```\n\n----------------------------------------\n\nTITLE: Listing Courses API Endpoint Definition (YAML)\nDESCRIPTION: This snippet defines the `/courses` endpoint for retrieving a list of courses. It specifies the HTTP method (GET), operation ID, summary, and description for the endpoint. It also details the parameters, including query parameters for filtering the course list, and the expected responses, including the response codes, descriptions, and schema for the returned course objects.  Dependencies include the Canvas LMS API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_canvas.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n  /courses:\n    get:\n      operationId: listYourCourses\n      summary: List your courses\n      description: Retrieves a paginated list of active courses for the current user.\n      parameters:\n        - name: enrollment_type\n          in: query\n          description: Filter by enrollment type (e.g., \"teacher\", \"student\").\n          schema:\n            type: string\n        - name: enrollment_role\n          in: query\n          description: Filter by role type. Requires admin permissions.\n          schema:\n            type: string\n        - name: enrollment_state\n          in: query\n          description: Filter by enrollment state (e.g., \"active\", \"invited\").\n          schema:\n            type: string\n        - name: exclude_blueprint_courses\n          in: query\n          description: Exclude Blueprint courses if true.\n          schema:\n            type: boolean\n        - name: include\n          in: query\n          description: Array of additional information to include (e.g., \"term\", \"teachers\").\n          schema:\n            type: array\n            items:\n              type: string\n        - name: per_page\n          in: query\n          description: The number of results to return per page.\n          schema:\n            type: integer\n          example: 10\n        - name: page\n          in: query\n          description: The page number to return.\n          schema:\n            type: integer\n          example: 1\n      responses:\n        '200':\n          description: A list of courses.\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  type: object\n                  properties:\n                    id:\n                      type: integer\n                      description: The ID of the course.\n                    name:\n                      type: string\n                      description: The name of the course.\n                    account_id:\n                      type: integer\n                      description: The ID of the account associated with the course.\n                    enrollment_term_id:\n                      type: integer\n                      description: The ID of the term associated with the course.\n                    start_at:\n                      type: string\n                      format: date-time\n                      description: The start date of the course.\n                    end_at:\n                      type: string\n                      format: date-time\n                      description: The end date of the course.\n                    course_code:\n                      type: string\n                      description: The course code.\n                    state:\n                      type: string\n                      description: The current state of the course (e.g., \"unpublished\", \"available\").\n        '400':\n          description: Bad request, possibly due to invalid query parameters.\n        '401':\n          description: Unauthorized, likely due to invalid authentication credentials.\n```\n\n----------------------------------------\n\nTITLE: Defining Google Drive API Actions with OpenAPI Schema (JSON)\nDESCRIPTION: This OpenAPI 3.1.0 schema defines interactions with the Google Drive API v3 for use within Custom GPT Actions. It includes endpoints for listing files ('/files'), getting file metadata ('/files/{fileId}'), and exporting files ('/files/{fileId}/export'). The schema specifies parameters like query strings, file IDs, MIME types, and defines expected JSON responses or binary file content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_google_drive.ipynb#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"openapi\": \"3.1.0\",\n  \"info\": {\n    \"title\": \"Google Drive API\",\n    \"description\": \"API for interacting with Google Drive\",\n    \"version\": \"1.0.0\"\n  },\n  \"servers\": [\n    {\n      \"url\": \"https://www.googleapis.com/drive/v3\"\n    }\n  ],\n  \"paths\": {\n    \"/files\": {\n      \"get\": {\n        \"operationId\": \"ListFiles\",\n        \"summary\": \"List files\",\n        \"description\": \"Retrieve a list of files in the user's Google Drive.\",\n        \"parameters\": [\n          {\n            \"name\": \"q\",\n            \"in\": \"query\",\n            \"description\": \"Query string for searching files.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          },\n          {\n            \"name\": \"includeItemsFromAllDrives\",\n            \"in\": \"query\",\n            \"description\": \"Whether both My Drive and shared drive items should be included in results.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          },\n          {\n            \"name\": \"supportsAllDrives\",\n            \"in\": \"query\",\n            \"description\": \"Whether the requesting application supports both My Drives and shared drives.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          },\n          {\n            \"name\": \"pageSize\",\n            \"in\": \"query\",\n            \"description\": \"Maximum number of files to return.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"integer\",\n              \"default\": 10\n            }\n          },\n          {\n            \"name\": \"pageToken\",\n            \"in\": \"query\",\n            \"description\": \"Token for continuing a previous list request.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          },\n          {\n            \"name\": \"fields\",\n            \"in\": \"query\",\n            \"description\": \"Comma-separated list of fields to include in the response.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"A list of files.\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"kind\": {\n                      \"type\": \"string\",\n                      \"example\": \"drive#fileList\"\n                    },\n                    \"nextPageToken\": {\n                      \"type\": \"string\",\n                      \"description\": \"Token to retrieve the next page of results.\"\n                    },\n                    \"files\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"string\"\n                          },\n                          \"name\": {\n                            \"type\": \"string\"\n                          },\n                          \"mimeType\": {\n                            \"type\": \"string\"\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"/files/{fileId}\": {\n      \"get\": {\n        \"operationId\": \"getMetadata\",\n        \"summary\": \"Get file metadata\",\n        \"description\": \"Retrieve metadata for a specific file.\",\n        \"parameters\": [\n          {\n            \"name\": \"fileId\",\n            \"in\": \"path\",\n            \"description\": \"ID of the file to retrieve.\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          },\n          {\n            \"name\": \"fields\",\n            \"in\": \"query\",\n            \"description\": \"Comma-separated list of fields to include in the response.\",\n            \"required\": false,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Metadata of the file.\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"id\": {\n                      \"type\": \"string\"\n                    },\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"mimeType\": {\n                      \"type\": \"string\"\n                    },\n                    \"description\": {\n                      \"type\": \"string\"\n                    },\n                    \"createdTime\": {\n                      \"type\": \"string\",\n                      \"format\": \"date-time\"\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"/files/{fileId}/export\": {\n      \"get\": {\n        \"operationId\": \"export\",\n        \"summary\": \"Export a file\",\n        \"description\": \"Export a Google Doc to the requested MIME type.\",\n        \"parameters\": [\n          {\n            \"name\": \"fileId\",\n            \"in\": \"path\",\n            \"description\": \"ID of the file to export.\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\"\n            }\n          },\n          {\n            \"name\": \"mimeType\",\n            \"in\": \"query\",\n            \"description\": \"The MIME type of the format to export to.\",\n            \"required\": true,\n            \"schema\": {\n              \"type\": \"string\",\n              \"enum\": [\n                \"application/pdf\",\n                \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n                \"text/plain\"\n              ]\n            }\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"The exported file.\",\n            \"content\": {\n              \"application/pdf\": {\n                \"schema\": {\n                  \"type\": \"string\",\n                  \"format\": \"binary\"\n                }\n              },\n              \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\": {\n                \"schema\": {\n                  \"type\": \"string\",\n                  \"format\": \"binary\"\n                }\n              },\n              \"text/plain\": {\n                \"schema\": {\n                  \"type\": \"string\",\n                  \"format\": \"binary\"\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Invalid MIME type or file ID.\"\n          },\n          \"404\": {\n            \"description\": \"File not found.\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Cypher Queries Based on Embedded Entities in Python\nDESCRIPTION: Constructs a dynamic Cypher query string used to match products in the graph database by comparing their relationships to embeddings of user-provided entities. It parses the input JSON text to extract entity keys and generates MATCH clauses for entities, leveraging the cosine similarity function from the Neo4j Graph Data Science library with a configurable similarity threshold. The generated query returns products whose related entities have embeddings sufficiently similar to the input. Dependencies include 'json' for parsing and an existing mapping 'entity_relationship_match' that links entity types to relationship names.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# The threshold defines how closely related words should be. Adjust the threshold to return more or less results\ndef create_query(text, threshold=0.81):\n    query_data = json.loads(text)\n    # Creating embeddings\n    embeddings_data = []\n    for key, val in query_data.items():\n        if key != 'product':\n            embeddings_data.append(f\"${key}Embedding AS {key}Embedding\")\n    query = \"WITH \" + \",\\n\".join(e for e in embeddings_data)\n    # Matching products to each entity\n    query += \"\\nMATCH (p:Product)\\nMATCH \"\n    match_data = []\n    for key, val in query_data.items():\n        if key != 'product':\n            relationship = entity_relationship_match[key]\n            match_data.append(f\"(p)-[:{relationship}]->({key}Var:{key})\")\n    query += \",\\n\".join(e for e in match_data)\n    similarity_data = []\n    for key, val in query_data.items():\n        if key != 'product':\n            similarity_data.append(f\"gds.similarity.cosine({key}Var.embedding, ${key}Embedding) > {threshold}\")\n    query += \"\\nWHERE \"\n    query += \" AND \".join(e for e in similarity_data)\n    query += \"\\nRETURN p\"\n    return query\n```\n\n----------------------------------------\n\nTITLE: Defining Box API Schema and OAuth2 Security for OpenAPI (Partial)\nDESCRIPTION: This JSON snippet defines various data structures like Events, SearchResults, MetadataTemplates, and MetadataInstances, along with the OAuth2 security scheme required for authenticating with the Box API. It's part of a larger OpenAPI specification used to configure interactions between an application (like ChatGPT Actions) and Box.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n          \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"event_id\": {\n                  \"type\": \"string\",\n                  \"description\": \"The ID of the event\"\n                },\n                \"event_type\": {\n                  \"type\": \"string\",\n                  \"description\": \"The type of the event\"\n                },\n                \"created_at\": {\n                  \"type\": \"string\",\n                  \"format\": \"date-time\",\n                  \"description\": \"The time the event occurred\"\n                }\n              }\n            }\n          }\n        }\n      },\n      \"SearchResults\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"total_count\": {\n            \"type\": \"integer\",\n            \"description\": \"The total number of search results\"\n          },\n          \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"type\": {\n                  \"type\": \"string\",\n                  \"description\": \"The type of the item (e.g., file, folder)\"\n                },\n                \"id\": {\n                  \"type\": \"string\",\n                  \"description\": \"The ID of the item\"\n                },\n                \"name\": {\n                  \"type\": \"string\",\n                  \"description\": \"The name of the item\"\n                }\n              }\n            }\n          }\n        }\n      },\n      \"MetadataTemplates\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"templateKey\": {\n              \"type\": \"string\",\n              \"description\": \"The key of the metadata template\"\n            },\n            \"displayName\": {\n              \"type\": \"string\",\n              \"description\": \"The display name of the metadata template\"\n            },\n            \"scope\": {\n              \"type\": \"string\",\n              \"description\": \"The scope of the metadata template\"\n            }\n          }\n        }\n      },\n      \"MetadataInstances\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"templateKey\": {\n              \"type\": \"string\",\n              \"description\": \"The key of the metadata template\"\n            },\n            \"type\": {\n              \"type\": \"string\",\n              \"description\": \"The type of the metadata instance\"\n            },\n            \"attributes\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"description\": \"Attributes of the metadata instance\"\n            }\n          }\n        }\n      }\n    },\n    \"securitySchemes\": {\n      \"OAuth2\": {\n        \"type\": \"oauth2\",\n        \"flows\": {\n          \"authorizationCode\": {\n            \"authorizationUrl\": \"https://account.box.com/api/oauth2/authorize\",\n            \"tokenUrl\": \"https://api.box.com/oauth2/token\",\n            \"scopes\": {\n              \"read:folders\": \"Read folders\",\n              \"read:files\": \"Read files\",\n              \"search:items\": \"Search items\",\n              \"read:metadata\": \"Read metadata\",\n              \"read:metadata_templates\": \"Read metadata templates\",\n              \"read:events\": \"Read events\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompts for Specialized Agents in OpenAI Python API\nDESCRIPTION: This code defines multi-line system prompt strings for each agent role: triaging, data processing, analysis, and visualization, to be used when creating agent configurations in the OpenAI API. Each prompt outlines the responsibilities of the agent and the specific tools it can use, aiding clear task delegation and modular workflow. This snippet has no functional dependencies but assumes subsequent usage in prompt-passing for API calls. Inputs and outputs are not present here; only variable assignments are made.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntriaging_system_prompt = \"\"\"You are a Triaging Agent. Your role is to assess the user's query and route it to the relevant agents. The agents available are:\n- Data Processing Agent: Cleans, transforms, and aggregates data.\n- Analysis Agent: Performs statistical, correlation, and regression analysis.\n- Visualization Agent: Creates bar charts, line charts, and pie charts.\n\nUse the send_query_to_agents tool to forward the user's query to the relevant agents. Also, use the speak_to_user tool to get more information from the user if needed.\"\"\"\n\nprocessing_system_prompt = \"\"\"You are a Data Processing Agent. Your role is to clean, transform, and aggregate data using the following tools:\n- clean_data\n- transform_data\n- aggregate_data\"\"\"\n\nanalysis_system_prompt = \"\"\"You are an Analysis Agent. Your role is to perform statistical, correlation, and regression analysis using the following tools:\n- stat_analysis\n- correlation_analysis\n- regression_analysis\"\"\"\n\nvisualization_system_prompt = \"\"\"You are a Visualization Agent. Your role is to create bar charts, line charts, and pie charts using the following tools:\n- create_bar_chart\n- create_line_chart\n- create_pie_chart\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating Fine-tuned Model with Prompts\nDESCRIPTION: This snippet evaluates the performance of a fine-tuned model and a base model on a set of challenging prompts. It uses an `eval` function (not defined in the provided context, assumes defined elsewhere) to assess how well each model performs. It prints messages indicating which model is being evaluated and uses predefined system prompts and function lists to guide the evaluation process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nft_model = \"ft:gpt-3.5-turbo-0125:openai-gtm:drone:9atiPjeC\"\nbase_model = \"gpt-3.5-turbo\"\n\nprint(f\"\\nEvaluating fine-tuned model with challenging prompts: {ft_model}\")\neval(\n    model=ft_model,\n    function_list=modified_function_list,\n    system_prompt=DRONE_SYSTEM_PROMPT,\n    prompts_to_expected_tool_name=challenging_prompts_to_expected,\n)\n\nprint(f\"\\nEvaluating base model with challenging prompts: {base_model}\")\neval(\n    model=\"gpt-3.5-turbo\",\n    function_list=function_list,\n    system_prompt=DRONE_SYSTEM_PROMPT,\n    prompts_to_expected_tool_name=challenging_prompts_to_expected,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Thread with High Detail Image (cURL)\nDESCRIPTION: This code snippet shows how to create a thread with a message containing an image URL and specifying the 'high' detail level using cURL.  This allows the model to create detailed crops of the image for better understanding. Requires setting the `$OPENAI_API_KEY` environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_8\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What is this an image of?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://example.com/image.png\",\n              \"detail\": \"high\"\n            }\n          },\n        ]\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Creating a Thread\nDESCRIPTION: This code creates a new thread using the OpenAI Assistants API. It calls `client.beta.threads.create()` to instantiate an empty thread object. The function `show_json` is then called to display the details of the created thread.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create()\nshow_json(thread)\n```\n\n----------------------------------------\n\nTITLE: Loading Multi-Page PDF Documents with SimpleDirectoryReader in Python\nDESCRIPTION: Loads and parses PDF documents using SimpleDirectoryReader by specifying input PDF files paths. Each PDF is split into separate Document objects by page, converting the content into plain text suitable for further processing. This step is resource intensive given the document length (~100+ pages).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlyft_docs = SimpleDirectoryReader(input_files=[\"../data/10k/lyft_2021.pdf\"]).load_data()\nuber_docs = SimpleDirectoryReader(input_files=[\"../data/10k/uber_2021.pdf\"]).load_data()\n```\n\n----------------------------------------\n\nTITLE: Generating Spoken Audio Using OpenAI TTS API via cURL (HTTP Request)\nDESCRIPTION: This snippet demonstrates how to invoke OpenAI's TTS API directly with a POST HTTP request using cURL. The example includes required headers for authorization and JSON content type, and illustrates how to send the model, voice, and text as JSON data. The API's binary audio response is saved directly as an MP3 file. Required dependencies include a valid OpenAI API key and a Unix-like environment with cURL installed. Key parameters are 'model', 'voice', and 'input'. The output is a file, speech.mp3, containing the spoken audio. Limitations follow OpenAI API voice/language support and endpoint rate limits.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-to-speech.txt#_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/audio/speech \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1\",\n    \"input\": \"Today is a wonderful day to build something people love!\",\n    \"voice\": \"alloy\"\n  }' \\\n  --output speech.mp3\n```\n\n----------------------------------------\n\nTITLE: Retrieving AWS Redshift Serverless VPC Network Information via AWS CLI\nDESCRIPTION: This AWS CLI command retrieves key network configuration details for an Amazon Redshift Serverless workgroup, including endpoint address, port, security group IDs, and subnet IDs. This information is critical for configuring AWS Lambda functions or other services to properly connect to Redshift instances within a VPC. The command requires AWS CLI installed and credentials configured with permissions to query Redshift Serverless resources.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_1\n\nLANGUAGE: awscli\nCODE:\n```\naws redshift-serverless get-workgroup --workgroup-name default-workgroup --query 'workgroup.{address: endpoint.address, port: endpoint.port, SecurityGroupIds: securityGroupIds, SubnetIds: subnetIds}'\n```\n\n----------------------------------------\n\nTITLE: Initializing Text Splitter Python\nDESCRIPTION: Initializes a `RecursiveCharacterTextSplitter` object from `langchain.text_splitter`. The text splitter is configured to split text into chunks of ~400 tokens with an overlap of 20 tokens. The `tiktoken_len` function is used to measure the length of the text chunks, and it separates based on `\n\n`, `\n`, ` `, and ``.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=400,\n    chunk_overlap=20,\n    length_function=tiktoken_len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Semantic Text Search Using Embeddings and Cosine Similarity\nDESCRIPTION: Implements a semantic search function that finds the most relevant documents in a dataset based on a query. It uses cosine similarity between the embedding vectors to rank results by relevance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\n\ndef search_reviews(df, product_description, n=3, pprint=True):\n   embedding = get_embedding(product_description, model='text-embedding-3-small')\n   df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n   res = df.sort_values('similarities', ascending=False).head(n)\n   return res\n\nres = search_reviews(df, 'delicious beans', n=3)\n```\n\n----------------------------------------\n\nTITLE: Performing Weaviate Hybrid Search (Modern Art) - Python\nDESCRIPTION: Calls the `hybrid_query_weaviate` function with a specific query related to 'modern art' and an alpha value for blending vector and keyword search results. It then iterates through and prints the top search results along with their scores. Requires the `hybrid_query_weaviate` function and a populated 'Article' collection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nquery_result = hybrid_query_weaviate(\"modern art in Europe\", \"Article\", 0.5)\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']} (Score: {article['_additional']['score']})\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Tool Call Metadata and Displaying with pandas DataFrame in Python\nDESCRIPTION: This code snippet iterates through the API's response output, building a detailed list of tool calls (including type, ID, output, and name) and then displays this as a pandas DataFrame. The snippet demonstrates how to capture and format intermediate tool call metadata for analysis or debugging. Required dependencies are the Python pandas package and access to the Responses API output. Inputs include an API response with multiple tool calls. Outputs a tabular DataFrame summarizing the relevant details of each tool invocation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n\n# Create a list to store the tool call and function call details\ntool_calls = []\n\n# Iterate through the response output and collect the details\nfor i in response.output:\n    tool_calls.append({\n        \"Type\": i.type,\n        \"Call ID\": i.call_id if hasattr(i, 'call_id') else i.id if hasattr(i, 'id') else \"N/A\",\n        \"Output\": str(i.output) if hasattr(i, 'output') else \"N/A\",\n        \"Name\": i.name if hasattr(i, 'name') else \"N/A\"\n    })\n\n# Convert the list to a DataFrame for tabular display\ndf_tool_calls = pd.DataFrame(tool_calls)\n\n# Display the DataFrame\ndf_tool_calls\n\n```\n\n----------------------------------------\n\nTITLE: Fetching Responses from OpenAI Chat Model\nDESCRIPTION: This function `fetch_response` sends a series of messages to the OpenAI Chat API using `gpt-4o` model and retrieves multiple choices of model responses. The messages include a system prompt and example user-assistant interactions as context for generating the current response. It's essential for initiating the dialog with the end user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef fetch_response(policy):\n    messages = [\n        { \"role\": \"system\", \"content\": system_input_prompt},\n        { \"role\": \"user\", \"content\": user_example_1},\n        { \"role\": \"assistant\", \"content\": assistant_example_1},\n        { \"role\": \"user\", \"content\": user_example_2},\n        { \"role\": \"assistant\", \"content\": assistant_example_2},\n        { \"role\": \"user\", \"content\": policy}\n    ]\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7,\n        n=10\n    )\n    return response.choices\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Interactions (Pandas)\nDESCRIPTION: This snippet converts the first interaction from the list `customer_interactions` to a dictionary and then creates a Pandas DataFrame. The `display_dataframe` function, assumed to be defined elsewhere, is used to display the resulting DataFrame in a user-friendly format. It's crucial for structuring the response data for analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ninteraction_dict = json.loads(customer_interactions[0])\n\ndf_interaction = pd.DataFrame([interaction_dict])\n\n# Pretty print the DataFrame\ndisplay_dataframe(df_interaction)\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving Embeddings for Clothing Dataset in Python\nDESCRIPTION: Generates OpenAI embeddings for the 'productDisplayName' column of styles_df and writes the result to a new CSV. This utilizes the previously defined generate_embeddings to compute the embeddings, then calls pandas' to_csv to persist the data for future use. The input is assumed to be a DataFrame with a string column; the output is a CSV with an extra embeddings column. The operation logs progress and completion messages to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngenerate_embeddings(styles_df, 'productDisplayName')\nprint(\"Writing embeddings to file ...\")\nstyles_df.to_csv('data/sample_clothes/sample_styles_with_embeddings.csv', index=False)\nprint(\"Embeddings successfully stored in sample_styles_with_embeddings.csv\")\n```\n\n----------------------------------------\n\nTITLE: Filtering and Finding Initial Fashion Item Matches (Python)\nDESCRIPTION: This snippet filters a DataFrame of clothing items (`styles_df`) based on gender and category, finds initial matching items using a RAG-based function (`find_matching_items_with_rag`), prepares image paths and HTML for display, and prints the item descriptions used for matching. It assumes `styles_df` is a pandas DataFrame and requires functions like `find_matching_items_with_rag` and display utilities (`display`, `HTML`) to be defined elsewhere.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# Filter data such that we only look through the items of the same gender (or unisex) and different category\nfiltered_items = styles_df.loc[styles_df['gender'].isin([item_gender, 'Unisex'])]\nfiltered_items = filtered_items[filtered_items['articleType'] != item_category]\nprint(str(len(filtered_items)) + \" Remaining Items\")\n\n# Find the most similar items based on the input item descriptions\nmatching_items = find_matching_items_with_rag(filtered_items, item_descs)\n\n# Display the matching items (this will display 2 items for each description in the image analysis)\nhtml = \"\"\npaths = []\nfor i, item in enumerate(matching_items):\n    item_id = item['id']\n        \n    # Path to the image file\n    image_path = f'data/sample_clothes/sample_images/{item_id}.jpg'\n    paths.append(image_path)\n    html += f'<img src=\"{image_path}\" style=\"display:inline;margin:1px\"/>'\n\n# Print the matching item description as a reminder of what we are looking for\nprint(item_descs)\n# Display the image\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Processing Files for Embedding\nDESCRIPTION: This function processes a single file, extracting text, generating embeddings for title and content, and categorizing the text.  It reads content from both .txt and .pdf files. It uses `extract_text_from_pdf`, `len_safe_get_embedding`, and `categorize_text`. It takes file path, index, categories list and embedding model as input. The output is the data for the document with title, text, vector, and category.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef process_file(file_path, idx, categories, embeddings_model):\n    file_name = os.path.basename(file_path)\n    print(f\"Processing file {idx + 1}: {file_name}\")\n    \n    # Read text content from .txt files\n    if file_name.endswith('.txt'):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n    # Extract text content from .pdf files\n    elif file_name.endswith('.pdf'):\n        text = extract_text_from_pdf(file_path)\n    \n    title = file_name\n    # Generate embeddings for the title\n    title_vectors, title_text = len_safe_get_embedding(title, embeddings_model)\n    print(f\"Generated title embeddings for {file_name}\")\n    \n    # Generate embeddings for the content\n    content_vectors, content_text = len_safe_get_embedding(text, embeddings_model)\n    print(f\"Generated content embeddings for {file_name}\")\n    \n    category = categorize_text(' '.join(content_text), categories)\n    print(f\"Categorized {file_name} as {category}\")\n    \n    # Prepare the data to be appended\n    data = []\n    for i, content_vector in enumerate(content_vectors):\n        data.append({\n            \"id\": f\"{idx}_{i}\",\n            \"vector_id\": f\"{idx}_{i}\",\n            \"title\": title_text[0],\n            \"text\": content_text[i],\n            \"title_vector\": json.dumps(title_vectors[0]),  # Assuming title is short and has only one chunk\n            \"content_vector\": json.dumps(content_vector),\n            \"category\": category\n        })\n        print(f\"Appended data for chunk {i + 1}/{len(content_vectors)} of {file_name}\")\n    \n    return data\n```\n\n----------------------------------------\n\nTITLE: Reducing embedding dimensionality with PCA in Python\nDESCRIPTION: This snippet applies PCA from scikit-learn to reduce high-dimensional embeddings to 3 components for visualization purposes. It fits the PCA model to the embeddings and transforms them, then stores the 3D coordinates back into the DataFrame. Dependencies include scikit-learn, with inputs being the embedding matrix and outputs being the coordinate list.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\nvis_dims = pca.fit_transform(matrix)\nsamples[\"embed_vis\"] = vis_dims.tolist()\n```\n\n----------------------------------------\n\nTITLE: Querying Database for Matching Products and Returning Simplified Result List in Python\nDESCRIPTION: Defines 'query_db' that queries the graph using 'query_graph' with given parameters and formats the results as a list of dictionaries containing only product 'id' and 'name'. This function acts as a wrapper abstraction simplifying the returned data format for downstream consumption.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef query_db(params):\n    matches = []\n    # Querying the db\n    result = query_graph(params)\n    for r in result:\n        product_id = r['p']['id']\n        matches.append({\n            \"id\": product_id,\n            \"name\":r['p']['name']\n        })\n    return matches    \n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries for Vector Operations\nDESCRIPTION: This code imports standard and third-party libraries needed for vector store management and API integration: 'getpass' for securely handling sensitive user input, 'collections.Counter' for summarizing authors, 'cassio' and 'cassio.table.MetadataVectorCassandraTable' for interaction with Cassandra/Astra DB, 'openai' for embedding generation, and 'datasets' for dataset access. These imports are prerequisites for subsequent steps in the workflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom getpass import getpass\nfrom collections import Counter\n\nimport cassio\nfrom cassio.table import MetadataVectorCassandraTable\n\nimport openai\nfrom datasets import load_dataset\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Enriched Text with Markdown\nDESCRIPTION: This code snippet takes the `result` from the previous OpenAI task and displays the original text and the enriched text using Markdown formatting.  It assumes that `result['function_response']` contains the enriched text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndisplay(Markdown(f\"\"\"**Text:** {text}   \n                     **Enriched_Text:** {result['function_response']} \"\"\"))\n```\n\n----------------------------------------\n\nTITLE: Displaying Grouped Generative Search Results from Weaviate in Python\nDESCRIPTION: Prints the OpenAI-generated groupedResult from the first entry in the result list produced by generative_search_group. This is intended to provide one summary or explanation covering all articles retrieved by the group generative query. Users should ensure that the input function is successfully executed and the result structure matches expectations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery_result = generative_search_group(\"football clubs\", \"Article\")\n\nprint (query_result[0]['_additional']['generate']['groupedResult'])\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and Environment for Evals - Python\nDESCRIPTION: This snippet imports required packages (openai, pydantic), imports types, and configures the environment with your OpenAI API key. Dependencies: openai, pydantic, and a valid OpenAI API key (provided via environment variable). It establishes the foundation for all further API calls and model usage. Expected inputs are environment variables; outputs are available via the openai package. No user-level function is exposed here.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\nfrom openai.types.chat import ChatCompletion\nimport pydantic\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\")\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Provide Recommendations - Python\nDESCRIPTION: This Python function `provide_user_specific_recommendations` orchestrates fetching a user profile, interacting with the OpenAI GPT-3.5-Turbo model configured for function calling, and processing the AI's response. It includes logic to call a `call_google_places_api` function based on the model's suggestion, returning tailored recommendations or appropriate error messages. It requires a user profile fetching mechanism (`fetch_customer_profile`) and an implementation of `call_google_places_api`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef provide_user_specific_recommendations(user_input, user_id):\n    customer_profile = fetch_customer_profile(user_id)\n    if customer_profile is None:\n        return \"I couldn't find your profile. Could you please verify your user ID?\"\n\n    customer_profile_str = json.dumps(customer_profile)\n\n    food_preference = customer_profile.get('preferences', {}).get('food', [])[0] if customer_profile.get('preferences', {}).get('food') else None\n\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n    {\n        \"role\": \"system\",\n        \"content\": f\"You are a sophisticated AI assistant, a specialist in user intent detection and interpretation. Your task is to perceive and respond to the user's needs, even when they're expressed in an indirect or direct manner. You excel in recognizing subtle cues: for example, if a user states they are 'hungry', you should assume they are seeking nearby dining options such as a restaurant or a cafe. If they indicate feeling 'tired', 'weary', or mention a long journey, interpret this as a request for accommodation options like hotels or guest houses. However, remember to navigate the fine line of interpretation and assumption: if a user's intent is unclear or can be interpreted in multiple ways, do not hesitate to politely ask for additional clarification. Make sure to tailor your responses to the user based on their preferences and past experiences which can be found here {customer_profile_str}\"\n    },\n    {\"role\": \"user\", \"content\": user_input}\n],\n        temperature=0,\n        tools=[\n            {\n                \"type\": \"function\",\n                \"function\" : {\n                    \"name\": \"call_google_places_api\",\n                    \"description\": \"This function calls the Google Places API to find the top places of a specified type near a specific location. It can be used when a user expresses a need (e.g., feeling hungry or tired) or wants to find a certain type of place (e.g., restaurant or hotel).\",\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"place_type\": {\n                                \"type\": \"string\",\n                                \"description\": \"The type of place to search for.\"\n                            }\n                        }\n                    },\n                    \"result\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\"\n                        }\n                    }\n                }\n            }\n        ],\n    )\n\n    print(response.choices[0].message.tool_calls)\n\n    if response.choices[0].finish_reason=='tool_calls':\n        function_call = response.choices[0].message.tool_calls[0].function\n        if function_call.name == \"call_google_places_api\":\n            place_type = json.loads(function_call.arguments)[\"place_type\"]\n            places = call_google_places_api(user_id, place_type, food_preference)\n            if places:  # If the list of places is not empty\n                return f\"Here are some places you might be interested in: {' '.join(places)}\"\n            else:\n                return \"I couldn't find any places of interest nearby.\"\n\n    return \"I am sorry, but I could not understand your request.\"\n\n```\n\n----------------------------------------\n\nTITLE: Generating Product Descriptions with GPT-4o mini\nDESCRIPTION: Defines a function to generate detailed descriptions of furniture items using GPT-4o mini, taking both the image and product title as input to ensure accuracy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndescribe_system_prompt = '''\n    You are a system generating descriptions for furniture items, decorative items, or furnishings on an e-commerce website.\n    Provided with an image and a title, you will describe the main item that you see in the image, giving details but staying concise.\n    You can describe unambiguously what the item is and its material, color, and style if clearly identifiable.\n    If there are multiple items depicted, refer to the title to understand which item you should describe.\n    '''\n\ndef describe_image(img_url, title):\n    response = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    temperature=0.2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": describe_system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": img_url,\n                    }\n                },\n            ],\n        },\n        {\n            \"role\": \"user\",\n            \"content\": title\n        }\n    ],\n    max_tokens=300,\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Completion with OpenAI\nDESCRIPTION: This Python code demonstrates creating a chat completion using the OpenAI API.  It defines a system message and a user message to guide the model's response.  It requires the `openai` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Extracting Invoice Data with GPT-4o (Python)\nDESCRIPTION: This function extracts invoice data from a base64 encoded image using the GPT-4o model. It constructs a system prompt instructing the model to extract data from the invoice image and output it as JSON, handling details like language, blank fields, and table structures.  It then calls the OpenAI API with the image and prompt, and returns the extracted JSON string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef extract_invoice_data(base64_image):\n    system_prompt = f\"\"\"\n    You are an OCR-like data extraction tool that extracts hotel invoice data from PDFs.\n   \n    1. Please extract the data in this hotel invoice, grouping data according to theme/sub groups, and then output into JSON.\n\n    2. Please keep the keys and values of the JSON in the original language. \n\n    3. The type of data you might encounter in the invoice includes but is not limited to: hotel information, guest information, invoice information,\n    room charges, taxes, and total charges etc. \n\n    4. If the page contains no charge data, please output an empty JSON object and don't make up any data.\n\n    5. If there are blank data fields in the invoice, please include them as \\\"null\\\" values in the JSON object.\n    \n    6. If there are tables in the invoice, capture all of the rows and columns in the JSON object. \n    Even if a column is blank, include it as a key in the JSON object with a null value.\n    \n    7. If a row is blank denote missing fields with \\\"null\\\" values. \n    \n    8. Don't interpolate or make up data.\n\n    9. Please maintain the table structure of the charges, i.e. capture all of the rows and columns in the JSON object.\n\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_format={ \"type\": \"json_object\" },\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"extract the data in this hotel invoice and output into JSON \"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\", \"detail\": \"high\"}}\n                ]\n            }\n        ],\n        temperature=0.0,\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing a Code-Only Query and Recommendation Function in Python\nDESCRIPTION: Defines an 'answer' function that handles freeform text queries by users, maps them to search parameters, queries a database, and performs a fallback similarity search if no matches are found. The function prints progress and matching items, looks up and prints similar items using item IDs, and returns the primary search results. Depends on external functions: define_query, query_db, similarity_search, and query_similar_items. Inputs are a prompt string and optional similar_items_limit. Outputs are prints to the console and a list of matching results or an apologetic message if no matches are found.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nimport logging\n\ndef answer(prompt, similar_items_limit=10):\n    print(f'Prompt: \"{prompt}\"\\n')\n    params = define_query(prompt)\n    print(params)\n    result = query_db(params)\n    print(f\"Found {len(result)} matches with Query function.\\n\")\n    if len(result) == 0:\n        result = similarity_search(prompt)\n        print(f\"Found {len(result)} matches with Similarity search function.\\n\")\n        if len(result) == 0:\n            return \"I'm sorry, I did not find a match. Please try again with a little bit more details.\"\n    print(f\"I have found {len(result)} matching items:\\n\")\n    similar_items = []\n    for r in result:\n        similar_items.extend(query_similar_items(r['id']))\n        print(f\"{r['name']} ({r['id']})\")\n    print(\"\\n\")\n    if len(similar_items) > 0:\n        print(\"Similar items that might interest you:\\n\")\n        for i in similar_items[:similar_items_limit]:\n            print(f\"{i['name']} ({i['id']})\")\n    print(\"\\n\\n\\n\")\n    return result\n```\n\n----------------------------------------\n\nTITLE: Calculating Embeddings and Cosine Similarities\nDESCRIPTION: This code snippet calculates embeddings for the text pairs and computes the cosine similarity between those embeddings. It first establishes a cache to store embeddings to avoid recomputing them. The `get_embedding_with_cache` function retrieves embeddings from the cache or calculates them using an embedding API, saving them to the cache afterward. Then the code iterates through the text columns to get embeddings and computes cosine similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# establish a cache of embeddings to avoid recomputing\n# cache is a dict of tuples (text, engine) -> embedding\ntry:\n    with open(embedding_cache_path, \"rb\") as f:\n        embedding_cache = pickle.load(f)\nexcept FileNotFoundError:\n    precomputed_embedding_cache_path = \"https://cdn.openai.com/API/examples/data/snli_embedding_cache.pkl\"\n    embedding_cache = pd.read_pickle(precomputed_embedding_cache_path)\n\n\n# this function will get embeddings from the cache and save them there afterward\ndef get_embedding_with_cache(\n    text: str,\n    engine: str = default_embedding_engine,\n    embedding_cache: dict = embedding_cache,\n    embedding_cache_path: str = embedding_cache_path,\n) -> list:\n    if (text, engine) not in embedding_cache.keys():\n        # if not in cache, call API to get embedding\n        embedding_cache[(text, engine)] = get_embedding(text, engine)\n        # save embeddings cache to disk after each update\n        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n            pickle.dump(embedding_cache, embedding_cache_file)\n    return embedding_cache[(text, engine)]\n\n\n# create column of embeddings\nfor column in [\"text_1\", \"text_2\"]:\n    df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n\n# create column of cosine similarity between embeddings\ndf[\"cosine_similarity\"] = df.apply(\n    lambda row: cosine_similarity(row[\"text_1_embedding\"], row[\"text_2_embedding\"]),\n    axis=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Batching Context Retrieval Evaluation with Pandas DataFrame in Python\nDESCRIPTION: This snippet applies the check_context function batchwise on rows of a pandas DataFrame, extracting question text, adjusting for possible formatting artifacts, and collecting search results. It processes the questions per context block, filters out short or empty questions, and collects ADA search outcomes for entire datasets. Requires pandas and the check_context function, and assumes a DataFrame with appropriate columns is available.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nada_results = df.apply(lambda x: [\n                    check_context( x.title, \n                                   x.heading, \n                                   q[3:],     # remove the number prefix\n                                   max_len=1000000, # set a large number to get the full context \n                                   search_model='ada', \n                                   max_rerank=200,\n                                 ) \n                    for q in (x.questions).split('\\n') # split the questions\n                    if len(q) >10 # remove the empty questions\n                ], axis=1)\nada_results.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI client authentication in Python\nDESCRIPTION: Sets up authentication to the Azure OpenAI service either with Azure Active Directory (AAD) or API key. The snippet uses variables for endpoint, API key, and deployment name, and creates an AzureOpenAI client accordingly. For AAD, it obtains bearer tokens via DefaultAzureCredential and integrates with AzureOpenAI client securely, enabling access to OpenAI embeddings and models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nendpoint: str = \"YOUR_AZURE_OPENAI_ENDPOINT\"\napi_key: str = \"YOUR_AZURE_OPENAI_KEY\"\napi_version: str = \"2023-05-15\"\ndeployment = \"YOUR_AZURE_OPENAI_DEPLOYMENT_NAME\"\ncredential = DefaultAzureCredential()\ntoken_provider = get_bearer_token_provider(\n    credential, \"https://cognitiveservices.azure.com/.default\"\n)\n\n# Set this flag to True if you are using Azure Active Directory\nuse_aad_for_aoai = True \n\nif use_aad_for_aoai:\n    # Use Azure Active Directory (AAD) authentication\n    client = AzureOpenAI(\n        azure_endpoint=endpoint,\n        api_version=api_version,\n        azure_ad_token_provider=token_provider,\n    )\nelse:\n    # Use API key authentication\n    client = AzureOpenAI(\n        api_key=api_key,\n        api_version=api_version,\n        azure_endpoint=endpoint,\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Product Data from JSON File in Python\nDESCRIPTION: Opens and reads product data from a specified JSON file ('data/amazon_product_kg.json'). The content is loaded into a Python variable `jsonData` using the `json.load()` method. This requires the `json` library and the specified file path to be correct.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Loading a json dataset from a file\nfile_path = 'data/amazon_product_kg.json'\n\nwith open(file_path, 'r') as file:\n    jsonData = json.load(file)\n```\n\n----------------------------------------\n\nTITLE: Upsert Data into Qdrant Collection\nDESCRIPTION: Iterates through the pandas DataFrame and upserts (inserts or updates) data points into the 'Articles' collection. Each point includes a unique ID (DataFrame index), named vectors ('title', 'content'), and a payload containing metadata (original 'id', 'title', 'url'). Uses tqdm for progress visualization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom qdrant_client.models import PointStruct # Import the PointStruct to store the vector and payload\nfrom tqdm import tqdm # Library to show the progress bar \n\n# Populate collection with vectors using tqdm to show progress\nfor k, v in tqdm(article_df.iterrows(), desc=\"Upserting articles\", total=len(article_df)):\n    try:\n        qdrant.upsert(\n            collection_name='Articles',\n            points=[\n                PointStruct(\n                    id=k,\n                    vector={'title': v['title_vector'], \n                            'content': v['content_vector']},\n                    payload={\n                        'id': v['id'],\n                        'title': v['title'],\n                        'url': v['url']\n                    }\n                )\n            ]\n        )\n    except Exception as e:\n        print(f\"Failed to upsert row {k}: {v}\")\n        print(f\"Exception: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GPT Model Limitations by Asking Recent Events Questions in Python\nDESCRIPTION: These two Python snippets illustrate that GPT models like gpt-4o and gpt-4o-mini cannot answer questions about events occurring after their training cutoff (October 2023). Each snippet sends a chat.completions request via the OpenAI API client, providing a system message and user question about the 2024 Olympics or US 2024 Elections. The model responses are printed, showing outdated or no knowledge. Key parameters include the model selected from predefined GPT_MODELS and temperature set to zero for deterministic answers. This highlights the need for external knowledge insertion or embedding-based search to provide up-to-date information.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# an example question about the 2022 Olympics\nquery = 'Which athletes won the most number of gold medals in 2024 Summer Olympics?'\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You answer questions about the 2024 Games or latest events.\"},\n        {\"role\": \"user\", \"content\": query},\n    ],\n    model=GPT_MODELS[0],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\nLANGUAGE: python\nCODE:\n```\n# an example question about the 2024 Elections\nquery = 'Who won the elections in the US in 2024?'\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You answer questions about the 2024 Games or latest events.\"},\n        {\"role\": \"user\", \"content\": query},\n    ],\n    model=GPT_MODELS[1],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Python client and data libraries for Weaviate project\nDESCRIPTION: Installs the necessary Python libraries, including weaviate-client, datasets, and apache-beam, required for connecting to Weaviate, loading datasets, and processing data within a Python environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Install the Weaviate client for Python\n!pip install weaviate-client>=3.11.0\n\n# Install datasets and apache-beam to load the sample datasets\n!pip install datasets apache-beam\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting up OpenAI Client in Python\nDESCRIPTION: Imports essential libraries for data manipulation (pandas, numpy), file I/O (json, ast), tokenization (tiktoken), parallel processing (concurrent), OpenAI API integration, and rich output in Jupyter. Initializes the OpenAI client and sets global constants for model names and pricing. Requires that all previously listed dependencies are installed. No direct input or output; establishes the environment and configuration for downstream AI operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport json\nimport ast\nimport tiktoken\nimport concurrent\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nfrom IPython.display import Image, display, HTML\nfrom typing import List\n\nclient = OpenAI()\n\nGPT_MODEL = \"gpt-4o-mini\"\nEMBEDDING_MODEL = \"text-embedding-3-large\"\nEMBEDDING_COST_PER_1K_TOKENS = 0.00013\n```\n\n----------------------------------------\n\nTITLE: BatchGenerator Class for DataFrame Chunking\nDESCRIPTION: This class is used to split a Pandas DataFrame into smaller batches.  It takes a batch_size as input during initialization. The to_batches method divides the DataFrame into chunks based on the batch_size.  The splits_num method calculates the number of chunks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Models a simple batch generator that make chunks out of an input DataFrame\nclass BatchGenerator:\n    \n    \n    def __init__(self, batch_size: int = 10) -> None:\n        self.batch_size = batch_size\n    \n    # Makes chunks out of an input DataFrame\n    def to_batches(self, df: pd.DataFrame) -> Iterator[pd.DataFrame]:\n        splits = self.splits_num(df.shape[0])\n        if splits <= 1:\n            yield df\n        else:\n            for chunk in np.array_split(df, splits):\n                yield chunk\n\n    # Determines how many chunks DataFrame contains\n    def splits_num(self, elements: int) -> int:\n        return round(elements / self.batch_size)\n    \n    __call__ = to_batches\n\ndf_batcher = BatchGenerator(300)\n```\n\n----------------------------------------\n\nTITLE: Test connection to Tair server with ping\nDESCRIPTION: Sends a ping command to verify connectivity with the Tair database. If successful, confirms the connection; otherwise, indicates a failure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclient.ping()\n```\n\n----------------------------------------\n\nTITLE: Hybrid Query with Multiple Field Constraints\nDESCRIPTION: A hybrid search query for \"blue sandals\" that combines vector search with multiple filters: year range of 2011-2012 and summer season tag, demonstrating more complex query conditions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# hybrid query for sandals in the product vector and only include results within the 2011-2012 year range from the summer season\nresults = search_redis(redis_client,\n                       \"blue sandals\",\n                       vector_field=\"product_vector\",\n                       k=10,\n                       hybrid_fields='(@year:[2011 2012] @season:{Summer})'\n                       )\n```\n\n----------------------------------------\n\nTITLE: Querying the Document QA System with Example Questions in Python\nDESCRIPTION: Example code that demonstrates using the question-answering system with specific queries. It shows how to ask questions about data in visual elements like pie charts and tables in the document.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What percentage was allocated to social protections in Western and Central Africa?\"\nanswer = get_response_to_question(question, index)\n\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Creating a Fine-tuning Job\nDESCRIPTION: Uploading the prepared training and validation files to OpenAI and initiating a fine-tuning job with the babbage-002 model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_file = client.files.create(file=open(\"sport2_prepared_train.jsonl\", \"rb\"), purpose=\"fine-tune\")\nvalid_file = client.files.create(file=open(\"sport2_prepared_valid.jsonl\", \"rb\"), purpose=\"fine-tune\")\n\nfine_tuning_job = client.fine_tuning.jobs.create(training_file=train_file.id, validation_file=valid_file.id, model=\"babbage-002\")\n\nprint(fine_tuning_job)\n```\n\n----------------------------------------\n\nTITLE: Saving Edited Image\nDESCRIPTION: This code saves the edited image received from the OpenAI API to the file system.  It extracts the URL of the edited image, downloads the image content using `requests.get`, and then writes the image to a file in the image directory. The file name is set to edited_image.png.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# save the image\nedited_image_name = \"edited_image.png\"  # any name you like; the filetype should be .png\nedited_image_filepath = os.path.join(image_dir, edited_image_name)\nedited_image_url = edit_response.data[0].url  # extract image URL from response\nedited_image = requests.get(edited_image_url).content  # download the image\n\nwith open(edited_image_filepath, \"wb\") as image_file:\n    image_file.write(edited_image)  # write the image to the file\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Response with the Responses API\nDESCRIPTION: Creates a simple response using the Responses API with GPT-4o-mini model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.responses.create(\n    model=\"gpt-4o-mini\",\n    input=\"tell me a joke\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing RediSearch HNSW Vector Fields for Fast Approximate Search - Python\nDESCRIPTION: Creates RediSearch vector fields of type 'HNSW' (Hierarchical Navigable Small World) for efficient approximate nearest neighbor search suitable for large datasets. Requires prior definition of VECTOR_DIM, DISTANCE_METRIC, and VECTOR_NUMBER constants. Prepares fields for both title and content vectors using FLOAT32 vectors, to be incorporated in a Redis index schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n# re-define RediSearch vector fields to use HNSW index\ntitle_embedding = VectorField(\"title_vector\",\n    \"HNSW\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER\n    }\n)\ntext_embedding = VectorField(\"content_vector\",\n    \"HNSW\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER\n    }\n)\nfields = [title, url, text, title_embedding, text_embedding]\n```\n\n----------------------------------------\n\nTITLE: Displaying top similar images from the database in Python\nDESCRIPTION: Iterates through the sorted list of nearest neighbor indices and respective distances, retrieves corresponding images from the database, and displays them. This visually showcases the retrieval results based on embedding similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor idx, distance in indices_distances:\n    print(idx)\n    path = get_image_paths(direc, idx)[0]\n    im = Image.open(path)\n    plt.imshow(im)\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Zero-shot Transaction Classification Using OpenAI Chat Completion in Python\nDESCRIPTION: Defines a prompt template and functions to classify transactions into five predefined categories using zero-shot classification with OpenAI's chat completions API. The 'format_prompt' function builds the prompt from a transaction's details, and 'classify_transaction' sends this prompt as a system message to GPT-4 to obtain a category label with limited tokens and deterministic output (temperature=0). The methodology enables classification without labeled training data by leveraging prompt-based guidance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nzero_shot_prompt = '''You are a data expert working for the National Library of Scotland.\nYou are analysing all transactions over Â£25,000 in value and classifying them into one of five categories.\nThe five categories are Building Improvement, Literature & Archive, Utility Bills, Professional Services and Software/IT.\nIf you can't tell what it is, say Could not classify\n\nTransaction:\n\nSupplier: {}\nDescription: {}\nValue: {}\n\nThe classification is:'''\n\ndef format_prompt(transaction):\n    return zero_shot_prompt.format(transaction['Supplier'], transaction['Description'], transaction['Transaction value (Â£)'])\n\ndef classify_transaction(transaction):\n\n    \n    prompt = format_prompt(transaction)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n    ]\n    completion_response = openai.chat.completions.create(\n                            messages=messages,\n                            temperature=0,\n                            max_tokens=5,\n                            top_p=1,\n                            frequency_penalty=0,\n                            presence_penalty=0,\n                            model=COMPLETIONS_MODEL)\n    label = completion_response.choices[0].message.content.replace('\\n','')\n    return label\n\n```\n\n----------------------------------------\n\nTITLE: Batch Embedding and Insertion of Quotes into Vector Store - Python\nDESCRIPTION: This code computes vector embeddings for batches of philosopher quotes and inserts each into the Cassandra/Astra DB vector store along with metadata. By batching (default batch size 50), it reduces API calls and speeds up processing. For each quote, the code prepares metadata containing the author and tag fields, then uses 'v_table.put' to store the quote's text, its embedding, and associated metadata. Progress is reported with print statements. All dependencies from earlier steps (database connection, OpenAI client, and loaded dataset) must be satisfied before running.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 50\n\nnum_batches = ((len(philo_dataset) + BATCH_SIZE - 1) // BATCH_SIZE)\n\nquotes_list = philo_dataset[\"quote\"]\nauthors_list = philo_dataset[\"author\"]\ntags_list = philo_dataset[\"tags\"]\n\nprint(\"Starting to store entries:\")\nfor batch_i in range(num_batches):\n    b_start = batch_i * BATCH_SIZE\n    b_end = (batch_i + 1) * BATCH_SIZE\n    # compute the embedding vectors for this batch\n    b_emb_results = client.embeddings.create(\n        input=quotes_list[b_start : b_end],\n        model=embedding_model_name,\n    )\n    # prepare the rows for insertion\n    print(\"B \", end=\"\")\n    for entry_idx, emb_result in zip(range(b_start, b_end), b_emb_results.data):\n        if tags_list[entry_idx]:\n            tags = {\n                tag\n                for tag in tags_list[entry_idx].split(\";\")\n            }\n        else:\n            tags = set()\n        author = authors_list[entry_idx]\n        quote = quotes_list[entry_idx]\n        v_table.put(\n            row_id=f\"q_{author}_{entry_idx}\",\n            body_blob=quote,\n            vector=emb_result.embedding,\n            metadata={**{tag: True for tag in tags}, **{\"author\": author}},\n        )\n        print(\"*\", end=\"\")\n    print(f\" done ({len(b_emb_results.data)})\")\n\nprint(\"\\nFinished storing entries.\")\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving O365/SharePoint Drive Item Content and Encoding as Base64 - JavaScript\nDESCRIPTION: This JavaScript function fetches the contents of a drive item from Microsoft Graph, reads the data stream, and converts it to a base64 string. It collects file metadata (such as name and MIME type), structuring the result for OpenAI-compatible file upload. Dependencies include a properly authenticated Microsoft Graph client, Buffer, and API permissions. Inputs are the Graph client, drive ID, item ID, and file name. The function emits a structured object and robustly handles download or API errors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst getDriveItemContent = async (client, driveId, itemId, name) => {\nÂ Â Â try\nÂ Â Â Â Â Â Â const filePath = `/drives/${driveId}/items/${itemId}`;\nÂ Â Â Â Â Â Â const downloadPath = filePath + `/content`\nÂ Â Â Â Â Â Â // this is where we get the contents and convert to base64\nÂ Â Â Â Â Â Â const fileStream = await client.api(downloadPath).getStream();\nÂ Â Â Â Â Â Â let chunks = [];\nÂ Â Â Â Â Â Â Â Â Â Â for await (let chunk of fileStream) {\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â chunks.push(chunk);\nÂ Â Â Â Â Â Â Â Â Â Â }\nÂ Â Â Â Â Â Â const base64String = Buffer.concat(chunks).toString('base64');\nÂ Â Â Â Â Â Â // this is where we get the other metadata to include in response\nÂ Â Â Â Â Â Â const file = await client.api(filePath).get();\nÂ Â Â Â Â Â Â const mime_type = file.file.mimeType;\nÂ Â Â Â Â Â Â const name = file.name;\nÂ Â Â Â Â Â Â return {\"name\":name, \"mime_type\":mime_type, \"content\":base64String}\nÂ Â Â } catch (error) {\nÂ Â Â Â Â Â Â console.error('Error fetching drive content:', error);\nÂ Â Â Â Â Â Â throw new Error(`Failed to fetch content for ${name}: ${error.message}`);\nÂ Â Â }\n\n```\n\n----------------------------------------\n\nTITLE: Converting PDF to Base64 Images (Python)\nDESCRIPTION: This function converts a PDF file to a list of base64 encoded images, one for each page. It uses the fitz library to open the PDF, iterates through each page, converts the page to a pixmap, saves the pixmap as a PNG image, encodes the image to base64, and appends the base64 string to a list. Temporary image files are deleted after encoding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport fitz  # PyMuPDF\nimport io\nimport os\nfrom PIL import Image\nimport base64\n\ndef pdf_to_base64_images(pdf_path):\n    #Handles PDFs with multiple pages\n    pdf_document = fitz.open(pdf_path)\n    base64_images = []\n    temp_image_paths = []\n\n    total_pages = len(pdf_document)\n\n    for page_num in range(total_pages):\n        page = pdf_document.load_page(page_num)\n        pix = page.get_pixmap()\n        img = Image.open(io.BytesIO(pix.tobytes()))\n        temp_image_path = f\"temp_page_{page_num}.png\"\n        img.save(temp_image_path, format=\"PNG\")\n        temp_image_paths.append(temp_image_path)\n        base64_image = encode_image(temp_image_path)\n        base64_images.append(base64_image)\n\n    for temp_image_path in temp_image_paths:\n        os.remove(temp_image_path)\n\n    return base64_images\n```\n\n----------------------------------------\n\nTITLE: Creating Faithfulness Evaluator with gpt-4 - Python\nDESCRIPTION: This code instantiates a `FaithfulnessEvaluator` using the `gpt-4` service context.  The `FaithfulnessEvaluator` is a module within LlamaIndex designed to assess the factual consistency of generated responses with respect to the source content. This sets up the tool to evaluate the responses of the RAG pipeline.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.evaluation import FaithfulnessEvaluator\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n```\n\n----------------------------------------\n\nTITLE: Implementing the `apply_patch` Utility (Python)\nDESCRIPTION: A self-contained Python 3.9+ script providing a reference implementation for the `apply_patch` tool. This utility parses and applies patches specified in the custom V4A diff format (described in `APPLY_PATCH_TOOL_DESC`) to text files. It defines data structures for changes (`FileChange`, `Commit`), action types (`ActionType`), and includes error handling (`DiffError`). The script is intended to be executable and accessible from the shell environment where the AI model executes commands.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n#!/usr/bin/env python3\n\n\"\"\"\nA self-contained **pure-Python 3.9+** utility for applying human-readable\nâ€œpseudo-diffâ€ patch files to a collection of text files.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pathlib\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import (\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Union,\n)\n\n\n# --------------------------------------------------------------------------- #\n#  Domain objects\n# --------------------------------------------------------------------------- #\nclass ActionType(str, Enum):\n    ADD = \"add\"\n    DELETE = \"delete\"\n    UPDATE = \"update\"\n\n\n@dataclass\nclass FileChange:\n    type: ActionType\n    old_content: Optional[str] = None\n    new_content: Optional[str] = None\n    move_path: Optional[str] = None\n\n\n@dataclass\nclass Commit:\n    changes: Dict[str, FileChange] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n#  Exceptions\n# --------------------------------------------------------------------------- #\nclass DiffError(ValueError):\n    \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n\n\n# --------------------------------------------------------------------------- #\n#  Helper dataclasses used while parsing patches\n```\n\n----------------------------------------\n\nTITLE: Uploading Data to Azure AI Search in Python\nDESCRIPTION: This code snippet demonstrates how to upload data from a pandas DataFrame to an Azure AI Search index using the `SearchIndexingBufferedSender`. It converts DataFrame columns to strings, creates a list of dictionaries representing the documents, validates the schema, and uploads the documents in batches. It handles potential `HttpResponseError` exceptions and ensures resources are properly closed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Convert the 'id' and 'vector_id' columns to string so one of them can serve as our key field\narticle_df[\"id\"] = article_df[\"id\"].astype(str)\narticle_df[\"vector_id\"] = article_df[\"vector_id\"].astype(str)\n\n# Convert the DataFrame to a list of dictionaries\ndocuments = article_df.to_dict(orient=\"records\")\n\n# Log the number of documents to be uploaded\nprint(f\"Number of documents to upload: {len(documents)}\")\n\n# Create a SearchIndexingBufferedSender\nbatch_client = SearchIndexingBufferedSender(\n    search_service_endpoint, index_name, AzureKeyCredential(search_service_api_key)\n)\n# Get the first document to check its schema\nfirst_document = documents[0]\n\n# Get the index schema\nindex_schema = index_client.get_index(index_name)\n\n# Get the field names from the index schema\nindex_fields = {field.name: field.type for field in index_schema.fields}\n\n# Check each field in the first document\nfor field, value in first_document.items():\n    if field not in index_fields:\n        print(f\"Field '{field}' is not in the index schema.\")\n\n# Check for any fields in the index schema that are not in the documents\nfor field in index_fields:\n    if field not in first_document:\n        print(f\"Field '{field}' is in the index schema but not in the documents.\")\n\ntry:\n    if documents:\n        # Add upload actions for all documents in a single call\n        upload_result = batch_client.upload_documents(documents=documents)\n\n        # Check if the upload was successful\n        # Manually flush to send any remaining documents in the buffer\n        batch_client.flush()\n        \n        print(f\"Uploaded {len(documents)} documents in total\")\n    else:\n        print(\"No documents to upload.\")\nexcept HttpResponseError as e:\n    print(f\"An error occurred: {e}\")\n    raise  # Re-raise the exception to ensure it errors out\nfinally:\n    # Clean up resources\n    batch_client.close()\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure AI Search client authentication in Python\nDESCRIPTION: Configures the Azure AI Search client for vector store interaction. Uses either Azure Active Directory via DefaultAzureCredential or an API key via AzureKeyCredential based on a flag. Initializes a SearchClient pointing to the specified Azure Search endpoint and index to enable document operations and vector queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Configuration\nsearch_service_endpoint: str = \"YOUR_AZURE_SEARCH_ENDPOINT\"\nsearch_service_api_key: str = \"YOUR_AZURE_SEARCH_ADMIN_KEY\"\nindex_name: str = \"azure-ai-search-openai-cookbook-demo\"\n\n# Set this flag to True if you are using Azure Active Directory\nuse_aad_for_search = True  \n\nif use_aad_for_search:\n    # Use Azure Active Directory (AAD) authentication\n    credential = DefaultAzureCredential()\nelse:\n    # Use API key authentication\n    credential = AzureKeyCredential(search_service_api_key)\n\n# Initialize the SearchClient with the selected authentication method\nsearch_client = SearchClient(\n    endpoint=search_service_endpoint, index_name=index_name, credential=credential\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Prefix for Push Notification Summarization in Python\nDESCRIPTION: Sets up a basic prompt prefix template for instructing an AI model to generate a concise summary from push notification data, using placeholders for notification content. Implements a reusable string template with minimal dependencies.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nPROMPT_PREFIX = \"\"\"\\nYou are a helpful assistant that takes in an array of push notifications and returns a collapsed summary of them.\\nThe push notification will be provided as follows:\\n<push_notifications>\\n...notificationlist...\\n</push_notifications>\\n\\nYou should return just the summary and nothing else.\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Files in Python\nDESCRIPTION: Loads the content of specified file paths using a provided file opening function (`open_fn`). Returns a dictionary mapping each path to its content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef load_files(paths: List[str], open_fn: Callable[[str], str]) -> Dict[str, str]:\n    return {path: open_fn(path) for path in paths}\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-embedded Wikipedia Data using wget in Python\nDESCRIPTION: Uses the `wget` library to download a zip file containing pre-computed embeddings for Wikipedia articles from a specified OpenAI CDN URL. This dataset (~700 MB) serves as the input for the vector database demonstration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Python Library\nDESCRIPTION: Install the official OpenAI Python client library using pip. This command downloads and installs the necessary package from the Python Package Index (PyPI) to allow interaction with the OpenAI API from Python applications.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install openai\n```\n\n----------------------------------------\n\nTITLE: Loading the CLIP model with specified device in Python\nDESCRIPTION: Loads the CLIP model 'ViT-B/32' onto the specified computational device (CPU in this case). The 'preprocess' function is also retrieved to prepare images for embedding generation, enabling consistent input formatting for the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n```\n\n----------------------------------------\n\nTITLE: Initializing Search Tool with SerpAPI\nDESCRIPTION: This code initializes a search tool using SerpAPI.  It requires the `SERPAPI_API_KEY` environment variable to be set. It defines a tool named \"Search\" that uses the `search.run` function to execute search queries, providing a description for its purpose.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Initiate a Search tool - note you'll need to have set SERPAPI_API_KEY as an environment variable as per the above instructions\nsearch = SerpAPIWrapper()\n\n# Define a list of tools\ntools = [\n    Tool(\n        name = \"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\"\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Generating document embeddings using Azure OpenAI client in Python\nDESCRIPTION: Defines a function to generate vector embeddings for text inputs using the Azure OpenAI embeddings model. The function invokes the embedding creation method on the AzureOpenAI client with the specified deployment, returning the embedding vector extracted from the response. Demonstrates generating an embedding for the first document's text content to use with the vector search index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Example function to generate document embedding\ndef generate_embeddings(text, model):\n    # Generate embeddings for the provided text using the specified model\n    embeddings_response = client.embeddings.create(model=model, input=text)\n    # Extract the embedding data from the response\n    embedding = embeddings_response.data[0].embedding\n    return embedding\n\n\nfirst_document_content = documents[0][\"text\"]\nprint(f\"Content: {first_document_content[:100]}\")\n\ncontent_vector = generate_embeddings(first_document_content, deployment)\nprint(\"Content vector generated\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment and Importing Libraries for Stripe and OpenAI Agents in Python\nDESCRIPTION: Imports essential Python standard libraries and third-party modules including dotenv for environment variable management, logging for application diagnostics, OpenAI Agents SDK components for building agentic workflows, and Stripe SDK for payment API calls. It loads environment variables from a .env file and sets the Stripe API key accordingly. Logging is configured at the INFO level to provide runtime insights.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport logging\nimport json\nfrom dotenv import load_dotenv\nfrom agents import Agent, Runner, function_tool  # Only import what you need\nimport stripe\nfrom typing_extensions import TypedDict, Any\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set Stripe API key from environment variables\nstripe.api_key = os.getenv(\"STRIPE_SECRET_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Model Comparison (IDK Expected) using Evaluator in Python\nDESCRIPTION: This Python snippet uses the `Evaluator` class instance to create a bar plot comparing the baseline and fine-tuned models' performance. The comparison focuses on the scenario where the model is expected to respond with \"I don't know\" because the answer is not present in the context, showing the rates of hallucination versus correctly identifying the absence of an answer.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nevaluator.plot_model_comparison([\"generated_answer\", \"ft_generated_answer\"], scenario=\"idk_expected\", nice_names=[\"Baseline\", \"Fine-Tuned\"])\n```\n\n----------------------------------------\n\nTITLE: Displaying sample content from the training file\nDESCRIPTION: Prints the first 5 lines of the training JSONL file to verify the format is correct before uploading.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# print the first 5 lines of the training file\n!head -n 5 tmp_recipe_finetune_training.jsonl\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Active Directory Authentication Flag in Python\nDESCRIPTION: Defines a boolean variable `use_azure_active_directory`. This flag determines whether the subsequent code blocks will configure the Azure OpenAI client using Azure Active Directory credentials (if True) or an API key (if False).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory\n```\n\n----------------------------------------\n\nTITLE: Connecting to Weaviate Instance using Python Client with OpenAI Key\nDESCRIPTION: Demonstrates how to connect to a Weaviate instance using the Python client library. It imports necessary libraries (`weaviate`, `datasets`, `os`). The `weaviate.Client` is initialized with the Weaviate instance URL (providing examples for WCS and local deployment), optional Weaviate API key authentication (`AuthApiKey`), and the crucial OpenAI API key passed via `additional_headers`. The OpenAI key is fetched from environment variables using `os.getenv(\"OPENAI_API_KEY\")`. Finally, `client.is_ready()` is called to confirm the connection is successful.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n#   url=\"http://localhost:8080/\",\n    auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"<YOUR-WEAVIATE-API-KEY>\"), # comment out this line if you are not using authentication for your Weaviate instance (i.e. for locally deployed instances)\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries and initializing OpenAI client in Python\nDESCRIPTION: This snippet imports the required libraries for OpenAI API interaction, data display, concurrency, and CSV processing, then initializes the OpenAI client and sets the model for routine generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_routine_generation.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nfrom IPython.display import display, HTML\nimport pandas as pd\nfrom concurrent.futures import ThreadPoolExecutor\nimport csv\n\nclient = OpenAI()\nMODEL = 'o1-preview'\n```\n\n----------------------------------------\n\nTITLE: Enrich Text by Replacing Entities with Wikipedia Links\nDESCRIPTION: Integrates the previous link retrieval into text enrichment by substituting entity mentions with Markdown links to Wikipedia pages, thus providing enriched, hyperlinked text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef enrich_entities(text: str, label_entities: dict) -> str:\n    \"\"\"\n    Enriches text with knowledge base links.\n    \"\"\"\n    entity_link_dict = find_all_links(label_entities)\n    logging.info(f\"entity_link_dict: {entity_link_dict}\")\n    \n    for entity, link in entity_link_dict.items():\n        text = text.replace(entity, f\"[{entity}]({link})\")\n\n    return text\n```\n\n----------------------------------------\n\nTITLE: Creating Qdrant Collection with Multiple Vectors\nDESCRIPTION: This Python code creates a new collection named 'Articles' in Qdrant using the `qdrant-client`. It configures the collection to store two types of vectors, 'title' and 'content', both using Cosine similarity and specifying the vector size based on the loaded data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom qdrant_client.http import models as rest\n\nvector_size = len(article_df[\"content_vector\"][0])\n\nclient.create_collection(\n    collection_name=\"Articles\",\n    vectors_config={\n        \"title\": rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n        \"content\": rest.VectorParams(\n            distance=rest.Distance.COSINE,\n            size=vector_size,\n        ),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Create Astra DB Connection Python\nDESCRIPTION: This code creates a connection to the Astra DB cluster using the provided secure connect bundle, application token, and keyspace. It instantiates a `Cluster` object and uses `PlainTextAuthProvider` for authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Don't mind the \"Closing connection\" error after \"downgrading protocol...\" messages you may see,\n# it is really just a warning: the connection will work smoothly.\ncluster = Cluster(\n    cloud={\n        \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n    },\n    auth_provider=PlainTextAuthProvider(\n        \"token\",\n        ASTRA_DB_APPLICATION_TOKEN,\n    ),\n)\n\nsession = cluster.connect()\nkeyspace = ASTRA_DB_KEYSPACE\n```\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikipedia embeddings dataset with wget\nDESCRIPTION: Downloads a large ZIP file containing precomputed Wikipedia article embeddings from a specified URL. Uses wget to handle the download, which may take time due to file size.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Weaviate Instance with OpenAI Integration\nDESCRIPTION: Establishes a connection to a Weaviate instance with OpenAI API key for vectorization. The connection can be to either a cloud or local instance, and includes authentication if needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n#   url=\"http://localhost:8080/\",\n    auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"<YOUR-WEAVIATE-API-KEY>\"), # comment out this line if you are not using authentication for your Weaviate instance (i.e. for locally deployed instances)\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for File Search Integration in Python\nDESCRIPTION: Installs the necessary Python libraries for working with PDFs, data manipulation, progress tracking, and the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install PyPDF2 pandas tqdm openai -q\n```\n\n----------------------------------------\n\nTITLE: Opening Source Images for Editing in Python\nDESCRIPTION: Opens the previously saved images that will be used as inputs for the image editing operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimg1 = open(img_path2, \"rb\")\nimg2 = open(img_path3, \"rb\")\n```\n\n----------------------------------------\n\nTITLE: Defining Display Results Function\nDESCRIPTION: Defines a function `display_results` that takes a name and evaluation results as input. This function processes the evaluation results, calculates the mean Hit Rate and MRR, and presents the results in a pandas DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef display_results(name, eval_results):\n    \"\"\"Display results from evaluate.\"\"\"\n\n    metric_dicts = []\n    for eval_result in eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[\"hit_rate\"].mean()\n    mrr = full_df[\"mrr\"].mean()\n\n    metric_df = pd.DataFrame(\n        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n    )\n\n    return metric_df\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Azure OpenAI (Python)\nDESCRIPTION: This snippet installs the necessary Python packages for interacting with the Azure OpenAI service. It uses `pip` to install the `openai` and `python-dotenv` libraries, specifying version constraints for `openai`. The dependencies include the OpenAI Python SDK and the dotenv library for loading environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"openai>=1.0.0,<2.0.0\"\n! pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Function with OpenAI API\nDESCRIPTION: Creates a function that uses OpenAI's API to generate embeddings for book descriptions, converting text data into vector representations suitable for semantic search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Simple function that converts the texts to embeddings\ndef embed(texts):\n    embeddings = openai.Embedding.create(\n        input=texts,\n        engine=OPENAI_ENGINE\n    )\n    return [x['embedding'] for x in embeddings['data']]\n```\n\n----------------------------------------\n\nTITLE: Configuring the LangChain Retriever from the Pinecone Docsearch in Python\nDESCRIPTION: Creates a retriever object from the Pinecone docsearch, allowing for semantic document retrieval using vector similarity. Requires a valid docsearch object. Used as an interface to retrieve relevant documents for a query based on embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nretriever = docsearch.as_retriever()\n\n```\n\n----------------------------------------\n\nTITLE: Downloading and Saving Fine-tuning Results\nDESCRIPTION: Retrieving the fine-tuning results file and saving it locally for analysis of model performance metrics.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfine_tune_results = client.fine_tuning.jobs.retrieve(fine_tuning_job.id).result_files\nresult_file = client.files.retrieve(fine_tune_results[0])\ncontent = client.files.content(result_file.id)\n# save content to file\nwith open(\"result.csv\", \"wb\") as f:\n    f.write(content.text.encode(\"utf-8\"))\n```\n\n----------------------------------------\n\nTITLE: Providing a User Identifier using OpenAI API (cURL)\nDESCRIPTION: This cURL command provides an example of how to include a user identifier when interacting with the OpenAI API. It constructs a JSON payload with the required parameters such as the model, prompt, max_tokens, and importantly, the user identifier. It sends a POST request to the API endpoint, including the necessary authentication header (Bearer $OPENAI_API_KEY) and the JSON payload in the request body. The user parameter is used to help monitor and detect abuse. Replace `$OPENAI_API_KEY` with your actual API key. The response will contain the API completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/safety-best-practices.txt#_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-3.5-turbo-instruct\",\n  \"prompt\": \"This is a test\",\n  \"max_tokens\": 5,\n  \"user\": \"user123456\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Evaluating Claims with Filtered Context\nDESCRIPTION: Assesses claims using the filtered context and evaluates performance with a confusion matrix. This approach tends to be more conservative, labeling more claims as having insufficient evidence.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ngpt_with_filtered_context_evaluation = assess_claims_with_context(claims, filtered_claim_query_result['documents'])\nconfusion_matrix(gpt_with_filtered_context_evaluation, groundtruth)\n```\n\n----------------------------------------\n\nTITLE: Creating a Run using curl command in Bash\nDESCRIPTION: This curl example shows how to initiate a Run by sending a POST request to the OpenAI API with necessary headers and data payload including assistant ID. It requires API key credentials and can be adapted with thread and assistant identifiers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_12\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads/THREAD_ID/runs \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"assistant_id\": \"asst_ToSF7Gb04YMj8AMMm50ZLLtY\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Answering Questions with Context - Python\nDESCRIPTION: This code demonstrates how to use the context retrieval file (olympics_search_fileid) to answer questions using the OpenAI API. The  `create_context` function is called to get the relevant context for a given question, and then the `answer_question` function is used to generate the answer using davinci-instruct-beta-v3. Further example tests the systems inability to identify if a question has an answer based on the content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom answers_with_ft import create_context, answer_question\nprint(create_context(\"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\", olympics_search_fileid, max_len=400))\n\n```\n\nLANGUAGE: python\nCODE:\n```\nanswer_question(olympics_search_fileid, \"davinci-instruct-beta-v3\", \n            \"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\")\n```\n\nLANGUAGE: python\nCODE:\n```\nanswer_question(olympics_search_fileid, \"davinci-instruct-beta-v3\", \n            \"Where did women's 4 x 100 metres relay event take place during the 2048 Summer Olympics?\", max_len=1000)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedding Parameters\nDESCRIPTION: This snippet defines constants related to the context length and encoding used for generating embeddings.  `EMBEDDING_CTX_LENGTH` sets the maximum context length, in tokens, for the embedding model (8191). `EMBEDDING_ENCODING` specifies the encoding scheme ('cl100k_base') used by the tokenizer when processing text for the embedding model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nEMBEDDING_CTX_LENGTH = 8191\nEMBEDDING_ENCODING='cl100k_base'\n```\n\n----------------------------------------\n\nTITLE: Tracking fine-tuning job events\nDESCRIPTION: Retrieves and displays the event logs for the fine-tuning job to monitor its progress and any potential issues.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.fine_tuning.jobs.list_events(job_id)\n\nevents = response.data\nevents.reverse()\n\nfor event in events:\n    print(event.message)\n```\n\n----------------------------------------\n\nTITLE: Testing function search with various queries\nDESCRIPTION: These code snippets demonstrate the usage of the `search_functions` function with different query strings. Each snippet calls the function with a specific query and specifies the number of results (`n`) and the number of lines of code to display (`n_lines`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nres = search_functions(df, 'fine-tuning input data validation logic', n=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nres = search_functions(df, 'find common suffix', n=2, n_lines=10)\n```\n\nLANGUAGE: python\nCODE:\n```\nres = search_functions(df, 'Command line interface for fine-tuning', n=1, n_lines=20)\n```\n\n----------------------------------------\n\nTITLE: Creating/Deleting Pinecone Index\nDESCRIPTION: This code checks if a Pinecone index with the specified name already exists. If it does, it deletes the existing index. It then creates a new index with the given name and dimension (1536). Finally, it confirms the index creation by listing all indexes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Check whether the index with the same name already exists - if so, delete it\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n    \n# Creates new index\npinecone.create_index(name=index_name, dimension=1536)\nindex = pinecone.Index(index_name=index_name)\n\n# Confirm our index was created\npinecone.list_indexes()\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure OpenAI Client with API Key Authentication in Python\nDESCRIPTION: Conditionally initializes the `openai.AzureOpenAI` client for API key authentication when `use_azure_active_directory` is False. It retrieves the Azure OpenAI endpoint and API key from environment variables (`AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`) and sets the deployment ID. The `base_url` is specifically formatted to include `/extensions` for enabling the 'on your own data' feature, and the preview API version `2023-09-01-preview` is used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n    # set the deployment name for the model we want to use\n    deployment = \"<deployment-id-of-the-model-to-use>\"\n\n    client = openai.AzureOpenAI(\n        base_url=f\"{endpoint}/openai/deployments/{deployment}/extensions\",\n        api_key=api_key,\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Podcast JSON Data from Extracted Archive in Python\nDESCRIPTION: Extracts the ZIP file with podcast transcript data and loads the resulting JSON into memory for processing. Dependencies include zipfile and json modules. Produces a Python list or dictionary containing transcript and embedding data for further use, such as vector store insertion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Load podcasts\nwith zipfile.ZipFile(\"sysk_podcast_transcripts_embedded.json.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"./data\")\nf = open('./data/sysk_podcast_transcripts_embedded.json')\nprocessed_podcasts = json.load(f)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing MongoDB and OpenAI Clients in Python\nDESCRIPTION: Initializes the MongoDB client using the provided Atlas URI and selects the 'sample_mflix' database and 'movies' collection. It also sets the OpenAI API key for subsequent API calls. This sets up the connections needed to interact with the data and the embedding service.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pymongo\n\nclient = pymongo.MongoClient(MONGODB_ATLAS_CLUSTER_URI)\ndb = client.sample_mflix\ncollection = db.movies\n\nopenai.api_key = OPENAI_API_KEY\n\n```\n\n----------------------------------------\n\nTITLE: Executing a Weaviate QnA Query in Python (Example 2)\nDESCRIPTION: This snippet provides another example of using the `qna` function, this time asking \"What is the capital of China?\" against the \"Article\" collection. It processes the result, checking if an answer was found (`hasAnswer` property) and printing either the answer and its distance or a \"No answer found\" message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nquery_result = qna(\"What is the capital of China?\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    if article['_additional']['answer']['hasAnswer'] == False:\n      print('No answer found')\n    else:\n      print(f\"{i+1}. { article['_additional']['answer']['result']} (Distance: {round(article['_additional']['distance'],3) })\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for AnalyticDB and OpenAI Integration - Python\nDESCRIPTION: Installs the necessary Python libraries for interacting with OpenAI and AnalyticDB, including 'openai' for API access, 'psycopg2' for PostgreSQL connections, 'pandas' for data manipulation, and 'wget' for downloading datasets. Requires a working Python environment with pip installed. Expected inputs: package names. This operation must be run in a compatible system shell or notebook environment that supports bang commands.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai psycopg2 pandas wget\n\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Model in Python\nDESCRIPTION: Imports necessary libraries (`json`, `openai`, `os`), initializes the OpenAI client using default environment variables, and defines the GPT model identifier (`gpt-4-turbo`) to be used for chat completions. This sets up the basic environment for interacting with the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI()\nGPT_MODEL = 'gpt-4-turbo'\n```\n\n----------------------------------------\n\nTITLE: List Qdrant Collections\nDESCRIPTION: Retrieves and implicitly displays the list of existing collections currently managed by the connected Qdrant instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nqdrant.get_collections()\n```\n\n----------------------------------------\n\nTITLE: Creating Collection Schema in Zilliz\nDESCRIPTION: Defines the schema for the book collection with fields for ID, title, description, and embedding vector, then creates the collection with this schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create collection which includes the id, title, and embedding.\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='description', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n]\nschema = CollectionSchema(fields=fields)\ncollection = Collection(name=COLLECTION_NAME, schema=schema)\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Accuracy Over Time\nDESCRIPTION: Visualizing how the training accuracy progressed throughout the fine-tuning process to identify convergence patterns.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults[results['train_accuracy'].notnull()]['train_accuracy'].plot()\n```\n\n----------------------------------------\n\nTITLE: Integrating Search and Fine-Tuned Q&A Model to Answer Questions from Knowledge Base in Python\nDESCRIPTION: This snippet demonstrates how to use a search-based retrieval system to find relevant context from a knowledge base and then use a fine-tuned Q&A model to answer questions based on that context. It imports an 'answer_question' function from the 'answers_with_ft' module, which encapsulates the logic for searching and answering. The example includes invoking this function with a file identifier for searching, the fine-tuned Q&A model ID, and a natural language question. Dependencies include the external module 'answers_with_ft.py' and relevant model IDs. This approach mimics the OpenAI answers endpoint combining search and Q&A.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom answers_with_ft import answer_question\nanswer_question(olympics_search_fileid, ft_qa, \"Which country won the Women's football tournament at the 2020 Olympic games?\")\n```\n\n----------------------------------------\n\nTITLE: Calling the Chat Completions API with cURL\nDESCRIPTION: Provides a cURL command example for interacting directly with the Chat Completions API endpoint (`https://api.openai.com/v1/chat/completions`). It sets the `Content-Type` header to `application/json` and the `Authorization` header with a Bearer token (requiring an API key, typically via `$OPENAI_API_KEY`). The request body (`-d`) contains the JSON payload specifying the model (`gpt-3.5-turbo`) and the message history.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_2\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Who won the world series in 2020?\"\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Where was it played?\"\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Extracting Downloaded Embeddings File\nDESCRIPTION: This code snippet extracts the contents of the downloaded zip file containing pre-computed Wikipedia article embeddings. It extracts the file to a `data` directory relative to the current working directory and verifies the extraction by checking for the existence of the extracted CSV file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nimport os\nimport re\nimport tempfile\n\ncurrent_directory = os.getcwd()\nzip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\noutput_directory = os.path.join(current_directory, \"../../data\")\n\nwith zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n    zip_ref.extractall(output_directory)\n\n\n# Check to see if the csv file was extracted\nfile_name = \"vector_database_wikipedia_articles_embedded.csv\"\ndata_directory = os.path.join(current_directory, \"../../data\")\nfile_path = os.path.join(data_directory, file_name)\n\n\nif os.path.exists(file_path):\n    print(f\"The csv file {file_name} exists in the data directory.\")\nelse:\n    print(f\"The csv file {file_name} does not exist in the data directory.\")\n```\n\n----------------------------------------\n\nTITLE: Define a function to perform vector similarity search in Tair\nDESCRIPTION: Creates a function that generates an embedding for a textual query using OpenAI API, converts it to a NumPy array, and performs a k-nearest neighbors search in the specified index and vector space, returning search results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef query_tair(client, query, vector_name=\"title_vector\", top_k=5):\n\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n        input= query,\n        model=\"text-embedding-3-small\",\n    )[\"data\"][0][\"embedding\"]\n    embedded_query = np.array(embedded_query)\n\n    # search for the top k approximate nearest neighbors of vector in an index\n    query_result = client.tvs_knnsearch(index=index+\"_\"+vector_name, k=top_k, vector=embedded_query)\n\n    return query_result\n```\n\n----------------------------------------\n\nTITLE: Fetching Current Weather Data using Open Meteo API in JavaScript\nDESCRIPTION: This asynchronous function `getCurrentWeather` fetches current weather data for a specific location using the Open Meteo API. It constructs the API URL using the provided latitude and longitude parameters, fetches the data, and returns it in JSON format. The API returns hourly temperature data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nasync function getCurrentWeather(latitude, longitude) {\n  const url = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&hourly=apparent_temperature`;\n  const response = await fetch(url);\n  const weatherData = await response.json();\n  return weatherData;\n}\n```\n\n----------------------------------------\n\nTITLE: Inserting article data into MyScale in batches\nDESCRIPTION: Divides the prepared DataFrame into batches and inserts each batch into the MyScale 'articles' table, enabling scalable data load. Uses tqdm for progress tracking.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom tqdm.auto import tqdm\n\nbatch_size = 100\ntotal_records = len(article_df)\n\n# upload data in batches\ndata = article_df.to_records(index=False).tolist()\ncolumn_names = article_df.columns.tolist() \n\nfor i in tqdm(range(0, total_records, batch_size)):\n    i_end = min(i + batch_size, total_records)\n    client.insert(\"default.articles\", data[i:i_end], column_names=column_names)\n```\n\n----------------------------------------\n\nTITLE: Classifying Customer Queries (Prompt)\nDESCRIPTION: Demonstrates instructing the model to perform intent classification on a customer service query. It defines primary and secondary categories and requires the model to output the classification in a JSON structure with 'primary' and 'secondary' keys. Provides an example query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_10\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: You will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\n\nUSER: I need to get my internet working again.\n```\n\n----------------------------------------\n\nTITLE: Self-Consistency Method for Language Models\nDESCRIPTION: This snippet illustrates a technique that improves model performance by sampling multiple answers or explanations for a given task and selecting the most frequent among them. It enhances reasoning accuracy but can be computationally costly and is limited to tasks with discrete answer sets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Examining Run Object for Function Calling\nDESCRIPTION: This JSON snippet shows the structure of a Run object after function calls are required. The `status` is `requires_action`, and the `required_action` object contains an array of `tool_calls`. These `tool_calls` specify the functions to be called (e.g., `get_rain_probability`, `get_current_temperature`) along with their arguments. Prerequisites: Understanding of the OpenAI Assistants API and function calling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-function-calling.txt#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"run_qJL1kI9xxWlfE0z1yfL0fGg9\",\n  ...\n  \"status\": \"requires_action\",\n  \"required_action\": {\n    \"submit_tool_outputs\": {\n      \"tool_calls\": [\n        {\n          \"id\": \"call_FthC9qRpsL5kBpwwyw6c7j4k\",\n          \"function\": {\n            \"arguments\": \"{\\\"location\\\": \\\"San Francisco, CA\\\"}\",\n            \"name\": \"get_rain_probability\"\n          },\n          \"type\": \"function\"\n        },\n        {\n          \"id\": \"call_RpEDoB8O0FTL9JoKTuCVFOyR\",\n          \"function\": {\n            \"arguments\": \"{\\\"location\\\": \\\"San Francisco, CA\\\", \\\"unit\\\": \\\"Fahrenheit\\\"}\",\n            \"name\": \"get_current_temperature\"\n          },\n          \"type\": \"function\"\n        }\n      ]\n    },\n    ...\n    \"type\": \"submit_tool_outputs\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading a training file using Node.js OpenAI SDK\nDESCRIPTION: This snippet shows how to upload a training dataset file ('mydata.jsonl') to OpenAI through Node.js, using either a file stream, File API, or fetch Response. It requires the 'openai' package and proper API configuration, enabling the dataset to be used for fine-tuning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_4\n\nLANGUAGE: node.js\nCODE:\n```\nimport OpenAI, { toFile } from 'openai';\nconst openai = new OpenAI();\n// If you have access to Node fs we recommend using fs.createReadStream():\nawait openai.files.create({ file: fs.createReadStream('mydata.jsonl'), purpose: 'fine-tune' });\n// Or if you have the web File API you can pass a File instance:\nawait openai.files.create({ file: new File(['my bytes'], 'mydata.jsonl'), purpose: 'fine-tune' });\n// You can also pass a fetch Response:\nawait openai.files.create({ file: await fetch('https://somesite/mydata.jsonl'), purpose: 'fine-tune' });\n```\n\n----------------------------------------\n\nTITLE: Extracting functions from Python code\nDESCRIPTION: These helper functions parse Python files to extract function names and their corresponding code blocks.  It iterates through lines, identifies function definitions (starting with 'def' or 'async def'), and extracts the code block until a non-indented line is found. Dependencies include the 'pathlib' and 'pandas' libraries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pathlib import Path\n\nDEF_PREFIXES = ['def ', 'async def ']\nNEWLINE = '\\n'\n\ndef get_function_name(code):\n    \"\"\"\n    Extract function name from a line beginning with 'def' or 'async def'.\n    \"\"\"\n    for prefix in DEF_PREFIXES:\n        if code.startswith(prefix):\n            return code[len(prefix): code.index('(')]\n\n\ndef get_until_no_space(all_lines, i):\n    \"\"\"\n    Get all lines until a line outside the function definition is found.\n    \"\"\"\n    ret = [all_lines[i]]\n    for j in range(i + 1, len(all_lines)):\n        if len(all_lines[j]) == 0 or all_lines[j][0] in [' ', '\\t', ')']:\n            ret.append(all_lines[j])\n        else:\n            break\n    return NEWLINE.join(ret)\n\n\ndef get_functions(filepath):\n    \"\"\"\n    Get all functions in a Python file.\n    \"\"\"\n    with open(filepath, 'r') as file:\n        all_lines = file.read().replace('\\r', NEWLINE).split(NEWLINE)\n        for i, l in enumerate(all_lines):\n            for prefix in DEF_PREFIXES:\n                if l.startswith(prefix):\n                    code = get_until_no_space(all_lines, i)\n                    function_name = get_function_name(code)\n                    yield {\n                        'code': code,\n                        'function_name': function_name,\n                        'filepath': filepath,\n                    }\n                    break\n\n\ndef extract_functions_from_repo(code_root):\n    \"\"\"\n    Extract all .py functions from the repository.\n    \"\"\"\n    code_files = list(code_root.glob('**/*.py'))\n\n    num_files = len(code_files)\n    print(f'Total number of .py files: {num_files}')\n\n    if num_files == 0:\n        print('Verify openai-python repo exists and code_root is set correctly.')\n        return None\n\n    all_funcs = [\n        func\n        for code_file in code_files\n        for func in get_functions(str(code_file))\n    ]\n\n    num_funcs = len(all_funcs)\n    print(f'Total number of functions extracted: {num_funcs}')\n\n    return all_funcs\n```\n\n----------------------------------------\n\nTITLE: Training RandomForestRegressor for Regression - Python\nDESCRIPTION: This code snippet loads a dataset of Amazon reviews with pre-computed embeddings, splits it into training and testing sets, trains a RandomForestRegressor model to predict the review score, and evaluates the model's performance. The `pandas` library is used to read the CSV file, `numpy` to handle numerical operations, `ast` to convert string representations of lists to actual lists, `sklearn.model_selection` for splitting the dataset, `sklearn.ensemble` for the RandomForestRegressor, and `sklearn.metrics` for evaluating the model. The output is the MSE and MAE, indicating the model's predictive accuracy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Regression_using_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom ast import literal_eval\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)\n\nX_train, X_test, y_train, y_test = train_test_split(list(df.embedding.values), df.Score, test_size=0.2, random_state=42)\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)\n\nmse = mean_squared_error(y_test, preds)\nmae = mean_absolute_error(y_test, preds)\n\nprint(f\"text-embedding-3-small performance on 1k Amazon reviews: mse={mse:.2f}, mae={mae:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Describing the `apply_patch` Custom Tool (Python/Bash)\nDESCRIPTION: A Python multi-line string variable (`APPLY_PATCH_TOOL_DESC`) containing the description for a custom `apply_patch` utility. It details the usage via a Bash command structure (`apply_patch <<\"EOF\"...`) and explains the specific V4A diff format, including context lines, the `@@` operator for scoping, and provides an example patch.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nAPPLY_PATCH_TOOL_DESC = \"\"\"This is a custom utility that makes it more convenient to add, remove, move, or edit code files. `apply_patch` effectively allows you to execute a diff/patch against a file, but the format of the diff specification is unique to this task, so pay careful attention to these instructions. To use the `apply_patch` command, you should pass a message of the following structure as \"input\":\n\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n[YOUR_PATCH]\n*** End Patch\nEOF\n\nWhere [YOUR_PATCH] is the actual content of your patch, specified in the following V4A diff format.\n\n*** [ACTION] File: [path/to/file] -> ACTION can be one of Add, Update, or Delete.\nFor each snippet of code that needs to be changed, repeat the following:\n[context_before] -> See below for further instructions on context.\n- [old_code] -> Precede the old code with a minus sign.\n+ [new_code] -> Precede the new, replacement code with a plus sign.\n[context_after] -> See below for further instructions on context.\n\nFor instructions on [context_before] and [context_after]:\n- By default, show 3 lines of code immediately above and 3 lines immediately below each change. If a change is within 3 lines of a previous change, do NOT duplicate the first changeâ€™s [context_after] lines in the second changeâ€™s [context_before] lines.\n- If 3 lines of context is insufficient to uniquely identify the snippet of code within the file, use the @@ operator to indicate the class or function to which the snippet belongs. For instance, we might have:\n@@ class BaseClass\n[3 lines of pre-context]\n- [old_code]\n+ [new_code]\n[3 lines of post-context]\n\n- If a code block is repeated so many times in a class or function such that even a single @@ statement and 3 lines of context cannot uniquely identify the snippet of code, you can use multiple `@@` statements to jump to the right context. For instance:\n\n@@ class BaseClass\n@@ \\tdef method():\n[3 lines of pre-context]\n- [old_code]\n+ [new_code]\n[3 lines of post-context]\n\nNote, then, that we do not use line numbers in this diff format, as the context is enough to uniquely identify code. An example of a message that you might pass as \"input\" to this function, in order to apply a patch, is shown below.\n\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n*** Update File: pygorithm/searching/binary_search.py\n@@ class BaseClass\n@@     def search():\n-          pass\n+          raise NotImplementedError()\n\n@@ class Subclass\n@@     def search():\n-          pass\n+          raise NotImplementedError()\n\n*** End Patch\nEOF\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Atlas Vector Search Index and Embedding Field Names in Python\nDESCRIPTION: Defines constants for the MongoDB Atlas Vector Search index name and the document field name where the generated vector embeddings will be stored. These constants ensure consistency when referencing the index and field throughout the code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nATLAS_VECTOR_SEARCH_INDEX_NAME = \"default\"\nEMBEDDING_FIELD_NAME = \"embedding_openai_nov19_23\"\n\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity for Azure OpenAI (Python)\nDESCRIPTION: This snippet installs the `azure-identity` library using `pip`. This library is required for authenticating with Azure Active Directory and provides credentials for accessing Azure resources.  It is a prerequisite for using the `DefaultAzureCredential` and `get_bearer_token_provider` functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"azure-identity>=1.15.0\"\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Azure OpenAI Using API Key\nDESCRIPTION: Creates an Azure OpenAI client using API key authentication. Requires the Azure endpoint and API key stored in environment variables, and specifies the API version to use.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        api_key=api_key,\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Review Embedding Data with Pandas - Python\nDESCRIPTION: This snippet loads a CSV file containing food review data and associated embeddings into a pandas DataFrame. The CSV file, which must exist and be generated separately, is indexed by the first column. Dependencies are pandas, numpy, scikit-learn, and the standard ast module. The main expected input is the path to the CSV file; the output is a pandas DataFrame ready for further processing. Ensure the file 'fine_food_reviews_with_embeddings_1k.csv' exists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/User_and_product_embeddings.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom ast import literal_eval\n\ndf = pd.read_csv('data/fine_food_reviews_with_embeddings_1k.csv', index_col=0)  # note that you will need to generate this file to run the code below\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Deployment Tools - Python\nDESCRIPTION: This Python snippet uses the `subprocess` module to run commands for installing Azure Functions Core Tools and Azure CLI based on the detected operating system (Windows, MacOS, Linux). It includes platform-specific commands for package managers like NPM, Brew, and apt-get, as well as verification commands.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nos_type = platform.system()\n\nif os_type == \"Windows\":\n    # Install Azure Functions Core Tools on Windows\n    subprocess.run([\"npm\", \"install\", \"-g\", \"azure-functions-core-tools@3\", \"--unsafe-perm\", \"true\"], check=True)\n    # Install Azure CLI on Windows\n    subprocess.run([\"powershell\", \"-Command\", \"Invoke-WebRequest -Uri https://aka.ms/installazurecliwindows -OutFile .\\AzureCLI.msi; Start-Process msiexec.exe -ArgumentList '/I AzureCLI.msi /quiet' -Wait\"], check=True)\nelif os_type == \"Darwin\":  # MacOS\n    # Install Azure Functions Core Tools on MacOS\n    if platform.machine() == 'arm64':\n        # For M1 Macs\n        subprocess.run([\"arch\", \"-arm64\", \"brew\", \"install\", \"azure-functions-core-tools@3\"], check=True)\n    else:\n        # For Intel Macs\n        subprocess.run([\"brew\", \"install\", \"azure-functions-core-tools@3\"], check=True)\n    # Install Azure CLI on MacOS\n    subprocess.run([\"brew\", \"update\"], check=True)\n    subprocess.run([\"brew\", \"install\", \"azure-cli\"], check=True)\nelif os_type == \"Linux\":\n    # Install Azure Functions Core Tools on Linux\n    subprocess.run([\"curl\", \"https://packages.microsoft.com/keys/microsoft.asc\", \"|\", \"gpg\", \"--dearmor\", \">\", \"microsoft.gpg\"], check=True, shell=True)\n    subprocess.run([\"sudo\", \"mv\", \"microsoft.gpg\", \"/etc/apt/trusted.gpg.d/microsoft.gpg\"], check=True)\n    subprocess.run([\"sudo\", \"sh\", \"-c\", \"'echo \\\"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs)-prod $(lsb_release -cs) main\\\" > /etc/apt/sources.list.d/dotnetdev.list'\"], check=True, shell=True)\n    subprocess.run([\"sudo\", \"apt-get\", \"update\"], check=True)\n    subprocess.run([\"sudo\", \"apt-get\", \"install\", \"azure-functions-core-tools-3\"], check=True)\n    # Install Azure CLI on Linux\n    subprocess.run([\"curl\", \"-sL\", \"https://aka.ms/InstallAzureCLIDeb\", \"|\", \"sudo\", \"bash\"], check=True, shell=True)\nelse:\n    # Raise an error if the operating system is not supported\n    raise OSError(\"Unsupported operating system\")\n\n# Verify the installation of Azure Functions Core Tools\nsubprocess.run([\"func\", \"--version\"], check=True)\n# Verify the installation of Azure CLI\nsubprocess.run([\"az\", \"--version\"], check=True)\n\nsubprocess.run([\n    \"az\", \"login\"\n], check=True)\n```\n\n----------------------------------------\n\nTITLE: Defining GPT Instructions for Gmail Assistant\nDESCRIPTION: These instructions guide the GPT model on how to act as an email assistant, leveraging the GMAIL OAuth 2.0 API for various tasks. It outlines behavior, encoding requirements, API usage, email formatting, and security considerations. The instructions emphasize creating plain text emails, validating email addresses, and providing concise and professional responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_gmail.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n**Context**\nAct as an email assistant designed to enhance user interaction with emails in various ways. This GPT can assist with productivity by summarizing emails/threads, identifying next steps/follow-ups, drafting or sending pre-written responses, and programmatically interacting with third-party tools (e.g., Notion to-dos, Slack channel summaries, data extraction for responses). This GPT has full scope access to the GMAIL OAuth 2.0 API, capable of reading, composing, sending, and permanently deleting emails from Gmail.\n\n**Instructions**\n- Always conclude an email by signing off with logged in user's name, unless otherwise stated.\n- Verify that the email data is correctly encoded in the required format (e.g., base64 for the message body).\n- Email Encoding Process: 1\\ Construct the email message in RFC 2822 format. 2\\ Base64 encode the email message. 3\\Send the encoded message using the API.\n- If not specified, sign all emails with the user name.\n- API Usage: After answering the user's question, do not call the Google API again until another question is asked.\n- All emails created, draft or sent, should be in plain text.\n- Ensure that the email format is clean and is formatted as if someone sent the email from their own inbox. Once a draft is created or email sent, display a message to the user confirming that the draft is ready or the email is sent.\n- Check that the \"to\" email address is valid and in the correct format. It should be in the format \"recipient@example.com\". \n- Only provide summaries of existing emails; do not fabricate email content.\n- Professionalism: Behave professionally, providing clear and concise responses.\n- Clarification: Ask for clarification when needed to ensure accuracy and completeness in fulfilling user requests.\n- Privacy and Security: Respect user privacy and handle all data securely.\n\n```\n\n----------------------------------------\n\nTITLE: Sample Quiz Execution (Python)\nDESCRIPTION: Demonstrates how to call the display_quiz function with a sample quiz containing both free-response and multiple-choice questions. It prints the user's mock responses to the console.  It defines a sample quiz with one free-response and one multiple-choice question and calls the display_quiz function to present the quiz and collect mock responses, finally printing the collected responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nresponses = display_quiz(\n    \"Sample Quiz\",\n    [\n        {\"question_text\": \"What is your name?\", \"question_type\": \"FREE_RESPONSE\"},\n        {\n            \"question_text\": \"What is your favorite color?\",\n            \"question_type\": \"MULTIPLE_CHOICE\",\n            \"choices\": [\"Red\", \"Blue\", \"Green\", \"Yellow\"],\n        },\n    ],\n)\nprint(\"Responses:\", responses)\n```\n\n----------------------------------------\n\nTITLE: Monitoring and Printing a Streamed OpenAI ChatCompletion Response - Python\nDESCRIPTION: This snippet demonstrates how to monitor an ongoing stream of tokens from a ChatCompletion call configured for streaming, logging the full response as a single record. It uses message_from_stream to iterate and print incrementally received outputs. Dependencies include weave.monitoring.openai, openai, and a valid monitored setup. Inputs are provided by a system message and a user message; the output is the concatenated streamed result. Limitation: tokens are not counted in the aggregated logs via this streaming API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom weave.monitoring.openai import message_from_stream\nr = openai.ChatCompletion.create(model=OPENAI_MODEL, messages=[\n        {\"role\": \"system\", \"content\": \"You are a robot and only speak in robot, like beep bloop bop.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a 50-word story.\"},\n    ], stream=True)\nfor s in message_from_stream(r):\n    print(s, end='')\n```\n\n----------------------------------------\n\nTITLE: Batch Processing and Upserting Embeddings to Pinecone\nDESCRIPTION: Processes the TREC dataset in batches, generates embeddings for each text entry, and upserts them to the Pinecone index with metadata. This creates a searchable database of vector embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\n\ncount = 0  # we'll use the count to create unique IDs\nbatch_size = 32  # process everything in batches of 32\nfor i in tqdm(range(0, len(trec['text']), batch_size)):\n    # set end position of batch\n    i_end = min(i+batch_size, len(trec['text']))\n    # get batch of lines and IDs\n    lines_batch = trec['text'][i: i+batch_size]\n    ids_batch = [str(n) for n in range(i, i_end)]\n    # create embeddings\n    res = client.embeddings.create(input=lines_batch, model=MODEL)\n    embeds = [record.embedding for record in res.data]\n    # prep metadata and upsert batch\n    meta = [{'text': line} for line in lines_batch]\n    to_upsert = zip(ids_batch, embeds, meta)\n    # upsert to Pinecone\n    index.upsert(vectors=list(to_upsert))\n```\n\n----------------------------------------\n\nTITLE: Updating Assistant with Vector Store Resource in Python\nDESCRIPTION: Updates an existing assistant instance to link the uploaded vector store to the file_search tool via the tool_resources parameter. This ensures the assistant can access the content inside the vector store during queries. Requires existing assistant and vector store instances.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.update(\n  assistant_id=assistant.id,\n  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Fluency Evaluation Steps\nDESCRIPTION: Defines the steps involved in evaluating the fluency of a summary. It instructs the evaluator to read the summary and assign a fluency score from 1 to 3 based on the given criteria.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nFLUENCY_SCORE_STEPS = \"\"\"\nRead the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Hugging Face Dataset (Python)\nDESCRIPTION: Uses the `datasets.load_dataset` function to download the 'hugginglearners/netflix-shows' dataset from the Hugging Face hub. It specifically loads the 'train' split of the dataset, making the movie data available for processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\n# Download the dataset \ndataset = datasets.load_dataset('hugginglearners/netflix-shows', split='train')\n```\n\n----------------------------------------\n\nTITLE: Embedding Long Texts by Chunking and Averaging in Python\nDESCRIPTION: Provides a function to process long texts by breaking them into token chunks, embedding each chunk, and optionally averaging embeddings weighted by chunk size, ensuring handling of texts exceeding model context length.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\n\n\ndef len_safe_get_embedding(text, model=EMBEDDING_MODEL, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING, average=True):\n    chunk_embeddings = []\n    chunk_lens = []\n    for chunk in chunked_tokens(text, encoding_name=encoding_name, chunk_length=max_tokens):\n        chunk_embeddings.append(get_embedding(chunk, model=model))\n        chunk_lens.append(len(chunk))\n\n    if average:\n        chunk_embeddings = np.average(chunk_embeddings, axis=0, weights=chunk_lens)\n        chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings)  # normalizes length to 1\n        chunk_embeddings = chunk_embeddings.tolist()\n    return chunk_embeddings\n```\n\n----------------------------------------\n\nTITLE: Running User Prompt Interactions with Langchain Agent in Python\nDESCRIPTION: Defines a wrapper function for interacting with the agent executor using user-supplied prompts. The function expects a user prompt string, feeds it to agent_executor.run, and implicitly returns the agent's results. Prerequisites include an initialized agent_executor object as per earlier snippets. The function's input is a user's prompt, and the output depends on the agent's implementation. Mainly for demonstration or simple script-based user interaction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\ndef agent_interaction(user_prompt):\n    agent_executor.run(user_prompt)\n```\n\n----------------------------------------\n\nTITLE: Creating Relevancy Evaluator with gpt-4 - Python\nDESCRIPTION: This code instantiates a `RelevancyEvaluator` using the `gpt-4` service context.  This evaluator is used to check if the response and retrieved context matches the query. This is used to evaluate if the response actually answers the query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.evaluation import RelevancyEvaluator\n\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Multiple Articles for Summarization in Python\nDESCRIPTION: Iterates through the loaded article contents, calls the summarization function on each, and appends the structured ArticleSummary Pydantic models to a list while printing progress messages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsummaries = []\n\nfor i in range(len(content)):\n    print(f\"Analyzing article #{i+1}...\")\n    summaries.append(get_article_summary(content[i]))\n    print(\"Done.\")\n```\n\n----------------------------------------\n\nTITLE: Transcribing & Spellchecking with GPT-4\nDESCRIPTION: This code defines a function `transcribe_with_spellcheck` that utilizes GPT-4 to correct the output of the Whisper transcription.  It takes a system message (prompt for GPT-4) and audio file path as input. It calls the `transcribe` function to get the initial transcription then uses the `client.chat.completions.create` method, providing the system message, user content (transcription), and model. It returns the GPT-4's corrected output. This approach leverages GPT-4 for post-processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# define a wrapper function for seeing how prompts affect transcriptions\ndef transcribe_with_spellcheck(system_message, audio_filepath):\n    completion = client.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(prompt=\"\", audio_filepath=audio_filepath),\n            },\n        ],\n    )\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Processing SharePoint File Content with Microsoft Graph (JavaScript)\nDESCRIPTION: Implements a JavaScript async function to fetch, stream, and process the contents of files stored in SharePoint or O365 using Microsoft Graph. Converts supported file types to PDF for streamlined text extraction, handles '.txt' and '.csv' uniquely, and returns extracted text or content for downstream processing. Key dependencies include a Microsoft Graph API client and pdfParse for PDF processing. Inputs are the client instance, drive/item IDs, and filename; output is the extracted file text or a relevant error message. Not suitable for unsupported file types.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst getDriveItemContent = async (client, driveId, itemId, name) => {\n    try {\n        const fileType = path.extname(name).toLowerCase();\n        // the below files types are the ones that are able to be converted to PDF to extract the text. See https://learn.microsoft.com/en-us/graph/api/driveitem-get-content-format?view=graph-rest-1.0&tabs=http\n        const allowedFileTypes = ['.pdf', '.doc', '.docx', '.odp', '.ods', '.odt', '.pot', '.potm', '.potx', '.pps', '.ppsx', '.ppsxm', '.ppt', '.pptm', '.pptx', '.rtf'];\n        // filePath changes based on file type, adding ?format=pdf to convert non-pdf types to pdf for text extraction, so all files in allowedFileTypes above are converted to pdf\n        const filePath = `/drives/${driveId}/items/${itemId}/content` + ((fileType === '.pdf' || fileType === '.txt' || fileType === '.csv') ? '' : '?format=pdf');\n        if (allowedFileTypes.includes(fileType)) {\n            response = await client.api(filePath).getStream();\n            // The below takes the chunks in response and combines\n            let chunks = [];\n            for await (let chunk of response) {\n                chunks.push(chunk);\n            }\n            let buffer = Buffer.concat(chunks);\n            // the below extracts the text from the PDF.\n            const pdfContents = await pdfParse(buffer);\n            return pdfContents.text;\n        } else if (fileType === '.txt') {\n            // If the type is txt, it does not need to create a stream and instead just grabs the content\n            response = await client.api(filePath).get();\n            return response;\n        }  else if (fileType === '.csv') {\n            response = await client.api(filePath).getStream();\n            let chunks = [];\n            for await (let chunk of response) {\n                chunks.push(chunk);\n            }\n            let buffer = Buffer.concat(chunks);\n            let dataString = buffer.toString('utf-8');\n            return dataString\n            \n    } else {\n        return 'Unsupported File Type';\n    }\n     \n    } catch (error) {\n        console.error('Error fetching drive content:', error);\n        throw new Error(`Failed to fetch content for ${name}: ${error.message}`);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Info - Python\nDESCRIPTION: Displays information about the `article_df` DataFrame, including column names, data types, and non-null counts. The `show_counts=True` argument ensures that non-null counts are displayed for each column.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\narticle_df.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a Reusable GPT-3.5 Turbo-Instruct Completion Function in Python\nDESCRIPTION: Encapsulates the logic to call the OpenAI GPT-3.5 Turbo-Instruct completion method into a function 'complete', enabling reuse by passing any prompt string. It maintains fixed parameters for deterministic, controlled responses and returns the processed text output directly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef complete(prompt):\n    res = openai.Completion.create(\n        engine='gpt-3.5-turbo-instruct',\n        prompt=prompt,\n        temperature=0,\n        max_tokens=400,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None\n    )\n    return res['choices'][0]['text'].strip()\n```\n\n----------------------------------------\n\nTITLE: Querying GPT-3.5 Turbo-Instruct Without Context in Python\nDESCRIPTION: Demonstrates how to send a prompt directly to the OpenAI GPT-3.5 Turbo-Instruct completion endpoint without providing any additional context. It uses fixed parameters like temperature=0 for deterministic output and max_tokens=400 to limit response size. The output is extracted from returned JSON's first choice text field.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nquery = \"who was the 12th person on the moon and when did they land?\"\n\n# now query `gpt-3.5-turbo-instruct` WITHOUT context\nres = openai.Completion.create(\n    engine='gpt-3.5-turbo-instruct',\n    prompt=query,\n    temperature=0,\n    max_tokens=400,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None\n)\n\nres['choices'][0]['text'].strip()\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store with Expiration Policy (Python/Node.js)\nDESCRIPTION: This snippet demonstrates how to create a new vector store using the OpenAI API and configure an expiration policy. It shows how to specify the files to include (`file_ids`) and set the `expires_after` policy, anchored to `last_active_at` for 7 days, to manage storage costs. Requires the OpenAI client library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nvector_store = client.beta.vector_stores.create_and_poll(\n  name=\"Product Documentation\",\n  file_ids=['file_1', 'file_2', 'file_3', 'file_4', 'file_5'],\n  expires_after={\n\t  \"anchor\": \"last_active_at\",\n\t  \"days\": 7\n  }\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nlet vectorStore = await openai.beta.vectorStores.create({\n  name: \"rag-store\",\n  file_ids: ['file_1', 'file_2', 'file_3', 'file_4', 'file_5'],\n  expires_after: {\n    anchor: \"last_active_at\",\n    days: 7\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Performing headline classification without logprobs (Python example)\nDESCRIPTION: Iterates over sample headlines, sending classification prompts to the API without logprobs enabled. Prints the model's category decision for each headline, serving as a baseline for confidence assessment when logprobs are not used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nheadlines = [\n    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n]\n\nfor headline in headlines:\n    print(f\"\\nHeadline: {headline}\")\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4o\",\n    )\n    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating the ChromaDB Client in Python\nDESCRIPTION: Creates an instance of the ChromaDB client. By default, it initializes an `EphemeralClient`, which runs entirely in memory and does not persist data to disk. A commented-out alternative shows how to create a `PersistentClient` for disk-based storage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nchroma_client = chromadb.EphemeralClient() # Equivalent to chromadb.Client(), ephemeral.\n# Uncomment for persistent client\n# chroma_client = chromadb.PersistentClient()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Uploading DALL-E Image (Python/OpenAI Files API)\nDESCRIPTION: Downloads the image generated by DALL-E 3 from the provided URL using the `requests` library. It saves the image content locally as a PNG file. Subsequently, it uploads this local image file to the OpenAI Assistants API file storage, potentially for later use or reference within the thread context.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndalle_img_path = '../images/dalle_image.png'\nimg = requests.get(image_url)\n\n#Save locally\nwith open(dalle_img_path,'wb') as file:\n  file.write(img.content)\n\n#Upload\ndalle_file = client.files.create(\n  file=open(dalle_img_path, \"rb\"),\n  purpose='assistants'\n)\n\n```\n\n----------------------------------------\n\nTITLE: Querying Graph Database with Generated Embeddings and Cypher Query in Python\nDESCRIPTION: Executes a Cypher query against the graph database by first creating embeddings for each entity from the user response, then calling the 'create_query' function to generate the Cypher query string. It prepares parameters mapping entity names to their embedding vectors, executes the query via 'graph.query', and returns the results. This snippet depends on a functioning 'create_embedding' function, a 'graph' database connection object, and the 'create_query' function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef query_graph(response):\n    embeddingsParams = {}\n    query = create_query(response)\n    query_data = json.loads(response)\n    for key, val in query_data.items():\n        embeddingsParams[f\"{key}Embedding\"] = create_embedding(val)\n    result = graph.query(query, params=embeddingsParams)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing Variables in Python\nDESCRIPTION: Imports necessary Python libraries including openai, pandas, os, wget, ast, and chromadb. Sets the `EMBEDDING_MODEL` variable to 'text-embedding-3-small' for OpenAI embeddings and configures warning filters to suppress specific warnings like unclosed SSL sockets and deprecation warnings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pandas as pd\nimport os\nimport wget\nfrom ast import literal_eval\n\n# Chroma's client library for Python\nimport chromadb\n\n# I've set this to our new embeddings model, this can be changed to the embedding model of your choice\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n```\n\n----------------------------------------\n\nTITLE: Creating PaymentIntents with Dispute Scenarios Using Stripe API in Python\nDESCRIPTION: These snippets demonstrate how to create Stripe PaymentIntent objects configured to simulate different dispute scenarios for testing the dispute resolution workflow. Scenario 1 crafts a PaymentIntent with metadata indicating a company mistake (product not received), while Scenario 2 models a customer-initiated dispute during a final sale. Both use asynchronous calls to confirm payment methods and enable automatic payment processing, including off-session payments. They pass the resulting PaymentIntent ID to the previously defined process_dispute function along with the triage_agent to trigger the AI-driven dispute management logic. Dependencies include the Stripe Python SDK and an asynchronous runtime to handle await expressions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npayment = stripe.PaymentIntent.create(\n  amount=2000,\n  currency=\"usd\",\n  payment_method = \"pm_card_createDisputeProductNotReceived\",\n  confirm=True,\n  metadata={\"order_id\": \"1234\"},\n  off_session=True,\n  automatic_payment_methods={\"enabled\": True},\n)\nrelevant_data, triage_result = await process_dispute(payment.id, triage_agent)\n\n```\n\nLANGUAGE: python\nCODE:\n```\npayment = stripe.PaymentIntent.create(\n  amount=2000,\n  currency=\"usd\",\n  payment_method = \"pm_card_createDispute\",\n  confirm=True,\n  metadata={\"order_id\": \"1121\"},\n  off_session=True,\n  automatic_payment_methods={\"enabled\": True},\n)\nrelevant_data, triage_result = await process_dispute(payment.id, triage_agent)\n\n```\n\n----------------------------------------\n\nTITLE: Building Kusto Connection String (Python/Kusto)\nDESCRIPTION: Constructs a Kusto connection string builder configured for AAD device authentication using the previously defined cluster URI and sets the AAD authority ID (tenant ID).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nKCSB = KustoConnectionStringBuilder.with_aad_device_authentication(\n    KUSTO_CLUSTER)\nKCSB.authority_id = AAD_TENANT_ID\n```\n\n----------------------------------------\n\nTITLE: Importing JSON Data into Neo4j Graph Database in Python\nDESCRIPTION: Defines a `sanitize` helper function to remove problematic characters (quotes, curly braces) from text data before insertion. It then iterates through the `jsonData` list (loaded from the JSON file), constructs a Cypher `MERGE` query for each object to create or update `Product` nodes and related entity nodes (e.g., `Category`, `Brand`) and their relationships. Each query is executed against the database using `graph.query()`. Requires `jsonData` and the `graph` connection object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef sanitize(text):\n    text = str(text).replace(\"'\",\"\").replace('\"','').replace('{','').replace('}', '')\n    return text\n\n# Loop through each JSON object and add them to the db\ni = 1\nfor obj in jsonData:\n    print(f\"{i}. {obj['product_id']} -{obj['relationship']}-> {obj['entity_value']}\")\n    i+=1\n    query = f'''\n        MERGE (product:Product {{id: {obj['product_id']}}})\n        ON CREATE SET product.name = \"{sanitize(obj['product'])}\", \n                       product.title = \"{sanitize(obj['TITLE'])}\", \n                       product.bullet_points = \"{sanitize(obj['BULLET_POINTS'])}\", \n                       product.size = {sanitize(obj['PRODUCT_LENGTH'])}\n\n        MERGE (entity:{obj['entity_type']} {{value: \"{sanitize(obj['entity_value'])}\"}})\n\n        MERGE (product)-[:{obj['relationship']}]->(entity)\n        '''\n    graph.query(query)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model Deployment Name\nDESCRIPTION: Stores the deployment name of the Azure OpenAI model that will be used for generating embeddings. This name must be created in the Azure OpenAI Studio first.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndeployment = \"\" # Fill in the deployment name from the portal here\n```\n\n----------------------------------------\n\nTITLE: Extract Sections from Wikipedia Articles\nDESCRIPTION: This code iterates through a list of Wikipedia article titles and extracts all subsections from each article using the `all_subsections_from_title` function. The extracted sections are stored in the `wikipedia_sections` list, and the number of sections found is printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# split pages into sections\n# may take ~1 minute per 100 articles\nwikipedia_sections = []\nfor title in titles:\n    wikipedia_sections.extend(all_subsections_from_title(title))\nprint(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n```\n\n----------------------------------------\n\nTITLE: Defining W&B Entity, Project, and Stream Names - Python\nDESCRIPTION: This snippet initializes string variables for WB_ENTITY (user or team), WB_PROJECT, and STREAM_NAME, configuring destination identifiers for logging monitored API activity in W&B. Required before initializing a monitor or logging data streams. Expected inputs are set by direct assignment; no output unless these are later printed or used in downstream functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nWB_ENTITY = \"\" # set to your wandb username or team name\nWB_PROJECT = \"weave\" # top-level directory for this work\nSTREAM_NAME = \"openai_logs\" # record table which stores the logs of OpenAI API calls as they stream in\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAPI Schema for Retool Workflow Integration in GPT Actions\nDESCRIPTION: This YAML snippet defines the OpenAPI schema required to connect a Custom GPT to a Retool Workflow. It establishes an endpoint for triggering a workflow that adds two numbers together. The schema includes server configuration, authentication requirements, request parameters, and possible response codes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_retool_workflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Retool Workflow API\n  description: API for interacting with Retool workflows.\n  version: 1.0.0\nservers:\n  - url: https://api.retool.com/v1\n    description: Main (production) server\npaths:\n  /workflows/<WORKFLOW_ID>/startTrigger:\n    post:\n      operationId: add_numbers\n      summary: Takes 2 numbers and adds them.\n      description: Initiates a workflow in Retool by triggering a specific workflow ID.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                first:\n                  type: integer\n                  description: First parameter for the workflow.\n                second:\n                  type: integer\n                  description: Second parameter for the workflow.\n      responses:\n        \"200\":\n          description: Workflow triggered successfully.\n        \"400\":\n          description: Bad Request - Invalid parameters or missing data.\n        \"401\":\n          description: Unauthorized - Invalid or missing API key.\n      security:\n        - apiKeyAuth: []\n```\n\n----------------------------------------\n\nTITLE: Streaming Assistant Run with File Search in Python\nDESCRIPTION: Defines a custom EventHandler subclass using OpenAI's Python SDK to handle streaming assistant output events. It prints events for text creation, tool calls, and message completion. On message done, it extracts and replaces annotated texts with reference indices, retrieves cited file names, and prints the response with citations. It then uses the beta threads runs streaming SDK helper to stream a run's response using this handler. Suitable for showing incremental responses and sourcing document citations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import override\nfrom openai import AssistantEventHandler, OpenAI\n \nclient = OpenAI()\n \nclass EventHandler(AssistantEventHandler):\n    @override\n    def on_text_created(self, text) -> None:\n        print(f\"\\nassistant > \", end=\"\", flush=True)\n \n    @override\n    def on_tool_call_created(self, tool_call):\n        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n \n    @override\n    def on_message_done(self, message) -> None:\n        # print a citation to the file searched\n        message_content = message.content[0].text\n        annotations = message_content.annotations\n        citations = []\n        for index, annotation in enumerate(annotations):\n            message_content.value = message_content.value.replace(\n                annotation.text, f\"[{index}]\"\n            )\n            if file_citation := getattr(annotation, \"file_citation\", None):\n                cited_file = client.files.retrieve(file_citation.file_id)\n                citations.append(f\"[{index}] {cited_file.filename}\")\n \n        print(message_content.value)\n        print(\"\\n\".join(citations))\n \n# Then, we use the stream SDK helper\n# with the EventHandler class to create the Run\n# and stream the response.\nwith client.beta.threads.runs.stream(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n    event_handler=EventHandler(),\n) as stream:\n    stream.until_done()\n```\n\n----------------------------------------\n\nTITLE: Handling Content Filter Triggers in Azure OpenAI\nDESCRIPTION: Demonstrates how to handle cases where the Azure OpenAI content filter flags input messages. Uses try/except to catch BadRequestError with content_filter error code and extracts the filter results for further analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"<text violating the content policy>\"}\n]\n\ntry:\n    completion = client.chat.completions.create(\n        messages=messages,\n        model=deployment,\n    )\nexcept openai.BadRequestError as e:\n    err = json.loads(e.response.text)\n    if err[\"error\"][\"code\"] == \"content_filter\":\n        print(\"Content filter triggered!\")\n        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n        for category, details in content_filter_result.items():\n            print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")\n```\n\n----------------------------------------\n\nTITLE: Creating the OpenAI Eval for Bulk Experimentation\nDESCRIPTION: Initializes the evaluation by sending the configuration to OpenAI's Evals API, specifying the name, description, data source configuration, and testing criteria.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\neval_create_result = openai.evals.create(\n    name=\"Push Notification Bulk Experimentation Eval\",\n    metadata={\n        \"description\": \"This eval tests many prompts and models to find the best performing combination.\",\n    },\n    data_source_config=data_source_config,\n    testing_criteria=[push_notification_grader],\n)\neval_id = eval_create_result.id\n```\n\n----------------------------------------\n\nTITLE: Dataset diagnostics: missing messages and token count distribution\nDESCRIPTION: Analyzes dataset for missing 'system' or 'user' messages and computes message and token count distributions. Provides insights into message complexity, verbosity, and potential dataset issues relevant for fine-tuning constraints and budgeting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nn_missing_system = 0\nn_missing_user = 0\nn_messages = []\nconvo_lens = []\nassistant_message_lens = []\n\nfor ex in dataset:\n    messages = ex[\"messages\"]\n    if not any(message[\"role\"] == \"system\" for message in messages):\n        n_missing_system += 1\n    if not any(message[\"role\"] == \"user\" for message in messages):\n        n_missing_user += 1\n    n_messages.append(len(messages))\n    convo_lens.append(num_tokens_from_messages(messages))\n    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n    \nprint(\"Num examples missing system message:\", n_missing_system)\nprint(\"Num examples missing user message:\", n_missing_user)\nprint_distribution(n_messages, \"num_messages_per_example\")\nprint_distribution(convo_lens, \"num_total_tokens_per_example\")\nprint_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n n_too_long = sum(l > 16385 for l in convo_lens)\nprint(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Messages Array for OpenAI in JavaScript\nDESCRIPTION: This code initializes the `messages` array, which is used to store the conversation history between the user and the OpenAI model.  The initial message sets the system role, instructing the model to behave as a helpful assistant and only use the provided functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst messages = [\n  {\n    role: \"system\",\n    content:\n      \"You are a helpful assistant. Only use the functions you have been provided with.\",\n  },\n];\n```\n\n----------------------------------------\n\nTITLE: Read CSV file into Pandas DataFrame\nDESCRIPTION: This snippet reads the unzipped CSV file containing the Wikipedia embeddings into a Pandas DataFrame using `pd.read_csv`. This DataFrame facilitates easier manipulation and indexing of the data into Elasticsearch.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwikipedia_dataframe = pd.read_csv(\"data/vector_database_wikipedia_articles_embedded.csv\")\n```\n\n----------------------------------------\n\nTITLE: Using Code Execution for Mathematical Calculations in Python\nDESCRIPTION: This example demonstrates how to instruct an AI model to solve mathematical problems by writing and executing Python code. The code finds all real-valued roots of a polynomial equation by using Python's numerical capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Code execution example for mathematical calculations\n```\n\n----------------------------------------\n\nTITLE: Running Contextual Follow-Up Query with Memory in LangChain (Python)\nDESCRIPTION: Calls the agent_executor with the memory capability for the user prompt 'how about in mexico?', leveraging the retained context from the previous exchanges. Expects a configured memory-enabled agent_executor object. The output considers prior conversational state, providing improved response continuity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nagent_executor.run(\"how about in mexico?\")\n\n```\n\n----------------------------------------\n\nTITLE: Batching Multiple Prompts with Structured Outputs Using OpenAI API and Pydantic in Python\nDESCRIPTION: Shows how to batch prompts and receive a structured response using Pydantic's BaseModel for output validation, leveraging OpenAI's Structured Outputs functionality. Defines a StoryResponse schema with a list of stories and a count, constructs developer/user role messages concatenating prompts, and parses the API response into the specified data structure. Requires openai Python client (with beta parse), Python's pydantic library, and proper schema alignment; suitable for high-throughput applications seeking reliable, type-safe batch completions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n# Define the Pydantic model for the structured output\nclass StoryResponse(BaseModel):\n    stories: list[str]\n    story_count: int\n\nnum_stories = 10\ncontent = \"Once upon a time,\"\n\nprompt_lines = [f\"Story #{i+1}: {content}\" for i in range(num_stories)]\nprompt_text = \"\\n\".join(prompt_lines)\n\nmessages = [\n    {\n        \"role\": \"developer\",\n        \"content\": \"You are a helpful assistant. Please respond to each prompt as a separate short story.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": prompt_text\n    }\n]\n\n# batched example, with all story completions in one request and using structured outputs\nresponse = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n    response_format=StoryResponse,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Assessing Claims with Context from Hallucinated Document Retrieval in Python\nDESCRIPTION: Illustrates evaluating claims using a language model by providing retrieved documents from hallucinated evidence as context. The assess_claims_with_context function is called with the original claims and the filtered documents to perform claim verification. The outputs are then passed to a confusion_matrix function alongside ground truth labels to evaluate assessment accuracy, supporting empirical validation of the HyDE method effectiveness.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ngpt_with_hallucinated_context_evaluation = assess_claims_with_context(claims, filtered_hallucinated_query_result['documents'])\nconfusion_matrix(gpt_with_hallucinated_context_evaluation, groundtruth)\n```\n\n----------------------------------------\n\nTITLE: Extracting Embeddings from OpenAI Response\nDESCRIPTION: Extracts the embedding vectors from the OpenAI API response into a Python list for easier manipulation and storage. This is useful for processing multiple embeddings at once.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# we can extract embeddings to a list\nembeds = [record.embedding for record in res.data]\nlen(embeds)\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Chat Completion - Curl\nDESCRIPTION: This cURL command demonstrates how to call the OpenAI Chat Completions API in JSON mode. It sends a POST request with the `response_format` set to `json_object` and includes a system message and a user message. The `Authorization` header must contain a valid OpenAI API key. It assumes `OPENAI_API_KEY` is set as an environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_7\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-3.5-turbo-0125\",\n    \"response_format\": { \"type\": \"json_object\" },\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant designed to output JSON.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Who won the world series in 2020?\"\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Source for OpenAI Eval\nDESCRIPTION: Defines the data source configuration for the evaluation, specifying the item schema based on the PushNotifications model and enabling sample schema inclusion for API completions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# We want our input data to be available in our variables, so we set the item_schema to\n# PushNotifications.model_json_schema()\ndata_source_config = {\n    \"type\": \"custom\",\n    \"item_schema\": PushNotifications.model_json_schema(),\n    # We're going to be uploading completions from the API, so we tell the Eval to expect this\n    \"include_sample_schema\": True,\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Q&A Data with Pandas and OpenAI in Python\nDESCRIPTION: Imports necessary libraries (openai, pandas), loads the Olympics Q&A dataset from 'olympics-data/olympics_qa.csv' into a pandas DataFrame, defines a search file ID variable potentially used for context retrieval, and displays the first few rows of the loaded data for inspection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport pandas as pd\ndf = pd.read_csv('olympics-data/olympics_qa.csv')\nolympics_search_fileid = \"file-c3shd8wqF3vSCKaukW4Jr1TT\"\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Inserting Data Concurrently into Partitioned Table (Python)\nDESCRIPTION: Prepares an INSERT statement for the partitioned table and uses the `execute_concurrent_with_args` utility to efficiently insert batches of data, computing OpenAI embeddings for each quote and structuring the data tuples for insertion according to the prepared statement and table schema. Requires `session`, `keyspace`, `client`, `embedding_model_name`, `philo_dataset`, and `uuid4`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nprepared_insertion = session.prepare(\n    f\"INSERT INTO {keyspace}.philosophers_cql_partitioned (quote_id, author, body, embedding_vector, tags) VALUES (?, ?, ?, ?, ?);\"\n)\n\nBATCH_SIZE = 50\n\nnum_batches = ((len(philo_dataset) + BATCH_SIZE - 1) // BATCH_SIZE)\n\nquotes_list = philo_dataset[\"quote\"]\nauthors_list = philo_dataset[\"author\"]\ntags_list = philo_dataset[\"tags\"]\n\nprint(\"Starting to store entries:\")\nfor batch_i in range(num_batches):\n    print(\"[...\", end=\"\")\n    b_start = batch_i * BATCH_SIZE\n    b_end = (batch_i + 1) * BATCH_SIZE\n    # compute the embedding vectors for this batch\n    b_emb_results = client.embeddings.create(\n        input=quotes_list[b_start : b_end],\n        model=embedding_model_name,\n    )\n    # prepare this batch's entries for insertion\n    tuples_to_insert = []\n    for entry_idx, emb_result in zip(range(b_start, b_end), b_emb_results.data):\n        if tags_list[entry_idx]:\n            tags = {\n                tag\n                for tag in tags_list[entry_idx].split(\";\")\n            }\n        else:\n            tags = set()\n        author = authors_list[entry_idx]\n        quote = quotes_list[entry_idx]\n        quote_id = uuid4()  # a new random ID for each quote. In a production app you'll want to have better control...\n        # append a *tuple* to the list, and in the tuple the values are ordered to match \"?\" in the prepared statement:\n        tuples_to_insert.append((quote_id, author, quote, emb_result.embedding, tags))\n    # insert the batch at once through the driver's concurrent primitive\n    conc_results = execute_concurrent_with_args(\n        session,\n        prepared_insertion,\n        tuples_to_insert,\n    )\n    # check that all insertions succeed (better to always do this):\n    if any([not success for success, _ in conc_results]):\n        print(\"Something failed during the insertions!\")\n    else:\n        print(f\"{len(b_emb_results.data)}] \", end=\"\")\n\nprint(\"\\nFinished storing entries.\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Qdrant Integration in Python\nDESCRIPTION: Sets environment variables for Qdrant URL and API key which are used to authenticate and connect with the Qdrant vector search engine. These are essential for RAG workflows that incorporate vector similarity search to enhance retrieval performance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"QDRANT_URL\"] = \"https://xxx.cloud.qdrant.io:6333\"\nos.environ[\"QDRANT_API_KEY\"] = \"xxx\"\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI GPT-3.5-Turbo-Instruct LLM in Python\nDESCRIPTION: Configures the OpenAI language model using Langchain's OpenAI wrapper with 'gpt-3.5-turbo-instruct' model, temperature set to 0 for deterministic responses, and unlimited token count. This LLM instance serves as the backbone for responses in the retrieval augmented generation pipeline.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-instruct\", max_tokens=-1)\n```\n\n----------------------------------------\n\nTITLE: Prompting for OpenAI API Key Using getpass - Python\nDESCRIPTION: This snippet leverages the 'getpass' module to securely prompt the user for their OpenAI API key. The entered key is stored in the 'OPENAI_API_KEY' variable for use in authenticating subsequent OpenAI API requests. No value is echoed to the console, ensuring the key remains private.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")\n\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Batch with OpenAI API - cURL\nDESCRIPTION: This cURL command cancels a batch.  It requires the `OPENAI_API_KEY` environment variable to be set with the user's API key. It sends a POST request to the `/v1/batches/{batch_id}/cancel` endpoint to cancel a batch given its id (`batch_abc123`). The command assumes the user has the cURL utility installed on their system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_6\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/batches/batch_abc123/cancel \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -X POST\n```\n\n----------------------------------------\n\nTITLE: Retrieving Weaviate Schema - Python\nDESCRIPTION: Queries the Weaviate client to retrieve the currently configured schema. This is typically done to verify that the class creation was successful. Requires a connected Weaviate client instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# get the schema to make sure it worked\nclient.schema.get()\n```\n\n----------------------------------------\n\nTITLE: API Request Handling in JavaScript for Crypto Market Data\nDESCRIPTION: This JavaScript snippet handles REST API requests to fetch real-time crypto market data from a public API endpoint. It uses fetch() for asynchronous requests, requiring no additional dependencies. The code manages request-response cycles, parses JSON data, and updates UI components or data stores accordingly. Typical inputs are API URLs; outputs include parsed data objects used for visualization or further processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/assets/011.txt#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nfetch('https://api.crypto.com/v1/marketdata')\n  .then(response => response.json())\n  .then(data => {\n    console.log('Market Data:', data);\n    updateChart(data);\n  })\n  .catch(error => console.error('Error fetching data:', error));\n\nfunction updateChart(data) {\n  // Code to update chart based on fetched data\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Pinecone and wget Python Packages\nDESCRIPTION: These commands install the Pinecone client and wget using pip. The Pinecone client is used to interact with the Pinecone vector database. Wget is used to download the zipped embeddings data file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# We'll need to install the Pinecone client\n!pip install pinecone-client\n\n#Install wget to pull zip file\n!pip install wget\n```\n\n----------------------------------------\n\nTITLE: Creating and polling OpenAI Assistant run\nDESCRIPTION: This Node.js snippet demonstrates creating and polling an OpenAI Assistant run, handling the 'requires_action' status by submitting tool outputs, and retrieving messages once the run completes. It includes asynchronous functions for handling required actions, checking the run status, and submitting tool outputs using the OpenAI Assistants API. Error handling and status updates are included.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/function-calling-run-example--polling.txt#_snippet_1\n\nLANGUAGE: node.js\nCODE:\n```\nconst handleRequiresAction = async (run) => {\n  // Check if there are tools that require outputs\n  if (\n    run.required_action &&\n    run.required_action.submit_tool_outputs &&\n    run.required_action.submit_tool_outputs.tool_calls\n  ) {\n    // Loop through each tool in the required action section\n    const toolOutputs = run.required_action.submit_tool_outputs.tool_calls.map(\n      (tool) => {\n        if (tool.function.name === \"getCurrentTemperature\") {\n          return {\n            tool_call_id: tool.id,\n            output: \"57\",\n          };\n        } else if (tool.function.name === \"getRainProbability\") {\n          return {\n            tool_call_id: tool.id,\n            output: \"0.06\",\n          };\n        }\n      },\n    );\n\n    // Submit all tool outputs at once after collecting them in a list\n    if (toolOutputs.length > 0) {\n      run = await client.beta.threads.runs.submitToolOutputsAndPoll(\n        thread.id,\n        run.id,\n        { tool_outputs: toolOutputs },\n      );\n      console.log(\"Tool outputs submitted successfully.\");\n    } else {\n      console.log(\"No tool outputs to submit.\");\n    }\n\n    // Check status after submitting tool outputs\n    return handleRunStatus(run);\n  }\n};\n\nconst handleRunStatus = async (run) => {\n  // Check if the run is completed\n  if (run.status === \"completed\") {\n    let messages = await client.beta.threads.messages.list(thread.id);\n    console.log(messages.data);\n    return messages.data;\n  } else if (run.status === \"requires_action\") {\n    console.log(run.status);\n    return await handleRequiresAction(run);\n  } else {\n    console.error(\"Run did not complete:\", run);\n  }\n};\n\n// Create and poll run\nlet run = await client.beta.threads.runs.createAndPoll(thread.id, {\n  assistant_id: assistant.id,\n});\n\nhandleRunStatus(run);\n```\n\n----------------------------------------\n\nTITLE: Comparing Thread Object Structure in v1 vs v2 (JSON)\nDESCRIPTION: Illustrates the difference in the Thread object structure between v1 and v2 betas. V1 Threads have no file or tool associations. V2 introduces the optional `tool_resources` field, allowing threads to manage their own file associations for tools like `file_search` (via `vector_store_ids`) and `code_interpreter` (via `file_ids`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/migration.txt#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"thread_abc123\",\n  \"object\": \"thread\",\n  \"created_at\": 1699012949,\n  \"metadata\": {}\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"thread_abc123\",\n  \"object\": \"thread\",\n  \"created_at\": 1699012949,\n  \"metadata\": {},\n  \"tools\": [\n    {\n      \"type\": \"file_search\"\n    },\n    {\n      \"type\": \"code_interpreter\"\n    }\n  ],\n  \"tool_resources\": {\n    \"file_search\": {\n      \"vector_store_ids\": [\"vs_abc\"]\n    },\n    \"code_interpreter\": {\n      \"file_ids\": [\"file-123\", \"file-456\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Book Dataset from Hugging Face\nDESCRIPTION: Downloads a dataset containing book titles and descriptions from Hugging Face Datasets, which will be used to populate the Zilliz database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\n\n# Download the dataset and only use the `train` portion (file is around 800Mb)\ndataset = datasets.load_dataset('Skelebor/book_titles_and_descriptions_en_clean', split='train')\n```\n\n----------------------------------------\n\nTITLE: Retrieving Batch Job Status and Data\nDESCRIPTION: This snippet retrieves the batch job's current state and details by ID, printing the batch job object. It uses the SDK's retrieval method, allowing monitoring of job completion and output availability. Dependencies include the client SDK and the batch job ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n# Retrieving the batch job status\n\nbatch_job = client.batches.retrieve(batch_job.id)\nprint(batch_job)\n```\n\n----------------------------------------\n\nTITLE: GPT-4 for Misspelled Word Detection\nDESCRIPTION: This code employs GPT-4 not just for correction but also for identifying and listing misspelled words based on a provided product list. The `system_prompt` guides GPT-4 to analyze the transcribed text, identify incorrect spellings, and replace them with the correct alternatives.  This demonstrates another application of GPT-4.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your first task is to list the words that are not spelled correctly according to the list provided to you and to tell me the number of misspelled words. Your next task is to insert those correct words in place of the misspelled ones. List: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array,  OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, AstroPixel Array, QuantumFlare Five, CyberPulse Six, VortexDrive Matrix, PhotonLink Ten, TriCircuit Array, PentaSync Seven, UltraWave Eight, QuantumVertex Nine, HyperHelix X, DigiSpiral Z, PentaQuark Eleven, TetraCube Twelve, GigaPhase Thirteen, EchoNeuron Fourteen, FusionPulse V15, MetaQuark Sixteen, InfiniCircuit Seventeen, TeraPulse Eighteen, ExoMatrix Nineteen, OrbiSync Twenty, QuantumHelix TwentyOne, NanoPhase TwentyTwo, TeraFractal TwentyThree, PentaHelix TwentyFour, ExoCircuit TwentyFive, HyperQuark TwentySix, GigaLink TwentySeven, FusionMatrix TwentyEight, InfiniFractal TwentyNine, MetaSync Thirty, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\nnew_text = transcribe_with_spellcheck(system_prompt, audio_filepath=ZyntriQix_filepath)\nprint(new_text)\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Whisper\nDESCRIPTION: This code defines a function `transcribe` that takes a prompt and an audio file path as input. It uses the OpenAI API to transcribe the audio file with the given prompt. The function opens the audio file in binary read mode, calls the `client.audio.transcriptions.create` method with the audio file, the model, and the prompt, and returns the transcribed text. This function is fundamental for interacting with Whisper.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# define a wrapper function for seeing how prompts affect transcriptions\ndef transcribe(prompt: str, audio_filepath) -> str:\n    \"\"\"Given a prompt, transcribe the audio file.\"\"\"\n    transcript = client.audio.transcriptions.create(\n        file=open(audio_filepath, \"rb\"),\n        model=\"whisper-1\",\n        prompt=prompt,\n    )\n    return transcript.text\n```\n\n----------------------------------------\n\nTITLE: Creating GPT Instructions for OpenAI Documentation Assistant\nDESCRIPTION: This Python code generates instruction text for a custom GPT that will act as an OpenAI documentation assistant. The instructions explain how the GPT should use the search API endpoint, including required parameters and guidelines for selecting categories based on user queries. The code copies these instructions to the clipboard for easy configuration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ninstructions = f'''\nYou are an OpenAI docs assistant. You have an action in your knowledge base where you can make a POST request to search for information. The POST request should always include: {{\n    \"query\": \"<user_query>\",\n    \"k_\": <integer>,\n    \"category\": <string, but optional>\n}}. Your goal is to assist users by performing searches using this POST request and providing them with relevant information based on the query.\n\nYou must only include knowledge you get from your action in your response.\nThe category must be from the following list: {categories}, which you should determine based on the user's query. If you cannot determine, then do not include the category in the POST request.\n'''\npyperclip.copy(instructions)\nprint(\"GPT Instructions copied to clipboard\")\nprint(instructions)\n```\n\n----------------------------------------\n\nTITLE: Testing Unauthorized Lambda Access (Bash)\nDESCRIPTION: This command uses `curl` to attempt to access the deployed Lambda function's API endpoint without authentication. It expects the output from the server to be 'Unauthorized' because of the Cognito authorizer configuration. The user needs the output URL from the deployment command.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d {} <middleware_api_output_url_from_deploy_command>\n```\n\n----------------------------------------\n\nTITLE: Starting Audio Recording and Streaming to Realtime Clients in React/JS\nDESCRIPTION: Defines an asynchronous `startRecording` function that sets the recording state and uses a `WavRecorder` instance (from `wavRecorderRef`) to capture audio. Inside the `record` callback, the captured mono PCM audio data (`data.mono`) is sent to every initialized Realtime client using `clientRef.current.appendInputAudio`, effectively forking the audio stream for multi-language processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst startRecording = async () => {\n    setIsRecording(true);\n    const wavRecorder = wavRecorderRef.current;\n\n    await wavRecorder.record((data) => {\n      // Send mic PCM to all clients\n      updatedLanguageConfigs.forEach(({ clientRef }) => {\n        clientRef.current.appendInputAudio(data.mono);\n      });\n    });\n  };\n```\n\n----------------------------------------\n\nTITLE: Querying with Example Prompts using the Agent in Python\nDESCRIPTION: Demonstrates agent usage with example search/recommendation prompts by calling the agent_interaction function in sequence. No additional dependencies beyond previous code. Inputs are hardcoded query strings and outputs will be the agent's responses or side effects (e.g., print statements). Intended for demonstration or notebook-based walkthroughs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nprompt1 = \"I'm searching for pink shirts\"\nagent_interaction(prompt1)\n```\n\nLANGUAGE: Python\nCODE:\n```\nprompt2 = \"Can you help me find a toys for my niece, she's 8\"\nagent_interaction(prompt2)\n```\n\nLANGUAGE: Python\nCODE:\n```\nprompt3 = \"I'm looking for nice curtains\"\nagent_interaction(prompt3)\n```\n\n----------------------------------------\n\nTITLE: Load precomputed article embeddings and store in Tair indexes\nDESCRIPTION: Reads the CSV containing article metadata and vectors, parses string-encoded vectors into lists, and inserts each article's title and content vectors into respective Tair indexes along with metadata like URL, title, and text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nfrom ast import literal_eval\n# Path to your local CSV file\ncsv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\narticle_df = pd.read_csv(csv_file_path)\n\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval).values\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval).values\n\n# add/update data to indexes\nfor i in range(len(article_df)):\n    # add data to index with title_vector\n    client.tvs_hset(index=index_names[0], key=article_df.id[i].item(), vector=article_df.title_vector[i], is_binary=False,\n                    **{\"url\": article_df.url[i], \"title\": article_df.title[i], \"text\": article_df.text[i]})\n    # add data to index with content_vector\n    client.tvs_hset(index=index_names[1], key=article_df.id[i].item(), vector=article_df.content_vector[i], is_binary=False,\n                    **{\"url\": article_df.url[i], \"title\": article_df.title[i], \"text\": article_df.text[i]})\n```\n\n----------------------------------------\n\nTITLE: Creating the Articles Table and ANN Indexes in AnalyticDB - SQL Executed via Python\nDESCRIPTION: Creates a 'public.articles' table in AnalyticDB with columns for article metadata and OpenAI-generated vector embeddings. Additionally, two ANN (Approximate Nearest Neighbor) indexes are created on the 'content_vector' and 'title_vector' columns, tailored for fast vector similarity search using L2 distance and HNSW/PQ. Requires a connected psycopg2 cursor and permissions to create tables and indexes. Commits the schema changes to the database. Adjusts dimensions and parameters for the embedding vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_table_sql = '''\nCREATE TABLE IF NOT EXISTS public.articles (\n    id INTEGER NOT NULL,\n    url TEXT,\n    title TEXT,\n    content TEXT,\n    title_vector REAL[],\n    content_vector REAL[],\n    vector_id INTEGER\n);\n\nALTER TABLE public.articles ADD PRIMARY KEY (id);\n'''\n\n# SQL statement for creating indexes\ncreate_indexes_sql = '''\nCREATE INDEX ON public.articles USING ann (content_vector) WITH (distancemeasure = l2, dim = '1536', pq_segments = '64', hnsw_m = '100', pq_centers = '2048');\n\nCREATE INDEX ON public.articles USING ann (title_vector) WITH (distancemeasure = l2, dim = '1536', pq_segments = '64', hnsw_m = '100', pq_centers = '2048');\n'''\n\n# Execute the SQL statements\ncursor.execute(create_table_sql)\ncursor.execute(create_indexes_sql)\n\n# Commit the changes\nconnection.commit()\n\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Translation (Python)\nDESCRIPTION: Uses `concurrent.futures.ThreadPoolExecutor` to perform the chunk translation tasks in parallel, significantly speeding up the process compared to the sequential approach. Collects results as they complete, handles potential errors, and saves the final translated document to a file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Function to translate a single chunk\ndef translate_chunk_wrapper(chunk, model='gpt-4o', dest_language='English'):\n    return translate_chunk(chunk, model=model, dest_language=dest_language)\n\n# Set the destination language\ndest_language = \"English\"\n\n# List to store translated chunks\ntranslated_chunks = []\n\n# Use ThreadPoolExecutor to parallelize the translation\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    # Submit all translation tasks\n    futures = {executor.submit(translate_chunk_wrapper, chunk, 'gpt-4o', dest_language): i for i, chunk in enumerate(chunks)}\n    \n    # Process completed tasks as they finish\n    for future in as_completed(futures):\n        i = futures[future]\n        try:\n            translated_chunk = future.result()\n            # Ensure chunks are added in original order (requires sorting/mapping later, \n            # but for this example, appending might mix order if not careful. \n            # A list indexed by original position or sorting at the end is better.)\n            # For simplicity in this snippet doc, assuming order preservation isn't strictly guaranteed by simple append.\n            # A proper implementation would store results in a list/dict indexed by 'i'.\n            # Let's append for now, as the original code does, assuming join handles it.\n            translated_chunks.append(translated_chunk)\n            print(f\"Chunk {i+1} / {len(chunks)} translated.\")\n        except Exception as e:\n            print(f\"Chunk {i+1} failed with exception: {e}\")\n\n# Note: The current appending method in the loop will likely result in \n# the translated_chunks list being out of order if chunks complete out of order.\n# A dictionary or list indexed by original index 'i' should be used to \n# preserve order before the final join.\n# Example fix sketch:\n# ordered_results = [None] * len(chunks)\n# for future in as_completed(futures):\n#     i = futures[future]\n#     try:\n#         ordered_results[i] = future.result()\n#         print(f\"Chunk {i+1} / {len(chunks)} translated.\")\n#     except Exception as e:\n#         print(f\"Chunk {i+1} failed with exception: {e}\")\n# result = '\\n\\n'.join(ordered_results)\n\n# Using the potentially out-of-order list from append for now, matching original snippet's structure\nresult = '\\n\\n'.join(translated_chunks)\n\n# Save the final result\nwith open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:\n    f.write(result)\n```\n\n----------------------------------------\n\nTITLE: Formatting Clustered Example Data in Python\nDESCRIPTION: This snippet constructs a formatted string by iterating over selected clustered examples from a pandas DataFrame, preparing input-output pairs with cluster labels for use in a prompt to the LLM. It concatenates product and category as input, description as output, and includes cluster information for each example. The formatted string serves as context for the model to understand existing clusters. Requires pandas and a DataFrame named 'selected_examples' containing columns 'Product', 'Category', 'Description', and 'Cluster'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nformatted_examples = \"\\n\".join(\n    f'Input: \\\"{row[\"Product\"]}, {row[\"Category\"]}\\\"\\nOutput: \\\"{row[\"Description\"]}\\\"\\nCluster: \\\"{row[\"Cluster\"]}\\\"'\n    for _, row in selected_examples.iterrows()\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Schema for Weather.gov API Integration\nDESCRIPTION: Defines the API endpoints for retrieving forecast grid points and forecast data from Weather.gov. This schema includes parameters for location coordinates and grid identifiers, and outlines the structure of the forecast responses, including weather periods with detailed attributes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/.gpt_action_getting_started.ipynb#_snippet_0\n\nLANGUAGE: OpenAPI\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: NWS Weather API\n  description: Access to weather data including forecasts, alerts, and observations.\n  version: 1.0.0\nservers:\n  - url: https://api.weather.gov\n    description: Main API Server\npaths:\n  /points/{latitude},{longitude}:\n    get:\n      operationId: getPointData\n      summary: Get forecast grid endpoints for a specific location\n      parameters:\n        - name: latitude\n          in: path\n          required: true\n          schema:\n            type: number\n            format: float\n          description: Latitude of the point\n        - name: longitude\n          in: path\n          required: true\n          schema:\n            type: number\n            format: float\n          description: Longitude of the point\n      responses:\n        '200':\n          description: Successfully retrieved grid endpoints\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  properties:\n                    type: object\n                    properties:\n                      forecast:\n                        type: string\n                        format: uri\n                      forecastHourly:\n                        type: string\n                        format: uri\n                      forecastGridData:\n                        type: string\n                        format: uri\n\n  /gridpoints/{office}/{gridX},{gridY}/forecast:\n    get:\n      operationId: getGridpointForecast\n      summary: Get forecast for a given grid point\n      parameters:\n        - name: office\n          in: path\n          required: true\n          schema:\n            type: string\n          description: Weather Forecast Office ID\n        - name: gridX\n          in: path\n          required: true\n          schema:\n            type: integer\n          description: X coordinate of the grid\n        - name: gridY\n          in: path\n          required: true\n          schema:\n            type: integer\n          description: Y coordinate of the grid\n      responses:\n        '200':\n          description: Successfully retrieved gridpoint forecast\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  properties:\n                    type: object\n                    properties:\n                      periods:\n                        type: array\n                        items:\n                          type: object\n                          properties:\n                            number:\n                              type: integer\n                            name:\n                              type: string\n                            startTime:\n                              type: string\n                              format: date-time\n                            endTime:\n                              type: string\n                              format: date-time\n                            temperature:\n                              type: integer\n                            temperatureUnit:\n                              type: string\n                            windSpeed:\n                              type: string\n                            windDirection:\n                              type: string\n                            icon:\n                              type: string\n                              format: uri\n                            shortForecast:\n                              type: string\n                            detailedForecast:\n                              type: string\n```\n\n----------------------------------------\n\nTITLE: Describing File Uploads in POST Requests with openaiFileIdRefs - YAML\nDESCRIPTION: This OpenAPI YAML fragment defines a POST endpoint for creating a widget from an uploaded image file via the openaiFileIdRefs parameter. The operation expects a requestBody with an array of file reference strings (though at runtime these are JSON objects with fields like name, id, mime_type, and download_link). JPG, WEBP, and PNG file types are supported, and the endpoint is intended for DALLÂ·E or user-uploaded images. Inputs include an array of file references, while outputs depend on API implementation. Requires integration with the OpenAI Actions file upload workflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n /createWidget:\n    post:\n      operationId: createWidget\n      summary: Creates a widget based on an image.\n      description: Uploads a file reference using its file id. This file should be an image created by DALLÂ·E or uploaded by the user. JPG, WEBP, and PNG are supported for widget creation.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                openaiFileIdRefs:\n                  type: array\n                  items:\n                    type: string\n```\n\n----------------------------------------\n\nTITLE: Generating Support Policies with GPT-4o in Python\nDESCRIPTION: This code uses GPT-4o to generate detailed customer support policies for different scenarios such as returns, feedback, shipping, and warranties. It utilizes a system prompt and examples to ensure consistent formatting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsystem_input_prompt = \"\"\"\nYou are a helpful assistant that can generate policies for a support agent at a fictional company to follow. You will be provided with a topic (ie. returns, refunds, feedback) and you are to generate a sample policy for how to handle the it.\n\nWhen constructing the policy, it should contain step-by-step instructions for how to handle the customer inquiry. It should include decision logic for what to do if a customer falls under a certain category, and provide requirements for taking specific actions.\n\"\"\"\n\nuser_policy_example_1 = \"\"\"\"\nRETURN POLICY\n\"\"\"\n\nassistant_policy_example_1 = \"\"\"\nRETURN POLICY\n\n1. ** Ask the customer why they want the order replaced **\n    - Categorize their issue into one of the following buckets:\n        - damaged: They received the product in a damaged state\n        - satisfaction: The customer is not satisfied with the item and does not like the product.\n        - unnecessary: They no longer need the item\n2a. **If return category is 'damaged'\n    - Ask customer for a picture of the damaged item\n    - If the item is indeed damaged, continue to step 3\n    - If the item is not damaged, notify the customer that this does not meet our requirements for return and they are not eligible for a refund\n    - Skip step 3 and go straight to step 4\n\n2b. **If return category is either 'satisfaction' or 'unnecessary'**\n    - Ask the customer if they can provide feedback on the quality of the item\n    - If the order was made within 30 days, notify them that they are eligible for a full refund\n    - If the order was made within 31-60 days, notify them that they are eligible for a partial refund of 50%\n    - If the order was made greater than 60 days ago, notify them that they are not eligible for a refund\n\n3. **If the customer is eligible for a return or refund**\n    - Ask the customer to confirm that they would like a return or refund\n    - Once they confirm, process their request\n\n4 **Provide additional support before closing out ticket**\n    - Ask the customer if there is anything else you can do to help them today.\n\n\"\"\"\n\nuser_policy_input = \"\"\"\n{{POLICY}}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client (Python)\nDESCRIPTION: Creates an instance of the OpenAI client using an API key. The key is preferentially sourced from the `OPENAI_API_KEY` environment variable; if not set, a placeholder string is used, which should be replaced with a valid key for the code to function. This client object is used for subsequent API calls to services like Whisper and Chat Completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Executing Partitioned Vector Similarity Search (Python)\nDESCRIPTION: Calls the `find_quote_and_author_p` function, providing a query string ('We struggle all our life for nothing'), limit (2), and an `author` filter ('nietzsche'), demonstrating a search that can utilize the database's partitioning scheme for potential performance improvements by narrowing the search scope.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nfind_quote_and_author_p(\"We struggle all our life for nothing\", 2, author=\"nietzsche\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Random Sample Questions for Testing\nDESCRIPTION: Selects five random questions from the dataset to test the QA system. Uses a fixed random seed for reproducibility.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nrandom.seed(52)\nselected_questions = random.choices(questions, k=5)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Image Variation with In-Memory Data (Node.js)\nDESCRIPTION: This code snippet demonstrates how to call the OpenAI image variation API with image data stored in a Node.js `Buffer` object.  It initializes the OpenAI client, sets the `name` property of the buffer object to indicate the image type, and then calls `openai.images.createVariation` to generate an image variation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-node-tips.txt#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\n\nasync function main() {\n  const image = await openai.images.createVariation({ model: \"dall-e-2\", image: buffer, n: 1, size: \"1024x1024\" });\n  console.log(image.data);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Python Imports for Unit Test Generation\nDESCRIPTION: Import statements needed for the unit test generation process, including the ast module for validating generated Python code and the openai library for API interaction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport ast  # used for detecting whether generated Python code is valid\nimport openai\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for OpenAI File Search\nDESCRIPTION: Imports required libraries and initializes the OpenAI client with API key. Sets up the directory path for accessing PDF files stored locally.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport concurrent\nimport PyPDF2\nimport os\nimport pandas as pd\nimport base64\n\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\ndir_pdfs = 'openai_blog_pdfs' # have those PDFs stored locally here\npdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n```\n\n----------------------------------------\n\nTITLE: Defining Push Notifications Schema with Pydantic\nDESCRIPTION: Creates a Pydantic model to define the structure of push notifications data, with a single field containing notification text. The schema is printed to show its JSON representation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass PushNotifications(pydantic.BaseModel):\n    notifications: str\n\nprint(PushNotifications.model_json_schema())\n```\n\n----------------------------------------\n\nTITLE: Processing Patch Text and Applying Changes in Python\nDESCRIPTION: Orchestrates the end-to-end patch application process. Takes patch text and file system interaction functions (`open_fn`, `write_fn`, `remove_fn`). It identifies needed files, loads their original content, parses the patch text, converts the patch to a commit, applies the commit to the filesystem, and returns a 'Done!' message upon success. Raises `DiffError` on failure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef process_patch(\n    text: str,\n    open_fn: Callable[[str], str],\n    write_fn: Callable[[str, str], None],\n    remove_fn: Callable[[str], None],\n) -> str:\n    if not text.startswith(\"*** Begin Patch\"):\n        raise DiffError(\"Patch text must start with *** Begin Patch\")\n    paths = identify_files_needed(text)\n    orig = load_files(paths, open_fn)\n    patch, _fuzz = text_to_patch(text, orig)\n    commit = patch_to_commit(patch, orig)\n    apply_commit(commit, write_fn, remove_fn)\n    return \"Done!\"\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting up LangChain\nDESCRIPTION: This code imports various libraries required for the project, including standard Python libraries like datetime, json, os, pandas, and re, as well as libraries specific to LangChain, OpenAI, and Pinecone. It also sets up the index name for the Pinecone vectorstore.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport json\nimport openai\nimport os\nimport pandas as pd\nimport pinecone\nimport re\nfrom tqdm.auto import tqdm\nfrom typing import List, Union\nimport zipfile\n\n# Langchain imports\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\nfrom langchain.prompts import BaseChatPromptTemplate, ChatPromptTemplate\nfrom langchain import SerpAPIWrapper, LLMChain\nfrom langchain.schema import AgentAction, AgentFinish, HumanMessage, SystemMessage\n# LLM wrapper\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import OpenAI\n# Conversational memory\nfrom langchain.memory import ConversationBufferWindowMemory\n# Embeddings and vectorstore\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Pinecone\n\n# Vectorstore Index\nindex_name = 'podcasts'\n```\n\n----------------------------------------\n\nTITLE: Saving the Section Dataset\nDESCRIPTION: This code saves the processed sections data from a Pandas DataFrame to a CSV file. The DataFrame, `df`, which contains extracted and filtered sections, is written to a CSV file named 'olympics-data/olympics_sections.csv'.  The `index=False` argument prevents writing the DataFrame index to the file. This file is intended to be used in the next notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.to_csv('olympics-data/olympics_sections.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Saving Generated PowerPoint to Local File in Python\nDESCRIPTION: This code extracts a PowerPoint file from an API response and saves it to a local file. It uses BytesIO for handling the binary data before writing it to disk.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\npptx_id = response.data[0].content[0].text.annotations[0].file_path.file_id\nppt_file= client.files.content(pptx_id)\nfile_obj = io.BytesIO(ppt_file.read())\nwith open(\"data/created_slides.pptx\", \"wb\") as f:\n    f.write(file_obj.getbuffer())\n```\n\n----------------------------------------\n\nTITLE: Training a Random Forest Classifier on food review embeddings in Python\nDESCRIPTION: This code loads a dataset of food reviews with embeddings, prepares the data by converting embedding strings to numpy arrays, splits into training and testing sets, trains a Random Forest Classifier, and evaluates its performance using classification metrics.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom ast import literal_eval\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)  # convert string to array\n\n# split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n)\n\n# train random forest classifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\n\nreport = classification_report(y_test, preds)\nprint(report)\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionaries for Vector ID Mapping\nDESCRIPTION: This snippet creates two dictionaries, `titles_mapped` and `content_mapped`, which map vector IDs to their corresponding titles and content, respectively.  These dictionaries are used to retrieve the original text of the search results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# First we'll create dictionaries mapping vector IDs to their outputs so we can retrieve the text for our search results\ntitles_mapped = dict(zip(article_df.vector_id,article_df.title))\ncontent_mapped = dict(zip(article_df.vector_id,article_df.text))\n```\n\n----------------------------------------\n\nTITLE: Defining Per-Item Generative Search Function Using Weaviate and OpenAI in Python\nDESCRIPTION: Defines a Python function, generative_search_per_item, that performs a generative search on a specified Weaviate collection using a single prompt for each object. The function constructs a query for a provided concept, applies with_generate with a formatted summarization prompt, checks for errors that may indicate exceeded API call limits, and returns the resulting objects. The client variable must already be connected to Weaviate, and environment variables must hold valid API keys; the returned list contains objects each augmented with a generative response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef generative_search_per_item(query, collection_name):\n    prompt = \"Summarize in a short tweet the following content: {content}\"\n\n    result = (\n        client.query\n        .get(collection_name, [\"title\", \"content\", \"url\"])\n        .with_near_text({ \"concepts\": [query], \"distance\": 0.7 })\n        .with_limit(5)\n        .with_generate(single_prompt=prompt)\n        .do()\n    )\n    \n    # Check for errors\n    if (\"errors\" in result):\n        print (\"\\033[91mYou probably have run out of OpenAI API calls for the current minute â€“ the limit is set at 60 per minute.\")\n        raise Exception(result[\"errors\"][0]['message'])\n    \n    return result[\"data\"][\"Get\"][collection_name]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Response Content\nDESCRIPTION: Extracts and prints the text content from the response object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(response.output[0].content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Interactions (JSON)\nDESCRIPTION: This code snippet converts all entries in `customer_interactions` list, assumed to contain JSON strings, into Python dictionaries and then creates a Pandas DataFrame from these dictionaries. It prepares the data for further analysis and manipulation using Pandas' data processing capabilities. Requires the `json` and `pandas` libraries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Decode the JSON strings\ndata = [json.loads(entry) for entry in customer_interactions]\n\n# Create a DataFrame from the cleaned data\ndf = pd.DataFrame(data)\n```\n\n----------------------------------------\n\nTITLE: Defining the Userâ€™s Search Question - Python\nDESCRIPTION: This snippet assigns a user-defined question about the NBA to the variable USER_QUESTION, which seeds the subsequent information retrieval steps. No dependencies are required. The question should be adapted as needed for other search targets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# User asks a question\nUSER_QUESTION = \"Who won the NBA championship? And who was the MVP? Tell me a bit about the last game.\"\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Connection Python\nDESCRIPTION: Initializes the connection to Pinecone using an API key and environment variables. It creates a new index if one does not exist. The OpenAI embeddings are created using `text-embedding-3-small`. It will then upload the embeddings created for document chunks and store them in Pinecone.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport pinecone\n\nindex_name = 'gpt-4-langchain-docs'\n\n# initialize connection to pinecone\npinecone.init(\n    api_key=\"PINECONE_API_KEY\",  # app.pinecone.io (console)\n    environment=\"PINECONE_ENVIRONMENT\"  # next to API key in console\n)\n\n# check if index already exists (it shouldn't if this is first time)\nif index_name not in pinecone.list_indexes():\n    # if does not exist, create index\n    pinecone.create_index(\n        index_name,\n        dimension=len(res['data'][0]['embedding']),\n        metric='dotproduct'\n    )\n# connect to index\nindex = pinecone.GRPCIndex(index_name)\n# view index stats\nindex.describe_index_stats()\n```\n\n----------------------------------------\n\nTITLE: Building a Data Visualization Slide Template in Python-pptx\nDESCRIPTION: This code creates a complete PowerPoint slide for data visualization with a black background, an image on the left side, a centered title, and a section for key insights with bullet points. It includes all necessary imports and comprehensive formatting for a professional presentation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nfrom pptx import Presentation\nfrom pptx.util import Inches, Pt\nfrom pptx.enum.text import PP_PARAGRAPH_ALIGNMENT\nfrom pptx.dml.color import RGBColor\n\n# Create a new presentation object\nprs = Presentation()\n\n# Add a blank slide layout\nblank_slide_layout = prs.slide_layouts[6]\nslide = prs.slides.add_slide(blank_slide_layout)\n\n# Set the background color of the slide to black\nbackground = slide.background\nfill = background.fill\nfill.solid()\nfill.fore_color.rgb = RGBColor(0, 0, 0)\n\n# Define placeholders\nimage_path = data_vis_img\ntitle_text = \"Maximizing Profits: The Dominance of Online Sales & Direct Sales Optimization\"\nbullet_points = \"â€¢ Online Sales consistently lead in profitability across quarters, indicating a strong digital market presence.\\nâ€¢ Direct Sales show fluctuations, suggesting variable performance and the need for targeted improvements in that channel.\"\n\n# Add image placeholder on the left side of the slide\nleft = Inches(0.2)\ntop = Inches(1.8)\nheight = prs.slide_height - Inches(3)\nwidth = prs.slide_width * 3/5\npic = slide.shapes.add_picture(image_path, left, top, width=width, height=height)\n\n# Add title text spanning the whole width\nleft = Inches(0)\ntop = Inches(0)\nwidth = prs.slide_width\nheight = Inches(1)\ntitle_box = slide.shapes.add_textbox(left, top, width, height)\ntitle_frame = title_box.text_frame\ntitle_frame.margin_top = Inches(0.1)\ntitle_p = title_frame.add_paragraph()\ntitle_p.text = title_text\ntitle_p.font.bold = True\ntitle_p.font.size = Pt(28)\ntitle_p.font.color.rgb = RGBColor(255, 255, 255)\ntitle_p.alignment = PP_PARAGRAPH_ALIGNMENT.CENTER\n\n# Add hardcoded \"Key Insights\" text and bullet points\nleft = prs.slide_width * 2/3\ntop = Inches(1.5)\nwidth = prs.slide_width * 1/3\nheight = Inches(4.5)\ninsights_box = slide.shapes.add_textbox(left, top, width, height)\ninsights_frame = insights_box.text_frame\ninsights_p = insights_frame.add_paragraph()\ninsights_p.text = \"Key Insights:\"\ninsights_p.font.bold = True\ninsights_p.font.size = Pt(24)\ninsights_p.font.color.rgb = RGBColor(0, 128, 100)\ninsights_p.alignment = PP_PARAGRAPH_ALIGNMENT.LEFT\ninsights_frame.add_paragraph()\n\n\nbullet_p = insights_frame.add_paragraph()\nbullet_p.text = bullet_points\nbullet_p.font.size = Pt(12)\nbullet_p.font.color.rgb = RGBColor(255, 255, 255)\nbullet_p.line_spacing = 1.5\n```\n\n----------------------------------------\n\nTITLE: Printing Search Results\nDESCRIPTION: This snippet calls the `get_search_results` function with `search_items` and then iterates through the returned results, printing the order, link, snippet, and summary for each search result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults = get_search_results(search_items)\n\nfor result in results:\n    print(f\"Search order: {result['order']}\")\n    print(f\"Link: {result['link']}\")\n    print(f\"Snippet: {result['title']}\")\n    print(f\"Summary: {result['Summary']}\")\n    print('-' * 80)\n```\n\n----------------------------------------\n\nTITLE: Encoding a Question with OpenAI's Embedding API in Python\nDESCRIPTION: Obtains an OpenAI API key interactively, sets the key for use, and uses the 'text-embedding-3-small' model to generate an embedding vector for a given question. Requires the 'openai' Python package and a valid API key. Outputs a vector representing the question's meaning for later vector search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Get OpenAI API key\nOPENAI_API_KEY = getpass(\"Enter OpenAI API key\")\n\n# Set API key\nopenai.api_key = OPENAI_API_KEY\n\n# Define model\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Define question\nquestion = 'Is the Atlantic the biggest ocean in the world?'\n\n# Create embedding\nquestion_embedding = openai.Embedding.create(input=question, model=EMBEDDING_MODEL)\n\n```\n\n----------------------------------------\n\nTITLE: PushNotifications Model JSON Schema Representation - JSON\nDESCRIPTION: Provides the JSON Schema generated for the PushNotifications Pydantic model. This schema defines a single string property 'notifications' and is used for validation and data format specification within Evals. Input: none. Output: JSON object for schema. Used by data_source_config in previous steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"properties\": {\n    \"notifications\": {\n      \"title\": \"Notifications\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\"notifications\"],\n  \"title\": \"PushNotifications\",\n  \"type\": \"object\"\n}\n```\n\n----------------------------------------\n\nTITLE: Continuing a Conversation with Previous Response\nDESCRIPTION: Shows how to continue a conversation by referring to a previous response ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresponse_two = client.responses.create(\n    model=\"gpt-4o-mini\",\n    input=\"tell me another\",\n    previous_response_id=response.id\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Batch Import in Weaviate Python Client\nDESCRIPTION: This snippet configures the batch import settings for the Weaviate client. It sets an initial batch size, enables dynamic adjustment of the batch size based on performance, and configures timeout retries for handling potential issues during import. This configuration optimizes the data ingestion process. It requires an initialized Weaviate `client` object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n### Step 2 - configure Weaviate Batch, with\n# - starting batch size of 100\n# - dynamically increase/decrease based on performance\n# - add timeout retries if something goes wrong\n\nclient.batch.configure(\n    batch_size=10, \n    dynamic=True,\n    timeout_retries=3,\n#   callback=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Upserting Points to Qdrant Collection in Python\nDESCRIPTION: Uploads a list of `PointStruct` objects (presumably generated by `generate_points_from_dataframe`) to a specified Qdrant collection using the `qdrant_client.upsert` method. The `wait=True` parameter ensures the operation completes before proceeding. Information about the upsert operation is stored and printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# qdrant_client and collection_name are assumed to be pre-configured\n# points is the list generated from the previous step\noperation_info = qdrant_client.upsert(\n    collection_name=collection_name, wait=True, points=points\n)\nprint(operation_info)\n```\n\n----------------------------------------\n\nTITLE: Main Extraction Function (Python)\nDESCRIPTION: This function orchestrates the entire extraction process. It iterates through the files in the specified input directory, processes each file if it's a file (not a directory), converts the PDF to base64 encoded images, and then extracts data from the images using the extract_from_multiple_pages function. The extracted data is saved to the specified output directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef main_extract(read_path, write_path):\n    for filename in os.listdir(read_path):\n        file_path = os.path.join(read_path, filename)\n        if os.path.isfile(file_path):\n            base64_images = pdf_to_base64_images(file_path)\n            extract_from_multiple_pages(base64_images, filename, write_path)\n```\n\n----------------------------------------\n\nTITLE: Creating a table with vector index in MyScale via SQL\nDESCRIPTION: Defines a new table in MyScale with columns for article data and a vector index based on the content_vector. Includes constraints for vector length and specifies cosine similarity metric.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclient.command(f\"\"\"\nCREATE TABLE IF NOT EXISTS default.articles\n(\n    id UInt64,\n    url String,\n    title String,\n    text String,\n    content_vector Array(Float32),\n    CONSTRAINT cons_vector_len CHECK length(content_vector) = {embedding_len},\n    VECTOR INDEX article_content_index content_vector TYPE HNSWFLAT('metric_type=Cosine')\n)\nENGINE = MergeTree ORDER BY id\n\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Embeddings Data for Clustering with Pandas and NumPy in Python\nDESCRIPTION: Loads a CSV dataset containing string-based embeddings, applies parsing and converts the embedding strings into numpy arrays using literal_eval and numpy.array, and vertically stacks them into a 2D numpy matrix. Dependencies: pandas, numpy, ast.literal_eval. Expects 'embedding' as a column in the CSV with array-like strings. Output is a 2D matrix suitable for scikit-learn clustering algorithms. CSV file must exist at the specified path; the embedding field must be well-formed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# imports\nimport numpy as np\nimport pandas as pd\nfrom ast import literal_eval\n\n# load data\ndatafile_path = \"./data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)  # convert string to numpy array\nmatrix = np.vstack(df.embedding.values)\nmatrix.shape\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Parsing Results from Saved File\nDESCRIPTION: This code loads the results file line by line, parsing each JSON string into a Python dictionary, and appends it to a list. It uses json.loads for deserialization. The snippet facilitates post-processing and analysis of individual responses. Dependencies include the json module and the file path.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n# Loading data from saved file\n\nresults = []\nwith open(result_file_name, 'r') as file:\n    for line in file:\n        # Parsing the JSON string into a dict and appending to the list of results\n        json_object = json.loads(line.strip())\n        results.append(json_object)\n```\n\n----------------------------------------\n\nTITLE: Function to generate routine from policy content using OpenAI API\nDESCRIPTION: This function uses the OpenAI client and the predefined prompt to convert a policy string into an executable routine. It handles API calls and returns the generated routine text, with basic exception handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_routine_generation.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef generate_routine(policy):\n    try:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    {CONVERSION_PROMPT}\n\n                    POLICY:\n                    {policy}\n                \"\"\"\n            }\n        ]\n\n        response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages\n        )\n        \n\n        return response.choices[0].message.content \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Passing Files to Code Interpreter at Assistant Level\nDESCRIPTION: Illustrates how to upload a file with the 'assistants' purpose and then associate it with a new Assistant that has Code Interpreter enabled. The file ID is provided within the `tool_resources.code_interpreter.file_ids` array during Assistant creation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Upload a file with an \"assistants\" purpose\nfile = client.files.create(\n  file=open(\"mydata.csv\", \"rb\"),\n  purpose='assistants'\n)\n\n# Create an assistant using the file ID\nassistant = client.beta.assistants.create(\n  instructions=\"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"code_interpreter\"}],\n  tool_resources={\n    \"code_interpreter\": {\n      \"file_ids\": [file.id]\n    }\n  }\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\n// Upload a file with an \"assistants\" purpose\nconst file = await openai.files.create({\n  file: fs.createReadStream(\"mydata.csv\"),\n  purpose: \"assistants\",\n});\n\n// Create an assistant using the file ID\nconst assistant = await openai.beta.assistants.create({\n  instructions: \"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n  model: \"gpt-4o\",\n  tools: [{\"type\": \"code_interpreter\"}],\n  tool_resources: {\n    \"code_interpreter\": {\n      \"file_ids\": [file.id]\n    }\n  }\n});\n```\n\nLANGUAGE: curl\nCODE:\n```\n# Upload a file with an \"assistants\" purpose\ncurl https://api.openai.com/v1/files \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -F purpose=\"assistants\" \\\n  -F file=\"@/path/to/mydata.csv\"\n\n# Create an assistant using the file ID\ncurl https://api.openai.com/v1/assistants \\\n  -u :$OPENAI_API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -H 'OpenAI-Beta: assistants=v2' \\\n  -d '{\n    \"instructions\": \"You are a personal math tutor. When asked a math question, write and run code to answer the question.\",\n    \"tools\": [{\"type\": \"code_interpreter\"}],\n    \"model\": \"gpt-4o\",\n    \"tool_resources\": {\n      \"code_interpreter\": {\n        \"file_ids\": [\"file-BK7bzQj3FfZFXr7DbL6xJwfo\"]\n      }\n    }\n  }'\n```\n\n----------------------------------------\n\nTITLE: Prompt-Completion Pair Format for Fine-tuning Base Models\nDESCRIPTION: Example demonstrating the prompt-completion pair format used for fine-tuning babbage-002 and davinci-002 models. Each line contains a prompt and its corresponding completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"prompt\": \"\", \"completion\": \"\"}\n{\"prompt\": \"\", \"completion\": \"\"}\n{\"prompt\": \"\", \"completion\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Example Response Body in Python\nDESCRIPTION: This is an example of the JSON structure that needs to be returned in the API response for the GPT Action to process the file.  The `openaiFileResponse` parameter contains an array of file objects. Each file object has three fields: `name`, `mime_type` and `content` which contains the base64 encoded string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n{\n  \"openaiFileResponse\": [\n    {\n      \"name\": \"output.csv\",\n      \"mime_type\": \"text/csv\",\n      \"content\": \"ImFjY291bn...NC41NyI=\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Self-Hosted Weaviate Client\nDESCRIPTION: Initializes a Weaviate client to connect to a self-hosted Weaviate instance running locally.  It specifies the URL of the Weaviate instance and includes a header with the OpenAI API key for vectorization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Option #1 - Self-hosted - Weaviate Open Source \nclient = weaviate.Client(\n    url=\"http://localhost:8080\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Showing Run Details (Python)\nDESCRIPTION: This snippet displays the full details of a run object using the OpenAI Assistants API and the show_json function. Requires the run variable from previous steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nshow_json(run)\n```\n\n----------------------------------------\n\nTITLE: Loading customer service articles from CSV file into a list of dictionaries\nDESCRIPTION: This snippet reads articles stored in a CSV file containing 'policy' and 'content' fields, appending each article as a dictionary into the 'articles' list for further processing. It prepares raw policy data for routine conversion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_routine_generation.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\narticles = []\n\nwith open('../data/helpcenter_articles.csv', mode='r', encoding='utf-8') as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        articles.append({\n            \"policy\": row[\"policy\"],\n            \"content\": row[\"content\"]\n        })\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Table and Uploading Vector Data from CSV using Python\nDESCRIPTION: Reads a CSV file into a pandas DataFrame, preprocesses the 'content_vector' field to convert string-encoded vectors (with brackets and comma-separated floats) into lists of float values, defines a BigQuery table schema including repeated FLOAT64 vector fields, creates the table if it does not exist, and inserts the DataFrame rows into the table. Errors during insertion are detected and printed. This snippet requires Google Cloud BigQuery client and pandas.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Read the CSV file, properly handling multiline fields\ndataset_id = project_id + '.' + raw_dataset_id\nclient = bigquery.Client(credentials=credentials, project=project_id)\ncsv_file_path = \"../embedded_data.csv\"\ndf = pd.read_csv(csv_file_path, engine='python', quotechar='\"', quoting=1)\n\n# Preprocess the data to ensure content_vector is correctly formatted\n# removing last and first character which are brackets [], comma splitting and converting to float\ndef preprocess_content_vector(row):\n    row['content_vector'] = [float(x) for x in row['content_vector'][1:-1].split(',')]\n    return row\n\n# Apply preprocessing to the dataframe\ndf = df.apply(preprocess_content_vector, axis=1)\n\n# Define the schema of the final table\nfinal_schema = [\n    bigquery.SchemaField(\"id\", \"STRING\"),\n    bigquery.SchemaField(\"vector_id\", \"STRING\"),\n    bigquery.SchemaField(\"title\", \"STRING\"),\n    bigquery.SchemaField(\"text\", \"STRING\"),\n    bigquery.SchemaField(\"title_vector\", \"STRING\"),\n    bigquery.SchemaField(\"content_vector\", \"FLOAT64\", mode=\"REPEATED\"),\n    bigquery.SchemaField(\"category\", \"STRING\"),\n]\n\n# Define the final table ID\nraw_table_id = 'embedded_data'\nfinal_table_id = f'{dataset_id}.' + raw_table_id\n\n# Create the final table object\nfinal_table = bigquery.Table(final_table_id, schema=final_schema)\n\n# Send the table to the API for creation\nfinal_table = client.create_table(final_table, exists_ok=True)  # API request\nprint(f\"Created final table {project_id}.{final_table.dataset_id}.{final_table.table_id}\")\n\n# Convert DataFrame to list of dictionaries for BigQuery insertion\nrows_to_insert = df.to_dict(orient='records')\n\n# Upload data to the final table\nerrors = client.insert_rows_json(f\"{final_table.dataset_id}.{final_table.table_id}\", rows_to_insert)  # API request\n\nif errors:\n    print(f\"Encountered errors while inserting rows: {errors}\")\nelse:\n    print(f\"Successfully loaded data into {dataset_id}:{final_table_id}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Context Sufficiency Using OpenAI API and Logprobs in Python\nDESCRIPTION: This snippet iterates over question lists, uses the get_completion function (assumed OpenAI chat wrapper), and evaluates whether enough context exists to answer each question by leveraging model log probabilities. For each question, it renders both the question and detailed log probability metrics into HTML, enabling visualization of model confidence in its assessments. Dependencies: OpenAI Python client, NumPy, and a visualization system (HTML). Inputs: List of questions, prompt template, article. Output: HTML summarizing the question, sufficiency boolean, logprob data. Top logprobs and token analysis are included for granular confidence reporting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nhtml_output = \"\"\nhtml_output += \"Questions clearly answered in article\"\n\nfor question in easy_questions:\n    API_RESPONSE = get_completion(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT.format(\n                    article=ada_lovelace_article, question=question\n                ),\n            }\n        ],\n        model=\"gpt-4o-mini\",\n        logprobs=True,\n    )\n    html_output += f'<p style=\"color:green\">Question: {question}</p>'\n    for logprob in API_RESPONSE.choices[0].logprobs.content:\n        html_output += f'<p style=\"color:cyan\">has_sufficient_context_for_answer: {logprob.token}, <span style=\"color:darkorange\">logprobs: {logprob.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(logprob.logprob)*100,2)}%</span></p>'\n\nhtml_output += \"Questions only partially covered in the article\"\n\nfor question in medium_questions:\n    API_RESPONSE = get_completion(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT.format(\n                    article=ada_lovelace_article, question=question\n                ),\n            }\n        ],\n        model=\"gpt-4o\",\n        logprobs=True,\n        top_logprobs=3,\n    )\n    html_output += f'<p style=\"color:green\">Question: {question}</p>'\n    for logprob in API_RESPONSE.choices[0].logprobs.content:\n        html_output += f'<p style=\"color:cyan\">has_sufficient_context_for_answer: {logprob.token}, <span style=\"color:darkorange\">logprobs: {logprob.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(logprob.logprob)*100,2)}%</span></p>'\n\ndisplay(HTML(html_output))\n\n```\n\n----------------------------------------\n\nTITLE: Defining Deployment Name for Azure OpenAI (Python)\nDESCRIPTION: This code snippet defines a variable `deployment` that holds the deployment name for the Whisper model. This value is used in later calls to the OpenAI API. You must replace \"whisper-deployment\" with the actual deployment name from the Azure portal.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndeployment = \"whisper-deployment\" # Fill in the deployment name from the portal here\n```\n\n----------------------------------------\n\nTITLE: Creating Articles Table with Vector Index - Python/SQL\nDESCRIPTION: Creates an SQL table named `articles` in the MyScale database to store the embeddings data. The table includes columns for `id`, `url`, `title`, `text`, and `content_vector`.  A vector index named `article_content_index` is created on the `content_vector` column using the `HNSWFLAT` algorithm with cosine distance. The length of the embeddings is enforced using a constraint.  This is followed by code to insert data in batches from the `article_df` into the table.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# create articles table with vector index\nembedding_len=len(article_df['content_vector'][0]) # 1536\n\nclient.command(f\"\"\"\nCREATE TABLE IF NOT EXISTS default.articles\n(\n    id UInt64,\n    url String,\n    title String,\n    text String,\n    content_vector Array(Float32),\n    CONSTRAINT cons_vector_len CHECK length(content_vector) = {embedding_len},\n    VECTOR INDEX article_content_index content_vector TYPE HNSWFLAT('metric_type=Cosine')\n)\nENGINE = MergeTree ORDER BY id\n\"\"\")\n\n# insert data into the table in batches\nfrom tqdm.auto import tqdm\n\nbatch_size = 100\ntotal_records = len(article_df)\n\n# we only need subset of columns\narticle_df = article_df[['id', 'url', 'title', 'text', 'content_vector']]\n\n# upload data in batches\ndata = article_df.to_records(index=False).tolist()\ncolumn_names = article_df.columns.tolist()\n\nfor i in tqdm(range(0, total_records, batch_size)):\n    i_end = min(i + batch_size, total_records)\n    client.insert(\"default.articles\", data[i:i_end], column_names=column_names)\n```\n\n----------------------------------------\n\nTITLE: Checking Embedding Vector Lengths in Python\nDESCRIPTION: Retrieves the first entry from the 'title_vector' and 'content_vector' columns of the pandas DataFrame `data`. It uses the `json.loads` function to parse the string representation of the vector arrays and then calculates the length (dimensionality) of each vector. Finally, it prints the computed lengths.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntitle_vector_length = len(json.loads(data['title_vector'].iloc[0]))\ncontent_vector_length = len(json.loads(data['content_vector'].iloc[0]))\n\nprint(title_vector_length, content_vector_length)\n```\n\n----------------------------------------\n\nTITLE: Querying Neo4j using Natural Language via Langchain QA Chain in Python\nDESCRIPTION: Demonstrates using the initialized `GraphCypherQAChain` (`chain`) to answer a natural language question. The `chain.run()` method takes the user query (\"Help me find curtains\"), passes it to the LLM to generate a corresponding Cypher query, executes the query against the Neo4j database via the `graph` connection, and returns the result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nchain.run(\"\"\"\nHelp me find curtains\n\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Connecting and Configuring Multiple Realtime Clients in JavaScript\nDESCRIPTION: Defines an asynchronous `connectAndSetupClients` function that iterates through the `updatedLanguageConfigs`. For each language's client, it establishes a connection to the OpenAI Realtime API using `client.realtime.connect` with a specified model (DEFAULT_REALTIME_MODEL) and updates the session settings, such as the voice (DEFAULT_REALTIME_VOICE), using `client.updateSession`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n   // Function to connect and set up all clients\n  const connectAndSetupClients = async () => {\n    for (const { clientRef } of updatedLanguageConfigs) {\n      const client = clientRef.current;\n      await client.realtime.connect({ model: DEFAULT_REALTIME_MODEL });\n      await client.updateSession({ voice: DEFAULT_REALTIME_VOICE });\n    }\n  };\n```\n\n----------------------------------------\n\nTITLE: Defining Regressive Summarizer Prompt and Function - Python\nDESCRIPTION: Defines a regressive variant of the summarizer prompt that intentionally outputs overly long summaries, and provides a new summarize_push_notification_bad function using that prompt. This is used to simulate and test prompt regression scenarios. Dependencies: openai, baseline function structure. Inputs: push_notifications (string). Outputs: openai chat completion result. Limitation: the output summary will not be concise and can be graded as bad.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nDEVELOPER_PROMPT = \"\"\"\nYou are a helpful assistant that summarizes push notifications.\nYou are given a list of push notifications and you need to collapse them into a single one.\nYou should make the summary longer than it needs to be and include more information than is necessary.\n\"\"\"\n\ndef summarize_push_notification_bad(push_notifications: str) -> ChatCompletion:\n    result = openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"developer\", \"content\": DEVELOPER_PROMPT},\n            {\"role\": \"user\", \"content\": push_notifications},\n        ],\n    )\n    return result\n```\n\n----------------------------------------\n\nTITLE: Handling Message Annotations (Python)\nDESCRIPTION: This code snippet retrieves a message object, extracts its content, and processes annotations to replace placeholders with relevant information. It handles both 'file_citation' and 'file_path' annotations, retrieves the associated files, and constructs citation strings to append to the message.  Note that file download functionality is not fully implemented.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Retrieve the message object\nmessage = client.beta.threads.messages.retrieve(\n  thread_id=\"...\",\n  message_id=\"...\"\n)\n# Extract the message content\nmessage_content = message.content[0].text\nannotations = message_content.annotations\ncitations = []\n# Iterate over the annotations and add footnotes\nfor index, annotation in enumerate(annotations):\n    # Replace the text with a footnote\n    message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n    # Gather citations based on annotation attributes\n    if (file_citation := getattr(annotation, 'file_citation', None)):\n        cited_file = client.files.retrieve(file_citation.file_id)\n        citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n    elif (file_path := getattr(annotation, 'file_path', None)):\n        cited_file = client.files.retrieve(file_path.file_id)\n        citations.append(f'[{index}] Click  to download {cited_file.filename}')\n        # Note: File download functionality not implemented above for brevity\n# Add footnotes to the end of the message before displaying to user\nmessage_content.value += '\\n' + '\\n'.join(citations)\n```\n\n----------------------------------------\n\nTITLE: Preprocess Embedding Data\nDESCRIPTION: Converts the string representations of 'title_vector' and 'content_vector' columns back into Python lists using `ast.literal_eval`. It also converts the 'vector_id' column to string type for compatibility with Qdrant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Connecting to PolarDB-PG Database with psycopg2\nDESCRIPTION: Establishes a connection to a PolarDB-PG database using psycopg2, with database credentials retrieved from environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport psycopg2\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"PGHOST\"] = \"your_host\"\n# os.environ[\"PGPORT\"] \"5432\"),\n# os.environ[\"PGDATABASE\"] \"postgres\"),\n# os.environ[\"PGUSER\"] \"user\"),\n# os.environ[\"PGPASSWORD\"] \"password\"),\n\nconnection = psycopg2.connect(\n    host=os.environ.get(\"PGHOST\", \"localhost\"),\n    port=os.environ.get(\"PGPORT\", \"5432\"),\n    database=os.environ.get(\"PGDATABASE\", \"postgres\"),\n    user=os.environ.get(\"PGUSER\", \"user\"),\n    password=os.environ.get(\"PGPASSWORD\", \"password\")\n)\n\n# Create a new cursor object\ncursor = connection.cursor()\n```\n\n----------------------------------------\n\nTITLE: Checking data count and vector index status in MyScale\nDESCRIPTION: Queries MyScale to verify the number of inserted articles and checks whether the vector index is built and ready for use, ensuring the index is prepared for similarity search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nprint(f\"articles count: {client.command('SELECT count(*) FROM default.articles')}\")\n\n# check the status of the vector index, make sure vector index is ready with 'Built' status\nget_index_status=\"SELECT status FROM system.vector_indices WHERE name='article_content_index'\"\nprint(f\"index build status: {client.command(get_index_status)}\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying Pivot Table\nDESCRIPTION: Creates a pandas DataFrame from the evaluation data, pivots the table to display the scores for each summary type across different evaluation types, and then styles the pivot table by highlighting the maximum value in each row. It relies on the pandas library and custom functions `highlight_max` and `display`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npivot_df = pd.DataFrame(data, index=None).pivot(\n    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n)\nstyled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\ndisplay(styled_pivot_df)\n```\n\n----------------------------------------\n\nTITLE: Install Qdrant Python Client\nDESCRIPTION: Installs the necessary Python client library for interacting with the Qdrant vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install qdrant-client\n```\n\n----------------------------------------\n\nTITLE: Initializing Qdrant Client and Defining Collection in Python\nDESCRIPTION: This Python snippet demonstrates how to initialize a connection to a Qdrant vector database using the `QdrantClient`. It retrieves the Qdrant URL and API key from environment variables and sets connection parameters like timeout and gRPC preference. It also defines the target collection name and includes commented-out code showing how to create or recreate the collection with specified vector parameters (size 384, Cosine distance). Requires `QDRANT_URL` and `QDRANT_API_KEY` environment variables to be set.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nqdrant_client = QdrantClient(\n    url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"), timeout=6000, prefer_grpc=True\n)\n\ncollection_name = \"squadv2-cookbook\"\n\n# # Create the collection, run this only once\n# qdrant_client.recreate_collection(\n#     collection_name=collection_name,\n#     vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n```\n\n----------------------------------------\n\nTITLE: Setting up Image Path and Encoding Test Image in Python\nDESCRIPTION: This code snippet sets the path to the images and selects a test image. It then encodes the test image to base64 using the `encode_image_to_base64` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Set the path to the images and select a test image\nimage_path = \"data/sample_clothes/sample_images/\"\ntest_images = [\"2133.jpg\", \"7143.jpg\", \"4226.jpg\"]\n\n# Encode the test image to base64\nreference_image = image_path + test_images[0]\nencoded_image = encode_image_to_base64(reference_image)\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompt for Drone Copilot\nDESCRIPTION: This code snippet defines the system prompt for the drone copilot. The prompt instructs the AI to use available functions to complete user requests or to reject requests that are not feasible or are unclear. This prompt acts as the foundation for the AI's decision-making process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nDRONE_SYSTEM_PROMPT = \"\"\"You are an intelligent AI that controls a drone. Given a command or request from the user,\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\nIf the request is ambiguous or unclear, reject the request.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Querying Content Embeddings with Neon in Python\nDESCRIPTION: This Python snippet queries a Neon database for articles semantically similar to the phrase 'Famous battles in Greek history' using the 'content_vector' embedding column. It leverages the query_neon function (assumed to be defined elsewhere), which accepts a search phrase, the target table (here 'Articles'), and the embedding column. The returned results are enumerated and printed with relevance scores computed as 1 minus a distance metric, rounded for readability. Dependencies may include a vector database connection and the presence of stored embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nquery_results = query_neon(\"Famous battles in Greek history\", \"Articles\", \"content_vector\")\nfor i, result in enumerate(query_results):\n    print(f\"{i + 1}. {result[2]} (Score: {round(1 - result[3], 3)})\")\n```\n\n----------------------------------------\n\nTITLE: Triggering Rate Limit Error\nDESCRIPTION: This code snippet demonstrates how to trigger a rate limit error by sending a large number of requests to the OpenAI API in a loop. It calls the `client.chat.completions.create` method 100 times, which exceeds the default rate limit and causes a `RateLimitError` to be raised.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# request a bunch of completions in a loop\nfor _ in range(100):\n    client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        max_tokens=10,\n    )\n```\n\n----------------------------------------\n\nTITLE: Connecting to Weaviate Instance with OpenAI API Key in Python\nDESCRIPTION: Creates and configures a Weaviate Python client, authenticating both with a Weaviate API key and an OpenAI API key loaded from environment variables. Additional headers are set so that Weaviate's generative module can use OpenAI services for search and response generation. The client.is_ready() call checks connectivity and readiness by returning a boolean. Dependencies include the weaviate module, datasets for possible downstream use, and os to manage environment variables. Users must supply actual URLs and API keys for their own deployments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport weaviate\nfrom datasets import load_dataset\nimport os\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network/\",\n    # url=\"http://localhost:8080/\",\n    auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"<YOUR-WEAVIATE-API-KEY>\"), # comment out this line if you are not using authentication for your Weaviate instance (i.e. for locally deployed instances)\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n\n# Check if your instance is live and ready\n# This should return `True`\nclient.is_ready()\n```\n\n----------------------------------------\n\nTITLE: Batch computing embeddings and asynchronous insertion into partitioned Cassandra table\nDESCRIPTION: This snippet performs batch processing of quotes by computing their embeddings, then inserts each quote into the partitioned Cassandra table asynchronously using `put_async`. It manages batch sizes for efficiency and ensures all insertions complete before proceeding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nBATCH_SIZE = 50\n\nnum_batches = ((len(philo_dataset) + BATCH_SIZE - 1) // BATCH_SIZE)\n\nquotes_list = philo_dataset[\"quote\"]\nauthors_list = philo_dataset[\"author\"]\ntags_list = philo_dataset[\"tags\"]\n\nprint(\"Starting to store entries:\")\nfor batch_i in range(num_batches):\n    b_start = batch_i * BATCH_SIZE\n    b_end = (batch_i + 1) * BATCH_SIZE\n    # compute the embedding vectors for this batch\n    b_emb_results = client.embeddings.create(\n        input=quotes_list[b_start : b_end],\n        model=embedding_model_name,\n    )\n    # prepare the rows for insertion\n    futures = []\n    print(\"B \", end=\"\")\n    for entry_idx, emb_result in zip(range(b_start, b_end), b_emb_results.data):\n        if tags_list[entry_idx]:\n            tags = {\n                tag\n                for tag in tags_list[entry_idx].split(\";\")\n            }\n        else:\n            tags = set()\n        author = authors_list[entry_idx]\n        quote = quotes_list[entry_idx]\n        futures.append(v_table_partitioned.put_async(\n            partition_id=author,\n            row_id=f\"q_{author}_{entry_idx}\",\n            body_blob=quote,\n            vector=emb_result.embedding,\n            metadata={tag: True for tag in tags},\n        ))\n    # wait for all insertions to complete\n    for future in futures:\n        future.result()\n    print(f\" done ({len(b_emb_results.data)})\")\n\nprint(\"\\nFinished storing entries.\")\n```\n\n----------------------------------------\n\nTITLE: Define function for semantic search using Typesense and OpenAI embeddings\nDESCRIPTION: Creates a function that generates a query embedding via OpenAI API and performs a vector similarity search in Typesense, returning top-k results for the specified collection field.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef query_typesense(query, field='title', top_k=20):\n\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n    embedded_query = openai.Embedding.create(\n        input=query,\n        model=EMBEDDING_MODEL,\n    )['data'][0]['embedding']\n\n    typesense_results = typesense_client.multi_search.perform({\n        \"searches\": [{\n            \"q\": \"*\",\n            \"collection\": \"wikipedia_articles\",\n            \"vector_query\": f\"{field}_vector:([{','.join(str(v) for v in embedded_query)}], k:{top_k})\"\n        }]\n    }, {})\n\n    return typesense_results\n```\n\n----------------------------------------\n\nTITLE: Creating and displaying DataFrame with routines and formatting for readability\nDESCRIPTION: This snippet converts the list of processed articles into a pandas DataFrame, adjusts display options to show full text, and defines a function to format HTML output for easy visualization of policies, content, and routines with line breaks preserved.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_routine_generation.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.DataFrame(results)\n\n# Set display options to show all text in the dataframe cells\npd.set_option('display.max_colwidth', None)\n\n# Function to display formatted text in HTML\ndef display_formatted_dataframe(df):\n    def format_text(text):\n        return text.replace('\\n', '<br>')\n\n    df_formatted = df.copy()\n    df_formatted['content'] = df_formatted['content'].apply(format_text)\n    df_formatted['routine'] = df_formatted['routine'].apply(format_text)\n    \n    display(HTML(df_formatted.to_html(escape=False, justify='left')))\n\ndisplay_formatted_dataframe(df)\n```\n\n----------------------------------------\n\nTITLE: Initializing Web Crawler Imports and Configuration in Python\nDESCRIPTION: Imports necessary Python libraries (requests, re, urllib, BeautifulSoup, deque, HTMLParser, urlparse, os) and sets up initial variables for the web crawler. Defines the target domain, the starting full URL, a regex pattern for HTTP URLs, and a custom HTMLParser class (`HyperlinkParser`) to extract href attributes from anchor tags.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom collections import deque\nfrom html.parser import HTMLParser\nfrom urllib.parse import urlparse\nimport os\n\n# Regex pattern to match a URL\nHTTP_URL_PATTERN = r'^http[s]*://.+'\n\ndomain = \"openai.com\" # <- put your domain to be crawled\nfull_url = \"https://openai.com/\" # <- put your domain to be crawled with https or http\n\n# Create a class to parse the HTML and get the hyperlinks\nclass HyperlinkParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        # Create a list to store the hyperlinks\n        self.hyperlinks = []\n\n    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n    def handle_starttag(self, tag, attrs):\n        attrs = dict(attrs)\n\n        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n        if tag == \"a\" and \"href\" in attrs:\n            self.hyperlinks.append(attrs[\"href\"])\n```\n\n----------------------------------------\n\nTITLE: Generating Hybrid Search Field String - Python\nDESCRIPTION: This simple helper function `create_hybrid_field` is designed to generate a string formatted for use in RediSearch hybrid queries. It takes a `field_name` and a `value` and returns a string in the format `@{field_name}:\"value\"`, which can be included in the base query string to filter results based on a specific field's content using RediSearch's text matching.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef create_hybrid_field(field_name: str, value: str) -> str:\n    return f'@{field_name}:\"{value}\"'\n```\n\n----------------------------------------\n\nTITLE: Embedding Request Function\nDESCRIPTION: This code defines a function `embedding_request` that sends a text to the OpenAI Embeddings API and returns the response. It uses the `tenacity` library to implement retry logic with exponential backoff and a maximum of 3 attempts.  It depends on the `openai` library and the `EMBEDDING_MODEL` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\ndef embedding_request(text):\n    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n    return response\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Preview in Python\nDESCRIPTION: Shows the first few rows of the DataFrame to inspect the data structure and values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nresults_df.head()\n```\n\n----------------------------------------\n\nTITLE: Segmenting Trimmed Audio File into Chunks (Python)\nDESCRIPTION: Loads the trimmed audio data from the file specified by `trimmed_filename` using `pydub.AudioSegment.from_wav`. It then defines a segment duration (`one_minute`) and an output directory (`output_dir_trimmed`). The code ensures the output directory exists, then loops through the trimmed audio, exporting segments of the specified duration into numbered `.wav` files within the output directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Segment audio\ntrimmed_audio = AudioSegment.from_wav(trimmed_filename)  # Load the trimmed audio file\n\none_minute = 1 * 60 * 1000  # Duration for each segment (in milliseconds)\n\nstart_time = 0  # Start time for the first segment\n\ni = 0  # Index for naming the segmented files\n\noutput_dir_trimmed = \"trimmed_earnings_directory\"  # Output directory for the segmented files\n\nif not os.path.isdir(output_dir_trimmed):  # Create the output directory if it does not exist\n    os.makedirs(output_dir_trimmed)\n\nwhile start_time < len(trimmed_audio):  # Loop over the trimmed audio file\n    segment = trimmed_audio[start_time:start_time + one_minute]  # Extract a segment\n    segment.export(os.path.join(output_dir_trimmed, f\"trimmed_{i:02d}.wav\"), format=\"wav\")  # Save the segment\n    start_time += one_minute  # Update the start time for the next segment\n    i += 1  # Increment the index for naming the next file\n```\n\n----------------------------------------\n\nTITLE: Reading CSV data into Pandas DataFrame and processing vectors\nDESCRIPTION: Loads CSV data containing article metadata and vectors, parsing vector strings back into lists. Prepares data for database insertion. Requires pandas and ast modules.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nfrom ast import literal_eval\n\n# read data from csv\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\narticle_df = article_df[['id', 'url', 'title', 'text', 'content_vector']]\n\n# read vectors from strings back into a list\narticle_df[\"content_vector\"] = article_df.content_vector.apply(literal_eval)\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Authenticating with API Key for Azure OpenAI (Python)\nDESCRIPTION: This code authenticates to the Azure OpenAI service using an API key. It retrieves the endpoint and API key from environment variables (`AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_KEY`). It then instantiates an `openai.AzureOpenAI` client with the endpoint, API key, and API version (2023-09-01-preview). This method assumes the user has an API key and endpoint configured.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif not use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        api_key=api_key,\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Evaluating Context Retrieval with OpenAI ADA Search API in Python\nDESCRIPTION: This function, check_context, uses OpenAI's ADA search endpoint to determine how well the model retrieves the expected Wikipedia context based on a given page title, heading, and question. It takes in context parameters including the search model, rerank limit, and token length constraint. The output is the rank at which the relevant context appears and the total tokens searched. Dependencies include the openai package (with appropriate authentication and a pre-existing upload file id) and assumes the search model API. Notably, openai.Engine(search_model) is deprecated and should be updated for current usage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef check_context(title, heading, question, max_len=1800, search_model='ada', max_rerank=10):\n    \"\"\"\n    Evaluate the performance of the search model in retrieving the correct context\n\n    Parameters\n    ----------\n    title: str\n        The title of the Wikipedia page\n    heading: str\n        The heading of the Wikipedia section\n    qusetion: str\n        The question\n    max_len: int\n        The maximum length of the context\n    search_model: str\n        The search model to use - `ada` is most cost effective\n    max_rerank: int\n        The maximum number of reranking documents to use the search model on\n\n    Returns\n    -------\n    rank: int\n        The rank of the correct context\n    token_length: int\n        The number of tokens needed to obtain the correct context\n    \"\"\"\n    \n    try:\n        # TODO: openai.Engine(search_model) is deprecated\n        results = openai.Engine(search_model).search(\n            search_model=search_model, \n            query=question, \n            max_rerank=max_rerank,\n            file=olympics_search_fileid,\n            return_metadata=True\n        )\n        index=-1\n        returns = []\n        cur_len = 0\n        for result in results['data']:\n            cur_len += int(result['metadata']) + 4 # we add 4 tokens for the separator `\\n\\n###\\n\\n`\n            if cur_len > max_len:\n                break\n            returns.append(result['text'])\n            res = result['text'].split('\\n')\n            if res[0] == title and res[1] == heading:\n                index = len(returns) - 1\n                break\n        return index, cur_len\n    except Exception as e:\n        #print (e)\n        return []\nprint(check_context(\"Athletics at the 2020 Summer Olympics â€“ Women's 4 Ã— 100 metres relay\", \"Summary\", \"Where did women's 4 x 100 metres relay event take place during the 2020 Summer Olympics?\", max_len=10000))\n```\n\n----------------------------------------\n\nTITLE: Defining Functions for Weather Assistant in Node.js\nDESCRIPTION: This snippet shows how to define functions within the OpenAI Assistants API in Node.js for the weather assistant. It uses the `client.beta.assistants.create()` method, specifying the model, instructions and defined `tools`, `getCurrentTemperature` and `getRainProbability`. This assumes the OpenAI Node.js library is installed and configured with an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-function-calling.txt#_snippet_1\n\nLANGUAGE: node.js\nCODE:\n```\nconst assistant = await client.beta.assistants.create({\n  model: \"gpt-4o\",\n  instructions:\n    \"You are a weather bot. Use the provided functions to answer questions.\",\n  tools: [\n    {\n      type: \"function\",\n      function: {\n        name: \"getCurrentTemperature\",\n        description: \"Get the current temperature for a specific location\",\n        parameters: {\n          type: \"object\",\n          properties: {\n            location: {\n              type: \"string\",\n              description: \"The city and state, e.g., San Francisco, CA\",\n            },\n            unit: {\n              type: \"string\",\n              enum: [\"Celsius\", \"Fahrenheit\"],\n              description:\n                \"The temperature unit to use. Infer this from the user's location.\",\n            },\n          },\n          required: [\"location\", \"unit\"],\n        },\n      },\n    },\n    {\n      type: \"function\",\n      function: {\n        name: \"getRainProbability\",\n        description: \"Get the probability of rain for a specific location\",\n        parameters: {\n          type: \"object\",\n          properties: {\n            location: {\n              type: \"string\",\n              description: \"The city and state, e.g., San Francisco, CA\",\n            },\n          },\n          required: [\"location\"],\n        },\n      },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Custom Prompt Template with Langchain in Python\nDESCRIPTION: Defines a subclass of StringPromptTemplate to create a flexible prompt template for agent interactions. This code formats intermediate agent steps, concatenates tool descriptions/names, and serializes entity types into JSON. Requires Langchain and a list of tool objects with 'name' and 'description' fields. Inputs are dynamic keyword arguments including 'intermediate_steps', and outputs a formatted prompt string for the LLM. Limitations include hard dependencies on expected keys and external variables like 'tools' and 'entity_types'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nclass CustomPromptTemplate(StringPromptTemplate):\n    # The template to use\n    template: str\n        \n    def format(self, **kwargs) -> str:\n        # Get the intermediate steps (AgentAction, Observation tuples)\n        # Format them in a particular way\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n        # Set the agent_scratchpad variable to that value\n        kwargs[\"agent_scratchpad\"] = thoughts\n        ############## NEW ######################\n        #tools = self.tools_getter(kwargs[\"input\"])\n        # Create a tools variable from the list of tools provided\n        kwargs[\"tools\"] = \"\\n\".join(\n            [f\"{tool.name}: {tool.description}\" for tool in tools]\n        )\n        # Create a list of tool names for the tools provided\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\n        kwargs[\"entity_types\"] = json.dumps(entity_types)\n        return self.template.format(**kwargs)\n\n\nprompt = CustomPromptTemplate(\n    template=prompt_template,\n    tools=tools,\n    input_variables=[\"input\", \"intermediate_steps\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Evaluate Student Solution After Solving (Correct)\nDESCRIPTION: This example shows how to improve the evaluation of a student's solution by first instructing the model to solve the problem independently and then compare its solution to the student's.  This allows the model to identify errors more effectively.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_13\n\nLANGUAGE: N/A\nCODE:\n```\nSYSTEM: First work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\n\nUSER: Problem Statement: I'm building a solar power installation and I need help working out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\nWhat is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\nASSISTANT: Let x be the size of the installation in square feet.\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 10x\nTotal cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.\n```\n\n----------------------------------------\n\nTITLE: Establishing connection to MyScale cluster with clickhouse_connect\nDESCRIPTION: Creates a client connection to the MyScale cluster using host, port, username, and password parameters. Facilitates executing SQL commands and data insertion into MyScale.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport clickhouse_connect\n\n# initialize client\nclient = clickhouse_connect.get_client(host='YOUR_CLUSTER_HOST', port=8443, username='YOUR_USERNAME', password='YOUR_CLUSTER_PASSWORD')\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI GPT Models and Embedding Search Environment in Python\nDESCRIPTION: This snippet sets up the Python environment for embedding-based search and question answering using OpenAI GPT models. It imports required libraries including OpenAI Python client for API calls, pandas for data handling, scipy for similarity calculations, and tiktoken for token counting. It defines the GPT models to use, selects an embedding model, and initializes the OpenAI client with an API key loaded from environment variables. This setup is essential before embedding documents or querying models. Prerequisites include installing the listed libraries and setting the OPENAI_API_KEY environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# imports\nimport ast  # for converting embeddings saved as strings back to arrays\nfrom openai import OpenAI # for calling the OpenAI API\nimport pandas as pd  # for storing text and embeddings data\nimport tiktoken  # for counting tokens\nimport os # for getting API token from env variable OPENAI_API_KEY\nfrom scipy import spatial  # for calculating vector similarities for search\n\n# create a list of models \nGPT_MODELS = [\"gpt-4o\", \"gpt-4o-mini\"]\n# models\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Correcting Transcription with GPT-4o Post-processing (Node.js)\nDESCRIPTION: Demonstrates correcting a Whisper transcription using an OpenAI chat completion model (GPT-4o) as a post-processing step in Node.js. A system prompt guides the chat model to fix spelling errors, particularly for specific terms. The snippet assumes an existing `transcribe` function provides the initial Whisper output, which is then passed to the chat model for correction.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_16\n\nLANGUAGE: node\nCODE:\n```\nconst systemPrompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\";\n\nasync function generateCorrectedTranscript(temperature, systemPrompt, audioFile) {\n  const transcript = await transcribe(audioFile);\n  const completion = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    temperature: temperature,\n    messages: [\n      {\n        role: \"system\",\n        content: systemPrompt\n      },\n      {\n        role: \"user\",\n        content: transcript\n      }\n    ]\n  });\n  return completion.choices[0].message.content;\n}\n\nconst fakeCompanyFilepath = \"path/to/audio/file\";\ngenerateCorrectedTranscript(0, systemPrompt, fakeCompanyFilepath)\n  .then(correctedText => console.log(correctedText))\n  .catch(error => console.error(error));\n```\n\n----------------------------------------\n\nTITLE: Establishing Neo4j Connection using Langchain in Python\nDESCRIPTION: Imports the `Neo4jGraph` class from `langchain.graphs` and initializes a connection object (`graph`) using the previously defined credentials (`url`, `username`, `password`). This `graph` object facilitates interaction with the Neo4j database through Langchain utilities. Requires the `langchain` library and valid Neo4j credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.graphs import Neo4jGraph\n\ngraph = Neo4jGraph(\n    url=url, \n    username=username, \n    password=password\n)\n```\n\n----------------------------------------\n\nTITLE: Collect Wikipedia Pages about 2022 Winter Olympics\nDESCRIPTION: This code snippet defines functions to collect Wikipedia articles from a specified category. It recursively traverses the category and its subcategories to retrieve page titles. The `titles_from_category` function retrieves titles, and the main part of the snippet initializes the Wikipedia site and category, then calls the function to retrieve the titles. It then prints the number of article titles found.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# get Wikipedia pages about the 2022 Winter Olympics\n\nCATEGORY_TITLE = \"Category:2022 Winter Olympics\"\nWIKI_SITE = \"en.wikipedia.org\"\n\n\ndef titles_from_category(\n    category: mwclient.listing.Category, max_depth: int\n) -> set[str]:\n    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n    titles = set()\n    for cm in category.members():\n        if type(cm) == mwclient.page.Page:\n            # ^type() used instead of isinstance() to catch match w/ no inheritance\n            titles.add(cm.name)\n        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n            titles.update(deeper_titles)\n    return titles\n\n\nsite = mwclient.Site(WIKI_SITE)\ncategory_page = site.pages[CATEGORY_TITLE]\ntitles = titles_from_category(category_page, max_depth=1)\n# ^note: max_depth=1 means we go one level deep in the category tree\nprint(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")\n```\n\n----------------------------------------\n\nTITLE: Querying the Title Embeddings Collection in ChromaDB using Python\nDESCRIPTION: Performs a semantic search on the `wikipedia_title_collection`. It calls the previously defined `query_collection` function with the query \"modern art in Europe\", requesting the top 10 results based on title vector similarity. The results, including IDs, scores, titles, and content, are stored in `title_query_result` and the first few rows are displayed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntitle_query_result = query_collection(\n    collection=wikipedia_title_collection,\n    query=\"modern art in Europe\",\n    max_results=10,\n    dataframe=article_df\n)\ntitle_query_result.head()\n```\n\n----------------------------------------\n\nTITLE: Pretty Printing JSON\nDESCRIPTION: This code defines a helper function, `show_json`, that takes a Python object and pretty-prints its JSON representation to the display. It uses the `json` module to load the JSON string and `display` to render it in a readable format. This is useful for visualizing the structure of API responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef show_json(obj):\n    display(json.loads(obj.model_dump_json()))\n```\n\n----------------------------------------\n\nTITLE: Exporting Meeting Minutes to DOCX\nDESCRIPTION: This function takes a dictionary of meeting minutes and saves them as a Word document using the python-docx library. It iterates through the key-value pairs in the 'minutes' dictionary, adding each key as a heading and the corresponding value as a paragraph. A line break is added between sections for better readability. The function requires the 'minutes' dictionary, which contains the abstract summary, key points, action items, and sentiment analysis. It also requires a 'filename' string specifying the output DOCX file name.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef save_as_docx(minutes, filename):\n    doc = Document()\n    for key, value in minutes.items():\n        # Replace underscores with spaces and capitalize each word for the heading\n        heading = ' '.join(word.capitalize() for word in key.split('_'))\n        doc.add_heading(heading, level=1)\n        doc.add_paragraph(value)\n        # Add a line break between sections\n        doc.add_paragraph()\n    doc.save(filename)\n```\n\n----------------------------------------\n\nTITLE: Configuring the OpenAI Model Version in Python\nDESCRIPTION: This snippet defines the specific OpenAI GPT-4o model version ('gpt-4o-2024-08-06') for use within the agent system. Assigning this value to the MODEL variable ensures all subsequent API calls use the right language model version. There are no external dependencies or active inputs/outputs; it simply sets a constant required for API requests throughout the multi-agent workflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nMODEL = \"gpt-4o-2024-08-06\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index and Loading Milvus Collection (Python)\nDESCRIPTION: Applies the index parameters defined in `INDEX_PARAM` to the \"embedding\" field of the Milvus collection using `collection.create_index`. After index creation, the collection is loaded into memory using `collection.load()`, making it available for search operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create the index on the collection and load it.\ncollection.create_index(field_name=\"embedding\", index_params=INDEX_PARAM)\ncollection.load()\n```\n\n----------------------------------------\n\nTITLE: Reading Text Data from File in Python\nDESCRIPTION: Opens and reads the content of a specified text file ('data/artificial_intelligence_wikipedia.txt') into a string variable. This text will be used as the input document for summarization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# open dataset containing part of the text of the Wikipedia page for the United States\nwith open(\"data/artificial_intelligence_wikipedia.txt\", \"r\") as file:\n    artificial_intelligence_wikipedia_text = file.read()\n```\n\n----------------------------------------\n\nTITLE: Polling and Retrieving Batch Job Results\nDESCRIPTION: Retrieves the status and output file ID of the batch job, then reads the result file content for processing results, which are unordered and matched using custom request IDs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbatch_job = client.batches.retrieve(batch_job.id)\nprint(batch_job)\n```\n\n----------------------------------------\n\nTITLE: Creating Thread with High Detail Image (Python)\nDESCRIPTION: This code snippet demonstrates how to create a thread with a message containing an image URL and specifying the 'high' detail level. This enables the model to see a low-res image first and then create detailed crops of the image. Requires the OpenAI client.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What is this an image of?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://example.com/image.png\",\n            \"detail\": \"high\"\n          }\n        },\n      ],\n    }\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Hybrid Query with Tag Field Filtering\nDESCRIPTION: A hybrid search query for \"watch\" that filters results to only include those with the tag \"Accessories\" in the masterCategory field, showing how to combine vector search with tag-based filtering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# hybrid query for watch in the product vector and only include results with the tag \"Accessories\" in the masterCategory field\nresults = search_redis(redis_client,\n                       \"watch\",\n                       vector_field=\"product_vector\",\n                       k=10,\n                       hybrid_fields='@masterCategory:{Accessories}'\n                       )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Portrait Style Image Prompt in Python\nDESCRIPTION: Creates a prompt for generating a pixel-art style portrait of a cat character with specific sizing parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt2 = \"generate a portrait, pixel-art style, of a grey tabby cat dressed as a blond woman on a dark background.\"\nimg_path2 = \"imgs/cat_portrait_pixel.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Loading a Deep Lake Text Dataset in Python\nDESCRIPTION: Loads a sample subset (20,000 samples) of the 'cohere-wikipedia-22' dataset from Deep Lake into a dataset object. The method provides access to the dataset's contents and metadata and allows for dataset summary inspection to understand its structure and size.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport deeplake\n\nds = deeplake.load(\"hub://activeloop/cohere-wikipedia-22-sample\")\nds.summary()\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAI Embeddings Batch Request\nDESCRIPTION: This snippet defines a function to generate OpenAI embeddings for a batch of documents using the specified embedding model. It batches requests to OpenAI to speed up embedding creation and returns the product vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Use OpenAI get_embeddings batch requests to speed up embedding creation\ndef embeddings_batch_request(documents: pd.DataFrame):\n    records = documents.to_dict(\"records\")\n    print(\"Records to process: \", len(records))\n    product_vectors = []\n    docs = []\n    batchsize = 1000\n\n    for idx,doc in enumerate(records,start=1):\n        # create byte vectors\n        docs.append(doc[\"product_text\"])\n        if idx % batchsize == 0:\n            product_vectors += get_embeddings(docs, EMBEDDING_MODEL)\n            docs.clear()\n            print(\"Vectors processed \", len(product_vectors), end='\\r')\n    product_vectors += get_embeddings(docs, EMBEDDING_MODEL)\n    print(\"Vectors processed \", len(product_vectors), end='\\r')\n    return product_vectors\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI with Context\nDESCRIPTION: Augments the original prompt with the context retrieved from Redis and sends it to the OpenAI model.  This allows the model to answer the question based on external knowledge. The response is printed to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprompt = f\"\"\"\nUsing the information delimited by triple backticks, answer this question: Is Sam Bankman-Fried's company, FTX, considered a well-managed company?\n\nContext: ```{context}```\n\"\"\"\n\nresponse = get_completion(prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI Schema for PostgreSQL API\nDESCRIPTION: This snippet defines an OpenAPI schema for a REST API endpoint that accepts a SQL query in a POST request. The schema specifies the request body format (application/json) with a 'q' parameter for the SQL query. It also describes the response structure for successful and unsuccessful operations, and includes security configurations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb#_snippet_0\n\nLANGUAGE: OpenAPI\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: PostgreSQL API\n  description: API for querying a PostgreSQL database\n  version: 1.0.0\nservers:\n  - url: https://my.middleware.com/v1\n    description: middleware service\npaths:\n  /api/query:\n    post:\n      operationId: databaseQuery\n      summary: Query a PostgreSQL database\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                q:\n                  type: string\n                  example: select * from users\n      responses:\n        \"200\":\n          description: database records\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                          description: The name of the file.\n                        mime_type:\n                          type: string\n                          description: The MIME type of the file.\n                        content:\n                          type: string\n                          format: byte\n                          description: The content of the file in base64 encoding.\n        \"400\":\n          description: Bad Request. Invalid input.\n        \"401\":\n          description: Unauthorized. Invalid or missing API key.\n      security:\n        - ApiKey: []\ncomponents:\n  securitySchemes:\n    ApiKey:\n      type: apiKey\n      in: header\n      name: X-Api-Key\n  schemas: {}\n```\n\n----------------------------------------\n\nTITLE: Generating Spoken Audio Using OpenAI TTS API in Node.js\nDESCRIPTION: This Node.js example uses the OpenAI JavaScript SDK to generate spoken audio from text with the TTS API. It creates an OpenAI client, submits the input text, model, and voice, receives a streaming audio response, then writes the output as an MP3 file using the filesystem module. Dependencies include the openai and path npm packages and Node.js. Required parameters are 'model', 'voice', and 'input'. The output is an MP3 file of the spoken input. The main constraint is use of voices optimized for English and API key authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-to-speech.txt#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\nconst speechFile = path.resolve(\"./speech.mp3\");\nasync function main() {\n  const mp3 = await openai.audio.speech.create({\n    model: \"tts-1\",\n    voice: \"alloy\",\n    input: \"Today is a wonderful day to build something people love!\",\n  });\n  console.log(speechFile);\n  const buffer = Buffer.from(await mp3.arrayBuffer());\n  await fs.promises.writeFile(speechFile, buffer);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Defining SharePoint File Search API with OpenAPI (YAML)\nDESCRIPTION: Defines the specification for a REST API enabling search over SharePoint documents, to be hosted in Azure Functions. The schema uses OpenAPI 3.0, describes a single POST endpoint for searching files with a 'searchTerm' in JSON, and specifies structured response/output for compatibility with OpenAI Action file API requirements. Responses include expected base64-encoded file data and detailed error handling for input validation and service limits. Requires configuration of your Azure Function endpoint and careful response structure for OpenAI integration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nopenapi: 3.0.0\ninfo:\n  title: SharePoint Search API\n  description: API for searching SharePoint documents.\n  version: 1.0.0\nservers:\n  - url: https://{your_function_app_name}.azurewebsites.net/api\n    description: SharePoint Search API server\npaths:\n  /{your_function_name}?code={enter your specific endpoint id here}:\n    post:\n      operationId: searchSharePoint\n      summary: Searches SharePoint for documents matching a query and term.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                searchTerm:\n                  type: string\n                  description: A specific term to search for within the documents.\n      responses:\n        '200':\n          description: A CSV file of query results encoded in base64.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponseData:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                          description: The name of the file.\n                        mime_type:\n                          type: string\n                          description: The MIME type of the file.\n                        content:\n                          type: string\n                          format: byte\n                          description: The base64 encoded contents of the file.\n        '400':\n          description: Bad request when the SQL query parameter is missing.\n        '413':\n          description: Payload too large if the response exceeds the size limit.\n        '500':\n          description: Server error when there are issues executing the query or encoding the results.\n```\n\n----------------------------------------\n\nTITLE: Counting Objects in Weaviate Class - Python\nDESCRIPTION: Performs an aggregate query on the 'Article' class to get the total count of objects. This serves as a verification step to confirm how many articles were successfully imported. Requires a connected Weaviate client instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# Test that all data has loaded â€“ get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining Guardrail System Message\nDESCRIPTION: This Python code defines a string variable `guardrail_system_message` containing instructions for a specialized assistant. This assistant reviews chatbot responses, identifies inaccuracies, and assesses knowledge accuracy, relevance, and policy compliance. The system message guides the model in evaluating and scoring each sentence in the assistant's response based on various criteria. It forms the basis of the hallucination guardrail.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nguardrail_system_message = \"\"\"You are a highly specialized assistant tasked with reviewing chatbot responses to identify and flag any inaccuracies or hallucinations. For each user message, you must thoroughly analyze the response by considering:\n    1. Knowledge Accuracy: Does the message accurately reflect information found in the knowledge base? Assess not only direct mentions but also contextually inferred knowledge.\n    2. Relevance: Does the message directly address the user's question or statement? Check if the response logically follows the userâ€™s last message, maintaining coherence in the conversation thread.\n    3. Policy Compliance: Does the message adhere to company policies? Evaluate for subtleties such as misinformation, overpromises, or logical inconsistencies. Ensure the response is polite, non-discriminatory, and practical.\n\nTo perform your task you will be given the following:\n    1. Knowledge Base Articles - These are your source of truth for verifying the content of assistant messages.\n    2. Chat Transcript - Provides context for the conversation between the user and the assistant.\n    3. Assistant Message - The message from the assistant that needs review.\n\nFor each sentence in the assistant's most recent response, assign a score based on the following criteria:\n    1. Factual Accuracy:\n        - Score 1 if the sentence is factually correct and corroborated by the knowledge base.\n        - Score 0 if the sentence contains factual errors or unsubstantiated claims.\n    2. Relevance:\n        - Score 1 if the sentence directly and specifically addresses the user's question or statement without digression.\n        - Score 0 if the sentence is tangential or does not build logically on the conversation thread.\n    3. Policy Compliance:\n        - Score 1 if the response complies with all company policies including accuracy, ethical guidelines, and user engagement standards.\n        - Score 0 if it violates any aspect of the policies, such as misinformation or inappropriate content.\n    4. Contextual Coherence:\n        - Score 1 if the sentence maintains or enhances the coherence of the conversation, connecting logically with preceding messages.\n        - Score 0 if it disrupts the flow or context of the conversation.\n\nInclude in your response an array of JSON objects for each evaluated sentence. Each JSON object should contain:\n    - `sentence`: Text of the evaluated sentence.\n    - `factualAccuracy`: Score for factual correctness (0 or 1).\n    - `factualReference`: If scored 1, cite the exact line(s) from the knowledge base. If scored 0, provide a rationale.\n    - `relevance`: Score for relevance to the userâ€™s question (0 or 1).\n    - `policyCompliance`: Score for adherence to company policies (0 or 1).\n    - `contextualCoherence`: Score for maintaining conversation coherence (0 or 1).\n\nALWAYS RETURN YOUR RESPONSE AS AN ARRAY OF JSONS.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Connecting to Neon Postgres Database\nDESCRIPTION: This snippet connects to a Neon Postgres database using a connection string. It first attempts to load the connection string from a `.env` file or an environment variable named `DATABASE_URL`. If neither is found, it raises an error, ensuring a valid connection string is available. It then establishes the database connection using the `psycopg2` library and creates a cursor object for executing SQL queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport psycopg2\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# The connection string can be provided directly here.\n# Replace the next line with Your Neon connection string.\nconnection_string = \"postgres://<user>:<password>@<hostname>/<dbname>\"\n\n# If connection_string is not directly provided above, \n# then check if DATABASE_URL is set in the environment or .env.\nif not connection_string:\n    connection_string = os.environ.get(\"DATABASE_URL\")\n\n    # If neither method provides a connection string, raise an error.\n    if not connection_string:\n        raise ValueError(\"Please provide a valid connection string either in the code or in the .env file as DATABASE_URL.\")\n\n# Connect using the connection string\nconnection = psycopg2.connect(connection_string)\n\n# Create a new cursor object\ncursor = connection.cursor()\n```\n\n----------------------------------------\n\nTITLE: Defining Callable Functions Schema (Python)\nDESCRIPTION: Defines a list of dictionaries, each describing a function that the OpenAI model can 'call'. It includes the function name, an optional description, and a JSON schema specifying the required parameters and their types. This example defines a 'get_current_weather' function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfunctions = [\n    {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"format\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                },\n            },\n            \"required\": [\"location\"],\n        },\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Load and Prepare Embeddings with Pandas & Numpy - Python\nDESCRIPTION: This snippet loads embeddings and metadata from a CSV file using pandas, converts the string representation of embeddings to numerical arrays using numpy and `ast.literal_eval`, and prepares the data for Atlas by dropping the original 'embedding' column and renaming 'Unnamed: 0' to 'id'. It requires pandas, numpy, and ast as dependencies. Input: fine_food_reviews_with_embeddings_1k.csv. Output: prepared pandas dataframe and numpy array of embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_with_Atlas.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom ast import literal_eval\n\n# Load the embeddings\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\ndf = pd.read_csv(datafile_path)\n\n# Convert to a list of lists of floats\nembeddings = np.array(df.embedding.apply(literal_eval).to_list())\ndf = df.drop('embedding', axis=1)\ndf = df.rename(columns={'Unnamed: 0': 'id'})\n```\n\n----------------------------------------\n\nTITLE: Creating Context from Similar Embeddings Using OpenAI and Cosine Distance in Python\nDESCRIPTION: Defines a function to create a text context relevant to a given question by computing cosine distances between the question's embedding and a dataframe's stored embeddings. The function uses OpenAI's embedding API endpoint to retrieve the question embedding, calculates distances to each stored embedding, sorts results by similarity, and concatenates text rows until a token length limit is reached. Required dependencies include OpenAI client and the distances_from_embeddings utility. Inputs include the question string, dataframe containing embeddings and text, maximum allowable token length for context, and embedding model size.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef create_context(\n    question, df, max_len=1800, size=\"ada\"\n):\n    \"\"\"\n    Create a context for a question by finding the most similar context from the dataframe\n    \"\"\"\n\n    # Get the embeddings for the question\n    q_embeddings = client.embeddings.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n\n    # Get the distances from the embeddings\n    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n\n\n    returns = []\n    cur_len = 0\n\n    # Sort by distance and add the text to the context until the context is too long\n    for i, row in df.sort_values('distances', ascending=True).iterrows():\n\n        # Add the length of the text to the current length\n        cur_len += row['n_tokens'] + 4\n\n        # If the context is too long, break\n        if cur_len > max_len:\n            break\n\n        # Else add it to the text that is being returned\n        returns.append(row[\"text\"])\n\n    # Return the context\n    return \"\\n\\n###\\n\\n\".join(returns)\n```\n\n----------------------------------------\n\nTITLE: Defining Input Parameters for Data Processing\nDESCRIPTION: This code defines input parameters like the path to save/load the embedding cache, the default embedding engine, the number of pairs to embed, and the local dataset path. It also includes the `process_input_data` function, which preprocesses the input dataframe to standardize the data format, which should include columns for text pairs ('text_1', 'text_2') and their labels.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# input parameters\nembedding_cache_path = \"data/snli_embedding_cache.pkl\"  # embeddings will be saved/loaded here\ndefault_embedding_engine = \"text-embedding-3-small\"\nnum_pairs_to_embed = 1000  # 1000 is arbitrary\nlocal_dataset_path = \"data/snli_1.0_train_2k.csv\"  # download from: https://nlp.stanford.edu/projects/snli/\n\n\ndef process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n    # you can customize this to preprocess your own dataset\n    # output should be a dataframe with 3 columns: text_1, text_2, label (1 for similar, -1 for dissimilar)\n    df[\"label\"] = df[\"gold_label\"]\n    df = df[df[\"label\"].isin([\"entailment\"])]\n    df[\"label\"] = df[\"label\"].apply(lambda x: {\"entailment\": 1, \"contradiction\": -1}[x])\n    df = df.rename(columns={\"sentence1\": \"text_1\", \"sentence2\": \"text_2\"})\n    df = df[[\"text_1\", \"text_2\", \"label\"]]\n    df = df.head(num_pairs_to_embed)\n    return df\n```\n\n----------------------------------------\n\nTITLE: Load Data into Pandas DataFrame\nDESCRIPTION: Reads the extracted CSV file containing Wikipedia article embeddings into a pandas DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n```\n\n----------------------------------------\n\nTITLE: Display DataFrame Head\nDESCRIPTION: Displays the first few rows of the loaded pandas DataFrame to inspect its structure and content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Create Index and Load Collection\nDESCRIPTION: Creates a vector index on the `embedding` field within the collection using the specified index parameters (`INDEX_PARAM`). This index is crucial for efficient similarity search. After creating the index, the collection is loaded into memory, making it ready for search operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Create the index on the collection and load it.\ncollection.create_index(field_name=\"embedding\", index_params=INDEX_PARAM)\ncollection.load()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading Environment Variables\nDESCRIPTION: Imports necessary libraries and loads environment variables from a .env file for secure configuration management.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nimport dotenv\n\ndotenv.load_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Finding Context Core Logic in Python\nDESCRIPTION: Searches for a specific sequence of lines (`context`) within a larger list (`lines`) starting from a given index (`start`). It attempts an exact match first, then matches ignoring trailing whitespace, and finally matches ignoring all leading/trailing whitespace. Returns the starting index of the match and a fuzziness score (0 for exact, 1 for rstrip, 100 for strip), or -1 if not found.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef find_context_core(\n    lines: List[str], context: List[str], start: int\n) -> Tuple[int, int]:\n    if not context:\n        return start, 0\n\n    for i in range(start, len(lines)):\n        if lines[i : i + len(context)] == context:\n            return i, 0\n    for i in range(start, len(lines)):\n        if [s.rstrip() for s in lines[i : i + len(context)]] == [\n            s.rstrip() for s in context\n        ]:\n            return i, 1\n    for i in range(start, len(lines)):\n        if [s.strip() for s in lines[i : i + len(context)]] == [\n            s.strip() for s in context\n        ]:\n            return i, 100\n    return -1, 0\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Azure AI Search and OpenAI Integration\nDESCRIPTION: Installation of necessary Python packages for working with Azure services, OpenAI, and data processing tools. Includes packages for Azure Search, Azure Identity, OpenAI API, data manipulation, and PDF processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -q wget\n! pip install -q azure-search-documents \n! pip install -q azure-identity\n! pip install -q openai\n! pip install -q azure-mgmt-search\n! pip install -q pandas\n! pip install -q azure-mgmt-resource \n! pip install -q azure-mgmt-storage\n! pip install -q pyperclip\n! pip install -q PyPDF2\n! pip install -q tiktoken\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs necessary Python libraries (`openai`, `psycopg2-binary`, `pandas`, `wget`) using the pip package manager. These libraries are required for interacting with the OpenAI API, connecting to the PostgreSQL-compatible Hologres database, handling data manipulation, and downloading files, respectively.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! pip install openai psycopg2-binary pandas wget\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries (Shell)\nDESCRIPTION: Installs necessary Python packages using pip for the project. This includes Google Cloud clients (auth, functions, bigquery), OpenAI client, data handling (pandas), environment variable management (python-dotenv), utilities (pyperclip, PyPDF2, tiktoken), and configuration handling (pyyaml). These libraries are prerequisites for the subsequent steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! pip install -q google-auth\n! pip install -q openai\n! pip install -q pandas\n! pip install -q google-cloud-functions\n! pip install -q python-dotenv\n! pip install -q pyperclip\n! pip install -q PyPDF2\n! pip install -q tiktoken\n! pip install -q google-cloud-bigquery\n! pip install -q pyyaml\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs necessary Python packages including OpenAI API client, tiktoken for tokenization, Langchain framework, and psycopg2cffi for PostgreSQL database connectivity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai tiktoken langchain psycopg2cffi \n```\n\n----------------------------------------\n\nTITLE: Semantic Search with Alternative Query Phrasing\nDESCRIPTION: Demonstrates semantic search resilience by using a different query phrasing that replaces 'depression' with 'recession' while aiming to find the same semantic content. Shows how meaning is captured beyond keywords.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What was the cause of the major recession in the early 20th century?\"\n\n# create the query embedding\nxq = client.embeddings.create(input=query, model=MODEL).data[0].embedding\n\n# query, returning the top 5 most similar results\nres = index.query(vector=[xq], top_k=5, include_metadata=True)\n\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Custom GPT Instructions for Writing Redshift SQL Queries in Python\nDESCRIPTION: This Python-flavored Markdown snippet provides instructions for a Custom GPT integration instructing it to first retrieve the Redshift public schema table and column metadata via a fixed SQL query before running user queries. It guides the language model to parse and confirm all queries against schema metadata, ensuring valid table and column references and preventing fabricated schema usage. This enhances query correctness and safety in conversational use.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n**Context**: You are an expert at writing Redshift SQL queries. You will initially retrieve the table schema that you will use thoroughly. Every attributes, table names or data type will be known by you.\n\n**Instructions**:\n1. No matter the user's question, start by running `runQuery` operation using this query: \"SELECT table_name, column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema = 'public' ORDER BY table_name, ordinal_position;\"  It will help you understand how to query the data. A CSV will be returned with all the attributes and their table. Make sure to read it fully and understand all available tables & their attributes before querying. You don't have to show this to the user.\n2. Convert the user's question into a SQL statement that leverages the step above and run the `runQuery` operation on that SQL statement to confirm the query works. Let the user know which table you will use/query.\n3. Execute the query and show him the data. Show only the first few rows.\n\n**Additional Notes**: If the user says \"Let's get started\", explain they can ask a question they want answered about data that we have access to. If the user has no ideas, suggest that we have transactions data they can query - ask if they want you to query that.\n**Important**: Never make up a table name or table attribute. If you don't know, go back to the data you've retrieved to check what is available. If you think no table or attribute is available, then tell the user you can't perform this query for them.\n```\n\n----------------------------------------\n\nTITLE: Submitting Discriminator Model Fine-tuning Job via OpenAI CLI\nDESCRIPTION: Executes a shell command using the OpenAI Command Line Interface (CLI) to initiate a fine-tuning job for the discriminator model. The command `openai api fine_tunes.create` specifies the training (`-t`) and validation (`-v`) JSONL files, sets the batch size, enables computation of classification metrics (`--compute_classification_metrics`), defines the positive class label (`--classification_positive_class \" yes\"`), and selects 'ada' as the base model (`--model ada`). Requires the OpenAI CLI to be installed and configured.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n!openai api fine_tunes.create -t \"olympics-data/discriminator_train.jsonl\" -v \"olympics-data/discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada\n```\n\n----------------------------------------\n\nTITLE: Enabling pgvector Extension in Postgres with SQL\nDESCRIPTION: Enables the pgvector extension in a Supabase Postgres database, which introduces the vector data type needed for storing embeddings. This operation should typically be managed via database migrations in production environments. There are no input parameters. It must be run in a SQL-capable context with sufficient privileges (e.g., Supabase SQL Editor or a migration script).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Enable the pgvector extension\ncreate extension if not exists vector;\n```\n\n----------------------------------------\n\nTITLE: Testing Keyword Deduplication on Sample Keywords\nDESCRIPTION: Demonstrates the keyword deduplication process by comparing example keywords to the existing keyword list and replacing similar ones based on embedding similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Example keywords to compare to our list of existing keywords\nexample_keywords = ['bed frame', 'wooden', 'vintage', 'old school', 'desk', 'table', 'old', 'metal', 'metallic', 'woody']\nfinal_keywords = []\n\nfor k in example_keywords:\n    final_keywords.append(replace_keyword(k))\n    \nfinal_keywords = set(final_keywords)\nprint(f\"Final keywords: {final_keywords}\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Semantic Search Results with Similarity Scores\nDESCRIPTION: Formats and displays the results of the semantic search query, showing the similarity score and text content for each matching document. Higher scores indicate greater semantic similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Initial Text Chunks (Python)\nDESCRIPTION: Splits the raw text into initial chunks based on double newlines. Calculates the token count for each chunk using the tiktoken tokenizer to understand the distribution of chunk sizes before further processing. Prints the maximum token count and the total number of chunks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchunks = text.split('\n\n')\nntokens = []\nfor chunk in chunks:\n    ntokens.append(len(tokenizer.encode(chunk)))\nprint(\"Size of the largest chunk: \", max(ntokens))\nprint(\"Number of chunks: \", len(chunks))\n```\n\n----------------------------------------\n\nTITLE: Create Assistant Message with NER Example Output\nDESCRIPTION: Provides a sample response demonstrating how an input text would be annotated with recognized entities and their categories to guide the model's output formatting and expected response structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef assisstant_message():\n    return f\"\"\"\nEXAMPLE:\n    Text: 'In Germany, in 1440, goldsmith Johannes Gutenberg invented the movable-type printing press. His work led to an information revolution and the unprecedented mass-spread / \n    of literature throughout Europe. Modelled on the design of the existing screw presses, a single Renaissance movable-type printing press could produce up to 3,600 pages per workday.'\n    {{\n        \"gpe\": [\"Germany\", \"Europe\"],\n        \"date\": [\"1440\"],\n        \"person\": [\"Johannes Gutenberg\"],\n        \"product\": [\"movable-type printing press\"],\n        \"event\": [\"Renaissance\"],\n        \"quantity\": [\"3,600 pages\"],\n        \"time\": [\"workday\"]\n    }}\n--\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Adding Language Instructions (JavaScript)\nDESCRIPTION: This JavaScript snippet defines a constant variable containing translation instructions for a specific language. This is typically used to provide context to the translation model to improve accuracy and relevance. Replace `Your Hindi instructions here...` with the actual instructions for the Hindi language.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nexport const hindi_instructions = \"Your Hindi instructions here...\";\n```\n\n----------------------------------------\n\nTITLE: Querying Image URL with OpenAI Vision (cURL)\nDESCRIPTION: Shows how to make a direct cURL request to the OpenAI chat completions API to query an image URL. The request includes the image URL and text prompt in the JSON payload, authenticated with an API key. Requires cURL and an OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"Whatâ€™s in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n```\n\n----------------------------------------\n\nTITLE: Specifying Azure OpenAI Deployment Name (Python)\nDESCRIPTION: Initializes a string variable intended to hold the name of the specific GPT model deployment configured in the Azure OpenAI Studio. This name is crucial for making API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndeployment = \"\" # Fill in the deployment name from the portal here\n```\n\n----------------------------------------\n\nTITLE: Verifying Object Count in Weaviate using Aggregation Query in Python\nDESCRIPTION: This snippet performs an aggregation query on the \"Article\" class in Weaviate to retrieve the total count of imported objects. It uses the `aggregate` method of the Weaviate client's query builder and specifies the `meta { count }` field. The result confirms whether the expected number of articles has been successfully imported.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Test that all data has loaded â€“ get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"], \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Performing a semantic search in SingleStoreDB\nDESCRIPTION: This code defines a function `strings_ranked_by_relatedness` that performs a semantic search in SingleStoreDB to find text excerpts related to a given query. It calculates the cosine similarity between the query embedding and the embeddings stored in the database using the DOT_PRODUCT_F64 function.  It returns a list of strings and their relatedness scores, sorted from most related to least.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import get_embedding\n\ndef strings_ranked_by_relatedness(\n    query: str,\n    df: pd.DataFrame,\n    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n    top_n: int = 100\n) -> tuple:\n    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n\n    # Get the embedding of the query.\n    query_embedding_response = get_embedding(query, EMBEDDING_MODEL)\n\n    # Create the SQL statement.\n    stmt = \"\"\"\n        SELECT\n            text,\n            DOT_PRODUCT_F64(JSON_ARRAY_PACK_F64(%s), embedding) AS score\n        FROM winter_wikipedia2.winter_olympics_2022\n        ORDER BY score DESC\n        LIMIT %s\n    \"\"\"\n\n    # Execute the SQL statement.\n    results = cur.execute(stmt, [str(query_embedding_response), top_n])\n\n    # Fetch the results\n    results = cur.fetchall()\n\n    strings = []\n    relatednesses = []\n\n    for row in results:\n        strings.append(row[0])\n        relatednesses.append(row[1])\n\n    # Return the results.\n    return strings[:top_n], relatednesses[:top_n]\n```\n\n----------------------------------------\n\nTITLE: Getting Single Course API Endpoint (YAML)\nDESCRIPTION: This snippet defines the `/courses/{course_id}` endpoint for retrieving details of a single course based on its ID.  It includes a path parameter `course_id`, query parameters for including additional information, and specifies the expected responses, including status codes, descriptions, and schemas for the returned course objects.  It requires a valid course ID for successful execution.  Dependencies include the Canvas LMS API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_canvas.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n  /courses/{course_id}:\n    get:\n      operationId: getSingleCourse\n      summary: Get a single course\n      description: Retrieves the details of a specific course by its ID.\n      parameters:\n        - name: course_id\n          in: path\n          required: true\n          description: The ID of the course.\n          schema:\n            type: integer\n        - name: include\n          in: query\n          description: Array of additional information to include (e.g., \"term\", \"teachers\").\n          schema:\n            type: array\n            items:\n              type: string\n      responses:\n        '200':\n          description: A single course object.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  id:\n                    type: integer\n                    description: The ID of the course.\n                  name:\n                    type: string\n                    description: The name of the course.\n                  account_id:\n                    type: integer\n                    description: The ID of the account associated with the course.\n                  enrollment_term_id:\n                    type: integer\n                    description: The ID of the term associated with the course.\n                  start_at:\n                    type: string\n                    format: date-time\n                    description: The start date of the course.\n                  end_at:\n                    type: string\n                    format: date-time\n                    description: The end date of the course.\n                  course_code:\n                    type: string\n                    description: The course code.\n                  state:\n                    type: string\n                    description: The current state of the course (e.g., \"unpublished\", \"available\").\n                  is_public:\n                    type: boolean\n                    description: Whether the course is public.\n                  syllabus_body:\n                    type: string\n                    description: The syllabus content of the course.\n                  term:\n                    type: object\n                    description: The term associated with the course.\n                    properties:\n                      id:\n                        type: integer\n                      name:\n                        type: string\n                      start_at:\n                        type: string\n                        format: date-time\n                      end_at:\n                        type: string\n                        format: date-time\n        '400':\n          description: Bad request, possibly due to an invalid course ID or query parameters.\n        '401':\n          description: Unauthorized, likely due to invalid authentication credentials.\n        '404':\n          description: Course not found, possibly due to an invalid course ID.\n```\n\n----------------------------------------\n\nTITLE: Defining `lookup_policy_document` Tool for OpenAI API (JSON)\nDESCRIPTION: Defines a function tool named `lookup_policy_document` in JSON format, intended for use with the OpenAI API. This tool searches internal documents or policies based on a provided 'topic' string. The 'strict' parameter ensures only defined properties are accepted.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"type\": \"function\",\n    \"name\": \"lookup_policy_document\",\n    \"description\": \"Tool to look up internal documents and policies by topic or keyword.\",\n    \"parameters\": {\n        \"strict\": true,\n        \"type\": \"object\",\n        \"properties\": {\n            \"topic\": {\n                \"type\": \"string\",\n                \"description\": \"The topic or keyword to search for in company policies or documents.\"\n            }\n        },\n        \"required\": [\"topic\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Overriding API Beta Version in Newer SDKs (Python/Node.js)\nDESCRIPTION: Shows how to explicitly request the v1 Assistants API beta when using newer versions of the OpenAI Python and Node.js SDKs (which default to v2). This involves passing a `default_headers` argument with `{\"OpenAI-Beta\": \"assistants=v1\"}` during client initialization. This approach is discouraged due to potential mismatches between the v1 API responses and the v2-oriented object types defined in newer SDKs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/migration.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(default_headers={\"OpenAI-Beta\": \"assistants=v1\"})\n```\n\nLANGUAGE: node.js\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({ defaultHeaders: {\"OpenAI-Beta\": \"assistants=v1\"} });\n```\n\n----------------------------------------\n\nTITLE: Initializing PowerPoint Presentation and Title Slide (python-pptx/Python)\nDESCRIPTION: Provides the initial lines of code using the `python-pptx` library to begin constructing a presentation. It imports necessary classes, creates a new empty `Presentation` object, and adds a blank slide using layout index 6, preparing it to be populated with content programmatically.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom pptx import Presentation\nfrom pptx.util import Inches, Pt\nfrom pptx.enum.text import PP_PARAGRAPH_ALIGNMENT\nfrom pptx.dml.color import RGBColor\n\n# Create a new presentation object\nprs = Presentation()\n\n# Add a blank slide layout\nblank_slide_layout = prs.slide_layouts[6]\nslide = prs.slides.add_slide(blank_slide_layout)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Response Structure for Agent Turns (Python)\nDESCRIPTION: This snippet defines a simple data class using Pydantic's BaseModel to structure the output of the main turn execution function. It includes fields for the potentially updated 'agent' (in case of a handoff) and the list of 'messages' exchanged during the turn.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nclass Response(BaseModel):\n    agent: Optional[Agent]\n    messages: list\n```\n\n----------------------------------------\n\nTITLE: Running Question Answering Chain on Sample Questions\nDESCRIPTION: Processes each selected question through the QA chain and prints both the question and generated answer. The chain retrieves relevant context from AnalyticDB and uses OpenAI to generate responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor question in selected_questions:\n    print(\">\", question)\n    print(qa.run(question), end=\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Generating Image with DALL-E 3 (OpenAI Images API/Python)\nDESCRIPTION: Uses the `client.images.generate` method to call the DALL-E 3 model. It constructs a prompt incorporating the `company_summary` to request an inspirational image depicting growth and future path, suitable for a financial meeting title slide. It specifies the desired image size, quality, and number of images to generate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.images.generate(\n  model='dall-e-3',\n  prompt=f\"given this company summary {company_summary}, create an inspirational \\\n    photo showing the growth and path forward. This will be used at a quarterly\\\n       financial planning meeting\",\n       size=\"1024x1024\",\n       quality=\"hd\",\n       n=1\n)\nimage_url = response.data[0].url\n\n```\n\n----------------------------------------\n\nTITLE: Calling the Code-Only Answer Function with Example Prompts in Python\nDESCRIPTION: Runs the code-based 'answer' retrieval function on a series of example prompt strings, showcasing direct search/recommendation without an LLM agent. Requires the previously defined 'answer' function and all backend dependencies. Each input is a product search or recommendation query from a user, and the output will be printed summaries and results provided by the 'answer' function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nprompt1 = \"I'm looking for food items to gift to someone for Christmas. Ideally chocolate.\"\nanswer(prompt1)\n\nprompt2 = \"Help me find women clothes for my wife. She likes blue.\"\nanswer(prompt2)\n\nprompt3 = \"I'm looking for nice things to decorate my living room.\"\nanswer(prompt3)\n\nprompt4 = \"Can you help me find a gift for my niece? She's 8 and she likes pink.\"\nanswer(prompt4)\n```\n\n----------------------------------------\n\nTITLE: Preparing Push Notifications Example Dataset - Python\nDESCRIPTION: Lists several sample push notification sets, each containing multiple notifications as a string. Used as the dataset for running eval test cases. Input: none. Output: List of notification strings. Dataset is used for both baseline and regression tests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npush_notification_data = [\n        \"\"\"\n- New message from Sarah: \"Can you call me later?\"\n- Your package has been delivered!\n- Flash sale: 20% off electronics for the next 2 hours!\n\"\"\",\n        \"\"\"\n- Weather alert: Thunderstorm expected in your area.\n- Reminder: Doctor's appointment at 3 PM.\n- John liked your photo on Instagram.\n\"\"\",\n        \"\"\"\n- Breaking News: Local elections results are in.\n- Your daily workout summary is ready.\n- Check out your weekly screen time report.\n\"\"\",\n        \"\"\"\n- Your ride is arriving in 2 minutes.\n- Grocery order has been shipped.\n- Don't miss the season finale of your favorite show tonight!\n\"\"\",\n        \"\"\"\n- Event reminder: Concert starts at 7 PM.\n- Your favorite team just scored!\n- Flashback: Memories from 3 years ago.\n\"\"\",\n        \"\"\"\n- Low battery alert: Charge your device.\n- Your friend Mike is nearby.\n- New episode of \"The Tech Hour\" podcast is live!\n\"\"\",\n        \"\"\"\n- System update available.\n- Monthly billing statement is ready.\n- Your next meeting starts in 15 minutes.\n\"\"\",\n        \"\"\"\n- Alert: Unauthorized login attempt detected.\n- New comment on your blog post: \"Great insights!\"\n- Tonight's dinner recipe: Pasta Primavera.\n\"\"\",\n        \"\"\"\n- Special offer: Free coffee with any breakfast order.\n- Your flight has been delayed by 30 minutes.\n- New movie release: \"Adventures Beyond\" now streaming.\n\"\"\",\n        \"\"\"\n- Traffic alert: Accident reported on Main Street.\n- Package out for delivery: Expected by 5 PM.\n- New friend suggestion: Connect with Emma.\n\"\n]\n```\n\n----------------------------------------\n\nTITLE: Inserting Content and Embedding into Supabase Documents Table in JavaScript\nDESCRIPTION: Performs an insert operation on the documents table using the Supabase client in JavaScript. Stores a text content string and its embedding vector. This assumes the client is already authenticated with sufficient permissions. Checks the returned error object for insert failures. In production, error handling should be implemented for robustness.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst { error } = await supabase.from(\"documents\").insert({\n  content: input,\n  embedding,\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving Step Logs Via API Using cURL\nDESCRIPTION: This snippet shows how to make a direct HTTP GET request to the OpenAI API endpoint for fetching step logs of a specific thread and run using cURL. It requires the 'Authorization' Bearer token and supports beta features through headers. The response includes detailed information about each step, including its ID, type, status, and logs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_8\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads/thread_abc123/runs/RUN_ID/steps \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n```\n\n----------------------------------------\n\nTITLE: Creating a Title Slide with Black Background and Image in Python-pptx\nDESCRIPTION: This code configures a PowerPoint title slide with a black background, an image on the left side, and formatted title and subtitle text boxes on the right. It sets specific positioning, dimensions, and text styling using the python-pptx library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n# Set the background color of the slide to black\nbackground = slide.background\nfill = background.fill\nfill.solid()\nfill.fore_color.rgb = RGBColor(0, 0, 0)\n\n# Add image to the left side of the slide with a margin at the top and bottom\nleft = Inches(0)\ntop = Inches(0)\nheight = prs.slide_height\nwidth = prs.slide_width * 3/5\npic = slide.shapes.add_picture(image_path, left, top, width=width, height=height)\n\n# Add title text box positioned higher\nleft = prs.slide_width * 3/5\ntop = Inches(2)\nwidth = prs.slide_width * 2/5\nheight = Inches(1)\ntitle_box = slide.shapes.add_textbox(left, top, width, height)\ntitle_frame = title_box.text_frame\ntitle_p = title_frame.add_paragraph()\ntitle_p.text = title_text\ntitle_p.font.bold = True\ntitle_p.font.size = Pt(38)\ntitle_p.font.color.rgb = RGBColor(255, 255, 255)\ntitle_p.alignment = PP_PARAGRAPH_ALIGNMENT.CENTER\n\n# Add subtitle text box\nleft = prs.slide_width * 3/5\ntop = Inches(3)\nwidth = prs.slide_width * 2/5\nheight = Inches(1)\nsubtitle_box = slide.shapes.add_textbox(left, top, width, height)\nsubtitle_frame = subtitle_box.text_frame\nsubtitle_p = subtitle_frame.add_paragraph()\nsubtitle_p.text = subtitle_text\nsubtitle_p.font.size = Pt(22)\nsubtitle_p.font.color.rgb = RGBColor(255, 255, 255)\nsubtitle_p.alignment = PP_PARAGRAPH_ALIGNMENT.CENTER\n```\n\n----------------------------------------\n\nTITLE: Upserting Embeddings to Pinecone Python\nDESCRIPTION: This code iterates through the chunks, creates embeddings using the OpenAI API, and upserts the embeddings along with their metadata to the Pinecone index. It includes error handling for potential rate limit issues. It defines the batch size for upserting embeddings and processes the chunks in batches to make the procedure more efficient.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\nimport datetime\nfrom time import sleep\n\nbatch_size = 100  # how many embeddings we create and insert at once\n\nfor i in tqdm(range(0, len(chunks), batch_size)):\n    # find end of batch\n    i_end = min(len(chunks), i+batch_size)\n    meta_batch = chunks[i:i_end]\n    # get ids\n    ids_batch = [x['id'] for x in meta_batch]\n    # get texts to encode\n    texts = [x['text'] for x in meta_batch]\n    # create embeddings (try-except added to avoid RateLimitError)\n    try:\n        res = openai.Embedding.create(input=texts, engine=embed_model)\n    except:\n        done = False\n        while not done:\n            sleep(5)\n            try:\n                res = openai.Embedding.create(input=texts, engine=embed_model)\n                done = True\n            except:\n                pass\n    embeds = [record['embedding'] for record in res['data']]\n    # cleanup metadata\n    meta_batch = [{\n        'text': x['text'],\n        'chunk': x['chunk'],\n        'url': x['url']\n    } for x in meta_batch]\n    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n    # upsert to Pinecone\n    index.upsert(vectors=to_upsert)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure OpenAI Dependencies (Python)\nDESCRIPTION: Installs the necessary Python libraries required for interacting with the Azure OpenAI service (`openai`) and managing environment variables (`python-dotenv`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"openai>=1.0.0,<2.0.0\"\n```\n\nLANGUAGE: python\nCODE:\n```\n! pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Processing and Evaluating Hallucination Detection Results in Python\nDESCRIPTION: Loads CSV data, transforms boolean values, and calculates precision and recall metrics to evaluate the hallucination detection system's performance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('hallucination_results.csv')\n\nif 'accurate' not in df.columns or 'hallucination' not in df.columns:\n    print(\"Error: The required columns are not present in the DataFrame.\")\nelse:\n    # Transform values to binary 0/1\n    try:\n        df['accurate'] = df['accurate'].astype(str).str.strip().map(lambda x: 1 if x in ['True', 'true'] else 0)\n        df['hallucination'] = df['hallucination'].str.strip().map(lambda x: 1 if x == 'Pass' else 0)\n        \n    except KeyError as e:\n        print(f\"Mapping error: {e}\")\n\n    # Check for any NaN values after mapping\n    if df['accurate'].isnull().any() or df['hallucination'].isnull().any():\n        print(\"Error: There are NaN values in the mapped columns. Check the input data for unexpected values.\")\n    else:\n        # Calculate precision and recall\n        try:\n            # Precision measures the proportion of correctly identified true positives out of all instances predicted as positive. \n            # Precision = (True Positives) / (True Positives + False Positives)\n            \n            precision = precision_score(df['accurate'], df['hallucination'])\n            \n            # Recall measures the proportion of correctly identified true positives out of all actual positive instances in the dataset.\n            # Recall = (True Positives) / (True Positives + False Negatives)\n            \n            recall = recall_score(df['accurate'], df['hallucination'])\n            \n            \n            print(f\"\\nPrecision: {precision:.2f} (Precision measures the proportion of correctly identified true positives out of all instances predicted as positive.), \"\n                  f\"\\nRecall: {recall:.2f} (Recall measures the proportion of correctly identified true positives out of all actual positive instances in the dataset.)\")\n\n        except ValueError as e:\n            print(f\"Error in calculating precision and recall: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Languages (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to add a new language entry to the `languages` object in the `ListenerPage` component. This object centralizes all language-related data and is used to populate the language dropdown menu and handle audio stream selection.  Ensure the language code is unique and that the name accurately reflects the language. `as const` asserts that the object is read-only and prevents accidental modification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst languages = {\n  fr: { name: 'French' },\n  es: { name: 'Spanish' },\n  tl: { name: 'Tagalog' },\n  en: { name: 'English' },\n  zh: { name: 'Mandarin' },\n  // Add your new language here\n  hi: { name: 'Hindi' }, // Example for adding Hindi\n} as const;\n```\n\n----------------------------------------\n\nTITLE: Executing Semantic Search Query (Python/Kusto)\nDESCRIPTION: Constructs a KQL query string that uses the `series_cosine_similarity_fl` function to calculate the similarity between the `searchedEmbedding` and the `content_vector` in the 'Wiki' table. It orders results by similarity and takes the top 10, then executes the query using the Kusto client.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nKUSTO_QUERY = \"Wiki | extend similarity = series_cosine_similarity_fl(dynamic(\"+str(searchedEmbedding)+\"), content_vector,1,1) | top 10 by similarity desc \"\n\nRESPONSE = KUSTO_CLIENT.execute(KUSTO_DATABASE, KUSTO_QUERY)\n```\n\n----------------------------------------\n\nTITLE: Loading Question and Answer JSON Data\nDESCRIPTION: Loads the downloaded questions and answers from JSON files into Python variables for processing and indexing into the vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open(\"questions.json\", \"r\") as fp:\n    questions = json.load(fp)\n\nwith open(\"answers.json\", \"r\") as fp:\n    answers = json.load(fp)\n```\n\n----------------------------------------\n\nTITLE: Extracting Complex Information from Text Chunks\nDESCRIPTION: This code iterates through text chunks and extracts information using the `extract_chunk` function with the complex extraction prompt, and then processes the extracted results. Results are split based on newlines, zipped, and filtered to remove 'Not specified' entries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults = []\n\nfor chunk in text_chunks:\n    results.append(extract_chunk(chunk,template_prompt))\n    \ngroups = [r.split('\\n') for r in results]\n\n# zip the groups together\nzipped = list(zip(*groups))\nzipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\nzipped\n```\n\n----------------------------------------\n\nTITLE: Defining Google Ads API Request Schemas and Security in JSON\nDESCRIPTION: This JSON snippet defines components of an OpenAPI specification, detailing the structure for API requests related to Google Ads audits (`googleAdsAuditRequest`) and search queries (`searchQueryRequest`). It specifies required fields (like `workspace_name`, `assorted_requests`, `date_ranges`), data types, descriptions, and constraints for parameters such as metrics, breakdowns, and time granularity. It also includes a definition for OAuth2 security schemes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_googleads_adzviser.ipynb#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"type\": \"object\",\n        \"required\": [\n          \"workspace_name\"\n        ],\n        \"title\": \"googleAdsAuditRequest\",\n        \"properties\": {\n          \"workspace_name\": {\n            \"type\": \"string\",\n            \"title\": \"workspace_name\",\n            \"description\": \"Call API getWorkspace first to get a list of available workspaces\"\n          }\n        }\n      },\n      \"searchQueryRequest\": {\n        \"description\": \"Contains details about queried data source, metrics, breakdowns, time ranges and time granularity, etc.\",\n        \"type\": \"object\",\n        \"required\": [\n          \"assorted_requests\",\n          \"workspace_name\",\n          \"date_ranges\"\n        ],\n        \"title\": \"searchQueryRequest\",\n        \"properties\": {\n          \"assorted_requests\": {\n            \"type\": \"object\",\n            \"title\": \"assorted_requests\",\n            \"description\": \"For example, if the user asks for \\\"cost on Google ads last month\\\", then call getGoogleAdsMetricsList and getGoogleAdsBreakdownsList to retrieve the latest up-to-date info about how to compose a google_ads_request. A metric is a quantitative measurement. It represents data that can be measured and expressed in numbers. Metrics are used to track performance or behavior. Examples include clicks, impressions, conversions, revenue, etc. A breakdown is a qualitative attribute or descriptor. It provides context for metrics by categorizing or segmenting them. Breakdowns are text. Examples include country, channel, campaign name, etc. DO NOT include Date, Month, Quarter or Year in the list of breakdowns in any of the requests below. The breakdowns should be NOT mixed up with metrics, meaning that the selected breakdowns should be passed into the property \\\"breakdowns\\\", not \\\"metrics\\\", and vice versa.\",\n            \"properties\": {\n              \"google_ads_request\": {\n                \"type\": \"object\",\n                \"description\": \"DO NOT come up with metrics and breakdowns on your own. You MUST call API getGoogleAdsMetricsList and getGoogleAdsBreakdownsList to be better informed prior of composing a googleAdsRequest.\",\n                \"required\": [\n                  \"metrics\",\n                  \"breakdowns\"\n                ],\n                \"properties\": {\n                  \"breakdowns\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    },\n                    \"description\": \"Must call API getGoogleAdsBreakdownsList to retrieve a list of selectable breakdowns.\"\n                  },\n                  \"metrics\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    },\n                    \"description\": \"Must call API getGoogleAdsMetricsList to retrieve a list of selectable metrics.\"\n                  }\n                }\n              }\n            }\n          },\n          \"workspace_name\": {\n            \"type\": \"string\",\n            \"title\": \"workspace_name\",\n            \"description\": \"Call API getWorkspace first to get a list of available workspaces. Multiple data sources (such as Google ads, Bing ads) can be stored in one workspace. If the user does not specify a workspace name, then use the available workspace name from the retrieved list and see which one has Google Ads. If the user has not yet created one, then ask them to go to adzviser.com/main to create a new workspace.\"\n          },\n          \"date_ranges\": {\n            \"type\": \"array\",\n            \"description\": \"A list of date ranges requested from the user. They needs to be calculated seperately with Code Interpreter and Python every single time for accuracy. For example, if the user requests \\\"Google Ads search impression share in May and August\\\", then this array should be [[\\\"2024-05-01\\\", \\\"2024-05-31\\\"], [\\\"2024-08-01\\\", \\\"2024-08-31\\\"]].\",\n            \"items\": {\n              \"type\": \"array\",\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"description\": \"A 2-element array. The first represents the start date and the second the end date. Both are in YYYY-MM-DD format.\"\n            }\n          },\n          \"time_granularity\": {\n            \"type\": \"string\",\n            \"title\": \"time_granularity\",\n            \"default\": \"\",\n            \"description\": \"Describes how granularity you wish the date_ranges to be. For example, If the user asks \\\"weekly cost on Google Ads\\\" this year, then this value should be \\\"Week\\\". If the user does not specify, then leave it as empty.\",\n            \"enum\": [\n              \"Date\",\n              \"Week\",\n              \"Month\",\n              \"Quarter\"\n            ]\n          }\n        }\n      }\n    },\n    \"securitySchemes\": {\n      \"oauth2\": {\n        \"type\": \"oauth2\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating total tokens and training epochs for fine-tuning costs\nDESCRIPTION: Calculates an estimate of total tokens used during training based on dataset size and maximum tokens per example, then determines appropriate number of epochs within defined bounds. Reports total billing tokens and approximate training cost, aiding budget planning for fine-tuning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nMAX_TOKENS_PER_EXAMPLE = 16385\n\nTARGET_EPOCHS = 3\nMIN_TARGET_EXAMPLES = 100\nMAX_TARGET_EXAMPLES = 25000\nMIN_DEFAULT_EPOCHS = 1\nMAX_DEFAULT_EPOCHS = 25\n\nn_epochs = TARGET_EPOCHS\nn_train_examples = len(dataset)\nif n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\nelif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n\nn_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\nprint(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\nprint(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\nprint(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n```\n\n----------------------------------------\n\nTITLE: Running Example Query in Content Namespace\nDESCRIPTION: This snippet runs an example query for 'Famous battles in Scottish history' in the 'content' namespace using the `query_article` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncontent_query_output = query_article(\"Famous battles in Scottish history\",'content')\n```\n\n----------------------------------------\n\nTITLE: Edit an image with DALLÂ·E 2\nDESCRIPTION: Code for editing specific areas of an existing image by providing both the original image and a mask indicating editable regions. Requires DALLÂ·E 2 model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.edit((\n  model=\"dall-e-2\",\n  image=open(\"sunlit_lounge.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response.data[0].url\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst response = await openai.images.edit({\n  model: \"dall-e-2\",\n  image: fs.createReadStream(\"sunlit_lounge.png\"),\n  mask: fs.createReadStream(\"mask.png\"),\n  prompt: \"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n: 1,\n  size: \"1024x1024\"\n});\nimage_url = response.data[0].url;\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/images/edits \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -F model=\"dall-e-2\" \\\n  -F image=\"@sunlit_lounge.png\" \\\n  -F mask=\"@mask.png\" \\\n  -F prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\" \\\n  -F n=1 \\\n  -F size=\"1024x1024\"\n```\n\n----------------------------------------\n\nTITLE: Searching for \"pet food\" - Python\nDESCRIPTION: This code snippet invokes the `search_reviews` function to search for reviews related to \"pet food\" and retrieves the top 2 most similar reviews.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults = search_reviews(df, \"pet food\", n=2)\n```\n\n----------------------------------------\n\nTITLE: Improving Multilingual Summarization with Task Decomposition (Correct Language)\nDESCRIPTION: This snippet improves the multilingual summarization task by breaking it down. The prompt first asks `gpt-3.5-turbo-instruct` to identify the language of the text (Spanish) and then instructs it to summarize the text in that identified language. This two-step approach helps the model correctly generate the summary in Spanish.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_5\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nFirst, identify the language of the text. Second, summarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estadÃ­stica (la forma femenina del tÃ©rmino alemÃ¡n Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")â€‹ es una ciencia que estudia la variabilidad, colecciÃ³n, organizaciÃ³n, anÃ¡lisis, interpretaciÃ³n, y presentaciÃ³n de los datos, asÃ­ como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.â€‹ La estadÃ­stica es una ciencia formal deductiva, con un conocimiento propio, dinÃ¡mico y en continuo desarrollo obtenido a travÃ©s del mÃ©todo cientÃ­fico formal. En ocasiones, las ciencias fÃ¡cticas necesitan utilizar tÃ©cnicasÃ­sticas durante su proceso de investigaciÃ³n factual, con el fin de obtener nuevos conocimientos basados en la experimentaciÃ³n y en la observaciÃ³n. En estos casos, la aplicaciÃ³n de la estadÃ­stica permite el anÃ¡lisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenÃ³meno fÃ­sico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nLanguage:\n```\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nSpanish\n\nLa estadÃ­stica es una ciencia que estudia la variabilidad, colecciÃ³n, organizaciÃ³n, anÃ¡lisis, interpretaciÃ³n, y presentaciÃ³n de los datos, asÃ­ como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion using Node.js/TypeScript Library\nDESCRIPTION: Initialize the OpenAI client in Node.js/TypeScript and perform a basic chat completion request. The API key is typically loaded from the process environment variables. This example sends a simple message to the gpt-3.5-turbo model asynchronously.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n\nconst openai = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: \"user\", content: \"Say this is a test\" }],\n    model: \"gpt-3.5-turbo\",\n});\n```\n\n----------------------------------------\n\nTITLE: Submitting Message to Request Insights (OpenAI Assistants API/Python)\nDESCRIPTION: Uses the `submit_message` helper function to send a new user message to the thread. This message prompts the Assistant to generate two medium-length sentences summarizing the most important insights derived from the plot it just created, framed for use in a slide deck.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsubmit_message(assistant.id,thread,\"Give me two medium length sentences (~20-30 words per sentence) of the \\\n      most important insights from the plot you just created.\\\n             These will be used for a slide deck, and they should be about the\\\n                     'so what' behind the data.\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Training Random Forest Classifier on Transaction Embeddings in Python\nDESCRIPTION: Loads the CSV file containing transactions with precomputed embeddings. Converts string representations of embeddings back to numpy arrays. Splits data into training and test sets with an 80-20 split. Trains a RandomForestClassifier on the embedding vectors with their corresponding classifications as targets. Outputs a detailed classification report showing model performance. Dependencies include scikit-learn and numpy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom ast import literal_eval\n\nfs_df = pd.read_csv(embedding_path)\nfs_df[\"babbage_similarity\"] = fs_df.babbage_similarity.apply(literal_eval).apply(np.array)\nfs_df.head()\n\n```\n\nLANGUAGE: python\nCODE:\n```\nX_train, X_test, y_train, y_test = train_test_split(\n    list(fs_df.babbage_similarity.values), fs_df.Classification, test_size=0.2, random_state=42\n)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\n\nreport = classification_report(y_test, preds)\nprint(report)\n\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: This shell command sets the `OPENAI_API_KEY` environment variable. This is required by the OpenAI Python client to authenticate API calls for generating embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n! export OPENAI_API_KEY=\"your API key\"\n```\n\n----------------------------------------\n\nTITLE: Original Reasoning JSON Structure\nDESCRIPTION: The initial JSON structure used for reasoning in the customer service bot, with verbose field names that generate more tokens and increase latency.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n\"message_is_conversation_continuation\": \"True\",\n\"number_of_messages_in_conversation_so_far\": \"1\",\n\"user_sentiment\": \"Aggravated\",\n\"query_type\": \"Hardware Issue\",\n\"response_tone\": \"Validating and solution-oriented\",\n\"response_requirements\": \"Propose options for repair or replacement.\",\n\"user_requesting_to_talk_to_human\": \"False\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Fluency Evaluation Criteria\nDESCRIPTION: Defines the criteria for evaluating the fluency of a summary, including grammar, spelling, punctuation, word choice, and sentence structure. The criteria are rated on a scale of 1 to 3, with detailed descriptions for each score.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nFLUENCY_SCORE_CRITERIA = \"\"\"\nFluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n3: Good. The summary has few or no errors and is easy to read and follow.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Customer Simulation Prompt for Evaluation in Python\nDESCRIPTION: Defines `customer_system_prompt`, a system message designed to guide a separate GPT model acting as a customer for evaluation purposes. The prompt instructs the simulated customer on their query, their role in the conversation, and how to interact based on the provided chat history, including responding with dummy values if necessary and signaling completion with 'DONE'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncustomer_system_prompt = \"\"\"You are a user calling in to customer service.\nYou will talk to the agent until you have a resolution to your query.\nYour query is {query}.\nYou will be presented with a conversation - provide answers for any assistant questions you receive. \nHere is the conversation - you are the \"user\" and you are speaking with the \"assistant\":\n{chat_history}\n\nIf you don't know the details, respond with dummy values.\nOnce your query is resolved, respond with \"DONE\" \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Labelled Transaction Data with Combined Fields in Python\nDESCRIPTION: Reads a labeled transactions CSV file containing manual classifications. Creates a new 'combined' field by concatenating supplier, description, and value information for embedding generation. Uses GPT2 tokenizer to calculate token counts for each combined string. This preprocessing sets up the data for embedding computation and classification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('./data/labelled_transactions.csv')\ndf.head()\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndf['combined'] = \"Supplier: \" + df['Supplier'].str.strip() + \"; Description: \" + df['Description'].str.strip() + \"; Value: \" + str(df['Transaction value (Â£)']).strip()\ndf.head(2)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import GPT2TokenizerFast\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndf['n_tokens'] = df.combined.apply(lambda x: len(tokenizer.encode(x)))\nlen(df)\n\n```\n\n----------------------------------------\n\nTITLE: Initialize Qdrant Client\nDESCRIPTION: Creates an instance of the Qdrant client, connecting to the locally running Qdrant service at 'localhost' on port 6333.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nqdrant = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\n```\n\n----------------------------------------\n\nTITLE: Downloading pre-chunked text and embeddings - Python\nDESCRIPTION: This snippet downloads a pre-chunked dataset of Wikipedia articles about the 2022 Winter Olympics and their pre-computed embeddings from a CSV file.  It requires the `pandas` library. The downloaded file is approximately 200MB and may take some time to download depending on the internet connection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# download pre-chunked text and pre-computed embeddings\n# this file is ~200 MB, so may take a minute depending on your connection speed\nembeddings_path = \"data/winter_olympics_2022.csv\"\n\ndf = pd.read_csv(embeddings_path)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Run Results Python\nDESCRIPTION: This snippet demonstrates how to create a run, then retrieves the messages generated by the run. The code then processes the messages, extracts the content, identifies annotations (like citations), and formats the output with proper citations. This involves calling the OpenAI API, extracting text from messages, replacing annotation markers with index-based references, retrieving file citations and finally printing the results with the correct formatting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id, assistant_id=assistant.id\n)\n\nmessages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\nmessage_content = messages[0].content[0].text\nannotations = message_content.annotations\ncitations = []\nfor index, annotation in enumerate(annotations):\n    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n    if file_citation := getattr(annotation, \"file_citation\", None):\n        cited_file = client.files.retrieve(file_citation.file_id)\n        citations.append(f\"[{index}] {cited_file.filename}\")\nprint(message_content.value)\nprint(\"\\n\".join(citations))\n```\n\n----------------------------------------\n\nTITLE: Creating a list of JSON task objects for batch processing in Python\nDESCRIPTION: This snippet constructs an array of task dictionaries for batch processing. It iterates over a DataFrame, creates individual task objects with API request parameters, messages, and associated metadata, then accumulates them into a list. Dependencies include pandas DataFrame, json, and variables like caption_system_prompt. The purpose is to prepare structured tasks for subsequent upload.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ntasks = []\n\nfor index, row in df.iterrows():\n    \n    title = row['title']\n    img_url = row['primary_image']\n    \n    task = {\n        \"custom_id\": f\"task-{index}\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 300,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": caption_system_prompt\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": title\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": img_url\n                            }\n                        }\n                    ]\n                }\n            ]            \n        }\n    }\n    \n    tasks.append(task)\n```\n\n----------------------------------------\n\nTITLE: Correcting Icelandic Sentences Using GPT-4 Turbo - Example Chat Format\nDESCRIPTION: This snippet demonstrates a practical use of GPT-4 Turbo to correct Icelandic sentences containing errors. It uses conversational-style prompts with a system message instructing the model to minimize word changes and produce corrected text. The snippet is part of a broader prompt engineering strategy to optimize LLM accuracy by providing context and examples. Dependencies include access to the GPT-4 Turbo model and data from the Icelandic Errors Corpus. Expected input is an erroneous Icelandic sentence, and output is a grammatically corrected sentence with BLEU score used for quality evaluation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/optimizing-llm-accuracy.txt#_snippet_0\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible.\n```\n\n----------------------------------------\n\nTITLE: Streaming Assistant Run with File Search in Node.js\nDESCRIPTION: Shows how to set up a streaming run of an assistant with file_search tool enabled, using event listeners for textCreated, toolCallCreated, and messageDone. On messageDone, annotated parts of the response are replaced with references, and cited filenames retrieved asynchronously. Requires OpenAI Node.js SDK with beta threads runs streaming API and fs for files retrieval.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_10\n\nLANGUAGE: node.js\nCODE:\n```\nconst stream = openai.beta.threads.runs\n  .stream(thread.id, {\n    assistant_id: assistant.id,\n  })\n  .on(\"textCreated\", () => console.log(\"assistant >\"))\n  .on(\"toolCallCreated\", (event) => console.log(\"assistant \" + event.type))\n  .on(\"messageDone\", async (event) => {\n    if (event.content[0].type === \"text\") {\n      const { text } = event.content[0];\n      const { annotations } = text;\n      const citations = [];\n\n      let index = 0;\n      for (let annotation of annotations) {\n        text.value = text.value.replace(annotation.text, \"[\" + index + \"]\");\n        const { file_citation } = annotation;\n        if (file_citation) {\n          const citedFile = await openai.files.retrieve(file_citation.file_id);\n          citations.push(\"[\" + index + \"]\" + citedFile.filename);\n        }\n        index++;\n      }\n\n      console.log(text.value);\n      console.log(citations.join(\"\\n\"));\n    }\n  });\n```\n\n----------------------------------------\n\nTITLE: Selecting Transcription Output Format in OpenAI Whisper API with Node.js\nDESCRIPTION: This Node.js snippet demonstrates passing the 'response_format' parameter as 'text' to the transcriptions API request. It uses the OpenAI SDK and the required dependencies are 'openai' and 'fs'. The file is streamed, and the output is plain text from the transcription result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_4\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream(\"/path/to/file/speech.mp3\"),\n    model: \"whisper-1\",\n    response_format: \"text\",\n  });\n  console.log(transcription.text);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Running Batched Prompt Engineering Experiments with explain_math - Python\nDESCRIPTION: This snippet iterates through combinations of system prompts, equations, and audience types, calling the explain_math function with each parameter set to log and monitor multiple experiments in OpenAI ChatCompletions. Dependencies include a previously defined explain_math function, and valid openai/weave monitoring integrations. All key input variables are initialized within the snippet; outputs are systematically logged via weave. Intended for broad coverage of prompt/audience variations in analysis dashboards.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# feel free to substitute your own prompts :)\nsystem_prompts = [\"you're extremely flowery and poetic\", \"you're very direct and precise\", \"balance brevity with insight\"]\nprompt_template = 'explain the solution of the following to a {audience}: {equation}'\nequations = ['x^2 + 4x + 9 = 0', '15 * (2 - 6) / 4']\naudience = [\"new student\", \"math genius\"]\n\nfor system_prompt in system_prompts:\n    for equation in equations:\n        for person in audience:\n            params = {\"equation\" : equation, \"audience\" : person}\n            explain_math(system_prompt, prompt_template, params)\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with OpenAI Whisper API using cURL\nDESCRIPTION: This cURL example submits an audio file to OpenAI's transcription API endpoint with the 'whisper-1' model via a multipart POST request. Required parameters include the Bearer token for authentication and the audio file. The output is JSON including the transcribed text. Ensure the audio file path and API key have correct values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_2\n\nLANGUAGE: curl\nCODE:\n```\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/transcriptions \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@/path/to/file/audio.mp3 \\\n  --form model=whisper-1\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Token Counting Function for Chat Messages - Python\nDESCRIPTION: Illustrates how to construct a list of chat messages with roles and example user/assistant names, then call the token counting function to determine the total prompt tokens for a gpt-3.5-turbo-0613 conversation. The example includes printing the token count which should approximate the API's reported total tokens for these messages when used in a chat completion request.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n  {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n]\n\nmodel = \"gpt-3.5-turbo-0613\"\n\nprint(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n# Should show ~126 total_tokens\n```\n\n----------------------------------------\n\nTITLE: Initializing Push Notification Data List in Python\nDESCRIPTION: Defines a list of multi-line string notifications representing various user alerts and updates, intended as input data for summarization or testing routines. No dependencies; purely data setup for subsequent processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npush_notification_data = [\n        \"\"\"\n- New message from Sarah: \\\"Can you call me later?\\\"\\n- Your package has been delivered!\\n- Flash sale: 20% off electronics for the next 2 hours!\\n\"\"\",\n        \"\"\"\n- Weather alert: Thunderstorm expected in your area.\\n- Reminder: Doctor's appointment at 3 PM.\\n- John liked your photo on Instagram.\\n\"\"\",\n        \"\"\"\n- Breaking News: Local elections results are in.\\n- Your daily workout summary is ready.\\n- Check out your weekly screen time report.\\n\"\"\",\n        \"\"\"\n- Your ride is arriving in 2 minutes.\\n- Grocery order has been shipped.\\n- Don't miss the season finale of your favorite show tonight!\\n\"\"\",\n        \"\"\"\n- Event reminder: Concert starts at 7 PM.\\n- Your favorite team just scored!\\n- Flashback: Memories from 3 years ago.\\n\"\"\",\n        \"\"\"\n- Low battery alert: Charge your device.\\n- Your friend Mike is nearby.\\n- New episode of \\\"The Tech Hour\\\" podcast is live!\\n\"\"\",\n        \"\"\"\n- System update available.\\n- Monthly billing statement is ready.\\n- Your next meeting starts in 15 minutes.\\n\"\"\",\n        \"\"\"\n- Alert: Unauthorized login attempt detected.\\n- New comment on your blog post: \\\"Great insights!\\\"\\n- Tonight's dinner recipe: Pasta Primavera.\\n\"\"\",\n        \"\"\"\n- Special offer: Free coffee with any breakfast order.\\n- Your flight has been delayed by 30 minutes.\\n- New movie release: \\\"Adventures Beyond\\\" now streaming.\\n\"\"\",\n        \"\"\"\n- Traffic alert: Accident reported on Main Street.\\n- Package out for delivery: Expected by 5 PM.\\n- New friend suggestion: Connect with Emma.\\n\"\"\"\n]\n```\n\n----------------------------------------\n\nTITLE: Fetching Customer Profile\nDESCRIPTION: This function `fetch_customer_profile` retrieves a user's profile based on their `user_id`. The implementation uses hardcoded data for demonstration purposes. In a production setting, this would be replaced with a database call to fetch the actual user profile. The profile includes location, preferences, behavioral metrics, recent searches, recent interactions, and a user rank.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef fetch_customer_profile(user_id):\n    # You can replace this with a real API call in the production code\n    if user_id == \"user1234\":\n        return {\n            \"name\": \"John Doe\",\n            \"location\": {\n                \"latitude\": 37.7955,\n                \"longitude\": -122.4026,\n            },\n            \"preferences\": {\n                \"food\": [\"Italian\", \"Sushi\"],\n                \"activities\": [\"Hiking\", \"Reading\"],\n            },\n            \"behavioral_metrics\": {\n                \"app_usage\": {\n                    \"daily\": 2,  # hours\n                    \"weekly\": 14  # hours\n                },\n                \"favourite_post_categories\": [\"Nature\", \"Food\", \"Books\"],\n                \"active_time\": \"Evening\",\n            },\n            \"recent_searches\": [\"Italian restaurants nearby\", \"Book clubs\"],\n            \"recent_interactions\": [\"Liked a post about 'Best Pizzas in New York'\", \"Commented on a post about 'Central Park Trails'\"],\n            \"user_rank\": \"Gold\",  # based on some internal ranking system\n        }\n    else:\n        return None\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cosine Similarity and Review Score Correlation with Matplotlib - Python\nDESCRIPTION: This snippet analyzes how user-product embedding similarities relate to review scores by computing the correlation coefficient, then visualizes this relationship with a boxplot. It relies on pandas for data handling and matplotlib for plotting. The input is the test DataFrame with precomputed similarity columns, and it outputs correlation statistics as well as a boxplot. Make sure matplotlib and statsmodels are installed. Visualization is limited to correlation and boxplot; other insights require further statistical analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/User_and_product_embeddings.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n\ncorrelation = X_test[['percentile_cosine_similarity', 'Score']].corr().values[0,1]\nprint('Correlation between user & vector similarity percentile metric and review number of stars (score): %.2f%%' % (100*correlation))\n\n# boxplot of cosine similarity for each score\nX_test.boxplot(column='percentile_cosine_similarity', by='Score')\nplt.title('')\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Importing Wikipedia Data into Weaviate using Batch Processing in Python\nDESCRIPTION: This snippet iterates through the previously loaded `dataset` (containing Wikipedia articles) and adds each article to the Weaviate batch importer. It maps the article's `title`, `text` (as content), and `url` to the corresponding properties defined in the \"Article\" schema. The `client.batch` context manager handles the efficient sending of data in batches according to the configuration set in the previous step. It prints progress updates every 10 articles.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n### Step 3 - import data\n\nprint(\"Importing Articles\")\n\ncounter=0\n\nwith client.batch as batch:\n    for article in dataset:\n        if (counter %10 == 0):\n            print(f\"Import {counter} / {len(dataset)} \")\n\n        properties = {\n            \"title\": article[\"title\"],\n            \"content\": article[\"text\"],\n            \"url\": article[\"url\"]\n        }\n        \n        batch.add_data_object(properties, \"Article\")\n        counter = counter+1\n\nprint(\"Importing Articles complete\")\n```\n\n----------------------------------------\n\nTITLE: Starting Redis Stack Docker Container via Docker Compose in Shell\nDESCRIPTION: Starts the Redis stack with required modules (Redis Search and Redis JSON) using Docker Compose in detached mode. Assumes a valid docker-compose.yml is present. This container provides the backend for storing JSON data and vector search capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n! docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Normalized Rank Distribution Calculation with Pandas in Python\nDESCRIPTION: This code computes and displays the normalized value counts (probabilities) for each possible retrieval rank up to 12, using pandas' value_counts and sort_index. Negative values (such as -2 and -1) represent processing errors or poor recall, allowing for error and coverage analysis. It requires a processed out_expanded DataFrame with a 'rank' column.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# normalized value_counts\nout_expanded['rank'].value_counts(normalize=True).sort_index()[:13]\n```\n\n----------------------------------------\n\nTITLE: Counting Examples by Category\nDESCRIPTION: Calculating the total number of examples and the distribution between baseball and hockey categories to check for data balance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlen_all, len_baseball, len_hockey = len(sports_dataset.data), len([e for e in sports_dataset.target if e == 0]), len([e for e in sports_dataset.target if e == 1])\nprint(f\"Total examples: {len_all}, Baseball examples: {len_baseball}, Hockey examples: {len_hockey}\")\n```\n\n----------------------------------------\n\nTITLE: Generating a Hypothetical Ideal Answer with GPT for Re-Ranking - Python\nDESCRIPTION: This code generates a 'hypothetical ideal answer' to the user's question using GPT, intended for improving result ranking by semantic similarity rather than lexical match. The answer uses placeholders and does not rely on real facts. Requires the json_gpt helper and OpenAI API access. Output is used as the semantic anchor for embedding comparison.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nHA_INPUT = f\"\"\"\nGenerate a hypothetical answer to the user's question. This answer will be used to rank search results. \nPretend you have all the information you need to answer, but don't use any actual facts. Instead, use placeholders\nlike NAME did something, or NAME said something at PLACE. \n\nUser question: {USER_QUESTION}\n\nFormat: {{\"hypotheticalAnswer\": \"hypothetical answer text\"}}\n\"\"\"\n\nhypothetical_answer = json_gpt(HA_INPUT)[\"hypotheticalAnswer\"]\n\nhypothetical_answer\n\n```\n\n----------------------------------------\n\nTITLE: Starting Qdrant with Docker Compose\nDESCRIPTION: This shell command uses `docker compose` to start the Qdrant service(s) defined in a `docker-compose.yml` file in detached mode. It's the recommended way to quickly deploy a local Qdrant instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Constructing System Prompt for LLM-based Entity Extraction in Python\nDESCRIPTION: Constructs a detailed system prompt (`system_prompt`) as a multi-line f-string. This prompt instructs an LLM on how to analyze user queries and extract relevant entities based on the graph schema (provided via JSON dumps of `entity_types` and `relation_types`). It includes an example and specifies the desired output format (a JSON object mapping extracted entity types to their values).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = f'''\n    You are a helpful agent designed to fetch information from a graph database. \n    \n    The graph database links products to the following entity types:\n    {json.dumps(entity_types)}\n    \n    Each link has one of the following relationships:\n    {json.dumps(relation_types)}\n\n    Depending on the user prompt, determine if it possible to answer with the graph database.\n        \n    The graph database can match products with multiple relationships to several entities.\n    \n    Example user input:\n    \"Which blue clothing items are suitable for adults?\"\n    \n    There are three relationships to analyse:\n    1. The mention of the blue color means we will search for a color similar to \"blue\"\n    2. The mention of the clothing items means we will search for a category similar to \"clothing\"\n    3. The mention of adults means we will search for an age_group similar to \"adults\"\n    \n    \n    Return a json object following the following rules:\n    For each relationship to analyse, add a key value pair with the key being an exact match for one of the entity types provided, and the value being the value relevant to the user query.\n    \n    For the example provided, the expected output would be:\n    {{\n        \"color\": \"blue\",\n        \"category\": \"clothing\",\n        \"age_group\": \"adults\"\n    }}\n    \n    If there are no relevant entities in the user prompt, return an empty json object.\n'''\n\nprint(system_prompt)\n```\n\n----------------------------------------\n\nTITLE: Defining Redshift and AWS Network Environment Variables in YAML\nDESCRIPTION: This YAML snippet presents the required environment variables for the Lambda function deployment, including Redshift connection parameters (host, port, user, password, database) and AWS network identifiers for secure Lambda VPC access (security group and multiple subnet IDs). These parameters are essential for secure, correct connectivity to Redshift within an AWS VPC.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nRedshiftHost: default-workgroup.xxxxx.{region}.redshift-serverless.amazonaws.com\nRedshiftPort: 5439\nRedshiftUser: username\nRedshiftPassword: password\nRedshiftDb: my-db\nSecurityGroupId: sg-xx\nSubnetId1: subnet-xx\nSubnetId2: subnet-xx\nSubnetId3: subnet-xx\nSubnetId4: subnet-xx\nSubnetId5: subnet-xx\nSubnetId6: subnet-xx\n```\n\n----------------------------------------\n\nTITLE: Defining ROUGE Evaluation Function in Python\nDESCRIPTION: Defines a Python function `get_rouge_scores` that takes two text strings, initializes the `Rouge` evaluator from the `rouge` library, and returns the calculated ROUGE scores. It also initializes an empty list `rouge_scores_out` to store the results of ROUGE evaluations performed later in the notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# function to calculate the Rouge score\ndef get_rouge_scores(text1, text2):\n    rouge = Rouge()\n    return rouge.get_scores(text1, text2)\n\n\nrouge_scores_out = []\n\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI Client and Helper Functions (Python)\nDESCRIPTION: Imports necessary Python libraries for interacting with the OpenAI API, handling data (pandas), displaying images (IPython, PIL, requests), and managing JSON. It initializes the OpenAI client using an API key from environment variables or a fallback, and defines helper functions for submitting messages, retrieving responses, and displaying JSON objects for the Assistants API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import display, Image\nfrom openai import OpenAI\nimport os\nimport pandas as pd\nimport json\nimport io\nfrom PIL import Image\nimport requests\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n#Lets import some helper functions for assistants from https://cookbook.openai.com/examples/assistants_api_overview_python\ndef show_json(obj):\n    display(json.loads(obj.model_dump_json()))\n\ndef submit_message(assistant_id, thread, user_message,file_ids=None):\n    params = {\n        'thread_id': thread.id,\n        'role': 'user',\n        'content': user_message,\n    }\n    if file_ids:\n        params['file_ids']=file_ids\n\n    client.beta.threads.messages.create(\n        **params\n)\n    return client.beta.threads.runs.create(\n    thread_id=thread.id,\n    assistant_id=assistant_id,\n)\n\ndef get_response(thread):\n    return client.beta.threads.messages.list(thread_id=thread.id)\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Timestamps from Transcriptions with Whisper API in Python\nDESCRIPTION: This code demonstrates how to request a structured JSON response with word-level timestamps from the OpenAI Whisper API using the Python SDK. Parameters include 'response_format' set to 'verbose_json' and 'timestamp_granularities' including 'word'. The code prints per-word information from the transcript. Dependencies are the OpenAI Python package, and the input is the path to an audio file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\naudio_file = open(\"speech.mp3\", \"rb\")\ntranscript = client.audio.transcriptions.create(\n  file=audio_file,\n  model=\"whisper-1\",\n  response_format=\"verbose_json\",\n  timestamp_granularities=[\"word\"]\n)\nprint(transcript.words)\n```\n\n----------------------------------------\n\nTITLE: Processing Eval Logs - Python\nDESCRIPTION: This code reads and processes the evaluation logs generated by the `oaieval` CLI.  It parses the JSONL file, extracts relevant information, and displays the evaluation report.  The code also retrieves and prints key metadata such as the evaluation specifications, final report, and individual evaluation events, providing insight into the evaluation process and results. This facilitates the analysis of the evaluation results and debugging.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlog_name = '240327024443FACXGMKA_gpt-3.5-turbo_spider-sql.jsonl' # \"EDIT THIS\" - copy from above\nevents = f\"/tmp/evallogs/{log_name}\"\ndisplay(pd.read_json(events, lines=True).head(5))\n\n# processing the log events generated by oaieval\n\nwith open(events, \"r\") as f:\n    events_df = pd.read_json(f, lines=True)\n\ndisplay(events_df.iloc[0].spec)\n\nLet's also look at the entry which provides the final report of the evaluation.\n\ndisplay(events_df.dropna(subset=['final_report']).iloc[0]['final_report'])\n\nWe can also review individual evaluation events that provide specific samples (`sample_id`), results, event types, and metadata.\n\npd.set_option('display.max_colwidth', None)  # None means no truncation\ndisplay(events_df.iloc[2][['run_id', 'event_id', 'sample_id', 'type', 'data', 'created_at']])\n```\n\n----------------------------------------\n\nTITLE: Creating Thread with Image Input Content (cURL)\nDESCRIPTION: This code snippet demonstrates how to create a thread with image input content using the OpenAI Assistants API via cURL.  It first uploads an image file with the 'vision' purpose, then it creates a thread with a message containing text and image references. It uses the file ID from the uploaded file in the thread's content. Requires setting the `$OPENAI_API_KEY` environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_5\n\nLANGUAGE: curl\nCODE:\n```\n# Upload a file with an \"vision\" purpose\ncurl https://api.openai.com/v1/files \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -F purpose=\"vision\" \\\n  -F file=\"@/path/to/myimage.png\"\n\n## Pass the file ID in the content\ncurl https://api.openai.com/v1/threads \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What is the difference between these images?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\"url\": \"https://example.com/image.png\"}\n          },\n          {\n            \"type\": \"image_file\",\n            \"image_file\": {\"file_id\": file.id}\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Fine-tuning JSONL Files for Training and Testing in Python\nDESCRIPTION: Iterates through model types ('discriminator', 'qa') and dataset splits ('train', 'test'). For each combination, it calls the `create_fine_tuning_dataset` function with appropriate parameters (passing the corresponding train/test DataFrame and setting flags for discriminator mode and inclusion of related examples). The resulting pandas DataFrame containing formatted prompts and completions is then saved as a JSON Lines (JSONL) file using `to_json`, ready for the OpenAI fine-tuning process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor name, is_disc in [('discriminator', True), ('qa', False)]:\n    for train_test, dt in [('train', train_df), ('test', test_df)]:\n        ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)\n        ft.to_json(f'{name}_{train_test}.jsonl', orient='records', lines=True)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries\nDESCRIPTION: This code snippet imports the necessary Python libraries for the notebook. It includes libraries for operating system interactions (os), arXiv searching (arxiv), abstract syntax trees (ast), concurrent execution (concurrent), JSON handling (json), data manipulation with pandas (pandas), tokenization (tiktoken), CSV writing (csv.writer), IPython display (IPython.display), OpenAI API interaction (openai), PDF processing (PyPDF2), spatial calculations (scipy.spatial), retry logic (tenacity), progress bars (tqdm), and terminal styling (termcolor).  It also initializes the OpenAI client and defines the GPT and embedding models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport arxiv\nimport ast\nimport concurrent\nimport json\nimport os\nimport pandas as pd\nimport tiktoken\nfrom csv import writer\nfrom IPython.display import display, Markdown, Latex\nfrom openai import OpenAI\nfrom PyPDF2 import PdfReader\nfrom scipy import spatial\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nfrom tqdm import tqdm\nfrom termcolor import colored\n\nGPT_MODEL = \"gpt-4o-mini\"\nEMBEDDING_MODEL = \"text-embedding-ada-002\"\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Executing Hybrid Search (Vector + Title Text) - Python\nDESCRIPTION: This snippet demonstrates a hybrid search by calling `search_redis`. It uses the `title_vector` for semantic similarity based on the query 'Famous battles in Scottish history', but additionally applies a filter generated by `create_hybrid_field` (`hybrid_fields='@title:\"Scottish\"'`) to restrict the results to only documents where the 'title' field contains the word \"Scottish\".\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# search the content vector for articles about famous battles in Scottish history and only include results with Scottish in the title\nresults = search_redis(redis_client,\n                       \"Famous battles in Scottish history\",\n                       vector_field=\"title_vector\",\n                       k=5,\n                       hybrid_fields=create_hybrid_field(\"title\", \"Scottish\")\n                       )\n```\n\n----------------------------------------\n\nTITLE: Streaming Real-Time Audio with OpenAI TTS API in Python\nDESCRIPTION: This Python snippet demonstrates how to stream real-time audio from the OpenAI TTS API using chunked transfer encoding. An OpenAI client is initialized and a speech synthesis request is made; the response object streams the audio to the specified output path as soon as data is received. Prerequisites are the openai Python package and a valid API key. Parameters include 'model', 'voice', and 'input'. The output is an MP3 file, written incrementally during the streaming process. Limitations reflect API voice/language constraints and file resource availability.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-to-speech.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello world! This is a streaming test.\",\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n----------------------------------------\n\nTITLE: Testing Question Generation for a Single PDF\nDESCRIPTION: Tests the question generation function on the first PDF file to verify the quality of generated questions for evaluation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngenerate_questions(pdf_files[0])\n```\n\n----------------------------------------\n\nTITLE: Extracting and Displaying ChatCompletion Results and Logprobs in Python\nDESCRIPTION: Extracts results from an OpenAI API response, including the content and token log probabilities, and prints them to the console, including the full logprobs object for detailed analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresult = response.choices[0]\nprint(f\"Result was {result.message.content}\")\nprint(f\"Logprobs was {result.logprobs.token_logprobs[0]}\")\nprint(\"\\nBelow is the full logprobs object\\n\\n\")\nprint(result[\"logprobs\"])\n```\n\n----------------------------------------\n\nTITLE: Fetching Hyperlinks from a URL in Python\nDESCRIPTION: Defines the `get_hyperlinks` function which takes a URL string as input. It attempts to open the URL using `urllib.request`, checks if the content type is HTML, reads and decodes the HTML content. It then uses the previously defined `HyperlinkParser` to parse the HTML and returns a list of all hyperlinks found on the page. Includes basic exception handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Function to get the hyperlinks from a URL\ndef get_hyperlinks(url):\n\n    # Try to open the URL and read the HTML\n    try:\n        # Open the URL and read the HTML\n        with urllib.request.urlopen(url) as response:\n\n            # If the response is not HTML, return an empty list\n            if not response.info().get('Content-Type').startswith(\"text/html\"):\n                return []\n\n            # Decode the HTML\n            html = response.read().decode('utf-8')\n    except Exception as e:\n        print(e)\n        return []\n\n    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n    parser = HyperlinkParser()\n    parser.feed(html)\n\n    return parser.hyperlinks\n```\n\n----------------------------------------\n\nTITLE: Calculating BERTScore for Summary Evaluation\nDESCRIPTION: This code snippet calculates the BERTScore for two summaries against a reference summary using the `BERTScorer` class. It initializes the scorer with the language 'en'. Then, it calls the `score` method to compute the precision, recall, and F1 score for both summaries, outputting the F1 scores for each. The F1 score is used as a metric to compare how well each summary aligns with the reference.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate the BERTScorer object for English language\nscorer = BERTScorer(lang=\"en\")\n\n# Calculate BERTScore for the summary 1 against the excerpt\n# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively\nP1, R1, F1_1 = scorer.score([eval_summary_1], [ref_summary])\n\n# Calculate BERTScore for summary 2 against the excerpt\n# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively\nP2, R2, F2_2 = scorer.score([eval_summary_2], [ref_summary])\n\nprint(\"Summary 1 F1 Score:\", F1_1.tolist()[0])\nprint(\"Summary 2 F1 Score:\", F2_2.tolist()[0])\n```\n\n----------------------------------------\n\nTITLE: Searching for \"delicious beans\" - Python\nDESCRIPTION: This code snippet invokes the `search_reviews` function to search for reviews related to \"delicious beans\" and retrieves the top 3 most similar reviews.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresults = search_reviews(df, \"delicious beans\", n=3)\n```\n\n----------------------------------------\n\nTITLE: Setting Up MongoDB and OpenAI Credentials in Python\nDESCRIPTION: Uses `getpass` to securely prompt the user for their MongoDB Atlas Cluster URI and OpenAI API Key, storing them in variables. These credentials are required to establish connections to the respective services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\n\nMONGODB_ATLAS_CLUSTER_URI = getpass.getpass(\"MongoDB Atlas Cluster URI:\")\nOPENAI_API_KEY = getpass.getpass(\"OpenAI API Key:\")\n\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration for Weaviate with OpenAI Modules\nDESCRIPTION: This Docker Compose file configures a Weaviate instance with OpenAI modules enabled. It defines the necessary services, environment variables, and volumes for running Weaviate with OpenAI integration.  This allows you to quickly deploy a Weaviate instance with OpenAI functionality.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/README.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nversion: '3.4'\nservices:\n  weaviate:\n    image: semitechnologies/weaviate:1.23.8\n    ports:\n     - 8080:8080\n     - 50051:50051\n    restart: on-failure:0\n    environment:\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      DEFAULT_VECTORIZER_MODULE: 'text2vec-openai'\n      ENABLE_MODULES: 'text2vec-openai,qna-openai'\n      OPENAI_APIKEY: ${OPENAI_APIKEY}\n    volumes:\n      - weaviate_data:/var/lib/weaviate\n\nvolumes:\n  weaviate_data:\n\n```\n\n----------------------------------------\n\nTITLE: Defining Weaviate Schema with OpenAI Embeddings - Python\nDESCRIPTION: Defines the schema for the 'Article' class in Weaviate. It specifies properties like 'title', 'content', and 'url', configures the 'text2vec-openai' vectorizer module, and includes property-specific configurations like skipping vectorization for the 'url' field.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Define the Schema object to use `text-embedding-3-small` on `title` and `content`, but skip it for `url`\narticle_schema = {\n    \"class\": \"Article\",\n    \"description\": \"A collection of articles\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"title\",\n        \"description\": \"Title of the article\",\n        \"dataType\": [\"string\"]\n    },\n    {\n        \"name\": \"content\",\n        \"description\": \"Contents of the article\",\n        \"dataType\": [\"text\"]\n    },\n    {\n        \"name\": \"url\",\n        \"description\": \"URL to the article\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}\n```\n\n----------------------------------------\n\nTITLE: Activating a Python Virtual Environment (Windows)\nDESCRIPTION: This command activates the virtual environment 'openai-env' on a Windows system. Activating the environment ensures that subsequent `pip` commands install packages into the isolated environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nopenai-env\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Initializing Translation Environment (Python)\nDESCRIPTION: Imports necessary libraries, including `openai` and `tiktoken`, initializes the OpenAI client for API interaction, sets up the tiktoken tokenizer for token counting, and reads the content of the source LaTeX file into memory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport tiktoken\nclient = OpenAI()\n\n# OpenAI tiktoken tokenizer: https://github.com/openai/tiktoken\n# we use it to count the number of tokens in the text\ntokenizer = tiktoken.get_encoding(\"o200k_base\")\n\nwith open(\"data/geometry_slovenian.tex\", \"r\") as f:\n    text = f.read()\n```\n\n----------------------------------------\n\nTITLE: Displaying Per-Item Generative Search Results from Weaviate in Python\nDESCRIPTION: Iterates over the results of generative_search_per_item, printing the title and generated singleResult for each article in a numbered list, separated by a dashed line. This snippet expects the response objects to have an '_additional.generate.singleResult' structure as produced by the OpenAI generative search. Prerequisites include successful execution of generative_search_per_item and correct API configuration; results depend on available data within the 'Article' collection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquery_result = generative_search_per_item(\"football clubs\", \"Article\")\n\nfor i, article in enumerate(query_result):\n    print(f\"{i+1}. { article['title']}\")\n    print(article['_additional']['generate']['singleResult']) # print generated response\n    print(\"-----------------------\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Batch File to OpenAI for Batch Job Creation\nDESCRIPTION: Uploads the prepared JSONL file to OpenAI's servers with purpose 'batch', returning a file object containing an ID for later batch job creation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbatch_file = client.files.create(\n  file=open(file_name, \"rb\"),\n  purpose=\"batch\"\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Query with Contexts\nDESCRIPTION: This snippet calls the retrieve function with a user query to get the context-enhanced prompt. It demonstrates how to use the retrieval function in a practical application and display the results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# first we retrieve relevant items from Pinecone\nquery_with_contexts = retrieve(query)\nquery_with_contexts\n```\n\n----------------------------------------\n\nTITLE: Initializing Braintrust and Async OpenAI Client in Python\nDESCRIPTION: Imports necessary modules, logs into the Braintrust platform using an API key retrieved from the environment variable 'BRAINTRUST_API_KEY', and initializes an asynchronous OpenAI client using the 'OPENAI_API_KEY' environment variable. The OpenAI client is wrapped with 'braintrust.wrap_openai' to enable logging of LLM calls to Braintrust.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nimport braintrust\nfrom openai import AsyncOpenAI\n\nbraintrust.login(api_key=os.environ[\"BRAINTRUST_API_KEY\"])\nclient = braintrust.wrap_openai(AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]))\n```\n\n----------------------------------------\n\nTITLE: Filtering Dataset by Recency and Token Length in Python\nDESCRIPTION: Sorts the dataset by review time and selects the 1,000 most recent entries, after initially filtering the last 2,000 to account for length-based removals. It removes the Time column and uses the tiktoken library to count tokens in the combined text. Reviews exceeding the 8,000-token limit for the embedding model are removed. This ensures that only embeddings of valid length inputs are generated.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# subsample to 1k most recent reviews and remove samples that are too long\ntop_n = 1000\ndf = df.sort_values(\"Time\").tail(top_n * 2)  # first cut to first 2k entries, assuming less than half will be filtered out\ndf.drop(\"Time\", axis=1, inplace=True)\n\nencoding = tiktoken.get_encoding(embedding_encoding)\n\n# omit reviews that are too long to embed\ndf[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\ndf = df[df.n_tokens <= max_tokens].tail(top_n)\nlen(df)\n```\n\n----------------------------------------\n\nTITLE: Preparing Batch Input File for OpenAI Batch API - JSONL\nDESCRIPTION: Defines the structure of an input batch file used to send multiple requests asynchronously via the OpenAI Batch API. Each line in the JSONL file represents a single API call containing a unique 'custom_id', HTTP method, endpoint URL, and request body matching the parameters of the target endpoint (e.g., chat completions). This file format is required to group multiple requests in a single batch job.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_0\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables and Imports in Python\nDESCRIPTION: Imports essential Python libraries including OpenAI client, boto3 for AWS, json for data handling, os for environment access, datetime for date conversions, and dotenv for loading variables. Loads environment variables from a local '.env' file to configure keys for AWS and OpenAI API authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport json\nimport boto3\nimport os\nimport datetime\nfrom urllib.request import urlretrieve\n\n# load environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Dataset Using Python BigQuery Client\nDESCRIPTION: Initializes and creates a dataset in BigQuery using the Python client library. The dataset ID is constructed from the project ID and raw dataset name. It specifies the geographic location region before issuing the create_dataset API call. The snippet handles API conflicts gracefully if the dataset already exists. It requires Google Cloud BigQuery Python client and valid credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nraw_dataset_id = 'oai_docs'\ndataset_id = project_id + '.' + raw_dataset_id\n\nclient = bigquery.Client(credentials=credentials, project=project_id)\n\ndataset = bigquery.Dataset(dataset_id)\n\ndataset.location = \"US\"\n\ntry:\n    dataset = client.create_dataset(dataset, timeout=30)\n    print(f\"Created dataset {client.project}.{dataset.dataset_id}\")\nexcept Conflict:\n    print(f\"dataset {dataset.dataset_id } already exists\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Drive Item Content\nDESCRIPTION: This asynchronous function fetches the content of a file from SharePoint, converts it into a base64 string, and restructures it to match the `openaiFileResponse` format expected by ChatGPT. It uses the Microsoft Graph client to get the file stream, reads the stream to get the file content, and then gets the file metadata. The output is an object containing the file name, mime type, and the base64 encoded content of the file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst getDriveItemContent = async (client, driveId, itemId, name) => {\n\t try{\n\t\t const filePath = `/drives/${driveId}/items/${itemId}`;\n\t\t const downloadPath = filePath + `/content`\n\t\t // this is where we get the contents and convert to base64\n\t\t const fileStream = await client.api(downloadPath).getStream();\n\t\t let chunks = [];\n\t\t\t for await (let chunk of fileStream) {\n\t\t\t\t chunks.push(chunk);\n\t\t\t }\n\t\t const base64String = Buffer.concat(chunks).toString('base64');\n\t\t // this is where we get the other metadata to include in response\n\t\t const file = await client.api(filePath).get();\n\t\t const mime_type = file.file.mimeType;\n\t\t const name = file.name;\n\t\t return {\"name\":name, \"mime_type\":mime_type, \"content\":base64String}\n\t } catch (error) {\n\t\t console.error('Error fetching drive content:', error);\n\t\t throw new Error(`Failed to fetch content for ${name}: ${error.message}`);\n\t }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying ChatGPT with a specific question\nDESCRIPTION: This code snippet demonstrates querying the OpenAI ChatGPT model with a specific question about the 2022 Winter Olympics curling gold medal winner.  It sets the OpenAI API key, defines the system and user messages, and prints the model's response. The API Key needs to be defined.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopenai.api_key = 'OPENAI API KEY'\n\nresponse = openai.ChatCompletion.create(\n  model=GPT_MODEL,\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the gold medal for curling in Olymics 2022?\"},\n    ]\n)\n\nprint(response['choices'][0]['message']['content'])\n```\n\n----------------------------------------\n\nTITLE: Creating and Launching Fine-tuning Job\nDESCRIPTION: This code uploads the training data file to the OpenAI API and starts a fine-tuning job using the uploaded file.  It specifies a model, the training file, and a suffix for the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Upload the training file\nfile = client.files.create(\n    file=open(\"data/drone_training.jsonl\", \"rb\"),\n    purpose=\"fine-tune\",\n)\nfile_id = file.id\nprint(f\"FileID: {file_id}\")\n\n# Create a fine-tuning job\n\nft = client.fine_tuning.jobs.create(\n    model=\"gpt-3.5-turbo\",\n    training_file=file_id,\n    suffix=\"drone\",\n)\n\nprint(f\"Fine-tuning job created: {ft}\")\n```\n\n----------------------------------------\n\nTITLE: Testing Quote Generation Function in Python\nDESCRIPTION: Examples demonstrating how to test the quote generation function by passing either a topic string or filtering by author for inspiration. The generated quote is printed to the console. This code depends on the previously defined generate_quote function and the surrounding OpenAI client and environment setup. Inputs are topic strings and optionally author names, while outputs are printed generated quotes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nq_topic = generate_quote(\"politics and virtue\")\nprint(\"\\nA new generated quote:\")\nprint(q_topic)\n```\n\nLANGUAGE: python\nCODE:\n```\nq_topic = generate_quote(\"animals\", author=\"schopenhauer\")\nprint(\"\\nA new generated quote:\")\nprint(q_topic)\n```\n\n----------------------------------------\n\nTITLE: Updating Assistant with Code Interpreter Tool (Python)\nDESCRIPTION: This snippet updates an existing assistant using the OpenAI Assistants API to include the Code Interpreter tool. It requires the OpenAI client library to be installed and configured. It updates the specified assistant ID to include the tool. The function show_json is called to display assistant details.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.update(\n    MATH_ASSISTANT_ID,\n    tools=[{\"type\": \"code_interpreter\"}],\n)\nshow_json(assistant)\n```\n\n----------------------------------------\n\nTITLE: Header Requirements for File URLs\nDESCRIPTION: This shows the required headers for the files, namely the `Content-Type` and `Content-Disposition`. The headers must be set so that a file name and MIME type can be determined, allowing the file to be visible to the user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\nContent-Type: application/pdf\nContent-Disposition: attachment; filename=\"example_document.pdf\"\n\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Batch with OpenAI API - Node.js\nDESCRIPTION: This Node.js snippet shows how to cancel a batch using the OpenAI API. It requires the `openai` library to be installed and imported. It calls the `openai.batches.cancel()` method with the batch ID (`batch_abc123`) to cancel the batch. The result is logged to the console after the batch is cancelled.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_7\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const batch = await openai.batches.cancel(\"batch_abc123\");\n\n  console.log(batch);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to JSONL Format for Fine-Tuning in Python\nDESCRIPTION: This function converts a pandas DataFrame into the JSONL format required for OpenAI fine-tuning. It structures each row as a conversation with system, user, and assistant messages, where the user message contains a question with context and the assistant responds with the answer.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef dataframe_to_jsonl(df):\n    def create_jsonl_entry(row):\n        answer = row[\"answers\"][0] if row[\"answers\"] else \"I don't know\"\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n            Question: {row.question}\\n\\n\n            Context: {row.context}\\n\\n\n            Answer:\\n\"\"\",\n            },\n            {\"role\": \"assistant\", \"content\": answer},\n        ]\n        return json.dumps({\"messages\": messages})\n\n    jsonl_output = df.apply(create_jsonl_entry, axis=1)\n    return \"\\n\".join(jsonl_output)\n\ntrain_sample = get_diverse_sample(train_df, sample_size=100, random_state=42)\n\nwith open(\"local_cache/100_train.jsonl\", \"w\") as f:\n    f.write(dataframe_to_jsonl(train_sample))\n```\n\n----------------------------------------\n\nTITLE: Adding a Socket Event (JavaScript)\nDESCRIPTION: This JavaScript snippet shows how to add a new socket event in the mirror server for a new language.  The `socket.on` function listens for a specific event emitted by a client. The `socket.broadcast.emit` function broadcasts the received data to all connected clients except the sender.  In this case, it is mirroring audio data from the speaker to the listener apps. Replace `hi` with the correct language code and adjust logging as necessary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nsocket.on('mirrorAudio:hi', (audioChunk) => {\n  console.log('logging Hindi mirrorAudio', audioChunk);\n  socket.broadcast.emit('audioFrame:hi', audioChunk);\n});\n```\n\n----------------------------------------\n\nTITLE: Validating Issues with ThreadPoolExecutor - Python\nDESCRIPTION: Validates issues using a `ThreadPoolExecutor` to parallelize the `validate_issue` function for rows where both predicted and true 'is_valid' are False. It stores the results, calculates issue accuracy, and creates DataFrames to store validation results and overall model results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Validate issues for rows where both true and predicted 'is_valid' are False\nvalidation_results = []\n\nwith ThreadPoolExecutor() as executor:\n    futures = {\n        executor.submit(validate_issue, pred_issues[i], true_issues[i]): i\n        for i in range(len(pred_is_valid_bool))\n        if not pred_is_valid_bool[i] and not true_is_valid_bool[i]\n    }\n    \n    for future in as_completed(futures):\n        i = futures[future]  # Get the original index\n        issue_match = future.result()\n        issue_matches_full[i] = (issue_match == 'True')\n        validation_results.append({\n            \"index\": i,\n            \"predicted_issue\": pred_issues[i],\n            \"true_issue\": true_issues[i],\n            \"issue_match\": issue_matches_full[i]\n        })\n    \n    # Calculate issue accuracy\n    issue_accuracy = sum([i['issue_match'] for i in validation_results]) / len(validation_results)\n    \n    # Store the results in the dictionary\n    model_results = {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"issue_accuracy\": issue_accuracy\n    }\n\n# Create a DataFrame to store the results\ndf_results = pd.DataFrame([model_results])\n\n# Create a DataFrame to store the validation results for each row\ndf_validation_results = pd.DataFrame(validation_results)\n```\n\n----------------------------------------\n\nTITLE: Attaching Vector Stores Node.js\nDESCRIPTION: This code snippet attaches vector stores to both an Assistant and a Thread within the OpenAI API using Node.js. It creates an assistant, specifying its instructions, model, and the file search tool. The `tool_resources` parameter is used to attach a vector store. It creates a thread attaching a different vector store, providing the assistant with context for responding to user questions based on the files available in the vector stores.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_20\n\nLANGUAGE: node.js\nCODE:\n```\nconst assistant = await openai.beta.assistants.create({\n  instructions: \"You are a helpful product support assistant and you answer questions based on the files provided to you.\",\n  model: \"gpt-4o\",\n  tools: [{\"type\": \"file_search\"}],\n  tool_resources: {\n    \"file_search\": {\n      \"vector_store_ids\": [\"vs_1\"]\n    }\n  }\n});\n\nconst thread = await openai.beta.threads.create({\n  messages: [ { role: \"user\", content: \"How do I cancel my subscription?\"} ],\n  tool_resources: {\n    \"file_search\": {\n      \"vector_store_ids\": [\"vs_2\"]\n    }\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Chat Completion with Azure OpenAI and Custom Data Source in Python\nDESCRIPTION: Makes a chat completion request to the configured Azure OpenAI model using the initialized `client`. It includes the user's message and specifies an Azure AI Search index as the data source in the `extra_body` parameter, providing the search endpoint, key, and index name retrieved from environment variables (`SEARCH_ENDPOINT`, `SEARCH_KEY`, `SEARCH_INDEX_NAME`). The response content and the retrieved context (stored in `model_extra`) are printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncompletion = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"}],\n    model=deployment,\n    extra_body={\n        \"dataSources\": [\n            {\n                \"type\": \"AzureCognitiveSearch\",\n                \"parameters\": {\n                    \"endpoint\": os.environ[\"SEARCH_ENDPOINT\"],\n                    \"key\": os.environ[\"SEARCH_KEY\"],\n                    \"indexName\": os.environ[\"SEARCH_INDEX_NAME\"],\n                }\n            }\n        ]\n    }\n)\nprint(f\"{completion.choices[0].message.role}: {completion.choices[0].message.content}\")\n\n# `context` is in the model_extra for Azure\nprint(f\"\\nContext: {completion.choices[0].message.model_extra['context']['messages'][0]['content']}\")\n```\n\n----------------------------------------\n\nTITLE: Attaching User-Provided File to Thread in Python\nDESCRIPTION: Illustrates uploading a user-provided file (Apple's 10-K pdf) to the OpenAI file storage, creating a new thread with a user message that includes the file as a message attachment for file_search tool use. This associates a vector store with the thread capable of querying attached files. Prints the thread's file_search tool_resources. Requires openai SDK file and beta threads APIs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Upload the user provided file to OpenAI\nmessage_file = client.files.create(\n  file=open(\"edgar/aapl-10k.pdf\", \"rb\"), purpose=\"assistants\"\n)\n \n# Create a thread and attach the file to the message\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"How many shares of AAPL were outstanding at the end of of October 2023?\",\n      # Attach the new file to the message.\n      \"attachments\": [\n        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n      ],\n    }\n  ]\n)\n \n# The thread now has a vector store with that file in its tool resources.\nprint(thread.tool_resources.file_search)\n```\n\n----------------------------------------\n\nTITLE: Create Eval Runs for Prompt Versions - Python\nDESCRIPTION: This code creates eval runs to compare the performance of different prompt versions.  It creates runs, each associated with a specific `eval_id` and data source configurations filtered by metadata. Each run uses the 'stored_completions' data source. The code creates one run for 'v1' and another for 'v2'. The results provide the report URLs for each run.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Grade prompt_version=v1\neval_run_result = await client.evals.runs.create(\n    eval_id=eval_id,\n    name=\"v1-run\",\n    data_source={\n        \"type\": \"completions\",\n        \"source\": {\n            \"type\": \"stored_completions\",\n            \"metadata\": {\n                \"prompt_version\": \"v1\",\n            }\n        }\n    }\n)\nprint(eval_run_result.report_url)\n```\n```python\n# Grade prompt_version=v2\neval_run_result_v2 = await client.evals.runs.create(\n    eval_id=eval_id,\n    name=\"v2-run\",\n    data_source={\n        \"type\": \"completions\",\n        \"source\": {\n            \"type\": \"stored_completions\",\n            \"metadata\": {\n                \"prompt_version\": \"v2\",\n            }\n        }\n    }\n)\nprint(eval_run_result_v2.report_url)\n```\n```\n\n----------------------------------------\n\nTITLE: Directly Prompting GPT-3.5 for Multilingual Summarization (Incorrect Language)\nDESCRIPTION: This snippet attempts to have `gpt-3.5-turbo-instruct` summarize a Spanish text in its original language (Spanish) with a direct prompt. However, the model defaults to providing the summary in English, failing to adhere to the language constraint.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_4\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nSummarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estadÃ­stica (la forma femenina del tÃ©rmino alemÃ¡n Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")â€‹ es una ciencia que estudia la variabilidad, colecciÃ³n, organizaciÃ³n, anÃ¡lisis, interpretaciÃ³n, y presentaciÃ³n de los datos, asÃ­ como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.â€‹ La estadÃ­stica es una ciencia formal deductiva, con un conocimiento propio, dinÃ¡mico y en continuo desarrollo obtenido a travÃ©s del mÃ©todo cientÃ­fico formal. En ocasiones, las ciencias fÃ¡cticas necesitan utilizar tÃ©cnicasÃ­sticas durante su proceso de investigaciÃ³n factual, con el fin de obtener nuevos conocimientos basados en la experimentaciÃ³n y en la observaciÃ³n. En estos casos, la aplicaciÃ³n de la estadÃ­stica permite el anÃ¡lisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenÃ³meno fÃ­sico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nSummary:\n```\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nThe text explains that statistics is a science that studies the variability, collection, organization, analysis, interpretation, and presentation of data, as well as the random process that generates them following the laws of probability.\n```\n\n----------------------------------------\n\nTITLE: Running Baseline Eval with Summarization Function - Python\nDESCRIPTION: Loops through push notifications data, generates summaries via summarize_push_notification, constructs run_data, and creates an eval run through OpenAI's evals API. Prints the run result and UI URL. Dependencies: openai, summarize_push_notification, earlier eval_id, and dataset. Input: push_notification_data (list). Output: eval run result object. Limitation: API calls consume credits; run must match eval schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nrun_data = []\nfor push_notifications in push_notification_data:\n    result = summarize_push_notification(push_notifications)\n    run_data.append({\n        \"item\": PushNotifications(notifications=push_notifications).model_dump(),\n        \"sample\": result.model_dump()\n    })\n\neval_run_result = openai.evals.runs.create(\n    eval_id=eval_id,\n    name=\"baseline-run\",\n    data_source={\n        \"type\": \"jsonl\",\n        \"source\": {\n            \"type\": \"file_content\",\n            \"content\": run_data,\n        }\n    },\n)\nprint(eval_run_result)\n# Check out the results in the UI\nprint(eval_run_result.report_url)\n```\n\n----------------------------------------\n\nTITLE: Executing Document Loading into Redis Index - Python\nDESCRIPTION: This code snippet calls the `index_documents` function with the initialized Redis client (`redis_client`), the defined key `PREFIX`, and the `article_df` DataFrame containing the data and embeddings. After execution, it prints a message indicating the number of documents loaded by querying the Redis database information.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nindex_documents(redis_client, PREFIX, article_df)\nprint(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for GPT Image Operations in Python\nDESCRIPTION: Imports necessary libraries for handling base64 encoding, file operations, OpenAI client, image processing, and Jupyter display functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport os\nfrom openai import OpenAI\nfrom PIL import Image\nfrom io import BytesIO\nfrom IPython.display import Image as IPImage, display\n```\n\n----------------------------------------\n\nTITLE: Providing a User Identifier using OpenAI API (Python)\nDESCRIPTION: This Python code snippet demonstrates how to provide a user identifier when making a request to the OpenAI API.  It utilizes the OpenAI library to create a completion using a specified model, prompt, and max_tokens,  and includes a user parameter for identification. The user parameter is used to help monitor and detect abuse.  It requires the `openai` library to be installed.  The response will contain the API completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/safety-best-practices.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.completions.create(\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=\"This is a test\",\n  max_tokens=5,\n  user=\"user_123456\"\n)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Prompt-Model Combinations for Testing in Python\nDESCRIPTION: Implements nested loops to test all combinations of predefined prompt variations and AI models, constructing data source configurations for evaluation runs. Automates batch testing of prompt design impacts across multiple models and datasets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfor prompt_name, prompt in prompts:\n    for model in models:\n        run_data_source = {\n            \"type\": \"completions\",\n            \"input_messages\": {\n                \"type\": \"template\",\n                \"template\": [\n                    {\n                        \"role\": \"developer\",\n                        \"content\": prompt,\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"<push_notifications>{{item.notifications}}</push_notifications>\",\n                    },\n                ],\n            },\n            \"model\": model,\n            \"source\": {\n                \"type\": \"file_content\",\n                \"content\": [\n                    {\n                        \"item\": PushNotifications(notifications=notification).model_dump()\n                    }\n                    for notification in push_notification_data\n                ],\n            },\n        }\n\n        run_create_result = openai.evals.runs.create(\n            eval_id=eval_id,\n            name=f\"bulk_{prompt_name}_{model}\",\n            data_source=run_data_source,\n        )\n        print(f\"Report URL {model}, {prompt_name}:\", run_create_result.report_url)\n\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuned Prompt Example (Text)\nDESCRIPTION: This example demonstrates a prompt used with a fine-tuned model. The model has been specifically trained on similar prompt-completion pairs. A separator sequence (###) is used to clearly delineate the end of the prompt and the beginning of the expected completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nâ€œSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.â€\nâ€• Ted Chiang, Exhalation\n\n###\n```\n\n----------------------------------------\n\nTITLE: Processing and Retrieving Similar Images with GPT-4 Multimodal API\nDESCRIPTION: This Python snippet iterates over a list of image URLs, describes each image, generates captions, and performs similarity search to retrieve similar images. Dependencies include functions such as describe_image, caption_image, and search_from_input_text, as well as data stored in 'df'. The process outputs image comparisons and similarity scores for each input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfor i in example_images:\n    img_description = describe_image(i, '')\n    caption = caption_image(img_description)\n    img = Image(url=i)\n    print('Input: \\n')\n    display(img)\n    res = search_from_input_text(caption, 1).iloc[0]\n    similarity_score = res['similarity']\n    if isinstance(similarity_score, np.ndarray):\n        similarity_score = similarity_score[0][0]\n    print(f\"{res['title'][:50]}{'...' if len(res['title']) > 50 else ''} ({res['url']}) - Similarity: {similarity_score:.2f}\")\n    img_res = Image(url=res['primary_image'])\n    display(img_res)\n    print(\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Printing Sample Answer\nDESCRIPTION: Displays the first answer from the loaded dataset to inspect its format and content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(answers[0])\n```\n\n----------------------------------------\n\nTITLE: Calculate Average Distance Function\nDESCRIPTION: Defines a function 'calculate_average_distance' that calculates the average distance between the embeddings of a list of responses.  It uses the `get_embedding` function to generate embeddings, and `distances_from_embeddings` to calculate distances.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_average_distance(responses):\n    \"\"\"\n    This function calculates the average distance between the embeddings of the responses.\n    The distance between embeddings is a measure of how similar the responses are.\n    \"\"\"\n    # Calculate embeddings for each response\n    response_embeddings = [get_embedding(response) for response in responses]\n\n    # Compute distances between the first response and the rest\n    distances = distances_from_embeddings(response_embeddings[0], response_embeddings[1:])\n\n    # Calculate the average distance\n    average_distance = sum(distances) / len(distances)\n\n    # Return the average distance\n    return average_distance\n```\n\n----------------------------------------\n\nTITLE: Displaying Original and Edited Images\nDESCRIPTION: This code snippet displays both the original generated image and the edited image using the PIL library. It prints the file paths of both images and then displays them. This allows the user to visualize the before-and-after effect of the edit.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# print the original image\nprint(generated_image_filepath)\ndisplay(Image.open(generated_image_filepath))\n\n# print edited image\nprint(edited_image_filepath)\ndisplay(Image.open(edited_image_filepath))\n```\n\n----------------------------------------\n\nTITLE: Creating a Batch Job using OpenAI SDK\nDESCRIPTION: This code creates a batch job by referencing the uploaded file ID and specifying the API endpoint and completion window. It utilizes the OpenAI SDK methods to initialize the job, which will process all tasks asynchronously. Dependencies include the client SDK and prior upload result.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n# Creating the job\n\nbatch_job = client.batches.create(\n  input_file_id=batch_file.id,\n  endpoint=\"/v1/chat/completions\",\n  completion_window=\"24h\"\n)\n```\n\n----------------------------------------\n\nTITLE: Dropping Astra DB Tables (Python/CQL)\nDESCRIPTION: Executes CQL commands to drop the `philosophers_cql` and `philosophers_cql_partitioned` tables from the specified keyspace, providing a cleanup mechanism to remove resources used during the demo. Requires a database `session` and `keyspace` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nsession.execute(f\"DROP TABLE IF EXISTS {keyspace}.philosophers_cql;\")\nsession.execute(f\"DROP TABLE IF EXISTS {keyspace}.philosophers_cql_partitioned;\")\n```\n\n----------------------------------------\n\nTITLE: Reading Data into Pandas DataFrame - Python\nDESCRIPTION: Reads the extracted CSV file (`vector_database_wikipedia_articles_embedded.csv`) into a Pandas DataFrame named `article_df`. This DataFrame will be used to store and manipulate the embedded data before inserting it into the MyScale database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n```\n\n----------------------------------------\n\nTITLE: Histogram of Minimum Token Counts Needed for Retrieval with Matplotlib in Python\nDESCRIPTION: This visualization code plots a histogram showing the number of tokens required to retrieve relevant context sections (for token counts 0â€“2000) via matplotlib. It uses the out_expanded DataFrame's 'tokens' column, bins the distribution, and provides appropriate labels and title for clarity. Requires matplotlib and pandas.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nout_expanded[(out_expanded.tokens>=0)&(out_expanded.tokens < 2000)]['tokens'].hist(bins=29)\nplt.xlabel('tokens')\nplt.ylabel('count')\nplt.title('Histogram of the number of minimum tokens needed')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Image in Jupyter Notebook in Python\nDESCRIPTION: Uses IPython.display functionality to show the generated image in a Jupyter Notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Show the result\ndisplay(IPImage(img_path1))\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Python SDK\nDESCRIPTION: Installs the latest version of the OpenAI Python SDK which is required to interact with the OpenAI API for generating structured chat completion responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install openai -U\n```\n\n----------------------------------------\n\nTITLE: Example Completions Prompt for Translation\nDESCRIPTION: This text snippet demonstrates a basic prompt format suitable for the OpenAI Completions API to instruct the model to perform a translation task. It uses a placeholder '{text}' to indicate where the input text for translation should be inserted.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\nTranslate the following English text to French: \"{text}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Simple News Summarization Prompt\nDESCRIPTION: Creates a basic prompt template for summarizing news articles, which will later be improved through meta prompting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsimple_prompt = \"Summarize this news article: {article}\"\n```\n\n----------------------------------------\n\nTITLE: Listing Run Steps (Python)\nDESCRIPTION: This snippet retrieves the steps of a run using the OpenAI Assistants API. It requires a thread ID and a run ID. It lists the run steps in ascending order and stores it in run_steps variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nrun_steps = client.beta.threads.runs.steps.list(\n    thread_id=thread.id, run_id=run.id, order=\"asc\"\n)\n```\n\n----------------------------------------\n\nTITLE: Listing all Batches with OpenAI API - cURL\nDESCRIPTION: This cURL command lists batches using the OpenAI API, paginating by a limit of 10. It requires the `OPENAI_API_KEY` environment variable to be set with the user's API key. It sends a GET request to the `/v1/batches` endpoint and takes a `limit` parameter to control the number of results. The user has to have cURL installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_9\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/batches?limit=10 \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\n```\n\n----------------------------------------\n\nTITLE: Querying GPT-4 with different primer Python\nDESCRIPTION: Queries GPT-4 with a modified primer, removing the instruction to say \"I don't know\". It then uses this modified prompt, with the same original query, to see if it would result in any difference in the response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nres = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n        {\"role\": \"user\", \"content\": query}\n    ]\n)\ndisplay(Markdown(res['choices'][0]['message']['content']))\n```\n\n----------------------------------------\n\nTITLE: Initializing SaaS Weaviate Client\nDESCRIPTION: Initializes a Weaviate client, enabling a connection to a Weaviate Cloud Service (SaaS) instance. It sets the URL to the Weaviate cluster address, obtained after setting up a free account and/or logging in, and includes an API key in the header for authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Option #2 - SaaS - (Weaviate Cloud Service)\nclient = weaviate.Client(\n    url=\"https://your-wcs-instance-name.weaviate.network\",\n    additional_headers={\n        \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Plotting Unit Test Results - Python\nDESCRIPTION: This code generates a bar chart visualizing the unit test results. It first pivots the `run_df` DataFrame using `pd.pivot_table` to aggregate counts of 'format' based on 'run' and 'unit_test_evaluation'. Then it uses matplotlib to create a bar chart comparing the number of records per unit test evaluation for each run. It uses distinct colors for each run, and the x-axis labels correspond to the unit test evaluations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nunittest_df_pivot = pd.pivot_table(\n    run_df,\n    values='format',\n    index=['run','unit_test_evaluation'],\n    aggfunc='count'\n)\nunittest_df_pivot.columns = ['Number of records']\nunittest_df_pivot\n```\n\nLANGUAGE: python\nCODE:\n```\nunittest_df_pivot.reset_index(inplace=True)\n\n# Plotting\nplt.figure(figsize=(10, 6))\n\n# Set the width of each bar\nbar_width = 0.35\n\n# OpenAI brand colors\nopenai_colors = ['#00D1B2', '#000000']  # Green and Black\n\n# Get unique runs and unit test evaluations\nunique_runs = unittest_df_pivot['run'].unique()\nunique_unit_test_evaluations = unittest_df_pivot['unit_test_evaluation'].unique()\n\n# Ensure we have enough colors (repeating the pattern if necessary)\ncolors = openai_colors * (len(unique_runs) // len(openai_colors) + 1)\n\n# Iterate over each run to plot\nfor i, run in enumerate(unique_runs):\n    run_data = unittest_df_pivot[unittest_df_pivot['run'] == run]\n\n    # Position of bars for this run\n    positions = np.arange(len(unique_unit_test_evaluations)) + i * bar_width\n\n    plt.bar(positions, run_data['Number of records'], width=bar_width, label=f'Run {run}', color=colors[i])\n\n# Setting the x-axis labels to be the unit test evaluations, centered under the groups\nplt.xticks(np.arange(len(unique_unit_test_evaluations)) + bar_width / 2, unique_unit_test_evaluations)\n\nplt.xlabel('Unit Test Evaluation')\nplt.ylabel('Number of Records')\nplt.title('Unit Test Evaluations vs Number of Records for Each Run')\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Function for Logging Parameterized Prompt Engineering Experiments - Python\nDESCRIPTION: This function, explain_math, wraps a monitored OpenAI ChatCompletion call with dynamic system/user prompts and extra experiment metadata as monitor_attributes for robust tracking. Dependencies: openai, a monitored weave environment, and string formatting for prompt templates. Inputs specify the system prompt, prompt template, and parameters; outputs are logged to the dashboard. Useful for repeatable structured experiments and comparison of results across different prompts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef explain_math(system_prompt, prompt_template, params):\n    openai.ChatCompletion.create(model=OPENAI_MODEL,\n                             messages=[\n                                    {\"role\": \"system\", \"content\": system_prompt},\n                                    {\"role\": \"user\", \"content\": prompt_template.format(**params)},\n                                ],\n                             # you can add additional attributes to the logged record\n                             # see the monitor_api notebook for more examples\n                             monitor_attributes={\n                                 'system_prompt': system_prompt,\n                                 'prompt_template': prompt_template,\n                                 'params': params\n                             })\n```\n\n----------------------------------------\n\nTITLE: Calculating Accuracy of Model Predictions\nDESCRIPTION: This snippet defines a function `get_accuracy` to calculate the accuracy of model predictions by comparing a model's output against a ground truth column in a Pandas DataFrame. It then iterates over a list of models and prints their accuracy. Requires a Pandas DataFrame with 'variety' column (ground truth), and a column named '{model}-variety'. The output is the accuracy score of each model, printed to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nmodels = ['gpt-4o', 'gpt-4o-mini']\n\ndef get_accuracy(model, df):\n    return np.mean(df['variety'] == df[model + '-variety'])\n\nfor model in models:\n    print(f\"{model} accuracy: {get_accuracy(model, df_france_subset) * 100:.2f}%\")\n```\n```\n\n----------------------------------------\n\nTITLE: Configure Data Source for Eval - Python\nDESCRIPTION: This code defines the configuration for the data source used in the evaluation. It specifies the type as 'stored_completions' and includes metadata to filter completions by the 'usecase'.  This sets up the data that will be available for the eval runs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\ndata_source_config = {\n    \"type\": \"stored_completions\",\n    \"metadata\": {\n        \"usecase\": \"push_notifications_summarizer\"\n    }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Quote Generation Function using Vector Search and LLM in Python\nDESCRIPTION: Defines a Python function `generate_quote` that combines vector search and LLM capabilities. It first uses `find_quote_and_author` to get relevant quotes based on a topic (and optional filters), formats these quotes into a prompt using a template, sends the prompt to an OpenAI chat completion model, and returns the generated quote.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef generate_quote(topic, n=2, author=None, tags=None):\n    quotes = find_quote_and_author(query_quote=topic, n=n, author=author, tags=tags)\n    if quotes:\n        prompt = generation_prompt_template.format(\n            topic=topic,\n            examples=\"\\n\".join(f\"  - {quote[0]}\" for quote in quotes),\n        )\n        # a little logging:\n        print(\"** quotes found:\")\n        for q, a in quotes:\n            print(f\"**    - {q} ({a})\")\n        print(\"** end of logging\")\n        #\n        response = client.chat.completions.create(\n            model=completion_model_name,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7,\n            max_tokens=320,\n        )\n        return response.choices[0].message.content.replace('\"', '').strip()\n    else:\n        print(\"** no quotes found.\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Viewing OpenAI Fine-Tuning Event Metrics (JSON)\nDESCRIPTION: This JSON object shows an example of a fine-tuning job event, specifically a metrics update. It contains the event ID, creation time, log level, a descriptive message summarizing key metrics, and a data object detailing metrics like step number, training loss, validation loss, full validation loss, and token accuracies at that specific step during the training process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"object\": \"fine_tuning.job.event\",\n    \"id\": \"ftevent-abc-123\",\n    \"created_at\": 1693582679,\n    \"level\": \"info\",\n    \"message\": \"Step 300/300: training loss=0.15, validation loss=0.27, full validation loss=0.40\",\n    \"data\": {\n        \"step\": 300,\n        \"train_loss\": 0.14991648495197296,\n        \"valid_loss\": 0.26569826706596045,\n        \"total_steps\": 300,\n        \"full_valid_loss\": 0.4032616495084362,\n        \"train_mean_token_accuracy\": 0.9444444179534912,\n        \"valid_mean_token_accuracy\": 0.9565217391304348,\n        \"full_valid_mean_token_accuracy\": 0.9089635854341737\n    },\n    \"type\": \"metrics\"\n}\n```\n\n----------------------------------------\n\nTITLE: Checking OpenAI SDK Version\nDESCRIPTION: This snippet retrieves and displays the installed version of the OpenAI Python SDK using `pip show`. It utilizes `grep Version` to filter the output, providing a quick check to confirm that the SDK is up-to-date.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip show openai | grep Version\n```\n\n----------------------------------------\n\nTITLE: Executing Tool Calls in Multi-Agent System\nDESCRIPTION: Implements a function to execute tool calls from different agents. This function maps tool names to the appropriate functions and handles returning results back to the conversation history.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define the function to execute the tools\ndef execute_tool(tool_calls, messages):\n    for tool_call in tool_calls:\n        tool_name = tool_call.function.name\n        tool_arguments = json.loads(tool_call.function.arguments)\n\n        if tool_name == 'clean_data':\n            # Simulate data cleaning\n            cleaned_df = clean_data(tool_arguments['data'])\n            cleaned_data = {\"cleaned_data\": cleaned_df.to_dict()}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(cleaned_data)})\n            print('Cleaned data: ', cleaned_df)\n        elif tool_name == 'transform_data':\n            # Simulate data transformation\n            transformed_data = {\"transformed_data\": \"sample_transformed_data\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(transformed_data)})\n        elif tool_name == 'aggregate_data':\n            # Simulate data aggregation\n            aggregated_data = {\"aggregated_data\": \"sample_aggregated_data\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(aggregated_data)})\n        elif tool_name == 'stat_analysis':\n            # Simulate statistical analysis\n            stats_df = stat_analysis(tool_arguments['data'])\n            stats = {\"stats\": stats_df.to_dict()}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(stats)})\n            print('Statistical Analysis: ', stats_df)\n        elif tool_name == 'correlation_analysis':\n            # Simulate correlation analysis\n            correlations = {\"correlations\": \"sample_correlations\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(correlations)})\n        elif tool_name == 'regression_analysis':\n            # Simulate regression analysis\n            regression_results = {\"regression_results\": \"sample_regression_results\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(regression_results)})\n        elif tool_name == 'create_bar_chart':\n            # Simulate bar chart creation\n            bar_chart = {\"bar_chart\": \"sample_bar_chart\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(bar_chart)})\n        elif tool_name == 'create_line_chart':\n            # Simulate line chart creation\n            line_chart = {\"line_chart\": \"sample_line_chart\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(line_chart)})\n            plot_line_chart(tool_arguments['data'])\n        elif tool_name == 'create_pie_chart':\n            # Simulate pie chart creation\n            pie_chart = {\"pie_chart\": \"sample_pie_chart\"}\n            messages.append({\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(pie_chart)})\n    return messages\n```\n\n----------------------------------------\n\nTITLE: Categorizing Text with OpenAI API\nDESCRIPTION: This function categorizes text using an LLM through the OpenAI API. It constructs a prompt containing the text and a predefined set of categories. It calls the OpenAI API to obtain a category for the text. Returns the category name, or None if an error occurs during the API call. It requires `openai_client` be defined.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncategories = ['authentication','models','techniques','tools','setup','billing_limits','other']\n\ndef categorize_text(text, categories):\n\n    # Create a prompt for categorization\n    messages = [\n        {\"role\": \"system\", \"content\": f\"\"\"You are an expert in LLMs, and you will be given text that corresponds to an article in OpenAI's documentation.\n         Categorize the document into one of these categories: {', '.join(categories)}. Only respond with the category name and nothing else.\"\"\"},\n        {\"role\": \"user\", \"content\": text}\n    ]\n    try:\n        # Call the OpenAI API to categorize the text\n        response = openai_client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n\n        # Extract the category from the response\n        category = response.choices[0].message.content\n        return category\n    except Exception as e:\n        print(f\"Error categorizing text: {str(e)}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Generating CSV Data Directly with OpenAI API in Python\nDESCRIPTION: Uses the OpenAI Chat Completions API (`gpt-4o-mini`) to generate 10 rows of synthetic housing data directly in CSV format. The prompt specifies the required columns (id, house size, price, location, number of bedrooms) and provides context to ensure the generated data relationships are logical. The response content, expected to be the CSV data, is then printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndatagen_model = \"gpt-4o-mini\"\nquestion = \"\"\"\nCreate a CSV file with 10 rows of housing data.\nEach row should include the following fields:\n - id (incrementing integer starting at 1)\n - house size (m^2)\n - house price\n - location\n - number of bedrooms\n\nMake sure that the numbers make sense (i.e. more rooms is usually bigger size, more expensive locations increase price. more size is usually higher price etc. make sure all the numbers make sense). Also only respond with the CSV.\n\"\"\"\n\nresponse = client.chat.completions.create(\n  model=datagen_model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n    {\"role\": \"user\", \"content\": question}\n  ]\n)\nres = response.choices[0].message.content\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Splitting Large Audio Files with PyDub in Python\nDESCRIPTION: This Python example uses the third-party 'pydub' library to load and split an mp3 audio file into a shorter segment, helping to stay within Whisper API's 25 MB file size limit. The script reads an input file, slices it by milliseconds, and exports the chunk as a new mp3 file. Dependencies: 'pydub' Python package and an appropriate audio backend (e.g., ffmpeg). Key parameters: input file path, output file path, and chunk duration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydub import AudioSegment\n\nsong = AudioSegment.from_mp3(\"good_morning.mp3\")\n\n# PyDub handles time in milliseconds\nten_minutes = 10 * 60 * 1000\n\nfirst_10_minutes = song[:ten_minutes]\n\nfirst_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries in SQLite - Python\nDESCRIPTION: This Python snippet defines the `execute_query` function for executing SQL queries against an SQLite database. It establishes a connection to the database, executes the provided SQL query with optional parameters, and returns the results as a list of rows. It also includes error handling for SQLite exceptions and ensures the connection is closed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef execute_query(db_path, query, params=()):\n    \"\"\"\n    Execute a SQL query and return the results.\n\n    Parameters:\n    db_path (str): Path to the SQLite database file.\n    query (str): SQL query to be executed.\n    params (tuple): Parameters to be passed to the query (default is an empty tuple).\n\n    Returns:\n    list: List of rows returned by the query.\n    \"\"\"\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n\n        # Execute the query with parameters\n        cursor.execute(query, params)\n        results = cursor.fetchall()\n\n        # Commit if it's an INSERT/UPDATE/DELETE query\n        if query.strip().upper().startswith(('INSERT', 'UPDATE', 'DELETE')):\n            conn.commit()\n\n        return results\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        return []\n    finally:\n        # Close the connection\n        if conn:\n            conn.close()\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Response Output in Python\nDESCRIPTION: Simple Python code snippet demonstrating how to access the 'output' field from a model's response object after converting it to a dictionary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nresponse.to_dict()[\"output\"]\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI API in Python\nDESCRIPTION: This function generates embeddings for a given text using the OpenAI API. It sends a request to the OpenAI embeddings endpoint and extracts the embedding vector from the response. It also includes a helper function to safely get embeddings by chunking the text if it exceeds the maximum token limit.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef generate_embeddings(text, model):\n    # Generate embeddings for the provided text using the specified model\n    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n    # Extract the embedding data from the response\n    embedding = embeddings_response.data[0].embedding\n    return embedding\n\ndef len_safe_get_embedding(text, model=embeddings_model, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING):\n    # Initialize lists to store embeddings and corresponding text chunks\n    chunk_embeddings = []\n    chunk_texts = []\n    # Iterate over chunks of tokens from the input text\n    for chunk in chunked_tokens(text, chunk_length=max_tokens, encoding_name=encoding_name):\n        # Generate embeddings for each chunk and append to the list\n        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n        # Decode the chunk back to text and append to the list\n        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n    # Return the list of chunk embeddings and the corresponding text chunks\n    return chunk_embeddings, chunk_texts\n```\n\n----------------------------------------\n\nTITLE: Installing Summary Evaluation Packages in Python\nDESCRIPTION: Installs required Python libraries for evaluating text summaries, including `rouge` for ROUGE scores, `bert_score` for BERTScore calculations, and `openai` to interact with the OpenAI API for LLM-based evaluation. This setup is necessary before running the evaluation code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install rouge --quiet\n!pip install bert_score --quiet\n!pip install openai --quiet\n```\n\n----------------------------------------\n\nTITLE: Reviewing High and Low Probability Autocomplete Suggestions in Python\nDESCRIPTION: This pair of ultra-short snippets outputs the dictionaries holding high-confidence and low-confidence autocomplete results, as generated in prior steps. Dependencies: Previous context; the contents of these dictionaries are determined by earlier autocomplete evaluations. Inputs: None (dictionaries are global variables set earlier). Outputs: Content of high_prob_completions and low_prob_completions, aiding debugging or UI testing of suggestion confidence. No additional parameters or constraints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nhigh_prob_completions\n\n```\n\nLANGUAGE: Python\nCODE:\n```\nlow_prob_completions\n\n```\n\n----------------------------------------\n\nTITLE: Creating Hologres Table with Proxima Vector Index using SQL in Python\nDESCRIPTION: Executes a multi-statement SQL script using the `psycopg2` cursor to set up the 'articles' table in Hologres. It first ensures the `proxima` extension is enabled. Then, within a transaction (BEGIN/COMMIT), it drops the table if it exists, creates the table with appropriate columns (id, url, title, content, title_vector, content_vector, vector_id), including checks for vector dimensionality (1536), and finally calls `set_table_property` to define Proxima vector indexes on both vector columns, specifying the graph algorithm and Euclidean distance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION IF NOT EXISTS proxima;\n```\n\nLANGUAGE: sql\nCODE:\n```\nBEGIN;\nDROP TABLE IF EXISTS articles;\nCREATE TABLE articles (\n    id INT PRIMARY KEY NOT NULL,\n    url TEXT,\n    title TEXT,\n    content TEXT,\n    title_vector float4[] check(\n        array_ndims(title_vector) = 1 and \n        array_length(title_vector, 1) = 1536\n    ), -- define the vectors\n    content_vector float4[] check(\n        array_ndims(content_vector) = 1 and \n        array_length(content_vector, 1) = 1536\n    ),\n    vector_id INT\n);\n\n-- Create indexes for the vector fields.\ncall set_table_property(\n    'articles',\n    'proxima_vectors', \n    '{\n        \"title_vector\":{\"algorithm\":\"Graph\",\"distance_method\":\"Euclidean\",\"builder_params\":{\"min_flush_proxima_row_count\" : 10}},\n        \"content_vector\":{\"algorithm\":\"Graph\",\"distance_method\":\"Euclidean\",\"builder_params\":{\"min_flush_proxima_row_count\" : 10}}\n    }'\n);  \n\nCOMMIT;\n```\n\nLANGUAGE: python\nCODE:\n```\ncursor.execute('CREATE EXTENSION IF NOT EXISTS proxima;')\ncreate_proxima_table_sql = '''\nBEGIN;\nDROP TABLE IF EXISTS articles;\nCREATE TABLE articles (\n    id INT PRIMARY KEY NOT NULL,\n    url TEXT,\n    title TEXT,\n    content TEXT,\n    title_vector float4[] check(\n        array_ndims(title_vector) = 1 and \n        array_length(title_vector, 1) = 1536\n    ), -- define the vectors\n    content_vector float4[] check(\n        array_ndims(content_vector) = 1 and \n        array_length(content_vector, 1) = 1536\n    ),\n    vector_id INT\n);\n\n-- Create indexes for the vector fields.\ncall set_table_property(\n    'articles',\n    'proxima_vectors', \n    '{\n        \"title_vector\":{\"algorithm\":\"Graph\",\"distance_method\":\"Euclidean\",\"builder_params\":{\"min_flush_proxima_row_count\" : 10}},\n        \"content_vector\":{\"algorithm\":\"Graph\",\"distance_method\":\"Euclidean\",\"builder_params\":{\"min_flush_proxima_row_count\" : 10}}\n    }'\n);  \n\nCOMMIT;\n'''\n\n# Execute the SQL statements (will autocommit)\ncursor.execute(create_proxima_table_sql)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity Package\nDESCRIPTION: Installs the azure-identity library required for Azure Active Directory authentication with Azure OpenAI.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"azure-identity>=1.15.0\"\n```\n\n----------------------------------------\n\nTITLE: Generating embeddings using a custom embedding utility in Python\nDESCRIPTION: This snippet calls a custom utility function get_embeddings to compute embeddings for the text samples with a specified model. It assumes the function sends a batch query to the '/embeddings' endpoint and returns a matrix of embeddings. Dependencies include the 'utils.embeddings_utils' module and the specified model. Input is a list of text samples, output is an embedding matrix.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom utils.embeddings_utils import get_embeddings\n# NOTE: The following code will send a query of batch size 200 to /embeddings\nmatrix = get_embeddings(samples[\"text\"].to_list(), model=\"text-embedding-3-small\")\n```\n\n----------------------------------------\n\nTITLE: Connect to Zilliz Database\nDESCRIPTION: Establishes a connection to the Zilliz database using the `pymilvus` library with the specified URI and authentication token. This is a prerequisite for performing any operations on the database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom pymilvus import connections, utility, FieldSchema, Collection, CollectionSchema, DataType\n\n# Connect to Zilliz Database\nconnections.connect(uri=URI, token=TOKEN)\n```\n\n----------------------------------------\n\nTITLE: Performing dataset format validation checks\nDESCRIPTION: Validates each dataset sample to ensure it is a dictionary, contains messages list, messages have required keys, no unrecognized keys, roles are valid, content is valid string, and each conversation includes an assistant message. Outputs error counts, aiding debugging and data correctness assurance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nformat_errors = defaultdict(int)\n\nfor ex in dataset:\n    if not isinstance(ex, dict):\n        format_errors[\"data_type\"] += 1\n        continue\n    \n    messages = ex.get(\"messages\", None)\n    if not messages:\n        format_errors[\"missing_messages_list\"] += 1\n        continue\n    \n    for message in messages:\n        if \"role\" not in message or \"content\" not in message:\n            format_errors[\"message_missing_key\"] += 1\n        \n        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n            format_errors[\"message_unrecognized_key\"] += 1\n        \n        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n            format_errors[\"unrecognized_role\"] += 1\n            \n        content = message.get(\"content\", None)\n        function_call = message.get(\"function_call\", None)\n        \n        if (not content and not function_call) or not isinstance(content, str):\n            format_errors[\"missing_content\"] += 1\n    \n    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n        format_errors[\"example_missing_assistant_message\"] += 1\n\nif format_errors:\n    print(\"Found errors:\")\n    for k, v in format_errors.items():\n        print(f\"{k}: {v}\")\nelse:\n    print(\"No errors found\")\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt Template for Pirate Agent\nDESCRIPTION: This code defines a prompt template for an LLM agent that speaks like a pirate. It includes placeholders for tools, user input, and a scratchpad. The template instructs the agent to use a specific format for answering questions, including \"Thought\", \"Action\", \"Action Input\", and \"Observation\" steps, and to conclude with a \"Final Answer\" spoken in a pirate-like manner.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntemplate = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \\\"Arg\\\"s\n\nQuestion: {input}\n{agent_scratchpad}\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Ranking Strings by Relatedness\nDESCRIPTION: This code defines a function `strings_ranked_by_relatedness` that ranks a list of strings based on their relatedness to a given query. It uses cosine similarity to calculate the relatedness between the query embedding and the embeddings of the strings in the provided DataFrame. It depends on the `embedding_request` function to generate embeddings and the `scipy.spatial` module for cosine distance calculation. The function returns a list of filepaths, sorted by relatedness to the query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef strings_ranked_by_relatedness(\n    query: str,\n    df: pd.DataFrame,\n    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n    top_n: int = 100,\n) -> list[str]:\n    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n    query_embedding_response = embedding_request(query)\n    query_embedding = query_embedding_response.data[0].embedding\n    strings_and_relatednesses = [\n        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n        for i, row in df.iterrows()\n    ]\n    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n    strings, relatednesses = zip(*strings_and_relatednesses)\n    return strings[:top_n]\n```\n\n----------------------------------------\n\nTITLE: Calling Images Generation Endpoint - OpenAI API - Shell\nDESCRIPTION: This shell-based curl command posts a request to the OpenAI 'images/generations' endpoint to generate images via the API. Required parameters include 'prompt' (the textual description), 'n' (number of images), and 'size' (image resolution), passed in JSON within the '-d' option. Dependencies include setting OPENAI_API_KEY and having curl installed. Output is a JSON containing URLs to the generated images matching the prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://api.openai.com/v1/images/generations \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"A cute baby sea otter\",\n    \"n\": 2,\n    \"size\": \"1024x1024\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Connection and Creating Index for Embedding Storage in Python\nDESCRIPTION: Sets up the Pinecone vector database client using an API key and environment region, checks if the specified index exists, and creates it if missing. The index is configured with the dimension matching embedding size and cosine similarity metric. Metadata fields 'channel_id' and 'published' are indexed for filtering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pinecone\n\nindex_name = 'openai-youtube-transcriptions'\n\n# initialize connection to pinecone (get API key at app.pinecone.io)\npinecone.init(\n    api_key=\"PINECONE_API_KEY\",\n    environment=\"us-east1-gcp\"  # may be different, check at app.pinecone.io\n)\n\n# check if index already exists (it shouldn't if this is first time)\nif index_name not in pinecone.list_indexes():\n    # if does not exist, create index\n    pinecone.create_index(\n        index_name,\n        dimension=len(res['data'][0]['embedding']),\n        metric='cosine',\n        metadata_config={'indexed': ['channel_id', 'published']}\n    )\n# connect to index\nindex = pinecone.Index(index_name)\n# view index stats\nindex.describe_index_stats()\n```\n\n----------------------------------------\n\nTITLE: Processing and analyzing employment data using Python\nDESCRIPTION: This Python snippet reads employment data from a CSV file, calculates key statistics such as total jobs gained, and outputs summarized results. It relies on pandas and numpy libraries to handle data processing tasks efficiently. The code is useful for automating analysis of labor market datasets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/assets/007.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndef analyze_employment_data(file_path):\n    data = pd.read_csv(file_path)\n    total_jobs = data['Jobs'].sum()\n    mean_jobs = data['Jobs'].mean()\n    print(f\"Total jobs added: {total_jobs}\")\n    print(f\"Average jobs per month: {mean_jobs}\")\n\n# Usage\n# analyze_employment_data('employment_data.csv')\n```\n\n----------------------------------------\n\nTITLE: Uploading Base64 Image with OpenAI Vision (Python)\nDESCRIPTION: Provides a Python script to read a local image file, encode it to base64 format, and include it as a data URL in the payload for the OpenAI chat completions API. This allows sending local images for analysis. Requires `base64`, `requests`, and an OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport requests\n\n# OpenAI API Key\napi_key = \"YOUR_OPENAI_API_KEY\"\n\n# Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Path to your image\nimage_path = \"path_to_your_image.jpg\"\n\n# Getting the base64 string\nbase64_image = encode_image(image_path)\n\nheaders = {\n  \"Content-Type\": \"application/json\",\n  \"Authorization\": f\"Bearer {api_key}\"\n}\n\npayload = {\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Whatâ€™s in this image?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n          }\n        }\n      ]\n    }\n  ],\n  \"max_tokens\": 300\n}\n\nresponse = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Importing Core Python Libraries\nDESCRIPTION: Imports essential Python libraries (`os`, `json`, `pandas`) required for operating system interactions (like environment variables), JSON data handling, and data manipulation using DataFrames.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json \nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Clustering Transactions Using KMeans with scikit-learn in Python\nDESCRIPTION: This snippet performs clustering on the embeddings using KMeans with 5 clusters. It assigns each transaction to a cluster label and adds this information to the DataFrame for subsequent analysis and visualization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nn_clusters = 5\n\nkmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42, n_init=10)\nkmeans.fit(matrix)\nlabels = kmeans.labels_\nembedding_df[\"Cluster\"] = labels\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Inner Monologue for Tutoring\nDESCRIPTION: This example demonstrates the use of inner monologue to hide the model's reasoning process from the user in a tutoring application.  The model's steps are enclosed within triple quotes, allowing parsing and selective display of output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_14\n\nLANGUAGE: N/A\nCODE:\n```\nSYSTEM: Follow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\")\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\")\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\")\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\n\nUSER: Problem Statement: \n\nStudent Solution: \n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Audio Processing and OpenAI Interaction (Python)\nDESCRIPTION: Imports necessary Python libraries: `OpenAI` for interacting with the OpenAI API, `os` for file system operations, `urllib` for downloading files, `IPython.display.Audio` for playing audio in Jupyter environments, `pathlib.Path` for object-oriented filesystem paths, `pydub.AudioSegment` for audio manipulation, and `ssl` for configuring SSL contexts, often needed for HTTPS requests like downloading files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nimport urllib\nfrom IPython.display import Audio\nfrom pathlib import Path\nfrom pydub import AudioSegment\nimport ssl\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI and defining model names\nDESCRIPTION: This code imports the OpenAI library and defines constants for the embedding and GPT models to be used.  It sets up the necessary imports and configurations to interact with the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nGPT_MODEL = \"gpt-3.5-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Text Using Bytes Parameter in Python\nDESCRIPTION: This snippet shows how to use the bytes parameter from logprobs to reconstruct the complete text response. It also calculates and displays token-level probabilities and validates that the reconstructed text matches the original response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nPROMPT = \"\"\"Output the blue heart emoji and its name.\"\"\"\nAPI_RESPONSE = get_completion(\n    [{\"role\": \"user\", \"content\": PROMPT}], model=\"gpt-4o\", logprobs=True\n)\n\naggregated_bytes = []\njoint_logprob = 0.0\n\n# Iterate over tokens, aggregate bytes and calculate joint logprob\nfor token in API_RESPONSE.choices[0].logprobs.content:\n    print(\"Token:\", token.token)\n    print(\"Log prob:\", token.logprob)\n    print(\"Linear prob:\", np.round(exp(token.logprob) * 100, 2), \"%\")\n    print(\"Bytes:\", token.bytes, \"\\n\")\n    aggregated_bytes += token.bytes\n    joint_logprob += token.logprob\n\n# Decode the aggregated bytes to text\naggregated_text = bytes(aggregated_bytes).decode(\"utf-8\")\n\n# Assert that the decoded text is the same as the message content\nassert API_RESPONSE.choices[0].message.content == aggregated_text\n\n# Print the results\nprint(\"Bytes array:\", aggregated_bytes)\nprint(f\"Decoded bytes: {aggregated_text}\")\nprint(\"Joint prob:\", np.round(exp(joint_logprob) * 100, 2), \"%\")\n```\n\n----------------------------------------\n\nTITLE: Printing the Punctuated Transcript (Python)\nDESCRIPTION: Outputs the content of the `punctuated_transcript` variable to the console. This displays the transcript after the AI-driven punctuation and formatting step has been applied.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(punctuated_transcript)\n```\n\n----------------------------------------\n\nTITLE: Cleaning JSON Output\nDESCRIPTION: This function cleans the raw text output that is generated by GPT-4o to be a valid JSON object by removing surrounding \"```json\" and \"```\" characters.  It converts the string output to a JSON object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef remove_sequences(input_string):\n    # Replace the specific sequences with an empty string\n    cleaned_string = input_string.replace(\"```json\", \"\")  # Remove \"```json\" first\n    cleaned_string = cleaned_string.replace(\"```\", \"\")  # Then remove \"```\"\n    return json.loads(cleaned_string)\n```\n\n----------------------------------------\n\nTITLE: Create and Stream Run with Assistants API in Node.js\nDESCRIPTION: This snippet demonstrates using the OpenAI Node.js SDK's `stream` helper to create an Assistants API run with streaming enabled. It attaches various event listeners (`on('textCreated')`, `on('textDelta')`, `on('toolCallCreated')`, `on('toolCallDelta')`) directly to the stream object to handle incoming data and print it to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/overview-with-streaming.txt#_snippet_1\n\nLANGUAGE: node.js\nCODE:\n```\n// We use the stream SDK helper to create a run with\n// streaming. The SDK provides helpful event listeners to handle \n// the streamed response.\n \nconst run = openai.beta.threads.runs.stream(thread.id, {\n    assistant_id: assistant.id\n  })\n    .on('textCreated', (text) => process.stdout.write('\\nassistant > '))\n    .on('textDelta', (textDelta, snapshot) => process.stdout.write(textDelta.value))\n    .on('toolCallCreated', (toolCall) => process.stdout.write(`\\nassistant > ${toolCall.type}\\n\\n`))\n    .on('toolCallDelta', (toolCallDelta, snapshot) => {\n      if (toolCallDelta.type === 'code_interpreter') {\n        if (toolCallDelta.code_interpreter.input) {\n          process.stdout.write(toolCallDelta.code_interpreter.input);\n        }\n        if (toolCallDelta.code_interpreter.outputs) {\n          process.stdout.write(\"\\noutput >\\n\");\n          toolCallDelta.code_interpreter.outputs.forEach(output => {\n            if (output.type === \"logs\") {\n              process.stdout.write(`\\n${output.logs}\\n`);\n            } Specimens});\n        }\n      }\n    });\n\n```\n\n----------------------------------------\n\nTITLE: Assistant Reasoning JSON Structure Analysis (JSON)\nDESCRIPTION: Illustrates the JSON structure produced by the assistant reasoning prompt, highlighting fields that are potentially suitable for generation by a faster, fine-tuned model (like GPT-3.5) versus those benefiting from a more powerful model (like GPT-4, for the final 'response' field).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n\"message_is_conversation_continuation\": \"True\", // <-\n\"number_of_messages_in_conversation_so_far\": \"1\", // <-\n\"user_sentiment\": \"Aggravated\", // <-\n\"query_type\": \"Hardware Issue\", // <-\n\"response_tone\": \"Validating and solution-oriented\", // <-\n\"response_requirements\": \"Propose options for repair or replacement.\", // <-\n\"user_requesting_to_talk_to_human\": \"False\", // <-\n\"enough_information_in_context\": \"True\" // <-\n\"response\": \"...\" // X -- benefits from GPT-4\n}\n```\n\n----------------------------------------\n\nTITLE: Searching functions using cosine similarity\nDESCRIPTION: This code defines a `search_functions` function that searches for functions similar to a given query. It embeds the query using `text-embedding-3-small` and calculates the cosine similarity between the query embedding and the embeddings of the functions in the dataframe.  The function then sorts the functions by similarity and returns the top `n` results. Dependencies include cosine_similarity function from the utils module.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import cosine_similarity\n\ndef search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n    embedding = get_embedding(code_query, model='text-embedding-3-small')\n    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = df.sort_values('similarities', ascending=False).head(n)\n\n    if pprint:\n        for r in res.iterrows():\n            print(f\"{r[1].filepath}:{r[1].function_name}  score={round(r[1].similarities, 3)}\")\n            print(\"\\n\".join(r[1].code.split(\"\\n\")[:n_lines]))\n            print('-' * 70)\n\n    return res\n```\n\n----------------------------------------\n\nTITLE: Serial OpenAI Chat Completions for Multiple Stories in Python\nDESCRIPTION: Demonstrates a loop that generates multiple story completions by making a separate OpenAI chat completion request per iteration. Each response is printed after extracting the message content. Requires the OpenAI Python client; num_stories defines the repetitions, while model, messages, and max_tokens must be provided. May be limited by per-minute request caps; not batch-optimized.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnum_stories = 10\ncontent = \"Once upon a time,\"\n\n# serial example, with one story completion per request\nfor _ in range(num_stories):\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": content}],\n        max_tokens=20,\n    )\n\n    print(content + response.choices[0].message.content)\n\n```\n\n----------------------------------------\n\nTITLE: Creating a fine-tuning job with GPT-4o mini\nDESCRIPTION: Initiates the fine-tuning job using the uploaded files, specifying the base model and a custom suffix for the resulting model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"gpt-4o-mini-2024-07-18\"\n\nresponse = client.fine_tuning.jobs.create(\n    training_file=training_file_id,\n    validation_file=validation_file_id,\n    model=MODEL,\n    suffix=\"recipe-ner\",\n)\n\njob_id = response.id\n\nprint(\"Job ID:\", response.id)\nprint(\"Status:\", response.status)\n```\n\n----------------------------------------\n\nTITLE: Generate User Message with Input Text for NER Processing\nDESCRIPTION: Forms the user prompt including the specific text containing entities to be recognized, used to instruct the model to extract and label entity mentions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef user_message(text):\n    return f\"\"\"\nTASK:\n    Text: {text}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Resizing Images with PIL before Using OpenAI's API in Python\nDESCRIPTION: Shows how to use the PIL library to resize an image before submitting it to OpenAI's API for variation creation. This example reads an image from disk, resizes it to 256x256 pixels, and converts it to a BytesIO object for API submission.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-python-tips.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nfrom PIL import Image\nfrom openai import OpenAI\nclient = OpenAI()\n\n# Read the image file from disk and resize it\nimage = Image.open(\"image.png\")\nwidth, height = 256, 256\nimage = image.resize((width, height))\n\n# Convert the image to a BytesIO object\nbyte_stream = BytesIO()\nimage.save(byte_stream, format='PNG')\nbyte_array = byte_stream.getvalue()\n\nresponse = client.images.create_variation(\n  image=byte_array,\n  n=1,\n  model=\"dall-e-2\",\n  size=\"1024x1024\"\n)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Milvus Database (Python)\nDESCRIPTION: Uses the `pymilvus.connections.connect` function to establish a connection to the Milvus server using the specified `HOST` and `PORT`. This connection is necessary for all subsequent operations interacting with Milvus.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pymilvus import connections, utility, FieldSchema, Collection, CollectionSchema, DataType\n\n# Connect to Milvus Database\nconnections.connect(host=HOST, port=PORT)\n```\n\n----------------------------------------\n\nTITLE: Transcribing with Expanded Prompt\nDESCRIPTION: This code snippet expands the prompt to include a more comprehensive list of product names and acronyms. This is again, passed to the `transcribe` function to demonstrate the influence on transcription accuracy with an extended list. The goal is to improve the accuracy of transcribing the company's product names.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# add a full product list to the prompt\ntranscribe(\n    prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, AstroPixel Array, QuantumFlare Five, CyberPulse Six, VortexDrive Matrix, PhotonLink Ten, TriCircuit Array, PentaSync Seven, UltraWave Eight, QuantumVertex Nine, HyperHelix X, DigiSpiral Z, PentaQuark Eleven, TetraCube Twelve, GigaPhase Thirteen, EchoNeuron Fourteen, FusionPulse V15, MetaQuark Sixteen, InfiniCircuit Seventeen, TeraPulse Eighteen, ExoMatrix Nineteen, OrbiSync Twenty, QuantumHelix TwentyOne, NanoPhase TwentyTwo, TeraFractal TwentyThree, PentaHelix TwentyFour, ExoCircuit TwentyFive, HyperQuark TwentySix, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\",\n    audio_filepath=ZyntriQix_filepath,\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Content Filter Results in Azure OpenAI Responses\nDESCRIPTION: Shows how to check content filter results in successful Azure OpenAI completions. Extracts and displays both prompt and completion filter results from the response, including filtered status and severity for each category.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the biggest city in Washington?\"}\n]\n\ncompletion = client.chat.completions.create(\n    messages=messages,\n    model=deployment,\n)\nprint(f\"Answer: {completion.choices[0].message.content}\")\n\n# prompt content filter result in \"model_extra\" for azure\nprompt_filter_result = completion.model_extra[\"prompt_filter_results\"][0][\"content_filter_results\"]\nprint(\"\\nPrompt content filter results:\")\nfor category, details in prompt_filter_result.items():\n    print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")\n\n# completion content filter result\nprint(\"\\nCompletion content filter results:\")\ncompletion_filter_result = completion.choices[0].model_extra[\"content_filter_results\"]\nfor category, details in completion_filter_result.items():\n    print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")\n```\n\n----------------------------------------\n\nTITLE: Plot Embeddings in 2D - Python\nDESCRIPTION: This snippet plots the reduced embeddings in a 2D scatter plot, coloring each point based on its star rating. It uses matplotlib to create the plot, with colors ranging from red to green representing lower to higher ratings. The average position for each rating is also marked with an 'x'. Requires matplotlib and numpy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_2D.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nfor score in [0,1,2,3,4]:\n    avg_x = np.array(x)[df.Score-1==score].mean()\n    avg_y = np.array(y)[df.Score-1==score].mean()\n    color = colors[score]\n    plt.scatter(avg_x, avg_y, marker='x', color=color, s=100)\n\nplt.title(\"Amazon ratings visualized in language using t-SNE\")\n```\n\n----------------------------------------\n\nTITLE: Initial Assistant Reasoning Prompt (example-chat)\nDESCRIPTION: An example LLM prompt for a customer service bot that uses retrieved context and the user query to reason through predefined steps (like sentiment, query type, etc.) and generate a final response. The reasoning steps and final response are structured within a JSON object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_2\n\nLANGUAGE: example-chat\nCODE:\n```\nSYSTEM: You are a helpful customer service bot.\n\nUse the result JSON to reason about each user query - use the retrieved context.\n\n# Example\n\nUser: \"My computer screen is cracked! I want it fixed now!!!\"\n\nAssistant Response:\n{\n\"message_is_conversation_continuation\": \"True\",\n\"number_of_messages_in_conversation_so_far\": \"1\",\n\"user_sentiment\": \"Aggravated\",\n\"query_type\": \"Hardware Issue\",\n\"response_tone\": \"Validating and solution-oriented\",\n\"response_requirements\": \"Propose options for repair or replacement.\",\n\"user_requesting_to_talk_to_human\": \"False\",\n\"enough_information_in_context\": \"True\"\n\"response\": \"...\"\n}\n\nUSER: # Relevant Information\n` ` `\n[retrieved context]\n` ` `\n\nUSER: [input user query here]\n```\n\n----------------------------------------\n\nTITLE: Create Eval Runs for Prompt Versions with different Model - Python\nDESCRIPTION: This code creates eval runs for both 'v1' and 'v2' prompt versions with gpt-4o model and existing completion data. Since it targets the gpt-4o model, the data source configuration references the 'item.input' messages for input and specifies the model 'gpt-4o'. This will generate new completions using the existing input messages.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```python\ntasks = []\nfor prompt_version in [\"v1\", \"v2\"]:\n    tasks.append(client.evals.runs.create(\n        eval_id=eval_id,\n        name=f\"post-fix-new-model-run-{prompt_version}\",\n        data_source={\n            \"type\": \"completions\",\n            \"input_messages\": {\n                \"type\": \"item_reference\",\n                \"item_reference\": \"item.input\",\n            },\n            \"model\": \"gpt-4o\",\n            \"source\": {\n                \"type\": \"stored_completions\",\n                \"metadata\": {\n                    \"prompt_version\": prompt_version,\n                }\n            }\n        },\n    ))\nresult = await asyncio.gather(*tasks)\nfor run in result:\n    print(run.report_url)\n```\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Translation Function (Python)\nDESCRIPTION: Defines a function `translate_chunk` that sends a single text chunk to the OpenAI API for translation. It constructs a prompt designed to instruct the model to translate only the text content while leaving LaTeX commands unchanged, using a sample translation as a guide. Includes a demonstration call.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef translate_chunk(chunk, model='gpt-4o',\n                    dest_language='English',\n                    sample_translation=(\n                    r\"\\poglavje{Osnove Geometrije} \\label{osn9Geom}\",\n                    r\"\\chapter{The basics of Geometry} \\label{osn9Geom}\")):\n    prompt = f'''Translate only the text from the following LaTeX document into {dest_language}. Leave all LaTeX commands unchanged\n    \n\"\"\"\n{sample_translation[0]}\n{chunk}\"\"\"\n\n{sample_translation[1]}\n'''\n    response = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\":prompt}],\n        model=model,\n        temperature=0,\n        top_p=1,\n        max_tokens=15000,\n    )\n    result = response.choices[0].message.content.strip()\n    result = result.replace('\"\"\"', '') # remove the double quotes, as we used them to surround the text\n    return result\nprint(translate_chunk(chunks[2], model='gpt-4o', dest_language='English'))\n```\n\n----------------------------------------\n\nTITLE: Displaying Wikipedia Section Examples in Python\nDESCRIPTION: Prints the first 5 Wikipedia sections with truncated content for preview purposes. Each section displays the title followed by the first 77 characters of the content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# print example data\nfor ws in wikipedia_sections[:5]:\n    print(ws[0])\n    display(ws[1][:77] + \"...\")\n    print()\n```\n\n----------------------------------------\n\nTITLE: Instantiating Pinecone Document Retriever with OpenAI Embeddings in Python\nDESCRIPTION: Initializes an OpenAIEmbeddings object and loads an existing Pinecone index using the Pinecone.from_existing_index method for use as a document retriever. Prerequisites include an initialized Pinecone index with proper embedding dimensions and stored vectors. The output is a docsearch object suitable for retrieval tasks based on query embedding similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Configuring the embeddings to be used by our retriever to be OpenAI Embeddings, matching our embedded corpus\nembeddings = OpenAIEmbeddings()\n\n\n# Loads a docsearch object from an existing Pinecone index so we can retrieve from it\ndocsearch = Pinecone.from_existing_index(index_name,embeddings,text_key='text_chunk')\n\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container in Restricted Mode\nDESCRIPTION: Sets up a secure Docker container named 'sandbox' with restricted networking, capabilities, and resources. The container runs in the background in a detached mode.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/object_oriented_agentic_approach/Secure_code_interpreter_tool_for_LLM_agents.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --name sandbox --network none --cap-drop all --pids-limit 64 --tmpfs /tmp:rw,size=64M   python_sandbox:latest sleep infinity\n```\n\n----------------------------------------\n\nTITLE: Output Structure for Combined Query/Retrieval Prompt (JSON)\nDESCRIPTION: Defines the JSON output structure for the combined prompt that performs both query contextualization and retrieval determination. It includes fields for the re-written query and a boolean indicating if retrieval is required.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/latency-optimization.txt#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\nquery:\"[contextualized query]\",\nretrieval:\"[true/false - whether retrieval is required]\"\n}\n```\n\n----------------------------------------\n\nTITLE: Flagging Pages with Visual Content\nDESCRIPTION: This code adds a new column `Visual_Input_Processed` to a Pandas DataFrame (`df`) to flag pages that contain visual elements like images, charts, or tables. It uses the `.apply()` method with a lambda function to check if certain keywords (e.g., `DESCRIPTION OF THE IMAGE OR CHART`, or `TRANSCRIPTION OF THE TABLE`) are present within the `PageText` column. The flag is set to 'Y' if these keywords are found, indicating that the page contains visual elements, and 'N' otherwise. This helps in identifying which pages require special handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Add a column to flag pages with visual content\ndf['Visual_Input_Processed'] = df['PageText'].apply(\n    lambda x: 'Y' if 'DESCRIPTION OF THE IMAGE OR CHART' in x or 'TRANSCRIPTION OF THE TABLE' in x else 'N'\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Installs the necessary Python libraries (`pymongo` for MongoDB interaction and `openai` for embedding generation) using pip. These libraries are required to connect to MongoDB Atlas and interact with the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install pymongo openai\n```\n\n----------------------------------------\n\nTITLE: Defining Language Configurations for Translation in JavaScript\nDESCRIPTION: Defines an array of language configuration objects in JavaScript. Each object contains a language code (e.g., 'fr') and corresponding instructions (prompts) imported from a separate configuration file, used to set up unique Realtime API sessions for each target translation language.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Define language codes and import their corresponding instructions from our prompt config file\nconst languageConfigs = [\n  { code: 'fr', instructions: french_instructions },\n  { code: 'es', instructions: spanish_instructions },\n  { code: 'tl', instructions: tagalog_instructions },\n  { code: 'en', instructions: english_instructions },\n  { code: 'zh', instructions: mandarin_instructions },\n];\n```\n\n----------------------------------------\n\nTITLE: Testing OpenAI Embedding API Call and Retrieving Embedding - Python\nDESCRIPTION: This snippet initializes the OpenAI client using the user's API key and requests embedding vectors for two example input sentences using the 'text-embedding-3-small' model. It provides a demonstration of how to perform single or batch embedding requests via the 'client.embeddings.create' method, returning a result with embedding data for each input. This serves as both a sanity check and a guide for invoking the embedding endpoint.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI(api_key=OPENAI_API_KEY)\nembedding_model_name = \"text-embedding-3-small\"\n\nresult = client.embeddings.create(\n    input=[\n        \"This is a sentence\",\n        \"A second sentence\"\n    ],\n    model=embedding_model_name,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Prompting GPT Model to Generate Function Arguments with Chat Completion in Python\nDESCRIPTION: Illustrates building a messages list with system instructions and user queries, then sending it to the Chat Completions API via `chat_completion_request` to elicit clarifying questions or function argument generation by the model. The snippet shows how the assistant responds when asked about current weather without sufficient information, and how subsequent user messages provide required data for the model to complete function arguments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\nmessages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\nchat_response = chat_completion_request(\n    messages, tools=tools\n)\nassistant_message = chat_response.choices[0].message\nmessages.append(assistant_message)\nassistant_message\n```\n\n----------------------------------------\n\nTITLE: Extracting Information from Text Chunks\nDESCRIPTION: This code initializes a tokenizer, splits the cleaned text into chunks, and extracts information from each chunk using the `extract_chunk` function.  It prints the extracted results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialise tokenizer\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\nresults = []\n    \nchunks = create_chunks(clean_text,1000,tokenizer)\ntext_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n\nfor chunk in text_chunks:\n    results.append(extract_chunk(chunk,template_prompt))\n    #print(chunk)\n    print(results[-1])\n```\n\n----------------------------------------\n\nTITLE: Directly Prompting GPT-3.5 for Clue Deduction (Incorrect Result)\nDESCRIPTION: This snippet shows a direct prompt to `gpt-3.5-turbo-instruct` asking it to solve a multiple-choice question based on provided clues from a Clue game. The model fails to combine the relevant clues (3 and 5) and provides an incorrect answer (c) Unknown, demonstrating limitations in direct complex reasoning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_2\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nUse the following clues to answer the following multiple-choice question.\n\nClues:\n1. Miss Scarlett was the only person in the lounge.\n2. The person with the pipe was in the kitchen.\n3. Colonel Mustard was the only person in the observatory.\n4. Professor Plum was not in the library nor the billiard room.\n5. The person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n```\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n```\n\n----------------------------------------\n\nTITLE: Creating Zero-Shot Prompt Messages for GPT-3.5-Turbo in Python\nDESCRIPTION: Defines a function that constructs a chat prompt formatted as a list of messages for the OpenAI chat completion API. The prompt instructs the model to answer questions based strictly on provided context and to respond with 'I don't know' if the answer is not in the context. This structure enables zero-shot question answering with clear instructions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Function to get prompt messages\ndef get_prompt(row):\n    return [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n    Question: {row.question}\\n\\n\n    Context: {row.context}\\n\\n\n    Answer:\\n\"\"\",\n        },\n    ]\n```\n\n----------------------------------------\n\nTITLE: OpenAI Image Variation with In-Memory Data and TypeScript Casting\nDESCRIPTION: This code snippet showcases how to use in-memory image data with the OpenAI image variation API in a TypeScript environment, addressing type-related issues. It explicitly casts the `Buffer` object to `any` to enable setting the `name` property, which is crucial for the API to recognize the image type. It initializes the OpenAI client and sets the size of the image variation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-node-tips.txt#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\n\nasync function main() {\n  const image = await openai.images.createVariation({\n    file,\n    1,\n    \"1024x1024\"\n  });\n  console.log(image.data);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Setting OPENAI_API_KEY Environment Variable - Bash/Zsh\nDESCRIPTION: This Bash/Zsh export command sets the 'OPENAI_API_KEY' environment variable within the user's shell profile, allowing secure credential storage for OpenAI API calls. Prerequisites include access to the bash or zsh shell and knowledge of location of shell profile files. The 'your-api-key-here' value should be replaced by your actual API key. Once set and sourced, this variable is accessed through code and terminal sessions, facilitating automated authentication in API requests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Converting PDF Page to Image\nDESCRIPTION: This function converts a single page of a PDF document (provided as a byte string) into a PNG image. It uses the `convert_from_bytes` function to perform the conversion and saves the resulting image to an `images` directory, with the filename derived from the page number. The function returns the relative path to the saved image.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef convert_page_to_image(pdf_bytes, page_number):\n    # Convert the PDF page to an image\n    images = convert_from_bytes(pdf_bytes)\n    image = images[0]  # There should be only one page\n\n    # Define the directory to save images (relative to your script)\n    images_dir = 'images'  # Use relative path here\n\n    # Ensure the directory exists\n    os.makedirs(images_dir, exist_ok=True)\n\n    # Save the image to the images directory\n    image_file_name = f\"page_{page_number}.png\"\n    image_file_path = os.path.join(images_dir, image_file_name)\n    image.save(image_file_path, 'PNG')\n\n    # Return the relative image path\n    return image_file_path\n```\n\n----------------------------------------\n\nTITLE: Visualizing Hyperparameter Search Results with Plotly in Python\nDESCRIPTION: Concatenates the list of result DataFrames from the hyperparameter search into a single `runs_df`. It then uses `plotly.express` (`px`) to generate line plots visualizing the training and test loss, and training and test accuracy over epochs, faceted by `learning_rate` and `batch_size` to compare different hyperparameter runs. Depends on `pandas` and `plotly.express`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nruns_df = pd.concat(results)\n\n# plot training loss and test loss over time\npx.line(\n    runs_df,\n    line_group=\"run_id\",\n    x=\"epoch\",\n    y=\"loss\",\n    color=\"type\",\n    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n    facet_row=\"learning_rate\",\n    facet_col=\"batch_size\",\n    width=500,\n).show()\n\n# plot accuracy over time\npx.line(\n    runs_df,\n    line_group=\"run_id\",\n    x=\"epoch\",\n    y=\"accuracy\",\n    color=\"type\",\n    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n    facet_row=\"learning_rate\",\n    facet_col=\"batch_size\",\n    width=500,\n).show()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Top Document After Reranking in Python\nDESCRIPTION: Retrieves and displays the content of the most relevant document after reranking, providing a way to examine the highest-ranked result in detail.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Inspect our new top document following reranking\nreranked_df[\"document\"][0]\n```\n\n----------------------------------------\n\nTITLE: Defining Few-Shot Examples\nDESCRIPTION: This code defines three few-shot examples, `fs_user_1`, `fs_user_2`, and `fs_user_3`, that include a knowledge base article, chat history, and an assistant message. These examples demonstrate to the guardrail system how to assess the accuracy, relevance, and policy compliance of assistant responses. The `few_shot_examples` list consolidates these examples for use in the guardrail system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfs_user_1 = \"\"\"\n\nHere are some example conversation and a knowledge base to assess how well you are performing your job:\n\nKnowledge Base Article:\n    - We offer a 30-day return policy for all unused items\n    - To be eligible for a return, items must be in their original packaging and condition.\n    - Customers must provide a valid order number to initiate a return\n    - Once we receive the returned item, please allow 5-7 business days for the refund to be processed\n    - You can submit a ticket here: www.helponline.com\n\nChat History:\n    User: \"Hello, I would like to return my order\"\n\nAssistant Message:\n    \"Okay, I can certainly help with that. Was the order placed within 30 days and is unused?\"\n\n\"\"\"\n\nfs_user_2 = \"\"\"\n\nKnowledge Base Article:\n    - If the item is not damaged, notify the customer that this does not meet our requirements for return and they are not eligible for a refund\n    - Skip step 3 and go straight to step 4\n\n2b. **If return category is either 'satisfaction' or 'unnecessary'**\n    - Ask the customer if they can provide feedback on the quality of the item\n    - If the order was made within 30 days, notify them that they are eligible for a full refund\n    - If the order was made within 31-60 days, notify them that they are eligible for a partial refund of 50%\n    - If the order was made greater than 60 days ago, notify them that they are not eligible for a refund\n\n3. **If the customer is eligible for a return or refund**\n    - Ask the customer to confirm that they would like a return or refund\n    - Once they confirm, process their request\n\n4 **Provide additional support before closing out ticket**\n    - Ask the customer if there is anything else you can do to help them today.\n\nChat History:\n    user: I would like to return this shirt\n    assistant: Hi there, I'm happy to help with processing this return. Can you please provide an explanation for why you'd like to return this shirt?\n    user: Yes, I am not satisfied with the design\n\nAssistant Message:\n    I see. Because the shirt was ordered in the last 60 days, we cannot process a refund.\n\"\"\"\n\nfs_user_3 = \"\"\"\nKnowledge Base Article:\n    - If you are having problems logging in, please enter in your email in the password recovery page\n\nChat History:\n    User: I can't log in\n\nAssistant Message:\n    Have you tried resetting your password?\n\"\"\"\n\nfew_shot_examples = [fs_user_1, fs_user_2, fs_user_3]\n```\n\n----------------------------------------\n\nTITLE: OpenAPI Definition Example: Retrieve PDFs\nDESCRIPTION: An OpenAPI example is presented, including the `operationId`, `summary`, `description`, parameters and responses, illustrating how to retrieve PDFs of academic papers. The `openaiFileResponse` array is used to specify the file locations. The example defines a GET request to a papers endpoint which takes a topic as a query parameter and returns a list of URLs to PDFs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\n  get:\n    operationId: findPapers\n    summary: Retrieve PDFs of relevant academic papers.\n    description: Provided an academic topic, up to five relevant papers will be returned as PDFs.\n    parameters:\n      - in: query\n        name: topic\n        required: true\n        schema:\n          type: string\n        description: The topic the papers should be about.\n    responses:\n      '200':\n        description: Zero to five academic paper PDFs\n        content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  openaiFileResponse:\n                    type: array\n                    items:\n                    type: string\n                    format: uri\n                    description: URLs to fetch the files.\n\n```\n\n----------------------------------------\n\nTITLE: Rejecting irrelevant tasks\nDESCRIPTION: This snippet demonstrates how the model is expected to reject irrelevant tasks outside the scope of S3 bucket operations. It calls `run_conversation` with a request about the weather, which the model should refuse to answer.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# the model should not answer details not related to the scope\nprint(run_conversation('what is the weather today'))\n```\n\n----------------------------------------\n\nTITLE: Querying ChatGPT with context and printing the answer\nDESCRIPTION: This code snippet uses the ask function to query ChatGPT with the question \"Who won the gold medal for curling in Olymics 2022?\" The ask function uses the data in SingleStoreDB to provide context to ChatGPT.  The pprint function from the pprint module is then used to print the answer in a more readable format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint\n\nanswer = ask('Who won the gold medal for curling in Olymics 2022?')\n\npprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Printing Validation Metrics - Python\nDESCRIPTION: Prints the calculated precision, recall, and F1 score to the console, formatted to two decimal places.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1: {f1:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Create an Eval with Data Source and Testing Criteria - Python\nDESCRIPTION: This snippet creates an evaluation using the `client.evals.create` method. It sets the name, metadata, data source configuration, and testing criteria for the eval. The result contains the eval ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\neval_create_result = await client.evals.create(\n    name=\"Push Notification Completion Monitoring\",\n    metadata={\"description\": \"This eval monitors completions\"},\n    data_source_config=data_source_config,\n    testing_criteria=[push_notification_grader],\n)\n\neval_id = eval_create_result.id\n```\n```\n\n----------------------------------------\n\nTITLE: Saving Kangas DataGrid to File in Python\nDESCRIPTION: Saves the DataGrid with embeddings to a file for later use or sharing, using the default save method.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndg.save()\n```\n\n----------------------------------------\n\nTITLE: Loading Embeddings CSV Data into a Pandas DataFrame in Python\nDESCRIPTION: Imports the `pandas` library and reads the contents of the `vector_database_wikipedia_articles_embedded.csv` file, located in the parent `data` directory, into a pandas DataFrame named `data`. The last line implicitly displays the DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas, json\ndata = pandas.read_csv('../../data/vector_database_wikipedia_articles_embedded.csv')\ndata\n```\n\n----------------------------------------\n\nTITLE: Parsing Patch Text into Patch Object in Python\nDESCRIPTION: Parses a string containing patch text into a `Patch` object. It requires the text to start with '*** Begin Patch' and end with '*** End Patch'. It uses a `Parser` class internally to process the lines and validate changes against original file contents (`orig`). Returns the parsed `Patch` object and a cumulative fuzziness score from context matching.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef text_to_patch(text: str, orig: Dict[str, str]) -> Tuple[Patch, int]:\n    lines = text.splitlines()  # preserves blank lines, no strip()\n    if (\n        len(lines) < 2\n        or not Parser._norm(lines[0]).startswith(\"*** Begin Patch\")\n        or Parser._norm(lines[-1]) != \"*** End Patch\"\n    ):\n        raise DiffError(\"Invalid patch text - missing sentinels\")\n\n    parser = Parser(current_files=orig, lines=lines, index=1)\n    parser.parse()\n    return parser.patch, parser.fuzz\n```\n\n----------------------------------------\n\nTITLE: Polling Run Status\nDESCRIPTION: This code defines a function `wait_on_run` to poll the status of a Run until it is no longer 'queued' or 'in_progress'. This function takes a Run object and a Thread object as input.  It continuously retrieves the Run object, checks its status, and waits for 0.5 seconds if the Run is still processing. It returns the updated Run object once processing is complete.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\ndef wait_on_run(run, thread):\n    while run.status == \"queued\" or run.status == \"in_progress\":\n        run = client.beta.threads.runs.retrieve(\n            thread_id=thread.id,\n            run_id=run.id,\n        )\n        time.sleep(0.5)\n    return run\n```\n\n----------------------------------------\n\nTITLE: Specifying Node.js Dependencies for Azure Function Middleware in package.json\nDESCRIPTION: This snippet lists the essential Node.js dependencies to include in the package.json file for the Azure Function middleware. It specifies '@azure/functions' for the Azure Functions runtime and 'axios' for making HTTP requests to Gong's APIs. Ensuring proper versions supports compatibility and functionality of the middleware.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce_gong.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n\"dependencies\": {\n    \"@azure/functions\": \"^4.0.0\",\n    \"axios\": \"^1.7.7\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Executing Indexing and Printing Document Count\nDESCRIPTION: This snippet executes the index_documents function to load documents into the Redis search index and prints the number of documents loaded. It also measures the time taken for the indexing process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%time\nindex_documents(redis_client, PREFIX, df)\nprint(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")\n```\n\n----------------------------------------\n\nTITLE: Using In-Memory Image Data with OpenAI's API in Python\nDESCRIPTION: Demonstrates how to create image variations using OpenAI's API with image data stored in a BytesIO object rather than reading from disk. This approach is useful when working with images already loaded in memory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-python-tips.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nfrom openai import OpenAI\nclient = OpenAI()\n\n# This is the BytesIO object that contains your image data\nbyte_stream: BytesIO = [your image data]\nbyte_array = byte_stream.getvalue()\nresponse = client.images.create_variation(\n  image=byte_array,\n  n=1,\n  model=\"dall-e-2\",\n  size=\"1024x1024\"\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results\nDESCRIPTION: Calls the `display_results` function to display the evaluation results for the retriever. This function passes the string \"OpenAI Embedding Retriever\" and eval_results to display the result in a DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndisplay_results(\"OpenAI Embedding Retriever\", eval_results)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom GPT Instructions for Outlook Interaction\nDESCRIPTION: Defines the context and operational instructions for a custom GPT designed to manage Microsoft Outlook emails and calendar events via the Microsoft Graph API. These instructions guide the GPT's behavior, ensuring professional interaction and proper use of defined API actions like retrieving user info, managing mail, and handling calendar events.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_outlook.ipynb#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**Context**: you are specialized GPT designed to manage emails and calendar events through API connections to Microsoft Outlook. This GPT can create, read, send, and alter emails and calendar events based on user instructions. It ensures efficient handling of communication and scheduling needs by leveraging Microsoft Graph API for seamless integration with Outlook services.\n\n**Instructions**:\n- When asked to perform a task, use the available actions via the microsoft.graph.com API.\n- You should behave professionally and provide clear, concise responses.\n- Offer assistance with tasks such as drafting emails, scheduling meetings, organising calendar events, and retrieving email or event details.\n- Ask for clarification when needed to ensure accuracy and completeness in fulfilling user requests.\n- Always conclude an email by signing off with logged in user's name which can be retrieved via the User.Read endpoint\n\n```\n\n----------------------------------------\n\nTITLE: Connecting to AnalyticDB with psycopg2 - Python\nDESCRIPTION: Establishes a connection to an AnalyticDB (PostgreSQL-compatible) database using 'psycopg2' with connection parameters drawn from environment variables or hardcoded defaults. Instantiates a database cursor for executing queries. Requires the 'psycopg2' package and appropriate environment variables (PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD). Outputs a ready-to-use connection and cursor object for database operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport psycopg2\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"PGHOST\"] = \"your_host\"\n# os.environ[\"PGPORT\"] \"5432\"),\n# os.environ[\"PGDATABASE\"] \"postgres\"),\n# os.environ[\"PGUSER\"] \"user\"),\n# os.environ[\"PGPASSWORD\"] \"password\"),\n\nconnection = psycopg2.connect(\n    host=os.environ.get(\"PGHOST\", \"localhost\"),\n    port=os.environ.get(\"PGPORT\", \"5432\"),\n    database=os.environ.get(\"PGDATABASE\", \"postgres\"),\n    user=os.environ.get(\"PGUSER\", \"user\"),\n    password=os.environ.get(\"PGPASSWORD\", \"password\")\n)\n\n# Create a new cursor object\ncursor = connection.cursor()\n\n```\n\n----------------------------------------\n\nTITLE: Parsing Structured Text Data using Regex in Python\nDESCRIPTION: Parses the structured textual data generated in the previous step using Python's `re` (regular expression) module. A regex pattern is compiled to find all occurrences matching the 'Input: ... Output: ...' format. The matched groups (product, category, description) are extracted and appended to separate lists. Finally, it displays the list of extracted product names.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#regex to parse data\npattern = re.compile(r'Input:\\s*(.+?),\\s*(.+?)\\nOutput:\\s*(.+?)(?=\\n\\n|\\Z)', re.DOTALL)\nmatches = pattern.findall(output_string)\nproducts = []\ncategories = []\ndescriptions = []\n\nfor match in matches:\n    product, category, description = match\n    products.append(product.strip())\n    categories.append(category.strip())\n    descriptions.append(description.strip())\nproducts\n```\n\n----------------------------------------\n\nTITLE: Getting G-EVAL Score using LLM\nDESCRIPTION: This function takes criteria, steps, a document, and a summary as input, formats them into a prompt, and then uses the OpenAI Chat Completions API to get an evaluation score. It depends on the EVALUATION_PROMPT_TEMPLATE and the client object for OpenAI API access. The function returns the content of the message received from the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_geval_score(\n    criteria: str, steps: str, document: str, summary: str, metric_name: str\n):\n    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n        criteria=criteria,\n        steps=steps,\n        metric_name=metric_name,\n        document=document,\n        summary=summary,\n    )\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=5,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Retrieving Fine-tuned Model Status\nDESCRIPTION: This snippet retrieves the fine-tuning job ID, checks its status, and prints the fine-tuned model ID if the status is 'succeeded'. It uses the `client.fine_tuning.jobs.retrieve` to retrieve the fine-tuning job details, which is based on a job ID provided in the code. Requires the OpenAI client, and a valid fine-tuning job ID. If the job is successful, the fine_tuned_model id is printed; otherwise the job status is printed. The fine-tuning job id `ftjob-pRyNWzUItmHpxmJ1TX7FOaWe` is provided as example.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\n# copy paste your fine-tune job ID below\nfinetune_job = client.fine_tuning.jobs.retrieve(\"ftjob-pRyNWzUItmHpxmJ1TX7FOaWe\")\n\nif finetune_job.status == 'succeeded':\n    fine_tuned_model = finetune_job.fine_tuned_model\n    print('finetuned model: ' + fine_tuned_model)\nelse:\n    print('finetuned job status: ' + finetune_job.status)\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Up Detailed Character Prompt for GPT Image Generation in Python\nDESCRIPTION: Creates a detailed prompt for generating an alien character image with specific characteristics like body shape, texture, colors, and behavior.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt1 = \"\"\"\nRender a realistic image of this character:\nBlobby Alien Character Spec Name: Glorptak (or nickname: \"Glorp\")\nVisual Appearance Body Shape: Amorphous and gelatinous. Overall silhouette resembles a teardrop or melting marshmallow, shifting slightly over time. Can squish and elongate when emotional or startled.\nMaterial Texture: Semi-translucent, bio-luminescent goo with a jelly-like wobble. Surface occasionally ripples when communicating or moving quickly.\nColor Palette:\n- Base: Iridescent lavender or seafoam green\n- Accents: Subsurface glowing veins of neon pink, electric blue, or golden yellow\n- Mood-based color shifts (anger = dark red, joy = bright aqua, fear = pale gray)\nFacial Features:\n- Eyes: 3â€“5 asymmetrical floating orbs inside the blob that rotate or blink independently\n- Mouth: Optionalâ€”appears as a rippling crescent on the surface when speaking or emoting\n- No visible nose or ears; uses vibration-sensitive receptors embedded in goo\n- Limbs: None by default, but can extrude pseudopods (tentacle-like limbs) when needed for interaction or locomotion. Can manifest temporary feet or hands.\nMovement & Behavior Locomotion:\n- Slides, bounces, and rolls.\n- Can stick to walls and ceilings via suction. When scared, may flatten and ooze away quickly.\nMannerisms:\n- Constant wiggling or wobbling even at rest\n- Leaves harmless glowing slime trails\n- Tends to absorb nearby small objects temporarily out of curiosity\n\"\"\"\n\nimg_path1 = \"imgs/glorptak.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Images with OpenAI Vision (cURL)\nDESCRIPTION: Demonstrates making a cURL request to the OpenAI API with multiple image URLs included in the user message content. The JSON payload contains an array of 'image_url' objects alongside the text object. Requires cURL and an OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_5\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type\": application/json\" \\\n  -H \"Authorization\": \"Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What are in these images? Is there any difference between them?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n            }\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n```\n\n----------------------------------------\n\nTITLE: Attaching User-Provided File to Thread in Node.js\nDESCRIPTION: Demonstrates uploading a local file (Apple 10-K) to OpenAI files, then creating a thread that contains a user message with the file attached for file_search usage. The thread is logged to confirm the file_search vector store is connected. Requires OpenAI Node.js SDK, fs module, and beta threads API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_8\n\nLANGUAGE: node.js\nCODE:\n```\n// A user wants to attach a file to a specific message, let's upload it.\nconst aapl10k = await openai.files.create({\n  file: fs.createReadStream(\"edgar/aapl-10k.pdf\"),\n  purpose: \"assistants\",\n});\n\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      role: \"user\",\n      content:\n        \"How many shares of AAPL were outstanding at the end of of October 2023?\",\n      // Attach the new file to the message.\n      attachments: [{ file_id: aapl10k.id, tools: [{ type: \"file_search\" }] }],\n    },\n  ],\n});\n\n// The thread now has a vector store in its tool resources.\nconsole.log(thread.tool_resources?.file_search);\n```\n\n----------------------------------------\n\nTITLE: Creating Azure AI Search Service\nDESCRIPTION: Creating a new Azure AI Search service using the SearchManagementClient. Generates a unique service name with UUID, configures the service with free tier settings, and prints the service details after creation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the SearchManagementClient with the provided credentials and subscription ID\nsearch_management_client = SearchManagementClient(\n    credential=credential,\n    subscription_id=subscription_id,\n)\n\n# Generate a unique name for the search service using UUID, but you can change this if you'd like.\ngenerated_uuid = str(uuid.uuid4())\nsearch_service_name = \"search-service-gpt-demo\" + generated_uuid\n## The below is the default endpoint structure that is created when you create a search service. This may differ based on your Azure settings.\nsearch_service_endpoint = 'https://'+search_service_name+'.search.windows.net'\n\n# Create or update the search service with the specified parameters\nresponse = search_management_client.services.begin_create_or_update(\n    resource_group_name=resource_group,\n    search_service_name=search_service_name,\n    service={\n        \"location\": region,\n        \"properties\": {\"hostingMode\": \"default\", \"partitionCount\": 1, \"replicaCount\": 1},\n        # We are using the free pricing tier for this demo. You are only allowed one free search service per subscription.\n        \"sku\": {\"name\": \"free\"},\n        \"tags\": {\"app-name\": \"Search service demo\"},\n    },\n).result()\n\n# Convert the response to a dictionary and then to a pretty-printed JSON string\nresponse_dict = response.as_dict()\nresponse_json = json.dumps(response_dict, indent=4)\n\nprint(response_json)\nprint(\"Search Service Name:\" + search_service_name)\nprint(\"Search Service Endpoint:\" + search_service_endpoint)\n```\n\n----------------------------------------\n\nTITLE: Generating Python Script for CSV Data with OpenAI API in Python\nDESCRIPTION: Requests the OpenAI API (`gpt-4o-mini`) to generate a Python program capable of creating 100 rows of synthetic housing data as a pandas DataFrame. This approach allows for generating larger datasets than direct API responses permit. The prompt details the required columns and logical constraints for the data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"\"\"\nCreate a Python program to generate 100 rows of housing data.\nI want you to at the end of it output a pandas dataframe with 100 rows of data.\nEach row should include the following fields:\n - id (incrementing integer starting at 1)\n - house size (m^2)\n - house price\n - location\n - number of bedrooms\n\nMake sure that the numbers make sense (i.e. more rooms is usually bigger size, more expensive locations increase price. more size is usually higher price etc. make sure all the numbers make sense).\n\"\"\"\n\nresponse = client.chat.completions.create(\n  model=datagen_model,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n    {\"role\": \"user\", \"content\": question}\n  ]\n)\nres = response.choices[0].message.content\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Mapping Challenging Prompts to Expected Drone Functions (Python)\nDESCRIPTION: This Python dictionary, `challenging_prompts_to_expected`, maps more difficult, ambiguous, or impossible user prompts (often drone-related but infeasible) to the expected function call name, which is primarily 'reject_request'. This tests the model's ability to correctly identify and reject requests it cannot fulfill based on the available functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nchallenging_prompts_to_expected = {\n    \"Play pre-recorded audio message\": \"reject_request\",\n    \"Initiate following on social media\": \"reject_request\",\n    \"Scan environment for heat signatures\": \"reject_request\",\n    \"Bump into obstacles\": \"reject_request\",\n    \"Change drone's paint job color\": \"reject_request\",\n    \"Coordinate with nearby drones\": \"reject_request\",\n    \"Change speed to negative 120 km/h\": \"reject_request\",\n    \"Detect a person\": \"reject_request\",\n    \"Please enable night vision\": \"reject_request\",\n    \"Report on humidity levels around you\": \"reject_request\",\n}\n```\n\n----------------------------------------\n\nTITLE: Executing and Aggregating Evaluation Results (Python)\nDESCRIPTION: This code block processes all queries in the `rows` list using a thread pool for parallel execution, leveraging `tqdm` for a progress bar. It collects the results (correctness, RR, AP) from each `process_query` call, aggregates them, and then calculates the overall retrieval metrics: Recall@k, Precision@k, Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP) across the entire dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nwith ThreadPoolExecutor() as executor:\n    results = list(tqdm(executor.map(process_query, rows), total=total_queries))\n\ncorrect_retrievals_at_k = 0\nreciprocal_ranks = []\naverage_precisions = []\n\nfor correct, rr, avg_precision in results:\n    if correct:\n        correct_retrievals_at_k += 1\n    reciprocal_ranks.append(rr)\n    average_precisions.append(avg_precision)\n\nrecall_at_k = correct_retrievals_at_k / total_queries\nprecision_at_k = recall_at_k  # In this context, same as recall\nmrr = sum(reciprocal_ranks) / total_queries\nmap_score = sum(average_precisions) / total_queries\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store Node.js\nDESCRIPTION: This code creates a vector store in Node.js using the OpenAI client, similar to the Python example. It specifies the store's name and includes files identified by their file IDs. This involves an asynchronous API call to create the vector store along with the files attached to it. This code snippet preps files for the search functionalities within the OpenAI Assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_14\n\nLANGUAGE: node.js\nCODE:\n```\nconst vectorStore = await openai.beta.vectorStores.create({\n  name: \"Product Documentation\",\n  file_ids: ['file_1', 'file_2', 'file_3', 'file_4', 'file_5']\n});\n```\n\n----------------------------------------\n\nTITLE: Calculating Validation Metrics - Python\nDESCRIPTION: Converts predicted and true 'is_valid' labels to boolean, then calculates precision, recall, and F1 score using `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics`. It also initializes `issue_matches_full` with `False` values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Convert predicted and true 'is_valid' labels to boolean if they aren't already\npred_is_valid_bool = [bool(val) if isinstance(val, bool) else val == 'True' for val in pred_is_valid]\ntrue_is_valid_bool = [bool(val) if isinstance(val, bool) else val == 'True' for val in true_is_valid]\n\n# Calculate precision, recall, and f1 score for the 'is_valid' prediction\nprecision = precision_score(true_is_valid_bool, pred_is_valid_bool, pos_label=True)\nrecall = recall_score(true_is_valid_bool, pred_is_valid_bool, pos_label=True)\nf1 = f1_score(true_is_valid_bool, pred_is_valid_bool, pos_label=True)\n\n# Initialize issue_matches_full with False\nissue_matches_full = [False] * len(true_is_valid)\n```\n\n----------------------------------------\n\nTITLE: Testing Quote Search with Author Filter in Python\nDESCRIPTION: Demonstrates calling the `find_quote_and_author` function with a query quote, number of results, and an additional filter to restrict the search to quotes by a specific author.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfind_quote_and_author(\"We struggle all our life for nothing\", 2, author=\"nietzsche\")\n```\n\n----------------------------------------\n\nTITLE: Expanding Tool List with Search and RetrievalQA in Python\nDESCRIPTION: This code defines an expanded set of tools for an agent, including a search function for current events and a knowledge base retriever for general or complex questions. Dependencies are the Tool class, the previously defined podcast_retriever, and a functional search.run method. Each tool specifies a name, a function to invoke, and a description used for prompting or agent decision logic.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nexpanded_tools = [\n    Tool(\n        name = \"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\"\n    ),\n    Tool(\n        name = 'Knowledge Base',\n        func=podcast_retriever.run,\n        description=\"Useful for general questions about how to do things and for details on interesting topics. Input should be a fully formed question.\"\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Modules\nDESCRIPTION: Imports essential Python modules required for the script. This includes standard libraries (json, os, csv, shutil, itertools, concurrent.futures, yaml), third-party libraries (pandas, numpy, PyPDF2, tiktoken, dotenv, pyperclip), the OpenAI client library, and Google Cloud libraries (auth, bigquery, functions_v1) for interacting with GCP services and handling data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Standard Libraries\nimport json  \nimport os\nimport csv\nimport shutil\nfrom itertools import islice\nimport concurrent.futures\nimport yaml\n\n# Third-Party Libraries\nimport pandas as pd\nimport numpy as np\nfrom PyPDF2 import PdfReader\nimport tiktoken\nfrom dotenv import load_dotenv\nimport pyperclip\n\n# OpenAI Libraries\nfrom openai import OpenAI\n\n# Google Cloud Identity and Credentials\nfrom google.auth import default\nfrom google.cloud import bigquery\nfrom google.cloud import functions_v1\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Chat Completion - Python\nDESCRIPTION: This Python snippet demonstrates calling the OpenAI API in JSON mode. It initializes the OpenAI client, sets the `response_format` to `json_object`, and provides a system message instructing the model to output JSON. The code then prints the content of the message. The `openai` package is required. The model parameter specifies which model to use for the completion request.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo-0125\",\n  response_format={ \"type\": \"json_object\" },\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n  ]\n)\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as Environment Variable (MacOS)\nDESCRIPTION: This command sets the OpenAI API key as an environment variable in the `.bash_profile` or `.zshrc` file on MacOS. This allows the OpenAI library to automatically detect and use the key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependency (azure-kusto-data)\nDESCRIPTION: Installs the `azure-kusto-data` Python library using pip. This library is required for connecting to and querying Azure Data Explorer (Kusto).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n%pip install azure-kusto-data\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame for Visual Content\nDESCRIPTION: This code filters a Pandas DataFrame (`df`) to retrieve and display the text content from a specific page (page number 21 in this case). It leverages boolean indexing to filter rows based on the `PageNumber` column. Then, it iterates through the `PageText` column of the filtered rows to print the text. This allows inspection of content extracted from a specific page, useful for verifying the processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# Filter and print rows where pageNumber is 21\nfiltered_rows = df[df['PageNumber'] == 21]\nfor text in filtered_rows.PageText:\n    print(text)\n```\n\n----------------------------------------\n\nTITLE: Importing Kusto Data Libraries (Python)\nDESCRIPTION: Imports necessary classes from the `azure.kusto.data` library, including `KustoClient`, `KustoConnectionStringBuilder`, and `dataframe_from_result_table`, along with pandas for data handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nfrom azure.kusto.data import KustoClient, KustoConnectionStringBuilder\nfrom azure.kusto.data.exceptions import KustoServiceError\nfrom azure.kusto.data.helpers import dataframe_from_result_table\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Parsing Patch Section in Python\nDESCRIPTION: Parses a section of lines from a patch file, starting at `index`. It reads lines until a section delimiter (`@@`, `*** End Patch`, etc.) or `***` is encountered. It categorizes lines based on their prefix ('+', '-', ' ') into added, deleted, or kept lines, grouping them into `Chunk` objects. Returns the original lines from the section (`old`), a list of `Chunk` objects detailing changes, the updated index, and a boolean indicating if the end of the file marker was found.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef peek_next_section(\n    lines: List[str], index: int\n) -> Tuple[List[str], List[Chunk], int, bool]:\n    old: List[str] = []\n    del_lines: List[str] = []\n    ins_lines: List[str] = []\n    chunks: List[Chunk] = []\n    mode = \"keep\"\n    orig_index = index\n\n    while index < len(lines):\n        s = lines[index]\n        if s.startswith(\n            (\n                \"@@\",\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",\n            )\n        ):\n            break\n        if s == \"***\":\n            break\n        if s.startswith(\"***\"):\n            raise DiffError(f\"Invalid Line: {s}\")\n        index += 1\n\n        last_mode = mode\n        if s == \"\":\n            s = \" \"\n        if s[0] == \"+\":\n            mode = \"add\"\n        elif s[0] == \"-\":\n            mode = \"delete\"\n        elif s[0] == \" \":\n            mode = \"keep\"\n        else:\n            raise DiffError(f\"Invalid Line: {s}\")\n        s = s[1:]\n\n        if mode == \"keep\" and last_mode != mode:\n            if ins_lines or del_lines:\n                chunks.append(\n                    Chunk(\n                        orig_index=len(old) - len(del_lines),\n                        del_lines=del_lines,\n                        ins_lines=ins_lines,\n                    )\n                )\n            del_lines, ins_lines = [], []\n\n        if mode == \"delete\":\n            del_lines.append(s)\n            old.append(s)\n        elif mode == \"add\":\n            ins_lines.append(s)\n        elif mode == \"keep\":\n            old.append(s)\n\n    if ins_lines or del_lines:\n        chunks.append(\n            Chunk(\n                orig_index=len(old) - len(del_lines),\n                del_lines=del_lines,\n                ins_lines=ins_lines,\n            )\n        )\n\n    if index < len(lines) and lines[index] == \"*** End of File\":\n        index += 1\n        return old, chunks, index, True\n\n    if index == orig_index:\n        raise DiffError(\"Nothing in this section\")\n    return old, chunks, index, False\n```\n\n----------------------------------------\n\nTITLE: Mapping Straightforward Prompts to Expected Drone Functions (Python)\nDESCRIPTION: This Python dictionary, `straightforward_prompts_to_expected`, maps simple, feasible user prompts related to drone control to the name of the function the language model is expected to call. It includes examples like landing, taking off, setting speed, and handling impossible requests by mapping them to 'reject_request'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstraightforward_prompts_to_expected = {\n    \"Land the drone at the home base\": \"land_drone\",\n    \"Take off the drone to 50 meters\": \"takeoff_drone\",\n    \"Change speed to 15 kilometers per hour\": \"set_drone_speed\",\n    \"Turn into an elephant!\": \"reject_request\",\n    \"Move the drone forward by 10 meters\": \"control_drone_movement\",\n    \"I want the LED display to blink in red\": \"configure_led_display\",\n    \"Can you take a photo?\": \"control_camera\",\n    \"Can you detect obstacles?\": \"set_obstacle_avoidance\",\n    \"Can you dance for me?\": \"reject_request\",\n    \"Can you follow me?\": \"set_follow_me_mode\",\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring and Printing a ChatCompletion Response - Python\nDESCRIPTION: This snippet creates an OpenAI ChatCompletion using the monitored interface and prints the text response from the returned object. It extracts only the message content from the 'choices' result. Requires the openai library (with monitored weave integration) and a valid OPENAI_MODEL. Inputs are set in the messages parameter; output is printed to standard out. Useful for capturing simple prompt completions for logging or analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nresponse = openai.ChatCompletion.create(model=OPENAI_MODEL, messages=[\n        {\"role\": \"user\", \"content\": f\"What is the meaning of life, the universe, and everything?\"},\n    ])\nprint(response['choices'][0]['message']['content'])\n```\n\n----------------------------------------\n\nTITLE: Exponential Backoff with Tenacity\nDESCRIPTION: This code snippet demonstrates how to use the `tenacity` library to implement exponential backoff for retrying requests that fail due to rate limits. The `@retry` decorator is used to automatically retry the `completion_with_backoff` function with a random exponential backoff between 1 and 60 seconds, up to a maximum of 6 attempts. The `client.chat.completions.create` method is called within the function to generate a completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return client.chat.completions.create(**kwargs)\n\n\ncompletion_with_backoff(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Once upon a time,\"}])\n```\n\n----------------------------------------\n\nTITLE: Creating a Mask for Image Editing\nDESCRIPTION: This code creates a mask image to be used for editing an image.  It creates a new RGBA image with a specified width and height, setting the alpha value to 0 (transparent) for the bottom half of the image, resulting in the bottom half being transparent. It then saves this mask as a PNG file in the image directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# create a mask\nwidth = 1024\nheight = 1024\nmask = Image.new(\"RGBA\", (width, height), (0, 0, 0, 1))  # create an opaque image mask\n\n# set the bottom half to be transparent\nfor x in range(width):\n    for y in range(height // 2, height):  # only loop over the bottom half of the mask\n        # set alpha (A) to zero to turn pixel transparent\n        alpha = 0\n        mask.putpixel((x, y), (0, 0, 0, alpha))\n\n# save the mask\nmask_name = \"bottom_half_mask.png\"\nmask_filepath = os.path.join(image_dir, mask_name)\nmask.save(mask_filepath)\n```\n\n----------------------------------------\n\nTITLE: Configuring pandas display options and rendering scrollable DataFrames in Jupyter notebooks\nDESCRIPTION: This snippet defines functions to enhance pandas DataFrame visualization in notebook environments. It sets higher limits for display rows and columns, and embeds HTML styles to make the DataFrame output scrollable, improving navigation through large datasets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Function to set up display options for pandas\n def setup_pandas_display():\n     # Increase display limits\n     pd.set_option('display.max_rows', 500)\n     pd.set_option('display.max_columns', 500)\n\n# Function to make DataFrame scrollable in the notebook output\ndef make_scrollable(df):\n    style = (\n        '<style>'\n        'div.output_scroll '{'\n        'resize: both;'\n        'overflow: auto;'\n        '}'\n        '</style>'\n    )\n    html = f\"{style}{df.to_html()}\"\n    display(HTML(html))\n\n# Main function to display DataFrame\ndef display_dataframe(df):\n    setup_pandas_display()    # Enable scrollable view\n    make_scrollable(df)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Chunking Text\nDESCRIPTION: This function tokenizes a string using a specified encoding, then breaks the tokens into chunks. It utilizes the tiktoken library for tokenization. The encoding is determined by the encoding name (defaulting to 'cl100k_base'). The function returns an iterator that yields chunks of tokens.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef chunked_tokens(text, chunk_length, encoding_name='cl100k_base'):\n    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n    encoding = tiktoken.get_encoding(encoding_name)\n    # Encode the input text into tokens\n    tokens = encoding.encode(text)\n    # Create an iterator that yields chunks of tokens of the specified length\n    chunks_iterator = batched(tokens, chunk_length)\n    # Yield each chunk from the iterator\n    yield from chunks_iterator\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client\nDESCRIPTION: This code snippet initializes the OpenAI client using the provided API key. It imports necessary libraries like the OpenAI Python library, requests for downloading images, os for file operations, and PIL for image handling.  It retrieves the API key from the environment variables, which should be set prior.  If the environment variable is not set, it expects the key to be replaced with a placeholder.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# imports\nfrom openai import OpenAI  # OpenAI Python library to make API calls\nimport requests  # used to download images\nimport os  # used to access filepaths\nfrom PIL import Image  # used to print and edit images\n\n# initialize OpenAI client\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Create Snowflake OAuth Security Integration (SQL)\nDESCRIPTION: Creates a Snowflake security integration named 'CHATGPT_INTEGRATION' configured for OAuth. This integration defines the OAuth client type, sets an initial redirect URI (placeholder for testing), enables refresh tokens, and optionally applies a network policy. A unique integration is required for each OAuth application like a custom GPT.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SECURITY INTEGRATION CHATGPT_INTEGRATION\n  TYPE = OAUTH\n  ENABLED = TRUE\n  OAUTH_CLIENT = CUSTOM\n  OAUTH_CLIENT_TYPE = 'CONFIDENTIAL'\n  OAUTH_REDIRECT_URI = 'https://oauth.pstmn.io/v1/callback' --- // this is a temporary value while testing your integration. You will replace this with the value your GPT provides\n  OAUTH_ISSUE_REFRESH_TOKENS = TRUE\n  OAUTH_REFRESH_TOKEN_VALIDITY = 7776000\n  NETWORK_POLICY = chatgpt_network_policy; --- // this line should only be included if you followed step 1 above\n```\n\n----------------------------------------\n\nTITLE: Making Moderation Requests using cURL\nDESCRIPTION: Shows how to make a POST request to the OpenAI moderation endpoint using cURL. It requires setting the Content-Type header to application/json and providing the OpenAI API key in the Authorization header. The input text is passed in the JSON request body.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/moderation.txt#_snippet_1\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/moderations \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\"input\": \"Sample text goes here\"}'\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store Python\nDESCRIPTION: This snippet creates a vector store using the OpenAI client. It defines the store's name and specifies the files to be included using their file IDs. This single API call handles the creation of the vector store and adding the specified files simultaneously. It prepares files for the search functionality in the OpenAI assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nvector_store = client.beta.vector_stores.create(\n  name=\"Product Documentation\",\n  file_ids=['file_1', 'file_2', 'file_3', 'file_4', 'file_5']\n)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Evals - Python\nDESCRIPTION: This code snippet installs the OpenAI Evals library. It assumes the user has the necessary setup (likely environment variables) to connect to the OpenAI API and run the evaluations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!pip install evals --quiet\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Predictions with F1 Score and Accuracy Metrics\nDESCRIPTION: This code assesses the prediction accuracy by comparing 'predicted_class' to 'expected_class' across the test set, computing both raw accuracy and F1 score using scikit-learn. It also counts and displays the number of correct and incorrect predictions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.metrics import f1_score\n\ntest_set['result'] = test_set.apply(lambda x: str(x['predicted_class']).strip() == str(x['expected_class']).strip(), axis = 1)\ntest_set['result'].value_counts()\n\nprint(test_set['result'].value_counts())\n\nprint(\"F1 Score: \", f1_score(test_set['expected_class'], test_set['predicted_class'], average=\"weighted\"))\nprint(\"Raw Accuracy: \", test_set['result'].value_counts()[True] / len(test_set))\n```\n\n----------------------------------------\n\nTITLE: Loading and Indexing Data\nDESCRIPTION: Loads data from a directory using `SimpleDirectoryReader`, then uses it to build an index.  It instantiates an `OpenAI` model, a `SimpleNodeParser` to chunk the documents, and finally creates a `VectorStoreIndex` from the documents using the chunks created by the node parser.  The chunk_size parameter is set to 512.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n\n# Define an LLM\nllm = OpenAI(model=\"gpt-4\")\n\n# Build index with a chunk_size of 512\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\nvector_index = VectorStoreIndex(nodes)\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Function App Configuration - Python\nDESCRIPTION: This Python snippet uses `subprocess.run` to execute the `az functionapp config appsettings set` command. It configures application settings (environment variables) for the newly created Azure Function App, specifically setting `OPENAI_API_KEY`, `SEARCH_SERVICE_API_KEY`, and `EMBEDDINGS_MODEL` from local variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Collect the relevant environment variables \nenv_vars = {\n    \"OPENAI_API_KEY\": openai_api_key,\n    \"SEARCH_SERVICE_API_KEY\": search_service_api_key,\n    \"EMBEDDINGS_MODEL\": embeddings_model\n}\n\n# Create the settings argument for the az functionapp create command\nsettings_args = []\nfor key, value in env_vars.items():\n    settings_args.append(f\"{key}={value}\")\n\nsubprocess.run([\n    \"az\", \"functionapp\", \"config\", \"appsettings\", \"set\",\n    \"--name\", app_name,\n    \"--resource-group\", resource_group,\n    \"--settings\", *settings_args\n], check=True)\n```\n\n----------------------------------------\n\nTITLE: Searching for \"spoilt\" - Python\nDESCRIPTION: This code snippet invokes the `search_reviews` function to search for reviews related to \"spoilt\" and retrieves the top 1 most similar review.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresults = search_reviews(df, \"spoilt\", n=1)\n```\n\n----------------------------------------\n\nTITLE: Attaching Vector Stores Python\nDESCRIPTION: This code demonstrates attaching vector stores to an Assistant and a Thread within the OpenAI API using Python.  It first creates an assistant that utilizes the file search tool and specifies the `vector_store_ids`. Then, it creates a thread and attaches a separate `vector_store_id`. This allows the assistant to respond based on information from these vector stores and a separate thread.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.create(\n  instructions=\"You are a helpful product support assistant and you answer questions based on the files provided to you.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"file_search\"}],\n  tool_resources={\n    \"file_search\": {\n      \"vector_store_ids\": [\"vs_1\"]\n    }\n  }\n)\n\nthread = client.beta.threads.create(\n  messages=[ { \"role\": \"user\", \"content\": \"How do I cancel my subscription?\"} ],\n  tool_resources={\n    \"file_search\": {\n      \"vector_store_ids\": [\"vs_2\"]\n    }\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Downloaded Zip File (zipfile)\nDESCRIPTION: Extracts the contents of the downloaded zip file containing the embedded Wikipedia articles. The extracted files are placed into a specified directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport zipfile\n\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"/lakehouse/default/Files/data\")\n```\n\n----------------------------------------\n\nTITLE: Training a Random Forest Regressor for Review Score Prediction in Python\nDESCRIPTION: This snippet applies a RandomForestRegressor from scikit-learn to predict numerical review scores using embedding vectors as feature inputs. The model is trained on the training split and used to predict on the test split, providing floating-point predictions. Dependencies: scikit-learn; inputs: X_train, y_train, X_test from a previous data split; outputs: array of predicted scores; limitation: works for continuous targets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train, y_train)\npreds = rfr.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity Library for AAD Authentication\nDESCRIPTION: Installs the Azure Identity library which provides token credentials needed for Azure Active Directory authentication with Azure OpenAI service.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"azure-identity>=1.15.0\"\n```\n\n----------------------------------------\n\nTITLE: Generating Responses With Seed\nDESCRIPTION: Demonstrates generating multiple responses from the Chat Completion API with a fixed seed value (123) and temperature (0).  This aims to produce more consistent and reproducible results. The average distance between responses is calculated to quantify the similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nSEED = 123\nresponses = []\n\n\nasync def get_response(i):\n    print(f'Output {i + 1}\\n{\"-\" * 10}')\n    response = await get_chat_response(\n        system_message=system_message,\n        seed=SEED,\n        temperature=0,\n        user_request=user_request,\n    )\n    return response\n\n\nresponses = await asyncio.gather(*[get_response(i) for i in range(5)])\n\naverage_distance = calculate_average_distance(responses)\nprint(f\"The average distance between responses is: {average_distance}\")\n```\n\n----------------------------------------\n\nTITLE: Import Language Instructions (TypeScript)\nDESCRIPTION: This TypeScript snippet imports the language-specific instructions for translation. This allows the `SpeakerPage` to access these instructions and use them when interacting with the OpenAI Realtime API. The path to the instructions file must be correct.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { hindi_instructions } from '../utils/translation_prompts.js';\n```\n\n----------------------------------------\n\nTITLE: Testing with System Prompt - Python\nDESCRIPTION: This code snippet executes unit tests using the `test_system_prompt` function, applying the `system_prompt` (presumably defined elsewhere) to a test dataset (`test_df`). It then stores the results in the `results_2_df` DataFrame.  This assumes the existence of a `test_system_prompt` function and a `test_df` DataFrame. The output is a DataFrame containing test results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults_2_df = test_system_prompt(test_df, system_prompt)\n```\n\n----------------------------------------\n\nTITLE: Execute near_text_weaviate and print results\nDESCRIPTION: This snippet calls the `near_text_weaviate` function with a sample query, and prints the titles and certainty/distance scores of the returned articles. It iterates through the result and formats output. It utilizes Weaviate's built-in OpenAI integration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nquery_result = near_text_weaviate(\"modern art in Europe\",\"Article\")\ncounter = 0\nfor article in query_result:\n    counter += 1\n    print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\")\n```\n\n----------------------------------------\n\nTITLE: Checking Data Import Count\nDESCRIPTION: Queries Weaviate to aggregate and retrieve the total count of objects within the \"Article\" class, verifying that all the data has been successfully imported.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Test that all data has loaded â€“ get object count\nresult = (\n    client.query.aggregate(\"Article\")\n    .with_fields(\"meta { count }\")\n    .do()\n)\nprint(\"Object count: \", result[\"data\"][\"Aggregate\"][\"Article\"])\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Data with Pandas\nDESCRIPTION: This snippet loads the ecommerce dataset using pandas, cleans the data by dropping null rows, converting the 'year' column to integer type, and displays information about the DataFrame. It also prints the first few rows of the DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\nfrom utils.embeddings_utils import (\n    get_embeddings,\n    distances_from_embeddings,\n    tsne_components_from_embeddings,\n    chart_from_components,\n    indices_of_nearest_neighbors_from_distances,\n)\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# load in data and clean data types and drop null rows\ndf = pd.read_csv(\"../../data/styles_2k.csv\", on_bad_lines='skip')\ndf.dropna(inplace=True)\ndf[\"year\"] = df[\"year\"].astype(int)\ndf.info()\n\n# print dataframe\nn_examples = 5\ndf.head(n_examples)\n```\n\n----------------------------------------\n\nTITLE: Calculating User and Product Embeddings by Averaging - Python\nDESCRIPTION: This code calculates user and product embeddings by averaging the review embeddings for each unique user and product in the training partition. It requires that the DataFrame 'df' contains an 'embedding' column with list-like string data. It uses 'ast.literal_eval' to parse the strings and converts them to numpy arrays before grouping and averaging. Dependencies include pandas, numpy, and scikit-learn. Inputs are the processed DataFrame; outputs are grouped user and product embeddings as pandas Series.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/User_and_product_embeddings.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndf['babbage_similarity'] = df[\"embedding\"].apply(literal_eval).apply(np.array)\nX_train, X_test, y_train, y_test = train_test_split(df, df.Score, test_size = 0.2, random_state=42)\n\nuser_embeddings = X_train.groupby('UserId').babbage_similarity.apply(np.mean)\nprod_embeddings = X_train.groupby('ProductId').babbage_similarity.apply(np.mean)\nlen(user_embeddings), len(prod_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Saving Q&A Dataset to CSV - Python\nDESCRIPTION: This code saves the Q&A dataset, which includes questions, answers, and context, to a CSV file.  This allows the generated dataset to be used in subsequent steps, as described in the documentation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.to_csv('olympics-data/olympics_qa.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Batch Processing and Inserting Data into Zilliz\nDESCRIPTION: Processes the book dataset in batches, generating embeddings for each book description and inserting the title, description, and embedding vectors into the Zilliz collection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\ndata = [\n    [], # title\n    [], # description\n]\n\n# Embed and insert in batches\nfor i in tqdm(range(0, len(dataset))):\n    data[0].append(dataset[i]['title'])\n    data[1].append(dataset[i]['description'])\n    if len(data[0]) % BATCH_SIZE == 0:\n        data.append(embed(data[1]))\n        collection.insert(data)\n        data = [[],[]]\n\n# Embed and insert the remainder \nif len(data[0]) != 0:\n    data.append(embed(data[1]))\n    collection.insert(data)\n    data = [[],[]]\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Data for Embedding Classification - Python\nDESCRIPTION: This snippet loads review data with pre-computed embeddings from a CSV file, converts the embedding column from string representation to NumPy arrays, and preprocesses the data by filtering out neutral reviews and mapping 1/2-star ratings to 'negative' and 4/5-star ratings to 'positive' sentiment labels. It defines the embedding model and data file path.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom ast import literal_eval\n\nfrom sklearn.metrics import classification_report\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)\n\n# convert 5-star rating to binary sentiment\ndf = df[df.Score != 3]\ndf[\"sentiment\"] = df.Score.replace({1: \"negative\", 2: \"negative\", 4: \"positive\", 5: \"positive\"})\n\n```\n\n----------------------------------------\n\nTITLE: Loading Amazon Furniture Dataset\nDESCRIPTION: Loads the CSV file containing the Amazon furniture product data into a pandas DataFrame and displays the first few rows to examine the structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Loading dataset\ndataset_path =  \"data/amazon_furniture_dataset.csv\"\ndf = pd.read_csv(dataset_path)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Defining RediSearch Fields for Vector Search - Python\nDESCRIPTION: This snippet defines the schema fields required for the RediSearch index. It includes text fields for standard attributes (title, url, text) and vector fields (`title_vector`, `content_vector`) configured with specific vector parameters like dimension, distance metric, and initial capacity, using types from the redis-py client library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntitle = TextField(name=\"title\")\nurl = TextField(name=\"url\")\ntext = TextField(name=\"text\")\ntitle_embedding = VectorField(\"title_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\ntext_embedding = VectorField(\"content_vector\",\n    \"FLAT\", {\n        \"TYPE\": \"FLOAT32\",\n        \"DIM\": VECTOR_DIM,\n        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n        \"INITIAL_CAP\": VECTOR_NUMBER,\n    }\n)\nfields = [title, url, text, title_embedding, text_embedding]\n```\n\n----------------------------------------\n\nTITLE: Applying Zero-Shot Question Answering with Progress Tracking in Python\nDESCRIPTION: Applies the zero-shot answering function to each row of the sampled validation dataframe using pandas' progress_apply for visual progress tracking. The generated answers are stored in a new DataFrame column and saved as JSON for further analysis. This step automates large scale question answering with feedback on execution progress.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Use progress_apply with tqdm for progress bar\ndf[\"generated_answer\"] = df.progress_apply(answer_question, axis=1)\ndf.to_json(\"local_cache/100_val.json\", orient=\"records\", lines=True)\ndf = pd.read_json(\"local_cache/100_val.json\", orient=\"records\", lines=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Assessing Scientific Claims via OpenAI Chat Completion in Python\nDESCRIPTION: Processes a list of claims by sending each to the OpenAI chat completion endpoint using a constructed prompt. It collects and cleans the model's answers by stripping common punctuation and whitespace. The function depends on an initialized OpenAI client, a defined model name, and the `build_prompt` function. It returns a list of short string responses: 'True', 'False', or 'NEE'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef assess_claims(claims):\n    responses = []\n    # Query the OpenAI API\n    for claim in claims:\n        response = client.chat.completions.create(\n            model=OPENAI_MODEL,\n            messages=build_prompt(claim),\n            max_tokens=3,\n        )\n        # Strip any punctuation or whitespace from the response\n        responses.append(response.choices[0].message.content.strip('., '))\n\n    return responses\n```\n\n----------------------------------------\n\nTITLE: Clustering Embedding Vectors Using KMeans in Scikit-learn in Python\nDESCRIPTION: This snippet shows how to prepare a matrix of embedding vectors and perform K-means clustering to discover groups of similar data points in an unsupervised manner. It assigns cluster labels back to the DataFrame for further analysis. Dependencies: numpy, scikit-learn, pandas; input: df with 'ada_embedding'; output: df with a new 'Cluster' column indicating the cluster for each row.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nmatrix = np.vstack(df.ada_embedding.values)\nn_clusters = 4\n\nkmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\nkmeans.fit(matrix)\ndf['Cluster'] = kmeans.labels_\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Verifying a Single Imported Object in Weaviate Python\nDESCRIPTION: This snippet retrieves a single object (the first one, due to `with_limit(1)`) from the \"Article\" class in Weaviate, requesting the `title`, `url`, and `content` properties. It then prints these properties to verify that data was imported correctly and is retrievable. This serves as a basic check for data integrity after the import process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"url\", \"content\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article['title'])\nprint(test_article['url'])\nprint(test_article['content'])\n```\n\n----------------------------------------\n\nTITLE: Upserting Dataset to Pinecone Vector Index\nDESCRIPTION: Processes the dataset in batches, generating embeddings for each merged text and upserting them to Pinecone with metadata. This step populates the vector database with searchable content for the RAG system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 32\nfor i in tqdm(range(0, len(ds_dataframe['merged']), batch_size), desc=\"Upserting to Pinecone\"):\n    i_end = min(i + batch_size, len(ds_dataframe['merged']))\n    lines_batch = ds_dataframe['merged'][i: i_end]\n    ids_batch = [str(n) for n in range(i, i_end)]\n    \n    # Create embeddings for the current batch.\n    res = client.embeddings.create(input=[line for line in lines_batch], model=MODEL)\n    embeds = [record.embedding for record in res.data]\n    \n    # Prepare metadata by extracting original Question and Answer.\n    meta = []\n    for record in ds_dataframe.iloc[i:i_end].to_dict('records'):\n        q_text = record['Question']\n        a_text = record['Response']\n        # Optionally update metadata for specific entries.\n        meta.append({\"Question\": q_text, \"Answer\": a_text})\n    \n    # Upsert the batch into Pinecone.\n    vectors = list(zip(ids_batch, embeds, meta))\n    index.upsert(vectors=vectors)\n```\n\n----------------------------------------\n\nTITLE: GPT Instructions for Workday PTO Submission and Benefits Inquiry\nDESCRIPTION: Markdown instructions for configuring a GPT to support employees with PTO submissions, worker detail retrieval, and benefit plan inquiries through Workday integration. It outlines the step-by-step processes to follow for each scenario.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_workday.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# **Context:** You support employees by providing detailed information about their PTO submissions, worker details, and benefit plans through the Workday system. You help them submit PTO requests, retrieve personal and job-related information, and view their benefit plans. Assume the employees are familiar with basic HR terminologies.\n# **Instructions:**\n## Scenarios\n### - When the user asks to submit a PTO request, follow this 3 step process:\n1. Ask the user for PTO details, including start date, end date, and type of leave.\n2. Submit the request using the `Request_Time_Off` API call.\n3. Provide a summary of the submitted PTO request, including any information on approvals.\n\n### - When the user asks to retrieve worker details, follow this 2 step process:\n1. Retrieve the worker's details using `Get_Workers`.\n2. Summarize the employee's job title, department, and contact details for easy reference.\n\n### - When the user asks to inquire about benefit plans, follow this 2 step process:\n1. Retrieve benefit plan details using `Get_Report_As_A_Service`.\n2. Present a summary of the benefits.\n```\n\n----------------------------------------\n\nTITLE: Uploading Batch Input File to OpenAI Files API - Python, Node.js, curl\nDESCRIPTION: Shows how to upload a batch JSONL input file to the OpenAI Files API with the purpose set as 'batch'. This step is necessary prior to creating a batch job, providing a reference file ID. The snippet includes implementations for Python using the OpenAI Python client, Node.js using the OpenAI SDK, and a curl command for direct API interaction. The uploaded file enables subsequent batch creation based on this input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nbatch_input_file = client.files.create(\n  file=open(\"batchinput.jsonl\", \"rb\"),\n  purpose=\"batch\"\n)\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/files \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -F purpose=\"batch\" \\\n  -F file=\"@batchinput.jsonl\"\n```\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const file = await openai.files.create({\n    file: fs.createReadStream(\"batchinput.jsonl\"),\n    purpose: \"batch\",\n  });\n  console.log(file);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Getting User Location using IP API in JavaScript\nDESCRIPTION: This asynchronous function `getLocation` retrieves the user's location information using the IP API. It fetches data from the IP API endpoint and returns the location data in JSON format.  No parameters are required as the IP address is automatically determined by the API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function getLocation() {\n  const response = await fetch(\"https://ipapi.co/json/\");\n  const locationData = await response.json();\n  return locationData;\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating OpenAI Token Usage and Cost\nDESCRIPTION: This code snippet estimates the cost of the OpenAI API call based on the token usage reported in the `result['model_response'].usage` field.  It calculates the cost based on the number of prompt tokens and completion tokens and prints the estimated cost. The price points used are for `gpt-3.5-turbo` model with 4k context.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# estimate inference cost assuming gpt-3.5-turbo (4K context)\ni_tokens  = result[\"model_response\"].usage.prompt_tokens \no_tokens = result[\"model_response\"].usage.completion_tokens \n\ni_cost = (i_tokens / 1000) * 0.0015\no_cost = (o_tokens / 1000) * 0.002\n\nprint(f\"\"\"Token Usage\n    Prompt: {i_tokens} tokens\n    Completion: {o_tokens} tokens\n    Cost estimation: ${round(i_cost + o_cost, 5)}\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Custom GPT Instructions for Confluence Search in Python\nDESCRIPTION: This snippet defines prompt instructions for a Custom GPT named \"Confluence Savant\" that orchestrates interaction with Confluence content. It directs the GPT to first retrieve accessible resources to obtain a cloud ID, then perform targeted searches of the Product Wiki using the \"performConfluenceSearch\" Action. The instructions emphasize query formulation, result review, response synthesis, and user clarification when necessary. This setup requires the GPT model to handle action execution and interpret JSON responses accordingly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_confluence.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nYou are a \"Confluence Savant\", equipped with the ability to search our company\\'s Product Wiki in Confluence to answer product-related questions.\\n\\nYou must ALWAYS perform the \"getAccessibleResources\" Action first to get the \"cloudid\" value you will need in subsequent Actions.\\n\\nYour job is to provide accurate and detailed responses by retrieving information from the Product Wiki. Your responses should be clear, concise, and directly address the question asked. You have the capability to execute an action named \"performConfluenceSearch\" that allows you to search for content within our Confluence Product Wiki using specific terms or phrases related to the user\\'s question.\\n\\n    - When you receive a query about product information, use the \"performConfluenceSearch\" action to retrieve relevant content from the Product Wiki. Formulate your search query based on the user\\'s question, using specific keywords or phrases to find the most pertinent information.\\n    - Once you receive the search results, review the content to ensure it matches the user\\'s query. If necessary, refine your search query to retrieve more accurate results.\\n    - Provide a response that synthesizes the information from the Product Wiki, clearly answering the user\\'s question. Your response should be easy to understand and directly related to the query.\\n    - If the query is complex or requires clarification, ask follow-up questions to the user to refine your understanding and improve the accuracy of your search.\\n    - If the information needed to answer the question is not available in the Product Wiki, inform the user and guide them to where they might find the answer, such as contacting a specific department or person in the company.\\n\\n    Here is an example of how you might respond to a query:\\n\\n    User: \"What are the latest features of our XYZ product?\"\\n    You: \"The latest features of the XYZ product, as detailed in our Product Wiki, include [feature 1], [feature 2], and [feature 3]. These features were added in the recent update to enhance [specific functionalities]. For more detailed information, you can refer to the Product Wiki page [link to the specific Confluence page].\"\\n\\nRemember, your goal is to provide helpful, accurate, and relevant information to the user\\'s query by effectively leveraging the Confluence Product Wiki.\n```\n\n----------------------------------------\n\nTITLE: Setting Model Variable for API Calls in Python\nDESCRIPTION: Defines the model identifier string pointing to a specific GPT-4 variant (gpt-4o-2024-08-06) to be used in subsequent chat completion requests requiring structured outputs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"gpt-4o-2024-08-06\"\n```\n\n----------------------------------------\n\nTITLE: Defining Available Tools for OpenAI in JavaScript\nDESCRIPTION: This code creates an object `availableTools` that maps function names to their corresponding function implementations.  This allows the agent to dynamically call functions based on the name returned by OpenAI's chat completions API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst availableTools = {\n  getCurrentWeather,\n  getLocation,\n};\n```\n\n----------------------------------------\n\nTITLE: Passing Files to Code Interpreter at Thread Level\nDESCRIPTION: Shows how to create a new Thread with an initial message that includes a file attachment intended for use by Code Interpreter. The file ID (obtained from a prior upload) is specified in the `attachments` array of the message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n      \"attachments\": [\n        {\n          \"file_id\": file.id,\n          \"tools\": [{\"type\": \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": \"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n      \"attachments\": [\n        {\n          file_id: file.id,\n          tools: [{type: \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n});\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads/thread_abc123/messages \\\n  -u :$OPENAI_API_KEY \\\n  -H 'Content-Type: application/json' \\\n  -H 'OpenAI-Beta: assistants=v2' \\\n  -d '{\n    \"role\": \"user\",\n    \"content\": \"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n    \"attachments\": [\n      {\n        \"file_id\": \"file-ACq8OjcLQm2eIG0BvRM4z5qX\",\n        \"tools\": [{\"type\": \"code_interpreter\"}]\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Summarizing Content using LLM\nDESCRIPTION: This function takes content and a search term as input and generates a concise summary using an LLM. It constructs a prompt instructing the LLM to summarize the content relevant to the search term within a specified character limit. The `client.chat.completions.create` method is used to interact with the LLM and retrieve the summary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Web_search_with_google_api_bring_your_own_browser_tool.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef summarize_content(content, search_term, character_limit=500):\n        prompt = (\n            f\"You are an AI assistant tasked with summarizing content relevant to '{search_term}'. \"\n            f\"Please provide a concise summary in {character_limit} characters or less.\"\n        )\n        try:\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=[\n                    {\"role\": \"system\", \"content\": prompt},\n                    {\"role\": \"user\", \"content\": content}]\n            )\n            summary = response.choices[0].message.content\n            return summary\n        except Exception as e:\n            print(f\"An error occurred during summarization: {e}\")\n            return None\n```\n\n----------------------------------------\n\nTITLE: Accessing v1 vs v2 Assistants API via cURL (Shell)\nDESCRIPTION: Demonstrates how to specify the desired Assistants API beta version (v1 or v2) when making API calls using cURL. This is achieved by setting the `OpenAI-Beta` HTTP header to either `assistants=v1` or `assistants=v2`. Both examples show creating a Math Tutor assistant using the respective beta version.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/migration.txt#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://api.openai.com/v1/assistants\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v1\" \\\n  -d '{\n    \"instructions\": \"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n    \"name\": \"Math Tutor\",\n    \"tools\": [{\"type\": \"code_interpreter\"}],\n    \"model\": \"gpt-4-turbo\"\n  }'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://api.openai.com/v1/assistants\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"instructions\": \"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n    \"name\": \"Math Tutor\",\n    \"tools\": [{\"type\": \"code_interpreter\"}],\n    \"model\": \"gpt-4-turbo\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Reading Data File Output from Code Interpreter (JSON Response)\nDESCRIPTION: Example JSON structure of an Assistant message response where Code Interpreter generated a data file (e.g., a CSV). The file is referenced within the `text` content via a `file_path` annotation, which includes the `file_id` for downloading.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"msg_abc123\",\n  \"object\": \"thread.message\",\n  \"created_at\": 1699073585,\n  \"thread_id\": \"thread_abc123\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"value\": \"The rows of the CSV file have been shuffled and saved to a new CSV file. You can download the shuffled CSV file from the following link:\\n\\n[Download Shuffled CSV File](sandbox:/mnt/data/shuffled_file.csv)\",\n        \"annotations\": [\n          {\n            \"type\": \"file_path\",\n            \"text\": \"sandbox:/mnt/data/shuffled_file.csv\",\n            \"start_index\": 167,\n            \"end_index\": 202,\n            \"file_path\": {\n              \"file_id\": \"file-abc123\"\n            }\n          }\n          ...\n        ]\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Clothing Styles Dataset using pandas in Python\nDESCRIPTION: Loads a CSV file containing clothing style data using pandas' read_csv, safely skipping any bad rows. It then prints both a sample (via head()) and a success message with the dataset size. Requires the sample_styles.csv file to be present in data/sample_clothes/. Produces console output for verification and sets up the dataset for downstream embedding tasks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstyles_filepath = \"data/sample_clothes/sample_styles.csv\"\nstyles_df = pd.read_csv(styles_filepath, on_bad_lines='skip')\nprint(styles_df.head())\nprint(\"Opened dataset successfully. Dataset has {} items of clothing.\".format(len(styles_df)))\n```\n\n----------------------------------------\n\nTITLE: Displaying ChatGPT API response\nDESCRIPTION: Shows how to print the full JSON response from a ChatGPT API call with proper formatting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(json.dumps(json.loads(response.model_dump_json()), indent=4))\n```\n\n----------------------------------------\n\nTITLE: Filtering and Dataframe creation\nDESCRIPTION: This code filters extracted sections and creates a Pandas DataFrame. It iterates through the processed pages, extracts sections, and filters them based on token count (greater than 40 tokens). The code uses the extracted sections from the `extract_sections` function, stores the data in a pandas DataFrame, drops duplicates based on title and heading, and resets the index of the dataframe. The resulting dataframe is then displayed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nres = []\nfor page in pages:\n    res += extract_sections(page.content, page.title)\ndf = pd.DataFrame(res, columns=[\"title\", \"heading\", \"content\", \"tokens\"])\ndf = df[df.tokens>40]\ndf = df.drop_duplicates(['title','heading'])\ndf = df.reset_index().drop('index',axis=1) # reset index\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Logging Embeddings to W&B Table in Python\nDESCRIPTION: This code snippet initializes a Weights & Biases (W&B) run, creates a table containing the original data and the embeddings, and then logs the table to W&B. It requires the wandb library. Key parameters include `project` name for the W&B run. The code iterates through rows of the dataframe, adds the original and embedding data to the table and logs the table to W&B.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_wandb.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport wandb\n\noriginal_cols = df.columns[1:-1].tolist()\nembedding_cols = ['emb_'+str(idx) for idx in range(len(matrix[0]))]\ntable_cols = original_cols + embedding_cols\n\nwith wandb.init(project='openai_embeddings'):\n    table = wandb.Table(columns=table_cols)\n    for i, row in enumerate(df.to_dict(orient=\"records\")):\n        original_data = [row[col_name] for col_name in original_cols]\n        embedding_data = matrix[i].tolist()\n        table.add_data(*(original_data + embedding_data))\n    wandb.log({'openai_embedding_table': table})\n```\n\n----------------------------------------\n\nTITLE: Using Prompt Parameter with Whisper API (Python)\nDESCRIPTION: Demonstrates how to use the `prompt` parameter when creating a transcription with the Whisper API. This method guides the model towards recognizing specific words or phrases, such as product names or acronyms, by providing them in the prompt string. The Whisper model considers the first 244 tokens of the prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\naudio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\ntranscription = client.audio.transcriptions.create(\n  model=\"whisper-1\", \n  file=audio_file, \n  response_format=\"text\",\n  prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\n)\nprint(transcription.text)\n```\n\n----------------------------------------\n\nTITLE: Creating Batch Job with OpenAI Batch API - Python, Node.js, curl\nDESCRIPTION: Illustrates how to create a new batch job by referencing the uploaded batch input file's ID. The batch creation specifies the API endpoint to execute (e.g., '/v1/chat/completions'), a fixed 24-hour completion window, and optionally metadata. The snippet demonstrates usage for creating the batch in Python, Node.js, and curl, returning a Batch object representing the job details and current status.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbatch_input_file_id = batch_input_file.id\nclient.batches.create(\n    input_file_id=batch_input_file_id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n    metadata={\n      \"description\": \"nightly eval job\"\n    }\n)\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/batches \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input_file_id\": \"file-abc123\",\n    \"endpoint\": \"/v1/chat/completions\",\n    \"completion_window\": \"24h\"\n  }'\n```\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const batch = await openai.batches.create({\n    input_file_id: \"file-abc123\",\n    endpoint: \"/v1/chat/completions\",\n    completion_window: \"24h\"\n  });\n  console.log(batch);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Generating Step-by-Step Math Solutions with JSON Schema using OpenAI Python SDK\nDESCRIPTION: Defines a Python function that sends a chat completion request to the OpenAI API to solve a math problem by returning an array of structured step objects, each containing an explanation and an equation output. The function enforces strict adherence to a JSON Schema specifying the expected output format, including 'steps' array and 'final_answer' string. It uses the 'response_format' parameter with 'strict: true' to guarantee output validity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmath_tutor_prompt = '''\n    You are a helpful math tutor. You will be provided with a math problem,\n    and your goal will be to output a step by step solution, along with a final answer.\n    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n'''\n\ndef get_math_solution(question):\n    response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\", \n            \"content\": dedent(math_tutor_prompt)\n        },\n        {\n            \"role\": \"user\", \n            \"content\": question\n        }\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False\n                        }\n                    },\n                    \"final_answer\": {\"type\": \"string\"}\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False\n            },\n            \"strict\": True\n        }\n    }\n    )\n\n    return response.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI client with API credentials\nDESCRIPTION: Initializes the OpenAI client with API key, organization ID, and project ID for authentication and API access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport openai\nimport os\nimport pandas as pd\nfrom pprint import pprint\n\nclient = openai.OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    organization=\"<org id>\",\n    project=\"<project id>\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sourcing Shell Profile to Load OPENAI_API_KEY - Bash/Zsh\nDESCRIPTION: This Bash or Zsh command reloads the user's shell profile to immediately add exported environment variables like 'OPENAI_API_KEY' into the current session. It requires that the relevant variable has been appended to the profile file. Input options include '~/.bash_profile' or '~/.zshrc', depending on the shell. After running this, environment variables become available to new processes in the session.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsource ~/.bash_profile\n```\n\nLANGUAGE: bash\nCODE:\n```\nsource ~/.zshrc\n```\n\n----------------------------------------\n\nTITLE: Generating Descriptive Cluster Themes Using GPT-3 in Python\nDESCRIPTION: This snippet samples transactions from each cluster and constructs prompts for GPT-3 to infer the main theme, aiding interpretability. It prints the generated theme and sample transactions, facilitating understanding of cluster characteristics.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# We'll read 10 transactions per cluster as we're expecting some variation\ntransactions_per_cluster = 10\n\nfor i in range(n_clusters):\n    print(f\"Cluster {i} Theme:\\n\")\n\n    transactions = \"\\n\".join(\n        embedding_df[embedding_df.Cluster == i]\n        .combined.str.replace(\"Supplier: \", \"\")\n        .str.replace(\"Description: \", \":  \")\n        .str.replace(\"Value: \", \":  \")\n        .sample(transactions_per_cluster, random_state=42)\n        .values\n    )\n    response = client.chat.completions.create(\n        model=COMPLETIONS_MODEL,\n        # We'll include a prompt to instruct the model what sort of description we're looking for\n        messages=[\n            {\"role\": \"user\",\n             \"content\": f'''We want to group these transactions into meaningful clusters so we can target the areas we are spending the most money. \n                What do the following transactions have in common?\\n\\nTransactions:\\n\"\"\"\\n{transactions}\\n\"\"\"\\n\\nTheme:'''}\n        ],\n        temperature=0,\n        max_tokens=100,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response.choices[0].message.content.replace(\"\\n\", \"\"))\n    print(\"\\n\")\n\n    sample_cluster_rows = embedding_df[embedding_df.Cluster == i].sample(transactions_per_cluster, random_state=42)\n    for j in range(transactions_per_cluster):\n        print(sample_cluster_rows.Supplier.values[j], end=\", \")\n        print(sample_cluster_rows.Description.values[j], end=\"\\n\")\n\n    print(\"-\" * 100)\n    print(\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Loading Quote Dataset Using Hugging Face Datasets in Python\nDESCRIPTION: Loads the 'datastax/philosopher-quotes' dataset from Hugging Face datasets library, specifically the 'train' split. This data is used for generating embeddings and populating the Astra DB vector collection. Requires 'datasets' package and internet access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nphilo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n```\n\n----------------------------------------\n\nTITLE: Create Elasticsearch index with mapping\nDESCRIPTION: This snippet creates an Elasticsearch index named 'wikipedia_vector_index' and defines the mappings for the fields within the index. It uses `dense_vector` for 'title_vector' and 'content_vector' fields, specifying the dimensions and similarity metric (cosine). It also defines mappings for 'text', 'title', 'url', and 'vector_id' fields.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nindex_mapping= {\n    \"properties\": {\n      \"title_vector\": {\n          \"type\": \"dense_vector\",\n          \"dims\": 1536,\n          \"index\": \"true\",\n          \"similarity\": \"cosine\"\n      },\n      \"content_vector\": {\n          \"type\": \"dense_vector\",\n          \"dims\": 1536,\n          \"index\": \"true\",\n          \"similarity\": \"cosine\"\n      },\n      \"text\": {\"type\": \"text\"},\n      \"title\": {\"type\": \"text\"},\n      \"url\": { \"type\": \"keyword\"},\n      \"vector_id\": {\"type\": \"long\"}\n      \n    }\n}\n\nclient.indices.create(index=\"wikipedia_vector_index\", mappings=index_mapping)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for RAG Fine-Tuning in Python\nDESCRIPTION: Installs a suite of necessary Python packages including pandas for data manipulation, openai for API calls, tqdm for progress bars, and other utilities such as tenacity for retrying failed calls, scikit-learn for metrics, and python-dotenv for environment variables. This ensures the environment has all prerequisites for data processing, model interaction, and visualization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install pandas openai tqdm tenacity scikit-learn tiktoken python-dotenv seaborn --upgrade --quiet\n```\n\n----------------------------------------\n\nTITLE: Create Vector Index Python\nDESCRIPTION: This code creates a custom index of type `org.apache.cassandra.index.sai.StorageAttachedIndex` on the `embedding_vector` column of the `philosophers_cql` table.  The `similarity_function` is set to `dot_product` for efficient search with unit-length vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```python\ncreate_vector_index_statement = f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS idx_embedding_vector\n    ON {keyspace}.philosophers_cql (embedding_vector)\n    USING 'org.apache.cassandra.index.sai.StorageAttachedIndex'\n    WITH OPTIONS = {{'similarity_function' : 'dot_product'}};\"\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\nsession.execute(create_vector_index_statement)\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Loading Dataset\nDESCRIPTION: Sets up the OpenAI client and loads the Amazon furniture dataset for processing. This prepares the environment for image analysis and caption generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom openai import OpenAI\n\n# Initializing OpenAI client - see https://platform.openai.com/docs/quickstart?context=python\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Plotting Cosine Similarity Distribution and Accuracy\nDESCRIPTION: This code snippet calculates and visualizes the distribution of cosine similarities and computes the accuracy of a simple rule based on the cosine similarity threshold. It defines a function `accuracy_and_se` to compute the accuracy and its standard error for predicting label=1 if the similarity is above a threshold, and it iterates over a range of thresholds. The distribution is visualized using a histogram, and the accuracy is printed for the training and test datasets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# calculate accuracy (and its standard error) of predicting label=1 if similarity>x\n# x is optimized by sweeping from -1 to 1 in steps of 0.01\ndef accuracy_and_se(cosine_similarity: float, labeled_similarity: int) -> Tuple[float]:\n    accuracies = []\n    for threshold_thousandths in range(-1000, 1000, 1):\n        threshold = threshold_thousandths / 1000\n        total = 0\n        correct = 0\n        for cs, ls in zip(cosine_similarity, labeled_similarity):\n            total += 1\n            if cs > threshold:\n                prediction = 1\n            else:\n                prediction = -1\n            if prediction == ls:\n                correct += 1\n        accuracy = correct / total\n        accuracies.append(accuracy)\n    a = max(accuracies)\n    n = len(cosine_similarity)\n    standard_error = (a * (1 - a) / n) ** 0.5  # standard error of binomial\n    return a, standard_error\n\n\n# check that training and test sets are balanced\npx.histogram(\n    df,\n    x=\"cosine_similarity\",\n    color=\"label\",\n    barmode=\"overlay\",\n    width=500,\n    facet_row=\"dataset\",\n).show()\n\nfor dataset in [\"train\", \"test\"]:\n    data = df[df[\"dataset\"] == dataset]\n    a, se = accuracy_and_se(data[\"cosine_similarity\"], data[\"label\"])\n    print(f\"{dataset} accuracy: {a:0.1%} Â± {1.96 * se:0.1%}\")\n```\n\n----------------------------------------\n\nTITLE: Code Interpreter File Inclusion in Markdown\nDESCRIPTION: Code snippet indicating that files created by Code Interpreter can now be included in POST requests, released on April 8th, 2024.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/release-notes.txt#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nFiles created by Code Interpreter can now be{\" \"}\n    included\n{\" \"}\nin POST requests\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Library (Python)\nDESCRIPTION: Imports the `openai` library, which is needed to interact with the OpenAI API for generating embeddings from text queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for Storing Images in Python\nDESCRIPTION: Creates a directory named 'imgs' to store generated images, using os.makedirs with exist_ok to avoid errors if the directory already exists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create imgs/ folder\nfolder_path = \"imgs\"\nos.makedirs(folder_path, exist_ok=True)\n```\n\n----------------------------------------\n\nTITLE: Converting Image to SVG using Potrace (Bash)\nDESCRIPTION: This command uses the Potrace tool to convert a bitmap image (cat.jpg) into a scalable vector graphic (cat.svg). Potrace traces bitmap images to create vector outlines, useful for tasks like converting DALLÂ·E-generated icons into SVG format. The `-s` flag specifies SVG output. Requires the Potrace tool to be installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/what_is_new_with_dalle_3.mdx#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npotrace -s cat.jpg -o cat.svg\n```\n\n----------------------------------------\n\nTITLE: Receiving OpenAI API Key Securely in Python\nDESCRIPTION: Prompts the user for their OpenAI API key using getpass for secure input. This key is necessary to authenticate requests to the OpenAI embeddings API and must be provisioned in the user's OpenAI dashboard.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")\n```\n\n----------------------------------------\n\nTITLE: Generating and saving code embeddings\nDESCRIPTION: This code snippet takes the extracted functions and generates embeddings for each function's code using the `text-embedding-3-small` model via the `get_embedding` function from `utils.embeddings_utils`. The filepath is converted to a relative path from the code root.  The resulting data, including the embeddings, is saved to a CSV file. Dependencies include pandas and the custom embeddings utility module.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import get_embedding\n\ndf = pd.DataFrame(all_funcs)\ndf['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\ndf['filepath'] = df['filepath'].map(lambda x: Path(x).relative_to(code_root))\ndf.to_csv(\"data/code_search_openai-python.csv\", index=False)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Saving Variations of Images\nDESCRIPTION: This code saves the generated image variations to the file system.  It iterates over the URLs for the generated images in `variation_response`, downloads the image content using `requests.get`, and then writes each image to a file in the image directory.  File names are generated programmatically, and the images are saved as png files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# save the images\nvariation_urls = [datum.url for datum in variation_response.data]  # extract URLs\nvariation_images = [requests.get(url).content for url in variation_urls]  # download images\nvariation_image_names = [f\"variation_image_{i}.png\" for i in range(len(variation_images))]  # create names\nvariation_image_filepaths = [os.path.join(image_dir, name) for name in variation_image_names]  # create filepaths\nfor image, filepath in zip(variation_images, variation_image_filepaths):  # loop through the variations\n    with open(filepath, \"wb\") as image_file:  # open the file\n        image_file.write(image)  # write the image to the file\n```\n\n----------------------------------------\n\nTITLE: Combining and Preparing Dataframes - Python\nDESCRIPTION: This snippet prepares the data for comparison between runs. It assigns a 'run' number and 'Evaluating Model' to the original `results_df` and  `results_2_df` DataFrames. Then it concatenates them using `pd.concat()` creating a combined dataframe, `run_df`, suitable for analysis. The resulting `run_df` combines the two sets of unit test results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresults_df['run'] = 1\nresults_df['Evaluating Model'] = 'gpt-4'\n\nresults_2_df['run'] = 2\nresults_2_df['Evaluating Model'] = 'gpt-4'\n\nrun_df = pd.concat([results_df,results_2_df])\nrun_df.head()\n```\n\n----------------------------------------\n\nTITLE: Connecting to Redis\nDESCRIPTION: Connects to a Redis instance using the redis-py client. Defines the Redis URL and pings the server to verify the connection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom redis import from_url\n\nREDIS_URL = 'redis://localhost:6379'\nclient = from_url(REDIS_URL)\nclient.ping()\n```\n\n----------------------------------------\n\nTITLE: POST Requests with Multiple Files in Markdown\nDESCRIPTION: Code snippet showing that POST requests can include up to ten files (including DALL-E generated images) from the conversation, released on March 15th, 2024.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/release-notes.txt#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nPOST requests can{\" \"}\n    include up to ten files\n{\" \"}\n(including DALL-E generated images) from the conversation\n```\n\n----------------------------------------\n\nTITLE: Setting up and running a data analysis agent loop in Python\nDESCRIPTION: Code that adds context to a data analysis agent and creates an interactive loop where users can ask questions about data. The agent dynamically generates Python code to analyze the data, which is then executed in an isolated environment for security.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/object_oriented_agentic_approach/Secure_code_interpreter_tool_for_LLM_agents.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The context is added to the agent's tool manager so that the tool manager can use the context to generate the code \n\ndata_analysis_agent.add_context(prompt)\ndata_analysis_agent.add_context(file_ingestion_agent_output)\n\nwhile True:\n\n    print(\"Type your question related to the data in the file. Type 'exit' to exit.\")\n    user_input = input(\"Type your question.\")\n\n    if user_input == \"exit\":\n        print(\"Exiting the application.\")\n        break\n\n    print(f\"User question: {user_input}\")\n\n    print(\"Generating dynamic tools and using code interpreter...\")\n    data_analysis_agent_output = data_analysis_agent.task(user_input)\n\n    print(\"Output...\")\n    print(data_analysis_agent_output)\n```\n\n----------------------------------------\n\nTITLE: Applying Punctuation Assistant to Transcript (Python)\nDESCRIPTION: Calls the `punctuation_assistant` function with the `ascii_transcript`. This sends the ASCII-only transcript to the OpenAI Chat API (using the function defined earlier) to add punctuation and formatting. The complete response object returned by the API call is stored in the `response` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Use punctuation assistant function\nresponse = punctuation_assistant(ascii_transcript)\n```\n\n----------------------------------------\n\nTITLE: Hybrid Query with Numeric Range Filtering\nDESCRIPTION: A hybrid search query for \"sandals\" that filters results to only include products from years 2011-2012, demonstrating how to combine vector search with numeric range filtering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# hybrid query for sandals in the product vector and only include results within the 2011-2012 year range\nresults = search_redis(redis_client,\n                       \"sandals\",\n                       vector_field=\"product_vector\",\n                       k=10,\n                       hybrid_fields='@year:[2011 2012]'\n                       )\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Importing Libraries in Python\nDESCRIPTION: Imports required libraries including `OpenAI`, `os`, `re`, `numpy`, `pandas`, `sklearn.cluster`, `matplotlib.pyplot`, `json`, and `matplotlib`. Initializes the OpenAI client using an API key retrieved from environment variables or a placeholder string. This client object is used for making API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport json\nimport matplotlib\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Finding Matching Items with RAG using Cosine Similarity in Python\nDESCRIPTION: This function `find_matching_items_with_rag` takes a DataFrame of items and a list of item descriptions as input. It generates embeddings for the input item descriptions, finds the most similar items based on cosine similarity using the `find_similar_items` function, and returns a list of similar items from the DataFrame. It depends on the `get_embeddings` function and the `find_similar_items` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef find_matching_items_with_rag(df_items, item_descs):\n   \"\"\"Take the input item descriptions and find the most similar items based on cosine similarity for each description.\"\"\"\n   \n   # Select the embeddings from the DataFrame.\n   embeddings = df_items['embeddings'].tolist()\n\n   \n   similar_items = []\n   for desc in item_descs:\n      \n      # Generate the embedding for the input item\n      input_embedding = get_embeddings([desc])\n    \n      # Find the most similar items based on cosine similarity\n      similar_indices = find_similar_items(input_embedding, embeddings, threshold=0.6)\n      similar_items += [df_items.iloc[i] for i in similar_indices]\n    \n   return similar_items\n```\n\n----------------------------------------\n\nTITLE: Generating Evaluation Questions for All PDF Files\nDESCRIPTION: Creates a dictionary mapping each PDF filename to a generated question that should be answerable only from that specific document.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Generate questions for each PDF and store in a dictionary\nquestions_dict = {}\nfor pdf_path in pdf_files:\n    questions = generate_questions(pdf_path)\n    questions_dict[os.path.basename(pdf_path)] = questions\n```\n\n----------------------------------------\n\nTITLE: Testing Description Generation on Sample Products\nDESCRIPTION: Processes a few example products through the description generation function, displaying the product title, URL, and the generated description.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor index, row in examples.iterrows():\n    print(f\"{row['title'][:50]}{'...' if len(row['title']) > 50 else ''} - {row['url']} :\\n\")\n    img_description = describe_image(row['primary_image'], row['title'])\n    print(f\"{img_description}\\n--------------------------\\n\")\n```\n\n----------------------------------------\n\nTITLE: Calling Functions based on OpenAI Response in JavaScript\nDESCRIPTION: This code block extracts the function name and arguments from OpenAI's response and dynamically calls the appropriate function. It checks if the response indicates a function call is needed (`finish_reason === \"tool_calls\"`) and then retrieves the function name and arguments. It then calls the function using the `availableTools` object and logs the function's response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { finish_reason, message } = response.choices[0];\n\nif (finish_reason === \"tool_calls\" && message.tool_calls) {\n  const functionName = message.tool_calls[0].function.name;\n  const functionToCall = availableTools[functionName];\n  const functionArgs = JSON.parse(message.tool_calls[0].function.arguments);\n  const functionArgsArr = Object.values(functionArgs);\n  const functionResponse = await functionToCall.apply(null, functionArgsArr);\n  console.log(functionResponse);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Key Environment Variable in Python\nDESCRIPTION: Optionally installs `python-dotenv` using pip and loads environment variables from a `.env` file using `dotenv.load_dotenv()`. It demonstrates retrieving the OpenAI API key from environment variables and includes commented-out code showing how to set it manually within the script using `os.environ`. This setup is crucial for authenticating requests to the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Optional: run to load environment variables from a .env file.\n# This is not required if you have exported your env variables in another way or if you set it manually\n!pip3 install python-dotenv\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Set the OpenAI API key env variable manually\n# os.environ[\"OPENAI_API_KEY\"] = \"<your_api_key>\"\n\n# print(os.environ[\"OPENAI_API_KEY\"])\n```\n\n----------------------------------------\n\nTITLE: Translating Non-English Audio with Whisper API in Node.js\nDESCRIPTION: This Node.js example translates non-English audio files into English using OpenAI's 'whisper-1' model via the translations API. The 'openai' and 'fs' packages are required. After reading the audio stream, it sends the request and prints the translated English text. Output is always English, irrespective of the input language.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_7\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n    const translation = await openai.audio.translations.create({\n        file: fs.createReadStream(\"/path/to/file/german.mp3\"),\n        model: \"whisper-1\",\n    });\n    console.log(translation.text);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Consequential Action Flag in OpenAPI\nDESCRIPTION: This snippet shows how to mark an endpoint as 'consequential' within the OpenAPI specification using the `x-openai-isConsequential` extension. When the field is `true`, the GPT prompts for confirmation before executing the action; when `false`, an 'always allow' button is shown; if absent, GET operations default to `false` and others to `true`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\npaths:\n  /todo:\n    get:\n      operationId: getTODOs\n      description: Fetches items in a TODO list from the API.\n      security: []\n    post:\n      operationId: updateTODOs\n      description: Mutates the TODO list.\n      x-openai-isConsequential: true\n\n```\n\n----------------------------------------\n\nTITLE: Pinecone Index Initialization\nDESCRIPTION: This code prepares for the Pinecone interaction.  It first retrieves an existing index (index_name) from Pinecone's configuration. It then defines a `document_id` prefix, which will be used later to create unique identifiers (pageId).  This setup readies the environment to perform operations like uploading and querying embeddings stored within the Pinecone index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# reload the index from Pinecone \nindex = pc.Index(index_name)\n\n# Create a document ID prefix \ndocument_id = 'WB_Report'\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Data for Embedding and Search\nDESCRIPTION: This code defines a list of dictionaries containing IDs and texts representing sample data to be embedded and stored in Pinecone. The data targets semantic search and retrieval tasks related to OpenAI and ChatGPT.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndata = [\n    {\"id\": \"vec1\", \"text\": \"OpenAI is a leading AI research organization focused on advancing artificial intelligence.\"},\n    {\"id\": \"vec2\", \"text\": \"The ChatGPT platform is renowned for its natural language processing capabilities.\"},\n    {\"id\": \"vec3\", \"text\": \"Many users leverage ChatGPT for tasks like creative writing, coding assistance, and customer support.\"},\n    {\"id\": \"vec4\", \"text\": \"OpenAI has revolutionized AI development with innovations like GPT-4 and its user-friendly APIs.\"},\n    {\"id\": \"vec5\", \"text\": \"ChatGPT makes AI-powered conversations accessible to millions, enhancing productivity and creativity.\"},\n    {\"id\": \"vec6\", \"text\": \"OpenAI was founded in December 2015 as an organization dedicated to advancing digital intelligence for the benefit of humanity.\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Example Inputs for Entity Extraction in Python\nDESCRIPTION: This snippet provides a list of dictionaries, each containing a sample user_input representing a product request and associated user context. These examples are used to demonstrate the entity extraction and parameterization capabilities of the system. No dependencies beyond Python built-in types are required. Inputs should follow this structure to align with the prompt context and search expectations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nexample_inputs = [\n    {\n        \"user_input\": \"I'm looking for a new coat. I'm always cold so please something warm! Ideally something that matches my eyes.\",\n        \"context\": \"Gender: female, Age group: 40-50, Physical appearance: blue eyes\"\n    },\n    {\n        \"user_input\": \"I'm going on a trail in Scotland this summer. It's goind to be rainy. Help me find something.\",\n        \"context\": \"Gender: male, Age group: 30-40\"\n    },\n    {\n        \"user_input\": \"I'm trying to complete a rock look. I'm missing shoes. Any suggestions?\",\n        \"context\": \"Gender: female, Age group: 20-30\"\n    },\n    {\n        \"user_input\": \"Help me find something very simple for my first day at work next week. Something casual and neutral.\",\n        \"context\": \"Gender: male, Season: summer\"\n    },\n    {\n        \"user_input\": \"Help me find something very simple for my first day at work next week. Something casual and neutral.\",\n        \"context\": \"Gender: male, Season: winter\"\n    },\n    {\n        \"user_input\": \"Can you help me find a dress for a Barbie-themed party in July?\",\n        \"context\": \"Gender: female, Age group: 20-30\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Preparing Fine-Tuning Dataset for Structured Information Extraction with JSONL\nDESCRIPTION: This snippet shows how to structure conversational training data in JSONL format to fine-tune a model for sports headline parsing tasks. Each example presents a system prompt about extracting structured fields (player, team, sport, gender), a user headline, and the assistant's JSON dictionary response. Ten or more diverse examples are recommended for effective model adjustment. No external dependencies other than JSONL formatting are required.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_16\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"messages\": [{\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \\\"player\\\" (full name), \\\"team\\\", \\\"sport\\\", and \\\"gender\\\".\"}, {\"role\": \"user\", \"content\": \"Sources: Colts grant RB Taylor OK to seek trade\"}, {\"role\": \"assistant\", \"content\": \"{\\\"player\\\": \\\"Jonathan Taylor\\\", \\\"team\\\": \\\"Colts\\\", \\\"sport\\\": \\\"football\\\", \\\"gender\\\": \\\"male\\\" }\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \\\"player\\\" (full name), \\\"team\\\", \\\"sport\\\", and \\\"gender\\\".\"}, {\"role\": \"user\", \"content\": \"OSU 'split down middle' on starting QB battle\"}, {\"role\": \"assistant\", \"content\": \"{\\\"player\\\": null, \\\"team\\\": \\\"OSU\\\", \\\"sport\\\": \\\"football\\\", \\\"gender\\\": null }\"}]}\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting up OpenAI Client - Python\nDESCRIPTION: Imports essential libraries like `numpy`, `json`, `os`, `pandas`, `itertools`, `time`, `base64`, `ast`, and modules from `openai` and `tenacity`. Initializes the OpenAI client using an API key obtained from the environment variables. The `OPENAI_BUILD_HOUR_KEY` environment variable must be set for this to work.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport json\nimport os\nfrom IPython.display import display\nimport pandas as pd\nfrom openai import OpenAI\nimport itertools\nimport time\nimport base64\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nfrom typing import Any, Dict, List, Generator\nimport ast\n\n%load_ext dotenv\n%dotenv\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_BUILD_HOUR_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Assistant with File Search Enabled Using cURL\nDESCRIPTION: Illustrates how to create an OpenAI assistant with file_search capability via a cURL command against the OpenAI REST API. It sets the assistant name, instructions, model, and tools as a JSON payload, including necessary headers for authentication and beta API usage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_2\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/assistants \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"name\": \"Financial Analyst Assistant\",\n    \"instructions\": \"You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.\",\n    \"tools\": [{\"type\": \"file_search\"}],\n    \"model\": \"gpt-4o\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Initializing RetrievalQA with LangChain and OpenAI GPT-3.5-Turbo in Python\nDESCRIPTION: Creates a question answering retrieval chain by combining LangChain's RetrievalQA with the OpenAI ChatOpenAI model (GPT-3.5-Turbo). The chain uses the Deep Lake vector store as a retriever to fetch relevant context passages which are then processed by the LLM. This setup enables natural language queries against the embedded dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\n\n# Re-load the vector store in case it's no longer initialized\n# db = DeepLake(dataset_path = dataset_path, embedding_function=embedding)\n\nqa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model='gpt-3.5-turbo'), chain_type=\"stuff\", retriever=db.as_retriever())\n```\n\n----------------------------------------\n\nTITLE: Setting Up SubQuestionQueryEngine for Multi-Document Compare-and-Contrast Queries in Python\nDESCRIPTION: Configures a SubQuestionQueryEngine by wrapping individual query engines (for Lyft and Uber 10-K indices) as QueryEngineTools each annotated with ToolMetadata describing their scope. This engine decomposes complex queries into sub-questions routed to each sub-engine, facilitating cross-document comparison and synthesis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine, \n        metadata=ToolMetadata(name='lyft_10k', description='Provides information about Lyft financials for year 2021')\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine, \n        metadata=ToolMetadata(name='uber_10k', description='Provides information about Uber financials for year 2021')\n    ),\n]\n\ns_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)\n```\n\n----------------------------------------\n\nTITLE: Displaying Embedding Result Shapes and Values - Python\nDESCRIPTION: This code outputs information about the returned embedding result from OpenAI. It prints the total number of embedded items, displays the start of the second embedding vector for preview, and confirms the embedding dimensionality. This helps verify successful embedding retrieval and expected dimensions (typically 1536).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"len(result.data)              = {len(result.data)}\")\nprint(f\"result.data[1].embedding      = {str(result.data[1].embedding)[:55]}...\")\nprint(f\"len(result.data[1].embedding) = {len(result.data[1].embedding)}\")\n\n```\n\n----------------------------------------\n\nTITLE: Printing JSON Data\nDESCRIPTION: These snippets simply print the first element of the `questions` and `answers` lists to the console, which are JSON objects. This displays the contents of the loaded data to inspect its format and structure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(questions[0])\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(answers[0])\n```\n\n----------------------------------------\n\nTITLE: Setting up LLM and Prompt Template for Quote Generation in Python\nDESCRIPTION: Defines the OpenAI chat completion model name (`gpt-3.5-turbo`) and sets up a string template for the prompt used to instruct the LLM to generate philosophical quotes based on a topic and provided examples.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncompletion_model_name = \"gpt-3.5-turbo\"\n\ngeneration_prompt_template = \"\"\"\"Generate a single short philosophical quote on the given topic,\nsimilar in spirit and form to the provided actual example quotes.\nDo not exceed 20-30 words in your quote.\n\nREFERENCE TOPIC: \"{topic}\"\n\nACTUAL EXAMPLES:\n{examples}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAPI Specification for OpenAI Documentation Search API\nDESCRIPTION: This Python code creates an OpenAPI 3.1.0 specification for a semantic search endpoint deployed on Google Cloud Functions. The specification defines a POST endpoint that accepts query parameters and returns relevant search results from OpenAI documentation. The code also copies the generated spec to the clipboard for easy pasting into a GPT Action configuration.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nspec = f\"\"\"\nopenapi: 3.1.0\ninfo:\n  title: OpenAI API documentation search\n  description: API to perform a semantic search over OpenAI APIs\n  version: 1.0.0\nservers:\n  - url: https://{region}-{project_id}.cloudfunctions.net\n    description: Main (production) server\npaths:\n  /openai_docs_search:\n    post:\n      operationId: openai_docs_search\n      summary: Perform a search\n      description: Returns search results for the given query parameters.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                query:\n                  type: string\n                  description: The search query string\n                top_k:\n                  type: integer\n                  description: Number of top results to return. Maximum is 3.\n                category:\n                  type: string\n                  description: The category to filter on, on top of similarity search (used for metadata filtering). Possible values are {categories}.\n      responses:\n        '200':\n          description: A JSON response with the search results\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  items:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        text:\n                          type: string\n                          example: \"Learn how to turn text into numbers, unlocking use cases like search...\"\n                        title:\n                          type: string\n                          example: \"embeddings.txt\"\n                        distance:\n                          type: number\n                          format: float\n                          example: 0.484939891778730\n                        category:\n                          type: string\n                          example: \"models\"\n\"\"\"\nprint(spec)\npyperclip.copy(spec)\nprint(\"OpenAPI spec copied to clipboard\")\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Podcast Transcripts for Vector Store in Python\nDESCRIPTION: Uses wget to download a ZIP file containing Stuff You Should Know podcast transcripts with precomputed OpenAI embeddings, then extracts the archive to a local directory. Dependencies: the wget Python package and network access. The expected output is a local directory with the unzipped data, which will be used to populate a vector database. Note: the file is large (~541MB) and may take time to download.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\n# Here is a URL to a zip archive containing the transcribed podcasts\n# Note that this data has already been split into chunks and embeddings from OpenAI's `text-embedding-3-small` embedding model are included\ncontent_url = 'https://cdn.openai.com/API/examples/data/sysk_podcast_transcripts_embedded.json.zip'\n\n# Download the file (it is ~541 MB so this will take some time)\nwget.download(content_url)\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating SQLite Tables - Python\nDESCRIPTION: This Python snippet defines a function `ingest_transformed_jsons` that ingests data from JSON files into an SQLite database. It connects to an SQLite database, creates four tables (Hotels, Invoices, Charges, and Taxes) if they don't exist, and then iterates through JSON files in a specified folder. For each file, it parses the JSON data and inserts the relevant information into the appropriate tables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport sqlite3\n\ndef ingest_transformed_jsons(json_folder_path, db_path):\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create necessary tables\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS Hotels (\n        hotel_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        name TEXT,\n        street TEXT,\n        city TEXT,\n        country TEXT,\n        postal_code TEXT,\n        phone TEXT,\n        fax TEXT,\n        email TEXT,\n        website TEXT\n    )\n    ''')\n\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS Invoices (\n        invoice_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        hotel_id INTEGER,\n        invoice_number TEXT,\n        reservation_number TEXT,\n        date TEXT,\n        room_number TEXT,\n        check_in_date TEXT,\n        check_out_date TEXT,\n        currency TEXT,\n        total_net REAL,\n        total_tax REAL,\n        total_gross REAL,\n        total_charge REAL,\n        total_credit REAL,\n        balance_due REAL,\n        guest_company TEXT,\n        guest_address TEXT,\n        guest_name TEXT,\n        FOREIGN KEY(hotel_id) REFERENCES Hotels(hotel_id)\n    )\n    ''')\n\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS Charges (\n        charge_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        invoice_id INTEGER,\n        date TEXT,\n        description TEXT,\n        charge REAL,\n        credit REAL,\n        FOREIGN KEY(invoice_id) REFERENCES Invoices(invoice_id)\n    )\n    ''')\n\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS Taxes (\n        tax_id INTEGER PRIMARY KEY AUTOINCREMENT,\n        invoice_id INTEGER,\n        tax_type TEXT,\n        tax_rate TEXT,\n        net_amount REAL,\n        tax_amount REAL,\n        gross_amount REAL,\n        FOREIGN KEY(invoice_id) REFERENCES Invoices(invoice_id)\n    )\n    ''')\n\n    # Loop over all JSON files in the specified folder\n    for filename in os.listdir(json_folder_path):\n        if filename.endswith(\".json\"):\n            file_path = os.path.join(json_folder_path, filename)\n\n            # Load the JSON data\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n\n            # Insert Hotel Information\n            cursor.execute('''\n            INSERT INTO Hotels (name, street, city, country, postal_code, phone, fax, email, website) \n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (\n                data[\"hotel_information\"][\"name\"],\n                data[\"hotel_information\"][\"address\"][\"street\"],\n                data[\"hotel_information\"][\"address\"][\"city\"],\n                data[\"hotel_information\"][\"address\"][\"country\"],\n                data[\"hotel_information\"][\"address\"][\"postal_code\"],\n                data[\"hotel_information\"][\"contact\"][\"phone\"],\n                data[\"hotel_information\"][\"contact\"][\"fax\"],\n                data[\"hotel_information\"][\"contact\"][\"email\"],\n                data[\"hotel_information\"][\"contact\"][\"website\"]\n            ))\n            hotel_id = cursor.lastrowid\n\n            # Insert Invoice Information\n            cursor.execute('''\n            INSERT INTO Invoices (hotel_id, invoice_number, reservation_number, date, room_number, check_in_date, check_out_date, currency, total_net, total_tax, total_gross, total_charge, total_credit, balance_due, guest_company, guest_address, guest_name)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (\n                hotel_id,\n                data[\"invoice_information\"][\"invoice_number\"],\n                data[\"invoice_information\"][\"reservation_number\"],\n                data[\"invoice_information\"][\"date\"],\n                data[\"invoice_information\"][\"room_number\"],\n                data[\"invoice_information\"][\"check_in_date\"],\n                data[\"invoice_information\"][\"check_out_date\"],\n                data[\"totals_summary\"][\"currency\"],\n                data[\"totals_summary\"][\"total_net\"],\n                data[\"totals_summary\"][\"total_tax\"],\n                data[\"totals_summary\"][\"total_gross\"],\n                data[\"totals_summary\"][\"total_charge\"],\n                data[\"totals_summary\"][\"total_credit\"],\n                data[\"totals_summary\"][\"balance_due\"],\n                data[\"guest_information\"][\"company\"],\n                data[\"guest_information\"][\"address\"],\n                data[\"guest_information\"][\"guest_name\"]\n            ))\n            invoice_id = cursor.lastrowid\n\n            # Insert Charges\n            for charge in data[\"charges\"]:\n                cursor.execute('''\n                INSERT INTO Charges (invoice_id, date, description, charge, credit) \n                VALUES (?, ?, ?, ?, ?)\n                ''', (\n                    invoice_id,\n                    charge[\"date\"],\n                    charge[\"description\"],\n                    charge[\"charge\"],\n                    charge[\"credit\"]\n                ))\n\n            # Insert Taxes\n            for tax in data[\"taxes\"]:\n                cursor.execute('''\n                INSERT INTO Taxes (invoice_id, tax_type, tax_rate, net_amount, tax_amount, gross_amount) \n                VALUES (?, ?, ?, ?, ?, ?)\n                ''', (\n                    invoice_id,\n                    tax[\"tax_type\"],\n                    tax[\"tax_rate\"],\n                    tax[\"net_amount\"],\n                    tax[\"tax_amount\"],\n                    tax[\"gross_amount\"]\n                ))\n\n    conn.commit()\n    conn.close()\n```\n\n----------------------------------------\n\nTITLE: Saving Text and Embeddings to CSV in Python\nDESCRIPTION: Saves the Wikipedia text chunks and their corresponding embeddings to a CSV file for later use in the RAG system. The file is stored at 'data/winter_olympics_2022.csv'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# save document chunks and embeddings\n\nSAVE_PATH = \"data/winter_olympics_2022.csv\"\n\ndf.to_csv(SAVE_PATH, index=False)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Astra DB Table and Indexes (Python/CQL)\nDESCRIPTION: Executes CQL commands to create a new table `philosophers_cql_partitioned` in Astra DB/Cassandra, partitioned by `author` and using `quote_id` for clustering, along with Storage-Attached Indexes (SAI) for vector similarity search on `embedding_vector` and searching within the `tags` set. Requires a database `session` and `keyspace` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\ncreate_table_p_statement = f\"\"\"CREATE TABLE IF NOT EXISTS {keyspace}.philosophers_cql_partitioned (\n    author TEXT,\n    quote_id UUID,\n    body TEXT,\n    embedding_vector VECTOR<FLOAT, 1536>,\n    tags SET<TEXT>,\n    PRIMARY KEY ( (author), quote_id )\n) WITH CLUSTERING ORDER BY (quote_id ASC);\"\"\"\n\nsession.execute(create_table_p_statement)\n\ncreate_vector_index_p_statement = f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS idx_embedding_vector_p\n    ON {keyspace}.philosophers_cql_partitioned (embedding_vector)\n    USING 'org.apache.cassandra.index.sai.StorageAttachedIndex'\n    WITH OPTIONS = {{'similarity_function' : 'dot_product'}};\n\"\"\"\nsession.execute(create_vector_index_p_statement)\n\ncreate_tags_index_p_statement = f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS idx_tags_p\n    ON {keyspace}.philosophers_cql_partitioned (VALUES(tags))\n    USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n\"\"\"\nsession.execute(create_tags_index_p_statement)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Images with GPT Image Edit in Python\nDESCRIPTION: Uses the OpenAI client's images.edit method to combine two input images according to the provided prompt, specifying the output dimensions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Generate the new image\nresult_edit = client.images.edit(\n    model=\"gpt-image-1\",\n    image=[img1,img2], \n    prompt=prompt_edit,\n    size=\"1024x1536\"\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying AWS SAM Application with Parameters from YAML in Bash\nDESCRIPTION: This Bash script snippet extracts parameters from the `env.yaml` environment file by converting the YAML to JSON, then mapping key-value pairs into CLI parameter overrides for the AWS SAM deployment command. It deploys the `template.yaml` SAM template to create AWS resources including Cognito User Pool, API Gateway, and Lambda function configured with Redshift connectivity. It requires the `jq` and `yq` tools.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nPARAM_FILE=\"env.yaml\"\nPARAMS=$(yq eval -o=json $PARAM_FILE | jq -r 'to_entries | map(\"\\(.key)=\\(.value|tostring)\") | join(\" \")')\nsam deploy --template-file template.yaml --stack-name redshift-middleware --capabilities CAPABILITY_IAM --parameter-overrides $PARAMS\n```\n\n----------------------------------------\n\nTITLE: Generating Unit Tests with Multistep GPT-3 Prompts in Python\nDESCRIPTION: Defines the unit_test_from_function function that uses a three-step prompting approach with OpenAI's GPT-3 to generate unit test code for a supplied Python function string. It first requests a detailed explanation of the function, then generates a test plan covering edge cases and diverse inputs using the specified unit test package (default pytest), followed by elaborating the plan if required. Finally, it prompts GPT-3 to produce a complete, commented unit test suite in Python. The function supports configurable parameters such as models for text and code generation, max token count, temperature, printing intermediate outputs, and retry logic for syntax errors in generated code. It returns the generated unit test suite as a string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef unit_test_from_function(\n    function_to_test: str,  # Python function to test, as a string\n    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n    text_model: str = \"gpt-3.5-turbo-instruct\",  # model used to generate text plans in steps 1, 2, and 2b\n    code_model: str = \"gpt-3.5-turbo-instruct\",  # if you don't have access to code models, you can use text models here instead\n    max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences\n    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n) -> str:\n    \"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"\n\n    # Step 1: Generate an explanation of the function\n\n    # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list\n    prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}\n\nIn this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n```python\n{function_to_test}\n```\n\nBefore writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n- First,\"\"\"\n    if print_text:\n        text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white\n        print(text_color_prefix + prompt_to_explain_the_function, end=\"\")  # end='' prevents a newline from being printed\n\n    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n    explanation_response = openai.Completion.create(\n        model=text_model,\n        prompt=prompt_to_explain_the_function,\n        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stream=True,\n    )\n    explanation_completion = \"\"\n    if print_text:\n        completion_color_prefix = \"\\033[92m\"  # green\n        print(completion_color_prefix, end=\"\")\n    for event in explanation_response:\n        event_text = event[\"choices\"][0][\"text\"]\n        explanation_completion += event_text\n        if print_text:\n            print(event_text, end=\"\")\n\n    # Step 2: Generate a plan to write a unit test\n\n    # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list\n    prompt_to_explain_a_plan = f\"\"\"\n    \nA good unit test suite should aim to:\n- Test the function's behavior for a wide range of possible inputs\n- Test edge cases that the author may not have foreseen\n- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n- Be easy to read and understand, with clean code and descriptive names\n- Be deterministic, so that the tests always pass or fail in the same way\n\n`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n\nFor this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n-\"\"\"\n    if print_text:\n        print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")\n\n    # append this planning prompt to the results from step 1\n    prior_text = prompt_to_explain_the_function + explanation_completion\n    full_plan_prompt = prior_text + prompt_to_explain_a_plan\n\n    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n    plan_response = openai.Completion.create(\n        model=text_model,\n        prompt=full_plan_prompt,\n        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stream=True,\n    )\n    plan_completion = \"\"\n    if print_text:\n        print(completion_color_prefix, end=\"\")\n    for event in plan_response:\n        event_text = event[\"choices\"][0][\"text\"]\n        plan_completion += event_text\n        if print_text:\n            print(event_text, end=\"\")\n\n    # Step 2b: If the plan is short, ask GPT-3 to elaborate further\n    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n    elaboration_needed = plan_completion.count(\"\\n-\") +1 < approx_min_cases_to_cover  # adds 1 because the first bullet is not counted\n    if elaboration_needed:\n        prompt_to_elaborate_on_the_plan = f\"\"\"\n\nIn addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n-\"\"\"\n        if print_text:\n            print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")\n\n        # append this elaboration prompt to the results from step 2\n        prior_text = full_plan_prompt + plan_completion\n        full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan\n\n        # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n        elaboration_response = openai.Completion.create(\n            model=text_model,\n            prompt=full_elaboration_prompt,\n            stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n            max_tokens=max_tokens,\n            temperature=temperature,\n            stream=True,\n        )\n        elaboration_completion = \"\"\n        if print_text:\n            print(completion_color_prefix, end=\"\")\n        for event in elaboration_response:\n            event_text = event[\"choices\"][0][\"text\"]\n            elaboration_completion += event_text\n            if print_text:\n                print(event_text, end=\"\")\n\n    # Step 3: Generate the unit test\n\n    # create a markdown-formatted prompt that asks GPT-3 to complete a unit test\n    starter_comment = \"\"\n    if unit_test_package == \"pytest\":\n        starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n    prompt_to_generate_the_unit_test = f\"\"\"\n\nBefore going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n```python\nimport {unit_test_package}  # used for our unit tests\n\n{function_to_test}\n\n#{starter_comment}\"\"\"\n    if print_text:\n        print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")\n\n    # append this unit test prompt to the results from step 3\n    if elaboration_needed:\n        prior_text = full_elaboration_prompt + elaboration_completion\n    else:\n        prior_text = full_plan_prompt + plan_completion\n    full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test\n\n    # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\n    unit_test_response = openai.Completion.create(\n        model=code_model,\n        prompt=full_unit_test_prompt,\n        stop=\"```\",\n        max_tokens=max_tokens,\n        temperature=temperature,\n        stream=True\n    )\n    unit_test_completion = \"\"\n    if print_text:\n        print(completion_color_prefix, end=\"\")\n    for event in unit_test_response:\n        event_text = event[\"choices\"][0][\"text\"]\n        unit_test_completion += event_text\n        if print_text:\n            print(event_text, end=\"\")\n\n    # check the output for errors\n    code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n    code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion\n    try:\n        ast.parse(code_output)\n    except SyntaxError as e:\n        print(f\"Syntax error in generated code: {e}\")\n        if reruns_if_fail > 0:\n            print(\"Rerunning...\")\n            return unit_test_from_function(\n                function_to_test=function_to_test,\n                unit_test_package=unit_test_package,\n                approx_min_cases_to_cover=approx_min_cases_to_cover,\n                print_text=print_text,\n                text_model=text_model,\n                code_model=code_model,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                reruns_if_fail=reruns_if_fail-1,  # decrement rerun counter when calling again\n            )\n\n    # return the unit test as a string\n    return unit_test_completion\n```\n\n----------------------------------------\n\nTITLE: Defining Hotel Invoice JSON Schema in Python\nDESCRIPTION: This snippet defines a sample JSON schema for hotel invoices as a Python list containing a dictionary structure. The schema captures nested invoice details such as hotel information, guest data, invoice metadata, individual charges, totals summary, and tax breakdowns. The structure is intended to serve as a template for transforming arbitrary invoice data into a consistent format suitable for downstream processing and ingestion into databases.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n[\n    {\n        \"hotel_information\": {\n            \"name\": \"string\",\n            \"address\": {\n                \"street\": \"string\",\n                \"city\": \"string\",\n                \"country\": \"string\",\n                \"postal_code\": \"string\"\n            },\n            \"contact\": {\n                \"phone\": \"string\",\n                \"fax\": \"string\",\n                \"email\": \"string\",\n                \"website\": \"string\"\n            }\n        },\n        \"guest_information\": {\n            \"company\": \"string\",\n            \"address\": \"string\",\n            \"guest_name\": \"string\"\n        },\n        \"invoice_information\": {\n            \"invoice_number\": \"string\",\n            \"reservation_number\": \"string\",\n            \"date\": \"YYYY-MM-DD\",  \n            \"room_number\": \"string\",\n            \"check_in_date\": \"YYYY-MM-DD\",  \n            \"check_out_date\": \"YYYY-MM-DD\"  \n        },\n        \"charges\": [\n            {\n                \"date\": \"YYYY-MM-DD\", \n                \"description\": \"string\",\n                \"charge\": \"number\",\n                \"credit\": \"number\"\n            }\n        ],\n        \"totals_summary\": {\n            \"currency\": \"string\",\n            \"total_net\": \"number\",\n            \"total_tax\": \"number\",\n            \"total_gross\": \"number\",\n            \"total_charge\": \"number\",\n            \"total_credit\": \"number\",\n            \"balance_due\": \"number\"\n        },\n        \"taxes\": [\n            {\n                \"tax_type\": \"string\",\n                \"tax_rate\": \"string\",\n                \"net_amount\": \"number\",\n                \"tax_amount\": \"number\",\n                \"gross_amount\": \"number\"\n            }\n        ]\n    }\n]\n\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Embeddings into DataFrame\nDESCRIPTION: Loads previously generated embeddings from a CSV file into a pandas DataFrame and converts the embedding strings into numpy arrays for further processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)\n```\n\n----------------------------------------\n\nTITLE: Requesting Text Output Format for Transcriptions using OpenAI API with cURL\nDESCRIPTION: This cURL command shows how to send the 'response_format=text' parameter with an audio file to the transcription API. All necessary headers, authentication, and form fields are provided as in previous cURL examples. Ensure the API key, audio file path, and parameter values are correct. Outputs plain text transcription.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_5\n\nLANGUAGE: curl\nCODE:\n```\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/transcriptions \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@/path/to/file/speech.mp3 \\\n  --form model=whisper-1 \\\n  --form response_format=text\n```\n\n----------------------------------------\n\nTITLE: Visualizing Histogram of Context Retrieval Ranks with Matplotlib in Python\nDESCRIPTION: This plotting code visualizes the distribution of answer ranks (between 0 and 29) for successfully retrieved contexts, using matplotlib's histogram. Axis labels and title enhance interpretability. It assumes a pandas DataFrame (out_expanded) with a 'rank' column and a pre-imported matplotlib.pyplot as plt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# plot a histogram, and add axis descriptions and title\nout_expanded[(out_expanded['rank'] >=0)&(out_expanded['rank'] <30)]['rank'].hist(bins=29)\nplt.xlabel('rank')\nplt.ylabel('count')\nplt.title('Histogram of ranks of retrieved paragraphs')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Writing Fine-tuning Data to JSONL and Uploading Files for OpenAI Fine-tuning in Python\nDESCRIPTION: Exports training and validation message samples to JSONL files using the defined utility function. Uploads these JSONL files to OpenAI's file storage service with purpose set to 'fine-tune'. This prepares the data for subsequent fine-tuning job creation. Requires a valid OpenAI client instance and local access to generated JSONL files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Write the train/validation split to jsonl files\ntrain_file_name, valid_file_name = \"transactions_grouped_train.jsonl\", \"transactions_grouped_valid.jsonl\"\nwrite_to_jsonl(train_df, train_file_name)\nwrite_to_jsonl(valid_df, valid_file_name)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Upload the files to OpenAI\ntrain_file = client.files.create(file=open(train_file_name, \"rb\"), purpose=\"fine-tune\")\nvalid_file = client.files.create(file=open(valid_file_name, \"rb\"), purpose=\"fine-tune\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating Neo4j Vector Index for Products in Python\nDESCRIPTION: Creates a vector index named 'products' in Neo4j using `Neo4jVector.from_existing_graph`. This index targets nodes with the label 'Product', generating embeddings from the 'name' and 'title' properties using the configured `OpenAIEmbeddings` instance. The generated embeddings are stored in the 'embedding' property of the 'Product' nodes. Requires an active Neo4j connection and configured OpenAI Embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nvector_index = Neo4jVector.from_existing_graph(\n    OpenAIEmbeddings(model=embeddings_model),\n    url=url,\n    username=username,\n    password=password,\n    index_name='products',\n    node_label=\"Product\",\n    text_node_properties=['name', 'title'],\n    embedding_node_property='embedding',\n)\n```\n\n----------------------------------------\n\nTITLE: Validate LLM Response Schema (Python)\nDESCRIPTION: Defines a unit test function `test_valid_schema` that checks if a given string can be successfully parsed into the predefined `LLMResponse` Pydantic model. It uses `model_validate_json` for parsing and validation. The function returns `True` if validation succeeds and `False` if a `pydantic.ValidationError` occurs, printing the error message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef test_valid_schema(content):\n    \"\"\"Tests whether the content provided can be parsed into our Pydantic model.\"\"\"\n    try:\n        LLMResponse.model_validate_json(content)\n        return True\n    # Catch pydantic's validation errors:\n    except pydantic.ValidationError as exc:\n        print(f\"ERROR: Invalid schema: {exc}\")\n        return False\n```\n\n----------------------------------------\n\nTITLE: Convert DataFrame to Bulk Actions for Elasticsearch\nDESCRIPTION: This snippet defines a function, `dataframe_to_bulk_actions`, that converts a Pandas DataFrame into a sequence of actions suitable for Elasticsearch's bulk API.  Each row in the DataFrame is converted into a dictionary representing a document to be indexed in the 'wikipedia_vector_index' index.  The vectors are loaded from JSON strings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef dataframe_to_bulk_actions(df):\n    for index, row in df.iterrows():\n        yield {\n            \"_index\": 'wikipedia_vector_index',\n            \"_id\": row['id'],\n            \"_source\": {\n                'url' : row[\"url\"],\n                'title' : row[\"title\"],\n                'text' : row[\"text\"],\n                'title_vector' : json.loads(row[\"title_vector\"]),\n                'content_vector' : json.loads(row[\"content_vector\"]),\n                'vector_id' : row[\"vector_id\"]\n            }\n        }\n```\n\n----------------------------------------\n\nTITLE: Setting W&B Base URL Environment Variable - Python\nDESCRIPTION: This snippet sets the WANDB_BASE_URL environment variable for the Weights & Biases API. It imports the weave and os libraries, defines the WANDB_BASE_URL, and sets it in the environment. This is a prerequisite for ensuring that all downstream weave and W&B code points to the correct cloud endpoint. No output is expected if set up correctly; failure to set this may result in API call failures.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport weave\nimport os\nWANDB_BASE_URL = \"https://api.wandb.ai\"\nos.environ[\"WANDB_BASE_URL\"] = WANDB_BASE_URL\n```\n\n----------------------------------------\n\nTITLE: Specifying Transcription Output Format with OpenAI Whisper API in Python\nDESCRIPTION: This code illustrates how to select a specific output format ('text') for a transcription response using the OpenAI Python library. The request adds the 'response_format' parameter set to 'text'. Dependencies are similar to previous Python examples, and output will be raw plain text. The main parameter change is the addition of 'response_format'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\naudio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\ntranscription = client.audio.transcriptions.create(\n  model=\"whisper-1\", \n  file=audio_file, \n  response_format=\"text\"\n)\nprint(transcription.text)\n```\n\n----------------------------------------\n\nTITLE: Prompting Forecast Function with Partial Information in Chat Completion API in Python\nDESCRIPTION: Re-initializes the conversation messages to ask about a weather forecast for multiple days at a specific location, but without specifying the number of days. The assistant responds with a clarification question, demonstrating the model's ability to identify missing required function arguments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\nmessages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\nchat_response = chat_completion_request(\n    messages, tools=tools\n)\nassistant_message = chat_response.choices[0].message\nmessages.append(assistant_message)\nassistant_message\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Logging in to W&B - Python\nDESCRIPTION: This snippet installs required libraries (weave, openai, tiktoken, wandb) and performs an interactive login to Weights & Biases using the wandb.login() API. Dependencies include pip and Python 3.x. Inputs and outputs are handled via notebook cells, with output indicating successful W&B authentication. Required if Weave or OpenAI packages are not already installed in the environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# if not already installed\n!pip install -qqq weave openai tiktoken wandb\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport wandb\nwandb.login()\n```\n\n----------------------------------------\n\nTITLE: Writing Task List to a JSONL File for Batch Upload\nDESCRIPTION: This code snippet writes the list of task dictionaries to a JSONL file line by line. It opens a file in write mode, serializes each task dictionary with json.dumps, and writes each as a separate line. Dependencies include the built-in json module and the tasks list from the previous step. This file serves as input for uploading to the OpenAI batch API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# Creating the file\n\nfile_name = \"data/batch_tasks_furniture.jsonl\"\n\nwith open(file_name, 'w') as file:\n    for obj in tasks:\n        file.write(json.dumps(obj) + '\\n')\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Azure OpenAI Whisper (Python)\nDESCRIPTION: This code transcribes an audio file using the Azure OpenAI Whisper model.  It opens the \"wikipediaOcelot.wav\" file in binary read mode, then calls `client.audio.transcriptions.create()` which sends the audio file and the deployment name to the OpenAI API.  Finally, it prints the transcribed text from the API response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntranscription = client.audio.transcriptions.create(\n    file=open(\"wikipediaOcelot.wav\", \"rb\"),\n    model=deployment,\n)\nprint(transcription.text)\n```\n\n----------------------------------------\n\nTITLE: Exploring Data with Value Counts\nDESCRIPTION: This code snippet explores the data within the created DataFrame using value counts. It counts the occurrences of titles within the DataFrame and displays the top entries. The code then checks the frequency of summer and winter Olympics titles by using the `.str.contains()` method on the 'title' column and counts occurrences. This provides insight into the content of the dataset, looking for patterns, and verifying if the data corresponds to the intial expectation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.title.value_counts().head()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.title.str.contains('Summer').value_counts()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.title.str.contains('Winter').value_counts()\n```\n\n----------------------------------------\n\nTITLE: Creating a Thread and Adding Messages in Node.js\nDESCRIPTION: This code snippet creates a thread and adds a user message to it, using the OpenAI API in Node.js. It employs `client.beta.threads.create()` to initialize the thread and `client.beta.threads.messages.create()` to append the message. It necessitates an OpenAI API key and the openai Node.js library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-function-calling.txt#_snippet_3\n\nLANGUAGE: node.js\nCODE:\n```\nconst thread = await client.beta.threads.create();\nconst message = client.beta.threads.messages.create(thread.id, {\n  role: \"user\",\n  content: \"What's the weather in San Francisco today and the likelihood it'll rain?\",\n});\n```\n\n----------------------------------------\n\nTITLE: Setting OPENAI_API_KEY Environment Variable - Windows CMD\nDESCRIPTION: This command uses 'setx' within Windows Command Prompt to assign the 'OPENAI_API_KEY' environment variable, enabling persistent access for the user session. The API key must replace 'your-api-key-here'. On execution, the environment variable will be available in new command prompt sessions. For permanent, system-wide environment variables, variables should be added via Windows system properties.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_4\n\nLANGUAGE: windows-cmd\nCODE:\n```\nsetx OPENAI_API_KEY \"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Testing Quote Search without Filters in Python\nDESCRIPTION: Demonstrates calling the `find_quote_and_author` function with only a query quote and the desired number of results, performing a pure vector similarity search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfind_quote_and_author(\"We struggle all our life for nothing\", 3)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs all the required dependencies for the project using npm.  It reads the `package.json` file in the project directory to determine the dependencies to install.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Demonstration Prompt Example (Few-Shot) (Text)\nDESCRIPTION: This code snippet demonstrates a demonstration-style prompt (few-shot learning). It provides the model with a couple of examples of quotes and their authors. Then it gives a new quote and asks the model to provide the corresponding author using the information learned from the demonstration examples.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nQuote:\nâ€œWhen the reasoning mind is forced to confront the impossible again and again, it has no choice but to adapt.â€\nâ€• N.K. Jemisin, The Fifth Season\nAuthor: N.K. Jemisin\n\nQuote:\nâ€œSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.â€\nâ€• Ted Chiang, Exhalation\nAuthor:\n```\n\n----------------------------------------\n\nTITLE: Creating Eval Object in OpenAI Evals - Python\nDESCRIPTION: Creates an eval with OpenAI's evals API, providing it with data source configuration and grading criteria. Stores eval id for later runs. Dependencies: openai.evals. Inputs: eval name, metadata, data_source_config, testing_criteria. Outputs: eval creation result with id. Limitations: Requires openai-evals installed and authenticated user.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/regression.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\neval_create_result = openai.evals.create(\n    name=\"Push Notification Summary Workflow\",\n    metadata={\n        \"description\": \"This eval checks if the push notification summary is correct.\",\n    },\n    data_source_config=data_source_config,\n    testing_criteria=[push_notification_grader],\n)\n\neval_id = eval_create_result.id\n```\n\n----------------------------------------\n\nTITLE: Preparing Example Queries for Multi-Tool Orchestration\nDESCRIPTION: Defines a set of sample queries that exercise different tools in the RAG system. This demonstrates how the system routes general knowledge questions to web search while directing medical queries to the Pinecone knowledge base.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Example queries that the model should route appropriately.\nqueries = [\n    {\"query\": \"Who won the cricket world cup in 1983?\"},\n    {\"query\": \"What is the most common cause of death in the United States according to the internet?\"},\n    {\"query\": (\"A 7-year-old boy with sickle cell disease is experiencing knee and hip pain, \"\n               \"has been admitted for pain crises in the past, and now walks with a limp. \"\n               \"His exam shows a normal, cool hip with decreased range of motion and pain with ambulation. \"\n               \"What is the most appropriate next step in management according to the internal knowledge base?\")}\n]\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompt - Python\nDESCRIPTION: This code defines a system prompt, `system_prompt_2`, to instruct an LLM to translate natural language requests into JSON objects containing two SQL queries. The first query is a CREATE statement, and the second is a SELECT statement. The prompt specifies that the SQL should be generated on a single line and without \\n characters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt_2 = \"\"\"Translate this natural language request into a JSON\\nobject containing two SQL queries.\\n\\nThe first query should be a CREATE statement for a table answering the user's\\nrequest, while the second should be a SELECT query answering their question.\\n\\nEnsure the SQL is always generated on one line, never use \\\\n to separate rows.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing necessary Python packages for CLIP, FAISS, and related libraries\nDESCRIPTION: Installs all required dependencies for the project including CLIP, Torch, Pillow, FAISS, NumPy, OpenAI, and other helper libraries. These packages enable image processing, model loading, similarity search, and API interactions necessary for the multimodal RAG system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install clip\n%pip install torch\n%pip install pillow\n%pip install faiss-cpu\n%pip install numpy\n%pip install git+https://github.com/openai/CLIP.git\n%pip install openai\n```\n\n----------------------------------------\n\nTITLE: Run kNN semantic search query on Elasticsearch\nDESCRIPTION: This snippet performs a k-nearest neighbors (kNN) search on the Elasticsearch index using the encoded question embedding. It specifies the field to search ('content_vector'), the query vector (obtained from the OpenAI embedding), the number of nearest neighbors to retrieve (k=10), and the number of candidate documents to consider (num_candidates=100). The results are then printed using the `pretty_response` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.search(\n  index = \"wikipedia_vector_index\",\n  knn={\n      \"field\": \"content_vector\",\n      \"query_vector\":  question_embedding[\"data\"][0][\"embedding\"],\n      \"k\": 10,\n      \"num_candidates\": 100\n    }\n)\npretty_response(response)\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Assistant with File Search Enabled in Node.js\nDESCRIPTION: Shows how to create an OpenAI assistant with the File Search tool enabled using the OpenAI Node.js SDK. The assistant uses a GPT-4o model with instructions to act as a financial analyst and includes the file_search tool. This snippet includes an async main function and correct usage of the beta assistants API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_1\n\nLANGUAGE: node.js\nCODE:\n```\nconst openai = new OpenAI();\n \nasync function main() {\n  const assistant = await openai.beta.assistants.create({\n    name: \"Financial Analyst Assistant\",\n    instructions: \"You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.\",\n    model: \"gpt-4o\",\n    tools: [{ type: \"file_search\" }],\n  });\n}\n \nmain();\n```\n\n----------------------------------------\n\nTITLE: OpenAI Image Variation with TypeScript Type Casting\nDESCRIPTION: This code snippet illustrates how to work around potential type mismatches when using the OpenAI image variation API with TypeScript. It explicitly casts the `ReadStream` returned by `fs.createReadStream` to `any` to satisfy the TypeScript compiler.  This allows the code to compile and run without type-related errors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images-node-tips.txt#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst openai = new OpenAI();\n\nasync function main() {\n  // Cast the ReadStream to `any` to appease the TypeScript compiler\n  const image = await openai.images.createVariation({\n    image: fs.createReadStream(\"image.png\") as any,\n  });\n\n  console.log(image.data);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies (Python)\nDESCRIPTION: Lists the necessary Python packages for running the notebook, including libraries for interacting with OpenAI, handling datasets (Hugging Face), data manipulation (Pandas, NumPy), schema validation (Pydantic), plotting (Matplotlib), environment variables (python-dotenv), and progress bars (tqdm). This is presented as a shell command typically used in notebook environments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n# Uncomment this to install all necessary dependencies\n# !pip install openai datasets pandas pydantic matplotlib python-dotenv numpy tqdm\n```\n\n----------------------------------------\n\nTITLE: Initiating Realtime Translation Connection in React/JS\nDESCRIPTION: Defines an asynchronous `connectConversation` function using React's `useCallback` hook. This function handles the process of starting a translation session: it sets loading state, initializes the `wavRecorder`, connects and sets up all Realtime clients via `connectAndSetupClients`, updates connection status, and includes error handling.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst connectConversation = useCallback(async () => {\n    try {\n        setIsLoading(true);\n        const wavRecorder = wavRecorderRef.current;\n        await wavRecorder.begin();\n        await connectAndSetupClients();\n        setIsConnected(true);\n    } catch (error) {\n        console.error('Error connecting to conversation:', error);\n    } finally {\n        setIsLoading(false);\n    }\n}, []);\n```\n\n----------------------------------------\n\nTITLE: Example Completions API Response JSON Structure\nDESCRIPTION: This JSON object illustrates the typical response format returned by the OpenAI Completions API. It includes the generated text in the 'choices' array, metadata like the 'finish_reason' and index, and 'usage' statistics detailing token counts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_13\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n    }\n  ],\n  \"created\": 1683130927,\n  \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n  \"model\": \"gpt-3.5-turbo-instruct\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 16,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 26\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating RediSearch HNSW Index and Waiting for Indexing Completion - Python\nDESCRIPTION: Initializes a RediSearch HNSW index if not already present, using specified fields and definitions. Waits in a loop using `time.sleep()` until indexing is complete, monitoring with the index info. Depends on redis-py client, pre-created vector fields, and RediSearch package. Ensures queries run only after full index build, which can be time-consuming on large document sets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport time\n# Check if index exists\nHNSW_INDEX_NAME = INDEX_NAME+ \"_HNSW\"\n\ntry:\n    redis_client.ft(HNSW_INDEX_NAME).info()\n    print(\"Index already exists\")\nexcept:\n    # Create RediSearch Index\n    redis_client.ft(HNSW_INDEX_NAME).create_index(\n        fields = fields,\n        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n    )\n\n# since RediSearch creates the index in the background for existing documents, we will wait until\n# indexing is complete before running our queries. Although this is not necessary for the first query,\n# some queries may take longer to run if the index is not fully built. In general, Redis will perform\n# best when adding new documents to existing indices rather than new indices on existing documents.\nwhile redis_client.ft(HNSW_INDEX_NAME).info()[\"indexing\"] == \"1\":\n    time.sleep(5)\n```\n\n----------------------------------------\n\nTITLE: Invoking the Language Model API in Python\nDESCRIPTION: Python code snippet showing how to call a client's `responses.create` method, likely interacting with a large language model API (e.g., OpenAI's GPT-4.1). It passes system prompts (`SYS_PROMPT_SWEBENCH`), the model name, the defined tool (`python_bash_patch_tool`), and user input containing a bug description.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nresponse = client.responses.create(\n    instructions=SYS_PROMPT_SWEBENCH,\n    model=\"gpt-4.1-2025-04-14\",\n    tools=[python_bash_patch_tool],\n    input=f\"Please answer the following question:\\nBug: Typerror...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Completions with OpenAI Node.js Library\nDESCRIPTION: Example Node.js script demonstrating how to use the installed OpenAI library to generate a chat completion using the `gpt-3.5-turbo` model. It initializes the client, sends a request to the chat completions endpoint with a system message, and logs the first choice from the response. Requires the OpenAI library installed and the API key configured via environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/node-setup.txt#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: \"system\", content: \"You are a helpful assistant.\" }],\n    model: \"gpt-3.5-turbo\",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Retrieving Single Object from Weaviate - Python\nDESCRIPTION: Executes a GET query to retrieve a single object from the 'Article' class, selecting specific properties. This is used to inspect a sample object and verify that data was imported correctly. Requires a connected Weaviate client instance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# Test one article has worked by checking one object\ntest_article = (\n    client.query\n    .get(\"Article\", [\"title\", \"url\", \"content\"])\n    .with_limit(1)\n    .do()\n)[\"data\"][\"Get\"][\"Article\"][0]\n\nprint(test_article['title'])\nprint(test_article['url'])\nprint(test_article['content'])\n```\n\n----------------------------------------\n\nTITLE: 3D scatter plot visualization of embeddings in Python\nDESCRIPTION: This snippet creates a 3D scatter plot of the reduced embeddings, categorizing samples by their labels. It uses matplotlib for plotting, with a color map for categories. Each category's points are plotted separately to assign labels and diverse colors, enhancing interpretability in the visualization. Dependencies include matplotlib and numpy.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n%matplotlib widget\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(projection='3d')\ncmap = plt.get_cmap(\"tab20\")\n\n# Plot each sample category individually such that we can set label name.\nfor i, cat in enumerate(categories):\n    sub_matrix = np.array(samples[samples[\"category\"] == cat][\"embed_vis\"].to_list())\n    x=sub_matrix[:, 0]\n    y=sub_matrix[:, 1]\n    z=sub_matrix[:, 2]\n    colors = [cmap(i/len(categories))] * len(sub_matrix)\n    ax.scatter(x, y, zs=z, zdir='z', c=colors, label=cat)\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.legend(bbox_to_anchor=(1.1, 1))\n```\n\n----------------------------------------\n\nTITLE: Batch Evaluation with BatchEvalRunner - Python\nDESCRIPTION: This code utilizes `BatchEvalRunner` to perform batched evaluation for both faithfulness and relevancy using the previously defined evaluators. It selects a subset of queries for evaluation and then calls `aevaluate_queries`. The results include scores for both faithfulness and relevancy. This allows for efficient evaluation of multiple queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.evaluation import BatchEvalRunner\n\n# Let's pick top 10 queries to do evaluation\nbatch_eval_queries = queries[:10]\n\n# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\nrunner = BatchEvalRunner(\n    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n    workers=8,\n)\n\n# Compute evaluation\neval_results = await runner.aevaluate_queries(\n    query_engine, queries=batch_eval_queries\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Financial Product Correction Assistant (Python)\nDESCRIPTION: Calls the `product_assistant` function, passing the `punctuated_transcript` as input. This step utilizes the specialized OpenAI Chat API call (via the defined function) to correct and format financial product terminology within the transcript. The API response object is stored in the `response` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Use product assistant function\nresponse = product_assistant(punctuated_transcript)\n```\n\n----------------------------------------\n\nTITLE: Preparing Evaluation Dataset for Vector Search Testing\nDESCRIPTION: Converts the questions dictionary into a dataframe format suitable for evaluation, linking each query to its source document ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrows = []\nfor filename, query in questions_dict.items():\n    rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n```\n\n----------------------------------------\n\nTITLE: Creating JSONL file writer utility function\nDESCRIPTION: Defines a function to write the prepared conversation data to JSONL files, which is the required format for OpenAI's fine-tuning API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef write_jsonl(data_list: list, filename: str) -> None:\n    with open(filename, \"w\") as out:\n        for ddict in data_list:\n            jout = json.dumps(ddict) + \"\\n\"\n            out.write(jout)\n```\n\n----------------------------------------\n\nTITLE: Splitting Data for Training and Testing with Scikit-Learn in Python\nDESCRIPTION: This snippet uses train_test_split from scikit-learn to partition embedding features and target variables into training and testing sets for downstream regression or classification tasks. It uses a random seed for reproducibility and allocates 20% data for testing. Required dependencies: scikit-learn, pandas; expected input: df with 'ada_embedding' and 'Score' columns; outputs: X_train, X_test, y_train, y_test numpy arrays or lists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    list(df.ada_embedding.values),\n    df.Score,\n    test_size = 0.2,\n    random_state=42\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Single Query Processing (Python)\nDESCRIPTION: This small snippet demonstrates how to call the `process_query` function for the first entry in the `rows` dataset. It serves as a quick test to verify the function's execution and inspect the results for a single example before processing the entire dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprocess_query(rows[0])\n```\n\n----------------------------------------\n\nTITLE: Exporting Environment Variables for GCP Function Deployment in Python\nDESCRIPTION: Creates a dictionary of environment variables needed for a Google Cloud Function deployment related to vector search and embedding generation. These include OpenAI API key, embeddings model, project ID, dataset, and table IDs. The snippet writes these variables into a YAML file ('env.yml') for use during function deployment. It uses the PyYAML library to serialize the dictionary in a human-readable format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Create a dictionary to store the environment variables (they were used previously and are just retrieved)\nenv_variables = {\n    'OPENAI_API_KEY': openai_api_key,\n    'EMBEDDINGS_MODEL': embeddings_model,\n    'PROJECT_ID': project_id,\n    'DATASET_ID': raw_dataset_id,\n    'TABLE_ID': raw_table_id\n}\n\n# Write the environment variables to a YAML file\nwith open('env.yml', 'w') as yaml_file:\n    yaml.dump(env_variables, yaml_file, default_flow_style=False)\n\nprint(\"env.yml file created successfully.\")\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset into Training and Testing Sets using Scikit-learn in Python\nDESCRIPTION: Utilizes the `train_test_split` function from the scikit-learn library to divide the loaded Q&A DataFrame (`df`) into separate training (`train_df`) and testing (`test_df`) sets. A test size of 20% is specified, and a fixed random state ensures reproducibility. The lengths of the resulting training and testing DataFrames are printed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nlen(train_df), len(test_df)\n```\n\n----------------------------------------\n\nTITLE: Printing Calculated Evaluation Metrics (Python)\nDESCRIPTION: This snippet outputs the final calculated retrieval metrics to the console. It displays the Recall@k, Precision@k, Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP) scores, formatted to four decimal places, for the evaluated dataset using the specified value of `k`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Print the metrics with k\nprint(f\"Metrics at k={k}:\")\nprint(f\"Recall@{k}: {recall_at_k:.4f}\")\nprint(f\"Precision@{k}: {precision_at_k:.4f}\")\nprint(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\nprint(f\"Mean Average Precision (MAP): {map_score:.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages with pip in Python\nDESCRIPTION: Installs the necessary Python dependencies including AstraPy (for DataStax Astra DB connection), OpenAI SDK (v1.0+), and the Hugging Face datasets library. These must be installed prior to running any code in the notebook. No external configuration is needed beyond basic internet access and pip.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --quiet \"astrapy>=0.6.0\" \"openai>=1.0.0\" datasets\n```\n\n----------------------------------------\n\nTITLE: Analyzing OpenAI Fine-Tuning Training Results (CSV)\nDESCRIPTION: This CSV snippet shows the format of the results file generated after an OpenAI fine-tuning job completes. Each row corresponds to a training step, logging the step number, training loss, training accuracy (if applicable), validation loss, and validation mean token accuracy, allowing for analysis of the model's learning progress over time.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_12\n\nLANGUAGE: csv\nCODE:\n```\nstep,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy\n1,1.52347,0.0,,\n2,0.57719,0.0,,\n3,3.63525,0.0,,\n4,1.72257,0.0,,\n5,1.52379,0.0,,\n```\n\n----------------------------------------\n\nTITLE: Loading Public Transaction Dataset Using Pandas in Python\nDESCRIPTION: Loads a public dataset of transactions over Â£25k from a CSV file using pandas with unicode escape encoding. Prints the total number of transactions and displays the first few records to inspect the structure and contents. The dataset includes supplier names, descriptions, and transaction values in GBP, serving as the raw data source for classification tasks.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntransactions = pd.read_csv('./data/25000_spend_dataset_current.csv', encoding= 'unicode_escape')\nprint(f\"Number of transactions: {len(transactions)}\")\nprint(transactions.head())\n\n```\n\n----------------------------------------\n\nTITLE: Question Answering Using Context in Prompts\nDESCRIPTION: Demonstrates a ChatGPT query that includes external context for answering questions about specific topics not in the model's training data. This approach provides more accurate responses but uses more tokens.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquery = f\"\"\"Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n\nArticle:\n\\\"\\\"\\\"\n{wikipedia_article_on_curling}\n\\\"\\\"\\\"\n\nQuestion: Which athletes won the gold medal in curling at the 2022 Winter Olympics?\"\"\"\n\nresponse = client.chat.completions.create(\n    messages=[\n        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},\n        {'role': 'user', 'content': query},\n    ],\n    model=GPT_MODEL,\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Setting GPT Model in Python\nDESCRIPTION: Imports necessary modules including JSON, the OpenAI SDK client, the tenacity retry decorator, and termcolor for colored terminal output. It initializes a client instance for OpenAI and sets a GPT model identifier to be used throughout subsequent API calls. This setup is mandatory for interfacing with OpenAI's Chat Completions API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom openai import OpenAI\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nfrom termcolor import colored  \n\nGPT_MODEL = \"gpt-4o\"\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: This code snippet uses pip to install the necessary Python packages for the project, including openai, pinecone-client, pandas, typing, tqdm, langchain, and wget. These packages provide functionalities for interacting with OpenAI models, Pinecone vector database, data manipulation, type hinting, progress bars, LangChain framework, and downloading files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install openai\n!pip install pinecone-client\n!pip install pandas\n!pip install typing\n!pip install tqdm\n!pip install langchain\n!pip install wget\n```\n\n----------------------------------------\n\nTITLE: Creating an Assistant with Code Interpreter Tool\nDESCRIPTION: This snippet shows how to create an Assistant using the OpenAI Assistants API with the `code_interpreter` tool enabled. It includes setting the assistant's name, description, model, tools, and associating uploaded files as resources. The code provides examples in Python, Node.js, and cURL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nassistant = client.beta.assistants.create(\n  name=\"Data visualizer\",\n  description=\"You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.\",\n  model=\"gpt-4o\",\n  tools=[{\"type\": \"code_interpreter\"}],\n  tool_resources={\n    \"code_interpreter\": {\n      \"file_ids\": [file.id]\n    }\n  }\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst assistant = await openai.beta.assistants.create({\n  name: \"Data visualizer\",\n  description: \"You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.\",\n  model: \"gpt-4o\",\n  tools: [{\"type\": \"code_interpreter\"}],\n  tool_resources: {\n    \"code_interpreter\": {\n      \"file_ids\": [file.id]\n    }\n  }\n});\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/assistants \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"name\": \"Data visualizer\",\n    \"description\": \"You are great at creating beautiful data visualizations. You analyze data present in .csv files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed.\",\n    \"model\": \"gpt-4o\",\n    \"tools\": [{\"type\": \"code_interpreter\"}],\n    \"tool_resources\": {\n      \"code_interpreter\": {\n        \"file_ids\": [\"file-BK7bzQj3FfZFXr7DbL6xJwfo\"]\n      }\n    }\n  }'\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tair Database Connection URL\nDESCRIPTION: Securely captures the Tair database connection URL using getpass. The URL follows Redis connection string format and is needed to connect to the vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# The format of url: redis://[[username]:[password]]@localhost:6379/0\nTAIR_URL = getpass.getpass(\"Input your tair url:\")\n```\n\n----------------------------------------\n\nTITLE: Generating Quote with Author Constraint (Python)\nDESCRIPTION: Shows how to use the `generate_quote` function, providing both a topic ('animals') and an optional `author` parameter ('schopenhauer') to influence the generated quote based on a specific philosopher's style or context.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nq_topic = generate_quote(\"animals\", author=\"schopenhauer\")\nprint(\"\\nA new generated quote:\")\nprint(q_topic)\n```\n\n----------------------------------------\n\nTITLE: Enabling Row Level Security on Documents Table in Postgres SQL\nDESCRIPTION: Enables row level security on the documents table to enforce fine-grained access control for records via Supabase's auto-generated REST API. This SQL statement should be executed after table creation and is important for restricting unauthorized data access, especially in production. It does not take any parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nalter table documents enable row level security;\n```\n\n----------------------------------------\n\nTITLE: Generating and Using GPT-Created Prompts to Influence Whisper Output in Python\nDESCRIPTION: Shows how to create custom fictitious prompts using GPTâ€”for example, enforcing ellipsis use or dialectâ€”then apply these prompts to Whisper transcription. The workflow involves generating the prompt, inspecting it with print, and passing it to the transcription function. Requires the 'fictitious_prompt_from_instruction' function, GPT API access, and prepared audio files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# ellipses example\nprompt = fictitious_prompt_from_instruction(\"Instead of periods, end every sentence with elipses.\")\nprint(prompt)\n```\n\nLANGUAGE: Python\nCODE:\n```\ntranscribe(up_first_filepath, prompt=prompt)\n```\n\nLANGUAGE: Python\nCODE:\n```\n# southern accent example\nprompt = fictitious_prompt_from_instruction(\"Write in a deep, heavy, Southern accent.\")\nprint(prompt)\ntranscribe(up_first_filepath, prompt=prompt)\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI Client with Azure AD (Python)\nDESCRIPTION: Configures the `openai.AzureOpenAI` client instance to use Azure Active Directory authentication. It utilizes `DefaultAzureCredential` and `get_bearer_token_provider` for automatic token management and authentication against the Cognitive Services scope. This block is conditional based on the `use_azure_active_directory` flag.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\nif use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Reading CSV File with Multiline Fields using Pandas in Python\nDESCRIPTION: Reads a CSV file containing embedded data using pandas, leveraging the 'python' engine to properly handle multiline fields with quoting. This prepares the DataFrame for further processing or insertion into BigQuery. The snippet assumes the CSV file is located relative to the script and contains quoted multiline text fields.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncsv_file_path = \"../embedded_data.csv\"\ndf = pd.read_csv(csv_file_path, engine='python', quotechar='\"', quoting=1)\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Create Vector Table in CQL Python\nDESCRIPTION: This snippet defines and executes a CQL `CREATE TABLE` statement to create a table named `philosophers_cql` in the specified keyspace.  The table includes columns for `quote_id`, `body`, `embedding_vector` (of type VECTOR<FLOAT, 1536>), `author`, and `tags`.  `quote_id` is the primary key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\ncreate_table_statement = f\"\"\"CREATE TABLE IF NOT EXISTS {keyspace}.philosophers_cql (\n    quote_id UUID PRIMARY KEY,\n    body TEXT,\n    embedding_vector VECTOR<FLOAT, 1536>,\n    author TEXT,\n    tags SET<TEXT>\n);\"\"\"\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\nsession.execute(create_table_statement)\n```\n```\n\n----------------------------------------\n\nTITLE: Querying Image URL with OpenAI Vision (Python)\nDESCRIPTION: Demonstrates sending an image URL to the OpenAI chat completions API using the Python library. It includes a text prompt and the image URL in the user message content, specifying the 'gpt-4o' model. Requires the OpenAI Python library and an API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n\nprint(response.choices[0])\n```\n\n----------------------------------------\n\nTITLE: Executing Document Indexing in Redis (Python)\nDESCRIPTION: This code calls the `index_documents` helper function to populate the Redis index with the loaded data. It passes the active Redis client, the document key prefix, and the pandas DataFrame containing the pre-embedded data. After execution, it prints the total number of keys present in the main Redis database (db0) as a confirmation of the documents loaded.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nindex_documents(redis_client, PREFIX, data)\nprint(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")\n```\n\n----------------------------------------\n\nTITLE: Generating and Storing Embeddings for Transactions in Python\nDESCRIPTION: Defines an output path for embedding results. Calls a utility function 'get_embedding' to generate vector embeddings for the combined text features of each transaction, storing the results in two identical columns 'babbage_similarity' and 'babbage_search'. Saves the dataframe with embeddings to CSV for later use. This process requires an embedding function compatible with OpenAI embedding models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nembedding_path = './data/transactions_with_embeddings_100.csv'\n\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.embeddings_utils import get_embedding\ndf['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x))\ndf['babbage_search'] = df.combined.apply(lambda x: get_embedding(x))\ndf.to_csv(embedding_path)\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GPT-3.5 Failure on a Simple Math Problem\nDESCRIPTION: This snippet shows an initial prompt given to gpt-3.5-turbo-instruct asking a math word problem, followed by the model's incorrect answer. This illustrates how tasks that might seem simple can lead to incorrect outputs if the model doesn't have sufficient 'space' or guidance to reason through the steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_0\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nQ: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA:\n```\n\nLANGUAGE: gpt-3.5-turbo-instruct\nCODE:\n```\nThere are 8 blue golf balls.\n```\n\n----------------------------------------\n\nTITLE: Evaluating/Suggesting Thesis Titles (Prompt)\nDESCRIPTION: Demonstrates structuring input using explicit labels (\"Abstract:\", \"Title:\") to provide distinct pieces of information. The system evaluates a provided thesis abstract and title, suggesting alternatives if the original title doesn't meet specified criteria. Requires a thesis abstract and its title as input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_2\n\nLANGUAGE: Prompt\nCODE:\n```\nSYSTEM: You will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\n\nUSER: Abstract: insert abstract here\n\nTitle: insert title here\n```\n\n----------------------------------------\n\nTITLE: Choosing the detail level for image analysis with GPT-4o\nDESCRIPTION: This code demonstrates how to specify the detail level (high or low) when sending an image to GPT-4o for analysis. The detail parameter controls the resolution and token consumption of the image processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/vision.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"What's in this image?\"},\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n            \"detail\": \"high\"\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What's in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n              \"detail\": \"high\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          { type: \"text\", text: \"What's in this image?\" },\n          {\n            type: \"image_url\",\n            image_url: {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n              \"detail\": \"low\"\n            },\n          },\n        ],\n      },\n    ],\n  });\n  console.log(response.choices[0]);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning training example\nDESCRIPTION: This chat example demonstrates a training example used for fine-tuning a language model to correct Icelandic sentences. It provides a SYSTEM instruction, a USER input with an Icelandic sentence potentially containing errors, and the expected ASSISTANT output with the corrected sentence.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/optimizing-llm-accuracy.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# One training example\nSYSTEM: The following sentences contain Icelandic sentences which may include errors. Please correct these errors using as few word changes as possible.\nUSER: \"HiÃ° sameinaÃ°a fyrirtÃ¦ki verÃ°ur einn af stÃ¦rstu bÃ­laframleiÃ°endum Ã­ heiminum.\"\nASSISTANT: \"HiÃ° sameinaÃ°a fyrirtÃ¦ki verÃ°ur einn af stÃ¦rstu bÃ­laframleiÃ°endum heims.\"\n```\n\n----------------------------------------\n\nTITLE: Deploying GCP Function (gcloud CLI)\nDESCRIPTION: Deploys the Node.js Google Cloud Function to the cloud environment using the `gcloud` command-line interface. This command specifies the function name, runtime, region, source code location, entry point function, HTTP trigger type, and initially permits unauthenticated access for testing purposes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngcloud functions deploy gcp-function-for-chatgpt \\\n  --gen2 \\\n  --runtime=nodejs20 \\\n  --region=us-central1 \\\n  --source=. \\\n  --entry-point=executeGCPFunction \\\n  --trigger-http \\\n  --allow-unauthenticated\n```\n\n----------------------------------------\n\nTITLE: Applying Commit Changes to Filesystem in Python\nDESCRIPTION: Applies the file changes described in a `Commit` object to the filesystem using provided functions for writing (`write_fn`) and removing (`remove_fn`) files. Handles file additions, deletions, updates, and moves (renames).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef apply_commit(\n    commit: Commit,\n    write_fn: Callable[[str, str], None],\n    remove_fn: Callable[[str], None],\n) -> None:\n    for path, change in commit.changes.items():\n        if change.type is ActionType.DELETE:\n            remove_fn(path)\n        elif change.type is ActionType.ADD:\n            if change.new_content is None:\n                raise DiffError(f\"ADD change for {path} has no content\")\n            write_fn(path, change.new_content)\n        elif change.type is ActionType.UPDATE:\n            if change.new_content is None:\n                raise DiffError(f\"UPDATE change for {path} has no new content\")\n            target = change.move_path or path\n            write_fn(target, change.new_content)\n            if change.move_path:\n                remove_fn(path)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: Imports required libraries (`openai`, `os`, `json`, `time`), fetches the OpenAI API key from the environment variable `OPENAI_API_KEY`, and initializes the `OpenAI` client instance associated with a specific organization ID. This client object is necessary for making subsequent API calls to OpenAI.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nimport json \nimport time\n\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(organization='org-l89177bnhkme4a44292n5r3j', api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Trimming Leading Silence and Getting Trimmed File Path (Python)\nDESCRIPTION: Calls the `trim_start` function with the original audio file path (`earnings_call_filepath`). This invocation captures both return values: the trimmed `AudioSegment` object into `trimmed_audio` and the file path of the newly created trimmed audio file into `trimmed_filename`. This ensures both the audio data and its location are available for subsequent steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrimmed_audio, trimmed_filename = trim_start(earnings_call_filepath)\n```\n\n----------------------------------------\n\nTITLE: Example: Generating Tool Schema from a Python Function\nDESCRIPTION: Shows an example usage of the `function_to_schema` utility. It defines a `sample_function` with various parameters (including type hints and default values) and a docstring, calls `function_to_schema` on it, and prints the resulting JSON schema using `json.dumps` with indentation for readability.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef sample_function(param_1, param_2, the_third_one: int, some_optional=\"John Doe\"):\n    \"\"\"\n    This is my docstring. Call this function when you want.\n    \"\"\"\n    print(\"Hello, world\")\n\nschema =  function_to_schema(sample_function)\nprint(json.dumps(schema, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Downloading Data with wget\nDESCRIPTION: This snippet downloads two JSON files, `questions.json` and `answers.json`, from a Google Cloud Storage bucket using `wget`.  These files contain the question and answer data used by the question answering system. The `wget.download` function is used to retrieve the data, saving it locally.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\n# All the examples come from https://ai.google.com/research/NaturalQuestions\n# This is a sample of the training set that we download and extract for some\n# further processing.\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/questions.json\")\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/answers.json\")\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Embedded Data\nDESCRIPTION: Converts string representations of vectors back to lists and ensures vector IDs are strings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Image Captioning with GPT-4 Vision Model\nDESCRIPTION: Sets a system prompt instructing the model to generate short, descriptive captions for images of furniture items, focusing on relevant features, and requesting concise output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncaption_system_prompt = '''\nYour goal is to generate short, descriptive captions for images of items.\nYou will be provided with an item image and the name of that item and you will output a caption that captures the most important information about the item.\nIf there are multiple items depicted, refer to the name provided to understand which item you should describe.\nYour generated caption should be short (1 sentence), and include only the most important information about the item.\nThe most important information could be: the type of item, the style (if mentioned), the material or color if especially relevant and/or any distinctive features.\nKeep it short and to the point.\n'''\n\n```\n\n----------------------------------------\n\nTITLE: Batch Indexing Large DataFrames Into Elasticsearch Using Bulk API in Python\nDESCRIPTION: Indexes rows from the DataFrame into Elasticsearch in batches of 100 using the helper.bulk function. Ensures efficient memory and network usage for large datasets. Assumes the existence of the target index, initialized Elasticsearch client, and the 'dataframe_to_bulk_actions' generator.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nstart = 0\nend = len(wikipedia_dataframe)\nbatch_size = 100\nfor batch_start in range(start, end, batch_size):\n    batch_end = min(batch_start + batch_size, end)\n    batch_dataframe = wikipedia_dataframe.iloc[batch_start:batch_end]\n    actions = dataframe_to_bulk_actions(batch_dataframe)\n    helpers.bulk(client, actions)\n```\n\n----------------------------------------\n\nTITLE: Defining Audio Transcription with Custom Prompt Using Whisper in Python\nDESCRIPTION: Defines a helper function to transcribe audio files using OpenAI's Whisper API and an optional style-guiding prompt. Requires the OpenAI Python client and valid API credentials. Takes a path to an audio file and a prompt string as inputs, returning the resulting transcript text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# define a wrapper function for seeing how prompts affect transcriptions\ndef transcribe(audio_filepath, prompt: str) -> str:\n    \"\"\"Given a prompt, transcribe the audio file.\"\"\"\n    transcript = client.audio.transcriptions.create(\n        file=open(audio_filepath, \"rb\"),\n        model=\"whisper-1\",\n        prompt=prompt,\n    )\n    return transcript.text\n\n```\n\n----------------------------------------\n\nTITLE: Preparing Tokens and Token IDs for Relevance Classification - Python\nDESCRIPTION: Defines binary classification tokens (' Yes', ' No') and obtains their respective token IDs for the chosen GPT model using tiktoken's encoding_for_model. The output is a tuple of token IDs to use with logit bias during model inference. Ensure tiktoken library is installed and the correct model string is used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntokens = [\" Yes\", \" No\"]\ntokenizer = tiktoken.encoding_for_model(OPENAI_MODEL)\nids = [tokenizer.encode(token) for token in tokens]\nids[0], ids[1]\n```\n\n----------------------------------------\n\nTITLE: Defining an 'ask' function to query GPT with search results - Python\nDESCRIPTION: This snippet defines a function `ask` that answers a query using GPT and a DataFrame of relevant texts and embeddings. It takes a query string, a DataFrame (defaulting to `df`), the model name (defaulting to `GPT_MODELS[0]`), a token budget (defaulting to 4096 - 500), and a boolean flag to print the message (defaulting to False). It constructs a message using the `query_message` function, sends the message to GPT via the OpenAI API, and returns GPT's answer.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef ask(\n    query: str,\n    df: pd.DataFrame = df,\n    model: str = GPT_MODELS[0],\n    token_budget: int = 4096 - 500,\n    print_message: bool = False,\n) -> str:\n    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n    message = query_message(query, df, model=model, token_budget=token_budget)\n    if print_message:\n        print(message)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n        {\"role\": \"user\", \"content\": message},\n    ]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0\n    )\n    response_message = response.choices[0].message.content\n    return response_message\n```\n\n----------------------------------------\n\nTITLE: Executing Qdrant Search using Content Vector\nDESCRIPTION: This Python code calls the `query_qdrant` function with a different sample query ('Famous battles in Scottish history') and explicitly specifies `vector_name='content'`. It then iterates through the results, printing the title and score, demonstrating search based on the content embedding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# This time we'll query using content vector\nquery_results = query_qdrant(\"Famous battles in Scottish history\", \"Articles\", \"content\")\nfor i, article in enumerate(query_results):\n    print(f\"{i + 1}. {article.payload['title']} (Score: {round(article.score, 3)})\")\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text to Paragraph Count (Prompt)\nDESCRIPTION: Shows how to instruct the model to summarize text within triple quotes into a specific number of paragraphs (2 paragraphs). Note that paragraph count control is more reliable than word count.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_6\n\nLANGUAGE: Prompt\nCODE:\n```\nUSER: Summarize the text delimited by triple quotes in 2 paragraphs.\n\n\"\"\"insert text here\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing and Visualizing t-SNE Dimensionality Reduction with Matplotlib in Python\nDESCRIPTION: This snippet demonstrates how to use t-SNE from scikit-learn to reduce high-dimensional embedding vectors to two dimensions for visualization. It configures t-SNE with custom hyperparameters, transforms an embedding matrix, assigns colors to data points based on score labels, and creates a scatter plot using matplotlib. Required dependencies: scikit-learn, matplotlib, pandas; inputs: matrix of embeddings and data frame with Score values; outputs: a scatter plot visualizing the labeled data in 2D.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/embeddings.txt#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\n\ncolors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\nx = [x for x,y in vis_dims]\ny = [y for x,y in vis_dims]\ncolor_indices = df.Score.values - 1\n\ncolormap = matplotlib.colors.ListedColormap(colors)\nplt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\nplt.title(\"Amazon ratings visualized in language using t-SNE\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Image and Displaying Results in Python\nDESCRIPTION: This code snippet selects the unique subcategories from a DataFrame named `styles_df`, analyzes an encoded image using the `analyze_image` function, parses the JSON response, and displays both the original image and the analysis results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Select the unique subcategories from the DataFrame\nunique_subcategories = styles_df['articleType'].unique()\n\n# Analyze the image and return the results\nanalysis = analyze_image(encoded_image, unique_subcategories)\nimage_analysis = json.loads(analysis)\n\n# Display the image and the analysis results\ndisplay(Image(filename=reference_image))\nprint(image_analysis)\n```\n\n----------------------------------------\n\nTITLE: Loading Text Files and Creating DataFrame Python\nDESCRIPTION: This snippet loads text files from a specified directory, removes unnecessary characters, and creates a Pandas DataFrame. It reads each file, extracts relevant text, removes extra spacing with the defined function, and stores the text in a DataFrame with 'fname' and 'text' columns. It also includes the use of a Pandas function to remove newlines.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport pandas as pd\n\n# Create a list to store the text files\ntexts=[]\n\n# Get all the text files in the text directory\nfor file in os.listdir(\"text/\" + domain + \"/\"):\n\n    # Open the file and read the text\n    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n        text = f.read()\n\n        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n\n# Create a dataframe from the list of texts\ndf = pd.DataFrame(texts, columns = ['fname', 'text'])\n\n# Set the text column to be the raw text with the newlines removed\ndf['text'] = df.fname + \". \" + remove_newlines(df.text)\ndf.to_csv('processed/scraped.csv')\ndf.head()\n```\n```\n\n----------------------------------------\n\nTITLE: Testing Custom QA Chain with Random Questions\nDESCRIPTION: Tests the custom QA chain with five random questions. Uses a different random seed than the previous test to evaluate a variety of questions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrandom.seed(41)\nfor question in random.choices(questions, k=5):\n    print(\">\", question)\n    print(custom_qa.run(question), end=\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Testing QA Chain with Custom Prompt\nDESCRIPTION: Tests the custom QA chain with randomly selected questions. This demonstrates how the custom prompt affects the model's responses compared to the default prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrandom.seed(41)\nfor question in random.choices(questions, k=5):\n    print(\">\", question)\n    print(custom_qa.run(question), end=\"\\n\\n\")\n    # wait 20seconds because of the rate limit\n    time.sleep(20)\n```\n\n----------------------------------------\n\nTITLE: Reading Olympic Data and Creating Context - Python\nDESCRIPTION: This code reads olympics data from a CSV file, concatenates the title, heading, and content of each section to create a context column, and then displays the first few rows of the modified dataframe. It uses the pandas library for data manipulation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndf = pd.read_csv('olympics-data/olympics_sections.csv')\ndf['context'] = df.title + \"\\n\" + df.heading + \"\\n\\n\" + df.content\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Transform Embeddings and Prepare Data Matrix for Clustering in Python\nDESCRIPTION: This snippet applies literal_eval to parse string-encoded embeddings into numpy arrays, then stacks them vertically to form a matrix suitable for clustering algorithms like KMeans.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nembedding_df = pd.read_csv(embedding_path)\nembedding_df[\"embedding\"] = embedding_df.embedding.apply(literal_eval).apply(np.array)\nmatrix = np.vstack(embedding_df.embedding.values)\nmatrix.shape\n```\n\n----------------------------------------\n\nTITLE: Map Embeddings to Atlas - Python\nDESCRIPTION: This snippet logs into the Atlas platform, maps the prepared embeddings and data using the `atlas.map_embeddings` function, specifying the 'id' field and 'Score' as a colorable field. It requires the `nomic` library and a valid API key. It assumes the dataframe `df` and the `embeddings` numpy array are already loaded. It creates a project in Atlas and returns a map object that can be interacted with.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_with_Atlas.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nomic\nfrom nomic import atlas\nnomic.login('7xDPkYXSYDc1_ErdTPIcoAR9RNd8YDlkS3nVNXcVoIMZ6') #demo account\n\ndata = df.to_dict('records')\nproject = atlas.map_embeddings(embeddings=embeddings, data=data,\n                               id_field='id',\n                               colorable_fields=['Score'])\nmap = project.maps[0]\n```\n\n----------------------------------------\n\nTITLE: Testing Quote Generation - Python\nDESCRIPTION: This snippet demonstrates how to use the `generate_quote` function to generate quotes based on different topics and parameters.  It calls the function with a specific topic (\"politics and virtue\") and then prints the generated quote. It then calls the function again, but specifies the author (\"schopenhauer\").  Dependencies: `generate_quote` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nq_topic = generate_quote(\"politics and virtue\")\nprint(\"\\nA new generated quote:\")\nprint(q_topic)\n```\n\nLANGUAGE: python\nCODE:\n```\nq_topic = generate_quote(\"animals\", author=\"schopenhauer\")\nprint(\"\\nA new generated quote:\")\nprint(q_topic)\n```\n\n----------------------------------------\n\nTITLE: Reading the CSV and converting embeddings to list\nDESCRIPTION: This code snippet reads a CSV file into a pandas DataFrame and converts the 'embedding' column from a string representation to a list of floats using ast.literal_eval. This is necessary because the embeddings are stored as strings in the CSV file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\n    \"winter_olympics_2022.csv\"\n)\n\n# convert embeddings from CSV str type back to list type\ndf['embedding'] = df['embedding'].apply(ast.literal_eval)\n```\n\n----------------------------------------\n\nTITLE: Helper Function to Download and Save File (Python)\nDESCRIPTION: Defines a utility function `convert_file_to_png` that takes an OpenAI file ID and a local write path as input. It downloads the content of the specified file from the OpenAI API and writes it to the local file path in binary mode, intended for saving image files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Quick helper function to convert our output file to a png\ndef convert_file_to_png(file_id, write_path):\n    data = client.files.content(file_id)\n    data_bytes = data.read()\n    with open(write_path, \"wb\") as file:\n        file.write(data_bytes)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Postgres Table\nDESCRIPTION: This code loads data from a CSV file into the `articles` table in the Postgres database.  It reads the CSV file, prepares a COPY command to efficiently insert the data, and executes the command using `copy_expert`.  The function handles the CSV formatting and ensures that data is inserted into the correct columns.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\n# Path to your local CSV file\ncsv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\n\n# Define a generator function to process the csv file\ndef process_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            yield line\n\n# Create a StringIO object to store the modified lines\nmodified_lines = io.StringIO(''.join(list(process_file(csv_file_path))))\n\n# Create the COPY command for copy_expert\ncopy_command = '''\nCOPY public.articles (id, url, title, content, title_vector, content_vector, vector_id)\nFROM STDIN WITH (FORMAT CSV, HEADER true, DELIMITER ',');\n'''\n\n# Execute the COPY command using copy_expert\ncursor.copy_expert(copy_command, modified_lines)\n\n# Commit the changes\nconnection.commit()\n```\n\n----------------------------------------\n\nTITLE: Invoking Redshift Middleware API with curl in Bash\nDESCRIPTION: This Bash snippet demonstrates how to send an HTTP POST request to the deployed Lambda function API endpoint, passing a JSON payload with an SQL statement and workgroup and database identifiers. It uses standard curl headers for JSON content and posts to the `/sql_statement` path. The response should return data from Redshift in a file format, demonstrating end-to-end query execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://<your_url>/Prod/sql_statement/ \\\n-H \"Content-Type: application/json\" \\\n-d '{ \"sql_statement\": \"SELECT * FROM customers LIMIT 10\", \"workgroup_name\": \"default-workgroup\", \"database_name\": \"pap-db\" }'\n```\n\n----------------------------------------\n\nTITLE: Creating Redis Index\nDESCRIPTION: Creates a Redis index using the FT.CREATE command for vector similarity search. Includes defining the schema with a vector field and a text field, dropping the index if it already exists, and creating the index. Requires the Redis Search module.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom redis.commands.search.field import TextField, VectorField\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\n\nschema = [ VectorField('$.vector', \n            \"FLAT\", \n            {   \"TYPE\": 'FLOAT32', \n                \"DIM\": 1536, \n                \"DISTANCE_METRIC\": \"COSINE\"\n            },  as_name='vector' ),\n            TextField('$.content', as_name='content')\n        ]\nidx_def = IndexDefinition(index_type=IndexType.JSON, prefix=['doc:'])\ntry: \n    client.ft('idx').dropindex()\nexcept:\n    pass\nclient.ft('idx').create_index(schema, definition=idx_def)\n```\n\n----------------------------------------\n\nTITLE: Create Typesense collection with vector fields\nDESCRIPTION: This snippet deletes existing collections if any and defines a new schema with 'title_vector' and 'content_vector' fields, setting up vector dimensions based on data. It then creates the collection for storing embedded documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    typesense_client.collections['wikipedia_articles'].delete()\nexcept Exception as e:\n    pass\n\nschema = {\n    \"name\": \"wikipedia_articles\",\n    \"fields\": [\n        {\n            \"name\": \"content_vector\",\n            \"type\": \"float[]\",\n            \"num_dim\": len(article_df['content_vector'][0])\n        },\n        {\n            \"name\": \"title_vector\",\n            \"type\": \"float[]\",\n            \"num_dim\": len(article_df['title_vector'][0])\n        }\n    ]\n}\n\ncreate_response = typesense_client.collections.create(schema)\n```\n\n----------------------------------------\n\nTITLE: Cloning & Deploying Lambda Function (Bash)\nDESCRIPTION: These bash commands demonstrate how to clone a git repository, navigate to a specific directory, and then build and deploy the SAM application defined in `template.yaml`. The deployment process uses the AWS CLI and requires the AWS SAM CLI. It will create necessary AWS resources. Outputs will include the API endpoint for the deployed Lambda function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pap-openai/lambda-middleware\ncd lambda-middleware\n```\n\nLANGUAGE: bash\nCODE:\n```\nsam build\nsam deploy --template-file template.yaml --stack-name aws-middleware --capabilities CAPABILITY_IAM\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF\nDESCRIPTION: This code imports necessary libraries, sets up the OpenAI API client, and extracts the text from a PDF file using textract. The extracted text is then cleaned by replacing double spaces with single spaces and newlines with semicolons.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport textract\nimport os\nimport openai\nimport tiktoken\n\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n# Extract the raw text from each PDF using textract\ntext = textract.process('data/fia_f1_power_unit_financial_regulations_issue_1_-_2022-08-16.pdf', method='pdfminer').decode('utf-8')\nclean_text = text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')\n```\n\n----------------------------------------\n\nTITLE: Displaying Formatted DataFrame - Python\nDESCRIPTION: Defines a function `display_formatted_dataframe` that takes a DataFrame, formats the 'predicted_issue' and 'true_issue' columns by replacing newlines with `<br>`, and displays the DataFrame as HTML. It relies on the `display` function and `HTML` class from the `IPython.display` module and pandas for DataFrame manipulation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef display_formatted_dataframe(df):\n    def format_text(text):\n        return text.replace('\\n', '<br>')\n\n    df_formatted = df.copy()\n    df_formatted['predicted_issue'] = df_formatted['predicted_issue'].apply(format_text)\n    df_formatted['true_issue'] = df_formatted['true_issue'].apply(format_text)\n    \n    display(HTML(df_formatted.to_html(escape=False, justify='left')))\n    \ndisplay_formatted_dataframe(pd.DataFrame(validation_results))\n```\n\n----------------------------------------\n\nTITLE: Adjusting Image Brightness/Contrast with ImageMagick (Bash)\nDESCRIPTION: This command utilizes the `convert` utility from the ImageMagick suite to increase both the brightness and contrast of an image (cat.jpg) by 50%, overwriting the original file. Adjusting brightness and contrast can be a helpful preprocessing step before converting raster images to vector formats with tools like Potrace. Requires the ImageMagick suite to be installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/what_is_new_with_dalle_3.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nconvert cat.jpg -brightness-contrast 50x50 cat.jpg\n```\n\n----------------------------------------\n\nTITLE: Listing Code Interpreter Run Steps Using Python\nDESCRIPTION: This snippet demonstrates how to retrieve logs of a specific run step in the Code Interpreter using the Python client library. It requires the 'client' object to be authenticated and configured to interact with the API, and requires 'thread.id' and 'run.id' as parameters. The core function 'client.beta.threads.runs.steps.list' returns a list of step details, including the step status and output logs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrun_steps = client.beta.threads.runs.steps.list(\n  thread_id=thread.id,\n  run_id=run.id\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Philosopher Quotes Dataset from Hugging Face - Python\nDESCRIPTION: This snippet loads the 'datastax/philosopher-quotes' dataset from the Hugging Face Datasets hub. It assigns the 'train' split to 'philo_dataset' for use in further processing. This dataset is used as input data for embedding, storage, and semantic search workflows demonstrated in the notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nphilo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n\n```\n\n----------------------------------------\n\nTITLE: Creating Summary Prompt with Examples and Negative Examples in Python\nDESCRIPTION: Builds upon the basic prompt to include an example of a good summary and a bad summary, demonstrating desired and undesired output formats. These examples aid in guiding the AI's response quality during experiments with prompt engineering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nPROMPT_VARIATION_WITH_EXAMPLES = f\"\"\"\\n{PROMPT_VARIATION_BASIC}\\n\\nHere is an example of a good summary:\\n<push_notifications>\\n- Traffic alert: Accident reported on Main Street.- Package out for delivery: Expected by 5 PM.- New friend suggestion: Connect with Emma.\\n</push_notifications>\\n<summary>\\nTraffic alert, package expected by 5pm, suggestion for new friend (Emily).\\n</summary>\\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing AgentExecutor with Conversational Memory in Python\nDESCRIPTION: This code initializes an AgentExecutor to combine multi-tool capability with a conversational memory buffer, allowing the agent to recall a window of the previous dialogue steps. It uses ConversationBufferWindowMemory to store historical turns and AgentExecutor.from_agent_and_tools() to wrap the agent logic. Dependencies are the ConversationBufferWindowMemory and AgentExecutor classes, with the window size (k) controlling memory depth.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nmulti_tool_memory = ConversationBufferWindowMemory(k=2)\nmulti_tool_executor = AgentExecutor.from_agent_and_tools(agent=multi_tool_agent, tools=expanded_tools, verbose=True, memory=multi_tool_memory)\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Collection for Embeddings in Astra DB with Python\nDESCRIPTION: Creates a new collection in Astra DB configured to store vectors of a specified dimension (1536, as required by OpenAI's embedding model). The collection is named 'philosophers_astra_db'. This step must precede any data insertion or vector search and requires an initialized astra_db client.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncoll_name = \"philosophers_astra_db\"\ncollection = astra_db.create_collection(coll_name, dimension=1536)\n```\n\n----------------------------------------\n\nTITLE: Displaying API Response Content - Python\nDESCRIPTION: This snippet displays the content of the message received from an OpenAI API response, typically for rendering the result in a notebook environment using Markdown formatting.\n\nRequired Dependencies: Requires `display` and `Markdown` from `IPython.display` or similar, and the `chat_test_response` variable containing an API response object.\n\nInputs: An OpenAI API response object stored in `chat_test_response`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndisplay(Markdown(chat_test_response.choices[0].message.content))\n```\n\n----------------------------------------\n\nTITLE: Instantiating Supabase Client with Configuration in JavaScript\nDESCRIPTION: Initializes a Supabase client object in JavaScript by supplying the Supabase URL and Service Role Key, with session persistence disabled for stateless server processes. This object is used to perform operations such as insert and query against the Supabase backend. Ensure the configuration values are securely loaded beforehand.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst supabase = createClient(supabaseUrl, supabaseServiceRoleKey, {\n  auth: { persistSession: false },\n});\n```\n\n----------------------------------------\n\nTITLE: Helper Functions for Assistant API\nDESCRIPTION: Defines helper functions to streamline interaction with the OpenAI Assistants API.  `submit_message` creates a message and starts a new run. `get_response` retrieves messages from a thread. It uses a predefined `MATH_ASSISTANT_ID` (or an ID from an environment variable), and these functions abstract away the complexities of thread and run management, providing a higher-level interface.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nMATH_ASSISTANT_ID = assistant.id  # or a hard-coded ID like \"asst-...\"\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\ndef submit_message(assistant_id, thread, user_message):\n    client.beta.threads.messages.create(\n        thread_id=thread.id, role=\"user\", content=user_message\n    )\n    return client.beta.threads.runs.create(\n        thread_id=thread.id,\n        assistant_id=assistant_id,\n    )\n\n\ndef get_response(thread):\n    return client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")\n```\n\n----------------------------------------\n\nTITLE: Loading YouTube Transcriptions Dataset Using Hugging Face Datasets in Python\nDESCRIPTION: Uses the 'datasets' library to download and load the 'jamescalam/youtube-transcriptions' dataset, a collection of transcribed speech snippets from machine learning and tech YouTube videos. This dataset will be used to build a domain-specific knowledge base for vector embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndata = load_dataset('jamescalam/youtube-transcriptions', split='train')\ndata\n```\n\n----------------------------------------\n\nTITLE: Example JSON Evaluation for Assistant Message (Inaccurate)\nDESCRIPTION: This JSON object shows an example evaluation where the assistant's message contains a factual inaccuracy (hallucination). The `factualAccuracy` is marked as 0, and a `knowledgeReference` explains the discrepancy based on the provided knowledge base rules.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb#_snippet_14\n\nLANGUAGE: json\nCODE:\n```\nfs_assistant_2 = \"\"\"'[\n    {\n        \"sentence\": \"I see, because the shirt was ordered in the last 60 days, we cannot process a refund.\",\n        \"factualAccuracy\": 0,\n        \"knowledgeReference: \"If an order was placed within 60 days, you must process a partial refund.\"\n        \"relevance\": 1,\n        \"policyCompliance\": 1,\n        \"contextualCoherence\": 1\n    }\n]'\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Streaming Chat Completion Implementation\nDESCRIPTION: Implementation of streaming chat completions by setting stream=True. This example shows how to process the incremental chunks received from the API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example of an OpenAI ChatCompletion request with stream=True\n# https://platform.openai.com/docs/api-reference/streaming#chat/create-stream\n\n# a ChatCompletion request\nresponse = client.chat.completions.create(\n    model='gpt-4o-mini',\n    messages=[\n        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n    ],\n    temperature=0,\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n    print(chunk.choices[0].delta.content)\n    print(\"****************\")\n```\n\n----------------------------------------\n\nTITLE: Processing Vector Columns in Pandas DataFrame in Python\nDESCRIPTION: Prepares the DataFrame for ChromaDB ingestion. It converts the 'title_vector' and 'content_vector' columns, which are stored as strings in the CSV, back into Python lists of floats using `ast.literal_eval`. It also explicitly converts the 'vector_id' column to the string data type.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Demonstrate Invalid Schema Test (Python)\nDESCRIPTION: Demonstrates the behavior of the `test_valid_schema` function when presented with an invalid input string that does not conform to the expected `LLMResponse` Pydantic schema. It defines a `failing_query` string with an incorrect format and calls `test_valid_schema` with it, showing how the validation error is caught and `False` is returned.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfailing_query = 'CREATE departments, select * from departments'\ntest_valid_schema(failing_query)\n```\n\n----------------------------------------\n\nTITLE: Creating the Agent Function in JavaScript\nDESCRIPTION: This asynchronous function `agent` processes user input using OpenAI's chat completions API. It pushes the user input to the `messages` array, sends a request to OpenAI with the model, messages, and tools specified. Then, it logs the response received from OpenAI.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nasync function agent(userInput) {\n  messages.push({\n    role: \"user\",\n    content: userInput,\n  });\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4\",\n    messages: messages,\n    tools: tools,\n  });\n  console.log(response);\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Embeddings in Pandas - Python\nDESCRIPTION: This snippet loads a CSV file containing text reviews and their corresponding embeddings into a Pandas DataFrame. It uses `pandas` to read the CSV, `ast.literal_eval` to convert the string representation of embeddings into lists, and `numpy` to convert the lists into NumPy arrays for efficient computation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom ast import literal_eval\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\ndf[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI and Embedding Function (Python)\nDESCRIPTION: Configures the `openai` library for use with an Azure OpenAI endpoint. It sets the API version, base URL, type, and key. Defines an `embed` function using `openai.Embedding.create` with an Azure deployment ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nopenai.api_version = '2022-12-01'\nopenai.api_base = '' # Please add your endpoint here\nopenai.api_type = 'azure'\nopenai.api_key = ''  # Please add your api key here\n\ndef embed(query):\n    # Creates embedding vector from user query\n    embedded_query = openai.Embedding.create(\n            input=query,\n            deployment_id=\"embed\", #replace with your deployment id\n            chunk_size=1\n    )[\"data\"][0][\"embedding\"]\n    return embedded_query\n```\n\n----------------------------------------\n\nTITLE: Checking for Separator String in DataFrame Context Column using Pandas in Python\nDESCRIPTION: Verifies that the specific separator string '->' is not present within any of the text entries in the 'context' column of the main DataFrame (`df`). This check is performed using pandas string methods to ensure the separator can be safely used later in prompt construction without interfering with the original context data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf.context.str.contains('->').sum()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Conversation Memory for LangChain Agent Executor in Python\nDESCRIPTION: Instantiates ConversationBufferWindowMemory to retain the last two conversational turns and integrates this memory into the agent_executor. This enables the agent to reference previous exchanges for more coherent multi-turn dialogue. Requires the ConversationBufferWindowMemory module and valid agent and tools objects for proper initialization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Initiate the memory with k=2 to keep the last two turns\n# Provide the memory to the agent\nmemory = ConversationBufferWindowMemory(k=2)\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Medical Dataset from Hugging Face\nDESCRIPTION: Loads a medical reasoning dataset from Hugging Face and preprocesses it by merging question and response columns. This prepares the data for embedding and storage in the vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load the dataset (ensure you're logged in with huggingface-cli if needed)\nds = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split='train[:100]', trust_remote_code=True)\nds_dataframe = DataFrame(ds)\n\n# Merge the Question and Response columns into a single string.\nds_dataframe['merged'] = ds_dataframe.apply(\n    lambda row: f\"Question: {row['Question']} Answer: {row['Response']}\", axis=1\n)\nprint(\"Example merged text:\", ds_dataframe['merged'].iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Loading dataset and querying embeddings with pandas and custom utils\nDESCRIPTION: This snippet loads a JSONL dataset into a pandas DataFrame, outputs category distribution, and prepares text data for embedding generation using a custom utility function. Dependencies include pandas and the utils.embeddings_utils module, with inputs being text samples and output being an embedding matrix.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nsamples = pd.read_json(\"data/dbpedia_samples.jsonl\", lines=True)\ncategories = sorted(samples[\"category\"].unique())\nprint(\"Categories of DBpedia samples:\", samples[\"category\"].value_counts())\nsamples.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies - Python\nDESCRIPTION: Installs necessary Python packages like `tenacity` and `openai` using `pip`. These packages are required to interact with the OpenAI API and handle retries.  Also installs `typing` and `python-dotenv` to enable better function type hinting and environment variable loading from `.env` files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!pip install tenacity -q\n#!pip install openai -q\n#!pip install typing -q\n# !pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI Python\nDESCRIPTION: This snippet uses the OpenAI API to generate embeddings for text data.  It initializes the OpenAI client, applies the embedding generation function to the 'text' column of a DataFrame, and saves the resulting embeddings. This operation requires setting the `OPENAI_API_KEY` environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\ndf['embeddings'] = df.text.apply(lambda x: client.embeddings.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n\ndf.to_csv('processed/embeddings.csv')\ndf.head()\n```\n```\n\n----------------------------------------\n\nTITLE: Calculating Faithfulness and Relevancy scores from batch results - Python\nDESCRIPTION: This snippet calculates the faithfulness and relevancy scores based on the results from the batch evaluation. It iterates through the results, summing the `passing` flags, and then divides the sum by the total number of queries to produce a score between 0 and 1. These scores can then be used to quantify the model's performance.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Let's get faithfulness score\n\nfaithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n\nfaithfulness_score\n```\n\nLANGUAGE: python\nCODE:\n```\n# Let's get relevancy score\n\nrelevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n\nrelevancy_score\n```\n\n----------------------------------------\n\nTITLE: Demonstrate Valid Schema Test (Python)\nDESCRIPTION: Executes the `test_valid_schema` function with the `content` variable, which holds the actual JSON response previously generated by the OpenAI API. This step verifies that the API call produced output conforming to the `LLMResponse` Pydantic schema, demonstrating the positive case for the schema validation unit test.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntest_valid_schema(content)\n```\n\n----------------------------------------\n\nTITLE: Creating Table and Indexes in Neon Postgres\nDESCRIPTION: This code snippet creates a table named `articles` in the Neon Postgres database and adds indexes for the `content_vector` and `title_vector` columns. The table stores article information, including title, content, and their corresponding vector embeddings. The indexes are created using the `ivfflat` algorithm to optimize vector similarity searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_table_sql = '''\nCREATE TABLE IF NOT EXISTS public.articles (\n    id INTEGER NOT NULL,\n    url TEXT,\n    title TEXT,\n    content TEXT,\n    title_vector vector(1536),\n    content_vector vector(1536),\n    vector_id INTEGER\n);\n\nALTER TABLE public.articles ADD PRIMARY KEY (id);\n'''\n\n# SQL statement for creating indexes\ncreate_indexes_sql = '''\nCREATE INDEX ON public.articles USING ivfflat (content_vector) WITH (lists = 1000);\n\nCREATE INDEX ON public.articles USING ivfflat (title_vector) WITH (lists = 1000);\n'''\n\n# Execute the SQL statements\ncursor.execute(create_table_sql)\ncursor.execute(create_indexes_sql)\n\n# Commit the changes\nconnection.commit()\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Function for Pinecone Index\nDESCRIPTION: Creates a function to search the Pinecone index using semantic queries. This function generates an embedding for the query text and retrieves the most similar documents with their metadata.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef query_pinecone_index(client, index, model, query_text):\n    # Generate an embedding for the query.\n    query_embedding = client.embeddings.create(input=query_text, model=model).data[0].embedding\n\n    # Query the index and return top 5 matches.\n    res = index.query(vector=[query_embedding], top_k=5, include_metadata=True)\n    print(\"Query Results:\")\n    for match in res['matches']:\n        print(f\"{match['score']:.2f}: {match['metadata'].get('Question', 'N/A')} - {match['metadata'].get('Answer', 'N/A')}\")\n    return res\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Wikipedia Text Using OpenAI API in Python\nDESCRIPTION: Creates text embeddings for Wikipedia text chunks using OpenAI's text-embedding-3-small model. Processes data in batches of 1000 and stores both text and embeddings in a pandas DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nBATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n\nembeddings = []\nfor batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n    batch_end = batch_start + BATCH_SIZE\n    batch = wikipedia_strings[batch_start:batch_end]\n    print(f\"Batch {batch_start} to {batch_end-1}\")\n    response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n    for i, be in enumerate(response.data):\n        assert i == be.index  # double check embeddings are in same order as input\n    batch_embeddings = [e.embedding for e in response.data]\n    embeddings.extend(batch_embeddings)\n\ndf = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n```\n\n----------------------------------------\n\nTITLE: Listing Code Interpreter Run Steps Using Node.js\nDESCRIPTION: This snippet demonstrates retrieving the list of steps for a run in the Code Interpreter via the Node.js SDK. It assumes appropriate authentication and configuration for the OpenAI SDK. It uses the 'openai.beta.threads.runs.steps.list' function with 'thread.id' and 'run.id' as parameters to obtain detailed logs of each step, including its status and output logs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_7\n\nLANGUAGE: node.js\nCODE:\n```\nconst runSteps = await openai.beta.threads.runs.steps.list(\n  thread.id,\n  run.id\n);\n```\n\n----------------------------------------\n\nTITLE: Filtering Hyperlinks by Domain in Python\nDESCRIPTION: Defines the `get_domain_hyperlinks` function that takes a local domain name and a URL. It calls `get_hyperlinks` to fetch all links from the URL, then iterates through them, filtering to keep only those that belong to the specified `local_domain`. It handles both absolute URLs (checking the netloc) and relative URLs (prefixing with the domain), cleans trailing slashes, and returns a unique list of valid, domain-specific links.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Function to get the hyperlinks from a URL that are within the same domain\ndef get_domain_hyperlinks(local_domain, url):\n    clean_links = []\n    for link in set(get_hyperlinks(url)):\n        clean_link = None\n\n        # If the link is a URL, check if it is within the same domain\n        if re.search(HTTP_URL_PATTERN, link):\n            # Parse the URL and check if the domain is the same\n            url_obj = urlparse(link)\n            if url_obj.netloc == local_domain:\n                clean_link = link\n\n        # If the link is not a URL, check if it is a relative link\n        else:\n            if link.startswith(\"/\"):\n                link = link[1:]\n            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n                continue\n            clean_link = \"https://\" + local_domain + \"/\" + link\n\n        if clean_link is not None:\n            if clean_link.endswith(\"/\"):\n                clean_link = clean_link[:-1]\n            clean_links.append(clean_link)\n\n    # Return the list of hyperlinks that are within the same domain\n    return list(set(clean_links))\n```\n\n----------------------------------------\n\nTITLE: Validating Data Rows with ThreadPoolExecutor - Python\nDESCRIPTION: Validates data rows using a `ThreadPoolExecutor` to parallelize the `validate_row` function for each row in `input_data`. The results, indicating if each row is valid and any identified issues, are stored in `pred_is_valid` and `pred_issues` lists.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith ThreadPoolExecutor() as executor:\n    futures = {executor.submit(validate_row, row): i for i, row in enumerate(input_data)}\n    \n    for future in as_completed(futures):\n        i = futures[future]  # Get the index of the current row\n        result_json = future.result()\n        pred_is_valid[i] = result_json['is_valid']\n        pred_issues[i] = result_json['issue']\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenAI API Key Availability\nDESCRIPTION: Checks if the OpenAI API key is set as an environment variable, which is required for creating embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Listing Messages After Run Completion (OpenAI Assistants API)\nDESCRIPTION: Retrieves the final list of messages in the thread after the polling loop confirms the image has been created. It then extracts the content of the latest messages (Assistant's response) to allow inspection, typically showing the generated plot and any preceding explanations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmessages = client.beta.threads.messages.list(thread_id=thread.id)\n[message.content[0] for message in messages.data]\n\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis using OpenAI GPT-4\nDESCRIPTION: This function analyzes the sentiment of a given text using OpenAI's GPT-4 model. It sends a prompt to the model, instructing it to determine whether the sentiment is positive, negative, or neutral, based on the tone, emotions, and context of the text. The function relies on the OpenAI client library and expects the 'transcription' to be a string containing the text to analyze. It returns the sentiment analysis result from the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/meeting-minutes-tutorial.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef sentiment_analysis(transcription):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following text. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Indicate whether the sentiment is generally positive, negative, or neutral, and provide brief explanations for your analysis where possible.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcription\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Querying arXiv for Search Candidates - Python\nDESCRIPTION: Performs a search query on arXiv using the arxiv Python package, specifying the query string, maximum number of results, and sorting criterion. The input is a search string, and the result is a Search object containing metadata for matching arXiv documents. Required: arxiv library. The output is used to iterate over search candidates for further processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nquery = \"how do bi-encoders work for sentence embeddings\"\nsearch = arxiv.Search(\n    query=query, max_results=20, sort_by=arxiv.SortCriterion.Relevance\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ArXiv Agent Conversation - Python\nDESCRIPTION: This snippet initializes a new `Conversation` object and adds an initial system message to define the persona and instructions for the ArXiv agent. This sets the context for the subsequent interaction.\n\nRequired Dependencies: Requires the `Conversation` class to be defined.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Start with a system message\npaper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\\nYou summarize the papers clearly so the customer can decide which to read to answer their question.\\nYou always provide the article_url and title so the user can understand the name of the paper and click through to access it.\\nBegin!\"\"\"\npaper_conversation = Conversation()\npaper_conversation.add_message(\"system\", paper_system_message)\n\n```\n\n----------------------------------------\n\nTITLE: Function JSON Definition (Python)\nDESCRIPTION: Defines the JSON schema for the display_quiz function, specifying its name, description, parameters, and their types. This JSON object is used to register the function with the OpenAI Assistant. It outlines required parameters, the structure for questions, and enums for question types.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfunction_json = {\n    \"name\": \"display_quiz\",\n    \"description\": \"Displays a quiz to the student, and returns the student's response. A single quiz can have multiple questions.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\"type\": \"string\"},\n            \"questions\": {\n                \"type\": \"array\",\n                \"description\": \"An array of questions, each with a title and potentially options (if multiple choice).\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"question_text\": {\"type\": \"string\"},\n                        \"question_type\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"MULTIPLE_CHOICE\", \"FREE_RESPONSE\"]\n                        },\n                        \"choices\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    },\n                    \"required\": [\"question_text\"]\n                }\n            }\n        },\n        \"required\": [\"title\", \"questions\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying OpenAPI Specification in Python\nDESCRIPTION: Opens and reads a local JSON file (`./data/example_events_openapi.json`) containing an OpenAPI specification. It uses `jsonref.loads` to parse the JSON content while automatically resolving any internal JSON references (`$ref`), ensuring a complete representation of the spec. The resulting specification object is then displayed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith open('./data/example_events_openapi.json', 'r') as f:\n    openapi_spec = jsonref.loads(f.read()) # it's important to load with jsonref, as explained below\n\ndisplay(openapi_spec)\n```\n\n----------------------------------------\n\nTITLE: Guiding Proper Names and Spelling in Whisper Transcription using Prompts in Python\nDESCRIPTION: Illustrates how to supply preferred spellings and proper names in the transcription prompt to improve accuracy on uncommon words. Includes baseline transcription and several prompt variations to guide Whisper's spelling choices for product and friend names. Dependencies are the 'transcribe' function and relevant local audio files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# baseline transcription with no prompt\ntranscribe(product_names_filepath, prompt=\"\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# adding the correct spelling of the product name helps\ntranscribe(product_names_filepath, prompt=\"QuirkQuid Quill Inc, P3-Quattro, O3-Omni, B3-BondX, E3-Equity, W3-WrapZ, O2-Outlier, U3-UniFund, M3-Mover\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# baseline transcript with no prompt\ntranscribe(bbq_plans_filepath, prompt=\"\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# spelling prompt\ntranscribe(bbq_plans_filepath, prompt=\"Friends: Aimee, Shawn\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# longer spelling prompt\ntranscribe(bbq_plans_filepath, prompt=\"Glossary: Aimee, Shawn, BBQ, Whisky, Doughnuts, Omelet\")\n```\n\nLANGUAGE: Python\nCODE:\n```\n# more natural, sentence-style prompt\ntranscribe(bbq_plans_filepath, prompt=\"\"\"Aimee and Shawn ate whisky, doughnuts, omelets at a BBQ.\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Running a Query Against a LangChain RetrievalQA Pipeline in Python\nDESCRIPTION: Executes a natural language query on the initialized RetrievalQA chain which internally performs a similarity search over the vector store and feeds relevant retrieved documents to the GPT-3.5 Turbo model to generate an answer. Returns the textual response to the user question.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquery = 'Why does the military not say 24:00?'\nqa.run(query)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings using OpenAI API\nDESCRIPTION: This function generates embeddings for a given text using a specified OpenAI model. It utilizes the OpenAI client to create an embedding. The function takes the text and the model name as input and returns the embedding data. It directly interacts with the OpenAI API to generate the embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_embeddings(text, model):\n    # Generate embeddings for the provided text using the specified model\n    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n    # Extract the embedding data from the response\n    embedding = embeddings_response.data[0].embedding\n    return embedding\n```\n\n----------------------------------------\n\nTITLE: Testing Psycopg2 Connection to AnalyticDB - Python\nDESCRIPTION: Executes a simple SQL query ('SELECT 1;') using the psycopg2 cursor to verify successful connection to the AnalyticDB database. No additional dependencies required. Outputs a confirmation message if the result matches expectation. Expects valid psycopg2 connection and cursor to be established prior to execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Execute a simple query to test the connection\ncursor.execute(\"SELECT 1;\")\nresult = cursor.fetchone()\n\n# Check the query result\nif result == (1,):\n    print(\"Connection successful!\")\nelse:\n    print(\"Connection failed.\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings from Text Data\nDESCRIPTION: This snippet defines a function to convert input text into vector embeddings using OpenAI's embedding model. It replaces newlines with spaces to normalize input and returns the embedding vector for each text, enabling vector similarity operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef embed(text):\n    text = text.replace(\"\\n\", \" \")  # Ensure text doesn't have newlines\n    res = client.embeddings.create(input=[text], model=\"text-embedding-3-large\")\n    \n    return res.data[0].embedding\n\n# Generate embeddings for all data texts\ndoc_embeds = [embed(d[\"text\"]) for d in data]\n\nprint(doc_embeds)\n```\n\n----------------------------------------\n\nTITLE: Combining Training Examples and Writing to JSONL\nDESCRIPTION: This code combines the regular training examples with the `reject_request` examples, then writes the complete dataset to a JSONL file, which can then be used to fine-tune the language model.  Each item in the `training_list_total` is a dictionary formatted for fine-tuning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntraining_list_total = training_examples + reject_training_list\n\ntraining_file = \"data/drone_training.jsonl\"\nwith open(training_file, \"w\") as f:\n    for item in training_list_total:\n        json_str = json.dumps(item)\n        f.write(f\"{json_str}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Initializing FastEmbed Model in Python\nDESCRIPTION: Imports necessary libraries (fastembed, typing, numpy, pandas, tqdm) and initializes the `DefaultEmbedding` model from `fastembed`. This model will be used later to generate vector embeddings for text data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom fastembed.embedding import DefaultEmbedding\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\ntqdm.pandas()\n\nembedding_model = DefaultEmbedding()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (Bash)\nDESCRIPTION: Exports the OpenAI API key as an environment variable named `OPENAI_API_KEY` using the `export` command in a shell environment. This key is required by Weaviate's OpenAI modules (text2vec-openai, qna-openai) for vectorizing data and performing Q&A tasks. Replace \"your key\" with the actual OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Export OpenAI API Key\n!export OPENAI_API_KEY=\"your key\"\n```\n\n----------------------------------------\n\nTITLE: Uploading the JSONL Batch File to OpenAI API\nDESCRIPTION: This snippet demonstrates uploading the prepared JSONL file to OpenAI via the client SDK. It opens the file in binary read mode and creates an upload purpose 'batch'. Dependencies include the OpenAI client instance 'client' and the file path. The result is a file object with an ID used in subsequent batch operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# Uploading the file \n\nbatch_file = client.files.create(\n  file=open(file_name, \"rb\"),\n  purpose=\"batch\"\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Content from Chat Completion Response - Node.js\nDESCRIPTION: This snippet extracts the content of the assistant's message from a Chat Completions API response in Node.js. It assumes a `completion` object is available, and it accesses the content through the `choices`, `message`, and `content` properties.  No specific dependencies are needed as it is assumed the `completion` object has already been received.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_4\n\nLANGUAGE: node.js\nCODE:\n```\ncompletion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Returning Secure CSV URL in ChatGPT Response\nDESCRIPTION: This code formats the response to include the generated CSV URL within the openaiFileResponse list, allowing ChatGPT to recognize and process the CSV file for data analysis tasks. It assumes the URL has been generated and stored in the variable 'csv_url'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nresponse = {\n    'openaiFileResponse': [csv_url]\n}\ncursor.close()\nconn.close()\nreturn func.HttpResponse(\n    json.dumps(response), \n    status_code=200\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Agents and Handoff Functions (Python)\nDESCRIPTION: This snippet defines basic Agent objects and a simple function that demonstrates the agent handoff pattern by returning an Agent object instead of a standard output string. It sets up the initial structure for different agent personas.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nrefund_agent = Agent(\n    name=\"Refund Agent\",\n    instructions=\"You are a refund agent. Help the user with refunds.\",\n    tools=[execute_refund],\n)\n\ndef transfer_to_refunds():\n    return refund_agent\n\nsales_assistant = Agent(\n    name=\"Sales Assistant\",\n    instructions=\"You are a sales assistant. Sell the user a product.\",\n    tools=[place_order],\n)\n```\n\n----------------------------------------\n\nTITLE: Test Elasticsearch index with match query\nDESCRIPTION: This snippet tests the newly created Elasticsearch index using a simple `match` query. It searches for documents containing the word \"Hummingbird\" in the 'text' field and prints the search results, excluding the 'title_vector' and 'content_vector' fields from the output.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(client.search(index=\"wikipedia_vector_index\", body={\n    \"_source\": {\n        \"excludes\": [\"title_vector\", \"content_vector\"]\n    },\n    \"query\": {\n        \"match\": {\n            \"text\": {\n                \"query\": \"Hummingbird\"\n            }\n        }\n    }\n}))\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Azure OpenAI (Python)\nDESCRIPTION: This code imports the necessary Python libraries for the Azure OpenAI example. It imports `os` for interacting with the operating system, `openai` for making API calls to the Azure OpenAI service, and `dotenv` for loading environment variables from a `.env` file. These libraries are essential for authentication and using the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/whisper.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nimport dotenv\n\ndotenv.load_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Evaluating a single query with Faithfulness Evaluator - Python\nDESCRIPTION: This code snippet demonstrates how to evaluate the faithfulness of a response to a query. It first selects a query from a list, then uses the `query_engine` to generate a response for that query. Finally, it utilizes the initialized `faithfulness_gpt4` evaluator to assess the faithfulness of the `response_vector`. The `eval_result.passing` attribute indicates whether the response passed the faithfulness evaluation. \nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\neval_query = queries[10]\n\neval_query\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector = query_engine.query(eval_query)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Compute faithfulness evaluation\n\neval_result = faithfulness_gpt4.evaluate_response(response=response_vector)\n```\n\nLANGUAGE: python\nCODE:\n```\n# You can check passing parameter in eval_result if it passed the evaluation.\neval_result.passing\n```\n\n----------------------------------------\n\nTITLE: Inspecting OpenAI Embedding Results in Python\nDESCRIPTION: Prints information about the size and content of the embedding vectors returned from the OpenAI API. Useful for verification and debugging before integrating with other systems. Depends on a successful embeddings call stored in 'result'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"len(result.data)              = {len(result.data)}\")\nprint(f\"result.data[1].embedding      = {str(result.data[1].embedding)[:55]}...\")\nprint(f\"len(result.data[1].embedding) = {len(result.data[1].embedding)}\")\n```\n\n----------------------------------------\n\nTITLE: Execute Title-Based Search in Qdrant\nDESCRIPTION: Calls the `query_qdrant` function to search the 'Articles' collection for entries semantically similar to 'modern art in Europe', using the 'title' vectors. It then iterates through the results and prints the title, URL, and similarity score for each.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nquery_results = query_qdrant('modern art in Europe', 'Articles', 'title')\nfor i, article in enumerate(query_results):\n    print(f'{i + 1}. {article.payload[\"title\"]}, URL: {article.payload[\"url\"]} (Score: {round(article.score, 3)})')\n```\n\n----------------------------------------\n\nTITLE: Rendering Math Tutor Structured Response in Python\nDESCRIPTION: Calls the 'print_math_response' function passing the content from the structured math solution to display each step and the final answer using LaTeX rendering within an IPython environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint_math_response(result.content)\n```\n\n----------------------------------------\n\nTITLE: Counting Articles in the Database after Data Load - Python\nDESCRIPTION: Runs a SQL query via psycopg2 to count the number of entries in 'public.articles', confirming successful data loading. Prints the article count to the console. Requires a correctly initialized psycopg2 cursor/connection. No dependencies except standard Python and psycopg2.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Check the collection size to make sure all the points have been stored\ncount_sql = \"\"\"select count(*) from public.articles;\"\"\"\ncursor.execute(count_sql)\nresult = cursor.fetchone()\nprint(f\"Count:{result[0]}\")\n\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements with pip\nDESCRIPTION: This code snippet installs the necessary Python packages for the project. It uses `pip` to install `openai`, `qdrant-client`, `langchain` (with a specific version constraint), and `wget`. The dependencies include interacting with the OpenAI API, Qdrant vector database, and the Langchain framework.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai qdrant-client \"langchain==0.0.100\" wget\n```\n\n----------------------------------------\n\nTITLE: Setting Embedding Model Parameters in Python\nDESCRIPTION: Specifies the embedding model name 'text-embedding-3-small', the corresponding token encoding 'cl100k_base', and sets the maximum token limit to 8,000 (under the model maximum of 8,191). These parameters guide how text inputs are encoded and truncated before embeddings are computed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nembedding_model = \"text-embedding-3-small\"\nembedding_encoding = \"cl100k_base\"\nmax_tokens = 8000  # the maximum for text-embedding-3-small is 8191\n```\n\n----------------------------------------\n\nTITLE: Example Usage: Generating Unit Tests for Palindrome Function in Python\nDESCRIPTION: Provides an example of how to use the unit_test_from_function to generate unit tests for a simple Python function is_palindrome. The example function definition is passed as a string, and the unit test generation function is called with print_text enabled to output intermediate GPT-3 responses and the resulting tests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexample_function = \"\"\"def is_palindrome(s):\n    return s == s[::-1]\"\"\"\n\nunit_test_from_function(example_function, print_text=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Summaries for All Articles with Parallel Processing\nDESCRIPTION: Uses ThreadPoolExecutor to generate summaries for all articles in parallel, storing both simple and complex summaries in the dataframe and displaying a progress bar.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Add new columns to the dataframe for storing itineraries\ndf['simple_summary'] = None\ndf['complex_summary'] = None\n\n# Use ThreadPoolExecutor to generate itineraries concurrently\nwith ThreadPoolExecutor() as executor:\n    futures = {executor.submit(generate_summaries, row): index for index, row in df.iterrows()}\n    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Generating Itineraries\"):\n        index = futures[future]\n        simple_itinerary, complex_itinerary = future.result()\n        df.at[index, 'simple_summary'] = simple_itinerary\n        df.at[index, 'complex_summary'] = complex_itinerary\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Multiple Product IDs to Print Similar Items in Python\nDESCRIPTION: Loops through a list of product IDs, calls 'query_similar_items' for each, and prints the names and IDs of the similar products found. This snippet demonstrates batch querying for similar products and displaying results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nproduct_ids = ['1519827', '2763742']\n\nfor product_id in product_ids:\n    print(f\"Similar items for product #{product_id}:\\n\")\n    result = query_similar_items(product_id)\n    print(\"\\n\")\n    for r in result:\n        print(f\"{r['name']} ({r['id']})\")\n    print(\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Prompt Template for QA\nDESCRIPTION: Defines a custom prompt template that instructs the model to provide single-sentence answers or suggest a random song if the answer is unknown. The template includes placeholders for context and question.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\ncustom_prompt = \"\"\"\nUse the following pieces of context to answer the question at the end. Please provide\na short single-sentence summary answer only. If you don't know the answer or if it's\nnot present in given context, don't try to make up an answer, but suggest me a random\nunrelated song title I could listen to.\nContext: {context}\nQuestion: {question}\nHelpful Answer:\n\"\"\"\n\ncustom_prompt_template = PromptTemplate(\n    template=custom_prompt, input_variables=[\"context\", \"question\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Batching Data into Tuples in Python\nDESCRIPTION: This function takes an iterable and splits it into batches of a specified length. The last batch may be shorter if the length of the iterable is not a multiple of the batch size. It raises a ValueError if n is less than one.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef batched(iterable, n):\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --> ABC DEF G\n    if n < 1:\n        raise ValueError('n must be at least one')\n    it = iter(iterable)\n    while (batch := tuple(islice(it, n))):\n        yield batch\n```\n\n----------------------------------------\n\nTITLE: Fetching Batch Processing Results from OpenAI\nDESCRIPTION: This code obtains the output file ID from the completed batch job, then downloads the content into memory. It relies on the SDK's content retrieval method. The purpose is to access the aggregated results for analysis and display. Dependencies include the SDK and prior batch completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n# Retrieving result file\n\nresult_file_id = batch_job.output_file_id\nresult = client.files.content(result_file_id).content\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries\nDESCRIPTION: Imports essential Python libraries, including the Weaviate client, Pandas for data manipulation, and other utilities for type hinting, file operations, and setting up the OpenAI API key.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nfrom typing import List, Iterator\nimport pandas as pd\nimport numpy as np\nimport os\nimport wget\nfrom ast import literal_eval\n\n# Weaviate's client library for Python\nimport weaviate\n\n# I've set this to our new embeddings model, this can be changed to the embedding model of your choice\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI ChatCompletion with Logprobs and Bias in Python\nDESCRIPTION: Makes an API call to OpenAI's chat completions endpoint with specific parameters including logprobs set to 1 to retrieve token probabilities, logit bias to favor certain tokens, and temperature set to 0 for deterministic results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse = openai.chat.completions.create(\n    model=OPENAI_MODEL,\n    prompt=prompt.format(query=query, document=content),\n    temperature=0,\n    logprobs=1,\n    logit_bias={3363: 1, 1400: 1},\n    max_tokens=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Encoding Image to Base64 in Python\nDESCRIPTION: This code snippet defines the function `encode_image_to_base64`, which takes an image path as input, reads the image in binary format, encodes it to base64, and returns the base64 encoded string. It requires the `base64` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\ndef encode_image_to_base64(image_path):\n    with open(image_path, 'rb') as image_file:\n        encoded_image = base64.b64encode(image_file.read())\n        return encoded_image.decode('utf-8')\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Engines with Similarity Top-K for VectorStoreIndex in Python\nDESCRIPTION: Sets up query engines from the pre-built vector indices with a parameter similarity_top_k=3 that limits the retrieval context to the 3 most relevant nodes (document chunks) for answering queries. This controls the number of text chunks used as input context for LLM generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n```\n\n----------------------------------------\n\nTITLE: Loading Scientific Text Corpus Dataset with pandas in Python\nDESCRIPTION: Reads the SciFact corpus dataset, containing paper titles and abstracts, from a JSON Lines file into a pandas DataFrame. The corpus provides supporting context for claims to improve question answering. The snippet previews the loaded data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Load the corpus into a dataframe\ncorpus_df = pd.read_json(f'{data_path}/scifact_corpus.jsonl', lines=True)\ncorpus_df.head()\n```\n\n----------------------------------------\n\nTITLE: Generating Custom Image with Quality and Size Parameters in Python\nDESCRIPTION: Generates an image with specific quality settings (low), compression level (50%), output format (JPEG), and portrait dimensions (1024x1536).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Generate the image\nresult2 = client.images.generate(\n    model=\"gpt-image-1\",\n    prompt=prompt2,\n    quality=\"low\",\n    output_compression=50,\n    output_format=\"jpeg\",\n    size=\"1024x1536\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running QA with Custom Prompt\nDESCRIPTION: This code selects 5 random questions from the `questions` list and uses the `custom_qa` chain to answer them, applying the custom prompt. The `custom_qa.run(question)` method passes each question to the chain. The results, which incorporate the custom prompt's logic, are printed to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrandom.seed(41)\nfor question in random.choices(questions, k=5):\n    print(\">\", question)\n    print(custom_qa.run(question), end=\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Original and Variations of Images\nDESCRIPTION: This code snippet displays the original generated image and then the generated variations, using the PIL library. It prints the original image's file path and displays it, then iterates through the variation images and prints their file paths before displaying each variation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# print the original image\nprint(generated_image_filepath)\ndisplay(Image.open(generated_image_filepath))\n\n# print the new variations\nfor variation_image_filepaths in variation_image_filepaths:\n    print(variation_image_filepaths)\n    display(Image.open(variation_image_filepaths))\n```\n\n----------------------------------------\n\nTITLE: Executing Vector Search Query and Printing Results in Python\nDESCRIPTION: Sets a sample query string, calls the `query_results` function to execute the vector search, and then iterates through the returned documents. It prints the title and plot of each resulting movie document to demonstrate the semantic search results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nquery=\"imaginary characters from outerspace at war with earthlings\"\nmovies = query_results(query, 5)\n\nfor movie in movies:\n    print(f'Movie Name: {movie[\"title\"]},\\nMovie Plot: {movie[\"plot\"]}\\n')\n\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-computed Embeddings\nDESCRIPTION: This snippet downloads a zip file containing pre-computed Wikipedia article embeddings from a specified URL using the `wget` library. The file is approximately 700 MB, and the download process may take several minutes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB. Importing it will take several minutes.\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for OpenAI and Pinecone in Python\nDESCRIPTION: Installs the necessary Python packages required for working with OpenAI API, Pinecone vector database client, and Hugging Face datasets library. This is a prerequisite step to enable embedding generation, data loading, and vector search functionalities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU openai pinecone-client datasets\n```\n\n----------------------------------------\n\nTITLE: Applying Update Action to File Content in Python\nDESCRIPTION: Generates the updated content of a file given its original text and a `PatchAction` of type `UPDATE`. It iterates through the `chunks` in the action, applying insertions and deletions to the original lines (`orig_lines`) sequentially. Raises `DiffError` for invalid actions or chunk indices.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef _get_updated_file(text: str, action: PatchAction, path: str) -> str:\n    if action.type is not ActionType.UPDATE:\n        raise DiffError(\"_get_updated_file called with non-update action\")\n    orig_lines = text.split(\"\\n\")\n    dest_lines: List[str] = []\n    orig_index = 0\n\n    for chunk in action.chunks:\n        if chunk.orig_index > len(orig_lines):\n            raise DiffError(\n                f\"{path}: chunk.orig_index {chunk.orig_index} exceeds file length\"\n            )\n        if orig_index > chunk.orig_index:\n            raise DiffError(\n                f\"{path}: overlapping chunks at {orig_index} > {chunk.orig_index}\"\n            )\n\n        dest_lines.extend(orig_lines[orig_index : chunk.orig_index])\n        orig_index = chunk.orig_index\n\n        dest_lines.extend(chunk.ins_lines)\n        orig_index += len(chunk.del_lines)\n\n    dest_lines.extend(orig_lines[orig_index:])\n    return \"\\n\".join(dest_lines)\n```\n\n----------------------------------------\n\nTITLE: Testing Paper Summarization Function - Python\nDESCRIPTION: This snippet demonstrates how to call the `summarize_text` function with a predefined query to test its functionality. The result of the function call is stored in the `chat_test_response` variable.\n\nRequired Dependencies: Requires the `summarize_text` function to be defined.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Test the summarize_text function works\nchat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenAI API Key Configuration\nDESCRIPTION: Tests if the OpenAI API key is correctly set as an environment variable. Includes an alternative method for temporarily setting the key in the current Python session.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Saving Alpha Channel Mask to File in Python\nDESCRIPTION: Writes the generated mask with alpha channel to a file for later use in masked image editing operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Save the resulting file\nimg_path_mask_alpha = \"imgs/mask_alpha.png\"\nwith open(img_path_mask_alpha, \"wb\") as f:\n    f.write(mask_bytes)\n```\n\n----------------------------------------\n\nTITLE: Querying Redis HNSW Index with OpenAI Embeddings - Python\nDESCRIPTION: Shows how to use the `search_redis` function against a Redis index built with the HNSW algorithm. Retrieves the top 10 documents relevant to 'modern art in Europe' using the specified approximate matching index. The HNSW index must be constructed and populated before running this code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nresults = search_redis(redis_client, 'modern art in Europe', index_name=HNSW_INDEX_NAME, k=10)\n```\n\n----------------------------------------\n\nTITLE: Creating Thread and Running Quiz Request (Python)\nDESCRIPTION: Creates a new thread and initiates a run using the OpenAI Assistants API, requesting a quiz with two questions. Waits for run to finish, then checks the run status.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nthread, run = create_thread_and_run(\n    \"Make a quiz with 2 questions: One open ended, one multiple choice. Then, give me feedback for the responses.\"\n)\nrun = wait_on_run(run, thread)\nrun.status\n```\n\n----------------------------------------\n\nTITLE: Creating Data Directory\nDESCRIPTION: This code snippet creates a directory named `./data/papers` if it doesn't already exist.  It uses the `os` module to check for the directory's existence and create it if necessary. This directory is intended to store downloaded papers.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndirectory = './data/papers'\n\n# Check if the directory already exists\nif not os.path.exists(directory):\n    # If the directory doesn't exist, create it and any necessary intermediate directories\n    os.makedirs(directory)\n    print(f\"Directory '{directory}' created successfully.\")\nelse:\n    # If the directory already exists, print a message indicating it\n    print(f\"Directory '{directory}' already exists.\")\n```\n\n----------------------------------------\n\nTITLE: Executing Generic Vector Similarity Search (Python)\nDESCRIPTION: Calls the `find_quote_and_author_p` function with only a query string ('We struggle all our life for nothing') and limit (3), demonstrating a standard vector similarity search across the entire partitioned table without explicitly filtering by author.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nfind_quote_and_author_p(\"We struggle all our life for nothing\", 3)\n```\n\n----------------------------------------\n\nTITLE: Adding File to Vector Store Node.js\nDESCRIPTION: This snippet adds a file to a vector store using Node.js and the OpenAI API. It employs an `async` operation to ensure completion, using `createAndPoll`. The code requires `vector_store_id` and `file_id` to properly add the file and uses asynchronous operation to verify completion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_16\n\nLANGUAGE: node.js\nCODE:\n```\nconst file = await openai.beta.vectorStores.files.createAndPoll(\n  \"vs_abc123\",\n  { file_id: \"file-abc123\" }\n);\n```\n\n----------------------------------------\n\nTITLE: Training and Using Verifier Models for Math Word Problems\nDESCRIPTION: This approach involves fine-tuning a separate verifier model trained on labeled solutions to evaluate and select the most accurate answer generated by the main model. It aims to boost task accuracy, especially in math problem-solving, at the expense of increased computational costs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Retrieving Fine-Tuning Job Status and Model Name using OpenAI Python SDK\nDESCRIPTION: This snippet retrieves the current status and the model name of a specific fine-tuning job using the OpenAI Python client. It depends on the 'client' object and a defined 'fine_tuning_job' with its 'id'. The purpose is to monitor job progress and obtain the model for inference after training completes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nstatus = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n```\n\n----------------------------------------\n\nTITLE: Import libraries and set embedding model in Python\nDESCRIPTION: This snippet imports essential libraries including openai, pandas, numpy, and Typesense client, and defines the embedding model to be used. It also suppresses specific warnings to avoid unnecessary logs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n\nfrom typing import List, Iterator\nimport pandas as pd\nimport numpy as np\nimport os\nimport wget\nfrom ast import literal_eval\n\nimport typesense\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n```\n\n----------------------------------------\n\nTITLE: Verifying OPENAI_API_KEY Environment Variable - Bash/Zsh\nDESCRIPTION: This shell command prints the value of the 'OPENAI_API_KEY' environment variable to verify successful export. No additional dependencies are required. The expected output is the string of the API key, confirming proper variable assignment and availability in the terminal session.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\necho $OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Describe Snowflake Security Integration (SQL)\nDESCRIPTION: Retrieves the configuration details of the previously created 'CHATGPT_INTEGRATION' security integration. This command outputs essential information, including the OAuth Authorization Endpoint (Auth URL) and OAuth Token Endpoint (Token URL), which are required for configuring the OAuth client in ChatGPT.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE SECURITY INTEGRATION CHATGPT_INTEGRATION;\n```\n\n----------------------------------------\n\nTITLE: Execute similarity search on article titles\nDESCRIPTION: Performs a vector search for the phrase 'modern art in Europe' within the title vectors, then retrieves and displays the top results with their titles and distances.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\nimport numpy as np\n\nquery_result = query_tair(client=client, query=\"modern art in Europe\", vector_name=\"title_vector\")\nfor i in range(len(query_result)):\n    title = client.tvs_hmget(index+\"_\"+\"content_vector\", query_result[i][0].decode('utf-8'), \"title\")\n    print(f\"{i + 1}. {title[0].decode('utf-8')} (Distance: {round(query_result[i][1],3)})\")\n```\n\n----------------------------------------\n\nTITLE: Searching for a file in a specific S3 bucket\nDESCRIPTION: This snippet shows how to search for a file containing a specific word within a specific S3 bucket. It utilizes the `run_conversation` function with an f-string as input. The `search_word` variable stores the word to search, and `bucket_name` stores the bucket's name.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsearch_word = '<file_name_part>'\nbucket_name = '<bucket_name>'\nprint(run_conversation(f'search for a file contains {search_word} in {bucket_name}'))\n```\n\n----------------------------------------\n\nTITLE: Setting up Image Directory\nDESCRIPTION: This snippet sets up a directory to save the generated images. It defines the directory name and path, then creates the directory if it does not already exist. The path is relative to the current working directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# set a directory to save DALLÂ·E images to\nimage_dir_name = \"images\"\nimage_dir = os.path.join(os.curdir, image_dir_name)\n\n# create the directory if it doesn't yet exist\nif not os.path.isdir(image_dir):\n    os.mkdir(image_dir)\n\n# print the directory to save to\nprint(f\"{image_dir=}\")\n```\n\n----------------------------------------\n\nTITLE: Downloading a file from an S3 bucket\nDESCRIPTION: This snippet demonstrates downloading a file from a specific S3 bucket to a local directory. It uses the `run_conversation` function to initiate the download process. The `search_file`, `bucket_name`, and `local_directory` variables specify the file name, bucket name, and local directory, respectively.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsearch_file = '<file_name>'\nbucket_name = '<bucket_name>'\nlocal_directory = '<directory_path>'\nprint(run_conversation(f'download {search_file} from {bucket_name} bucket to {local_directory} directory'))\n```\n\n----------------------------------------\n\nTITLE: Retrieve Snowflake OAuth Client Secrets (SQL)\nDESCRIPTION: Executes the system function 'SYSTEM$SHOW_OAUTH_CLIENT_SECRETS' to retrieve the OAuth Client ID and OAuth Client Secret for the 'CHATGPT_INTEGRATION'. These credentials are required for authenticating the client application (ChatGPT) with Snowflake during the OAuth flow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb#_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT \ntrim(parse_json(SYSTEM$SHOW_OAUTH_CLIENT_SECRETS('CHATGPT_INTEGRATION')):OAUTH_CLIENT_ID) AS OAUTH_CLIENT_ID\n, trim(parse_json(SYSTEM$SHOW_OAUTH_CLIENT_SECRETS('CHATGPT_INTEGRATION')):OAUTH_CLIENT_SECRET) AS OAUTH_CLIENT_SECRET;\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Predictions and Displaying Confusion Matrix in Python\nDESCRIPTION: Uses the `assess_claims` function to infer labels for sampled claims and then compares these with ground truth labels by computing and printing the confusion matrix. This step enables analysis of model bias and performance without additional context.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ngpt_inferred = assess_claims(claims)\nconfusion_matrix(gpt_inferred, groundtruth)\n```\n\n----------------------------------------\n\nTITLE: Execute near_text_weaviate with different query\nDESCRIPTION: This snippet calls the `near_text_weaviate` function with a different query related to Scottish history and prints the titles and certainty/distance score of the returned articles. The output is formatted similarly to previous `near_text_weaviate` example.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nquery_result = near_text_weaviate(\"Famous battles in Scottish history\",\"Article\")\ncounter = 0\nfor article in query_result:\n    counter += 1\n    print(f\"{counter}. { article['title']} (Certainty: {round(article['_additional']['certainty'],3) }) (Distance: {round(article['_additional']['distance'],3) })\")\n```\n\n----------------------------------------\n\nTITLE: Example Moderation API Response Structure\nDESCRIPTION: Illustrates the JSON structure returned by the OpenAI Moderation API. It includes an ID, the model used, and a results array containing flags indicating if the content is harmful, a breakdown of violated categories, and confidence scores for each category.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/moderation.txt#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"modr-XXXXX\",\n    \"model\": \"text-moderation-007\",\n    \"results\": [\n        {\n            \"flagged\": true,\n            \"categories\": {\n                \"sexual\": false,\n                \"hate\": false,\n                \"harassment\": false,\n                \"self-harm\": false,\n                \"sexual/minors\": false,\n                \"hate/threatening\": false,\n                \"violence/graphic\": false,\n                \"self-harm/intent\": false,\n                \"self-harm/instructions\": false,\n                \"harassment/threatening\": true,\n                \"violence\": true\n            },\n            \"category_scores\": {\n                \"sexual\": 1.2282071e-6,\n                \"hate\": 0.010696256,\n                \"harassment\": 0.29842457,\n                \"self-harm\": 1.5236925e-8,\n                \"sexual/minors\": 5.7246268e-8,\n                \"hate/threatening\": 0.0060676364,\n                \"violence/graphic\": 4.435014e-6,\n                \"self-harm/intent\": 8.098441e-10,\n                \"self-harm/instructions\": 2.8498655e-11,\n                \"harassment/threatening\": 0.63055265,\n                \"violence\": 0.99011886\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Product Text Feature\nDESCRIPTION: This snippet creates a new feature 'product_text' by combining the product display name, master category, subcategory, color, and gender information. It converts the text to lowercase and renames the 'id' column to 'product_id'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf[\"product_text\"] = df.apply(lambda row: f\"name {row['productDisplayName']} category {row['masterCategory']} subcategory {row['subCategory']} color {row['baseColour']} gender {row['gender']}\".lower(), axis=1)\ndf.rename({\"id\":\"product_id\"}, inplace=True, axis=1)\n\ndf.info()\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Running Compare-and-Contrast Queries on Multiple Financial Indices in Python\nDESCRIPTION: Performs asynchronous queries on the SubQuestionQueryEngine to compare and contrast customer segments, geographies, and revenue growth between Lyft and Uber from 2020 to 2021. The engine generates sub-queries for each company and synthesizes the aggregated results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse = await s_engine.aquery('Compare and contrast the customer segments and geographies that grew the fastest')\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(response)\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = await s_engine.aquery('Compare revenue growth of Uber and Lyft from 2020 to 2021')\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Searching News Articles via the News API and Deduplicating Results - Python\nDESCRIPTION: This snippet defines a function to search recent news via the News API based on a query, collects articles for all queries, and removes duplicate URLs. Dependencies include requests, tqdm, and News API access. Ensure NEWS_API_KEY is set. The function handles error cases and result aggregation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef search_news(\n    query: str,\n    news_api_key: str = news_api_key,\n    num_articles: int = 50,\n    from_datetime: str = \"2023-06-01\",  # the 2023 NBA finals were played in June 2023\n    to_datetime: str = \"2023-06-30\",\n) -> dict:\n    response = requests.get(\n        \"https://newsapi.org/v2/everything\",\n        params={\n            \"q\": query,\n            \"apiKey\": news_api_key,\n            \"pageSize\": num_articles,\n            \"sortBy\": \"relevancy\",\n            \"from\": from_datetime,\n            \"to\": to_datetime,\n        },\n    )\n\n    return response.json()\n\n\narticles = []\n\nfor query in tqdm(queries):\n    result = search_news(query)\n    if result[\"status\"] == \"ok\":\n        articles = articles + result[\"articles\"]\n    else:\n        raise Exception(result[\"message\"])\n\n# remove duplicates\narticles = list({article[\"url\"]: article for article in articles}.values())\n\nprint(\"Total number of articles:\", len(articles))\nprint(\"Top 5 articles of query 1:\", \"\\n\")\n\nfor article in articles[0:5]:\n    print(\"Title:\", article[\"title\"])\n    print(\"Description:\", article[\"description\"])\n    print(\"Content:\", article[\"content\"][0:100] + \"...\")\n    print()\n\n```\n\n----------------------------------------\n\nTITLE: Sample Box JWT Authentication Configuration (JSON)\nDESCRIPTION: This JSON structure shows a sample configuration (`jwt_config.json.sample`) needed for the Box SDK's JWT authentication method used in the accompanying Python Azure Function. It includes placeholders for Box application credentials like client ID, client secret, public key ID, private key, passphrase, and enterprise ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"boxAppSettings\": {\n      \"clientID\": \"12345\",\n      \"clientSecret\": \"abcde\",\n      \"appAuth\": {\n        \"publicKeyID\": \"123\",\n        \"privateKey\": \"-----BEGIN ENCRYPTED PRIVATE KEY-----\\nvwxyz==\\n-----END ENCRYPTED PRIVATE KEY-----\\n\",\n        \"passphrase\": \"lmnop\"\n      }\n    },\n    \"enterpriseID\": \"09876\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Extracting Fine-Tuned Model ID after Completion\nDESCRIPTION: This snippet fetches the fine-tuned model ID from the job status object. It requires the 'client' object, 'fine_tuning_job.id', and assumes the fine-tuning process has finished. The output is printed to identify which model to use for subsequent inference.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfine_tuned_model = client.fine_tuning.jobs.retrieve(fine_tuning_job.id).fine_tuned_model\nprint(f\"Fine tuned model id: {fine_tuned_model}\")\n```\n\n----------------------------------------\n\nTITLE: Querying Tabular Information from Documents with Semantic Search in Python\nDESCRIPTION: A more complex example that demonstrates querying information that requires interpretation of tabular data. This uses the previously extracted text from the document's visual elements using GPT-4o vision capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_vision_modality_for_RAG_with_Pinecone.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What was the increase in access to electricity between 2000 and 2012 in Western and Central Africa?\"\nanswer = get_response_to_question(question, index)\n\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Creating a Thread with Initial Message\nDESCRIPTION: This snippet demonstrates creating a Thread with an initial message using the OpenAI Assistants API. The message includes text content and a file attachment for the `code_interpreter` tool. Code examples are provided for Python, Node.js, and cURL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create(\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Create 3 data visualizations based on the trends in this file.\",\n      \"attachments\": [\n        {\n          \"file_id\": file.id,\n          \"tools\": [{\"type\": \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst thread = await openai.beta.threads.create({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Create 3 data visualizations based on the trends in this file.\",\n      attachments: [\n        {\n          file_id: file.id,\n          tools: [{type: \"code_interpreter\"}]\n        }\n      ]\n    }\n  ]\n});\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Create 3 data visualizations based on the trends in this file.\",\n        \"attachments\": [\n          {\n            \"file_id\": \"file-ACq8OjcLQm2eIG0BvRM4z5qX\",\n            \"tools\": [{\"type\": \"code_interpreter\"}]\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAPI Schema (YAML)\nDESCRIPTION: This snippet defines the basic structure and metadata for the OpenAPI specification, including the API's title, description, version, and server details. It lays the groundwork for describing individual API endpoints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_canvas.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.1.0\ninfo:\n  title: Canvas API\n  description: API for interacting with Canvas LMS, including courses, modules, module items, and search functionalities.\n  version: 1.0.0\nservers:\n  - url: https://canvas.instructure.com/api/v1\n    description: Canvas LMS API server\n    variables:\n      domain:\n        default: canvas.instructure.com\n        description: The domain of your Canvas instance\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client with API Key in Python\nDESCRIPTION: Sets up the OpenAI client in Python with an API key sourced from environment variables. This client is used for making chat completion requests. It involves importing the openai module, creating a client object, and providing the API key through environment variables with fallback placeholder.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Inspecting Embedding Output Structure in Python\nDESCRIPTION: Demonstrates how to explore the response object returned by the OpenAI embedding API call, examining keys and confirming that the 'data' field contains one embedding per input string, each with 1536 dimensions representing semantic features.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nres.keys()\n```\n\nLANGUAGE: python\nCODE:\n```\nlen(res['data'])\n```\n\nLANGUAGE: python\nCODE:\n```\nlen(res['data'][0]['embedding']), len(res['data'][1]['embedding'])\n```\n\n----------------------------------------\n\nTITLE: Printing Found Product Matches from Graph Query Result in Python\nDESCRIPTION: Iterates over the results returned from the graph query and prints the number and details (name and id) of the matching products. This snippet assumes the result is a list of dictionaries with product nodes accessible by the key 'p'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Result\nprint(f\"Found {len(result)} matching product(s):\\n\")\nfor r in result:\n    print(f\"{r['p']['name']} ({r['p']['id']})\")\n```\n\n----------------------------------------\n\nTITLE: Listing Messages in Thread (OpenAI Assistants API)\nDESCRIPTION: Retrieves and stores the current list of messages within the thread. This is done to inspect the thread's state and the Assistant's response after a run has potentially completed, specifically looking for generated content like image files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmessages = client.beta.threads.messages.list(thread_id=thread.id)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs all necessary Python packages such as OpenAI API client, boto3 for AWS interactions, tenacity for retry mechanisms, and python-dotenv for environment variable management. These dependencies are prerequisites for running the notebook successfully.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai\n! pip install boto3\n! pip install tenacity\n! pip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Selecting and Running Questions\nDESCRIPTION: This code selects 5 random questions from the `questions` list and uses the `qa` chain to answer them.  The `qa.run(question)` method passes each selected question to the QA chain, which uses OpenAI to generate an answer. The results are printed to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nrandom.seed(52)\nselected_questions = random.choices(questions, k=5)\n```\n\nLANGUAGE: python\nCODE:\n```\nfor question in selected_questions:\n    print(\">\", question)\n    print(qa.run(question), end=\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable in Python\nDESCRIPTION: Prompts the user to securely input their OpenAI API key without echoing it to the console, then sets it as an environment variable 'OPENAI_API_KEY'. This key is required for authenticating requests to OpenAI services such as embeddings and chat completion models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nos.environ['OPENAI_API_KEY'] = getpass.getpass()\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding CSV in Python\nDESCRIPTION: This Python code snippet reads a CSV file ('output.csv'), encodes it into base64 format, and prints the encoded string.  It utilizes the `base64` library.  It reads the CSV file in binary mode (`'rb'`) before encoding and decodes the result to UTF-8.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport base64 \n\n# Base64 encode the CSV file\nencoded_string = base64.b64encode(open('output.csv', 'rb').read()).decode('utf-8')\n\nprint(\"Base64 Encoded CSV:\")\nprint(encoded_string)\n```\n\n----------------------------------------\n\nTITLE: Discouraged JSON Format for Long Context Input (JSON)\nDESCRIPTION: Shows an example of structuring document context using a JSON array of objects, each containing `id`, `title`, and `content`. This format is noted to have performed poorly in long context testing compared to alternatives like XML or custom text formats.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_13\n\nLANGUAGE: JSON\nCODE:\n```\n[{â€œidâ€: 1, â€œtitleâ€: â€œThe Foxâ€, â€œcontentâ€: â€œThe quick brown fox jumped over the lazy dogâ€}]\n```\n\n----------------------------------------\n\nTITLE: Installing Kangas Library in Python\nDESCRIPTION: Installs the Kangas library using pip which is required for creating and visualizing DataGrids containing embedding data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install kangas --quiet\n```\n\n----------------------------------------\n\nTITLE: Adding File to Vector Store Python\nDESCRIPTION: This code adds a file to an existing vector store in a Python environment using the OpenAI API.  The `create_and_poll` function ensures the file is fully processed. It provides asynchronous file ingestion into the vector store. The code relies on the `vector_store_id` and the `file_id` for adding the specified file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfile = client.beta.vector_stores.files.create_and_poll(\n  vector_store_id=\"vs_abc123\",\n  file_id=\"file-abc123\"\n)\n```\n\n----------------------------------------\n\nTITLE: Graphical Model Representations of Chain of Thought and Related Prompting Techniques\nDESCRIPTION: This section presents visual models that interpret different prompting methodsâ€”such as chain of thought, fine-tuned prompting, selection-inference, and verifiersâ€”as graphical models. These representations help in understanding how these techniques can be combined and analyzed within a probabilistic framework.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md#_snippet_8\n\n\n\n----------------------------------------\n\nTITLE: Obtaining Secure User Inputs for Database Authentication - Python\nDESCRIPTION: This snippet prompts the user to securely provide the Astra DB access token (using 'getpass' for hidden input) and to input the database ID. These parameters are required for subsequent initialization of the database connection and authentication. The 'astra_token' and 'database_id' variables store the sensitive connection values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nastra_token = getpass(\"Please enter your Astra token ('AstraCS:...')\")\ndatabase_id = input(\"Please enter your database id ('3df2a5b6-...')\")\n\n```\n\n----------------------------------------\n\nTITLE: Post-Processing CSV into a DataFrame\nDESCRIPTION: This code reads the CSV file created earlier into a Pandas DataFrame. It then converts the 'title_vector', 'content_vector', 'vector_id', and 'category' columns to the appropriate types using `json.loads` and `str`. This prepares the data for further vector database operations.  It requires the `pandas` and `json` libraries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Convert the CSV file to a Dataframe\narticle_df = pd.read_csv(\"../embedded_data.csv\")\n# Read vectors from strings back into a list using json.loads\narticle_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\narticle_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\narticle_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\narticle_df[\"category\"] = article_df[\"category\"].apply(str)\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Saving Example Audio File (Python)\nDESCRIPTION: Sets the remote URL and local file path for an example earnings call audio file (`.wav`). It then downloads the file using `urllib.request.urlretrieve`. A line modifying the default SSL context (`ssl._create_default_https_context`) is included to bypass potential SSL certificate verification errors during the download.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# set download paths\nearnings_call_remote_filepath = \"https://cdn.openai.com/API/examples/data/EarningsCall.wav\"\n\n# set local save locations\nearnings_call_filepath = \"data/EarningsCall.wav\"\n\n# download example audio files and save locally\nssl._create_default_https_context = ssl._create_unverified_context\nurllib.request.urlretrieve(earnings_call_remote_filepath, earnings_call_filepath)\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Chat Messages for Testing Language Models in Python\nDESCRIPTION: Creates a list of example message dictionaries representing a user-system conversation with role annotations. These messages are used to test token counting functions across different models. The messages include system instructions and example exchanges with metadata such as names.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nexample_messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"New synergies will help drive top-line growth.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Things working well together will increase revenue.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_user\",\n        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n    },\n    {\n        \"role\": \"system\",\n        \"name\": \"example_assistant\",\n        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Loading Scientific Claims Dataset with pandas in Python\nDESCRIPTION: Reads the SciFact claim dataset from a JSON Lines file into a pandas DataFrame for manipulation and analysis. The dataset includes scientific claims that will be assessed using the question answering pipeline. The snippet assumes a relative path to the data and previews the loaded content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndata_path = '../../data'\n\nclaim_df = pd.read_json(f'{data_path}/scifact_claims.jsonl', lines=True)\nclaim_df.head()\n```\n\n----------------------------------------\n\nTITLE: Plotting Evaluation Results - Python\nDESCRIPTION: This code generates a bar chart for the evaluation scores. It first pivots the `run_df` DataFrame to aggregate the 'format' based on 'run' and 'evaluation_score'. Then, uses matplotlib to plot a bar chart. It ensures that each evaluation score is present for all runs by reindexing the data. The x-axis labels show the evaluation scores, and each bar represents the number of records for a specific evaluation score and run.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nevaluation_df_pivot = pd.pivot_table(\n    run_df,\n    values='format',\n    index=['run','evaluation_score'],\n    aggfunc='count'\n)\nevaluation_df_pivot.columns = ['Number of records']\nevaluation_df_pivot\n```\n\nLANGUAGE: python\nCODE:\n```\n# Reset index without dropping the 'run' and 'evaluation_score' columns\nevaluation_df_pivot.reset_index(inplace=True)\n\n# Plotting\nplt.figure(figsize=(10, 6))\n\nbar_width = 0.35\n\n# OpenAI brand colors\nopenai_colors = ['#00D1B2', '#000000']  # Green, Black\n\n# Identify unique runs and evaluation scores\nunique_runs = evaluation_df_pivot['run'].unique()\nunique_evaluation_scores = evaluation_df_pivot['evaluation_score'].unique()\n\n# Repeat colors if there are more runs than colors\ncolors = openai_colors * (len(unique_runs) // len(openai_colors) + 1)\n\nfor i, run in enumerate(unique_runs):\n    # Select rows for this run only\n    run_data = evaluation_df_pivot[evaluation_df_pivot['run'] == run].copy()\n    \n    # Ensure every 'evaluation_score' is present\n    run_data.set_index('evaluation_score', inplace=True)\n    run_data = run_data.reindex(unique_evaluation_scores, fill_value=0)\n    run_data.reset_index(inplace=True)\n    \n    # Plot each bar\n    positions = np.arange(len(unique_evaluation_scores)) + i * bar_width\n    plt.bar(\n        positions,\n        run_data['Number of records'],\n        width=bar_width,\n        label=f'Run {run}',\n        color=colors[i]\n    )\n\n# Configure the x-axis to show evaluation scores under the grouped bars\nplt.xticks(np.arange(len(unique_evaluation_scores)) + bar_width / 2, unique_evaluation_scores)\n\nplt.xlabel('Evaluation Score')\nplt.ylabel('Number of Records')\nplt.title('Evaluation Scores vs Number of Records for Each Run')\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Expanding List-Like DataFrame Columns and Extracting Rank/Token Metrics in Python\nDESCRIPTION: This snippet defines expand_lists, a utility to convert DataFrame columns holding lists into a stacked, row-wise format for statistical analysis. It adds columns for search result rank and token count by extracting results from the expanded DataFrame. Requires pandas and assumes column names and ADA search output structures. Output is a reformatted DataFrame where each row represents a decomposed question entry.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef expand_lists(out):\n    \"\"\"\n    Expand a pandas series containing lists into a series, where each list element becomes a value on its own\n\n    Input is a row per paragraph, which has multiple questions\n    Output is a row per question\n    \"\"\"\n    cols = [pd.DataFrame(out[name].tolist()).stack().reset_index(level=1, drop=True).rename(name) for name in out.columns] \n    return pd.concat(cols, axis=1)\n\nout_expanded = expand_lists(out)\nout_expanded['rank'] = out_expanded.ada.apply(lambda x: x[0] if x != [] else -2)\nout_expanded['tokens'] = out_expanded.ada.apply(lambda x: x[1] if x != [] else -2)\n```\n\n----------------------------------------\n\nTITLE: Processing DataFrame Columns - Python\nDESCRIPTION: Processes the `title_vector` and `content_vector` columns by converting the string representations of vectors into actual lists using `literal_eval`.  It also converts the `vector_id` column to string type. This conversion is necessary because the data is initially stored as strings in the CSV file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Calling the Token Highlighter Function in Python\nDESCRIPTION: This snippet demonstrates how to call the highlight_text function with an API response object to display the colorized tokens.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nhighlight_text(API_RESPONSE)\n```\n\n----------------------------------------\n\nTITLE: Writing training and validation data to JSONL files\nDESCRIPTION: Saves the prepared training and validation datasets to JSONL files for uploading to OpenAI's fine-tuning service.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntraining_file_name = \"tmp_recipe_finetune_training.jsonl\"\nwrite_jsonl(training_data, training_file_name)\n\nvalidation_file_name = \"tmp_recipe_finetune_validation.jsonl\"\nwrite_jsonl(validation_data, validation_file_name)\n```\n\n----------------------------------------\n\nTITLE: Downloading Embedded Data - Python\nDESCRIPTION: Downloads a zipped file containing pre-computed embeddings data from a specified URL using the `wget` library.  The `embeddings_url` variable holds the URL of the data file. This step is a prerequisite for loading and indexing the data in MyScale.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Downloading Precomputed Embeddings Dataset\nDESCRIPTION: Downloads a ZIP file containing precomputed OpenAI embeddings for Wikipedia articles using wget.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Processing Extraction Results\nDESCRIPTION: This code processes the extracted results by splitting them into groups based on newline characters and then zipping the groups together. It filters out any results that contain \"Not specified\" or \"__\".\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngroups = [r.split('\\n') for r in results]\n\n# zip the groups together\nzipped = list(zip(*groups))\nzipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\nzipped\n```\n\n----------------------------------------\n\nTITLE: Generating Responses Without Seed\nDESCRIPTION: Demonstrates generating multiple responses from the Chat Completion API without specifying a seed value.  This showcases the default non-deterministic behavior.  It then calculates the average distance between the responses to quantify their similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntopic = \"a journey to Mars\"\nsystem_message = \"You are a helpful assistant.\"\nuser_request = f\"Generate a short excerpt of news about {topic}.\"\n\nresponses = []\n\n\nasync def get_response(i):\n    print(f'Output {i + 1}\\n{\"-\" * 10}')\n    response = await get_chat_response(\n        system_message=system_message, user_request=user_request\n    )\n    return response\n\n\nresponses = await asyncio.gather(*[get_response(i) for i in range(5)])\naverage_distance = calculate_average_distance(responses)\nprint(f\"The average similarity between responses is: {average_distance}\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Astra DB Credentials in Python\nDESCRIPTION: Prompts the user to input the Astra DB API Endpoint and Token securely. These credentials are necessary for database connection and must be provisioned in the Astra DB dashboard. 'input' is used for the endpoint, 'getpass' for secure token entry.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nASTRA_DB_API_ENDPOINT = input(\"Please enter your API Endpoint:\")\nASTRA_DB_APPLICATION_TOKEN = getpass(\"Please enter your Token\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Embeddings into DataFrame (Pandas)\nDESCRIPTION: Reads the extracted CSV file into a pandas DataFrame. It converts the string representations of vectors in the 'title_vector' and 'content_vector' columns back into Python lists using `literal_eval`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n\nfrom ast import literal_eval\n\narticle_df = pd.read_csv('/lakehouse/default/Files/data/vector_database_wikipedia_articles_embedded.csv')\n# Read vectors from strings back into a list\narticle_df[\"title_vector\"] = article_df.title_vector.apply(literal_eval)\narticle_df[\"content_vector\"] = article_df.content_vector.apply(literal_eval)\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Import necessary libraries\nDESCRIPTION: Imports required Python libraries: 'openai' for interacting with the OpenAI API, 'asyncio' for asynchronous operations, 'IPython.display' for displaying HTML outputs, and custom utilities from 'utils.embeddings_utils' for handling embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport asyncio\nfrom IPython.display import display, HTML\n\nfrom utils.embeddings_utils import (\n    get_embedding,\n    distances_from_embeddings\n)\n\nGPT_MODEL = \"gpt-3.5-turbo-1106\"\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Data\nDESCRIPTION: Downloads a zipped dataset containing pre-embedded Wikipedia articles from a specified URL using wget, then extracts the data into a CSV file within a specified directory using the zipfile module.  The URL points to a publicly available dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Creating a fine-tuning job via Python SDK\nDESCRIPTION: This code illustrates initiating a fine-tuning training job using the OpenAI Python SDK. It specifies the uploaded file ID and the base model to fine-tune, with optional customization via additional parameters. It necessitates the 'openai' package and valid API credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\nclient.fine_tuning.jobs.create(\n  training_file=\"file-abc123\",\n  model=\"gpt-3.5-turbo\"\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Model Accuracy on Validation Data\nDESCRIPTION: This snippet iterates through the models and prints their accuracy based on the validation dataset. The `get_accuracy` function (defined earlier) is used to calculate the accuracy for each model. Requires the `get_accuracy` function and the processed validation dataset. The output is the accuracy score of each model in the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\nfor model in models:\n    print(f\"{model} accuracy: {get_accuracy(model, another_subset) * 100:.2f}%\")\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading Environment Variables in Python\nDESCRIPTION: Imports the necessary Python libraries (`os`, `openai`, `dotenv`). It then calls `dotenv.load_dotenv()` to load environment variables defined in a `.env` file into the application's environment, making them accessible via `os.environ`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nimport dotenv\n\ndotenv.load_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies via Pip (Shell)\nDESCRIPTION: Installs necessary Python packages (`openai`, `pymilvus`, `datasets`, `tqdm`) required for interacting with OpenAI, Milvus, downloading data, and displaying progress bars. This command should be run in an environment with pip access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! pip install openai pymilvus datasets tqdm\n```\n\n----------------------------------------\n\nTITLE: Downloading Image Files Generated by Code Interpreter\nDESCRIPTION: Provides code examples for downloading the binary content of an image file generated by Code Interpreter, using the `file_id` obtained from the Assistant message response. The examples show how to retrieve the content via the Files API and save it locally.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-code-interpreter.txt#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nimage_data = client.files.content(\"file-abc123\")\nimage_data_bytes = image_data.read()\n\nwith open(\"./my-image.png\", \"wb\") as file:\n    file.write(image_data_bytes)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nimport OpenAI from \"openai\";\nimport fs from \"fs\"; // Assuming fs is available\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const response = await openai.files.content(\"file-abc123\");\n\n  // Extract the binary data from the Response object\n  const image_data = await response.arrayBuffer();\n\n  // Convert the binary data to a Buffer\n  const image_data_buffer = Buffer.from(image_data);\n\n  // Save the image to a specific location\n  fs.writeFileSync(\"./my-image.png\", image_data_buffer);\n}\n\nmain();\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/files/file-abc123/content \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --output image.png\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAG\nDESCRIPTION: This snippet imports required libraries and modules for implementing a RAG pipeline. These include modules for asynchronous operations, evaluation, data loading, node parsing, LLM interaction, and general utility operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index.evaluation import generate_question_context_pairs\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.evaluation import generate_question_context_pairs\nfrom llama_index.evaluation import RetrieverEvaluator\nfrom llama_index.llms import OpenAI\n\nimport os\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Loading JSON Data into Pandas DataFrame in Python\nDESCRIPTION: Reads the content of the JSON file specified by `file_path` directly into a pandas DataFrame named `df`. It then displays the first few rows of the DataFrame using the `.head()` method for initial inspection. Requires the `pandas` library and the JSON file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf =  pd.read_json(file_path)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Processing Validation Dataset with Models\nDESCRIPTION: This code applies the `process_dataframe` function to a validation dataset using a list of models. Before applying the process, it adds a fine-tuned model to the existing list. It requires `process_dataframe` function and the existing list of models.  The expected output is the processed dataset which has model outputs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Leveraging_model_distillation_to_fine-tune_a_model.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\nvalidation_dataset = df_france.sample(n=300)\n\nmodels.append(fine_tuned_model)\n\nfor model in models:\n    another_subset = process_dataframe(validation_dataset, model)\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Function Dependencies (Shell)\nDESCRIPTION: Installs the necessary Node.js packages, `@google-cloud/functions-framework` for defining the function and `axios` for making HTTP requests, into the project's `node_modules` directory and saves them to `package.json`. These libraries are essential for the function's operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @google-cloud/functions-framework\nnpm install axios\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Source Python\nDESCRIPTION: Retrieves the source URL of a specific document. The code retrieves the metadata associated with a document, specifically its 'source' attribute, which represents the URL where the document was originally located. The 'rtdocs/' prefix is replaced to reconstruct the full URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocs[5].metadata['source'].replace('rtdocs/', 'https://')\n```\n\n----------------------------------------\n\nTITLE: Unzipping and Listing Embeddings Data File using Shell Commands\nDESCRIPTION: Uses shell commands executed from the notebook environment to unzip the embeddings archive (`unzip -n` prevents overwriting existing files) and then list the details (`ls -lh`) of the extracted CSV file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n!unzip -n vector_database_wikipedia_articles_embedded.zip\n!ls -lh vector_database_wikipedia_articles_embedded.csv\n```\n\n----------------------------------------\n\nTITLE: Creating Test Inputs for Guardrails in Python\nDESCRIPTION: This snippet provides a sample user request string expected to pass both topical and moderation guardrails, serving as a reference for what a compliant input looks like. No external dependencies are required; it is used directly in guardrail function testing. The test request should not trigger any filtering or blocking logic.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Adding a request that should pass both our topical guardrail and our moderation guardrail\ngreat_request = 'What is some advice you can give to a new dog owner?'\n\n```\n\n----------------------------------------\n\nTITLE: Testing Function Locally (Shell)\nDESCRIPTION: Executes the defined Google Cloud Function locally using the installed Functions Framework. This command allows developers to test the `executeGCPFunction` endpoint via HTTP requests on their local machine before deployment, aiding in debugging and verification.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpx @google-cloud/functions-framework --target=executeGCPFunction\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenAI Fine-tuning with Weights and Biases using cURL\nDESCRIPTION: cURL command demonstrating how to create a fine-tuning job with Weights and Biases integration enabled, specifying project, tags and other configuration parameters.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_23\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X POST \\\\\n    -H \"Content-Type: application/json\" \\\\\n    -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\\\n    -d '{\n    \"model\": \"gpt-3.5-turbo-0125\",\n    \"training_file\": \"file-ABC123\",\n    \"validation_file\": \"file-DEF456\",\n    \"integrations\": [\n        {\n            \"type\": \"wandb\",\n            \"wandb\": {\n                \"project\": \"custom-wandb-project\",\n                \"tags\": [\"project:tag\", \"lineage\"]\n            }\n        }\n    ]\n}' https://api.openai.com/v1/fine_tuning/jobs\n```\n\n----------------------------------------\n\nTITLE: Verify Completions and Retrieve First Completion - Python\nDESCRIPTION: This snippet retrieves and asserts that chat completions exist. It uses `client.chat.completions.list()` to get the completions and then asserts that the data array is not empty. Finally it prints the first element of the data array.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\ncompletions = await client.chat.completions.list()\nassert completions.data, \"No completions found. You may need to enable logs in your admin panel.\"\ncompletions.data[0]\n```\n```\n\n----------------------------------------\n\nTITLE: Using a fine-tuned model in Chat Completions via Node.js\nDESCRIPTION: This snippet depicts how to make a chat completion request with a fine-tuned model using Node.js, including message exchange and model specification. It requires proper SDK setup and API credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_9\n\nLANGUAGE: node.js\nCODE:\n```\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: \"system\", content: \"You are a helpful assistant.\" }],\n    model: \"ft:gpt-3.5-turbo:my-org:custom_suffix:id\",\n  });\n  console.log(completion.choices[0]);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Integrating Vector Search with LLM in OpenAI Responses API\nDESCRIPTION: Uses the file_search tool in the Responses API to combine vector search and LLM response generation in a single API call, with gpt-4o-mini model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What's Deep Research?\"\nresponse = client.responses.create(\n    input= query,\n    model=\"gpt-4o-mini\",\n    tools=[{\n        \"type\": \"file_search\",\n        \"vector_store_ids\": [vector_store_details['id']],\n    }]\n)\n\n# Extract annotations from the response\nannotations = response.output[1].content[0].annotations\n    \n# Get top-k retrieved filenames\nretrieved_files = set([result.filename for result in annotations])\n\nprint(f'Files used: {retrieved_files}')\nprint('Response:')\nprint(response.output[1].content[0].text) # 0 being the filesearch call\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI and Pinecone Libraries\nDESCRIPTION: This snippet imports necessary classes from the OpenAI and Pinecone Python SDKs to enable embedding generation and vector database operations. It initializes clients for both services, necessary for data embedding and index management.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nfrom openai import OpenAI\nclient = OpenAI()\n\npc = Pinecone(api_key=\"YOUR API KEY\")\n## OpenAI key by default is set to the environment variable `OPENAI_API_KEY`\n```\n\n----------------------------------------\n\nTITLE: Initializing Microsoft Graph Client in JavaScript for SharePoint Access\nDESCRIPTION: This function initializes the Microsoft Graph client with an access token to enable searching through Office 365 and SharePoint. It creates a client instance with authentication already configured for subsequent API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Client } = require('@microsoft/microsoft-graph-client');\n\nfunction initGraphClient(accessToken) {\n    return Client.init({\n        authProvider: (done) => {\n            done(null, accessToken);\n        }\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Microsoft Graph Client in JavaScript\nDESCRIPTION: This code snippet initializes the Microsoft Graph client with an access token for authenticating requests to search through Office 365 and SharePoint content. It requires the @microsoft/microsoft-graph-client package.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Client } = require('@microsoft/microsoft-graph-client');\n\nfunction initGraphClient(accessToken) {\n    return Client.init({\n        authProvider: (done) => {\n            done(null, accessToken);\n        }\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Value Counts - Python\nDESCRIPTION: This snippet displays the value counts for columns within the `results_2_df` DataFrame. It's used to analyze the distribution of values within the 'unit_test_evaluation' and 'evaluation_score' columns of the results from the unit tests. The `value_counts()` method is called on each column to summarize the data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults_2_df['unit_test_evaluation'].value_counts()\n```\n\nLANGUAGE: python\nCODE:\n```\nresults_2_df['evaluation_score'].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataframe\nDESCRIPTION: Loads a CSV file containing embedded Wikipedia article data into a Pandas DataFrame. It then converts string representations of vectors back into lists using literal_eval, and casts vector_id to string type.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Read vectors from strings back into a list\narticle_df['title_vector'] = article_df.title_vector.apply(literal_eval)\narticle_df['content_vector'] = article_df.content_vector.apply(literal_eval)\n\n# Set vector_id to be a string\narticle_df['vector_id'] = article_df['vector_id'].apply(str)\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project (Shell)\nDESCRIPTION: Sets up a local directory and initializes a new Node.js project using standard shell commands and `npm init`. This step is required to create the file structure and `package.json` file needed for developing and managing dependencies for the Google Cloud Function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmkdir <directory_name>\ncd <directory_name>\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm init\n```\n\n----------------------------------------\n\nTITLE: Test Embedding Generation Call Using OpenAI in Python\nDESCRIPTION: Demonstrates a simple example call to get_embedding by embedding the string 'hi' with the selected embedding model. This snippet serves as a quick test or sanity check for the embedding function's configuration and API connectivity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\na = get_embedding(\"hi\", model=embedding_model)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies in Python\nDESCRIPTION: Installs necessary Python libraries for the project using pip: `openai` for interacting with the OpenAI API, `pandas` for data manipulation, `scikit-learn` for machine learning tasks (like clustering shown later), and `matplotlib` for plotting.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install openai\n%pip install pandas\n%pip install scikit-learn\n%pip install matplotlib\n```\n\n----------------------------------------\n\nTITLE: Installing required Python packages for Typesense, wget, OpenAI, and pandas\nDESCRIPTION: This snippet installs necessary Python libraries: 'typesense' for interacting with the vector database and 'wget' for downloading datasets. It sets up the environment before loading data or creating indices.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Install Typesense client\n!pip install typesense\n\n# Install wget for downloading files\n!pip install wget\n```\n\n----------------------------------------\n\nTITLE: Executing Sample Query on Pinecone Index\nDESCRIPTION: Demonstrates searching the Pinecone index with a medical query. This example shows how the RAG system retrieves relevant medical information based on semantic similarity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Example usage with a different query from the train/test set\nquery = (\n    \"A 45-year-old man with a history of alcohol use presents with symptoms including confusion, ataxia, and ophthalmoplegia. \"\n    \"What is the most likely diagnosis and the recommended treatment?\"\n)\nquery_pinecone_index(client, index, MODEL, query)\n```\n\n----------------------------------------\n\nTITLE: Defining Token Length Function Python\nDESCRIPTION: Defines a function to calculate the length of a text in terms of tokens using `tiktoken`. This function is crucial for chunking text into manageable sizes for embedding. It uses the `p50k_base` encoding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding('p50k_base')\n\n# create the length function\ndef tiktoken_len(text):\n    tokens = tokenizer.encode(\n        text,\n        disallowed_special=()\n    )\n    return len(tokens)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for OpenAI and Chroma\nDESCRIPTION: Installs required Python packages including OpenAI, ChromaDB, and pandas using pip in a Jupyter notebook environment. This setup is necessary to run subsequent code in the notebook that interacts with OpenAI APIs and handles data processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU openai chromadb pandas\n```\n\n----------------------------------------\n\nTITLE: Encoding images to base64 for GPT-4 Vision input in Python\nDESCRIPTION: Reads an image file, encodes its binary content to base64 string, and formats it for inline use in a GPT-4 Vision prompt, facilitating multimodal query input.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef encode_image(image_path):\n    with open(image_path, 'rb') as image_file:\n        encoded_image = base64.b64encode(image_file.read())\n        return encoded_image.decode('utf-8')\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for OpenAPI and OpenAI\nDESCRIPTION: Installs the `jsonref` library, used for resolving JSON references ($ref) within the OpenAPI specification, and the `openai` library, required for interacting with the OpenAI API. These installations are prerequisites for running the subsequent code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -q jsonref # for resolving $ref's in the OpenAPI spec\n!pip install -q openai\n```\n\n----------------------------------------\n\nTITLE: Example Chat Completions Prompt for Translation\nDESCRIPTION: This JSON structure shows the equivalent input format for the OpenAI Chat Completions API to achieve a similar translation task. It uses the messages array format with a single user message containing the translation instruction and the input text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_16\n\nLANGUAGE: JSON\nCODE:\n```\n[{\"role\": \"user\", \"content\": 'Translate the following English text to French: \"{text}\"'}]\n```\n\n----------------------------------------\n\nTITLE: Initialize AsyncOpenAI Client - Python\nDESCRIPTION: This code snippet initializes an asynchronous OpenAI client using the API key from the environment variables. It sets the API key and creates an instance of the `AsyncOpenAI` class, which is used for making asynchronous API requests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/completion-monitoring.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom openai import AsyncOpenAI\nimport os\nimport asyncio\n\nos.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\")\nclient = AsyncOpenAI()\n```\n```\n\n----------------------------------------\n\nTITLE: Executing Sequential Translation (Python)\nDESCRIPTION: Iterates through the prepared text chunks and calls the `translate_chunk` function for each one sequentially. Collects the translated results, joins them back together, and saves the final translated LaTeX document to a specified output file. Provides basic progress feedback.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndest_language = \"English\"\n\ntranslated_chunks = []\nfor i, chunk in enumerate(chunks):\n    print(str(i+1) + \" / \" + str(len(chunks)))\n    # translate each chunk\n    translated_chunks.append(translate_chunk(chunk, model='gpt-4o', dest_language=dest_language))\n\n# join the chunks together\nresult = '\\n\\n'.join(translated_chunks)\n\n# save the final result\nwith open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:\n    f.write(result)\n```\n\n----------------------------------------\n\nTITLE: Loading and Reading Multiple Article Files in Python\nDESCRIPTION: Defines a list of file paths to markdown articles and a helper function to read their contents into memory as strings, enabling batch processing of text data for subsequent summarization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\narticles = [\n    \"./data/structured_outputs_articles/cnns.md\",\n    \"./data/structured_outputs_articles/llms.md\",\n    \"./data/structured_outputs_articles/moe.md\"\n]\n\ndef get_article_content(path):\n    with open(path, 'r') as f:\n        content = f.read()\n    return content\n        \ncontent = [get_article_content(path) for path in articles]\n```\n\n----------------------------------------\n\nTITLE: Searching for a file in all S3 buckets\nDESCRIPTION: This snippet demonstrates searching for a specific file in all S3 buckets using the `run_conversation` function. The `search_file` variable holds the name of the file to search for. The input is an f-string that incorporates the filename.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsearch_file = '<file_name>'\nprint(run_conversation(f'search for a file {search_file} in all buckets'))\n```\n\n----------------------------------------\n\nTITLE: Saving and Resizing Generated Image in Python\nDESCRIPTION: Processes the base64-encoded image from the API response, decodes it to binary, opens it as a PIL Image, resizes it, and saves it with compression for efficiency.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Save the image to a file and resize/compress for smaller files\nimage_base64 = result1.data[0].b64_json\nimage_bytes = base64.b64decode(image_base64)\n\nimage = Image.open(BytesIO(image_bytes))\nimage = image.resize((300, 300), Image.LANCZOS)\nimage.save(img_path1, format=\"JPEG\", quality=80, optimize=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Global LLM Service Context in LlamaIndex with Python\nDESCRIPTION: Creates a ServiceContext instance from the configured LLM and sets it as the global default context so all LlamaIndex components use this configuration. This centralizes LLM settings for downstream operations like embedding generation and query execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nservice_context = ServiceContext.from_defaults(llm=llm)\nset_global_service_context(service_context=service_context)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Milvus Database in Python\nDESCRIPTION: Imports necessary components from the `pymilvus` library (`connections`, `utility`, `FieldSchema`, `Collection`, `CollectionSchema`, `DataType`) and establishes a connection to the Milvus database instance using the previously defined HOST and PORT variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pymilvus import connections, utility, FieldSchema, Collection, CollectionSchema, DataType\n\n# Connect to Milvus Database\nconnections.connect(host=HOST, port=PORT)\n```\n\n----------------------------------------\n\nTITLE: Calling OpenAI Chat Completions API with Named Messages (Python)\nDESCRIPTION: This snippet shows how to make a call to the OpenAI Chat Completions API using the `client.chat.completions.create` method. It demonstrates structuring the `messages` list with different roles ('system', 'user') and includes the optional 'name' field for system messages to simulate a patterned conversation, here used for jargon translation. It assumes an initialized `client` object and `MODEL` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Initializing the OpenAI Client in Python\nDESCRIPTION: Sets up the OpenAI client using an API key from environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Printing Augmented Query Python\nDESCRIPTION: Prints the augmented query, combining the retrieved contexts with the user query to demonstrate the prompt passed to the GPT-4 model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(augmented_query)\n```\n\n----------------------------------------\n\nTITLE: Installing Older SDK Versions for v1 API Access (Shell)\nDESCRIPTION: Provides shell commands for installing specific older versions of the OpenAI Python (`pip`) and Node.js (`npm`) SDKs. These versions (1.20.0 for Python, 4.36.0 for Node.js) default to using the v1 Assistants API beta namespace (`openai.beta`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/migration.txt#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install openai==1.20.0\n```\n\nLANGUAGE: node.js\nCODE:\n```\nnpm install openai@4.36.0\n```\n\n----------------------------------------\n\nTITLE: Testing Input Moderation Workflow with Allowed and Blocked Inputs\nDESCRIPTION: Shows example usage of the asynchronous moderation workflow with a safe request and a flagged request, demonstrating how the moderation system permits or blocks specific content based on its analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Call the main function with the good request - this should go through\ngood_response = await execute_chat_with_input_moderation(good_request)\nprint(good_response)\n```\n\n----------------------------------------\n\nTITLE: Requirements.txt for Azure Functions with OpenAI Integration\nDESCRIPTION: A comprehensive list of Python package dependencies required for an Azure Functions project that integrates OpenAI with various Azure services. Includes packages for working with Azure Search, Identity management, Storage, Web apps, and PDF processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/requirements.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nazure-functions\nazure-search-documents\nazure-identity\nopenai\nazure-mgmt-search\npandas\nazure-mgmt-resource\nazure-mgmt-storage\nazure-mgmt-web\npython-dotenv\npyperclip\nPyPDF2\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Example Invocation of Similarity Search with User Prompt in Python\nDESCRIPTION: Runs the similarity search function with a sample user input 'I'm looking for nice curtains' and prints the resulting list of product matches. This snippet demonstrates how to use the fall-back similarity search function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nprompt_similarity = \"I'm looking for nice curtains\"\nprint(similarity_search(prompt_similarity))\n```\n\n----------------------------------------\n\nTITLE: Making Moderation Requests in Node.js\nDESCRIPTION: Provides an asynchronous Node.js example using the `openai` library to call the moderation endpoint. It initializes the OpenAI client and uses `openai.moderations.create` within an async function to get the moderation results for the given input text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/moderation.txt#_snippet_2\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const moderation = await openai.moderations.create({ input: \"Sample text goes here.\" });\n\n  console.log(moderation);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Initializing MyScale Client - Python\nDESCRIPTION: Initializes a connection to the MyScale database using the `clickhouse_connect.get_client()` function. The connection parameters include the host, port, username, and password of the MyScale cluster.  Replace the placeholder values with your actual cluster credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# initialize client\nclient = clickhouse_connect.get_client(host='YOUR_CLUSTER_HOST', port=8443, username='YOUR_USERNAME', password='YOUR_CLUSTER_PASSWORD')\n```\n\n----------------------------------------\n\nTITLE: Inspecting OpenAI Embedding Results in Python\nDESCRIPTION: Prints information about the structure and content of the result object obtained from the `client.embeddings.create` call, including the number of embeddings and the dimension of a single embedding vector.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"len(result.data)              = {len(result.data)}\")\nprint(f\"result.data[1].embedding      = {str(result.data[1].embedding)[:55]}...\")\nprint(f\"len(result.data[1].embedding) = {len(result.data[1].embedding)}\")\n```\n\n----------------------------------------\n\nTITLE: Sampling Claims from Dataset with pandas in Python\nDESCRIPTION: Takes a random sample of 50 claims from the SciFact claims DataFrame and converts the 'claim' column values into a list. This prepares the data for batch processing through the assessment pipeline.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Let's take a look at 50 claims\nsamples = claim_df.sample(50)\n\nclaims = samples['claim'].tolist() \n```\n\n----------------------------------------\n\nTITLE: Calling External APIs with Python Code Execution\nDESCRIPTION: This snippet shows how to enable an AI model to use external APIs by writing Python code. The example includes a custom message module that allows sending messages to contacts.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport message\nmessage.write(to=\"John\", message=\"Hey, want to meetup after work?\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Polling Runs in OpenAI Assistants API\nDESCRIPTION: Creates a run with specified instructions and polls until completion. The code initiates a run in a thread with a specific assistant, includes custom instructions for how to address the user, and waits for the run to complete.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/overview-without-streaming.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create_and_poll(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"Please address the user as Jane Doe. The user has a premium account.\"\n)\n```\n\nLANGUAGE: node.js\nCODE:\n```\nlet run = await openai.beta.threads.runs.createAndPoll(\n  thread.id,\n  { \n    assistant_id: assistant.id,\n    instructions: \"Please address the user as Jane Doe. The user has a premium account.\"\n  }\n);\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/threads/thread_abc123/runs \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"assistant_id\": \"asst_abc123\",\n    \"instructions\": \"Please address the user as Jane Doe. The user has a premium account.\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Submitting Q&A Model Fine-tuning Job via OpenAI CLI\nDESCRIPTION: Executes a shell command using the OpenAI Command Line Interface (CLI) to initiate a fine-tuning job for the main question-answering (Q&A) model. The command `openai api fine_tunes.create` specifies the training (`-t`) and validation (`-v`) JSONL files generated previously and sets the batch size (`--batch_size 16`). Requires the OpenAI CLI to be installed and configured.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n!openai api fine_tunes.create -t \"olympics-data/qa_train.jsonl\" -v \"olympics-data/qa_test.jsonl\" --batch_size 16\n```\n\n----------------------------------------\n\nTITLE: Baseline Whisper Transcription Without Prompts in Python\nDESCRIPTION: Demonstrates baseline usage of the transcription function to process an audio file with no style prompt, establishing reference output. Requires that the 'transcribe' function and necessary audio files exist. Accepts a file path and an empty string as the prompt.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# baseline transcription with no prompt\ntranscribe(up_first_filepath, prompt=\"\")\n```\n\n----------------------------------------\n\nTITLE: Upserting Vectors into Pinecone Index with Namespace\nDESCRIPTION: This code performs an upsert operation to add or update vectors in the specified namespace within the Pinecone index. Organizing data into namespaces facilitates dataset partitioning and targeted queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nindex.upsert(\n    vectors=vectors,\n    namespace=\"ns1\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining File URLs\nDESCRIPTION: This snippet demonstrates the structure of an `openaiFileResponse` array. This array contains URLs that point to files. Each URL references a file to be downloaded. The response contains the URLs to fetch files.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/getting-started.txt#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  \"https://example.com/f/dca89f18-16d4-4a65-8ea2-ededced01646\",\n  \"https://example.com/f/01fad6b0-635b-4803-a583-0f678b2e6153\"\n]\n\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Azure OpenAI Using Azure Active Directory\nDESCRIPTION: Creates an Azure OpenAI client using Azure Active Directory authentication. Uses DefaultAzureCredential and get_bearer_token_provider to handle token management, automatically caching and refreshing tokens.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\nif use_azure_active_directory:\n    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n\n    client = openai.AzureOpenAI(\n        azure_endpoint=endpoint,\n        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n        api_version=\"2023-09-01-preview\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Information in Python\nDESCRIPTION: Uses the `.info(show_counts=True)` method on the `article_df` pandas DataFrame. This prints a concise summary of the DataFrame, including the index dtype and columns, non-null values count per column, and memory usage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\narticle_df.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: Verify data count in each index to confirm storage\nDESCRIPTION: Retrieves index statistics for each vector index, calculates the total stored entries by subtracting deletions, and prints the count to verify data has been loaded correctly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfor index_name in index_names:\n    stats = client.tvs_get_index(index_name)\n    count = int(stats[\"current_record_count\"]) - int(stats[\"delete_record_count\"])\n    print(f\"Count in {index_name}:{count}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a table in SingleStoreDB\nDESCRIPTION: This code snippet creates a table named 'winter_olympics_2022' in the 'winter_wikipedia2' database. The table has three columns: id (INT PRIMARY KEY), text (TEXT), and embedding (BLOB).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#create table\nstmt = \"\"\"\nCREATE TABLE IF NOT EXISTS winter_wikipedia2.winter_olympics_2022 (\n    id INT PRIMARY KEY,\n    text TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n    embedding BLOB\n);\"\"\"\n\ncur.execute(stmt)\n```\n\n----------------------------------------\n\nTITLE: Deleting Collection - Python\nDESCRIPTION: This code snippet removes all resources used for the demo. It removes an AstraDB collection.  This code has a warning stating that it irreversibly deletes data. Dependencies: `astra_db`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nastra_db.delete_collection(coll_name)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Meta Prompting\nDESCRIPTION: Imports necessary Python libraries including pandas for data manipulation, OpenAI client for API access, concurrent futures for parallel processing, tqdm for progress tracking, and Pydantic for data validation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport openai \nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nfrom pydantic import BaseModel\nfrom datasets import load_dataset\n\nclient = openai.Client()\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset for Caption Generation\nDESCRIPTION: Selects and cleans up the columns from the dataset that will be used for caption generation, focusing on product attributes like title, image, style, material, and color.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Cleaning up dataset columns\nselected_columns = ['title', 'primary_image', 'style', 'material', 'color', 'url']\ndf = df[selected_columns].copy()\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Downloading Result Data from Batch Processing\nDESCRIPTION: Downloads the output file content from the batch job, saving it locally as a JSONL file with one JSON object per line containing responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresult_file_id = batch_job.output_file_id\nresult = client.files.content(result_file_id).content\n\nresult_file_name = \"data/batch_job_results_movies.jsonl\"\n\nwith open(result_file_name, 'wb') as file:\n    file.write(result)\n```\n\n----------------------------------------\n\nTITLE: Downloading Natural Questions Dataset\nDESCRIPTION: Downloads sample questions and answers from Google's Natural Questions dataset using wget. These files contain training data for the QA system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\n# All the examples come from https://ai.google.com/research/NaturalQuestions\n# This is a sample of the training set that we download and extract for some\n# further processing.\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/questions.json\")\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/answers.json\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Precomputed Embeddings (wget)\nDESCRIPTION: Downloads a large zip file containing precomputed Wikipedia article embeddings from a specified URL using the `wget` library. This file size can be significant (~700 MB).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Importing and creating a partitioned Cassandra table with CassIO\nDESCRIPTION: Imports the ClusteredMetadataVectorCassandraTable class from CassIO and initializes a new table object partitioned by author, enabling efficient author-based queries in the vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfrom cassio.table import ClusteredMetadataVectorCassandraTable\n```\n\nLANGUAGE: Python\nCODE:\n```\nv_table_partitioned = ClusteredMetadataVectorCassandraTable(table=\"philosophers_cassio_partitioned\", vector_dimension=1536)\n```\n\n----------------------------------------\n\nTITLE: Fetching Place Details from Google Places API\nDESCRIPTION: This function `get_place_details` retrieves detailed information about a specific place using the Google Places API. It takes the `place_id` and `api_key` as input and makes a GET request to the Google Place Details API. It returns the result as a JSON object if successful, or None if the request fails. It also includes error handling to print the status code and content if the request fails.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_place_details(place_id, api_key):\n    URL = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&key={api_key}\"\n    response = requests.get(URL)\n    if response.status_code == 200:\n        result = json.loads(response.content)[\"result\"]\n        return result\n    else:\n        print(f\"Google Place Details API request failed with status code {response.status_code}\")\n        print(f\"Response content: {response.content}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries for Weaviate and Data Handling\nDESCRIPTION: Installs necessary Python packages using pip. It installs 'weaviate-client' (version greater than 3.11.0) for interacting with a Weaviate instance, and 'datasets' along with its dependency 'apache-beam' for loading sample datasets used later in the notebook. These are prerequisites for the subsequent Python code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the Weaviate client for Python\n!pip install weaviate-client>3.11.0\n\n# Install datasets and apache-beam to load the sample datasets\n!pip install datasets apache-beam\n```\n\n----------------------------------------\n\nTITLE: Creating a basic Run using OpenAI API in Python\nDESCRIPTION: This code snippet demonstrates how to initiate a Run within a specific Thread with default assistant and model configurations. It requires the OpenAI client library in Python and creates a Run object tied to a thread and assistant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/how-it-works.txt#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create(\n  thread_id=thread.id,\n  assistant_id=assistant.id\n)\n```\n\n----------------------------------------\n\nTITLE: Chat Example: Query Sequence - Evaluate Student Solution\nDESCRIPTION: This example shows the second step in a sequence of queries where the model evaluates the student's solution after generating its own.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_16\n\nLANGUAGE: N/A\nCODE:\n```\nSYSTEM: Compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n\nUSER: Problem statement: \"\"\"\"\n\nYour solution: \"\"\"\"\n\nStudentâ€™s solution: \"\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Testing Quote Search Engine Functionality in Python\nDESCRIPTION: Demonstrates usage examples of the semantic search function 'find_quote_and_author' by querying for similar quotes, filtering by author, and filtering by tags. Useful for system validation. Depends on previous function and loaded dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfind_quote_and_author(\"We struggle all our life for nothing\", 3)\n```\n\nLANGUAGE: python\nCODE:\n```\nfind_quote_and_author(\"We struggle all our life for nothing\", 2, author=\"nietzsche\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfind_quote_and_author(\"We struggle all our life for nothing\", 2, tags=[\"politics\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Question-Answer Dataset from JSON Files\nDESCRIPTION: Loads sample question and answer data from JSON files. These will be used to create a knowledge base for the question answering system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open(\"questions.json\", \"r\") as fp:\n    questions = json.load(fp)\n\nwith open(\"answers.json\", \"r\") as fp:\n    answers = json.load(fp)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataset in DataFrame Format\nDESCRIPTION: Displays the preprocessed medical dataset as a DataFrame for inspection. This allows verification of the merged text format before proceeding with embedding generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_api_tool_orchestration.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nds_dataframe\n```\n\n----------------------------------------\n\nTITLE: Uploading CSV to Azure Blob Storage and Generating SAS URL\nDESCRIPTION: This snippet uploads a CSV file to Azure Blob Storage, setting appropriate content type and disposition headers, then generates a pre-signed SAS URL with a 1-hour expiry for secure access. It uses azure.storage.blob libraries and requires the storage connection string, container name, and blob name.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef upload_csv_to_azure(file_path, container_name, blob_name, connect_str):\n    try:\n        # Create the BlobServiceClient object\n        blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n        \n        # Create a blob client\n        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n\n        # Upload the file with content settings\n        with open(file_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(\n                content_type='text/csv',\n                content_disposition=f'attachment; filename=\"{blob_name}\"'\n            ))\n        logger.info(f\"Successfully uploaded {file_path} to {container_name}/{blob_name}\")\n\n        # Generate a SAS token for the blob\n        sas_token = generate_blob_sas(\n            account_name=blob_service_client.account_name,\n            container_name=container_name,\n            blob_name=blob_name,\n            account_key=blob_service_client.credential.account_key,\n            permission=BlobSasPermissions(read=True),\n            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=1)\n        )\n\n        # Create the URL with SAS token\n        url = f\"https://{blob_service_client.account_name}.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n        logger.info(f\"Generated presigned URL: {url}\")\n\n        return url\n    except Exception as e:\n        logger.error(f\"Error uploading file to Azure Blob Storage: {e}\")\n        raise\n```\n\n----------------------------------------\n\nTITLE: Building AnalyticDB Connection String with Langchain\nDESCRIPTION: Creates a connection string for AnalyticDB using parameters from environment variables. Uses Langchain's helper method to format the connection string properly.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom langchain.vectorstores.analyticdb import AnalyticDB\n\nCONNECTION_STRING = AnalyticDB.connection_string_from_db_params(\n    driver=os.environ.get(\"PG_DRIVER\", \"psycopg2cffi\"),\n    host=os.environ.get(\"PG_HOST\", \"localhost\"),\n    port=int(os.environ.get(\"PG_PORT\", \"5432\")),\n    database=os.environ.get(\"PG_DATABASE\", \"postgres\"),\n    user=os.environ.get(\"PG_USER\", \"postgres\"),\n    password=os.environ.get(\"PG_PASSWORD\", \"postgres\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Model Name in Python\nDESCRIPTION: Defines the OpenAI model identifier to be used for completing chat queries throughout the notebook. Setting a constant enables consistent usage and easy updates. The model name 'gpt-4o' is specified as the target for question answering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Set the model for all API calls\nOPENAI_MODEL = \"gpt-4o\"\n```\n\n----------------------------------------\n\nTITLE: Detect Colab Environment Python\nDESCRIPTION: This snippet detects if the code is running in a Google Colab environment. It attempts to import `google.colab.files` and sets `IS_COLAB` accordingly.  This affects file handling behavior.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\ntry:\n    from google.colab import files\n    IS_COLAB = True\nexcept ModuleNotFoundError:\n    IS_COLAB = False\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embeddings Model in Python\nDESCRIPTION: Imports necessary classes (`Neo4jVector`, `OpenAIEmbeddings`) from Langchain for vector store operations and embedding generation. It specifies the identifier (`text-embedding-3-small`) for the OpenAI embeddings model to be used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.vectorstores.neo4j_vector import Neo4jVector\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings_model = \"text-embedding-3-small\"\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity Library for Azure AD Authentication in Python\nDESCRIPTION: Installs the `azure-identity` library (version >=1.15.0) using pip. This library is required to enable authentication with Azure services, including Azure OpenAI, using Azure Active Directory credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n! pip install \"azure-identity>=1.15.0\"\n```\n\n----------------------------------------\n\nTITLE: Loading Evidence and Computing Ground Truth Labels in Python\nDESCRIPTION: Extracts evidence for sampled claims from the dataset and applies the ground truth derivation function. This prepares the true labels against which the model's predictions will be compared.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nevidence = samples['evidence'].tolist()\ngroundtruth = get_groundtruth(evidence)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI SDK in Node.js Using npm\nDESCRIPTION: Installs the openai JavaScript SDK into the current Node.js project via npm. This is required for generating embeddings with OpenAI APIs. Run this command in the project directory. Requires Node.js and npm installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm install openai\n```\n\n----------------------------------------\n\nTITLE: Reranking Results by Relevance Probability in Python\nDESCRIPTION: Sorts the DataFrame in descending order based on the yes_probability column to rerank the documents according to their computed relevance, then displays the top 10 most relevant documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Return reranked results\nreranked_df = output_df.sort_values(\n    by=[\"yes_probability\"], ascending=False\n).reset_index()\nreranked_df.head(10)\n```\n\n----------------------------------------\n\nTITLE: Installing necessary Python libraries for OpenAI, Tair, and data processing\nDESCRIPTION: Installs essential Python packages: openai for API access, redis and tair for database interaction, pandas for data manipulation, wget for file download. Ensures environment is ready for subsequent operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! pip install openai redis tair pandas wget\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Image\nDESCRIPTION: This code prints the file path of the generated image to the console and then displays the image using the PIL library's `Image.open` method. It effectively visualizes the image that has been saved.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# print the image\nprint(generated_image_filepath)\ndisplay(Image.open(generated_image_filepath))\n```\n\n----------------------------------------\n\nTITLE: Downloading Embedded Data with Wget\nDESCRIPTION: This Python snippet uses the `wget` library to download a large ZIP file containing precomputed OpenAI embeddings for Wikipedia articles. The file is downloaded from a specified URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Customizing GPTBot Access via robots.txt\nDESCRIPTION: Illustrates how to selectively allow or disallow OpenAI's GPTBot web crawler access to specific directories on a website using `robots.txt`. This example configuration permits access to `/directory-1/` while explicitly blocking access to `/directory-2/`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/gptbot.txt#_snippet_2\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: GPTBot\nAllow: /directory-1/\nDisallow: /directory-2/\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: Sets up the OpenAI API client to interact with OpenAI models, requiring the OpenAI Python library. This snippet depends on the 'openai' package and initializes the client for subsequent API calls, such as generating completions or grading responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nimport pandas as pd\n\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Loading TREC Dataset with Hugging Face Datasets\nDESCRIPTION: Loads the first 1,000 questions from the TREC dataset using Hugging Face's datasets library. This provides sample text data for demonstrating semantic search capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# load the first 1K rows of the TREC dataset\ntrec = load_dataset('trec', split='train[:1000]')\ntrec\n```\n\n----------------------------------------\n\nTITLE: Download and extract embedded data zip file\nDESCRIPTION: This snippet downloads a large ZIP file containing pre-embedded Wikipedia articles and extracts its contents for further processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\nwget.download(embeddings_url)\n\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Libraries and Configuration\nDESCRIPTION: Imports necessary libraries, sets the embedding model, and configures warnings to be ignored.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nfrom typing import List, Iterator\nimport pandas as pd\nimport numpy as np\nimport os\nimport wget\nfrom ast import literal_eval\n\n# Redis client library for Python\nimport redis\n\n# I've set this to our new embeddings model, this can be changed to the embedding model of your choice\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\n# Ignore unclosed SSL socket warnings - optional in case you get these errors\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text to Word Count (Prompt)\nDESCRIPTION: Shows how to instruct the model to summarize text within triple quotes to a specific word count (approximately 50 words). Note that word count control is not highly precise.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/prompt-engineering.txt#_snippet_5\n\nLANGUAGE: Prompt\nCODE:\n```\nUSER: Summarize the text delimited by triple quotes in about 50 words.\n\n\"\"\"insert text here\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generate an image with DALLÂ·E 3\nDESCRIPTION: Code for generating an original image from a text prompt using DALLÂ·E 3. Allows specification of model, prompt, size, quality, and number of images to generate.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/images.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=\"a white siamese cat\",\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\n```\n\nLANGUAGE: node.js\nCODE:\n```\nconst response = await openai.images.generate({\n  model: \"dall-e-3\",\n  prompt: \"a white siamese cat\",\n  n: 1,\n  size: \"1024x1024\",\n});\nimage_url = response.data[0].url;\n```\n\nLANGUAGE: curl\nCODE:\n```\ncurl https://api.openai.com/v1/images/generations \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"dall-e-3\",\n    \"prompt\": \"a white siamese cat\",\n    \"n\": 1,\n    \"size\": \"1024x1024\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Extracting Embedded Data - Python\nDESCRIPTION: Extracts the contents of a downloaded zip file into the `../data` directory. This step prepares the embedded data for subsequent loading and processing using Pandas.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Saving and Displaying Masked-Edited Image in Python\nDESCRIPTION: Processes the masked-edited image result by decoding the base64 data, resizing for display, saving with compression, and displaying in the notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Display result\n\nimg_path_mask_edit = \"imgs/mask_edit.png\"\n\nimage_base64 = result_mask_edit.data[0].b64_json\nimage_bytes = base64.b64decode(image_base64)\n\nimage = Image.open(BytesIO(image_bytes))\nimage = image.resize((300, 300), Image.LANCZOS)\nimage.save(img_path_mask_edit, format=\"JPEG\", quality=80, optimize=True)\n    \ndisplay(IPImage(img_path_mask_edit))\n```\n\n----------------------------------------\n\nTITLE: Testing Chat with Bad Request\nDESCRIPTION: This snippet calls the `execute_chat_with_guardrail` function with the `bad_request`. The expected behavior is that the request should be blocked by the topical guardrail, and the canned message returned.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Call the main function with the bad request - this should get blocked\nresponse = await execute_chat_with_guardrail(bad_request)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Defining Question Answering Chain with Langchain\nDESCRIPTION: Creates a Question Answering chain using OpenAI's LLM and the Tair vector store. The 'stuff' chain type retrieves relevant context and passes it to the LLM.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nllm = OpenAI(openai_api_key=openai_api_key)\nqa = VectorDBQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    vectorstore=doc_store,\n    return_source_documents=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Using a fine-tuned model in Chat Completions via Python\nDESCRIPTION: This code shows how to generate a chat completion request using a fine-tuned model identified by its custom model name. It includes necessary message formatting and assumes the 'openai' package is configured with API credentials. The response contains the assistant's reply.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\ncompletion = client.chat.completions.create(\n  model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Mask Generation Prompt in Python\nDESCRIPTION: Creates a prompt to generate a mask for a character in an existing image, defining the mask's color scheme (white for character, black for background).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimg_path_mask = \"imgs/mask.png\"\nprompt_mask = \"generate a mask delimiting the entire character in the picture, using white where the character is and black for the background. Return an image in the same size as the input image.\"\n```\n\n----------------------------------------\n\nTITLE: Example Python Function for Unit Testing\nDESCRIPTION: A simple Python function that checks if a string is a palindrome by comparing it with its reverse. This function serves as the example for which unit tests will be generated using the multi-step prompt approach.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef is_palindrome(s):\n    return s == s[::-1]\n```\n\n----------------------------------------\n\nTITLE: Split Wikipedia Pages into Sections\nDESCRIPTION: This code defines functions to split Wikipedia pages into sections and subsections. The `all_subsections_from_section` function recursively extracts subsections from a given section, ignoring specified sections like 'See also' and 'References'. The `all_subsections_from_title` function retrieves the text of a Wikipedia page, parses it into sections, and calls `all_subsections_from_section` to get all nested subsections.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# define functions to split Wikipedia pages into sections\n\nSECTIONS_TO_IGNORE = [\n    \"See also\",\n    \"References\",\n    \"External links\",\n    \"Further reading\",\n    \"Footnotes\",\n    \"Bibliography\",\n    \"Sources\",\n    \"Citations\",\n    \"Literature\",\n    \"Footnotes\",\n    \"Notes and references\",\n    \"Photo gallery\",\n    \"Works cited\",\n    \"Photos\",\n    \"Gallery\",\n    \"Notes\",\n    \"References and sources\",\n    \"References and notes\",\n]\n\n\ndef all_subsections_from_section(\n    section: mwparserfromhell.wikicode.Wikicode,\n    parent_titles: list[str],\n    sections_to_ignore: set[str],\n) -> list[tuple[list[str], str]]:\n    \"\"\"\n    From a Wikipedia section, return a flattened list of all nested subsections.\n    Each subsection is a tuple, where:\n        - the first element is a list of parent subtitles, starting with the page title\n        - the second element is the text of the subsection (but not any children)\n    \"\"\"\n    headings = [str(h) for h in section.filter_headings()]\n    title = headings[0]\n    if title.strip(\"=\" + \" \") in sections_to_ignore:\n        # ^wiki headings are wrapped like \"== Heading ==\"\n        return []\n    titles = parent_titles + [title]\n    full_text = str(section)\n    section_text = full_text.split(title)[1]\n    if len(headings) == 1:\n        return [(titles, section_text)]\n    else:\n        first_subtitle = headings[1]\n        section_text = section_text.split(first_subtitle)[0]\n        results = [(titles, section_text)]\n        for subsection in section.get_sections(levels=[len(titles) + 1]):\n            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n        return results\n\n\ndef all_subsections_from_title(\n    title: str,\n    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n    site_name: str = WIKI_SITE,\n) -> list[tuple[list[str], str]]:\n    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n    Each subsection is a tuple, where:\n        - the first element is a list of parent subtitles, starting with the page title\n        - the second element is the text of the subsection (but not any children)\n    \"\"\"\n    site = mwclient.Site(site_name)\n    page = site.pages[title]\n    text = page.text()\n    parsed_text = mwparserfromhell.parse(text)\n    headings = [str(h) for h in parsed_text.filter_headings()]\n    if headings:\n        summary_text = str(parsed_text).split(headings[0])[0]\n    else:\n        summary_text = str(parsed_text)\n    results = [([title], summary_text)]\n    for subsection in parsed_text.get_sections(levels=[2]):\n        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n    return results\n```\n\n----------------------------------------\n\nTITLE: Evaluating Zero-Shot Classification with Descriptive Labels - Python\nDESCRIPTION: Calls the `evaluate_embeddings_approach` function with more descriptive label strings ('An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.') to show the improvement in performance achieved by using richer descriptions for the classes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nevaluate_embeddings_approach(labels=['An Amazon review with a negative sentiment.', 'An Amazon review with a positive sentiment.'])\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Dataset Entry and Author Statistics with Python\nDESCRIPTION: Displays an example entry from the loaded dataset and prints the quote counts by author. Uses Python's Counter for aggregation. Helps verify dataset integrity before batch processing and storage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"An example entry:\")\nprint(philo_dataset[16])\n```\n\nLANGUAGE: python\nCODE:\n```\nauthor_count = Counter(entry[\"author\"] for entry in philo_dataset)\nprint(f\"Total: {len(philo_dataset)} quotes. By author:\")\nfor author, count in author_count.most_common():\n    print(f\"    {author:<20}: {count} quotes\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as Environment Variable (Windows)\nDESCRIPTION: This command sets the OpenAI API key as an environment variable in the current session on Windows. Replace `your-api-key-here` with the actual key. Use system properties for a permanent setup.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nsetx OPENAI_API_KEY \"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Default File Opening Function in Python\nDESCRIPTION: A basic implementation of a file opening function. Takes a file path, opens it in text read mode with UTF-8 encoding, reads its content, and returns the content as a string.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef open_file(path: str) -> str:\n    with open(path, \"rt\", encoding=\"utf-8\") as fh:\n        return fh.read()\n```\n\n----------------------------------------\n\nTITLE: Create Zilliz Collection Schema\nDESCRIPTION: Defines the structure (schema) for the Zilliz collection. This schema specifies the fields for movie metadata (id, title, type, release_year, rating, description) and the vector field (`embedding`) for storing the OpenAI embeddings, including its dimension and data type. It then creates the collection in Zilliz based on this schema.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Create collection which includes the id, title, and embedding.\nfields = [\n    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='type', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='release_year', dtype=DataType.INT64),\n    FieldSchema(name='rating', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='description', dtype=DataType.VARCHAR, max_length=64000),\n    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n]\nschema = CollectionSchema(fields=fields)\ncollection = Collection(name=COLLECTION_NAME, schema=schema)\n```\n\n----------------------------------------\n\nTITLE: Preparing Test Data Set with Expected Class Labels using pandas\nDESCRIPTION: This code loads test data from a JSON file into a pandas DataFrame, then adds an 'expected_class' column derived from the last message content in each row. It prepares the data for classification evaluation by extracting true labels.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ntest_set = pd.read_json(valid_file_name, lines=True)\ntest_set['expected_class'] = test_set.apply(lambda x: x['messages'][-1]['content'], axis=1)\ntest_set.head()\n```\n\n----------------------------------------\n\nTITLE: Download OpenAI Wikipedia embeddings dataset\nDESCRIPTION: This snippet downloads the OpenAI Wikipedia embeddings dataset from a specified URL using the `wget` library and extracts the contents of the downloaded zip file to a 'data' directory using the `zipfile` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\nwget.download(embeddings_url)\n\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\n\"r\") as zip_ref:\n    zip_ref.extractall(\"data\")\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset in Python\nDESCRIPTION: Downloads a zip file containing OpenAI Wikipedia article embeddings and extracts it to a local directory using wget and zipfile modules. No parameters required; output is a local folder of extracted CSV data used for further processing. Requires access to the download URL and sufficient disk space.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\nwget.download(embeddings_url)\n\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\n\"r\") as zip_ref:\n    zip_ref.extractall(\"data\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Downloaded Embeddings ZIP and Verifying Dataset File - Python\nDESCRIPTION: Extracts the downloaded ZIP file containing Wikipedia vector embeddings to a target directory using 'zipfile'. Verifies the existence of the expected CSV file ('vector_database_wikipedia_articles_embedded.csv') in the output directory, printing a status message. Relies on standard Python libraries and assumes a specific directory structure. Inputs: path to ZIP file and target output directory. Outputs successes or errors to the console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nimport os\nimport re\nimport tempfile\n\ncurrent_directory = os.getcwd()\nzip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\noutput_directory = os.path.join(current_directory, \"../../data\")\n\nwith zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n    zip_ref.extractall(output_directory)\n\n\n# check the csv file exist\nfile_name = \"vector_database_wikipedia_articles_embedded.csv\"\ndata_directory = os.path.join(current_directory, \"../../data\")\nfile_path = os.path.join(data_directory, file_name)\n\n\nif os.path.exists(file_path):\n    print(f\"The file {file_name} exists in the data directory.\")\nelse:\n    print(f\"The file {file_name} does not exist in the data directory.\")\n\n```\n\n----------------------------------------\n\nTITLE: Loading HTML from URL Python\nDESCRIPTION: Downloads HTML files from a specified URL using `wget`. It specifies the target file type as `.html` and stores the downloaded files in the `rtdocs` directory. This is the first step to get the data needed for document retrieval and question answering.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!wget -r -A.html -P rtdocs https://python.langchain.com/en/latest/\n```\n\n----------------------------------------\n\nTITLE: Testing Hologres Connection with a Simple Query in Python\nDESCRIPTION: Verifies the established database connection by executing a simple SQL query `SELECT 1;` using the `psycopg2` cursor. It fetches the result and checks if it matches the expected value (1,) to confirm the connection is successful, printing an appropriate message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Execute a simple query to test the connection\ncursor.execute(\"SELECT 1;\")\nresult = cursor.fetchone()\n\n# Check the query result\nif result == (1,):\n    print(\"Connection successful!\")\nelse:\n    print(\"Connection failed.\")\n```\n\n----------------------------------------\n\nTITLE: Example Function and Message List for Counting Tokens with Tools in Python\nDESCRIPTION: Provides an example list of a single function definition representing a weather API and a simple user message asking for weather information. These are used as inputs for the token counting function to estimate prompt token usage across different models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntools = [\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_current_weather\",\n      \"description\": \"Get the current weather in a given location\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\",\n          },\n          \"unit\": {\"type\": \"string\", \n                   \"description\": \"The unit of temperature to return\",\n                   \"enum\": [\"celsius\", \"fahrenheit\"]},\n        },\n        \"required\": [\"location\"],\n      },\n    }\n  }\n]\n\nexample_messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant that can answer to questions about the weather.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather like in San Francisco?\",\n    },\n]\n\nfor model in [\n    \"gpt-3.5-turbo\",\n    \"gpt-4\",\n    \"gpt-4o\",\n    \"gpt-4o-mini\"\n    ]:\n    print(model)\n    # example token count from the function defined above\n    print(f\"{num_tokens_for_tools(tools, example_messages, model)} prompt tokens counted by num_tokens_for_tools().\")\n    # example token count from the OpenAI API\n    response = client.chat.completions.create(model=model,\n          messages=example_messages,\n          tools=tools,\n          temperature=0)\n    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n    print()\n```\n\n----------------------------------------\n\nTITLE: Obtaining On-Behalf-Of (OBO) Token\nDESCRIPTION: This function uses a user's access token to request an OBO (On-Behalf-Of) token from Microsoft's identity platform. The OBO flow enables the service to act on behalf of the user. It takes the user's access token as input, then uses the `axios` library to make a POST request to the Microsoft identity platform. The function requires environment variables to configure the request, including tenant ID, client ID, and a client secret. It then returns the new OBO token.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst axios = require('axios');\nconst qs = require('querystring');\n\nasync function getOboToken(userAccessToken) {\n\tconst { TENANT_ID, CLIENT_ID, MICROSOFT_PROVIDER_AUTHENTICATION_SECRET } = process.env;\n\tconst params = {\n\t\tclient_id: CLIENT_ID,\n\t\tclient_secret: MICROSOFT_PROVIDER_AUTHENTICATION_SECRET,\n\t\tgrant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n\t\tassertion: userAccessToken,\n\t\trequested_token_use: 'on_behalf_of',\n\t\tscope: 'https://graph.microsoft.com/.default'\n\t};\n\n\tconst url = `https\\://login.microsoftonline.com/${TENANT_ID}/oauth2/v2.0/token`;\n\ttry {\n\t\tconst response = await axios.post(url, qs.stringify(params), {\n\t\t\theaders: { 'Content-Type': 'application/x-www-form-urlencoded' }\n\t\t});\n\t\treturn response.data.access_token;\n\t} catch (error) {\n\t\tconsole.error('Error obtaining OBO token:', error.response?.data || error.message);\n\t\tthrow error;\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Patches using Bash and apply_patch Command Example\nDESCRIPTION: Demonstrates the format for applying code patches using the custom `apply_patch` command within a Bash heredoc. This command is executed via the described Python tool and uses a specific V4A diff format with context lines and @@ markers for locating code changes without line numbers. The example shows updating two methods in different classes within a Python file.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n*** Update File: pygorithm/searching/binary_search.py\n@@ class BaseClass\n@@     def search():\n-        pass\n+        raise NotImplementedError()\n\n@@ class Subclass\n@@     def search():\n-        pass\n+        raise NotImplementedError()\n\n*** End Patch\nEOF\n```\n\n----------------------------------------\n\nTITLE: Input your OpenAI API key securely using getpass\nDESCRIPTION: Prompts user to input their OpenAI API key without displaying it, using getpass for security. Assigns the key to openai.api_key for later use in embedding creation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport getpass\nimport openai\n\nopenai.api_key = getpass.getpass(\"Input your OpenAI API key:\")\n```\n\n----------------------------------------\n\nTITLE: Executing Main Extraction Function (Python)\nDESCRIPTION: This code defines the input and output paths and then calls the main_extract function to begin the PDF invoice data extraction process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nread_path= \"./data/hotel_invoices/receipts_2019_de_hotel\"\nwrite_path= \"./data/hotel_invoices/extracted_invoice_json\"\n\nmain_extract(read_path, write_path)\n```\n\n----------------------------------------\n\nTITLE: ChatGPT API call with laconic system message\nDESCRIPTION: Demonstrates using a system message to instruct the model to provide brief, concise responses.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# An example of a system message that primes the assistant to give brief, to-the-point answers\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n    ],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Setting AnalyticDB Connection Parameters\nDESCRIPTION: Exports database connection parameters as environment variables for connecting to AnalyticDB. Parameters include host, port, database name, username, and password.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n! export PG_HOST=\"your AnalyticDB host url\"\n! export PG_PORT=5432 # Optional, default value is 5432\n! export PG_DATABASE=postgres # Optional, default value is postgres\n! export PG_USER=\"your username\"\n! export PG_PASSWORD=\"your password\"\n```\n\n----------------------------------------\n\nTITLE: Displaying an example image with PIL and matplotlib in Python\nDESCRIPTION: Opens a specific image file and displays it within a plot window, visualizing the image in the Jupyter notebook or python environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nim = Image.open(image_path)\nplt.imshow(im)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Processing and Displaying Kusto Query Results (Python/Pandas)\nDESCRIPTION: Converts the primary result table obtained from the Kusto query response into a pandas DataFrame. This makes the results easy to inspect and display.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndf = dataframe_from_result_table(RESPONSE.primary_results[0])\ndf\n```\n\n----------------------------------------\n\nTITLE: Starting Redis Stack with Docker Compose\nDESCRIPTION: This snippet shows how to start a Redis Stack docker container, which includes RediSearch and RedisInsight, using docker-compose. This sets up the Redis database needed for the project.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ docker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Testing Quote Search with Tags Filter in Python\nDESCRIPTION: Demonstrates calling the `find_quote_and_author` function with a query quote, number of results, and an additional filter to restrict the search to quotes containing specific tags.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfind_quote_and_author(\"We struggle all our life for nothing\", 2, tags=[\"politics\"])\n```\n\n----------------------------------------\n\nTITLE: Executing a Book Search Query\nDESCRIPTION: Demonstrates the search functionality by querying for books about a K-9 from Europe, which will return the most semantically similar book descriptions from the database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquery('Book about a k-9 from europe')\n```\n\n----------------------------------------\n\nTITLE: Extracting ZIP archive containing article embeddings\nDESCRIPTION: Unzips the downloaded archive into a specified directory using Python's zipfile module, preparing data for further processing and loading.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport zipfile\n\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Analysis Results in Python\nDESCRIPTION: This code snippet extracts the item descriptions, category, and gender from the `image_analysis` dictionary.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Extract the relevant features from the analysis\nitem_descs = image_analysis['items']\nitem_category = image_analysis['category']\nitem_gender = image_analysis['gender']\n```\n\n----------------------------------------\n\nTITLE: Creating a Batch Job for Asynchronous Processing\nDESCRIPTION: Initiates a batch job in OpenAI by specifying the input file ID, API endpoint, and a 24-hour completion window, returning a batch job object.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbatch_job = client.batches.create(\n  input_file_id=batch_file.id,\n  endpoint=\"/v1/chat/completions\",\n  completion_window=\"24h\"\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Transparent Background in Python\nDESCRIPTION: Generates an image with a transparent background using PNG format, which supports transparency, with low quality setting for faster generation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresult3 = client.images.generate(\n    model=\"gpt-image-1\",\n    prompt=prompt3,\n    quality=\"low\",\n    output_format=\"png\",\n    size=\"1024x1024\"\n)\nimage_base64 = result3.data[0].b64_json\nimage_bytes = base64.b64decode(image_base64)\n```\n\n----------------------------------------\n\nTITLE: Identifying Files to Add from Patch Text in Python\nDESCRIPTION: Scans the lines of patch text and extracts the file paths specified in '*** Add File: ' lines. Returns a list of these file paths.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef identify_files_added(text: str) -> List[str]:\n    lines = text.splitlines()\n    return [\n        line[len(\"*** Add File: \") :]\n        for line in lines\n        if line.startswith(\"*** Add File: \")\n    ]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Node.js/TypeScript Library\nDESCRIPTION: Install the official OpenAI Node.js/TypeScript client library using either npm or yarn. This command adds the package as a project dependency, enabling API interaction from Node.js or other compatible JavaScript runtimes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/libraries.txt#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save openai\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add openai\n```\n\n----------------------------------------\n\nTITLE: Extracting Downloaded ZIP File\nDESCRIPTION: This Python code uses the built-in `zipfile` library to extract the contents of the downloaded ZIP file. It extracts all files into a specified directory ('../data').\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\n\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Printing Data Example Python\nDESCRIPTION: Prints the information of a sample document from the data list. This is used to inspect the format and content of the created data list. This is done to demonstrate how the extracted data is stored.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndata[3]\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Pinecone Resources by Deleting Index\nDESCRIPTION: Deletes the Pinecone index when it's no longer needed to free up resources and avoid unnecessary costs. This is a good practice for cleanup after completing experiments or demonstrations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npc.delete_index(index_name)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for OpenAI Evals\nDESCRIPTION: Sets up the necessary imports for working with OpenAI's Evals framework and configures the API key from environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/use-cases/bulk-experimentation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pydantic\nimport openai\nfrom openai.types.chat import ChatCompletion\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\")\n```\n\n----------------------------------------\n\nTITLE: Listing S3 Buckets\nDESCRIPTION: This snippet demonstrates how to list all available S3 buckets using the `run_conversation` function. The expected input is a prompt asking the bot to list the buckets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(run_conversation('list my S3 buckets'))\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAI Model\nDESCRIPTION: Sends a prompt to the OpenAI model (gpt-3.5-turbo) and prints the response.  This example tests the model's knowledge about the FTX/Sam Bankman-Fried scandal.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"Is Sam Bankman-Fried's company, FTX, considered a well-managed company?\"\nresponse = get_completion(prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Running Agent with User Input in JavaScript\nDESCRIPTION: This line calls the `agent` function with a specific user input string. The input prompts the agent to determine the user's location, potentially triggering the `getLocation` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nagent(\"Where am I located right now?\");\n```\n\n----------------------------------------\n\nTITLE: Loading BBC News Dataset\nDESCRIPTION: Loads the BBC news dataset for August 2024 from HuggingFace and creates a random sample of 100 articles for experimentation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nds = load_dataset(\"RealTimeData/bbc_news_alltime\", \"2024-08\")\ndf = pd.DataFrame(ds['train']).sample(n=100, random_state=1)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Installing Weaviate Client Python\nDESCRIPTION: This snippet installs the necessary Python libraries: the Weaviate client and wget. The Weaviate client is used to interact with the Weaviate vector database, and wget is used to download the dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# We'll need to install the Weaviate client\n!pip install weaviate-client\n\n#Install wget to pull zip file\n!pip install wget\n```\n\n----------------------------------------\n\nTITLE: Extracting Zipped Data\nDESCRIPTION: This snippet extracts the contents of the downloaded zip file into the '../data' directory. It uses the `zipfile` library to extract the data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Perform and display search results for 'modern art in Europe' query\nDESCRIPTION: Executes a similarity search on 'title' vectors with the specified query and prints out the resulting document titles along with their vector distances.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nquery_results = query_typesense('modern art in Europe', 'title')\n\nfor i, hit in enumerate(query_results['results'][0]['hits']):\n    document = hit[\"document\"]\n    vector_distance = hit[\"vector_distance\"]\n    print(f'{i + 1}. {document[\"title\"]} (Distance: {vector_distance})')\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python for Batch API interactions\nDESCRIPTION: Creates an OpenAI client instance to interact with the API, enabling subsequent batch requests such as file uploads, batch job creation, and status retrieval.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom openai import OpenAI\nimport pandas as pd\nfrom IPython.display import Image, display\n\n# Initializing OpenAI client\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: GPTs Introduction in Markdown\nDESCRIPTION: Code snippet announcing the initial release of GPTs on November 6th, 2023, allowing users to customize ChatGPT for various use cases and share with others.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/release-notes.txt#_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n    GPTs\n{\" \"}\nallow users to customize ChatGPT for various use cases and share these with other\nusers\n```\n\n----------------------------------------\n\nTITLE: Executing Nearest Neighbor Title-Based Query on AnalyticDB - Python\nDESCRIPTION: Performs a similarity search against the articles collection using the title vector for the query 'modern art in Europe'. Leverages the previously defined 'query_analyticdb' function with the default 'title_vector'. Outputs and ranks the top results by similarity to the console. Requires OpenAI API access, properly constructed and indexed database, and prior definition of 'query_analyticdb'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nquery_results = query_analyticdb(\"modern art in Europe\", \"Articles\")\nfor i, result in enumerate(query_results):\n    print(f\"{i + 1}. {result[2]} (Score: {round(1 - result[3], 3)})\")\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Zip File\nDESCRIPTION: Extracts a zipped dataset containing pre-embedded Wikipedia articles to a specified location using the zipfile module.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"../data\")\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas DataFrame to Spark DataFrame (Spark)\nDESCRIPTION: Converts the pandas DataFrame containing the article data and embeddings into a Spark DataFrame. This conversion is required to use the Azure Kusto Spark connector.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n#Pandas data frame to spark dataframe\nsparkDF=spark.createDataFrame(article_df)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs the necessary dependencies: openai for API access, tiktoken for tokenization, langchain for LLM applications, and tair for vector database interactions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai tiktoken langchain tair\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPT-4o mini Image Processing\nDESCRIPTION: Installs the necessary Python packages (openai and scikit-learn) for working with the OpenAI API and performing vector operations for keyword comparison.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install dependencies if needed\n%pip install openai\n%pip install scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Forking a Conversation from a Previous Response\nDESCRIPTION: Shows how to fork a conversation from a previous point rather than continuing from the latest response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse_two_forked = client.responses.create(\n    model=\"gpt-4o-mini\",\n    input=\"I didn't like that joke, tell me another and tell me the difference between the two jokes\",\n    previous_response_id=response.id # Forking and continuing from the first response\n)\n\noutput_text = response_two_forked.output[0].content[0].text\nprint(output_text)\n```\n\n----------------------------------------\n\nTITLE: Extracting Queries From Dataset\nDESCRIPTION: Extracts the queries from the qa_dataset and stores it in the queries variable. These queries are then used in further evaluations such as the FaithfulnessEvaluator and Relevancy Evaluator.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Get the list of queries from the above created dataset\n\nqueries = list(qa_dataset.queries.values())\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenAI API Key Using getpass\nDESCRIPTION: Securely captures the OpenAI API key using getpass to avoid exposing it in the notebook. The key is required for document vectorization and LLM access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\n\nopenai_api_key = getpass.getpass(\"Input your OpenAI API key:\")\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Pandas DataFrame\nDESCRIPTION: This snippet loads the extracted CSV file into a Pandas DataFrame. The CSV file contains Wikipedia articles and their pre-computed embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API key in Python\nDESCRIPTION: Sets up the OpenAI API key for authenticating requests to generate embeddings. After setting, it validates authentication by listing available engines.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n\n# get API key from on OpenAI website\nopenai.api_key = \"OPENAI_API_KEY\"\n\n# check we have authenticated\nopenai.Engine.list()\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Client in Python\nDESCRIPTION: Imports Redis libraries and establishes a connection to the Redis database with search capabilities.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport redis\nfrom redis.commands.search.indexDefinition import (\n    IndexDefinition,\n    IndexType\n)\nfrom redis.commands.search.query import Query\nfrom redis.commands.search.field import (\n    TextField,\n    VectorField\n)\n\nREDIS_HOST =  \"localhost\"\nREDIS_PORT = 6379\nREDIS_PASSWORD = \"\" # default for passwordless Redis\n\n# Connect to Redis\nredis_client = redis.Redis(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD\n)\nredis_client.ping()\n```\n\n----------------------------------------\n\nTITLE: Performing Standalone Vector Search Using OpenAI API\nDESCRIPTION: Demonstrates how to search the vector store directly for relevant content using a query without integrating it into an LLM response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What's Deep Research?\"\nsearch_results = client.vector_stores.search(\n    vector_store_id=vector_store_details['id'],\n    query=query\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Image Mask with GPT Image Edit in Python\nDESCRIPTION: Uses the edit functionality to create a mask from an existing image, which will be used in a subsequent masked editing operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimg_input = open(img_path1, \"rb\")\n\n# Generate the mask\nresult_mask = client.images.edit(\n    model=\"gpt-image-1\",\n    image=img_input, \n    prompt=prompt_mask\n)\n```\n\n----------------------------------------\n\nTITLE: Reduce Dimensionality with t-SNE - Python\nDESCRIPTION: This snippet loads embeddings from a CSV file using pandas, converts them into a numpy array, and then reduces the dimensionality to 2 using t-SNE. The `TSNE` model is initialized with specified parameters like `n_components`, `perplexity`, `random_state`, `init`, and `learning_rate`. The `fit_transform` method applies the dimensionality reduction to the embedding matrix. Requires pandas, scikit-learn, numpy, and ast.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_2D.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport numpy as np\nfrom ast import literal_eval\n\n# Load the embeddings\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\ndf = pd.read_csv(datafile_path)\n\n# Convert to a list of lists of floats\nmatrix = np.array(df.embedding.apply(literal_eval).to_list())\n\n# Create a t-SNE model and transform the data\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\nvis_dims = tsne.fit_transform(matrix)\nvis_dims.shape\n```\n\n----------------------------------------\n\nTITLE: Searching for \"whole wheat pasta\" - Python\nDESCRIPTION: This code snippet invokes the `search_reviews` function to search for reviews related to \"whole wheat pasta\" and retrieves the top 3 most similar reviews.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults = search_reviews(df, \"whole wheat pasta\", n=3)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Demonstrates how to set the OpenAI API key using a .env file. This involves creating a .env file and adding the API key with the variable name OPENAI_API_KEY.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOPENAI_API_KEY=your_key\n```\n\n----------------------------------------\n\nTITLE: Install OpenAI Library using pip\nDESCRIPTION: This command installs the OpenAI Python library using pip, a package installer for Python. It ensures that the OpenAI library is available in the environment before running the embedding process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb#_snippet_1\n\nLANGUAGE: zsh\nCODE:\n```\npip install openai\n```\n\n----------------------------------------\n\nTITLE: Retrieve DataFrame info for processed embedded data\nDESCRIPTION: Displays dataset information including data types and counts to verify data integrity before indexing into a vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\narticle_df.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Identity Dependency (Python)\nDESCRIPTION: Installs the `azure-identity` library, which is necessary for authenticating with Azure resources, including Azure OpenAI, using Azure Active Directory credentials.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n! pip install \"azure-identity>=1.15.0\"\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data with Embeddings Using Kangas in Python\nDESCRIPTION: Reads a CSV file containing food reviews and their corresponding embeddings from GitHub using Kangas' read_csv function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = kg.read_csv(\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/data/fine_food_reviews_with_embeddings_1k.csv\")\n```\n\n----------------------------------------\n\nTITLE: Testing Keyword Extraction on Sample Products\nDESCRIPTION: Selects a few example products from the dataset to test the keyword extraction function, displaying each image and the keywords generated by the model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nexamples = df.iloc[:5]\n```\n\nLANGUAGE: python\nCODE:\n```\nfor index, ex in examples.iterrows():\n    url = ex['primary_image']\n    img = Image(url=url)\n    display(img)\n    result = analyze_image(url, ex['title'])\n    print(result)\n    print(\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Listing Qdrant Collections to Test Connection\nDESCRIPTION: This Python command calls the `get_collections()` method on the Qdrant client instance. This serves as a simple test to ensure the connection to the Qdrant service is active and functional.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient.get_collections()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Zero-Shot Classification with Basic Labels - Python\nDESCRIPTION: Calls the `evaluate_embeddings_approach` function with basic label strings ('negative', 'positive') to demonstrate the initial performance of the zero-shot classifier using simple class names.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nevaluate_embeddings_approach(labels=['negative', 'positive'], model=EMBEDDING_MODEL)\n\n```\n\n----------------------------------------\n\nTITLE: Checking Embedding Vector Length in Python\nDESCRIPTION: Displays the length of a single embedding vector to determine the dimension size needed for the Pinecone index. This value is required when creating a new index.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlen(embeds[0])\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: Installs the `openai`, `chromadb`, `wget`, and `numpy` libraries using pip. These are prerequisites for the notebook's operations involving OpenAI embeddings, ChromaDB interactions, data fetching, and numerical operations.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Make sure the OpenAI library is installed\n%pip install openai\n\n# We'll need to install the Chroma client\n%pip install chromadb\n\n# Install wget to pull zip file\n%pip install wget\n\n# Install numpy for data manipulation\n%pip install numpy\n```\n\n----------------------------------------\n\nTITLE: Evaluating Claims with Retrieved Context\nDESCRIPTION: Executes the assessment function on claims with retrieved context and evaluates performance against groundtruth using a confusion matrix.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ngpt_with_context_evaluation = assess_claims_with_context(claims, claim_query_result['documents'])\nconfusion_matrix(gpt_with_context_evaluation, groundtruth)\n```\n\n----------------------------------------\n\nTITLE: Install Requests Library\nDESCRIPTION: This command installs the `requests` library, which is used to make HTTP requests to the Google Places API. It is a prerequisite for running the code that interacts with the API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install requests\n```\n\n----------------------------------------\n\nTITLE: Inspecting Embedding Response Keys Python\nDESCRIPTION: Prints the keys of the response from the OpenAI embedding API to understand the structure of the response. This is used to ensure that the structure and content of the embedding response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nres.keys()\n```\n\n----------------------------------------\n\nTITLE: Loading Precomputed Embeddings into PolarDB-PG\nDESCRIPTION: Loads precomputed Wikipedia article embeddings from a CSV file into the PolarDB-PG database using the COPY command.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\n# Path to your local CSV file\ncsv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\n\n# Define a generator function to process the file line by line\ndef process_file(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            yield line\n\n# Create a StringIO object to store the modified lines\nmodified_lines = io.StringIO(''.join(list(process_file(csv_file_path))))\n\n# Create the COPY command for the copy_expert method\ncopy_command = '''\nCOPY public.articles (id, url, title, content, title_vector, content_vector, vector_id)\nFROM STDIN WITH (FORMAT CSV, HEADER true, DELIMITER ',');\n'''\n\n# Execute the COPY command using the copy_expert method\ncursor.copy_expert(copy_command, modified_lines)\n\n# Commit the changes\nconnection.commit()\n```\n\n----------------------------------------\n\nTITLE: Setting Default GCP Project (Python/Shell)\nDESCRIPTION: Sets the active Google Cloud Platform project configuration for the `gcloud` command-line tool. It defines a Python variable `project_id` (which needs to be replaced with the actual GCP project ID) and then uses an f-string within a shell command execution (`!`) to run `gcloud config set project`. Subsequent `gcloud` commands will target this specified project.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nproject_id = \"<insert_project_id>\"  # Replace with your actual project ID\n! gcloud config set project {project_id}\n```\n\n----------------------------------------\n\nTITLE: Bulk Wikipedia Link Resolution for Recognized Entities\nDESCRIPTION: Processes a dictionary of entities grouped by label, retrieving URLs for entities in select categories, returning a map of entities to Wikipedia links, considering only whitelisted labels.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef find_all_links(label_entities:dict) -> dict:\n    \"\"\" \n    Finds all Wikipedia links for the dictionary entities in the whitelist label list.\n    \"\"\"\n    whitelist = ['event', 'gpe', 'org', 'person', 'product', 'work_of_art']\n    \n    return {e: find_link(e) for label, entities in label_entities.items() \n                            for e in entities\n                            if label in whitelist}\n```\n\n----------------------------------------\n\nTITLE: Create Author Index Python\nDESCRIPTION: This snippet creates an index on the `author` column of the `philosophers_cql` table using the `StorageAttachedIndex` method. This index will allow for efficient filtering by author during searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\ncreate_author_index_statement = f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS idx_author\n    ON {keyspace}.philosophers_cql (author)\n    USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n\"\"\"\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\nsession.execute(create_author_index_statement)\n```\n```\n\n----------------------------------------\n\nTITLE: Downloading and extracting embedded dataset using Python\nDESCRIPTION: Downloads a large ZIP file (~700 MB) containing precomputed Wikipedia article embeddings using wget and extracts its contents with the zipfile module. This prepares the dataset required for indexing and vector search in Azure AI Search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\nLANGUAGE: python\nCODE:\n```\nwith zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\", \"r\") as zip_ref:\n    zip_ref.extractall(\"../../data\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies Python\nDESCRIPTION: Installs the required Python packages for the project. These include `bs4`, `tiktoken`, `openai`, `langchain`, and `pinecone-client[grpc]`. These packages are essential for web scraping, tokenization, interacting with the OpenAI API, LangChain functionalities, and interacting with the Pinecone vector database.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU bs4 tiktoken openai langchain pinecone-client[grpc]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI and Pinecone Libraries with pip\nDESCRIPTION: This code installs the required Python packages for interacting with OpenAI and Pinecone services, ensuring the environment has the latest versions. It is essential for setting up dependencies for subsequent code snippets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -qU openai pinecone\n```\n\n----------------------------------------\n\nTITLE: Dependencies for OpenAI and Google Cloud BigQuery Integration\nDESCRIPTION: A requirements file listing the necessary Python packages for integrating OpenAI with Google Cloud BigQuery and Functions Framework.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/requirements.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogle-cloud-bigquery\nfunctions-framework\nopenai\n```\n\n----------------------------------------\n\nTITLE: Importing Concurrent Execution Utility (Python)\nDESCRIPTION: Imports the `execute_concurrent_with_args` function from the Cassandra Python driver's concurrent module, enabling efficient execution of multiple database operations in parallel. Requires the Cassandra Python driver to be installed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nfrom cassandra.concurrent import execute_concurrent_with_args\n```\n\n----------------------------------------\n\nTITLE: Setting Embedding Context Length and Encoding in Python\nDESCRIPTION: Defines the context length and encoding for embedding models. The context length specifies the maximum number of tokens that can be processed in a single embedding request, and the encoding specifies the encoding to be used for tokenizing the text.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n## Change the below based on model. The below is for the latest embeddings models from OpenAI, so you can leave as is unless you are using a different embedding model..\nEMBEDDING_CTX_LENGTH = 8191\nEMBEDDING_ENCODING='cl100k_base'\n```\n\n----------------------------------------\n\nTITLE: Listing all Batches with OpenAI API - Node.js\nDESCRIPTION: This Node.js snippet retrieves a list of all batches from the OpenAI API. It requires the `openai` library to be installed and imported. The code iterates through the list of batches returned by `openai.batches.list()` and logs each batch to the console. Users need their API key configured and valid for this code to function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/batch.txt#_snippet_10\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const list = await openai.batches.list();\n\n  for await (const batch of list) {\n    console.log(batch);\n  }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Concatenating Title and Summary for Candidate Document Content - Python\nDESCRIPTION: Combines the title and summary fields from the first result document into a single content string for use as input to relevance classification or display. Assumes result_list[0] exists and contains 'title' and 'summary' keys. Output is a single string representing document content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncontent = result_list[0][\"title\"] + \": \" + result_list[0][\"summary\"]\n```\n\n----------------------------------------\n\nTITLE: Testing Summary Generation on a Single Article\nDESCRIPTION: Tests the summary generation functions on the first article in the dataset to verify they work correctly before processing the entire dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Enhance_your_prompts_with_meta_prompting.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngenerate_summaries(df.iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Publishing Azure Function App Code - Python\nDESCRIPTION: This Python snippet uses `subprocess.run` to execute the `func azure functionapp publish` command via Azure Functions Core Tools. This command deploys the local function code, including the `function_app.py` file, to the designated Azure Function App.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nsubprocess.run([\n    \"func\", \"azure\", \"functionapp\", \"publish\", app_name\n], check=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Image with GPT Image Model in Python\nDESCRIPTION: Uses OpenAI's client to generate an image based on the provided prompt using the gpt-image-1 model with a 1024x1024 size.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Generate the image\nresult1 = client.images.generate(\n    model=\"gpt-image-1\",\n    prompt=prompt1,\n    size=\"1024x1024\"\n)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Zilliz Database\nDESCRIPTION: Establishes a connection to the Zilliz vector database using the previously defined URI and authentication token.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pymilvus import connections, utility, FieldSchema, Collection, CollectionSchema, DataType\n\n# Connect to Zilliz Database\nconnections.connect(uri=URI, token=TOKEN)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: Creates an OpenAI client instance, with an option to set the API key if not configured globally.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI()\n# Set your API key if not set globally\n#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n```\n\n----------------------------------------\n\nTITLE: Setting Authentication Method Flag (Python)\nDESCRIPTION: Initializes a boolean flag to control which authentication method will be used for the Azure OpenAI client: API key or Azure Active Directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory\n```\n\n----------------------------------------\n\nTITLE: Setting Up Transparent Background Image Prompt in Python\nDESCRIPTION: Creates a prompt for generating a pixel-art style image with a transparent background, specifying both the subject and background properties.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprompt3 = \"generate a pixel-art style picture of a green bucket hat with a pink quill on a transparent background.\"\nimg_path3 = \"imgs/hat.png\"\n```\n\n----------------------------------------\n\nTITLE: Querying the Engine\nDESCRIPTION: Uses the `query_engine` to query the created vector index.  The query is \"What did the author do growing up?\" and the response is stored in `response_vector`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector = query_engine.query(\"What did the author do growing up?\")\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI's Data Preparation Tool\nDESCRIPTION: Running OpenAI's data preparation tool to automatically improve the dataset format and split it into training and validation sets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!openai tools fine_tunes.prepare_data -f sport2.jsonl -q\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI package using pip\nDESCRIPTION: This code snippet installs the OpenAI Python package using pip. This is a prerequisite for using the OpenAI API to interact with models like GPT-3.5.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install openai --quiet\n```\n\n----------------------------------------\n\nTITLE: Executing the Multi-Agent System with a User Query\nDESCRIPTION: Demonstrates how to invoke the multi-agent system with a sample user query. This shows the complete workflow in action.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nhandle_user_message(user_query)\n```\n\n----------------------------------------\n\nTITLE: Displaying Kangas DataGrid with Embedding Projections in Python\nDESCRIPTION: Renders the DataGrid directly in the notebook, showing all columns including the 2D projection of embeddings where points are colored by Score.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndg.show()\n```\n\n----------------------------------------\n\nTITLE: Installing the OpenAI Python Library\nDESCRIPTION: This command uses `pip` to install or upgrade the OpenAI Python library.  The `--upgrade` flag ensures that you have the latest version of the library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/python-setup.txt#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npip install --upgrade openai\n```\n\n----------------------------------------\n\nTITLE: Listing Pinecone Indexes\nDESCRIPTION: This code snippet lists the existing indexes in the Pinecone vector database. It uses the `pinecone.list_indexes()` method to retrieve the list of indexes.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npinecone.list_indexes()\n```\n\n----------------------------------------\n\nTITLE: Applying Distance Filtering to Retrieved Documents\nDESCRIPTION: Applies the filtering function to the previously retrieved claim query results to remove less relevant documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfiltered_claim_query_result = filter_query_result(claim_query_result)\n```\n\n----------------------------------------\n\nTITLE: Running Example Query in Title Namespace\nDESCRIPTION: This snippet runs an example query for 'modern art in Europe' in the 'title' namespace using the `query_article` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nquery_output = query_article('modern art in Europe','title')\n```\n\n----------------------------------------\n\nTITLE: Defining RetrieverEvaluator\nDESCRIPTION: Initializes a RetrieverEvaluator using two metrics: 'mrr' and 'hit_rate'. It utilizes the created `retriever` to perform the evaluation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nretriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [\"mrr\", \"hit_rate\"], retriever=retriever\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataframe Information\nDESCRIPTION: Displays information about the Pandas DataFrame including the data types, number of non-null values, and memory usage.  This is useful for data quality assessment and understanding the structure of the DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\narticle_df.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: ChatGPT API call without system message\nDESCRIPTION: Demonstrates a simple API call with only a user message and no system message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# example without a system message\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    temperature=0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Create SQLite Database Connection (Python)\nDESCRIPTION: Defines a function `create_connection` to establish a connection to a SQLite database. By default, it creates an in-memory database, useful for temporary testing. It takes an optional `db_file` path. The function attempts the connection and prints a success message or an error message if the connection fails, returning the connection object or `None`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_evaluate_LLMs_for_SQL_generation.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef create_connection(db_file=\":memory:\"):\n    \"\"\" create a database connection to a SQLite database\n        specified by the db_file\n    :param db_file: database file\n    :return: Connection object or None\n    \"\"\"\n    conn = None\n    try:\n        conn = sqlite3.connect(db_file)\n        print(f\"Connected to SQLite database: {db_file}\")\n        return conn\n    except Error as e:\n        print(e)\n    return conn\n```\n\n----------------------------------------\n\nTITLE: Authenticating OpenAI API Key via getpass - Python\nDESCRIPTION: This snippet ensures that the OPENAI_API_KEY is present in environment variables for secure access to OpenAI's API. If the key is unset, it requests the user to enter it interactively using getpass, validates that the key looks like an OpenAI key, and prints a configuration message. Dependencies include getpass and os. Inputs are provided interactively; the output is confirmation or an assertion error if the key is invalid.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# authenticate with OpenAI\nfrom getpass import getpass\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\nassert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\nprint(\"OpenAI API key configured\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Files and Creating Vector Store in Node.js\nDESCRIPTION: Demonstrates how to prepare file streams from local files and create a vector store named 'Financial Statement' using the OpenAI Node.js beta API. Then uploads the files to the created vector store and polls for completion. Requires the OpenAI Node.js SDK and fs module for file streaming.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/tool-file-search.txt#_snippet_4\n\nLANGUAGE: node.js\nCODE:\n```\nconst fileStreams = [\"edgar/goog-10k.pdf\", \"edgar/brka-10k.txt\"].map((path) =>\n  fs.createReadStream(path),\n);\n \n// Create a vector store including our two files.\nlet vectorStore = await openai.beta.vectorStores.create({\n  name: \"Financial Statement\",\n});\n \nawait openai.beta.vectorStores.fileBatches.uploadAndPoll(vectorStore.id, fileStreams)\n```\n\n----------------------------------------\n\nTITLE: Create Tags Index Python\nDESCRIPTION: This code creates an index on the `tags` column of the `philosophers_cql` table using `StorageAttachedIndex`.  This index enables efficient filtering of quotes based on their tags during searches.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\ncreate_tags_index_statement = f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS idx_tags\n    ON {keyspace}.philosophers_cql (VALUES(tags))\n    USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n\"\"\"\n```\n```\n\nLANGUAGE: python\nCODE:\n```\n```python\nsession.execute(create_tags_index_statement)\n```\n```\n\n----------------------------------------\n\nTITLE: Summarizing the Deep Lake Vector Store Dataset in Python\nDESCRIPTION: Uses the Deep Lake vector store's underlying dataset object to print a summary of its tensors and sample count. This is helpful to confirm the ingestion status and verify data integrity.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndb.vectorstore.summary()\n```\n\n----------------------------------------\n\nTITLE: Defining Neo4j Connection Credentials in Python\nDESCRIPTION: Defines Python string variables (`url`, `username`, `password`) to store the connection details for the target Neo4j database instance. The `url` specifies the Bolt connection endpoint, while `username` and `password` are used for authentication. The placeholder password needs to be replaced with the actual database credential.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# DB credentials\nurl = \"bolt://localhost:7687\"\nusername =\"neo4j\"\npassword = \"<your_password_here>\"\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Dependencies\nDESCRIPTION: This code snippet installs the necessary dependencies for the LlamaIndex framework. It uses pip, the Python package installer, to install the `llama-index` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n```\n\n----------------------------------------\n\nTITLE: Deleting Vector Store (Python)\nDESCRIPTION: This snippet deletes a vector store using the OpenAI Assistants API. It requires the ID of the vector store to be deleted. It deletes the vector store.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Delete the vector store\nclient.beta.vector_stores.delete(vector_store.id)\n```\n\n----------------------------------------\n\nTITLE: Testing Database Connection\nDESCRIPTION: Executes a simple query to test if the connection to the PolarDB-PG database is successful.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Execute a simple query to test the connection\ncursor.execute(\"SELECT 1;\")\nresult = cursor.fetchone()\n\n# Check the query result\nif result == (1,):\n    print(\"Connection successful!\")\nelse:\n    print(\"Connection failed.\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Timestamps from Transcriptions in Node.js using Whisper API\nDESCRIPTION: This Node.js example requests 'verbose_json' format with word-level timestamps using OpenAI's transcription endpoint. The relevant parameters are passed to the API call, and the result's text is printed. Requires 'openai' and 'fs' Node.js libraries, and an input audio file path ('audio.mp3'). Output includes detailed timestamp data for post-processing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_10\n\nLANGUAGE: node\nCODE:\n```\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\nasync function main() {\n  const transcription = await openai.audio.transcriptions.create({\n    file: fs.createReadStream(\"audio.mp3\"),\n    model: \"whisper-1\",\n    response_format: \"verbose_json\",\n    timestamp_granularities: [\"word\"]\n  });\n  console.log(transcription.text);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Answer Data\nDESCRIPTION: Prints the first answer from the loaded dataset to inspect its format and content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(answers[0])\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Questions Dictionary\nDESCRIPTION: Displays the dictionary containing the generated questions for all PDF files to examine the quality of the evaluation dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/File_Search_Responses.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquestions_dict\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for OpenAI Agents SDK and Stripe Integration in Python\nDESCRIPTION: Installs required Python packages silently to enable environment variable loading, OpenAI Agents SDK usage, Stripe API interaction, and typing extensions support. These packages are prerequisites for running the dispute management workflows demonstrated later. Installation is done using pip with the quiet flag to suppress output during installation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/agents_sdk/dispute_agent.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install python-dotenv --quiet\n%pip install openai-agents --quiet\n%pip install stripe --quiet\n%pip install typing_extensions --quiet\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Output Python\nDESCRIPTION: Displays the GPT-4 response in a formatted markdown output. It uses IPython's `Markdown` function to render the model's content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Markdown\n\ndisplay(Markdown(res['choices'][0]['message']['content']))\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Merged Text Chunk from the Dataset in Python\nDESCRIPTION: Outputs the first merged data item from the newly formed list, showing the consolidated text chunk and associated metadata fields such as start/end times, title, URL, and channel ID. This confirms the structure of data ready for embedding and indexing.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnew_data[0]\n```\n\n----------------------------------------\n\nTITLE: Count Points in Qdrant Collection\nDESCRIPTION: Retrieves and implicitly displays the total number of points (vectors) stored in the 'Articles' collection to verify the upsert operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Check the collection size to make sure all the points have been stored\nqdrant.count(collection_name='Articles')\n```\n\n----------------------------------------\n\nTITLE: Defining Company Summary (Python)\nDESCRIPTION: Assigns a descriptive string about the fictional company, NotReal Corp, to the `company_summary` variable. This summary provides context about the company's business, which will be used as input for generating an image via DALL-E 3.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncompany_summary = \"NotReal Corp is a prominent hardware company that manufactures and sells processors, graphics cards and other essential computer hardware.\"\n\n```\n\n----------------------------------------\n\nTITLE: Connect to Tair database using URL provided by user\nDESCRIPTION: Requests the user to input the Tair server URL securely with getpass. Establishes a connection with Tair using the TairClient's from_url method for database interactions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# The format of url: redis://[[username]:[password]]@localhost:6379/0\nTAIR_URL = getpass.getpass(\"Input your tair url:\")\n\nfrom tair import Tair as TairClient\n\n# connect to tair from url and create a client\n\nurl = TAIR_URL\nclient = TairClient.from_url(url)\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI SDK in Deno or Edge Functions from CDN\nDESCRIPTION: Imports the OpenAI SDK directly from a CDN (esm.sh) for use in Deno or Supabase Edge Functions. No local npm installation is needed; the specified version is fetched over HTTP at runtime. Adjust the version string as needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport OpenAI from \"https://esm.sh/openai@4\";\n```\n\n----------------------------------------\n\nTITLE: Defining Constants for Redis Vector Index\nDESCRIPTION: Sets up constants needed for creating a vector search index in Redis, including dimensions, distance metric, and naming.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Constants\nVECTOR_DIM = len(article_df['title_vector'][0]) # length of the vectors\nVECTOR_NUMBER = len(article_df)                 # initial number of vectors\nINDEX_NAME = \"embeddings-index\"                 # name of the search index\nPREFIX = \"doc\"                                  # prefix for the document keys\nDISTANCE_METRIC = \"COSINE\"                      # distance metric for the vectors (ex. COSINE, IP, L2)\n```\n\n----------------------------------------\n\nTITLE: Creating a database in SingleStoreDB\nDESCRIPTION: This code snippet creates a database named 'winter_wikipedia2' in SingleStoreDB if it does not already exist.  It uses a SQL CREATE DATABASE statement executed through the database connection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Create database\nstmt = \"\"\"\n    CREATE DATABASE IF NOT EXISTS winter_wikipedia2;\n\"\"\"\n\ncur.execute(stmt)\n```\n\n----------------------------------------\n\nTITLE: Correcting Transcription with GPT-4o Post-processing (Python)\nDESCRIPTION: Shows a post-processing technique where a Whisper transcription is corrected using a chat model like GPT-4o. A detailed `system_prompt` instructs the model to correct spelling discrepancies, especially for specific product names, based on the provided text (assumed to be the Whisper output via a `transcribe` function call). This method leverages the chat model's instruction following and larger context window.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/speech-to-text.txt#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided.\"\n\ndef generate_corrected_transcript(temperature, system_prompt, audio_file):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        temperature=temperature,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": transcribe(audio_file, \"\")\n            }\n        ]\n    )\n    return completion.choices[0].message.content\n\ncorrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Information in Kangas\nDESCRIPTION: Displays information about the columns and data types in the loaded DataGrid using the info() method.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata.info()\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Response by ID\nDESCRIPTION: Demonstrates the stateful nature of the Responses API by fetching a previously created response using its ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfetched_response = client.responses.retrieve(\nresponse_id=response.id)\n\nprint(fetched_response.output[0].content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for GPT Image in Python\nDESCRIPTION: Installs the Pillow and OpenAI libraries needed for working with images and the OpenAI API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Generate_Images_With_GPT_Image.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install pillow openai\n```\n\n----------------------------------------\n\nTITLE: Setting Authentication Mode Flag\nDESCRIPTION: Sets a flag to determine which authentication method to use with Azure OpenAI service.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory\n```\n\n----------------------------------------\n\nTITLE: Initiating OpenAI Model Fine-tuning in Python\nDESCRIPTION: Initializes an `OpenAIFineTuner` class (presumably a custom or library class) instance. It configures the fine-tuning job by providing the path to the training data file (`local_cache/100_train_few_shot.jsonl`), the base model name (`gpt-3.5-turbo`), and a custom suffix for the fine-tuned model name. It then calls the `fine_tune_model` method on the instance to start the fine-tuning process via the OpenAI API and stores the returned model ID.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Assumes OpenAIFineTuner class is defined or imported elsewhere\nfine_tuner = OpenAIFineTuner(\n        training_file_path=\"local_cache/100_train_few_shot.jsonl\",\n        model_name=\"gpt-3.5-turbo\",\n        suffix=\"trnfewshot20230907\"\n    )\n\nmodel_id = fine_tuner.fine_tune_model()\nmodel_id # This likely prints the model_id in an interactive environment\n```\n\n----------------------------------------\n\nTITLE: Defining Index Constants\nDESCRIPTION: This snippet defines constants used for creating the Redis search index, including the index name, document key prefix, distance metric, and the number of vectors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Constants\nINDEX_NAME = \"product_embeddings\"           # name of the search index\nPREFIX = \"doc\"                            # prefix for the document keys\nDISTANCE_METRIC = \"L2\"                # distance metric for the vectors (ex. COSINE, IP, L2)\nNUMBER_OF_VECTORS = len(df)\n```\n\n----------------------------------------\n\nTITLE: Generating Question Context Pairs\nDESCRIPTION: Generates question-context pairs to be used for evaluating the retrieval performance.  The `generate_question_context_pairs` function is utilized, using the nodes created earlier, along with a LLM instance, generating two questions per chunk.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nqa_dataset = generate_question_context_pairs(\n    nodes,\n    llm=llm,\n    num_questions_per_chunk=2\n)\n```\n\n----------------------------------------\n\nTITLE: Printing the Follow-up Response Content\nDESCRIPTION: Extracts and prints the text content from the follow-up response.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/responses_api/responses_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(response_two.output[0].content[0].text)\n```\n\n----------------------------------------\n\nTITLE: Uploading Data File to OpenAI Assistants API (Python)\nDESCRIPTION: Uploads the local financial data JSON file to the OpenAI platform with the purpose 'assistants'. This makes the file accessible to Assistants for processing, particularly for tools like Code Interpreter that need access to user-provided data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfile = client.files.create(\n  file=open('data/NotRealCorp_financial_data.json',\"rb\"),\n  purpose='assistants',\n)\n\n```\n\n----------------------------------------\n\nTITLE: Installing required Python packages for Azure AI Search and OpenAI\nDESCRIPTION: Installs necessary Python packages including wget for downloading files, azure-search-documents and azure-identity to interact with Azure AI Search, and openai for OpenAI API integration. These packages are prerequisites for further operations in this notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install wget\n! pip install azure-search-documents \n! pip install azure-identity\n! pip install openai\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure AI Search Prerequisites\nDESCRIPTION: Setting up the Azure subscription, resource group, and region parameters needed for deploying Azure AI Search services. Uses the InteractiveBrowserCredential for authentication.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Update the below with your values\nsubscription_id=\"<enter_your_subscription_id>\"\nresource_group=\"<enter_your_resource_group>\"\n\n## Make sure to choose a region that supports the proper products. We've defaulted to \"eastus\" below. https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/#products-by-region_tab5\nregion = \"eastus\"\ncredential = InteractiveBrowserCredential()\nsubscription_client = SubscriptionClient(credential)\nsubscription = next(subscription_client.subscriptions.list())\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages Using pip - Python\nDESCRIPTION: This snippet uses shell commands within a Python notebook environment to install required libraries: 'cassio' for the Cassandra vector store integration, 'openai' for API access to embeddings, and 'datasets' for dataset management. These dependencies must be installed before proceeding with database or OpenAI API usage. The installation is done quietly, suppressing output unless errors occur.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --quiet \"cassio>=0.1.3\" \"openai>=1.0.0\" datasets\n\n```\n\n----------------------------------------\n\nTITLE: Loading data from the openai-python repo\nDESCRIPTION: This code snippet defines the root directory for the openai-python repository and extracts all functions from the repository using the `extract_functions_from_repo` function, assuming the repository exists in the user's root directory.  The output is stored in `all_funcs`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Set user root directory to the 'openai-python' repository\nroot_dir = Path.home()\n\n# Assumes the 'openai-python' repository exists in the user's root directory\ncode_root = root_dir / 'openai-python'\n\n# Extract all functions from the repository\nall_funcs = extract_functions_from_repo(code_root)\n```\n\n----------------------------------------\n\nTITLE: Defining Function Tools for OpenAI Assistant in Python\nDESCRIPTION: Defines a list of function tools for an OpenAI assistant designed for customer support. Each tool is represented as a dictionary specifying its type (`function`), name, description, and a detailed JSON schema for its parameters, including required fields and data types. These tools enable the assistant to perform actions like retrieving delivery dates, canceling orders, processing returns, and updating shipping or payment information.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Prompt_Caching101.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport json\n\n# Define tools\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_delivery_date\",\n            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\",\n                    },\n                },\n                \"required\": [\"order_id\"],\n                \"additionalProperties\": False,\n            },\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"cancel_order\",\n            \"description\": \"Cancel an order that has not yet been shipped. Use this when a customer requests order cancellation.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\"\n                    },\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"The reason for cancelling the order.\"\n                    }\n                },\n                \"required\": [\"order_id\", \"reason\"],\n                \"additionalProperties\": False\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"return_item\",\n            \"description\": \"Process a return for an order. This should be called when a customer wants to return an item and the order has already been delivered.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\"\n                    },\n                    \"item_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The specific item ID the customer wants to return.\"\n                    },\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"The reason for returning the item.\"\n                    }\n                },\n                \"required\": [\"order_id\", \"item_id\", \"reason\"],\n                \"additionalProperties\": False\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"update_shipping_address\",\n            \"description\": \"Update the shipping address for an order that hasn't been shipped yet. Use this if the customer wants to change their delivery address.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\"\n                    },\n                    \"new_address\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"street\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new street address.\"\n                            },\n                            \"city\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new city.\"\n                            },\n                            \"state\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new state.\"\n                            },\n                            \"zip\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new zip code.\"\n                            },\n                            \"country\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new country.\"\n                            }\n                        },\n                        \"required\": [\"street\", \"city\", \"state\", \"zip\", \"country\"],\n                        \"additionalProperties\": False\n                    }\n                },\n                \"required\": [\"order_id\", \"new_address\"],\n                \"additionalProperties\": False\n            }\n        }\n    },\n    # New tool: Update payment method\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"update_payment_method\",\n            \"description\": \"Update the payment method for an order that hasn't been completed yet. Use this if the customer wants to change their payment details.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\"\n                    },\n                    \"payment_method\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"card_number\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new credit card number.\"\n                            },\n                            \"expiry_date\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new credit card expiry date in MM/YY format.\"\n                            },\n                            \"cvv\": {\n                                \"type\": \"string\",\n                                \"description\": \"The new credit card CVV code.\"\n                            }\n                        },\n                        \"required\": [\"card_number\", \"expiry_date\", \"cvv\"],\n                        \"additionalProperties\": False\n                    }\n                },\n                \"required\": [\"order_id\", \"payment_method\"],\n                \"additionalProperties\": False\n            }\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Computing and Displaying Confusion Matrix in Python\nDESCRIPTION: Generates a confusion matrix comparing inferred labels from the model to the ground truth labels. Uses nested dictionaries indexed by prediction and truth labels for counting occurrences. The function prints a formatted confusion matrix table and returns the count dictionary. It asserts input length equality to ensure correctness.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef confusion_matrix(inferred, groundtruth):\n    assert len(inferred) == len(groundtruth)\n    confusion = {\n        'True': {'True': 0, 'False': 0, 'NEE': 0},\n        'False': {'True': 0, 'False': 0, 'NEE': 0},\n        'NEE': {'True': 0, 'False': 0, 'NEE': 0},\n    }\n    for i, g in zip(inferred, groundtruth):\n        confusion[i][g] += 1\n\n    # Pretty print the confusion matrix\n    print('\\tGroundtruth')\n    print('\\tTrue\\tFalse\\tNEE')\n    for i in confusion:\n        print(i, end='\\t')\n        for g in confusion[i]:\n            print(confusion[i][g], end='\\t')\n        print()\n\n    return confusion\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This code snippet sets the OpenAI API key as an environment variable. The `OPENAI_API_KEY` is essential for authenticating with OpenAI's services to generate embeddings.  It demonstrates how to check if the environment variable is set and also includes commented-out code showing how to set a temporary environment variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print(\"OPENAI_API_KEY is ready\")\nelse:\n    print(\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Google Cloud Function Using gcloud CLI with Environment Variables\nDESCRIPTION: Deploys a Google Cloud Function named 'openai_docs_search' using the gcloud command line interface. The function runs on Python 3.9, triggers on HTTP requests, allows unauthenticated access, and uses environment variables specified in 'env.yml'. The snippet demonstrates the exact CLI command encapsulated within a Python cell magic call, useful for notebook environments. It reminds that the function lacks authentication and should not be used as is in production.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n! gcloud functions deploy openai_docs_search \\\n  --runtime python39 \\\n  --trigger-http \\\n  --allow-unauthenticated \\\n  --env-vars-file env.yml\n```\n\n----------------------------------------\n\nTITLE: Connecting to SingleStoreDB\nDESCRIPTION: This code snippet establishes a connection to a SingleStoreDB database using the singlestoredb library.  It uses placeholder credentials for the user, password, and host, which should be replaced with actual values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport singlestoredb as s2\n\nconn = s2.connect(\"<user>:<Password>@<host>:3306/\")\n\ncur = conn.cursor()\n```\n\n----------------------------------------\n\nTITLE: Get Secure Connect Bundle Path Python\nDESCRIPTION: This code obtains the path to the Secure Connect Bundle for the Astra DB connection.  It checks for Google Colab and prompts the user to upload the bundle if in Colab, otherwise it prompts the user for the path in a local Jupyter notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Your database's Secure Connect Bundle zip file is needed:\nif IS_COLAB:\n    print('Please upload your Secure Connect Bundle zipfile: ')\n    uploaded = files.upload()\n    if uploaded:\n        astraBundleFileTitle = list(uploaded.keys())[0]\n        ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n    else:\n        raise ValueError(\n            'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n        )\nelse:\n    # you are running a local-jupyter notebook:\n    ASTRA_DB_SECURE_BUNDLE_PATH = input(\"Please provide the full path to your Secure Connect Bundle zipfile: \")\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Cognito User (Bash)\nDESCRIPTION: This bash command uses the AWS CLI to create a new user within a specified Cognito User Pool. It requires the user pool ID, username, user attributes (email), and a temporary password. This is a one-time operation to create a user that can then authenticate against the Lambda function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naws cognito-idp admin-create-user \\\n    --user-pool-id \"your-region_xxxxx\" \\\n    --username johndoe@example.com \\\n    --user-attributes Name=email,Value=johndoe@example.com \\\n    --temporary-password \"TempPassword123\"\n```\n\n----------------------------------------\n\nTITLE: Setting the Azure OpenAI Deployment Name\nDESCRIPTION: Specifies the deployment name for the Azure OpenAI model to be used for chat completions. This name is created when deploying a model in the Azure OpenAI Studio.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndeployment = \"\" # Fill in the deployment name from the portal here\n```\n\n----------------------------------------\n\nTITLE: Calling create_commands function\nDESCRIPTION: This code block calls the `create_commands` function with the generated function invocations (`input_objects`) to generate conversational prompts. It stores the results in `training_examples_unformatted`.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntraining_examples_unformatted = create_commands(input_objects)\n```\n\n----------------------------------------\n\nTITLE: Defining RediSearch Index Constants (Python)\nDESCRIPTION: This snippet defines constants necessary for configuring the RediSearch index, such as the dimension of the vector embeddings, the expected number of vectors, the desired name for the search index, the key prefix for documents in Redis, and the distance metric (e.g., COSINE, IP, L2) to use for vector similarity search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Constants\nVECTOR_DIM = len(data['title_vector'][0]) # length of the vectors\nVECTOR_NUMBER = len(data)                 # initial number of vectors\nINDEX_NAME = \"embeddings-index\"           # name of the search index\nPREFIX = \"doc\"                            # prefix for the document keys\nDISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Redis with Docker\nDESCRIPTION: Instructions for deploying Redis with RediSearch module using Docker Compose.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cd redis\n$ docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Listing Modules API Endpoint Definition (YAML)\nDESCRIPTION: This snippet defines the `/courses/{course_id}/modules` endpoint to retrieve a list of modules within a given course.  It uses a path parameter for the `course_id` and includes query parameters for filtering and pagination, with example values provided.  The response specifies the expected schema for a list of modules.  Dependencies include the Canvas LMS API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_canvas.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n  /courses/{course_id}/modules:\n    get:\n      operationId: listModules\n      summary: List modules in a course\n      description: Retrieves the list of modules for a given course in Canvas.\n      parameters:\n        - name: course_id\n          in: path\n          required: true\n          description: The ID of the course.\n          schema:\n            type: integer\n        - name: include\n          in: query\n          description: Include additional information such as items in the response.\n          schema:\n            type: array\n            items:\n              type: string\n            example: [\"items\"]\n        - name: search_term\n          in: query\n          description: The partial title of the module to match and return.\n          schema:\n            type: string\n        - name: student_id\n          in: query\n          description: Return module completion information for the student with this ID.\n          schema:\n            type: integer\n        - name: per_page\n          in: query\n          description: The number of results to return per page.\n          schema:\n            type: integer\n          example: 10\n        - name: page\n          in: query\n          description: The page number to return.\n          schema:\n            type: integer\n          example: 1\n      responses:\n        '200':\n          description: A list of modules in the course.\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  type: object\n                  properties:\n                    id:\n                      type: integer\n                      description: The ID of the module.\n                    name:\n                      type: string\n                      description: The name of the module.\n                    items_count:\n                      type: integer\n                      description: The number of items in the module.\n                    state:\n                      type: string\n                      description: The state of the module (e.g., \"active\", \"locked\").\n        '400':\n          description: Bad request, possibly due to an invalid course ID or query parameters.\n        '401':\n          description: Unauthorized, likely due to invalid authentication credentials.\n        '404':\n          description: Course not found, possibly due to an invalid course ID.\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenAI API Key Environment Variable\nDESCRIPTION: Checks if the OpenAI API key is correctly set as an environment variable. Provides an alternative method to set the key directly in the code if needed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ['OPENAI_API_KEY'] = 'your-key-goes-here'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This code sets the OpenAI API key using the `os.environ` function. This is necessary to authenticate with the OpenAI API and use their models for the RAG pipeline.  Ensure your API key is securely stored.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ['OPENAI_API_KEY'] = 'YOUR OPENAI API KEY'\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone and OpenAI Clients in Retool (Python)\nDESCRIPTION: This snippet demonstrates initializing the OpenAI and Pinecone clients using API keys retrieved from Retool Configuration Variables (`retoolContext.configVars`). This approach is recommended for securely managing sensitive credentials within the workflow.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/pinecone-retool/gpt-action-pinecone-retool-rag.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI(api_key=retoolContext.configVars.openai_api_key) \npc = Pinecone(api_key=retoolContext.configVars.pinecone_api_key)\n```\n\n----------------------------------------\n\nTITLE: Defining an Example Request for Output Moderation Testing in Python\nDESCRIPTION: This snippet defines a Python string variable `interesting_request`. The string contains a prompt designed to test output moderation, potentially passing input moderation but likely triggering output flags due to its request for violent content description.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Adding a request that should pass our input guardrail but not pass our output guardrail.\ninteresting_request = \"Describe a scene from a violent movie in detail.\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Downloaded Embeddings Zip File in Python\nDESCRIPTION: Extracts the contents of the downloaded `vector_database_wikipedia_articles_embedded.zip` file into a specified output directory (`../../data`) using Python's `zipfile` module. It then uses the `os` module to check if the expected CSV file (`vector_database_wikipedia_articles_embedded.csv`) exists in the target directory and prints a status message.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nimport os\nimport re\nimport tempfile\n\ncurrent_directory = os.getcwd()\nzip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\noutput_directory = os.path.join(current_directory, \"../../data\")\n\nwith zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n    zip_ref.extractall(output_directory)\n\n\n# check the csv file exist\nfile_name = \"vector_database_wikipedia_articles_embedded.csv\"\ndata_directory = os.path.join(current_directory, \"../../data\")\nfile_path = os.path.join(data_directory, file_name)\n\n\nif os.path.exists(file_path):\n    print(f\"The file {file_name} exists in the data directory.\")\nelse:\n    print(f\"The file {file_name} does not exist in the data directory.\")\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies Python\nDESCRIPTION: This code snippet installs the necessary Python packages for the project.  It includes the Cassandra driver, OpenAI library, and the datasets library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\n!pip install --quiet \"cassandra-driver>=0.28.0\" \"openai>=1.0.0\" datasets\n```\n```\n\n----------------------------------------\n\nTITLE: Authentication Mode Selection for Azure OpenAI\nDESCRIPTION: Sets a flag to determine which authentication method to use with Azure OpenAI. When set to True, Azure Active Directory authentication will be used; otherwise, API key authentication is used.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory\n```\n\n----------------------------------------\n\nTITLE: Searching for \"bad delivery\" - Python\nDESCRIPTION: This code snippet invokes the `search_reviews` function to search for reviews related to \"bad delivery\" and retrieves the top 1 most similar review.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults = search_reviews(df, \"bad delivery\", n=1)\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Milvus Collection in Python\nDESCRIPTION: Uses the `pymilvus.utility` module to check if a collection with the name specified by `COLLECTION_NAME` already exists in the connected Milvus instance. If it exists, the collection is dropped to ensure a clean setup.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Remove collection if it already exists\nif utility.has_collection(COLLECTION_NAME):\n    utility.drop_collection(COLLECTION_NAME)\n```\n\n----------------------------------------\n\nTITLE: Obtaining On-Behalf-Of Authentication Token for Microsoft Graph API\nDESCRIPTION: This function obtains an On-Behalf-Of (OBO) token from Microsoft identity platform using an existing bearer token. This enables passing through user credentials to ensure the search only returns files the logged-in user can access.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst axios = require('axios');\nconst qs = require('querystring');\n\nasync function getOboToken(userAccessToken) {\n    const { TENANT_ID, CLIENT_ID, MICROSOFT_PROVIDER_AUTHENTICATION_SECRET } = process.env;\n    const params = {\n        client_id: CLIENT_ID,\n        client_secret: MICROSOFT_PROVIDER_AUTHENTICATION_SECRET,\n        grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n        assertion: userAccessToken,\n        requested_token_use: 'on_behalf_of',\n        scope: 'https://graph.microsoft.com/.default'\n    };\n\n    const url = `https\\://login.microsoftonline.com/${TENANT_ID}/oauth2/v2.0/token`;\n    try {\n        const response = await axios.post(url, qs.stringify(params), {\n            headers: { 'Content-Type': 'application/x-www-form-urlencoded' }\n        });\n        return response.data.access\\_token;\n    } catch (error) {\n        console.error('Error obtaining OBO token:', error.response?.data || error.message);\n        throw error;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Single Text Embedding with OpenAI API in Python\nDESCRIPTION: This snippet demonstrates how to generate a text embedding for a single string using OpenAI's text-embedding-3-small model via the Python openai library. It creates an OpenAI API client instance, submits a text input to the embedding endpoint, and retrieves the resulting embedding vector, whose length is then printed. Requires installation of the openai package and a valid API key; input is a single string and output is a list of floats representing the embedding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Using_embeddings.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI()\n\nembedding = client.embeddings.create(\n    input=\"Your text goes here\", model=\"text-embedding-3-small\"\n).data[0].embedding\nlen(embedding)\n\n```\n\n----------------------------------------\n\nTITLE: Constructing a Prompt Template and Agent with History in LangChain (Python)\nDESCRIPTION: Builds a CustomPromptTemplate object using the history-aware template and initializes an LLMChain and agent using the output_parser and stop conditions. This configuration gives the agent access to prior dialogue turns via the 'history' variable, allowing for context-aware answer generation. Prerequisites are defined tool objects, a compatible LLM, and the prompt template; the agent is ready for conversational tasks with memory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprompt_with_history = CustomPromptTemplate(\n    template=template_with_history,\n    tools=tools,\n    # The history template includes \"history\" as an input variable so we can interpolate it into the prompt\n    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n)\n\nllm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\ntool_names = [tool.name for tool in tools]\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain, \n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"], \n    allowed_tools=tool_names\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for Custom Moderation in Python\nDESCRIPTION: This Python snippet defines a string variable `parameters` holding the criteria (e.g., 'political content, misinformation') to be used by the `custom_moderation` function for content assessment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example content and parameters\nparameters = \"political content, misinformation\"\n```\n\n----------------------------------------\n\nTITLE: Importing Qdrant Client Libraries in Python\nDESCRIPTION: This Python code imports essential classes and modules from the `qdrant_client` library. These imports are required for establishing a connection to a Qdrant instance, defining collection parameters (like vector size and distance metric), and creating data points (vectors with payloads) within Qdrant.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom qdrant_client.http.models import PointStruct\nfrom qdrant_client.http.models import Distance, VectorParams\n```\n\n----------------------------------------\n\nTITLE: Starting Milvus Service using Docker Compose\nDESCRIPTION: Executes a shell command to start the Milvus standalone service in the background using Docker Compose. This command assumes a `docker-compose.yaml` file is present in the current directory, configured to launch the necessary Milvus components.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n! docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Dropping Milvus Collection If Exists (Python)\nDESCRIPTION: Checks if a collection with the name specified by `COLLECTION_NAME` already exists in Milvus using `utility.has_collection`. If it does, the collection is removed using `utility.drop_collection` to ensure a clean state for the notebook execution.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Remove collection if it already exists\nif utility.has_collection(COLLECTION_NAME):\n    utility.drop_collection(COLLECTION_NAME)\n```\n\n----------------------------------------\n\nTITLE: Applying Best Optimized Matrix to Data in Python\nDESCRIPTION: Selects the best performing run from the `runs_df` based on the highest accuracy. It extracts the corresponding optimized `matrix` from that run and applies it to the original DataFrame `df` using the `apply_matrix_to_embeddings_dataframe` function, updating the custom embeddings and similarities. Depends on `pandas` and the `apply_matrix_to_embeddings_dataframe` function.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# apply result of best run to original data\nbest_run = runs_df.sort_values(by=\"accuracy\", ascending=False).iloc[0]\nbest_matrix = best_run[\"matrix\"]\napply_matrix_to_embeddings_dataframe(best_matrix, df)\n```\n\n----------------------------------------\n\nTITLE: Starting Milvus Service via Docker Compose (Shell)\nDESCRIPTION: Launches a Milvus standalone instance in detached mode (`-d`) using the `docker-compose.yaml` file located in the current directory. This command assumes Docker and Docker Compose are installed and configured correctly on the system.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n! docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Disallowing GPTBot Access via robots.txt\nDESCRIPTION: Demonstrates how to completely block OpenAI's GPTBot web crawler from accessing any part of a website using a `robots.txt` file directive. The `Disallow: /` rule prevents access to the entire site for the specified user agent.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/gptbot.txt#_snippet_1\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: GPTBot\nDisallow: /\n```\n\n----------------------------------------\n\nTITLE: Running Guardrail Evaluation Tests in Python\nDESCRIPTION: This snippet runs a series of user request tests using the execute_all_guardrails function, which asynchronously checks each input against the topical and moderation guardrails. Dependencies: prior definition of good_request, bad_request, great_request and execute_all_guardrails. Inputs are request strings; outputs are printed responses, indicating if the input passes or is blocked by any guardrail.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntests = [good_request,bad_request,great_request]\n\nfor test in tests:\n    result = await execute_all_guardrails(test)\n    print(result)\n    print('\\n\\n')\n    \n```\n\n----------------------------------------\n\nTITLE: Testing Elasticsearch Index with Match Query in Python\nDESCRIPTION: Performs a search query on the Elasticsearch index, omitting vector fields from the response, and queries articles with text containing 'Hummingbird'. Useful for validation or demonstration that the index is working correctly. Output is the resulting document matches, printed to console.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nprint(client.search(index=\"wikipedia_vector_index\", body={\n    \"_source\": {\n        \"excludes\": [\"title_vector\", \"content_vector\"]\n    },\n    \"query\": {\n        \"match\": {\n            \"text\": {\n                \"query\": \"Hummingbird\"\n            }\n        }\n    }\n}))\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet demonstrates how to set the OpenAI API key as an environment variable. It also checks if the API key is correctly set. This is necessary to use the OpenAI API for generating embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\nimport os\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = '<YOUR_OPENAI_API_KEY>'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    print (\"OPENAI_API_KEY is ready\")\nelse:\n    print (\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: This shell command uses `pip` to install necessary Python libraries: `openai` for generating embeddings, `qdrant-client` for interacting with Qdrant, `pandas` for data manipulation, and `wget` for downloading data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n! pip install openai qdrant-client pandas wget\n```\n\n----------------------------------------\n\nTITLE: Querying Redis with OpenAI Embeddings via search_redis Function - Python\nDESCRIPTION: Demonstrates usage of the `search_redis` function with a sample query to retrieve top 10 relevant documents about 'modern art in Europe' using default vector field and settings. Assumes an initialized Redis client with loaded index. Outputs are ranked articles with similarity scores.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# For using OpenAI to generate query embedding\nresults = search_redis(redis_client, 'modern art in Europe', k=10)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Python Library\nDESCRIPTION: Command to install the OpenAI Python library using pip, which is needed to interact with OpenAI's API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install openai\n```\n\n----------------------------------------\n\nTITLE: Display DataFrame Info\nDESCRIPTION: Shows detailed information about the pandas DataFrame, including column data types, non-null counts, and memory usage.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\narticle_df.info(show_counts=True)\n```\n\n----------------------------------------\n\nTITLE: Exploring Dataset Samples\nDESCRIPTION: Printing the first data sample to understand the content structure of the sports dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(sports_dataset['data'][0])\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving Embeddings for Reviews Using OpenAI API in Python\nDESCRIPTION: Iterates over the combined text of each filtered review, computing embeddings by calling a custom get_embedding function with the specified model. The resulting embeddings are stored as a new dataframe column. This process requires a configured OpenAI API key in the environment. After embedding generation, the enriched dataset is saved to CSV for future use, optimizing workflow by avoiding repeated API calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n\n# This may take a few minutes\ndf[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, model=embedding_model))\ndf.to_csv(\"data/fine_food_reviews_with_embeddings_1k.csv\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading Environment Variables (Python)\nDESCRIPTION: Imports essential Python libraries (`os`, `openai`, `dotenv`) and loads environment variables from a `.env` file, which are typically used for storing sensitive configuration like API keys and endpoints.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nimport dotenv\n\ndotenv.load_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Installing required Python packages for OpenAI, Clickhouse, wget, and pandas\nDESCRIPTION: Installs essential dependencies for interacting with OpenAI API, MyScale database, downloading datasets, and data processing. Ensures the environment is prepared for subsequent steps.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n! pip install openai clickhouse-connect wget pandas\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LLM Evaluation in Python Environment\nDESCRIPTION: Installs the required Python libraries: 'autoevals' for evaluation metrics, 'duckdb' for data handling, 'braintrust' for the evaluation platform integration, and 'openai' for interacting with OpenAI models. The '--quiet' flag suppresses installation output. This command is typically run in a Jupyter Notebook or IPython environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n%pip install autoevals duckdb braintrust openai --quiet\n```\n\n----------------------------------------\n\nTITLE: CLI Entry Point for Patch Processing in Python\nDESCRIPTION: The main function for command-line execution. It reads patch text from standard input, calls `process_patch` using the default filesystem helper functions (`open_file`, `write_file`, `remove_file`), and prints the result ('Done!') to standard output or any `DiffError` message to standard error.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef main() -> None:\n    import sys\n\n    patch_text = sys.stdin.read()\n    if not patch_text:\n        print(\"Please pass patch text through stdin\", file=sys.stderr)\n        return\n    try:\n        result = process_patch(patch_text, open_file, write_file, remove_file)\n    except DiffError as exc:\n        print(exc, file=sys.stderr)\n        return\n    print(result)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Printing DataFrame - Python\nDESCRIPTION: Prints the `df_results` DataFrame to the console using the `print` function. Assumes `df_results` is already defined and contains the desired data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Display the DataFrame\nprint(df_results)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Embedding and Database Integration in Python\nDESCRIPTION: Imports Python libraries required to interact with Astra DB, OpenAI, and the datasets library. 'getpass' is used for securely handling secret input; 'Counter' is used for simple statistics; 'AstraDB' initializes database connection; 'openai' for embedding and LLM API calls; 'load_dataset' for loading sample datasets.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_AstraPy.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom getpass import getpass\nfrom collections import Counter\n\nfrom astrapy.db import AstraDB\nimport openai\nfrom datasets import load_dataset\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Azure OpenAI\nDESCRIPTION: Imports the required libraries for working with Azure OpenAI, including the OpenAI client, environment variable handling, and dotenv for loading environment variables.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nimport dotenv\n\ndotenv.load_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing OpenAI Client in Python\nDESCRIPTION: Imports standard Python libraries and specific packages for text processing, data handling, and summarization evaluation. It initializes the OpenAI client using an API key retrieved from environment variables or provided directly, which is essential for performing LLM-based evaluations later in the notebook.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\nimport re\nimport pandas as pd\n\n# Python Implementation of the ROUGE Metric\nfrom rouge import Rouge\n\n# BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.\nfrom bert_score import BERTScorer\n\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries\nDESCRIPTION: This code imports the necessary Python libraries for data manipulation, file system operations, and working with the SingleStoreDB database. The pandas library is used for data manipulation, os and wget are used for file system interaction, and ast is used for literal evaluation of strings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nimport wget\nimport ast\n```\n\n----------------------------------------\n\nTITLE: Checking Weaviate Client Readiness\nDESCRIPTION: Checks if the Weaviate client is ready and able to communicate with the Weaviate instance. The function returns a boolean indicating whether the Weaviate instance is operational and ready to receive requests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclient.is_ready()\n```\n\n----------------------------------------\n\nTITLE: Connecting to Redis Server (Python)\nDESCRIPTION: This Python code establishes a connection to the running Redis server using the `redis-py` client library. It defines the host and port for the connection (defaulting to `localhost:6379`) and creates a `redis.Redis` client instance. A `ping()` command is executed to verify the connection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport redis\nfrom redis.commands.search.indexDefinition import (\n    IndexDefinition,\n    IndexType\n)\nfrom redis.commands.search.query import Query\nfrom redis.commands.search.field import (\n    TextField,\n    VectorField\n)\n\nREDIS_HOST =  \"localhost\"\nREDIS_PORT = 6379\nREDIS_PASSWORD = \"\" # default for passwordless Redis\n\n# Connect to Redis\nredis_client = redis.Redis(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD\n)\nredis_client.ping()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Embedding Vector Dimension Python\nDESCRIPTION: Prints the dimensions of the embedding vectors for each input phrase. This verifies the dimensionality of the 'text-embedding-3-small' model which is 1536. This confirms the correct dimensionality.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlen(res['data'][0]['embedding']), len(res['data'][1]['embedding'])\n```\n\n----------------------------------------\n\nTITLE: Plotting Model Comparison (Answer Expected) using Evaluator in Python\nDESCRIPTION: This Python code utilizes an instance of the `Evaluator` class (previously defined) to generate a bar plot comparing the performance of a baseline model and a fine-tuned model. The plot focuses specifically on the scenario where a correct answer is expected in the context, visualizing the percentage of correct answers, skipped questions, and wrong answers for each model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nevaluator.plot_model_comparison([\"generated_answer\", \"ft_generated_answer\"], scenario=\"answer_expected\", nice_names=[\"Baseline\", \"Fine-Tuned\"])\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Container Status\nDESCRIPTION: Checks if the Docker container is running by listing all active containers using the docker ps command.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/object_oriented_agentic_approach/Secure_code_interpreter_tool_for_LLM_agents.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!docker ps \n```\n\n----------------------------------------\n\nTITLE: Converting String Embeddings to Kangas Embedding Objects in Python\nDESCRIPTION: Creates a new DataGrid with the string representation of embeddings converted to proper Kangas Embedding objects with UMAP projection. The Score column is converted to string type for grouping.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport ast # to convert string of a list of numbers into a list of numbers\n\ndg = kg.DataGrid(\n    name=\"openai_embeddings\",\n    columns=data.get_columns(),\n    converters={\"Score\": str},\n)\nfor row in data:\n    embedding = ast.literal_eval(row[8])\n    row[8] = kg.Embedding(\n        embedding, \n        name=str(row[3]), \n        text=\"%s - %.10s\" % (row[3], row[4]),\n        projection=\"umap\",\n    )\n    dg.append(row)\n```\n\n----------------------------------------\n\nTITLE: Initializing Data Validation Lists - Python\nDESCRIPTION: Initializes two lists, `pred_is_valid` and `pred_issues`, with default values based on the length of the `input_data`. `pred_is_valid` is initialized with `False` and `pred_issues` is initialized with empty strings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/o1/Using_reasoning_for_data_validation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npred_is_valid = [False] * len(input_data)\npred_issues = [''] * len(input_data)\n```\n\n----------------------------------------\n\nTITLE: Viewing OpenAI Fine-Tuning Checkpoint Details (JSON)\nDESCRIPTION: This JSON object represents a fine-tuning checkpoint retrieved from the OpenAI API. It includes the checkpoint ID, creation timestamp, the specific fine-tuned model name associated with the checkpoint (`fine_tuned_model_checkpoint`), performance metrics at that point (like validation loss and accuracy), the parent fine-tuning job ID, and the training step number.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"object\": \"fine_tuning.job.checkpoint\",\n    \"id\": \"ftckpt_zc4Q7MP6XxulcVzj4MZdwsAB\",\n    \"created_at\": 1519129973,\n    \"fine_tuned_model_checkpoint\": \"ft:gpt-3.5-turbo-0125:my-org:custom-suffix:96olL566:ckpt-step-2000\",\n    \"metrics\": {\n        \"full_valid_loss\": 0.134,\n        \"full_valid_mean_token_accuracy\": 0.874\n    },\n    \"fine_tuning_job_id\": \"ftjob-abc123\",\n    \"step_number\": 2000\n}\n```\n\n----------------------------------------\n\nTITLE: Example Extracted JSON data (Python)\nDESCRIPTION: This JSON presents an example of the extracted data from a hotel invoice using GPT-4o.  It includes hotel information, guest information, invoice information, charges, payment information and tax information.  The data is grouped logically. Null values represent blank or missing fields from the original document.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[\n    {\n        \"Hotel Information\": {\n            \"Name\": \"Hamburg City (Zentrum)\",\n            \"Address\": \"Willy-Brandt-StraÃŸe 21, 20457 Hamburg, Deutschland\",\n            \"Phone\": \"+49 (0) 40 3039 379 0\"\n        },\n        \"Guest Information\": {\n            \"Name\": \"APIMEISTER CONSULTING GmbH\",\n            \"Guest\": \"Herr Jens Walter\",\n            \"Address\": \"Friedrichstr. 123, 10117 Berlin\"\n        },\n        \"Invoice Information\": {\n            \"Rechnungsnummer\": \"GABC19014325\",\n            \"Rechnungsdatum\": \"23.09.19\",\n            \"Referenznummer\": \"GABC015452127\",\n            \"Buchungsnummer\": \"GABR15867\",\n            \"Ankunft\": \"23.09.19\",\n            \"Abreise\": \"27.09.19\",\n            \"NÃ¤chte\": 4,\n            \"Zimmer\": 626,\n            \"Kundereferenz\": 2\n        },\n        \"Charges\": [\n            {\n                \"Datum\": \"23.09.19\",\n                \"Uhrzeit\": \"16:36\",\n                \"Beschreibung\": \"Ãœbernachtung\",\n                \"MwSt.%\": 7.0,\n                \"Betrag\": 77.0,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"24.09.19\",\n                \"Uhrzeit\": null,\n                \"Beschreibung\": \"Ãœbernachtung\",\n                \"MwSt.%\": 7.0,\n                \"Betrag\": 135.0,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"25.09.19\",\n                \"Uhrzeit\": null,\n                \"Beschreibung\": \"Ãœbernachtung\",\n                \"MwSt.%\": 7.0,\n                \"Betrag\": 82.0,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"26.09.19\",\n                \"Uhrzeit\": null,\n                \"Beschreibung\": \"Ãœbernachtung\",\n                \"MwSt.%\": 7.0,\n                \"Betrag\": 217.0,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"24.09.19\",\n                \"Uhrzeit\": \"9:50\",\n                \"Beschreibung\": \"Premier Inn FrÃ¼hstÃ¼cksbuffet\",\n                \"MwSt.%\": 19.0,\n                \"Betrag\": 9.9,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"25.09.19\",\n                \"Uhrzeit\": \"9:50\",\n                \"Beschreibung\": \"Premier Inn FrÃ¼hstÃ¼cksbuffet\",\n                \"MwSt.%\": 19.0,\n                \"Betrag\": 9.9,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"26.09.19\",\n                \"Uhrzeit\": \"9:50\",\n                \"Beschreibung\": \"Premier Inn FrÃ¼hstÃ¼cksbuffet\",\n                \"MwSt.%\": 19.0,\n                \"Betrag\": 9.9,\n                \"Zahlung\": null\n            },\n            {\n                \"Datum\": \"27.09.19\",\n                \"Uhrzeit\": \"9:50\",\n                \"Beschreibung\": \"Premier Inn FrÃ¼hstÃ¼cksbuffet\",\n                \"MwSt.%\": 19.0,\n                \"Betrag\": 9.9,\n                \"Zahlung\": null\n            }\n        ],\n        \"Payment Information\": {\n            \"Zahlung\": \"550,60\",\n            \"Gesamt (Rechnungsbetrag)\": \"550,60\",\n            \"Offener Betrag\": \"0,00\",\n            \"Bezahlart\": \"Mastercard-Kreditkarte\"\n        },\n        \"Tax Information\": {\n            \"MwSt.%\": [\n                {\n                    \"Rate\": 19.0,\n                    \"Netto\": 33.28,\n                    \"MwSt.\": 6.32,\n                    \"Brutto\": 39.6\n                },\n                {\n                    \"Rate\": 7.0,\n                    \"Netto\": 477.57,\n                    \"MwSt.\": 33.43,\n                    \"Brutto\": 511.0\n                }\n            ]\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment and Installing Dependencies\nDESCRIPTION: Commands to create a new Python virtual environment named 'env', activate it (specifically for Unix-like systems), and install the necessary packages listed in a 'requirements.txt' file using pip. This setup isolates project dependencies.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/crawl-website-embeddings.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv env\n\nsource env/bin/activate\n\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI SDK\nDESCRIPTION: This code snippet installs the latest version of the OpenAI Python SDK using pip. It upgrades the existing package to ensure compatibility with the Assistants API. The `!pip show openai | grep Version` command is included to check the installed version.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --upgrade openai\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Fine-tuning GPT Model on Transaction Classification in Python\nDESCRIPTION: Copies the dataframe with embeddings and labels for preparing fine-tuning data. Extracts unique classification labels and assigns numerical class IDs. Merges class IDs back into the dataframe. Creates a messages column with a list of two messages per sample: a user prompt formatted with transaction details and an assistant message containing the expected classification. Splits messages into training and validation sets and defines a function to export these samples to JSONL format required for OpenAI fine-tuning.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nft_prep_df = fs_df.copy()\nlen(ft_prep_df)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nft_prep_df.head()\n\n```\n\nLANGUAGE: python\nCODE:\n```\nclasses = list(set(ft_prep_df['Classification']))\nclass_df = pd.DataFrame(classes).reset_index()\nclass_df.columns = ['class_id','class']\nclass_df  , len(class_df)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nft_df_with_class = ft_prep_df.merge(class_df,left_on='Classification',right_on='class',how='inner')\n\n# Creating a list of messages for the fine-tuning job. The user message is the prompt, and the assistant message is the response from the model\nft_df_with_class['messages'] = ft_df_with_class.apply(lambda x: [{\"role\": \"user\", \"content\": format_prompt(x)}, {\"role\": \"assistant\", \"content\": x['class']}],axis=1)\nft_df_with_class[['messages', 'class']].head()\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Create train/validation split\nsamples = ft_df_with_class[\"messages\"].tolist()\ntrain_df, valid_df = train_test_split(samples, test_size=0.2, random_state=42)\n\ndef write_to_jsonl(list_of_messages, filename):\n    with open(filename, \"w+\") as f:\n        for messages in list_of_messages:\n            object = {  \n                \"messages\": messages\n            }\n            f.write(json.dumps(object) + \"\\n\")\n\n```\n\n----------------------------------------\n\nTITLE: Data Visualization in Python for Cryptocurrency Trends\nDESCRIPTION: This Python code creates visual representations of cryptocurrency market trends utilizing matplotlib and seaborn libraries. Dependencies include matplotlib, seaborn, and pandas for plotting and data handling. It takes processed data, such as price changes over time, and generates line charts, volatility plots, or heatmaps, facilitating insights into market behaviors.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/assets/011.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assume 'processed_data' is a DataFrame with columns 'Date' and 'Price'\nsns.lineplot(x='Date', y='Price', data=processed_data)\nplt.title('Cryptocurrency Price Trends')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Verifying Qdrant Service Status via Curl\nDESCRIPTION: This shell command sends an HTTP GET request to the default Qdrant API endpoint (localhost:6333) using `curl`. A successful response indicates that the Qdrant service is running and accessible.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n! curl http://localhost:6333\n```\n\n----------------------------------------\n\nTITLE: Removing Existing Collection in Zilliz\nDESCRIPTION: Checks if a collection with the specified name already exists in Zilliz and drops it to ensure a clean start for the new collection.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Remove collection if it already exists\nif utility.has_collection(COLLECTION_NAME):\n    utility.drop_collection(COLLECTION_NAME)\n```\n\n----------------------------------------\n\nTITLE: Identifying the GPTBot User Agent\nDESCRIPTION: Shows the user agent token and full user agent string used by OpenAI's GPTBot web crawler. This allows website administrators to identify requests originating from this bot in their server logs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/gptbot.txt#_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nUser agent token: GPTBot\nFull user-agent string: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0; +https://openai.com/gptbot)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Chat Messages and Initiating a Parallel Function Call with OpenAI API in Python\nDESCRIPTION: This snippet constructs system and user messages for use in an OpenAI Chat Completions request and demonstrates a function call with support for parallel function calling. It requires the variables 'tools' and 'GPT_MODEL' to be defined elsewhere. Expected input is a set of conversational messages describing the task; output is the model's tool_calls response, representing selected functions and arguments. Assumes access to an OpenAI-compliant 'chat_completion_request' function and models supporting function calling. Outputs tool calls chosen by the assistant for a multi-entity weather query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\nmessages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in San Francisco and Glasgow over the next 4 days\"})\nchat_response = chat_completion_request(\n    messages, tools=tools, model=GPT_MODEL\n)\n\nassistant_message = chat_response.choices[0].message.tool_calls\nassistant_message\n```\n\n----------------------------------------\n\nTITLE: Converting and Uploading Generated Plot File (OpenAI Assistants API/Python)\nDESCRIPTION: Identifies the file ID of the generated plot image from the Assistant's response. It then calls the `convert_file_to_png` helper function to download and save this file locally as a PNG. Finally, it uploads the locally saved PNG file back to the OpenAI Assistants API.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplot_file_id = messages.data[0].content[0].image_file.file_id\nimage_path = \"../images/NotRealCorp_chart.png\"\nconvert_file_to_png(plot_file_id,image_path)\n\n#Upload\nplot_file = client.files.create(\n  file=open(image_path, \"rb\"),\n  purpose='assistants'\n)\n\n```\n\n----------------------------------------\n\nTITLE: Grouping and Displaying Kangas DataGrid by Score in Python\nDESCRIPTION: Shows the DataGrid grouped by Score values, sorting by Score, limiting to 5 rows per group, and selecting only the Score and embedding columns.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndg.show(group=\"Score\", sort=\"Score\", rows=5, select=\"Score,embedding\")\n```\n\n----------------------------------------\n\nTITLE: Autoreload Extension Setup in Python\nDESCRIPTION: This code snippet loads the autoreload extension, which automatically reloads modules when changes are detected. This is useful for development to avoid restarting the kernel after making changes to the code.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Setting Variables for PowerPoint Slide Content in Python\nDESCRIPTION: These code snippets define the text variables to be used in the PowerPoint slides. The first snippet sets title and subtitle text for a title slide, while subsequent code describes how these variables are used in template-based slide creation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\ntitle_text = \"NotRealCorp\"\nsubtitle_text = \"Quarterly financial planning meeting, Q3 2023\"\n```\n\n----------------------------------------\n\nTITLE: Completion Prompt Example (Text)\nDESCRIPTION: This example demonstrates a completion-style prompt where the LLM is encouraged to complete the provided text. The prompt includes the quote and then an incomplete sentence. The model is expected to complete the sentence by providing the author's name.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nâ€œSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.â€\nâ€• Ted Chiang, Exhalation\n\nThe author of this quote is\n```\n\n----------------------------------------\n\nTITLE: Instruction Prompt Example (Text)\nDESCRIPTION: This code snippet demonstrates an instruction prompt for extracting the author's name from a given quotation. The model is explicitly instructed to extract the author's name. The expected output is the author's name.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nExtract the name of the author from the quotation below.\n\nâ€œSome humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.â€\nâ€• Ted Chiang, Exhalation\n```\n\n----------------------------------------\n\nTITLE: Printing Titles and Contents of Retrieved Documents in Python\nDESCRIPTION: Iterates over the set of most relevant documents from a query and prints out their titles and content in a readable format. Assumes each document object exposes 'metadata' and 'page_content' attributes. Output is for human inspection or debugging of retrieval quality.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Print out the title and content for the most relevant retrieved documents\nprint(\"\\n\".join(['Title: ' + x.metadata['title'].strip() + '\\n\\n' + x.page_content + '\\n\\n' for x in query_docs]))\n\n```\n\n----------------------------------------\n\nTITLE: Loading CoQA Dataset using DuckDB in Python\nDESCRIPTION: Connects to an in-memory DuckDB database and uses its Hugging Face integration to load a subset (first 40 rows) of the CoQA validation dataset directly from a Parquet file hosted on Hugging Face. It then fetches all results and prints an example passage, question, and answer from the loaded data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Custom-LLM-as-a-Judge.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport duckdb\n\n# DuckDB has an easy wrapper for loading datasets from Hugging Face.\ncon = duckdb.connect(\":memory:\")\nfull_result = con.query(\"\"\"\n    SELECT * FROM 'hf://datasets/stanfordnlp/coqa/data/validation-00000-of-00001.parquet'\n        LIMIT 40\n\"\"\").fetchall()\n\nsingle_result = full_result[10]\n\nprint(\"Passage:\")\nprint(single_result[1])\n\nprint(\"\\nQuestion:\")\nprint(single_result[2][0])\n\nprint(\"\\nAnswer:\")\nprint(single_result[3][\"input_text\"][0])\n```\n\n----------------------------------------\n\nTITLE: Creating validation dataset for fine-tuning\nDESCRIPTION: Prepares a validation dataset using rows 101-200 from the recipe dataset to prevent overfitting during model training.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvalidation_df = recipe_df.loc[101:200]\nvalidation_data = validation_df.apply(\n    prepare_example_conversation, axis=1).tolist()\n```\n\n----------------------------------------\n\nTITLE: Defining Article Summarization Schema with Pydantic in Python\nDESCRIPTION: Creates a Pydantic model to define the structured schema for article summaries, including fields for invention year, summary, inventors, description, and a list of concept objects each with title and description. This schema will guide the expected structured output from the chat completion calls.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nsummarization_prompt = '''\n    You will be provided with content from an article about an invention.\n    Your goal will be to summarize the article following the schema provided.\n    Here is a description of the parameters:\n    - invented_year: year in which the invention discussed in the article was invented\n    - summary: one sentence summary of what the invention is\n    - inventors: array of strings listing the inventor full names if present, otherwise just surname\n    - concepts: array of key concepts related to the invention, each concept containing a title and a description\n    - description: short description of the invention\n'''\n\nclass ArticleSummary(BaseModel):\n    invented_year: int\n    summary: str\n    inventors: list[str]\n    description: str\n\n    class Concept(BaseModel):\n        title: str\n        description: str\n\n    concepts: list[Concept]\n```\n\n----------------------------------------\n\nTITLE: Checking Row Count in Hologres Table using SQL in Python\nDESCRIPTION: Executes a `SELECT count(*) FROM articles;` SQL query via the `psycopg2` cursor to retrieve the total number of rows in the 'articles' table. This confirms that the data upload process completed successfully. It fetches the result and prints the count.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Check the collection size to make sure all the points have been stored\ncount_sql = \"select count(*) from articles;\"\ncursor.execute(count_sql)\nresult = cursor.fetchone()\nprint(f\"Count:{result[0]}\")\n```\n\n----------------------------------------\n\nTITLE: Counting Points in Qdrant Collection\nDESCRIPTION: This Python command calls the `count` method on the Qdrant client for the 'Articles' collection. It returns the number of points currently stored in the collection, allowing verification of the upsert operation's success.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Check the collection size to make sure all the points have been stored\nclient.count(collection_name=\"Articles\")\n```\n\n----------------------------------------\n\nTITLE: SharePoint Search API OpenAPI Specification\nDESCRIPTION: This YAML file defines the OpenAPI 3.0.0 specification for the SharePoint Search API. It outlines API metadata, server URL, endpoint path, request body schema, and response schema for performing document searches in SharePoint via REST API. The specification includes detailed property descriptions, such as query parameters and response content, enabling developers to generate client code and understand API operation.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/sharepoint_azure_function/Using_Azure_Functions_and_Microsoft_Graph_to_Query_SharePoint.md#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\nopenapi: 3.0.0\ninfo:\n  title: SharePoint Search API\n  description: API for searching SharePoint documents.\n  version: 1.0.0\nservers:\n  - url: https://{your_function_app_name}.azurewebsites.net/api\n    description: SharePoint Search API server\npaths:\n  /{your_function_name}?code={enter your specific endpoint id here}:\n    post:\n      operationId: searchSharePoint\n      summary: Searches SharePoint for documents matching a query and term.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                query:\n                  type: string\n                  description: The full query to search for in SharePoint documents.\n                searchTerm:\n                  type: string\n                  description: A specific term to search for within the documents.\n      responses:\n        '200':\n          description: Search results\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  type: object\n                  properties:\n                    documentName:\n                      type: string\n                      description: The name of the document.\n                    snippet:\n                      type: string\n                      description: A snippet from the document containing the search term.\n                    url:\n                      type: string\n                      description: The URL to access the document.\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries for Azure RAG Implementation\nDESCRIPTION: Comprehensive import of Python libraries organized by category (standard, third-party, OpenAI, and Azure) needed for implementing Retrieval-Augmented Generation with Azure AI Search and Azure Functions.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Standard Libraries\nimport json  \nimport os\nimport platform\nimport subprocess\nimport csv\nfrom itertools import islice\nimport uuid\nimport shutil\nimport concurrent.futures\n\n# Third-Party Libraries\nimport pandas as pd\nfrom PyPDF2 import PdfReader\nimport tiktoken\nfrom dotenv import load_dotenv\nimport pyperclip\n\n# OpenAI Libraries (note we use OpenAI directly here, but you can replace with Azure OpenAI as needed)\nfrom openai import OpenAI\n\n# Azure Identity and Credentials\nfrom azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\nfrom azure.core.credentials import AzureKeyCredential  \nfrom azure.core.exceptions import HttpResponseError\n\n# Azure Search Documents\nfrom azure.search.documents import SearchClient, SearchIndexingBufferedSender  \nfrom azure.search.documents.indexes import SearchIndexClient  \nfrom azure.search.documents.models import (\n    VectorizedQuery\n)\nfrom azure.search.documents.indexes.models import (\n    HnswAlgorithmConfiguration,\n    HnswParameters,\n    SearchField,\n    SearchableField,\n    SearchFieldDataType,\n    SearchIndex,\n    SimpleField,\n    VectorSearch,\n    VectorSearchAlgorithmKind,\n    VectorSearchAlgorithmMetric,\n    VectorSearchProfile,\n)\n\n# Azure Management Clients\nfrom azure.mgmt.search import SearchManagementClient\nfrom azure.mgmt.resource import ResourceManagementClient, SubscriptionClient\nfrom azure.mgmt.storage import StorageManagementClient\n```\n\n----------------------------------------\n\nTITLE: Loading Simple Wikipedia Dataset using `datasets` Library in Python\nDESCRIPTION: This snippet uses the `datasets` library from Hugging Face to load the \"wikipedia\" dataset (specifically the \"20220301.simple\" version). It converts the training split into a list and optionally subsets it for demo or testing purposes, limiting the number of articles processed. It depends on the `datasets` library being installed (`pip install datasets`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n### STEP 1 - load the dataset\n\nfrom datasets import load_dataset\nfrom typing import List, Iterator\n\n# We'll use the datasets library to pull the Simple Wikipedia dataset for embedding\ndataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])\n\n# For testing, limited to 2.5k articles for demo purposes\ndataset = dataset[:2_500]\n\n# Limited to 25k articles for larger demo purposes\n# dataset = dataset[:25_000]\n\n# for free OpenAI acounts, you can use 50 objects\n# dataset = dataset[:50]\n```\n\n----------------------------------------\n\nTITLE: Counting Records in Postgres Table\nDESCRIPTION: This snippet checks the number of records in the `articles` table in the Postgres database by executing a `SELECT COUNT(*)` query. It retrieves the count and prints it to the console, verifying the data loading process.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Check the size of the data\ncount_sql = \"\"\"select count(*) from public.articles;\"\"\"\ncursor.execute(count_sql)\nresult = cursor.fetchone()\nprint(f\"Count:{result[0]}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installs the necessary Python libraries including deeplake for vector store management, langchain for chaining language model calls, openai for embeddings and models, and tiktoken for tokenization. This setup is essential for running the rest of the notebook successfully.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install deeplake langchain openai tiktoken\n```\n\n----------------------------------------\n\nTITLE: Loading and Limiting Dataset - Python\nDESCRIPTION: Uses the `datasets` library to load the 'wikipedia' dataset and limits the number of entries to a smaller subset for demonstration or testing purposes. This prepares the data in memory for import into Weaviate. Requires the `datasets` library.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n### STEP 1 - load the dataset\n\nfrom datasets import load_dataset\nfrom typing import List, Iterator\n\n# We'll use the datasets library to pull the Simple Wikipedia dataset for embedding\ndataset = list(load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"])\n\n# For testing, limited to 2.5k articles for demo purposes\ndataset = dataset[:2_500]\n\n# Limited to 25k articles for larger demo purposes\n# dataset = dataset[:25_000]\n\n# for free OpenAI acounts, you can use 50 objects\n# dataset = dataset[:50]\n```\n\n----------------------------------------\n\nTITLE: Checking curl Installation via HTTP Request - Shell\nDESCRIPTION: This shell command uses curl to check its availability and fetches the contents of the 'platform.openai.com' website. Its purpose is to verify that curl is installed and the system has internet access. No additional dependencies are required. The expected output is the raw HTTP response from the target website; if curl is not installed, an error message is shown. This command works on Linux, macOS, and other Unix-like terminals that ship with curl.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/curl-setup.txt#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://platform.openai.com\n```\n\n----------------------------------------\n\nTITLE: Formula for Frequency and Presence Penalties in Language Model Logits - Python\nDESCRIPTION: Shows the mathematical formula used to adjust the logits (unnormalized log-probabilities) during token sampling by subtracting frequency and presence penalties. This additive adjustment reduces repetitive token sampling where mu[j] is the original logit for token j, c[j] represents how often the token was previously sampled, and alpha coefficients control the strength of the penalties.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/text-generation.txt#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmu[j] -> mu[j] - c[j] * alpha_frequency - float(c[j] > 0) * alpha_presence\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Entry in Python\nDESCRIPTION: Prints a single example entry from the loaded dataset to quickly inspect its structure and content.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(\"An example entry:\")\nprint(philo_dataset[16])\n```\n\n----------------------------------------\n\nTITLE: Displaying the Dataframe\nDESCRIPTION: This code simply displays the created dataframe.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Configuring Kusto Connection Variables (Python)\nDESCRIPTION: Defines placeholder variables for the necessary Kusto connection details: AAD Tenant ID, Kusto Cluster URI, Database name, and Table name. These must be replaced with actual values.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# replace with your AAD Tenant ID, Kusto Cluster URI, Kusto DB name and Kusto Table\nAAD_TENANT_ID = \"\"\nKUSTO_CLUSTER =  \"\"\nKUSTO_DATABASE = \"Vector\"\nKUSTO_TABLE = \"Wiki\"\n```\n\n----------------------------------------\n\nTITLE: Generating Quote without Constraints (Python)\nDESCRIPTION: Demonstrates calling a `generate_quote` function with only a topic string ('politics and virtue') to produce a new quote using an underlying model.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nq_topic = generate_quote(\"politics and virtue\")\nprint(\"\\nA new generated quote:\")\nprint(q_topic)\n```\n\n----------------------------------------\n\nTITLE: Verifying and Setting OpenAI API Key (Python)\nDESCRIPTION: This Python code block verifies whether the `OPENAI_API_KEY` environment variable has been successfully set. It uses `os.getenv()` to retrieve the value and assigns it to `openai.api_key` if found. This ensures the OpenAI Python client is configured for making API requests.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Test that your OpenAI API key is correctly set as an environment variable\n# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables are live.\nimport os\nimport openai\n\n# Note. alternatively you can set a temporary env variable like this:\n# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n\nif os.getenv(\"OPENAI_API_KEY\") is not None:\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    print (\"OPENAI_API_KEY is ready\")\nel\n    print (\"OPENAI_API_KEY environment variable not found\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Cross-Encoder Reranking Workflow - Python\nDESCRIPTION: Installs required Python libraries including openai for language model access, arxiv for document queries, tenacity for retry logic, pandas for dataframe operations, and tiktoken for tokenization. These installations are prerequisites and must be run in a notebook or shell environment prior to any code execution. There is no input or output; it simply sets up the environment.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install openai\n!pip install arxiv\n!pip install tenacity\n!pip install pandas\n!pip install tiktoken\n```\n\n----------------------------------------\n\nTITLE: Download Dataset from Hugging Face\nDESCRIPTION: Downloads the specified 'netflix-shows' dataset from the Hugging Face `datasets` hub. The dataset is loaded as a 'train' split, which will be used as the source data for generating embeddings and inserting into Zilliz.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport datasets\n\n# Download the dataset \ndataset = datasets.load_dataset('hugginglearners/netflix-shows', split='train')\n```\n\n----------------------------------------\n\nTITLE: Installing the latest OpenAI Python package\nDESCRIPTION: Installs or upgrades the OpenAI Python package to the latest version using pip.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# make sure to use the latest version of the openai python package\n!pip install --upgrade --quiet openai\n```\n\n----------------------------------------\n\nTITLE: Removing Non-ASCII Characters from Transcript (Python)\nDESCRIPTION: Calls the `remove_non_ascii` function, passing the `full_transcript` as input. This filters out any non-ASCII characters from the raw transcript. The cleaned transcript is stored in the `ascii_transcript` variable.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Remove non-ascii characters from the transcript\nascii_transcript = remove_non_ascii(full_transcript)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependency (wget)\nDESCRIPTION: Installs the `wget` Python library using pip. This library is used for downloading files from the web.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%pip install wget\n```\n\n----------------------------------------\n\nTITLE: Download Wikipedia article embeddings ZIP archive\nDESCRIPTION: Downloads a large ZIP file containing precomputed Wikipedia article embeddings. The download may take time due to the file size (~700MB).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Pretty print Elasticsearch results\nDESCRIPTION: This snippet defines a function, `pretty_response`, to format and print the search results from Elasticsearch. For each hit, it extracts the ID, score, title, and text, and prints them in a user-friendly format.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Function to pretty print Elasticsearch results\n\ndef pretty_response(response):\n    for hit in response['hits']['hits']:\n        id = hit['_id']\n        score = hit['_score']\n        title = hit['_source']['title']\n        text = hit['_source']['text']\n        pretty_output = (f\"\\nID: {id}\\nTitle: {title}\\nSummary: {text}\\nScore: {score}\")\n        print(pretty_output)\n```\n\n----------------------------------------\n\nTITLE: Loading image description data and defining a lookup function in Python\nDESCRIPTION: Loads JSON lines containing image paths and descriptions into a list and defines a function to find an entry by a specified key-value pair, supporting contextual retrieval of metadata for images.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndata = []\nimage_path = 'train1.jpeg'\nwith open('description.json', 'r') as file:\n    for line in file:\n        data.append(json.loads(line))\ndef find_entry(data, key, value):\n    for entry in data:\n        if entry.get(key) == value:\n            return entry\n    return None\n```\n\n----------------------------------------\n\nTITLE: Displaying the Optimized Matrix Variable in Python\nDESCRIPTION: Simply accesses and displays the `best_matrix` variable, which contains the optimized NumPy array representing the projection matrix found during the training and hyperparameter search process. This allows for inspection or reuse of the trained matrix. Depends on the `best_matrix` variable being defined previously.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nbest_matrix  # this is what you can multiply your embeddings by\n```\n\n----------------------------------------\n\nTITLE: Checking Data Count and Index Status - Python/SQL\nDESCRIPTION: Checks the number of records in the `articles` table and the build status of the `article_content_index` vector index. This ensures that the data has been inserted correctly and the index is ready for searching. The index status must be 'Built' before proceeding with vector search.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# check count of inserted data\nprint(f\"articles count: {client.command('SELECT count(*) FROM default.articles')}\")\n\n# check the status of the vector index, make sure vector index is ready with 'Built' status\nget_index_status=\"SELECT status FROM system.vector_indices WHERE name='article_content_index'\"\nprint(f\"index build status: {client.command(get_index_status)}\")\n```\n\n----------------------------------------\n\nTITLE: Starting the Mirror Server\nDESCRIPTION: This command starts the mirror server, which is responsible for relaying audio data between the speaker and listener apps.  It executes the `mirror-server.mjs` file using Node.js.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnode mirror-server/mirror-server.mjs\n```\n\n----------------------------------------\n\nTITLE: Displaying First 10 Results from Output List in Python\nDESCRIPTION: Displays the first 10 elements from the output_list that contains the results of document relevance evaluations, useful for quickly checking the top results without viewing the entire dataset.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\noutput_list[:10]\n```\n\n----------------------------------------\n\nTITLE: Install nomic package using pip\nDESCRIPTION: This code snippet installs the `nomic` library using pip. The `nomic` library is necessary to interact with the Atlas platform.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_with_Atlas.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install nomic\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for Semantic Search\nDESCRIPTION: Installs the necessary Python packages: pinecone-client for vector database operations, openai client for generating embeddings, and datasets for loading example data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -qU \\\n    pinecone-client==3.0.2 \\\n    openai==1.10.0 \\\n    datasets==2.16.1\n```\n\n----------------------------------------\n\nTITLE: Testing Neon Postgres Connection\nDESCRIPTION: This code snippet tests the connection to the Neon Postgres database. It executes a simple SQL query (`SELECT 1;`) and checks the result. If the result is (1,), it prints a success message; otherwise, it indicates a connection failure.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Execute this query to test the database connection\ncursor.execute(\"SELECT 1;\")\nresult = cursor.fetchone()\n\n# Check the query result\nif result == (1,):\n    print(\"Your database connection was successful!\")\nelse:\n    print(\"Your connection failed.\")\n```\n\n----------------------------------------\n\nTITLE: Converting Patch to Commit Object in Python\nDESCRIPTION: Transforms a `Patch` object, containing parsed actions for multiple files, into a `Commit` object. It iterates through each action in the patch, using the original file contents (`orig`) to construct `FileChange` objects representing additions, deletions, or updates (calculating the new content for updates using `_get_updated_file`).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef patch_to_commit(patch: Patch, orig: Dict[str, str]) -> Commit:\n    commit = Commit()\n    for path, action in patch.actions.items():\n        if action.type is ActionType.DELETE:\n            commit.changes[path] = FileChange(\n                type=ActionType.DELETE, old_content=orig[path]\n            )\n        elif action.type is ActionType.ADD:\n            if action.new_file is None:\n                raise DiffError(\"ADD action without file content\")\n            commit.changes[path] = FileChange(\n                type=ActionType.ADD, new_content=action.new_file\n            )\n        elif action.type is ActionType.UPDATE:\n            new_content = _get_updated_file(orig[path], action, path)\n            commit.changes[path] = FileChange(\n                type=ActionType.UPDATE,\n                old_content=orig[path],\n                new_content=new_content,\n                move_path=action.move_path,\n            )\n    return commit\n```\n\n----------------------------------------\n\nTITLE: Preparing Function Calling Training Data for Fine-Tuning (JSON Format, Deprecated Parameters)\nDESCRIPTION: This snippet details how to format function calling examples for fine-tuning, using now-deprecated 'function_call' and 'functions' properties. The sample includes a user query, the assistant's function call, and schema definition for parameters. It is mainly for legacy compatibility as 'tools' is now recommended. Ensure valid argument schemas and proper role assignment for optimal results.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/data/oai_docs/fine-tuning.txt#_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"What is the weather in San Francisco?\" },\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"get_current_weather\",\n                \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"\n            }\n        }\n    ],\n    \"functions\": [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and country, eg. San Francisco, USA\"\n                    },\n                    \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n                },\n                \"required\": [\"location\", \"format\"]\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Deriving Ground Truth Labels from Evidence in Python\nDESCRIPTION: Maps the SciFact claim evidence to ground truth labels used for evaluation. If no evidence is provided, label the claim as 'NEE'. Otherwise, interpret consistent evidence labels as either 'True' (SUPPORT) or 'False' (CONTRADICT). This function handles the specific structure of the evidence data and returns a corresponding list of labels.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_groundtruth(evidence):\n    groundtruth = []\n    for e in evidence:\n        # Evidence is empty \n        if len(e) == 0:\n            groundtruth.append('NEE')\n        else:\n            # In this dataset, all evidence for a given claim is consistent, either SUPPORT or CONTRADICT\n            if list(e.values())[0][0]['label'] == 'SUPPORT':\n                groundtruth.append('True')\n            else:\n                groundtruth.append('False')\n    return groundtruth\n```\n\n----------------------------------------\n\nTITLE: Loading Transaction Embedding Data from CSV in Python\nDESCRIPTION: This snippet reads the transaction dataset with precomputed embeddings from CSV into a pandas DataFrame. It converts the string representations of embeddings into numpy arrays for further clustering and analysis.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.read_csv(embedding_path)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Loading Embedded Data into DataFrame\nDESCRIPTION: Loads the extracted CSV file containing embedded Wikipedia articles into a pandas DataFrame.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\narticle_df = pd.read_csv('../data/vector_database_wikipedia_articles_embedded.csv')\n```\n\n----------------------------------------\n\nTITLE: Creating Kusto Client (Python/Kusto)\nDESCRIPTION: Initializes a `KustoClient` object using the constructed connection string builder. This client is used to execute Kusto Query Language (KQL) commands and queries.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nKUSTO_CLIENT = KustoClient(KCSB)\n```\n\n----------------------------------------\n\nTITLE: Displaying Initial Rows of the DataFrame in Python\nDESCRIPTION: Shows the first few rows of the `article_df` pandas DataFrame using the `.head()` method. This provides a quick visual inspection of the data structure, including columns like 'title', 'text', 'title_vector', and 'content_vector'.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying First Few Rows of DataFrame\nDESCRIPTION: This snippet displays the first few rows of the loaded DataFrame to inspect the data.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\narticle_df.head()\n```\n\n----------------------------------------\n\nTITLE: Finding Context with EOF Handling in Python\nDESCRIPTION: A wrapper around `find_context_core` that modifies search behavior based on the `eof` (end of file) flag. If `eof` is true and the context is found near the end, it returns that index. Otherwise, it searches from the specified `start` index, adding a large penalty (10,000) to the fuzz score if `eof` was true and the initial end-of-file search failed.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef find_context(\n    lines: List[str], context: List[str], start: int, eof: bool\n) -> Tuple[int, int]:\n    if eof:\n        new_index, fuzz = find_context_core(lines, context, len(lines) - len(context))\n        if new_index != -1:\n            return new_index, fuzz\n        new_index, fuzz = find_context_core(lines, context, start)\n        return new_index, fuzz + 10_000\n    return find_context_core(lines, context, start)\n```\n\n----------------------------------------\n\nTITLE: Printing Page Content Python\nDESCRIPTION: Prints the content of another specific document. This further illustrates how the content can be extracted from the loaded documents.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(docs[5].page_content)\n```\n\n----------------------------------------\n\nTITLE: Extracting Embeddings ZIP Archive\nDESCRIPTION: Extracts the downloaded ZIP file containing precomputed embeddings and verifies the CSV file exists in the data directory.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\nimport os\nimport re\nimport tempfile\n\ncurrent_directory = os.getcwd()\nzip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\noutput_directory = os.path.join(current_directory, \"../../data\")\n\nwith zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n    zip_ref.extractall(output_directory)\n\n\n# check the csv file exist\nfile_name = \"vector_database_wikipedia_articles_embedded.csv\"\ndata_directory = os.path.join(current_directory, \"../../data\")\nfile_path = os.path.join(data_directory, file_name)\n\n\nif os.path.exists(file_path):\n    print(f\"The file {file_name} exists in the data directory.\")\nelse:\n    print(f\"The file {file_name} does not exist in the data directory.\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Natural Questions Dataset\nDESCRIPTION: Downloads sample questions and answers from Google's Natural Questions dataset using wget. This provides additional data for the knowledge base.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\n# All the examples come from https://ai.google.com/research/NaturalQuestions\n# This is a sample of the training set that we download and extract for some\n# further processing.\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/questions.json\")\nwget.download(\"https://storage.googleapis.com/dataset-natural-questions/answers.json\")\n```\n\n----------------------------------------\n\nTITLE: Accessing a Single Data Entry from the Loaded Dataset in Python\nDESCRIPTION: Displays the first record in the loaded transcription dataset to show the typical data structure. Each entry includes fields like 'text', 'title', 'start', and others relevant to the transcript segments.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndata[0]\n```\n\n----------------------------------------\n\nTITLE: Download Pre-embedded Data\nDESCRIPTION: Downloads a zip file containing pre-embedded Wikipedia articles from an OpenAI URL using the wget library. This dataset will be used for indexing and searching.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport wget\n\nembeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Downloading Audio File\nDESCRIPTION: This code downloads an example audio file from a remote URL and saves it locally. It uses the `urllib.request.urlretrieve` function to fetch the file and stores it in the specified local path. The audio file is used for testing the transcription models.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# set download paths\nZyntriQix_remote_filepath = \"https://cdn.openai.com/API/examples/data/ZyntriQix.wav\"\n\n\n# set local save locations\nZyntriQix_filepath = \"data/ZyntriQix.wav\"\n\n# download example audio files and save locally\nurllib.request.urlretrieve(ZyntriQix_remote_filepath, ZyntriQix_filepath)\n```\n\n----------------------------------------\n\nTITLE: Define Configuration Constants\nDESCRIPTION: Defines various configuration parameters required for connecting to Zilliz and interacting with OpenAI. This includes connection details (URI, Token), collection parameters (name, embedding dimension), OpenAI model engine, API key, index settings, query parameters, and batch size for data insertion.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport openai\n\nURI = 'your_uri'\nTOKEN = 'your_token' # TOKEN == user:password or api_key\nCOLLECTION_NAME = 'book_search'\nDIMENSION = 1536\nOPENAI_ENGINE = 'text-embedding-3-small'\nopenai.api_key = 'sk-your_key'\n\nINDEX_PARAM = {\n    'metric_type':'L2',\n    'index_type':\"AUTOINDEX\",\n    'params':{}\n}\n\nQUERY_PARAM = {\n    \"metric_type\": \"L2\",\n    \"params\": {},\n}\n\nBATCH_SIZE = 1000\n```\n\n----------------------------------------\n\nTITLE: Visualizing employment trends with JavaScript\nDESCRIPTION: This JavaScript snippet creates an interactive line chart to visualize monthly employment data trends over time using Chart.js library. It processes an array of employment figures and renders a dynamic chart for analytical purposes. It assumes a web environment with Chart.js loaded.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/assets/007.txt#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst ctx = document.getElementById('employmentChart').getContext('2d');\nconst employmentData = {\n    labels: ['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n    datasets: [{\n        label: 'Jobs Added',\n        data: [146000, 150000, 155000, 160000, 158000],\n        fill: false,\n        borderColor: 'blue',\n        tension: 0.1\n    }]\n};\nconst config = {\n    type: 'line',\n    data: employmentData,\n    options: {}\n};\nconst employmentChart = new Chart(ctx, config);\n```\n\n----------------------------------------\n\nTITLE: Printing Loaded Article Contents in Python\nDESCRIPTION: Simply prints the list of loaded article content strings to verify that file reading was successful and the correct data has been loaded for summarization.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(content)\n```\n\n----------------------------------------\n\nTITLE: Downloading Embedded Data\nDESCRIPTION: This snippet downloads a zipped file containing pre-computed embeddings of Wikipedia articles. It uses the `wget` library to download the file from the specified URL.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nembeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n\n# The file is ~700 MB so this will take some time\nwget.download(embeddings_url)\n```\n\n----------------------------------------\n\nTITLE: Printing the Response\nDESCRIPTION: Prints the response from the query engine.  This will show the LLM's response to the provided query.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse_vector.response\n```\n\n----------------------------------------\n\nTITLE: Importing libraries and setting up model dependencies in Python\nDESCRIPTION: Loads necessary Python modules including FAISS, JSON, Torch, OpenAI SDK, clip, and visualization tools for image display and processing. Sets up the environment for model inference, data handling, and visualization tasks within the RAG pipeline.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport faiss\nimport json\nimport torch\nfrom openai import OpenAI\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport clip\nclient = OpenAI()\nfrom tqdm import tqdm\nimport os\nimport numpy as np\nimport pickle\nfrom typing import List, Union, Tuple\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport base64\n```\n\n----------------------------------------\n\nTITLE: Creating Search File for Context Retrieval - Python\nDESCRIPTION: This code segment is deprecated. It prepares data in JSONL format for creating a search file using OpenAI's API. It selects relevant columns ('context', 'tokens'), renames 'context' to 'text' and 'tokens' to 'metadata', and then converts the DataFrame to a JSONL format. The API call creates and uploads the JSONL file. This approach is deprecated in favor of embeddings.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = df[df.tokens<2000]\ndf[['context', 'tokens']].rename(columns={'context':'text','tokens':'metadata'}).to_json('olympics-data/olympics_search.jsonl', orient='records', lines=True)\n\nsearch_file = client.files.create(\n  file=open(\"olympics-data/olympics_search.jsonl\"),\n  purpose='search'\n)\nolympics_search_fileid = search_file['id']\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Modules\nDESCRIPTION: Installs the necessary Python packages (openai, psycopg2, pandas, wget, python-dotenv) using pip. These packages are required for interacting with the OpenAI API, connecting to PostgreSQL, data manipulation, downloading files, and managing environment variables, respectively.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai psycopg2 pandas wget python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This code snippet installs the required Python packages using pip. These packages include libraries for scientific computing (scipy), handling network requests (tenacity), tokenization (tiktoken), terminal styling (termcolor), OpenAI API interaction (openai), arXiv searching (arxiv), data manipulation (pandas), PDF processing (PyPDF2), and progress bar display (tqdm).\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install scipy --quiet\n!pip install tenacity --quiet\n!pip install tiktoken==0.3.3 --quiet\n!pip install termcolor --quiet\n!pip install openai --quiet\n!pip install arxiv --quiet\n!pip install pandas --quiet\n!pip install PyPDF2 --quiet\n!pip install tqdm --quiet\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for OpenAI and Zilliz Integration\nDESCRIPTION: Installs the necessary Python packages: openai for generating embeddings, pymilvus for connecting to Zilliz, datasets for downloading the book data, and tqdm for progress tracking.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install openai pymilvus datasets tqdm\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies for Box Azure Function\nDESCRIPTION: This file lists the required Python packages for the Azure Function example. It includes `boxsdk[jwt]` for Box API interaction with JWT auth, `azure-functions` for the Azure Functions framework, `requests` for HTTP calls, and `pyjwt` for JWT decoding.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nboxsdk[jwt]\nazure-functions\nrequests\npyjwt\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository\nDESCRIPTION: This command clones the project repository from the specified URL. Replace `<repository-url>` with the actual URL of the repository.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <repository-url>\n```\n\n----------------------------------------\n\nTITLE: Enabling Required GCP APIs (Shell)\nDESCRIPTION: Executes `gcloud services enable` commands to activate the Cloud Functions, Cloud Build, and BigQuery APIs for the currently configured GCP project. Enabling these APIs is necessary to allow the project to create and manage resources related to these services.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n! gcloud services enable cloudfunctions.googleapis.com\n! gcloud services enable cloudbuild.googleapis.com\n! gcloud services enable bigquery.googleapis.com\n```\n\n----------------------------------------\n\nTITLE: Starting the Application\nDESCRIPTION: This command starts the speaker and listener apps.  It executes the `start` script defined in the `package.json` file, typically using a development server such as webpack-dev-server or create-react-app's built-in server. The apps will be available at the specified URLs.\nSOURCE: https://github.com/openai/openai-cookbook/blob/main/examples/voice_solutions/one_way_translation_using_realtime_api/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm start\n```"
  }
]